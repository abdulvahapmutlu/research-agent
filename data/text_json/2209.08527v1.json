{
  "filename": "2209.08527v1.pdf",
  "num_pages": 17,
  "pages": [
    "ADAPTIVE DIMENSION REDUCTION AND VARIATIONAL\nINFERENCE FOR TRANSDUCTIVE FEW-SHOT CLASSIFICATION\nA PREPRINT\nYuqing Hu\nOrange Labs, Cesson-Sévigné, France\nIMT-Atlantique, Brest, France\nyuqing.hu@imt-atlantique.fr\nStéphane Pateux\nOrange Labs, Cesson-Sévigné, France\nstephane.pateux@orange.com\nVincent Gripon\nIMT-Atlantique, Brest, France\nvincent.gripon@imt-atlantique.fr\nSeptember 20, 2022\nABSTRACT\nTransductive Few-Shot learning has gained increased attention nowadays considering the cost of data\nannotations along with the increased accuracy provided by unlabelled samples in the domain of few\nshot. Especially in Few-Shot Classiﬁcation (FSC), recent works explore the feature distributions\naiming at maximizing likelihoods or posteriors with respect to the unknown parameters. Following\nthis vein, and considering the parallel between FSC and clustering, we seek for better taking into\naccount the uncertainty in estimation due to lack of data, as well as better statistical properties of\nthe clusters associated with each class. Therefore in this paper we propose a new clustering method\nbased on Variational Bayesian inference, further improved by Adaptive Dimension Reduction based\non Probabilistic Linear Discriminant Analysis. Our proposed method signiﬁcantly improves accuracy\nin the realistic unbalanced transductive setting on various Few-Shot benchmarks when applied to\nfeatures used in previous studies, with a gain of up to 6% in accuracy. In addition, when applied\nto balanced setting, we obtain very competitive results without making use of the class-balance\nartefact which is disputable for practical use cases. We also provide the performance of our method\non a high performing pretrained backbone, with the reported results further surpassing the current\nstate-of-the-art accuracy, suggesting the genericity of the proposed method.\n1\nIntroduction\nFew-shot learning, and in particular Few-Shot Classiﬁcation, has become a subject of paramount importance in the last\nyears with a large number of methodologies and discussions. Where large datasets continuously beneﬁt from improved\nmachine learning architectures, the ability to transfer this performance to the low-data regime is still a challenge due\nto the high uncertainty posed using few labels. In more details, there are two main types of FSC tasks. In inductive\nFSC [1, 36, 46, 33], the situation comes to its extremes with only a few data samples available for each class, leading\nsometimes to completely intractable settings, such as when facing a black dog on the one hand and a white cat on\nthe other hand. In transductive FSC, additional unlabelled samples are available for prediction, leading to improved\nreliability and more elaborate solutions [24, 23, 2].\nInductive FSC is likely to occur when data acquisition is difﬁcult or expensive, or when categories of interest correspond\nto rare events. Transductive FSC is more likely encountered when data labeling is expensive, for fast prototyping of\nsolutions, or when the categories of interest are rare and hard to detect. Since the latter correspond to situations where it\nis possible to exploit, at least partially, the distribution of unlabelled samples, the trend evolved to using potentially\nvarying parts of this additional source of information. With most standardized benchmarks using very limited scope of\nvariability in the generated Few-Shot tasks, this even came to the point the best performing methods are often relying\narXiv:2209.08527v1  [cs.LG]  18 Sep 2022\n",
    "Adaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\n(a) Few-Shot task\n(b) Initialization\n(c) PLDA and VB inference\nFigure 1: Summary of the proposed method. Here we illustrate a 3-way classiﬁcation task in a standard 2-simplex\nusing soft-classiﬁcation probabilities. Trajectories show the evolution across iterations. For a given Few-Shot task\nwhich nearest-class-mean probabilities are depicted in (a), a Soft-KMEANS clustering method is performed in (b) to\ninitialize onk (see Alg. 1). Then in (c) an iteratively reﬁned Variational Bayesian (VB) model with Adaptive Dimension\nReduction using Probabilistic Linear Discriminant Analysis (PLDA) is applied to obtain the ﬁnal class predictions.\non questionable information, such as equidistribution between the various classes among the unlabelled samples, that is\nunlikely realistic in applications.\nThis limitation of benchmarking for transductive FSC has recently been discussed in [39]. In this paper, the authors\npropose a new way of generating transductive FSC benchmarks where the distribution of samples among classes can\ndrastically change from a Few-Shot generated task to the next one. Interestingly, they showed the impact of generating\nclass imbalance on the performance on various popular methods, resulting in some cases in drops in average accuracy\nof more than 10%.\nA simple way to reach state-of-the-art performance in transductive FSC consists in extracting features from the available\nsamples using a pretrained backbone deep learning architecture, and then using semi-supervised clustering routines to\nestimate samples distribution among classes. Due to the very limited number of available samples, distribution-agnostic\nclustering algorithms are often preferred, such as K-MEANS or its variants [29, 25, 32] or mean-shift [9] for instance.\nIn this paper, we are interested in showing it is possible to combine data reduction with statistical inference through a\nVariational Bayesian (VB) [13] approach. Here, data reduction helps considerably reduce the number of parameters\nto infer, while VB provides more ﬂexibility than the usual K-Means methods. Interestingly, the proposed approach\ncan easily cope with standard equidistributed Few-Shot tasks or the unbalanced ones proposed in [39], deﬁning a new\nstate-of-the-art for ﬁve popular transductive Few-Shot vision classiﬁcation benchmarks.\nOur claims are the following:\n• We introduce a novel semi-supervised clustering algorithm based on VB inference and Probabilistic Linear\nDiscriminant Analysis (PLDA),\n• We demonstrate the general utility of our proposed method being able to improve accuracy in a variety of deep\nlearning models and settings,\n• We show the ability of the proposed method to reach state-of-the-art transductive FSC performance on multiple\nvision benchmarks (balanced and unbalanced).\n2\nRelated work\nThere are two main frameworks in the ﬁeld of FSC: 1) only one unlabelled sample is processed at a time for class\npredictions, which is called inductive FSC, and 2) the entire unlabelled samples are available for further estimations,\nwhich is called transductive FSC. Inductive methods focus on training a feature extractor that generalizes well the\nembedding in a feature sub-space, they include meta learning methods such as [12, 26, 2, 40, 30, 37] that train a model\nin an episodic manner, and transfer learning methods [8, 28, 48, 5, 3, 33] that train a model with a set of mini-batches.\nRecent state-of-the-art works on inductive FSC [46, 47, 43, 19] combine the above two strategies and propose a transfer\nbased training used as model initialization, followed by an episodic training that adapts the model to better ﬁt the\nFew-Shot tasks.\nTransductive methods are becoming more and more popular thanks to their better performance due to the use of\nunlabelled data, as well as their utility in situations where data annotation is costly. Early literature of this branch\n2\n",
    "Adaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\noperates on a class-balanced setting where unlabelled instances are evenly distributed among targeted classes. Graph-\nbased methods [14, 7, 44, 21] make use of the afﬁnity among features and propose to group those that belong to the\nsame class. More recent works such as [16] propose methods based on Optimal Transport that realizes sample-class\nallocation with a minimum cost. While effective, these methods often require class-balanced priors to work well, which\nis not realistic due to the arbitrary unknown query set. In [39] the authors put forward a novel unbalanced setting that\ncomposes a query set with unlabelled instances sampled following a Dirichlet distribution, injecting more imbalance for\npredictions.\nIn this paper we propose a clustering method to solve transductive FSC, where the aim is to estimate cluster parameters\ngiving high predictions for unlabelled samples. Under Gaussian assumptions, previous works [25, 32] have utilised\nalgorithms such as Expectation Maximization [10] (EM), with the goal of maximizing likelihoods or posteriors with\nrespect to the parameters for a cluster, with the hidden variables marginalized. However, this may not be the most\nsuitable way due to the scarcity of available data in a given Few-Shot task, which increases the level of uncertainty for\ncluster estimations. Therefore, in this paper we propose a Variational Bayesian (VB) approach, in which we regard some\nunknown parameters as hidden variables in order to inject more ﬂexibility into the model, and we try to approximate\nthe posterior of the hidden variables by a variational distribution.\nAs models with too few labelled samples often give too much randomness for a cluster to be stably reckoned, they\noften require the use of feature dimension reduction techniques to stabilize cluster estimations. Previous literature\nsuch as [25] applies a PCA method that reduces dimension in a non-supervised manner, and [6] proposes a modiﬁed\nLDA during backbone training that maximizes the ratio of inter/intra-class distance. In this paper we propose to use\nProbabilistic Linear Discriminant Analysis [17] (PLDA) that 1) is applied on extracted features, 2) ﬁts data more\ndesirably into distribution assumptions, and 3) is semi-supervised in combination of a VB model. We integrate PLDA\ninto the VB model in order to reﬁne the reduced space through iterations.\n3\nMethodology\nIn this section, we ﬁrstly present the standard setting in transductive FSC, including the latest unbalanced setting\nproposed by [39] where unlabelled samples are non-uniformly distributed among classes. Then we present our proposed\nmethod combining PLDA and VB inference.\n3.1\nProblem formulation\nFollowing other works in the domain, our proposed method is operated on a feature space obtained from a pre-trained\nbackbone. Namely, we are given the extracted features of 1) a generic base class dataset Dbase = {xbase\ni\n}Nbase\ni=1\n∈Cbase\nthat contains Nbase labelled samples where each sample xbase\ni\nis a column vector of length D, and Cbase is the set of\nbase classes to which these samples belong. These base classes have been used to train the backbone. And similarly,\n2) a novel class dataset Dnovel = {xnovel\nn\n}N\nn=1 containing N samples belonging to a set of K novel classes Cnovel\n(Cbase ∩Cnovel = ∅). On this novel dataset, only a few elements are labelled, and the aim is to predict the missing\nlabels. Denote X the matrix obtained by aggregating elements in Dnovel row-wise.\nWhen benchmarking transductive FSC methods, it is common to randomly generate Few-Shot tasks by sampling Dnovel\nfrom a larger dataset. These tasks are generated by sampling K distinct classes, L distinct labelled elements for each\nclass (called support set) and Q total unlabelled elements without repetition and distinct from the labelled ones (called\nquery set). All these unlabelled elements belong to one of the selected classes. We obtain a total of N = KL + Q\nelements in the task, and compute the accuracy on the Q unlabelled ones. Depending on how unlabelled instances are\ndistributed among selected classes within a task, we further distinguish a balanced setting where the query set is evenly\ndistributed among the K classes, from an unbalanced setting where it can vary from class to class. An automatic way\nto generate such unbalanced Few-Shot tasks has been proposed in [39] where the number of elements to draw from\neach class is determined using a Dirichlet distribution parameterized by α∗\no1, where 1 is the all-one vector. To solve a\ntransductive FSC task, our method is composed of PLDA and VB inference, that we introduce in the next paragraphs.\n3.2\nProbabilistic Linear Discriminant Analysis (PLDA)\nIn our work, PLDA [17] is mainly used to reduce feature dimensions. For a Few-Shot task X, let Φw be a positive\ndeﬁnite matrix representing the estimated shared within-class covariance of a given class, and Φb be a positive semi-\ndeﬁnite matrix representing the estimated between-class covariance that generates class variables. The goal of PLDA is\nto project data onto a subspace while maximizing the signal-to-noise ratio for class labelling. In details, we obtain a\n3\n",
    "Adaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\nprojection matrix W that diagonalizes both Φw and Φb and yield the following equations:\nWT ΦwW = I,\nWT ΦbW = Ψ\n(1)\nwhere I is an identity matrix and Ψ is a diagonal matrix. In this paper, we assume a similar distribution between\nthe pre-trained base classes and the transferred novel classes [45]. Therefore we propose to estimate Φw to be the\nwithin-class scatter matrix of Dbase, denoted as Sbass\nw\n. In practice we implement PLDA by ﬁrstly transforming X using\na rotation matrix R ∈RD×D and a set of scaling values s ∈RD obtained from Sbase\nw\n. Note that we clamp the scaling\nvalues to be no larger than an upper-bound smax in order to prevent too large values, smax is a hyper-parameter. Then\nwe project the transformed data onto their estimated class centroids space, in accordance with the d largest eigenvalues\nof Ψ, and obtain dimension-reduced data U = [u1, ..., un, ..., uN]T ∈RN×d where un = WT xn and d = K −1.\nMore detailed implementation can be found in Appendix.\n3.3\nVariational Bayesian (VB) Inference\nDuring VB inference, we operate on a reduced d-dimensional space obtained after applying PLDA. Considering a\nGaussian mixture model for a given task U ∈RN×d in reduced space, let θ be the unknown variables of the model. In\nVB we attempt to ﬁnd a probability distribution q(θ) that approximates the true posterior p(θ|U), i.e. maximizes the\nELBO (see Appendix for more details). In our case, we deﬁne θ = {Z, π, µ} where Z = {zn}N\nn=1 is a set of latent\nvariables used as class indicators, each latent variable zn has an one-of-K representation, π is a K-dimensional vector\nrepresenting mixing ratios between the classes, and µ = {µk}K\nk=1 where µk is the centroid for class k. Note that 1)\ncontrary to EM where π, µ are seen as parameters that can be estimated directly, in VB they are deemed as hidden\nvariables following certain distribution laws. 2) This is not a full VB model due to the lack of precision matrix (i.e.\nthe inverse of covariance matrix) as a variable in θ. Although a VB model frees up more parameters for the unknown\nvariables, it also increases the instability in estimations so that the model becomes too sensible. Therefore, in this paper\nwe impose an assumption that all classes in U share the same precision matrix and it is ﬁxed during VB iterations.\nNamely we deﬁne Λk = Λ = TvbI for k = 1, ..., K, where Tvb is a hyper-parameter in order to compensate the\nvariation between base and estimated novel class distributions.\nIn order for a model to be in a variational bayesian setting, we deﬁne priors and likelihoods on the unknown variables,\nwith several initialization parameters attached:\npriors :\np(π) = Dir(π|αo),\np(µ) =\nK\nY\nk=1\nN(µk|mo, (βoΛ)−1),\nlikelihoods :\np(Z|π) =\nN\nY\nn=1\nCategorical(zn|π),\np(U|Z, µ) =\nN\nY\nn=1\nK\nY\nk=1\nN(un|µk, Λ−1)znk\n(2)\nwhere π follows a K-dimensional symmetric Dirichlet distribution, with αo being the prior of component weight for\neach class, which we set to 2.0 in accordance with [39], i.e. the same value as the Dirichlet distribution parameter α∗\no\nthat is used to generate Few-Shot tasks. The vector mo is the prior about the class centroid variables, we let it to be 0.\nAnd βo stands for the prior about the moving range of class centroid variables: the larger it is, the closer the centroids\nare to mo. We empirically found that βo = 10.0 gives consistent good results across datasets and FSC problems.\nAs previously stated, we approximate a variable distribution to the true posterior. To further simplify, we follow the\nMean-Field assumption [31, 18] and assume that the unknown variables are independent from one another. Therefore\nwe let q(θ) = q(Z, π, µ) = q(Z)q(π)q(µ) ≈p(Z, π, µ/U) and solve for each term. The explicit formulation for\nthese marginals is provided in Eq. 3, 4 (see Appendix for more details). The estimation of the various parameters is\nthen classically performed through an iterative EM framework as presented further.\nDenote on = [on1, ..., onk, ..., onK] as the soft class assignment for un (onk ≥0, PK\nk=1 onk = 1), and onk represents\nthe portion of nth sample allocated to kth class.\n4\n",
    "Adaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\nM step:\nIn this step we estimate q(π) and q(µ) in use of the class assignments onk:\np(π) = Dir(π|αo)\n=⇒\nq∗(π) = Dir(π|α)\nwith\nαk = αo + Nk,\np(µ) =\nK\nY\nk=1\nN(µk|mo, (βoΛ)−1)\n=⇒\nq∗(µ) =\nK\nY\nk=1\nN(µk|mk, (βkΛ)−1)\nwith\nβk = βo + Nk, mk = 1\nβk\n(βomo +\nN\nX\nn=1\nonkun),\n(3)\nwhere α = [α1, ..., αk, ..., αK] are the estimated component weights for classes, and Nk = PN\nn=1 onk is the sum of\nthe soft assignments for all samples in class k. We also estimate the moving range parameter βk and the centroid mk\nfor each class centroid variable. We observe that the posteriors take the same forms as the priors. Demonstration of\nthese results is presented in Appendix.\nE step:\nIn this step we estimate q(Z) by updating onk, using the current values of all other parameters computed in\nthe M-step, i.e. αk, βk and mk.\np(Z|π) =\nN\nY\nn=1\nCategorical(zn|π)\n=⇒\nq∗(Z) =\nN\nY\nn=1\nCategorical(zn|on)\n(4)\nwhere each element onk can be computed as onk =\nρnk\nPK\nj=1 ρnj in which:\nlog ρnk = ψ(αk) −ψ(\nK\nX\nj=1\nαj) + 1\n2 log |Λ| −d\n2 log 2π −1\n2[dβ−1\nk\n+ (un −mk)T Λ(un −mk)],\n(5)\nwith ψ(·) being the logarithmic derivative of the gamma function (also known as the digamma function). We observe\nthat q∗(Z) follows the same categorical distribution as the likelihood, and it is parameterized by onk. More details can\nbe found in Appendix.\nProposed algorithm\nThe proposed method combines PLDA and VB inference which leads to an Efﬁciency\nGuided Adaptive Dimension Reduction for VAriational BAyesian inference. We thus name our proposed method\n“BAVARDAGE”, and the detailed description is presented in Algorithm 1. Given a Few-Shot task X and a within-class\nscatter matrix Sbase\nw\n, we initialize onk using EM algorithm with an assumed covariance matrix, adjusted by a temperature\nhyper-parameter Tkm, for all classes. Note that this is equivalent to Soft-KMEANS [20] algorithm. And for each\niteration we update parameters: in M step we update αk, βk and centroids mk, in E step we only update onk, and we\napply PLDA with the updated onk to reduce feature dimensions. Finally, predicted labels are obtained by selecting the\nclass that corresponds to the largest value in onk.\nThe illustration of our proposed method is presented in Figure 1. For a Few-Shot task that has three classes (red, blue\nand green) with unlabelled samples depicted on the probability simplex, we ﬁrstly initialize onk with Soft-KMEANS\nwhich directs some data points to their belonging classes while further distancing some points from their targeted\nclasses. Then we apply the proposed VB inference integrated with PLDA, resulting in additional points moving towards\ntheir corresponding classes.\n4\nExperiments\nIn this section we provide details on the standard transductive Few-Shot classiﬁcation settings, and we evaluate the\nperformance of our proposed method.\nBenchmarks\nWe test our method on standard Few-Shot benchmarks: mini-Imagenet [35], tiered-Imagenet [32] and\ncaltech-ucsd birds-200-2011 (CUB) [41]. mini-Imagenet is a subset of ILSVRC-12 [35] dataset, it contains a total of\n60, 000 images of size 84 × 84 belonging to 100 classes (600 images per class) and follows a 64-16-20 split for base,\nvalidation and novel classes. tiered-Imagenet is a larger subset of ILSVRC-12 containing 608 classes with 779, 165\nimages of size 84×84 in total, and we use the standard 351-97-160 split, and CUB is composed of 200 classes following\na 100-50-50 split (Image size: 84 × 84). In Appendix we also show the performance of our proposed method on other\nwell-known Few-Shot benchmarks such as FC100 [30] and CIFAR-FS [4].\n5\n",
    "Adaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\nAlgorithm 1 BAVARDAGE\nInputs: X ∈RN×D, Sbase\nw\n∈RD×D\nHyper-parameters: Tkm, Tvb, smax\nPriors for VB: αo = 2.0, βo = 10.0, mo = 0, Λ = Tvb · I\nInitializations: onk = EM (X, Tkm)\nfor i = 1 to nstep do\nU = PLDA (X, Sbase\nw\n, smax, onk)\n# See more details in Appendix.\nVB (M step):\nαk = αo + PN\nn=1 onk\nβk = βo + PN\nn=1 onk\nmk =\n1\nβk (βomo + PN\nn=1 onkun)\nVB (E step):\nlog ρnk = ψ(αk) −ψ(PK\nj=1 αj) + 1\n2 log |Λ| −d\n2 log 2π −1\n2[dβ−1\nk\n+ (un −mk)T Λ(un −mk)]\nonk =\nρnk\nPK\nj=1 ρnj\nend for\nreturn ˆℓ(xn) = arg maxk(onk)\nSettings\nFollowing previous works [25, 34, 39], our proposed method is evaluated on 1-shot 5-way (K = 5, L = 1),\nand 5-shot 5-way (K = 5, L = 5) scenarios. As for the query set, we set a total number of Q = 75 unlabelled samples,\nfrom which we further deﬁne two settings: 1) a balanced setting where unlabelled instances are evenly distributed\namong K classes, and 2) an unbalanced setting where the query set is randomly distributed, following a Dirichlet\ndistribution parameterized by α∗\no. In our paper we follow the same setting as [39] and set α∗\no = 2.0, further experiments\nwith different values are conducted in the next sections. The performance of our proposed method is evaluated by\ncomputing the averaged accuracy over 10, 000 Few-Shot tasks.\nImplementation details\nIn this paper we ﬁrstly compare our proposed algorithm with the other state-of-the-art\nmethods using the same pretrained backbones and benchmarks provided in [39]. Namely we extract the features\nusing the same ResNet-18 (RN18) and WideResNet28_10 (WRN) neural models, and present the performance on\nmini-Imagenet, tiered-Imagenet and CUB datasets. In our proposed method, the raw features are preprocessed\nfollowing [42]. As for the hyper-parameters, we set Tkm = 10, Tvb = 50, smax = 2 for the balanced setting;\nTkm = 50, Tvb = 50, smax = 1 for the unbalanced setting, and we use the same VB priors for all settings. To further\nshow the functionality of our proposed method on different backbones and other benchmarks, we tested BAVARDAGE\non a recent high performing feature extractor trained on a ResNet-12 (RN12) neural model [28, 3], and we report the\naccuracy in Table 1 and in Appendix with various settings.\n4.1\nMain results\nThe main results on the relevant settings are presented in Table 1. Note that we report the accuracy of other methods\nfollowing [39], and add the performance of our proposed method in comparison, using the same pretrained RN18\nand WRN feature extractors, and we also report the result of a RN12 backbone pretrained following [3]. We observe\nthat our proposed algorithm reaches state-of-the-art performance for nearly all referenced datasets in the unbalanced\nsetting, surpassing previous methods by a noticeable margin especially on 1-shot. In the balanced setting we also\nreach competitive accuracy compared with [16] along with other works that make use of a perfectly balanced prior on\nunlabelled samples, while our proposed method suggests no such prior. In addition, we provide results on the other\nFew-Shot benchmarks with different settings in Appendix.\n4.2\nAblation studies\nAnalysis on the elements of BAVARDAGE\nIn this experiment we dive into our proposed method and conduct an\nablation study on the impact of each element. Namely, we report the performance in the following 3 scenarios: 1) only\nrun Soft-KMEANS on the extracted features to obtain a baseline accuracy; 2) run the VB model with onk initialized\nby Soft-KMEANS, without reducing the feature space; and 3) integrate PLDA into VB iterations. From Table 2 we\nobserve only a slight increase of accuracy compared with baseline when no dimensionality reduction is applied. This\nis due to the fact that high feature dimensions increase uncertainty in the estimations, making the model sensitive to\nparameters. With our implementation of PLDA iteratively applied in the VB model, we can see from the table that the\n6\n",
    "Adaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\nTable 1: Comparisons of the state-of-the-art methods on mini-Imagenet, tiered-Imagenet and CUB datasets using\nthe same pretrained backbones as [39], along with the accuracy of our proposed method on a ResNet-12 backbone\npretrained following [3].\nmini-Imagenet\nunbalanced\nbalanced\nMethod\nBackbone\n1-shot\n5-shot\n1-shot\n5-shot\nMAML [12]\nRN18/WRN [39]\n47.6/−\n64.5/−\n51.4/−\n69.5/−\nVersa [15]\n47.8/−\n61.9/−\n50.0/−\n65.6/−\nEntropy-min [11]\n58.5/60.4\n74.8/76.2\n63.6/66.1\n82.1/84.2\nPT-MAP [16]\n60.1/60.6\n67.1/66.8\n76.9/78.9\n85.3/86.6\nLaplacianShot [48]\n65.4/70.0\n81.6/83.2\n70.1/72.9\n82.1/83.8\nBD-CSPN [26]\n67.0/70.4\n80.2/82.3\n69.4/72.5\n82.0/83.7\nTIM [5]\n67.3/69.8\n79.8/81.6\n71.8/74.6\n83.9/85.9\nα-TIM [39]\n67.4/69.8\n82.5/84.8\n−/−\n−/−\nBAVARDAGE (ours)\n71.0/74.1\n83.6/85.5\n75.1/78.5\n84.5/87.4\nBAVARDAGE (ours)\nRN12 [3]\n77.8\n88.0\n82.7\n89.5\ntiered-Imagenet\nunbalanced\nbalanced\nMethod\nBackbone\n1-shot\n5-shot\n1-shot\n5-shot\nEntropy-min [11]\nRN18/WRN [39]\n61.2/62.9\n75.5/77.3\n67.0/68.9\n83.1/84.8\nPT-MAP [16]\n64.1/65.1\n70.0/71.0\n82.9/84.6\n88.8/90.0\nLaplacianShot [48]\n72.3/73.5\n85.7/86.8\n77.1/78.8\n86.2/87.3\nBD-CSPN [26]\n74.1/75.4\n84.8/85.9\n76.3/77.7\n86.2/87.4\nTIM [5]\n74.1/75.8\n84.1/85.4\n78.6/80.3\n87.7/88.9\nα-TIM [39]\n74.4/76.0\n86.6/87.8\n−/−\n−/−\nBAVARDAGE (ours)\n76.6/77.5\n86.5/87.5\n80.3/81.5\n87.1/88.3\nBAVARDAGE (ours)\nRN12 [3]\n79.4\n88.0\n83.5\n89.0\nCUB\nunbalanced\nbalanced\nMethod\nBackbone\n1-shot\n5-shot\n1-shot\n5-shot\nPT-MAP [16]\nRN18 [39]\n65.1\n71.3\n85.5\n91.3\nEntropy-min [11]\n67.5\n82.9\n72.8\n88.9\nLaplacianShot [48]\n73.7\n87.7\n78.9\n88.8\nBD-CSPN [26]\n74.5\n87.1\n77.9\n88.9\nTIM [5]\n74.8\n86.9\n80.3\n90.5\nα-TIM [39]\n75.7\n89.8\n−\n−\nBAVARDAGE (ours)\n82.0\n90.7\n85.6\n91.4\nBAVARDAGE (ours)\nRN12 [3]\n83.1\n90.8\n87.4\n92.0\nperformance increases by a relatively large margin, suggesting the effectiveness of our proposed adaptive dimension\nreduction method.\nVisualization of features for different projections\nTo further showcase the effect of proposed PLDA, in Fig. 2 we\nvisualize the extracted features of a 3-way Few-Shot task in the following 3 scenarios: (a) features in the original space,\nusing T-SNE [38] for visualization purpose; (b) features that are projected directly onto their centroids space, and ﬁnally\n(c) features projected using PLDA. The ellipses drawn in (b) and (c) are the cluster estimations computed using the real\nlabels of data samples, and we can thus observe a larger separation of different clusters with PLDA projection for the\ntask in which the original features overlap heavily between clusters in blue and green.\n7\n",
    "Adaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\nTable 2: Ablation study on the elements of our proposed method, with results tested on mini-Imagenet (backbone:\nWRN) and CUB (backbone: RN18) in the unbalanced setting.\nmini-Imagenet\nCUB\nSoft-KMEANS\nVB\nPLDA\n1-shot\n5-shot\n1-shot\n5-shot\n✓\n71.4\n82.4\n77.5\n86.7\n✓\n✓\n71.8\n82.5\n77.8\n87.2\n✓\n✓\n✓\n74.1\n85.5\n82.0\n90.7\n(a) T-SNE\n(b) Centroids projection\n(c) PLDA\nFigure 2: Visualization of extracted features of a Few-Shot task using different projection methods (dataset: mini-\nImagenet, backbone: WRN), we report a 86.7%, 90.0% and 95.0% prediction accuracy corresponding to each\nprojection.\nRobustness against imbalance\nIn Table 1 we show the accuracy of our proposed method using VB priors introduced\nin Section 3.3, in which αo is set to be equal to the Dirichlet’s parameter α∗\no for the level of imbalance in the query\nset. Therefore, in this experiment we test the robustness of BAVARDAGE, namely in Fig. 3 we alter αo and report the\naccuracy on different imbalance levels (varying α∗\no) in both 1-shot and 5-shot settings. Note that the proposed model\nbecomes slightly more sensitive to αo when the level of imbalance increases (smaller α∗\no), with an approximate 1%\ndrop of accuracy when increasing αo in the case of α∗\no = 1.\n2\n4\n6\n8\n10\n73\n74\n75\nαo\nAccuracy\n1-shot\n2\n4\n6\n8\n10\n85\n85.5\n86\nαo\n5-shot\nα∗\no = 1\nα∗\no = 2\nα∗\no = 4\nFigure 3: 1-shot and 5-shot accuracy on different imbalance levels (varying α∗\no) as a function of VB priors αo (dataset:\nmini-Imagenet, backbone: WRN).\nVarying Few-Shot settings\nIn this experiment we observe the performance of BAVARDAGE on different Few-Shot\nsettings, namely we vary the number of labelled samples per class L as well as the total number of unlabelled samples\nQ in a task, for further comparison we also report the accuracy using only Soft-KMEANS algorithm. In Fig. 4 we can\nobserve constant higher accuracy of our proposed method, and a slightly larger difference gap when Q increases.\n8\n",
    "Adaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\n0\n5\n10\n15\n20\n70\n75\n80\n85\n90\nL (# of labelled data per class)\nAccuracy\n20\n40\n60\n80\n100\n120\n140\n160\n68\n70\n72\n74\n76\nQ (# of total unlabelled data)\nBAVARDAGE\nSoft-KMEANS\nFigure 4: Accuracy as a funtion of L and Q in comparison with Soft-KMEANS (dataset: mini-Imagenet, backbone:\nWRN).\n5\nConclusion\nIn this paper we proposed a clustering method based on Variational Bayesian Inference and Probabilistic Linear\nDiscriminant Analysis for transductive Few-Shot Classiﬁcation. BAVARDAGE has reached state-of-the-art accuracy\non nearly all Few-Shot benchmarks in the realistic unbalanced setting, as well as competitive performance in the\nbalanced setting without using a perfectly class-balanced prior. As our proposed method assumes a shared isotropic\ncovariance matrix for all clusters, the estimations in VB models could be limited. Therefore the future work could study\na better estimation of covariance matrices associated with each cluster. An interesting asset of the proposed method\nis that it performs most of its processing in a reduced (K −1)-dimensional space, where K is the number of classes,\nsuggesting interests for visualization and suitability for more elaborate statistical machine learning methods. As in [39],\nwe encourage the community to rethink the works in transductive settings to provide fairer grounds of comparison\nbetween the various proposed approaches.\n6\nAppendix\n6.1\nImplementation details on the proposed PLDA\nIn this section we present more details on our implementation of PLDA proposed in section 3.2 in the paper. Given\nX ∈RN×D, we estimate its within-class covariance matrix to be Sbase\nw\ncalculated from Dbase. Denote Ibase\nc\nas the set\nof samples belonging to base class c where c ∈1, ..., |Cbase|, therefore Φw is approximated as follows:\nΦw ≈Sbase\nw\n=\nP\nc\nP\ni∈Ibase\nc\n(xbase\ni\n−mbase\nc\n)(xbase\ni\n−mbase\nc\n)T\nNbase\n,\n(6)\nwhere mbase\nc\n=\n1\n|Ibase\nc\n|\nP\ni∈Ibase\nc\nxbase\ni\nis the mean of c-th base class. Let λ = [λ1, ..., λi, ..., λD] ∈RD be the\neigenvalues of Sbase\nw\nin descending order, and we set R = [r1, ..., ri, ..., rD] ∈RD×D to be the corresponding\neigenvectors. In this paper we deﬁne a transformation matrix T = SR where S is a diagonal matrix with diagonal\nvalues being the square root of multiplicative inverse of λ, clamped to an upper bound smax. Namely, s = diag(S)\nwhere s = [s1, ...si, ...sD] ∈RD is a D-length vector containing the scaling value for each dimension, and we set si to\nbe as follows:\nsi =\n\u001a\nλ−0.5\ni\nif\nλ−0.5\ni\n≤smax\nsmax\notherwise\n.\n(7)\nWe can see from Eq. 7 that T is composed of a rotation matrix and scaling values on feature dimensions that help morph\nthe within-class distribution into an identity covariance matrix. This corresponds to a data sphering/whitening process in\nwhich we decorrelate samples in each of the dimensions. In our implementation we transform X by multiplying it with\nT. Therefore the sphered data samples, denoted as X′ = [x′\n1, ...x′\nn, ...x′\nN]T ∈RN×D, are obtained from x′\nn = Txn.\nNext, we project X′ onto a subspace that corresponds to the K −1 largest eigenvalues of its between-scatter matrix.\nDenote m′\nk as the estimated centroid for class k, given soft class assignments onk (1 ≤n ≤N, 1 ≤k ≤K), m′\nk is\ncomputed as:\nm′\nk =\nPN\nn=1 onkx′\nn\nγ + Nk\n, Nk =\nN\nX\nn=1\nonk ,\n(8)\n9\n",
    "Adaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\nAlgorithm 2 Proposed PLDA\nFonction PLDA (X, Sbase\nw\n, smax, onk)\nSphere X using T (Eq. 7), obtain X′.\nEstimate centroids m′\nk using onk (Eq. 8).\nCompute Ψ using m′\nk (Eq. 9).\nProject X′ onto the centroids space, obtain U.\nReturn U\nwhere γ is used as an offset indicating how close the centroids are to 0, in this paper we set it to 10.0, same as βo in the\nVB model in reduced space. Therefore, the between-class scatter matrix Ψ of sphered samples can be calculated as:\nΨ =\nK\nX\nk=1\n(m′\nk −m′)(m′\nk −m′)T ,\n(9)\nwhere m′ =\n1\nK\nPK\nk=1 m′\nk is the mean of estimated class centroids. Then we project X′ onto a d-length subspace,\nwhere d = K −1. In details, denote V = [v1, ..., vi, ..., vd] ∈RD×d to be the eigenvectors corresponding to the d\nlargest eigenvalues of Ψ, the projected data U are obtained as un = VT x′\nn for each sample. Note that the formulation\nof Ψ in Eq. 9 allows at most K −1 non-zero eigenvalues, therefore the resulting subspace projection using these\neigenvectors is equivalent to a projection onto the afﬁne subspace containing the centroids m′\nk. Furthermore, according\nto Eq. 1 in the paper, we can further deduce the projection matrix W to be as follows:\nun = WT xn = VT x′\nn = VT Txn = VT SRxn,\n=⇒W = (VT SR)T = RT SV.\n(10)\nThe entire process is described in Algorithm 2.\n6.2\nImplementation details on the proposed VB model\nIn this section we provide more detailed explanation of our proposed VB model. Given a posterior p(θ|U), we\napproximate it with a function variational distribution q(θ) by minimizing the Kullback-Leibler divergence:\nq∗(θ) = arg min\nq {DKL(q||p)}\n= arg min\nq {log p(U) −L(q)}\n= arg max\nq {L(q)}\n(11)\nwhere the evidence log p(U) is considered ﬁxed, and L(q) =\nR\nq(θ) log p(θ,U)\nq(θ) dθ stands for Evidence Lower BOund\n(ELBO) providing “evidence” that we have chosen the right model. We can see that minimizing the Kullback-\nLeibler divergence is equivalent to maximizing the ELBO. Suppose θ = {θ1, ..., θm, ..., θM}, we ﬁrstly factorize\nq(θ) = QM\nm=1 q(θm) according to the Mean-Field assumption, then we solve each term individually:\nL(q) =\nZ\nq(θ) log p(θ, U)\nq(θ) dθ\n=\nZ  M\nY\nm=1\nq(θm)\n!  \nlog p(θ, U) −\nM\nX\nm=1\nlog q(θm)\n!\ndθ1dθ2...dθM\n=\nM\nX\nm=1\n\u0012Z\nq(θm)\n\u0012Z\nq(θ−m) log p(θ, U)dθ−m\n\u0013\ndθm −\nZ\nq(θm) log q(θm)dθm\n\u0013\n,\n(12)\nand the ELBO is maximized when:\nlog q∗(θm) = Eθ−m[log p(θ, U)] + const,\n(13)\n10\n",
    "Adaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\nwhere Eθ−m[·] stands for the expectation with respect to all variables in θ except θm. In our method we deﬁne\nθ = {Z, π, µ}, the detailed formula of some variables are presented as follows:\nzn = [zn1, ..., znk, ..., znK] ∈{0, 1}K,\nK\nX\nk=1\nznk = 1,\nπ = [π1, ..., πk, ..., πK],\nπk ≥0,\nK\nX\nk=1\nπk = 1.\n(14)\nAccording to Bayes’ theorem, we rewrite the posterior to be:\np(θ|U) = p(Z, π, µ|U) = p(Z, π, µ, U)\np(U)\n= p(U|Z, µ)p(Z|π)p(π)p(µ)\np(U)\n,\n(15)\nin which:\np(U|Z, µ) =\nN\nY\nn=1\nK\nY\nk=1\nN(un|µk, Λ−1)znk,\np(Z|π) =\nN\nY\nn=1\nCategorical(zn|π) =\nN\nY\nn=1\nK\nY\nk=1\nπznk\nk\n,\np(π) = Dir(π|αo) = Γ(PK\nk=1 Kαo)\nQK\nk=1 Γ(αo)\nK\nY\nk=1\nπαo−1\nk\n= C(αo)\nK\nY\nk=1\nπαo−1\nk\n,\np(µ) =\nK\nY\nk=1\nN(µk|mo, (βoΛ)−1).\n(16)\nAccording to Eq. 13, q∗(π) can be computed as follows:\nlog q∗(π) = EZ,µ[log p(Z, π, µ, U)] + const\n= EZ[log p(Z|π)] + log p(π) + const\n=\nN\nX\nn=1\nK\nX\nk=1\nEZ[znk] log πk +\nK\nX\nk=1\n(αo −1) log πk + const\n=\nK\nX\nk=1\nN\nX\nn=1\nonk log πk +\nK\nX\nk=1\n(αo −1) log πk + const\n=\nK\nX\nk=1\n(Nk + αo −1) log πk + const,\n=⇒q∗(π) =\nK\nY\nk=1\nπNk+αo−1\nk\n+ const\n=\nK\nY\nk=1\nπαk−1\nk\n+ const\n= Dir(π|α).\n(17)\n11\n",
    "Adaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\nSimilarly for q∗(µ) we can compute it as shown below:\nlog q∗(µ) = EZ,π[log p(Z, π, µ, U)] + const\n= EZ[log p(U|Z, µ)] + log p(µ) + const\n=\nN\nX\nn=1\nK\nX\nk=1\nEZ[znk] log N(un|µk, Λ−1) +\nK\nX\nk=1\nlog N(µk|mo, (βoΛ−1) + const\n= 1\n2\nN\nX\nn=1\nK\nX\nk=1\nonk log |Λ| −1\n2\nN\nX\nn=1\nK\nX\nk=1\nonk(un −µk)T Λ(un −µk)\n+ 1\n2\nK\nX\nk=1\nlog |βoΛ| −1\n2\nK\nX\nk=1\n(µk −mo)T βoΛ(µk −mo).\n(18)\nTo compute βk, we gather the quadratic terms that contain µk in Eq. 18:\n(quad) = −1\n2\nN\nX\nn=1\nK\nX\nk=1\nonkµT\nk Λµk −1\n2\nK\nX\nk=1\nµT\nk βoΛµk\n= −1\n2\nK\nX\nk=1\nµT\nk (NkΛ + βoΛ)µk\n= −1\n2\nK\nX\nk=1\nµT\nk (βo + Nk)Λkµk,\n=⇒βk = βo + Nk.\n(19)\nAs for mk, we gather the linear terms that contain µk in Eq. 18:\n(linear) = 1\n2\nN\nX\nn=1\nK\nX\nk=1\nonkµT\nk Λun + 1\n2\nK\nX\nk=1\nµT\nk βoΛmo\n= 1\n2\nK\nX\nk=1\nµT\nk Λ(βomo +\nN\nX\nn=1\nonkun)\n= 1\n2\nK\nX\nk=1\nµT\nk βkΛmk,\n=⇒mk = 1\nβk\n(βomo +\nN\nX\nn=1\nonkun).\n(20)\nTherefore q∗(µ) can be reformulated as:\nq∗(µ) =\nK\nY\nk=1\nq∗(µk) =\nK\nY\nk=1\nN(µk|mk, (βkΛ)−1).\n(21)\nWe also provide a more detailed calculation of q∗(Z):\nlog q∗(Z) = Eπ,µ[log p(Z, π, µ, U)] + const\n= Eπ[log p(Z|π)] + Eµ[log p(U|Z, µ)] + const\n=\nN\nX\nn=1\nK\nX\nk=1\nznk\n\u0000Eπ[log πk] + Eµ[log N(un|µk, Λ−1)]\n\u0001\n+ const\n=\nN\nX\nn=1\nK\nX\nk=1\nznk log ρnk + const,\n(22)\n12\n",
    "Adaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\nwhere\nlog ρnk = Eπ[log πk] + Eµ[log N(un|µk, Λ−1)]\n= Eπ[log πk] + 1\n2 log |Λ| −d\n2 log 2π −1\n2Eµ[(un −µk)T Λ(un −µk)].\n(23)\nTherefore q∗(Z) can be expressed as:\nq∗(Z) =\nN\nY\nn=1\nK\nY\nk=1\noznk\nnk =\nN\nY\nn=1\nCategorical(zn|on),\nonk =\nρnk\nPK\nj=1 ρnj\n,\n(24)\nwe can see that the variable follows a categorical distribution, parameterized by onk, and onk = EZ[znk]. As for Eq. 23,\nmore details are shown as follows:\nEπ[log πk] = ψ(αk) −ψ(\nK\nX\nj=1\nαj),\nEµ[(un −µk)T Λ(un −µk)] =\nZ\n(un −µk)T Λ(un −µk)q∗(µk)dµk\n= (un −mk)T Λk(un −mk) + Tr[Λ · (βkΛ)−1]\n= dβ−1\nk\n+ (un −mk)T Λ(un −mk),\n(25)\nψ(·) is the logarithmic derivative of the gamma function, and the distribution for πk and µk follows Eq. 17 and 21.\nTherefore:\nlog ρnk = ψ(αk) −ψ(\nK\nX\nj=1\nαj) + 1\n2 log |Λ| −d\n2 log 2π −1\n2[dβ−1\nk\n+ (un −mk)T Λ(un −mk)].\n(26)\nFrom the above equations we observe a dependency between priors and posteriors, which can be estimated iteratively\ndepending on the class allocations. Therefore in this paper we propose to solve it under a basic Expectation Maximization\nframework where we estimate onk in the E-step, while updating αk, βk and mk in the M-step.\n6.3\nHyperparameter tuning\nIn this section we detail about how the hyperparameters in our proposed method are obtained. Namely, for a standard\nFew-Shot benchmark that has been split into base-validation-novel class set, we ﬁrstly tune our model using validation\nset and choose the hyperparameters accordingly before applying to the novel set. For example in Figure 5 we tune\ntwo temperature parameters Tkm, Tvb, the scaling up-bound parameter smax and the VB prior βo that are used in\nour proposed BAVARDAGE. The blue curves show the performance on validation set while the red curves show the\naccuracy on the novel set (benchmark: mini-Imagenet). From the ﬁgure we see a similar behavior between two sets in\nterms of performance, Tkm has little impact on the accuracy, same for Tvb when it is large. For smax we observe an\nuptick when it is around 1, followed by a slowing decrease and ﬁnally stabilizing to the same accuracy when it becomes\nlarger. In this paper we tune hyperparameters for each benchmark in the same way. For tiered-Imagenet we set Tkm,\nTvb and smax to be 10, 100, 2 in the balanced setting, 100, 100, 1 in the unbalanced setting; for CUB we set them to be\n10, 4, 5 in both balanced and unbalanced settings; and for FC100 and CIFAR-FS we set the hyperparameters to be the\nsame as mini-Imagenet. As for βo we set it to be 10 across datasets since it gives the best performance.\n6.4\nAdditional experiments on other Few-Shot benchmarks\nIn Section 4 in the paper we tested our proposed method on three standard Few-Shot benchmarks: mini-Imagenet1, tiered-\nImagenet2 and CUB3, following the same setting as presented in https://github.com/oveilleux/Realistic_\nTransductive_Few_Shot. In this section we further conduct experiments on two other well-known Few-Shot datasets:\n1) FC100 (https://github.com/ElementAI/TADAM) is a recent split dataset based on CIFAR-100 [22] that contains\n60 base classes for training, 20 classes for validation and 20 novel classes for evaluation, each class is composed of\n1https://github.com/yaoyao-liu/mini-imagenet-tools\n2https://github.com/yaoyao-liu/tiered-imagenet-tools\n3http://www.vision.caltech.edu/datasets/cub_200_2011\n13\n",
    "Adaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\n0\n20\n40\n60\n80\n100\n74\n76\n78\nTkm\nAccuracy\n1-shot\n0\n20\n40\n60\n80\n100\n86\n87\n88\nTkm\n5-shot\nval\ntest\n40\n60\n80\n100\n72\n74\n76\n78\nTvb\nAccuracy\n40\n60\n80\n100\n82\n84\n86\n88\nTvb\nval\ntest\n1\n2\n3\n4\n5\n74\n76\n78\nsmax\nAccuracy\n1\n2\n3\n4\n5\n85\n86\n87\n88\nsmax\nval\ntest\n0\n5\n10\n15\n20\n72\n74\n76\n78\nβo\nAccuracy\n0\n5\n10\n15\n20\n84\n86\n88\nβo\nval\ntest\nFigure 5: Hyperparameter tuning of our proposed method. Here we tune 4 hyperparameters of BAVARDAGE on\nmini-Imagenet (backbone: WRN) in the unbalanced setting.\n600 images of size 32x32 pixels; 2) CIFAR-FS (https://github.com/bertinetto/r2d2) is also sampled from\nCIFAR-100 and shares the same quantity of classes in the base-validation-novel splits as for mini-Imagenet. Each\nclass contains 600 images of size 32x32 pixels. In Table 3 below we report the accuracy of our proposed method on\nall benchmarks, note that for FC100 and CIFAR-FS we believe to be among the ﬁrst to conduct experiments in the\nunbalanced setting.\nIn Table 3 we also show the results using WRN and RN18 pretrained from [39] and RN12 pretrained from [3],\nsame as Table 1 in the paper, with a conﬁdence interval of 95% added next to the accuracy. In addition, given that\nsome works [27, 47] in the ﬁeld utilize data augmentation techniques to extract features based on images in original\ndimensions instead of reduced ones, here we apply our BAVARDAGE following the same setting and report the accuracy\non a pretrained RN12 feature extractor [3] with data augmentation (denote RN12*). For comparison purpose we also\nprovide a baseline accuracy on each Few-Shot benchmark using Soft-KMEANS algorithm.\nWith BAVARDAGE, we observe a clear increase of accuracy for all datasets compared with Soft-KMEANS in both\nbalanced and unbalanced settings, suggesting the genericity of the proposed method. As for the computational time, we\nevaluate an average of 1.72 seconds per accuracy (on 10,000 Few-Shot tasks) using a GeForce RTX 3090 GPU.\n14\n",
    "Adaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\nTable 3: Detailed results of BAVARDAGE with conﬁdence interval of 95% on the Few-Shot benchmarks, along with a\nbaseline accuracy using Soft-KMEANS. We use RN18 and WRN pretrained from [39], RN12 and RN12* pretrained\nfrom [3].\nmini-Imagenet\nunbalanced\nbalanced\nMethod\nBackbone\n1-shot\n5-shot\n1-shot\n5-shot\nSoft-KMEANS\nRN18 [39]\n68.82 ± 0.27%\n81.27 ± 0.17%\n73.47 ± 0.26%\n83.04 ± 0.15%\nWRN [39]\n71.35 ± 0.27%\n82.41 ± 0.16%\n75.70 ± 0.25%\n84.42 ± 0.14%\nRN12 [3]\n75.65 ± 0.25%\n86.35 ± 0.14%\n80.81 ± 0.24%\n87.92 ± 0.12%\nRN12* [3]\n77.51 ± 0.26%\n87.78 ± 0.14%\n82.14 ± 0.24%\n89.08 ± 0.12%\nBAVARDAGE\nRN18 [39]\n71.01 ± 0.31%\n83.60 ± 0.17%\n75.07 ± 0.28%\n84.49 ± 0.14%\nWRN [39]\n74.10 ± 0.30%\n85.52 ± 0.16%\n78.51 ± 0.27%\n87.41 ± 0.13%\nRN12 [3]\n77.85 ± 0.28%\n88.02 ± 0.14%\n82.67 ± 0.25%\n89.50 ± 0.11%\nRN12* [3]\n79.76 ± 0.29%\n89.85 ± 0.13%\n84.80 ± 0.25%\n91.65 ± 0.10%\ntiered-Imagenet\nunbalanced\nbalanced\nMethod\nBackbone\n1-shot\n5-shot\n1-shot\n5-shot\nSoft-KMEANS\nWRN [39]\n73.92 ± 0.28%\n85.02 ± 0.18%\n78.59 ± 0.27%\n85.76 ± 0.16%\nRN18 [39]\n73.79 ± 0.28%\n84.65 ± 0.18%\n78.34 ± 0.27%\n85.52 ± 0.17%\nRN12 [3]\n78.15 ± 0.27%\n87.65 ± 0.17%\n83.11 ± 0.25%\n88.80 ± 0.15%\nRN12* [3]\n79.62 ± 0.27%\n88.61 ± 0.16%\n84.08 ± 0.24%\n89.56 ± 0.14%\nBAVARDAGE\nWRN [39]\n77.45 ± 0.31%\n87.48 ± 0.18%\n81.47 ± 0.28%\n88.27 ± 0.16%\nRN18 [39]\n76.55 ± 0.31%\n86.46 ± 0.19%\n80.32 ± 0.28%\n87.14 ± 0.16%\nRN12 [3]\n79.38 ± 0.29%\n88.04 ± 0.18%\n83.52 ± 0.26%\n89.03 ± 0.15%\nRN12* [3]\n81.17 ± 0.29%\n89.63 ± 0.17%\n85.20 ± 0.25%\n90.41 ± 0.14%\nCUB\nunbalanced\nbalanced\nMethod\nBackbone\n1-shot\n5-shot\n1-shot\n5-shot\nSoft-KMEANS\nRN18 [39]\n77.54 ± 0.26%\n86.70 ± 0.14%\n82.67 ± 0.24%\n89.04 ± 0.11%\nRN12 [3]\n81.24 ± 0.25%\n87.27 ± 0.14%\n84.87 ± 0.22%\n89.64 ± 0.11%\nRN12* [3]\n82.40 ± 0.24%\n89.40 ± 0.13%\n87.38 ± 0.20%\n91.29 ± 0.10%\nBAVARDAGE\nRN18 [39]\n82.00 ± 0.28%\n90.67 ± 0.12%\n85.64 ± 0.25%\n91.42 ± 0.10%\nRN12 [3]\n83.12 ± 0.26%\n90.81 ± 0.12%\n87.41 ± 0.22%\n92.03 ± 0.09%\nRN12* [3]\n86.96 ± 0.24%\n92.84 ± 0.10%\n90.42 ± 0.20%\n93.50 ± 0.08%\nFC100\nunbalanced\nbalanced\nMethod\nBackbone\n1-shot\n5-shot\n1-shot\n5-shot\nSoft-KMEANS\nRN12 [3]\n51.24 ± 0.27%\n64.70 ± 0.22%\n54.59 ± 0.26%\n66.37 ± 0.20%\nRN12* [3]\n51.64 ± 0.27%\n65.26 ± 0.22%\n54.87 ± 0.26%\n66.89 ± 0.20%\nBAVARDAGE\nRN12 [3]\n52.60 ± 0.32%\n65.35 ± 0.25%\n56.66 ± 0.28%\n69.69 ± 0.21%\nRN12* [3]\n53.78 ± 0.30%\n68.75 ± 0.24%\n57.27 ± 0.29%\n70.60 ± 0.21%\nCIFAR-FS\nunbalanced\nbalanced\nMethod\nBackbone\n1-shot\n5-shot\n1-shot\n5-shot\nSoft-KMEANS\nRN12 [3]\n80.72 ± 0.25%\n88.31 ± 0.17%\n85.47 ± 0.22%\n89.36 ± 0.15%\nRN12* [3]\n81.75 ± 0.25%\n88.92 ± 0.17%\n86.07 ± 0.22%\n89.85 ± 0.15%\nBAVARDAGE\nRN12 [3]\n82.68 ± 0.27%\n88.97 ± 0.18%\n86.20 ± 0.23%\n89.58 ± 0.15%\nRN12* [3]\n83.82 ± 0.27%\n89.84 ± 0.18%\n87.35 ± 0.23%\n90.63 ± 0.16%\n15\n",
    "Adaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\nReferences\n[1] A. Antoniou, H. Edwards, and A. J. Storkey. How to train your MAML. In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n[2] S. Baik, J. Choi, H. Kim, D. Cho, J. Min, and K. M. Lee. Meta-learning with task-adaptive loss function for few-shot learning.\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9465–9474, 2021.\n[3] Y. Bendou, Y. Hu, R. Lafargue, G. Lioi, B. Pasdeloup, S. Pateux, and V. Gripon. Easy: Ensemble augmented-shot y-shaped\nlearning: State-of-the-art few-shot classiﬁcation with simple ingredients. arXiv preprint arXiv:2201.09699, 2022.\n[4] L. Bertinetto, J. F. Henriques, P. H. S. Torr, and A. Vedaldi. Meta-learning with differentiable closed-form solvers. In 7th\nInternational Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net,\n2019.\n[5] M. Boudiaf, I. Ziko, J. Rony, J. Dolz, P. Piantanida, and I. Ben Ayed. Information maximization for few-shot learning. In\nH. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems,\nvolume 33, pages 2445–2457. Curran Associates, Inc., 2020.\n[6] T. Cao, M. T. Law, and S. Fidler. A theoretical analysis of the number of shots in few-shot learning. In 8th International\nConference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\n[7] C. Chen, K. Li, W. Wei, J. T. Zhou, and Z. Zeng. Hierarchical graph neural networks for few-shot learning. IEEE Transactions\non Circuits and Systems for Video Technology, 32(1):240–252, 2021.\n[8] W. Chen, Y. Liu, Z. Kira, Y. F. Wang, and J. Huang. A closer look at few-shot classiﬁcation. In 7th International Conference\non Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n[9] D. Comaniciu and P. Meer. Mean shift: A robust approach toward feature space analysis. IEEE Transactions on pattern\nanalysis and machine intelligence, 24(5):603–619, 2002.\n[10] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the\nRoyal Statistical Society: Series B (Methodological), 39(1):1–22, 1977.\n[11] G. S. Dhillon, P. Chaudhari, A. Ravichandran, and S. Soatto. A baseline for few-shot image classiﬁcation. In 8th International\nConference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\n[12] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the\n34th International Conference on Machine Learning-Volume 70, pages 1126–1135. JMLR. org, 2017.\n[13] C. W. Fox and S. J. Roberts. A tutorial on variational bayesian inference. Artiﬁcial intelligence review, 38(2):85–95, 2012.\n[14] S. Gidaris and N. Komodakis. Generating classiﬁcation weights with gnn denoising autoencoders for few-shot learning. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 21–30, 2019.\n[15] J. Gordon, J. Bronskill, M. Bauer, S. Nowozin, and R. E. Turner. Meta-learning probabilistic inference for prediction. In 7th\nInternational Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net,\n2019.\n[16] Y. Hu, V. Gripon, and S. Pateux. Leveraging the feature distribution in transfer-based few-shot learning. In International\nConference on Artiﬁcial Neural Networks, pages 487–499. Springer, 2021.\n[17] S. Ioffe. Probabilistic linear discriminant analysis. In European Conference on Computer Vision, pages 531–542. Springer,\n2006.\n[18] T. S. Jaakkola and M. I. Jordan. Improving the mean ﬁeld approximation via the use of mixture distributions. In Learning in\ngraphical models, pages 163–173. Springer, 1998.\n[19] D. Kang, H. Kwon, J. Min, and M. Cho. Relational embedding for few-shot classiﬁcation. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 8822–8833, 2021.\n[20] M. Kearns, Y. Mansour, and A. Y. Ng. An information-theoretic analysis of hard and soft assignment methods for clustering.\nIn Learning in graphical models, pages 495–520. Springer, 1998.\n[21] J. Kim, T. Kim, S. Kim, and C. D. Yoo. Edge-labeling graph neural network for few-shot learning. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pages 11–20, 2019.\n[22] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.\n[23] M. Lazarou, T. Stathaki, and Y. Avrithis. Iterative label cleaning for transductive and semi-supervised few-shot learning. In\nProceedings of the IEEE/CVF International Conference on Computer Vision, pages 8751–8760, 2021.\n[24] E. Lee, C.-H. Huang, and C.-Y. Lee. Few-shot and continual learning with attentive independent mechanisms. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision, pages 9455–9464, 2021.\n[25] M. Lichtenstein, P. Sattigeri, R. Feris, R. Giryes, and L. Karlinsky. Tafssl: Task-adaptive feature sub-space learning for few-shot\nclassiﬁcation. In European Conference on Computer Vision, pages 522–539. Springer, 2020.\n[26] J. Liu, L. Song, and Y. Qin. Prototype rectiﬁcation for few-shot learning. In Computer Vision–ECCV 2020: 16th European\nConference, Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16, pages 741–756. Springer, 2020.\n16\n",
    "Adaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\n[27] X. Luo, L. Wei, L. Wen, J. Yang, L. Xie, Z. Xu, and Q. Tian. Rectifying the shortcut learning of background for few-shot\nlearning. Advances in Neural Information Processing Systems, 34, 2021.\n[28] P. Mangla, N. Kumari, A. Sinha, M. Singh, B. Krishnamurthy, and V. N. Balasubramanian. Charting the right manifold:\nManifold mixup for few-shot learning. In The IEEE Winter Conference on Applications of Computer Vision, pages 2218–2227,\n2020.\n[29] T. K. Moon. The expectation-maximization algorithm. IEEE Signal processing magazine, 13(6):47–60, 1996.\n[30] B. Oreshkin, P. Rodríguez López, and A. Lacoste. Tadam: Task dependent adaptive metric for improved few-shot learning.\nAdvances in neural information processing systems, 31, 2018.\n[31] O. V. Prezhdo. Mean ﬁeld approximation for the stochastic schrödinger equation. The Journal of chemical physics, 111(18):8366–\n8377, 1999.\n[32] M. Ren, E. Triantaﬁllou, S. Ravi, J. Snell, K. Swersky, J. B. Tenenbaum, H. Larochelle, and R. S. Zemel. Meta-learning for\nsemi-supervised few-shot classiﬁcation. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver,\nBC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.\n[33] M. N. Rizve, S. Khan, F. S. Khan, and M. Shah. Exploring complementary strengths of invariant and equivariant representations\nfor few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n10836–10846, 2021.\n[34] P. Rodríguez, I. Laradji, A. Drouin, and A. Lacoste. Embedding propagation: Smoother manifold for few-shot classiﬁcation. In\nEuropean Conference on Computer Vision, pages 121–138. Springer, 2020.\n[35] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet\nlarge scale visual recognition challenge. International journal of computer vision, 115(3):211–252, 2015.\n[36] J. Snell, K. Swersky, and R. Zemel. Prototypical networks for few-shot learning. In Advances in Neural Information Processing\nSystems, pages 4077–4087, 2017.\n[37] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and T. M. Hospedales. Learning to compare: Relation network for few-shot\nlearning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1199–1208, 2018.\n[38] L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.\n[39] O. Veilleux, M. Boudiaf, P. Piantanida, and I. Ben Ayed. Realistic evaluation of transductive few-shot learning. Advances in\nNeural Information Processing Systems, 34, 2021.\n[40] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al. Matching networks for one shot learning. In Advances in neural\ninformation processing systems, pages 3630–3638, 2016.\n[41] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.\n[42] Y. Wang, W. Chao, K. Q. Weinberger, and L. van der Maaten. Simpleshot: Revisiting nearest-neighbor classiﬁcation for\nfew-shot learning. CoRR, abs/1911.04623, 2019.\n[43] D. Wertheimer, L. Tang, and B. Hariharan. Few-shot classiﬁcation with feature map reconstruction networks. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8012–8021, 2021.\n[44] L. Yang, L. Li, Z. Zhang, X. Zhou, E. Zhou, and Y. Liu. Dpgn: Distribution propagation graph network for few-shot learning.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13390–13399, 2020.\n[45] S. Yang, L. Liu, and M. Xu. Free lunch for few-shot learning: Distribution calibration. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\n[46] H.-J. Ye, H. Hu, D.-C. Zhan, and F. Sha. Few-shot learning via embedding adaptation with set-to-set functions. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8808–8817, 2020.\n[47] C. Zhang, Y. Cai, G. Lin, and C. Shen. Deepemd: Few-shot image classiﬁcation with differentiable earth mover’s distance\nand structured classiﬁers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages\n12203–12213, 2020.\n[48] I. Ziko, J. Dolz, E. Granger, and I. B. Ayed. Laplacian regularized few-shot learning. In International Conference on Machine\nLearning, pages 11660–11670. PMLR, 2020.\n17\n"
  ],
  "full_text": "ADAPTIVE DIMENSION REDUCTION AND VARIATIONAL\nINFERENCE FOR TRANSDUCTIVE FEW-SHOT CLASSIFICATION\nA PREPRINT\nYuqing Hu\nOrange Labs, Cesson-Sévigné, France\nIMT-Atlantique, Brest, France\nyuqing.hu@imt-atlantique.fr\nStéphane Pateux\nOrange Labs, Cesson-Sévigné, France\nstephane.pateux@orange.com\nVincent Gripon\nIMT-Atlantique, Brest, France\nvincent.gripon@imt-atlantique.fr\nSeptember 20, 2022\nABSTRACT\nTransductive Few-Shot learning has gained increased attention nowadays considering the cost of data\nannotations along with the increased accuracy provided by unlabelled samples in the domain of few\nshot. Especially in Few-Shot Classiﬁcation (FSC), recent works explore the feature distributions\naiming at maximizing likelihoods or posteriors with respect to the unknown parameters. Following\nthis vein, and considering the parallel between FSC and clustering, we seek for better taking into\naccount the uncertainty in estimation due to lack of data, as well as better statistical properties of\nthe clusters associated with each class. Therefore in this paper we propose a new clustering method\nbased on Variational Bayesian inference, further improved by Adaptive Dimension Reduction based\non Probabilistic Linear Discriminant Analysis. Our proposed method signiﬁcantly improves accuracy\nin the realistic unbalanced transductive setting on various Few-Shot benchmarks when applied to\nfeatures used in previous studies, with a gain of up to 6% in accuracy. In addition, when applied\nto balanced setting, we obtain very competitive results without making use of the class-balance\nartefact which is disputable for practical use cases. We also provide the performance of our method\non a high performing pretrained backbone, with the reported results further surpassing the current\nstate-of-the-art accuracy, suggesting the genericity of the proposed method.\n1\nIntroduction\nFew-shot learning, and in particular Few-Shot Classiﬁcation, has become a subject of paramount importance in the last\nyears with a large number of methodologies and discussions. Where large datasets continuously beneﬁt from improved\nmachine learning architectures, the ability to transfer this performance to the low-data regime is still a challenge due\nto the high uncertainty posed using few labels. In more details, there are two main types of FSC tasks. In inductive\nFSC [1, 36, 46, 33], the situation comes to its extremes with only a few data samples available for each class, leading\nsometimes to completely intractable settings, such as when facing a black dog on the one hand and a white cat on\nthe other hand. In transductive FSC, additional unlabelled samples are available for prediction, leading to improved\nreliability and more elaborate solutions [24, 23, 2].\nInductive FSC is likely to occur when data acquisition is difﬁcult or expensive, or when categories of interest correspond\nto rare events. Transductive FSC is more likely encountered when data labeling is expensive, for fast prototyping of\nsolutions, or when the categories of interest are rare and hard to detect. Since the latter correspond to situations where it\nis possible to exploit, at least partially, the distribution of unlabelled samples, the trend evolved to using potentially\nvarying parts of this additional source of information. With most standardized benchmarks using very limited scope of\nvariability in the generated Few-Shot tasks, this even came to the point the best performing methods are often relying\narXiv:2209.08527v1  [cs.LG]  18 Sep 2022\n\n\nAdaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\n(a) Few-Shot task\n(b) Initialization\n(c) PLDA and VB inference\nFigure 1: Summary of the proposed method. Here we illustrate a 3-way classiﬁcation task in a standard 2-simplex\nusing soft-classiﬁcation probabilities. Trajectories show the evolution across iterations. For a given Few-Shot task\nwhich nearest-class-mean probabilities are depicted in (a), a Soft-KMEANS clustering method is performed in (b) to\ninitialize onk (see Alg. 1). Then in (c) an iteratively reﬁned Variational Bayesian (VB) model with Adaptive Dimension\nReduction using Probabilistic Linear Discriminant Analysis (PLDA) is applied to obtain the ﬁnal class predictions.\non questionable information, such as equidistribution between the various classes among the unlabelled samples, that is\nunlikely realistic in applications.\nThis limitation of benchmarking for transductive FSC has recently been discussed in [39]. In this paper, the authors\npropose a new way of generating transductive FSC benchmarks where the distribution of samples among classes can\ndrastically change from a Few-Shot generated task to the next one. Interestingly, they showed the impact of generating\nclass imbalance on the performance on various popular methods, resulting in some cases in drops in average accuracy\nof more than 10%.\nA simple way to reach state-of-the-art performance in transductive FSC consists in extracting features from the available\nsamples using a pretrained backbone deep learning architecture, and then using semi-supervised clustering routines to\nestimate samples distribution among classes. Due to the very limited number of available samples, distribution-agnostic\nclustering algorithms are often preferred, such as K-MEANS or its variants [29, 25, 32] or mean-shift [9] for instance.\nIn this paper, we are interested in showing it is possible to combine data reduction with statistical inference through a\nVariational Bayesian (VB) [13] approach. Here, data reduction helps considerably reduce the number of parameters\nto infer, while VB provides more ﬂexibility than the usual K-Means methods. Interestingly, the proposed approach\ncan easily cope with standard equidistributed Few-Shot tasks or the unbalanced ones proposed in [39], deﬁning a new\nstate-of-the-art for ﬁve popular transductive Few-Shot vision classiﬁcation benchmarks.\nOur claims are the following:\n• We introduce a novel semi-supervised clustering algorithm based on VB inference and Probabilistic Linear\nDiscriminant Analysis (PLDA),\n• We demonstrate the general utility of our proposed method being able to improve accuracy in a variety of deep\nlearning models and settings,\n• We show the ability of the proposed method to reach state-of-the-art transductive FSC performance on multiple\nvision benchmarks (balanced and unbalanced).\n2\nRelated work\nThere are two main frameworks in the ﬁeld of FSC: 1) only one unlabelled sample is processed at a time for class\npredictions, which is called inductive FSC, and 2) the entire unlabelled samples are available for further estimations,\nwhich is called transductive FSC. Inductive methods focus on training a feature extractor that generalizes well the\nembedding in a feature sub-space, they include meta learning methods such as [12, 26, 2, 40, 30, 37] that train a model\nin an episodic manner, and transfer learning methods [8, 28, 48, 5, 3, 33] that train a model with a set of mini-batches.\nRecent state-of-the-art works on inductive FSC [46, 47, 43, 19] combine the above two strategies and propose a transfer\nbased training used as model initialization, followed by an episodic training that adapts the model to better ﬁt the\nFew-Shot tasks.\nTransductive methods are becoming more and more popular thanks to their better performance due to the use of\nunlabelled data, as well as their utility in situations where data annotation is costly. Early literature of this branch\n2\n\n\nAdaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\noperates on a class-balanced setting where unlabelled instances are evenly distributed among targeted classes. Graph-\nbased methods [14, 7, 44, 21] make use of the afﬁnity among features and propose to group those that belong to the\nsame class. More recent works such as [16] propose methods based on Optimal Transport that realizes sample-class\nallocation with a minimum cost. While effective, these methods often require class-balanced priors to work well, which\nis not realistic due to the arbitrary unknown query set. In [39] the authors put forward a novel unbalanced setting that\ncomposes a query set with unlabelled instances sampled following a Dirichlet distribution, injecting more imbalance for\npredictions.\nIn this paper we propose a clustering method to solve transductive FSC, where the aim is to estimate cluster parameters\ngiving high predictions for unlabelled samples. Under Gaussian assumptions, previous works [25, 32] have utilised\nalgorithms such as Expectation Maximization [10] (EM), with the goal of maximizing likelihoods or posteriors with\nrespect to the parameters for a cluster, with the hidden variables marginalized. However, this may not be the most\nsuitable way due to the scarcity of available data in a given Few-Shot task, which increases the level of uncertainty for\ncluster estimations. Therefore, in this paper we propose a Variational Bayesian (VB) approach, in which we regard some\nunknown parameters as hidden variables in order to inject more ﬂexibility into the model, and we try to approximate\nthe posterior of the hidden variables by a variational distribution.\nAs models with too few labelled samples often give too much randomness for a cluster to be stably reckoned, they\noften require the use of feature dimension reduction techniques to stabilize cluster estimations. Previous literature\nsuch as [25] applies a PCA method that reduces dimension in a non-supervised manner, and [6] proposes a modiﬁed\nLDA during backbone training that maximizes the ratio of inter/intra-class distance. In this paper we propose to use\nProbabilistic Linear Discriminant Analysis [17] (PLDA) that 1) is applied on extracted features, 2) ﬁts data more\ndesirably into distribution assumptions, and 3) is semi-supervised in combination of a VB model. We integrate PLDA\ninto the VB model in order to reﬁne the reduced space through iterations.\n3\nMethodology\nIn this section, we ﬁrstly present the standard setting in transductive FSC, including the latest unbalanced setting\nproposed by [39] where unlabelled samples are non-uniformly distributed among classes. Then we present our proposed\nmethod combining PLDA and VB inference.\n3.1\nProblem formulation\nFollowing other works in the domain, our proposed method is operated on a feature space obtained from a pre-trained\nbackbone. Namely, we are given the extracted features of 1) a generic base class dataset Dbase = {xbase\ni\n}Nbase\ni=1\n∈Cbase\nthat contains Nbase labelled samples where each sample xbase\ni\nis a column vector of length D, and Cbase is the set of\nbase classes to which these samples belong. These base classes have been used to train the backbone. And similarly,\n2) a novel class dataset Dnovel = {xnovel\nn\n}N\nn=1 containing N samples belonging to a set of K novel classes Cnovel\n(Cbase ∩Cnovel = ∅). On this novel dataset, only a few elements are labelled, and the aim is to predict the missing\nlabels. Denote X the matrix obtained by aggregating elements in Dnovel row-wise.\nWhen benchmarking transductive FSC methods, it is common to randomly generate Few-Shot tasks by sampling Dnovel\nfrom a larger dataset. These tasks are generated by sampling K distinct classes, L distinct labelled elements for each\nclass (called support set) and Q total unlabelled elements without repetition and distinct from the labelled ones (called\nquery set). All these unlabelled elements belong to one of the selected classes. We obtain a total of N = KL + Q\nelements in the task, and compute the accuracy on the Q unlabelled ones. Depending on how unlabelled instances are\ndistributed among selected classes within a task, we further distinguish a balanced setting where the query set is evenly\ndistributed among the K classes, from an unbalanced setting where it can vary from class to class. An automatic way\nto generate such unbalanced Few-Shot tasks has been proposed in [39] where the number of elements to draw from\neach class is determined using a Dirichlet distribution parameterized by α∗\no1, where 1 is the all-one vector. To solve a\ntransductive FSC task, our method is composed of PLDA and VB inference, that we introduce in the next paragraphs.\n3.2\nProbabilistic Linear Discriminant Analysis (PLDA)\nIn our work, PLDA [17] is mainly used to reduce feature dimensions. For a Few-Shot task X, let Φw be a positive\ndeﬁnite matrix representing the estimated shared within-class covariance of a given class, and Φb be a positive semi-\ndeﬁnite matrix representing the estimated between-class covariance that generates class variables. The goal of PLDA is\nto project data onto a subspace while maximizing the signal-to-noise ratio for class labelling. In details, we obtain a\n3\n\n\nAdaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\nprojection matrix W that diagonalizes both Φw and Φb and yield the following equations:\nWT ΦwW = I,\nWT ΦbW = Ψ\n(1)\nwhere I is an identity matrix and Ψ is a diagonal matrix. In this paper, we assume a similar distribution between\nthe pre-trained base classes and the transferred novel classes [45]. Therefore we propose to estimate Φw to be the\nwithin-class scatter matrix of Dbase, denoted as Sbass\nw\n. In practice we implement PLDA by ﬁrstly transforming X using\na rotation matrix R ∈RD×D and a set of scaling values s ∈RD obtained from Sbase\nw\n. Note that we clamp the scaling\nvalues to be no larger than an upper-bound smax in order to prevent too large values, smax is a hyper-parameter. Then\nwe project the transformed data onto their estimated class centroids space, in accordance with the d largest eigenvalues\nof Ψ, and obtain dimension-reduced data U = [u1, ..., un, ..., uN]T ∈RN×d where un = WT xn and d = K −1.\nMore detailed implementation can be found in Appendix.\n3.3\nVariational Bayesian (VB) Inference\nDuring VB inference, we operate on a reduced d-dimensional space obtained after applying PLDA. Considering a\nGaussian mixture model for a given task U ∈RN×d in reduced space, let θ be the unknown variables of the model. In\nVB we attempt to ﬁnd a probability distribution q(θ) that approximates the true posterior p(θ|U), i.e. maximizes the\nELBO (see Appendix for more details). In our case, we deﬁne θ = {Z, π, µ} where Z = {zn}N\nn=1 is a set of latent\nvariables used as class indicators, each latent variable zn has an one-of-K representation, π is a K-dimensional vector\nrepresenting mixing ratios between the classes, and µ = {µk}K\nk=1 where µk is the centroid for class k. Note that 1)\ncontrary to EM where π, µ are seen as parameters that can be estimated directly, in VB they are deemed as hidden\nvariables following certain distribution laws. 2) This is not a full VB model due to the lack of precision matrix (i.e.\nthe inverse of covariance matrix) as a variable in θ. Although a VB model frees up more parameters for the unknown\nvariables, it also increases the instability in estimations so that the model becomes too sensible. Therefore, in this paper\nwe impose an assumption that all classes in U share the same precision matrix and it is ﬁxed during VB iterations.\nNamely we deﬁne Λk = Λ = TvbI for k = 1, ..., K, where Tvb is a hyper-parameter in order to compensate the\nvariation between base and estimated novel class distributions.\nIn order for a model to be in a variational bayesian setting, we deﬁne priors and likelihoods on the unknown variables,\nwith several initialization parameters attached:\npriors :\np(π) = Dir(π|αo),\np(µ) =\nK\nY\nk=1\nN(µk|mo, (βoΛ)−1),\nlikelihoods :\np(Z|π) =\nN\nY\nn=1\nCategorical(zn|π),\np(U|Z, µ) =\nN\nY\nn=1\nK\nY\nk=1\nN(un|µk, Λ−1)znk\n(2)\nwhere π follows a K-dimensional symmetric Dirichlet distribution, with αo being the prior of component weight for\neach class, which we set to 2.0 in accordance with [39], i.e. the same value as the Dirichlet distribution parameter α∗\no\nthat is used to generate Few-Shot tasks. The vector mo is the prior about the class centroid variables, we let it to be 0.\nAnd βo stands for the prior about the moving range of class centroid variables: the larger it is, the closer the centroids\nare to mo. We empirically found that βo = 10.0 gives consistent good results across datasets and FSC problems.\nAs previously stated, we approximate a variable distribution to the true posterior. To further simplify, we follow the\nMean-Field assumption [31, 18] and assume that the unknown variables are independent from one another. Therefore\nwe let q(θ) = q(Z, π, µ) = q(Z)q(π)q(µ) ≈p(Z, π, µ/U) and solve for each term. The explicit formulation for\nthese marginals is provided in Eq. 3, 4 (see Appendix for more details). The estimation of the various parameters is\nthen classically performed through an iterative EM framework as presented further.\nDenote on = [on1, ..., onk, ..., onK] as the soft class assignment for un (onk ≥0, PK\nk=1 onk = 1), and onk represents\nthe portion of nth sample allocated to kth class.\n4\n\n\nAdaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\nM step:\nIn this step we estimate q(π) and q(µ) in use of the class assignments onk:\np(π) = Dir(π|αo)\n=⇒\nq∗(π) = Dir(π|α)\nwith\nαk = αo + Nk,\np(µ) =\nK\nY\nk=1\nN(µk|mo, (βoΛ)−1)\n=⇒\nq∗(µ) =\nK\nY\nk=1\nN(µk|mk, (βkΛ)−1)\nwith\nβk = βo + Nk, mk = 1\nβk\n(βomo +\nN\nX\nn=1\nonkun),\n(3)\nwhere α = [α1, ..., αk, ..., αK] are the estimated component weights for classes, and Nk = PN\nn=1 onk is the sum of\nthe soft assignments for all samples in class k. We also estimate the moving range parameter βk and the centroid mk\nfor each class centroid variable. We observe that the posteriors take the same forms as the priors. Demonstration of\nthese results is presented in Appendix.\nE step:\nIn this step we estimate q(Z) by updating onk, using the current values of all other parameters computed in\nthe M-step, i.e. αk, βk and mk.\np(Z|π) =\nN\nY\nn=1\nCategorical(zn|π)\n=⇒\nq∗(Z) =\nN\nY\nn=1\nCategorical(zn|on)\n(4)\nwhere each element onk can be computed as onk =\nρnk\nPK\nj=1 ρnj in which:\nlog ρnk = ψ(αk) −ψ(\nK\nX\nj=1\nαj) + 1\n2 log |Λ| −d\n2 log 2π −1\n2[dβ−1\nk\n+ (un −mk)T Λ(un −mk)],\n(5)\nwith ψ(·) being the logarithmic derivative of the gamma function (also known as the digamma function). We observe\nthat q∗(Z) follows the same categorical distribution as the likelihood, and it is parameterized by onk. More details can\nbe found in Appendix.\nProposed algorithm\nThe proposed method combines PLDA and VB inference which leads to an Efﬁciency\nGuided Adaptive Dimension Reduction for VAriational BAyesian inference. We thus name our proposed method\n“BAVARDAGE”, and the detailed description is presented in Algorithm 1. Given a Few-Shot task X and a within-class\nscatter matrix Sbase\nw\n, we initialize onk using EM algorithm with an assumed covariance matrix, adjusted by a temperature\nhyper-parameter Tkm, for all classes. Note that this is equivalent to Soft-KMEANS [20] algorithm. And for each\niteration we update parameters: in M step we update αk, βk and centroids mk, in E step we only update onk, and we\napply PLDA with the updated onk to reduce feature dimensions. Finally, predicted labels are obtained by selecting the\nclass that corresponds to the largest value in onk.\nThe illustration of our proposed method is presented in Figure 1. For a Few-Shot task that has three classes (red, blue\nand green) with unlabelled samples depicted on the probability simplex, we ﬁrstly initialize onk with Soft-KMEANS\nwhich directs some data points to their belonging classes while further distancing some points from their targeted\nclasses. Then we apply the proposed VB inference integrated with PLDA, resulting in additional points moving towards\ntheir corresponding classes.\n4\nExperiments\nIn this section we provide details on the standard transductive Few-Shot classiﬁcation settings, and we evaluate the\nperformance of our proposed method.\nBenchmarks\nWe test our method on standard Few-Shot benchmarks: mini-Imagenet [35], tiered-Imagenet [32] and\ncaltech-ucsd birds-200-2011 (CUB) [41]. mini-Imagenet is a subset of ILSVRC-12 [35] dataset, it contains a total of\n60, 000 images of size 84 × 84 belonging to 100 classes (600 images per class) and follows a 64-16-20 split for base,\nvalidation and novel classes. tiered-Imagenet is a larger subset of ILSVRC-12 containing 608 classes with 779, 165\nimages of size 84×84 in total, and we use the standard 351-97-160 split, and CUB is composed of 200 classes following\na 100-50-50 split (Image size: 84 × 84). In Appendix we also show the performance of our proposed method on other\nwell-known Few-Shot benchmarks such as FC100 [30] and CIFAR-FS [4].\n5\n\n\nAdaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\nAlgorithm 1 BAVARDAGE\nInputs: X ∈RN×D, Sbase\nw\n∈RD×D\nHyper-parameters: Tkm, Tvb, smax\nPriors for VB: αo = 2.0, βo = 10.0, mo = 0, Λ = Tvb · I\nInitializations: onk = EM (X, Tkm)\nfor i = 1 to nstep do\nU = PLDA (X, Sbase\nw\n, smax, onk)\n# See more details in Appendix.\nVB (M step):\nαk = αo + PN\nn=1 onk\nβk = βo + PN\nn=1 onk\nmk =\n1\nβk (βomo + PN\nn=1 onkun)\nVB (E step):\nlog ρnk = ψ(αk) −ψ(PK\nj=1 αj) + 1\n2 log |Λ| −d\n2 log 2π −1\n2[dβ−1\nk\n+ (un −mk)T Λ(un −mk)]\nonk =\nρnk\nPK\nj=1 ρnj\nend for\nreturn ˆℓ(xn) = arg maxk(onk)\nSettings\nFollowing previous works [25, 34, 39], our proposed method is evaluated on 1-shot 5-way (K = 5, L = 1),\nand 5-shot 5-way (K = 5, L = 5) scenarios. As for the query set, we set a total number of Q = 75 unlabelled samples,\nfrom which we further deﬁne two settings: 1) a balanced setting where unlabelled instances are evenly distributed\namong K classes, and 2) an unbalanced setting where the query set is randomly distributed, following a Dirichlet\ndistribution parameterized by α∗\no. In our paper we follow the same setting as [39] and set α∗\no = 2.0, further experiments\nwith different values are conducted in the next sections. The performance of our proposed method is evaluated by\ncomputing the averaged accuracy over 10, 000 Few-Shot tasks.\nImplementation details\nIn this paper we ﬁrstly compare our proposed algorithm with the other state-of-the-art\nmethods using the same pretrained backbones and benchmarks provided in [39]. Namely we extract the features\nusing the same ResNet-18 (RN18) and WideResNet28_10 (WRN) neural models, and present the performance on\nmini-Imagenet, tiered-Imagenet and CUB datasets. In our proposed method, the raw features are preprocessed\nfollowing [42]. As for the hyper-parameters, we set Tkm = 10, Tvb = 50, smax = 2 for the balanced setting;\nTkm = 50, Tvb = 50, smax = 1 for the unbalanced setting, and we use the same VB priors for all settings. To further\nshow the functionality of our proposed method on different backbones and other benchmarks, we tested BAVARDAGE\non a recent high performing feature extractor trained on a ResNet-12 (RN12) neural model [28, 3], and we report the\naccuracy in Table 1 and in Appendix with various settings.\n4.1\nMain results\nThe main results on the relevant settings are presented in Table 1. Note that we report the accuracy of other methods\nfollowing [39], and add the performance of our proposed method in comparison, using the same pretrained RN18\nand WRN feature extractors, and we also report the result of a RN12 backbone pretrained following [3]. We observe\nthat our proposed algorithm reaches state-of-the-art performance for nearly all referenced datasets in the unbalanced\nsetting, surpassing previous methods by a noticeable margin especially on 1-shot. In the balanced setting we also\nreach competitive accuracy compared with [16] along with other works that make use of a perfectly balanced prior on\nunlabelled samples, while our proposed method suggests no such prior. In addition, we provide results on the other\nFew-Shot benchmarks with different settings in Appendix.\n4.2\nAblation studies\nAnalysis on the elements of BAVARDAGE\nIn this experiment we dive into our proposed method and conduct an\nablation study on the impact of each element. Namely, we report the performance in the following 3 scenarios: 1) only\nrun Soft-KMEANS on the extracted features to obtain a baseline accuracy; 2) run the VB model with onk initialized\nby Soft-KMEANS, without reducing the feature space; and 3) integrate PLDA into VB iterations. From Table 2 we\nobserve only a slight increase of accuracy compared with baseline when no dimensionality reduction is applied. This\nis due to the fact that high feature dimensions increase uncertainty in the estimations, making the model sensitive to\nparameters. With our implementation of PLDA iteratively applied in the VB model, we can see from the table that the\n6\n\n\nAdaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\nTable 1: Comparisons of the state-of-the-art methods on mini-Imagenet, tiered-Imagenet and CUB datasets using\nthe same pretrained backbones as [39], along with the accuracy of our proposed method on a ResNet-12 backbone\npretrained following [3].\nmini-Imagenet\nunbalanced\nbalanced\nMethod\nBackbone\n1-shot\n5-shot\n1-shot\n5-shot\nMAML [12]\nRN18/WRN [39]\n47.6/−\n64.5/−\n51.4/−\n69.5/−\nVersa [15]\n47.8/−\n61.9/−\n50.0/−\n65.6/−\nEntropy-min [11]\n58.5/60.4\n74.8/76.2\n63.6/66.1\n82.1/84.2\nPT-MAP [16]\n60.1/60.6\n67.1/66.8\n76.9/78.9\n85.3/86.6\nLaplacianShot [48]\n65.4/70.0\n81.6/83.2\n70.1/72.9\n82.1/83.8\nBD-CSPN [26]\n67.0/70.4\n80.2/82.3\n69.4/72.5\n82.0/83.7\nTIM [5]\n67.3/69.8\n79.8/81.6\n71.8/74.6\n83.9/85.9\nα-TIM [39]\n67.4/69.8\n82.5/84.8\n−/−\n−/−\nBAVARDAGE (ours)\n71.0/74.1\n83.6/85.5\n75.1/78.5\n84.5/87.4\nBAVARDAGE (ours)\nRN12 [3]\n77.8\n88.0\n82.7\n89.5\ntiered-Imagenet\nunbalanced\nbalanced\nMethod\nBackbone\n1-shot\n5-shot\n1-shot\n5-shot\nEntropy-min [11]\nRN18/WRN [39]\n61.2/62.9\n75.5/77.3\n67.0/68.9\n83.1/84.8\nPT-MAP [16]\n64.1/65.1\n70.0/71.0\n82.9/84.6\n88.8/90.0\nLaplacianShot [48]\n72.3/73.5\n85.7/86.8\n77.1/78.8\n86.2/87.3\nBD-CSPN [26]\n74.1/75.4\n84.8/85.9\n76.3/77.7\n86.2/87.4\nTIM [5]\n74.1/75.8\n84.1/85.4\n78.6/80.3\n87.7/88.9\nα-TIM [39]\n74.4/76.0\n86.6/87.8\n−/−\n−/−\nBAVARDAGE (ours)\n76.6/77.5\n86.5/87.5\n80.3/81.5\n87.1/88.3\nBAVARDAGE (ours)\nRN12 [3]\n79.4\n88.0\n83.5\n89.0\nCUB\nunbalanced\nbalanced\nMethod\nBackbone\n1-shot\n5-shot\n1-shot\n5-shot\nPT-MAP [16]\nRN18 [39]\n65.1\n71.3\n85.5\n91.3\nEntropy-min [11]\n67.5\n82.9\n72.8\n88.9\nLaplacianShot [48]\n73.7\n87.7\n78.9\n88.8\nBD-CSPN [26]\n74.5\n87.1\n77.9\n88.9\nTIM [5]\n74.8\n86.9\n80.3\n90.5\nα-TIM [39]\n75.7\n89.8\n−\n−\nBAVARDAGE (ours)\n82.0\n90.7\n85.6\n91.4\nBAVARDAGE (ours)\nRN12 [3]\n83.1\n90.8\n87.4\n92.0\nperformance increases by a relatively large margin, suggesting the effectiveness of our proposed adaptive dimension\nreduction method.\nVisualization of features for different projections\nTo further showcase the effect of proposed PLDA, in Fig. 2 we\nvisualize the extracted features of a 3-way Few-Shot task in the following 3 scenarios: (a) features in the original space,\nusing T-SNE [38] for visualization purpose; (b) features that are projected directly onto their centroids space, and ﬁnally\n(c) features projected using PLDA. The ellipses drawn in (b) and (c) are the cluster estimations computed using the real\nlabels of data samples, and we can thus observe a larger separation of different clusters with PLDA projection for the\ntask in which the original features overlap heavily between clusters in blue and green.\n7\n\n\nAdaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\nTable 2: Ablation study on the elements of our proposed method, with results tested on mini-Imagenet (backbone:\nWRN) and CUB (backbone: RN18) in the unbalanced setting.\nmini-Imagenet\nCUB\nSoft-KMEANS\nVB\nPLDA\n1-shot\n5-shot\n1-shot\n5-shot\n✓\n71.4\n82.4\n77.5\n86.7\n✓\n✓\n71.8\n82.5\n77.8\n87.2\n✓\n✓\n✓\n74.1\n85.5\n82.0\n90.7\n(a) T-SNE\n(b) Centroids projection\n(c) PLDA\nFigure 2: Visualization of extracted features of a Few-Shot task using different projection methods (dataset: mini-\nImagenet, backbone: WRN), we report a 86.7%, 90.0% and 95.0% prediction accuracy corresponding to each\nprojection.\nRobustness against imbalance\nIn Table 1 we show the accuracy of our proposed method using VB priors introduced\nin Section 3.3, in which αo is set to be equal to the Dirichlet’s parameter α∗\no for the level of imbalance in the query\nset. Therefore, in this experiment we test the robustness of BAVARDAGE, namely in Fig. 3 we alter αo and report the\naccuracy on different imbalance levels (varying α∗\no) in both 1-shot and 5-shot settings. Note that the proposed model\nbecomes slightly more sensitive to αo when the level of imbalance increases (smaller α∗\no), with an approximate 1%\ndrop of accuracy when increasing αo in the case of α∗\no = 1.\n2\n4\n6\n8\n10\n73\n74\n75\nαo\nAccuracy\n1-shot\n2\n4\n6\n8\n10\n85\n85.5\n86\nαo\n5-shot\nα∗\no = 1\nα∗\no = 2\nα∗\no = 4\nFigure 3: 1-shot and 5-shot accuracy on different imbalance levels (varying α∗\no) as a function of VB priors αo (dataset:\nmini-Imagenet, backbone: WRN).\nVarying Few-Shot settings\nIn this experiment we observe the performance of BAVARDAGE on different Few-Shot\nsettings, namely we vary the number of labelled samples per class L as well as the total number of unlabelled samples\nQ in a task, for further comparison we also report the accuracy using only Soft-KMEANS algorithm. In Fig. 4 we can\nobserve constant higher accuracy of our proposed method, and a slightly larger difference gap when Q increases.\n8\n\n\nAdaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\n0\n5\n10\n15\n20\n70\n75\n80\n85\n90\nL (# of labelled data per class)\nAccuracy\n20\n40\n60\n80\n100\n120\n140\n160\n68\n70\n72\n74\n76\nQ (# of total unlabelled data)\nBAVARDAGE\nSoft-KMEANS\nFigure 4: Accuracy as a funtion of L and Q in comparison with Soft-KMEANS (dataset: mini-Imagenet, backbone:\nWRN).\n5\nConclusion\nIn this paper we proposed a clustering method based on Variational Bayesian Inference and Probabilistic Linear\nDiscriminant Analysis for transductive Few-Shot Classiﬁcation. BAVARDAGE has reached state-of-the-art accuracy\non nearly all Few-Shot benchmarks in the realistic unbalanced setting, as well as competitive performance in the\nbalanced setting without using a perfectly class-balanced prior. As our proposed method assumes a shared isotropic\ncovariance matrix for all clusters, the estimations in VB models could be limited. Therefore the future work could study\na better estimation of covariance matrices associated with each cluster. An interesting asset of the proposed method\nis that it performs most of its processing in a reduced (K −1)-dimensional space, where K is the number of classes,\nsuggesting interests for visualization and suitability for more elaborate statistical machine learning methods. As in [39],\nwe encourage the community to rethink the works in transductive settings to provide fairer grounds of comparison\nbetween the various proposed approaches.\n6\nAppendix\n6.1\nImplementation details on the proposed PLDA\nIn this section we present more details on our implementation of PLDA proposed in section 3.2 in the paper. Given\nX ∈RN×D, we estimate its within-class covariance matrix to be Sbase\nw\ncalculated from Dbase. Denote Ibase\nc\nas the set\nof samples belonging to base class c where c ∈1, ..., |Cbase|, therefore Φw is approximated as follows:\nΦw ≈Sbase\nw\n=\nP\nc\nP\ni∈Ibase\nc\n(xbase\ni\n−mbase\nc\n)(xbase\ni\n−mbase\nc\n)T\nNbase\n,\n(6)\nwhere mbase\nc\n=\n1\n|Ibase\nc\n|\nP\ni∈Ibase\nc\nxbase\ni\nis the mean of c-th base class. Let λ = [λ1, ..., λi, ..., λD] ∈RD be the\neigenvalues of Sbase\nw\nin descending order, and we set R = [r1, ..., ri, ..., rD] ∈RD×D to be the corresponding\neigenvectors. In this paper we deﬁne a transformation matrix T = SR where S is a diagonal matrix with diagonal\nvalues being the square root of multiplicative inverse of λ, clamped to an upper bound smax. Namely, s = diag(S)\nwhere s = [s1, ...si, ...sD] ∈RD is a D-length vector containing the scaling value for each dimension, and we set si to\nbe as follows:\nsi =\n\u001a\nλ−0.5\ni\nif\nλ−0.5\ni\n≤smax\nsmax\notherwise\n.\n(7)\nWe can see from Eq. 7 that T is composed of a rotation matrix and scaling values on feature dimensions that help morph\nthe within-class distribution into an identity covariance matrix. This corresponds to a data sphering/whitening process in\nwhich we decorrelate samples in each of the dimensions. In our implementation we transform X by multiplying it with\nT. Therefore the sphered data samples, denoted as X′ = [x′\n1, ...x′\nn, ...x′\nN]T ∈RN×D, are obtained from x′\nn = Txn.\nNext, we project X′ onto a subspace that corresponds to the K −1 largest eigenvalues of its between-scatter matrix.\nDenote m′\nk as the estimated centroid for class k, given soft class assignments onk (1 ≤n ≤N, 1 ≤k ≤K), m′\nk is\ncomputed as:\nm′\nk =\nPN\nn=1 onkx′\nn\nγ + Nk\n, Nk =\nN\nX\nn=1\nonk ,\n(8)\n9\n\n\nAdaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\nAlgorithm 2 Proposed PLDA\nFonction PLDA (X, Sbase\nw\n, smax, onk)\nSphere X using T (Eq. 7), obtain X′.\nEstimate centroids m′\nk using onk (Eq. 8).\nCompute Ψ using m′\nk (Eq. 9).\nProject X′ onto the centroids space, obtain U.\nReturn U\nwhere γ is used as an offset indicating how close the centroids are to 0, in this paper we set it to 10.0, same as βo in the\nVB model in reduced space. Therefore, the between-class scatter matrix Ψ of sphered samples can be calculated as:\nΨ =\nK\nX\nk=1\n(m′\nk −m′)(m′\nk −m′)T ,\n(9)\nwhere m′ =\n1\nK\nPK\nk=1 m′\nk is the mean of estimated class centroids. Then we project X′ onto a d-length subspace,\nwhere d = K −1. In details, denote V = [v1, ..., vi, ..., vd] ∈RD×d to be the eigenvectors corresponding to the d\nlargest eigenvalues of Ψ, the projected data U are obtained as un = VT x′\nn for each sample. Note that the formulation\nof Ψ in Eq. 9 allows at most K −1 non-zero eigenvalues, therefore the resulting subspace projection using these\neigenvectors is equivalent to a projection onto the afﬁne subspace containing the centroids m′\nk. Furthermore, according\nto Eq. 1 in the paper, we can further deduce the projection matrix W to be as follows:\nun = WT xn = VT x′\nn = VT Txn = VT SRxn,\n=⇒W = (VT SR)T = RT SV.\n(10)\nThe entire process is described in Algorithm 2.\n6.2\nImplementation details on the proposed VB model\nIn this section we provide more detailed explanation of our proposed VB model. Given a posterior p(θ|U), we\napproximate it with a function variational distribution q(θ) by minimizing the Kullback-Leibler divergence:\nq∗(θ) = arg min\nq {DKL(q||p)}\n= arg min\nq {log p(U) −L(q)}\n= arg max\nq {L(q)}\n(11)\nwhere the evidence log p(U) is considered ﬁxed, and L(q) =\nR\nq(θ) log p(θ,U)\nq(θ) dθ stands for Evidence Lower BOund\n(ELBO) providing “evidence” that we have chosen the right model. We can see that minimizing the Kullback-\nLeibler divergence is equivalent to maximizing the ELBO. Suppose θ = {θ1, ..., θm, ..., θM}, we ﬁrstly factorize\nq(θ) = QM\nm=1 q(θm) according to the Mean-Field assumption, then we solve each term individually:\nL(q) =\nZ\nq(θ) log p(θ, U)\nq(θ) dθ\n=\nZ  M\nY\nm=1\nq(θm)\n!  \nlog p(θ, U) −\nM\nX\nm=1\nlog q(θm)\n!\ndθ1dθ2...dθM\n=\nM\nX\nm=1\n\u0012Z\nq(θm)\n\u0012Z\nq(θ−m) log p(θ, U)dθ−m\n\u0013\ndθm −\nZ\nq(θm) log q(θm)dθm\n\u0013\n,\n(12)\nand the ELBO is maximized when:\nlog q∗(θm) = Eθ−m[log p(θ, U)] + const,\n(13)\n10\n\n\nAdaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\nwhere Eθ−m[·] stands for the expectation with respect to all variables in θ except θm. In our method we deﬁne\nθ = {Z, π, µ}, the detailed formula of some variables are presented as follows:\nzn = [zn1, ..., znk, ..., znK] ∈{0, 1}K,\nK\nX\nk=1\nznk = 1,\nπ = [π1, ..., πk, ..., πK],\nπk ≥0,\nK\nX\nk=1\nπk = 1.\n(14)\nAccording to Bayes’ theorem, we rewrite the posterior to be:\np(θ|U) = p(Z, π, µ|U) = p(Z, π, µ, U)\np(U)\n= p(U|Z, µ)p(Z|π)p(π)p(µ)\np(U)\n,\n(15)\nin which:\np(U|Z, µ) =\nN\nY\nn=1\nK\nY\nk=1\nN(un|µk, Λ−1)znk,\np(Z|π) =\nN\nY\nn=1\nCategorical(zn|π) =\nN\nY\nn=1\nK\nY\nk=1\nπznk\nk\n,\np(π) = Dir(π|αo) = Γ(PK\nk=1 Kαo)\nQK\nk=1 Γ(αo)\nK\nY\nk=1\nπαo−1\nk\n= C(αo)\nK\nY\nk=1\nπαo−1\nk\n,\np(µ) =\nK\nY\nk=1\nN(µk|mo, (βoΛ)−1).\n(16)\nAccording to Eq. 13, q∗(π) can be computed as follows:\nlog q∗(π) = EZ,µ[log p(Z, π, µ, U)] + const\n= EZ[log p(Z|π)] + log p(π) + const\n=\nN\nX\nn=1\nK\nX\nk=1\nEZ[znk] log πk +\nK\nX\nk=1\n(αo −1) log πk + const\n=\nK\nX\nk=1\nN\nX\nn=1\nonk log πk +\nK\nX\nk=1\n(αo −1) log πk + const\n=\nK\nX\nk=1\n(Nk + αo −1) log πk + const,\n=⇒q∗(π) =\nK\nY\nk=1\nπNk+αo−1\nk\n+ const\n=\nK\nY\nk=1\nπαk−1\nk\n+ const\n= Dir(π|α).\n(17)\n11\n\n\nAdaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\nSimilarly for q∗(µ) we can compute it as shown below:\nlog q∗(µ) = EZ,π[log p(Z, π, µ, U)] + const\n= EZ[log p(U|Z, µ)] + log p(µ) + const\n=\nN\nX\nn=1\nK\nX\nk=1\nEZ[znk] log N(un|µk, Λ−1) +\nK\nX\nk=1\nlog N(µk|mo, (βoΛ−1) + const\n= 1\n2\nN\nX\nn=1\nK\nX\nk=1\nonk log |Λ| −1\n2\nN\nX\nn=1\nK\nX\nk=1\nonk(un −µk)T Λ(un −µk)\n+ 1\n2\nK\nX\nk=1\nlog |βoΛ| −1\n2\nK\nX\nk=1\n(µk −mo)T βoΛ(µk −mo).\n(18)\nTo compute βk, we gather the quadratic terms that contain µk in Eq. 18:\n(quad) = −1\n2\nN\nX\nn=1\nK\nX\nk=1\nonkµT\nk Λµk −1\n2\nK\nX\nk=1\nµT\nk βoΛµk\n= −1\n2\nK\nX\nk=1\nµT\nk (NkΛ + βoΛ)µk\n= −1\n2\nK\nX\nk=1\nµT\nk (βo + Nk)Λkµk,\n=⇒βk = βo + Nk.\n(19)\nAs for mk, we gather the linear terms that contain µk in Eq. 18:\n(linear) = 1\n2\nN\nX\nn=1\nK\nX\nk=1\nonkµT\nk Λun + 1\n2\nK\nX\nk=1\nµT\nk βoΛmo\n= 1\n2\nK\nX\nk=1\nµT\nk Λ(βomo +\nN\nX\nn=1\nonkun)\n= 1\n2\nK\nX\nk=1\nµT\nk βkΛmk,\n=⇒mk = 1\nβk\n(βomo +\nN\nX\nn=1\nonkun).\n(20)\nTherefore q∗(µ) can be reformulated as:\nq∗(µ) =\nK\nY\nk=1\nq∗(µk) =\nK\nY\nk=1\nN(µk|mk, (βkΛ)−1).\n(21)\nWe also provide a more detailed calculation of q∗(Z):\nlog q∗(Z) = Eπ,µ[log p(Z, π, µ, U)] + const\n= Eπ[log p(Z|π)] + Eµ[log p(U|Z, µ)] + const\n=\nN\nX\nn=1\nK\nX\nk=1\nznk\n\u0000Eπ[log πk] + Eµ[log N(un|µk, Λ−1)]\n\u0001\n+ const\n=\nN\nX\nn=1\nK\nX\nk=1\nznk log ρnk + const,\n(22)\n12\n\n\nAdaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\nwhere\nlog ρnk = Eπ[log πk] + Eµ[log N(un|µk, Λ−1)]\n= Eπ[log πk] + 1\n2 log |Λ| −d\n2 log 2π −1\n2Eµ[(un −µk)T Λ(un −µk)].\n(23)\nTherefore q∗(Z) can be expressed as:\nq∗(Z) =\nN\nY\nn=1\nK\nY\nk=1\noznk\nnk =\nN\nY\nn=1\nCategorical(zn|on),\nonk =\nρnk\nPK\nj=1 ρnj\n,\n(24)\nwe can see that the variable follows a categorical distribution, parameterized by onk, and onk = EZ[znk]. As for Eq. 23,\nmore details are shown as follows:\nEπ[log πk] = ψ(αk) −ψ(\nK\nX\nj=1\nαj),\nEµ[(un −µk)T Λ(un −µk)] =\nZ\n(un −µk)T Λ(un −µk)q∗(µk)dµk\n= (un −mk)T Λk(un −mk) + Tr[Λ · (βkΛ)−1]\n= dβ−1\nk\n+ (un −mk)T Λ(un −mk),\n(25)\nψ(·) is the logarithmic derivative of the gamma function, and the distribution for πk and µk follows Eq. 17 and 21.\nTherefore:\nlog ρnk = ψ(αk) −ψ(\nK\nX\nj=1\nαj) + 1\n2 log |Λ| −d\n2 log 2π −1\n2[dβ−1\nk\n+ (un −mk)T Λ(un −mk)].\n(26)\nFrom the above equations we observe a dependency between priors and posteriors, which can be estimated iteratively\ndepending on the class allocations. Therefore in this paper we propose to solve it under a basic Expectation Maximization\nframework where we estimate onk in the E-step, while updating αk, βk and mk in the M-step.\n6.3\nHyperparameter tuning\nIn this section we detail about how the hyperparameters in our proposed method are obtained. Namely, for a standard\nFew-Shot benchmark that has been split into base-validation-novel class set, we ﬁrstly tune our model using validation\nset and choose the hyperparameters accordingly before applying to the novel set. For example in Figure 5 we tune\ntwo temperature parameters Tkm, Tvb, the scaling up-bound parameter smax and the VB prior βo that are used in\nour proposed BAVARDAGE. The blue curves show the performance on validation set while the red curves show the\naccuracy on the novel set (benchmark: mini-Imagenet). From the ﬁgure we see a similar behavior between two sets in\nterms of performance, Tkm has little impact on the accuracy, same for Tvb when it is large. For smax we observe an\nuptick when it is around 1, followed by a slowing decrease and ﬁnally stabilizing to the same accuracy when it becomes\nlarger. In this paper we tune hyperparameters for each benchmark in the same way. For tiered-Imagenet we set Tkm,\nTvb and smax to be 10, 100, 2 in the balanced setting, 100, 100, 1 in the unbalanced setting; for CUB we set them to be\n10, 4, 5 in both balanced and unbalanced settings; and for FC100 and CIFAR-FS we set the hyperparameters to be the\nsame as mini-Imagenet. As for βo we set it to be 10 across datasets since it gives the best performance.\n6.4\nAdditional experiments on other Few-Shot benchmarks\nIn Section 4 in the paper we tested our proposed method on three standard Few-Shot benchmarks: mini-Imagenet1, tiered-\nImagenet2 and CUB3, following the same setting as presented in https://github.com/oveilleux/Realistic_\nTransductive_Few_Shot. In this section we further conduct experiments on two other well-known Few-Shot datasets:\n1) FC100 (https://github.com/ElementAI/TADAM) is a recent split dataset based on CIFAR-100 [22] that contains\n60 base classes for training, 20 classes for validation and 20 novel classes for evaluation, each class is composed of\n1https://github.com/yaoyao-liu/mini-imagenet-tools\n2https://github.com/yaoyao-liu/tiered-imagenet-tools\n3http://www.vision.caltech.edu/datasets/cub_200_2011\n13\n\n\nAdaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\n0\n20\n40\n60\n80\n100\n74\n76\n78\nTkm\nAccuracy\n1-shot\n0\n20\n40\n60\n80\n100\n86\n87\n88\nTkm\n5-shot\nval\ntest\n40\n60\n80\n100\n72\n74\n76\n78\nTvb\nAccuracy\n40\n60\n80\n100\n82\n84\n86\n88\nTvb\nval\ntest\n1\n2\n3\n4\n5\n74\n76\n78\nsmax\nAccuracy\n1\n2\n3\n4\n5\n85\n86\n87\n88\nsmax\nval\ntest\n0\n5\n10\n15\n20\n72\n74\n76\n78\nβo\nAccuracy\n0\n5\n10\n15\n20\n84\n86\n88\nβo\nval\ntest\nFigure 5: Hyperparameter tuning of our proposed method. Here we tune 4 hyperparameters of BAVARDAGE on\nmini-Imagenet (backbone: WRN) in the unbalanced setting.\n600 images of size 32x32 pixels; 2) CIFAR-FS (https://github.com/bertinetto/r2d2) is also sampled from\nCIFAR-100 and shares the same quantity of classes in the base-validation-novel splits as for mini-Imagenet. Each\nclass contains 600 images of size 32x32 pixels. In Table 3 below we report the accuracy of our proposed method on\nall benchmarks, note that for FC100 and CIFAR-FS we believe to be among the ﬁrst to conduct experiments in the\nunbalanced setting.\nIn Table 3 we also show the results using WRN and RN18 pretrained from [39] and RN12 pretrained from [3],\nsame as Table 1 in the paper, with a conﬁdence interval of 95% added next to the accuracy. In addition, given that\nsome works [27, 47] in the ﬁeld utilize data augmentation techniques to extract features based on images in original\ndimensions instead of reduced ones, here we apply our BAVARDAGE following the same setting and report the accuracy\non a pretrained RN12 feature extractor [3] with data augmentation (denote RN12*). For comparison purpose we also\nprovide a baseline accuracy on each Few-Shot benchmark using Soft-KMEANS algorithm.\nWith BAVARDAGE, we observe a clear increase of accuracy for all datasets compared with Soft-KMEANS in both\nbalanced and unbalanced settings, suggesting the genericity of the proposed method. As for the computational time, we\nevaluate an average of 1.72 seconds per accuracy (on 10,000 Few-Shot tasks) using a GeForce RTX 3090 GPU.\n14\n\n\nAdaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\nTable 3: Detailed results of BAVARDAGE with conﬁdence interval of 95% on the Few-Shot benchmarks, along with a\nbaseline accuracy using Soft-KMEANS. We use RN18 and WRN pretrained from [39], RN12 and RN12* pretrained\nfrom [3].\nmini-Imagenet\nunbalanced\nbalanced\nMethod\nBackbone\n1-shot\n5-shot\n1-shot\n5-shot\nSoft-KMEANS\nRN18 [39]\n68.82 ± 0.27%\n81.27 ± 0.17%\n73.47 ± 0.26%\n83.04 ± 0.15%\nWRN [39]\n71.35 ± 0.27%\n82.41 ± 0.16%\n75.70 ± 0.25%\n84.42 ± 0.14%\nRN12 [3]\n75.65 ± 0.25%\n86.35 ± 0.14%\n80.81 ± 0.24%\n87.92 ± 0.12%\nRN12* [3]\n77.51 ± 0.26%\n87.78 ± 0.14%\n82.14 ± 0.24%\n89.08 ± 0.12%\nBAVARDAGE\nRN18 [39]\n71.01 ± 0.31%\n83.60 ± 0.17%\n75.07 ± 0.28%\n84.49 ± 0.14%\nWRN [39]\n74.10 ± 0.30%\n85.52 ± 0.16%\n78.51 ± 0.27%\n87.41 ± 0.13%\nRN12 [3]\n77.85 ± 0.28%\n88.02 ± 0.14%\n82.67 ± 0.25%\n89.50 ± 0.11%\nRN12* [3]\n79.76 ± 0.29%\n89.85 ± 0.13%\n84.80 ± 0.25%\n91.65 ± 0.10%\ntiered-Imagenet\nunbalanced\nbalanced\nMethod\nBackbone\n1-shot\n5-shot\n1-shot\n5-shot\nSoft-KMEANS\nWRN [39]\n73.92 ± 0.28%\n85.02 ± 0.18%\n78.59 ± 0.27%\n85.76 ± 0.16%\nRN18 [39]\n73.79 ± 0.28%\n84.65 ± 0.18%\n78.34 ± 0.27%\n85.52 ± 0.17%\nRN12 [3]\n78.15 ± 0.27%\n87.65 ± 0.17%\n83.11 ± 0.25%\n88.80 ± 0.15%\nRN12* [3]\n79.62 ± 0.27%\n88.61 ± 0.16%\n84.08 ± 0.24%\n89.56 ± 0.14%\nBAVARDAGE\nWRN [39]\n77.45 ± 0.31%\n87.48 ± 0.18%\n81.47 ± 0.28%\n88.27 ± 0.16%\nRN18 [39]\n76.55 ± 0.31%\n86.46 ± 0.19%\n80.32 ± 0.28%\n87.14 ± 0.16%\nRN12 [3]\n79.38 ± 0.29%\n88.04 ± 0.18%\n83.52 ± 0.26%\n89.03 ± 0.15%\nRN12* [3]\n81.17 ± 0.29%\n89.63 ± 0.17%\n85.20 ± 0.25%\n90.41 ± 0.14%\nCUB\nunbalanced\nbalanced\nMethod\nBackbone\n1-shot\n5-shot\n1-shot\n5-shot\nSoft-KMEANS\nRN18 [39]\n77.54 ± 0.26%\n86.70 ± 0.14%\n82.67 ± 0.24%\n89.04 ± 0.11%\nRN12 [3]\n81.24 ± 0.25%\n87.27 ± 0.14%\n84.87 ± 0.22%\n89.64 ± 0.11%\nRN12* [3]\n82.40 ± 0.24%\n89.40 ± 0.13%\n87.38 ± 0.20%\n91.29 ± 0.10%\nBAVARDAGE\nRN18 [39]\n82.00 ± 0.28%\n90.67 ± 0.12%\n85.64 ± 0.25%\n91.42 ± 0.10%\nRN12 [3]\n83.12 ± 0.26%\n90.81 ± 0.12%\n87.41 ± 0.22%\n92.03 ± 0.09%\nRN12* [3]\n86.96 ± 0.24%\n92.84 ± 0.10%\n90.42 ± 0.20%\n93.50 ± 0.08%\nFC100\nunbalanced\nbalanced\nMethod\nBackbone\n1-shot\n5-shot\n1-shot\n5-shot\nSoft-KMEANS\nRN12 [3]\n51.24 ± 0.27%\n64.70 ± 0.22%\n54.59 ± 0.26%\n66.37 ± 0.20%\nRN12* [3]\n51.64 ± 0.27%\n65.26 ± 0.22%\n54.87 ± 0.26%\n66.89 ± 0.20%\nBAVARDAGE\nRN12 [3]\n52.60 ± 0.32%\n65.35 ± 0.25%\n56.66 ± 0.28%\n69.69 ± 0.21%\nRN12* [3]\n53.78 ± 0.30%\n68.75 ± 0.24%\n57.27 ± 0.29%\n70.60 ± 0.21%\nCIFAR-FS\nunbalanced\nbalanced\nMethod\nBackbone\n1-shot\n5-shot\n1-shot\n5-shot\nSoft-KMEANS\nRN12 [3]\n80.72 ± 0.25%\n88.31 ± 0.17%\n85.47 ± 0.22%\n89.36 ± 0.15%\nRN12* [3]\n81.75 ± 0.25%\n88.92 ± 0.17%\n86.07 ± 0.22%\n89.85 ± 0.15%\nBAVARDAGE\nRN12 [3]\n82.68 ± 0.27%\n88.97 ± 0.18%\n86.20 ± 0.23%\n89.58 ± 0.15%\nRN12* [3]\n83.82 ± 0.27%\n89.84 ± 0.18%\n87.35 ± 0.23%\n90.63 ± 0.16%\n15\n\n\nAdaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\nReferences\n[1] A. Antoniou, H. Edwards, and A. J. Storkey. How to train your MAML. In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n[2] S. Baik, J. Choi, H. Kim, D. Cho, J. Min, and K. M. Lee. Meta-learning with task-adaptive loss function for few-shot learning.\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9465–9474, 2021.\n[3] Y. Bendou, Y. Hu, R. Lafargue, G. Lioi, B. Pasdeloup, S. Pateux, and V. Gripon. Easy: Ensemble augmented-shot y-shaped\nlearning: State-of-the-art few-shot classiﬁcation with simple ingredients. arXiv preprint arXiv:2201.09699, 2022.\n[4] L. Bertinetto, J. F. Henriques, P. H. S. Torr, and A. Vedaldi. Meta-learning with differentiable closed-form solvers. In 7th\nInternational Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net,\n2019.\n[5] M. Boudiaf, I. Ziko, J. Rony, J. Dolz, P. Piantanida, and I. Ben Ayed. Information maximization for few-shot learning. In\nH. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems,\nvolume 33, pages 2445–2457. Curran Associates, Inc., 2020.\n[6] T. Cao, M. T. Law, and S. Fidler. A theoretical analysis of the number of shots in few-shot learning. In 8th International\nConference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\n[7] C. Chen, K. Li, W. Wei, J. T. Zhou, and Z. Zeng. Hierarchical graph neural networks for few-shot learning. IEEE Transactions\non Circuits and Systems for Video Technology, 32(1):240–252, 2021.\n[8] W. Chen, Y. Liu, Z. Kira, Y. F. Wang, and J. Huang. A closer look at few-shot classiﬁcation. In 7th International Conference\non Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n[9] D. Comaniciu and P. Meer. Mean shift: A robust approach toward feature space analysis. IEEE Transactions on pattern\nanalysis and machine intelligence, 24(5):603–619, 2002.\n[10] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the\nRoyal Statistical Society: Series B (Methodological), 39(1):1–22, 1977.\n[11] G. S. Dhillon, P. Chaudhari, A. Ravichandran, and S. Soatto. A baseline for few-shot image classiﬁcation. In 8th International\nConference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\n[12] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the\n34th International Conference on Machine Learning-Volume 70, pages 1126–1135. JMLR. org, 2017.\n[13] C. W. Fox and S. J. Roberts. A tutorial on variational bayesian inference. Artiﬁcial intelligence review, 38(2):85–95, 2012.\n[14] S. Gidaris and N. Komodakis. Generating classiﬁcation weights with gnn denoising autoencoders for few-shot learning. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 21–30, 2019.\n[15] J. Gordon, J. Bronskill, M. Bauer, S. Nowozin, and R. E. Turner. Meta-learning probabilistic inference for prediction. In 7th\nInternational Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net,\n2019.\n[16] Y. Hu, V. Gripon, and S. Pateux. Leveraging the feature distribution in transfer-based few-shot learning. In International\nConference on Artiﬁcial Neural Networks, pages 487–499. Springer, 2021.\n[17] S. Ioffe. Probabilistic linear discriminant analysis. In European Conference on Computer Vision, pages 531–542. Springer,\n2006.\n[18] T. S. Jaakkola and M. I. Jordan. Improving the mean ﬁeld approximation via the use of mixture distributions. In Learning in\ngraphical models, pages 163–173. Springer, 1998.\n[19] D. Kang, H. Kwon, J. Min, and M. Cho. Relational embedding for few-shot classiﬁcation. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 8822–8833, 2021.\n[20] M. Kearns, Y. Mansour, and A. Y. Ng. An information-theoretic analysis of hard and soft assignment methods for clustering.\nIn Learning in graphical models, pages 495–520. Springer, 1998.\n[21] J. Kim, T. Kim, S. Kim, and C. D. Yoo. Edge-labeling graph neural network for few-shot learning. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pages 11–20, 2019.\n[22] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.\n[23] M. Lazarou, T. Stathaki, and Y. Avrithis. Iterative label cleaning for transductive and semi-supervised few-shot learning. In\nProceedings of the IEEE/CVF International Conference on Computer Vision, pages 8751–8760, 2021.\n[24] E. Lee, C.-H. Huang, and C.-Y. Lee. Few-shot and continual learning with attentive independent mechanisms. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision, pages 9455–9464, 2021.\n[25] M. Lichtenstein, P. Sattigeri, R. Feris, R. Giryes, and L. Karlinsky. Tafssl: Task-adaptive feature sub-space learning for few-shot\nclassiﬁcation. In European Conference on Computer Vision, pages 522–539. Springer, 2020.\n[26] J. Liu, L. Song, and Y. Qin. Prototype rectiﬁcation for few-shot learning. In Computer Vision–ECCV 2020: 16th European\nConference, Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16, pages 741–756. Springer, 2020.\n16\n\n\nAdaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classiﬁcation\nA PREPRINT\n[27] X. Luo, L. Wei, L. Wen, J. Yang, L. Xie, Z. Xu, and Q. Tian. Rectifying the shortcut learning of background for few-shot\nlearning. Advances in Neural Information Processing Systems, 34, 2021.\n[28] P. Mangla, N. Kumari, A. Sinha, M. Singh, B. Krishnamurthy, and V. N. Balasubramanian. Charting the right manifold:\nManifold mixup for few-shot learning. In The IEEE Winter Conference on Applications of Computer Vision, pages 2218–2227,\n2020.\n[29] T. K. Moon. The expectation-maximization algorithm. IEEE Signal processing magazine, 13(6):47–60, 1996.\n[30] B. Oreshkin, P. Rodríguez López, and A. Lacoste. Tadam: Task dependent adaptive metric for improved few-shot learning.\nAdvances in neural information processing systems, 31, 2018.\n[31] O. V. Prezhdo. Mean ﬁeld approximation for the stochastic schrödinger equation. The Journal of chemical physics, 111(18):8366–\n8377, 1999.\n[32] M. Ren, E. Triantaﬁllou, S. Ravi, J. Snell, K. Swersky, J. B. Tenenbaum, H. Larochelle, and R. S. Zemel. Meta-learning for\nsemi-supervised few-shot classiﬁcation. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver,\nBC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.\n[33] M. N. Rizve, S. Khan, F. S. Khan, and M. Shah. Exploring complementary strengths of invariant and equivariant representations\nfor few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n10836–10846, 2021.\n[34] P. Rodríguez, I. Laradji, A. Drouin, and A. Lacoste. Embedding propagation: Smoother manifold for few-shot classiﬁcation. In\nEuropean Conference on Computer Vision, pages 121–138. Springer, 2020.\n[35] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet\nlarge scale visual recognition challenge. International journal of computer vision, 115(3):211–252, 2015.\n[36] J. Snell, K. Swersky, and R. Zemel. Prototypical networks for few-shot learning. In Advances in Neural Information Processing\nSystems, pages 4077–4087, 2017.\n[37] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and T. M. Hospedales. Learning to compare: Relation network for few-shot\nlearning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1199–1208, 2018.\n[38] L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.\n[39] O. Veilleux, M. Boudiaf, P. Piantanida, and I. Ben Ayed. Realistic evaluation of transductive few-shot learning. Advances in\nNeural Information Processing Systems, 34, 2021.\n[40] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al. Matching networks for one shot learning. In Advances in neural\ninformation processing systems, pages 3630–3638, 2016.\n[41] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.\n[42] Y. Wang, W. Chao, K. Q. Weinberger, and L. van der Maaten. Simpleshot: Revisiting nearest-neighbor classiﬁcation for\nfew-shot learning. CoRR, abs/1911.04623, 2019.\n[43] D. Wertheimer, L. Tang, and B. Hariharan. Few-shot classiﬁcation with feature map reconstruction networks. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8012–8021, 2021.\n[44] L. Yang, L. Li, Z. Zhang, X. Zhou, E. Zhou, and Y. Liu. Dpgn: Distribution propagation graph network for few-shot learning.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13390–13399, 2020.\n[45] S. Yang, L. Liu, and M. Xu. Free lunch for few-shot learning: Distribution calibration. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\n[46] H.-J. Ye, H. Hu, D.-C. Zhan, and F. Sha. Few-shot learning via embedding adaptation with set-to-set functions. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8808–8817, 2020.\n[47] C. Zhang, Y. Cai, G. Lin, and C. Shen. Deepemd: Few-shot image classiﬁcation with differentiable earth mover’s distance\nand structured classiﬁers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages\n12203–12213, 2020.\n[48] I. Ziko, J. Dolz, E. Granger, and I. B. Ayed. Laplacian regularized few-shot learning. In International Conference on Machine\nLearning, pages 11660–11670. PMLR, 2020.\n17\n"
}