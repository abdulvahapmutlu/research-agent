{
  "filename": "2006.03806v3.pdf",
  "num_pages": 11,
  "pages": [
    "Leveraging the Feature Distribution\nin Transfer-based Few-Shot Learning\nYuqing Hu\nVincent Gripon\nSt´ephane Pateux\nIMT Atlantique\nOrange Labs\nIMT Atlantique\nOrange Labs\nAbstract\nFew-shot classiﬁcation is a challenging prob-\nlem due to the uncertainty caused by using\nfew labelled samples. In the past few years,\nmany methods have been proposed to solve\nfew-shot classiﬁcation, among which transfer-\nbased methods have proved to achieve the\nbest performance.\nFollowing this vein, in\nthis paper we propose a novel transfer-based\nmethod that builds on two successive steps:\n1) preprocessing the feature vectors so that\nthey become closer to Gaussian-like distri-\nbutions, and 2) leveraging this preprocess-\ning using an optimal-transport inspired algo-\nrithm (in the case of transductive settings).\nUsing standardized vision benchmarks, we\nprove the ability of the proposed methodol-\nogy to achieve state-of-the-art accuracy with\nvarious datasets, backbone architectures and\nfew-shot settings.\nThe code can be found\nat https://github.com/yhu01/PT-MAP.\n1\nIntroduction\nThanks to their outstanding performance, Deep Learn-\ning methods are widely considered for vision tasks such\nas object classiﬁcation or detection. To reach top per-\nformance, these systems are typically trained using very\nlarge labelled datasets that are representative enough\nof the inputs to be processed afterwards.\nHowever, in many applications, it is costly to acquire\nor to annotate data, resulting in the impossibility to\ncreate such large labelled datasets. In this context, it\nis challenging to optimize Deep Learning architectures\nconsidering the fact they typically are made of way\nmore parameters than the dataset contains. This is\nwhy in the past few years, few-shot learning (i.e. the\nproblem of learning with few labelled examples) has\nbecome a trending research subject in the ﬁeld. In\nmore details, there are two settings that authors often\nconsider: a) “inductive few-shot”, where only a few\nlabelled samples are available during training and pre-\ndiction is performed on each test input independently,\nand b) “transductive few-shot”, where prediction is\nperformed on a batch of (non-labelled) test inputs,\nallowing to take into account their joint distribution.\nMany works in the domain are built based on a “learn-\ning to learn” guidance, where the pipeline is to train\nan optimizer [8, 23, 30] with diﬀerent tasks of limited\ndata so that the model is able to learn generic experi-\nence for novel tasks. Namely, the model learns a set of\ninitialization parameters that are in an advantageous\nposition for the model to adapt to a new (small) dataset.\nRecently, the trend evolved towards using well-thought-\nout transfer architectures (called backbones) [31, 6]\ntrained one time on the same training data, but seen\nas a unique large dataset.\nA main problem of using feature vectors extracted us-\ning a backbone architecture is that their distribution\nis likely to be complex, as the problem the backbone\nhas been optimized for most of the time diﬀers from\nthe considered task. As such, methods that rely on\nstrong assumptions about the data distributions are\nlikely to fail in leveraging the quality of features. In\nthis paper, we tackle the problem of transfer-based few-\nshot learning with a twofold strategy: 1) preprocessing\nthe data extracted from the backbone so that it ﬁts a\nparticular distribution (i.e. Gaussian-like) and 2) lever-\naging this speciﬁc distribution thanks to a well-thought\nproposed algorithm based on maximum a posteriori\nand optimal transport (only in the case of transductive\nfew-shot). Using standardized benchmarks in the ﬁeld,\nwe demonstrate the ability of the proposed method to\nobtain state-of-the-art accuracy, for various problems\nand backbone architectures in some inductive settings\nand most transductive ones.\narXiv:2006.03806v3  [cs.LG]  26 Jan 2021\n",
    "Running heading title breaks the line\npreprocessing\nMAP\nfϕ\nPT\nS ∪Q\nSinkhorn mapping\nInitialized cj\nCenter update\nnsteps\nPrediction\nfQ\nfS ∪fQ\nM∗\ncj\nAccuracy\nhj(k)\n˜hj(k)\nPT\nFigure 1: Illustration of the proposed method. First we extract feature vectors of all the inputs in Dnovel and\npreprocess them to obtain fS ∪fQ. Note that the Power transform (PT) has the eﬀect of mapping a skewed\nfeature distribution into a gaussian-like distribution (hj(k) denotes the histogram of feature k in class j). In\nMAP, we perform Sinkhorn mapping with class center cj initialized on fS to obtain the class allocation matrix\nM∗for fQ, and we update the class centers for the next iteration. After nsteps we evaluate the accuracy on fQ.\n2\nRelated work\nA large volume of works in few-shot classiﬁcation is\nbased on meta learning [30] methods, where the training\ndata is transformed into few-shot learning episodes\nto better ﬁt in the context of few examples. In this\nbranch, optimization based methods [30, 8, 23] train a\nwell-initialized optimizer so that it quickly adapts to\nunseen classes with a few epochs of training. Other\nworks [41, 4] utilize data augmentation techniques to\nartiﬁcially increase the size of the training datasets.\nIn the past few years, there have been a growing interest\nin transfer-based methods. The main idea consists in\ntraining feature extractors able to eﬃciently segregate\nnovel classes it never saw before. For example, in [3]\nthe authors train the backbone with a distance-based\nclassiﬁer [22] that takes into account the inter-class\ndistance. In [21], the authors utilize self-supervised\nlearning techniques [2] to co-train an extra rotation\nclassiﬁer for the output features, improving the accu-\nracy in few-shot settings. Many approaches are built\non top of a feature extractor. For instance, in [38] the\nauthors implement a nearest class mean classiﬁer to\nassociate an input with a class whose centroid is the\nclosest in terms of the ℓ2 distance. In [18] an iterative\napproach is used to adjust the class centers. In [13] the\nauthors build a graph neural neural network to gather\nthe feature information from similar samples. Transfer-\nbased techniques typically reach the best performance\non standardized benchmarks.\nAlthough many works involve feature extraction, few\nhave explored the features in terms of their distribu-\ntion [11]. Often, assumptions are made that the fea-\ntures in a class align to a certain distribution, even\nthough these assumptions are rarely experimentally\ndiscussed. In our work, we analyze the impact of the\nfeatures distributions and how they can be transformed\nfor better processing and accuracy. We also introduce a\nnew algorithm to improve the quality of the association\nbetween input features and corresponding classes in\ntypical few-shot settings.\nContributions.\nLet us highlight the main contribu-\ntions of this work. (1) We propose to preprocess the\nraw extracted features in order to make them more\naligned with Gaussian assumptions. Namely we intro-\nduce transforms of the features so that they become\nless skewed. (2) We use a wasserstein-based method\nto better align the distribution of features with that of\nthe considered classes. (3) We show that the proposed\nmethod can bring large increase in accuracy with a\nvariety of feature extractors and datasets, leading to\nstate-of-the-art results in the considered benchmarks.\n3\nMethodology\nIn this section we introduce the problem settings. We\ndiscuss the training of the feature extractors, the pre-\nprocessing steps that we apply on the trained features\nand the ﬁnal classiﬁcation algorithm. A summary of\nour proposed method is depicted in Figure 1.\n3.1\nProblem statement\nWe consider a typical few-shot learning problem. We\nare given a base dataset Dbase and a novel dataset\nDnovel such that Dbase ∩Dnovel = ∅. Dbase contains\na large number of labelled examples from K diﬀerent\nclasses. Dnovel, also referred to as a task in other works,\n",
    "Yuqing Hu, Vincent Gripon, St´ephane Pateux\ncontains a small number of labelled examples (support\nset S), along with some unlabelled ones (query set Q),\nall from w new classes. Our goal is to predict the\nclass of the unlabelled examples in the query set. The\nfollowing parameters are of particular importance to\ndeﬁne such a few-shot problem: the number of classes\nin the novel dataset w (called w-way), the number of\nlabelled samples per class s (called s-shot) and the\nnumber of unlabelled samples per class q. So the novel\ndataset contains a total of w(s + q) samples, ws of\nthem being labelled, and wq of them being those to\nclassify. In the case of inductive few-shot, the prediction\nis performed independently on each one of the wq\nsamples. In the case of transductive few-shot [20, 18],\nthe prediction is performed considering all wq samples\ntogether. In the latter case, most works exploit the\ninformation that there are exactly q samples in each\nclass. We discuss this point in the experiments.\n3.2\nFeature extraction\nThe ﬁrst step is to train a neural network backbone\nmodel using only the base dataset. In this work we\nconsider multiple backbones, with various training pro-\ncedures. Once the considered backbone is trained, we\nobtain robust embeddings that should generalize well to\nnovel classes. We denote by fϕ the backbone function,\nobtained by extracting the output of the penultimate\nlayer from the considered architecture, with ϕ being\nthe trained architecture parameters. Note that im-\nportantly, in all backbone architectures used in the\nexperiments of this work, the penultimate layers are\nobtained by applying a ReLU function, so that all\nfeature components coming out of fϕ are nonnegative.\n3.3\nFeature preprocessing\nAs mentioned in Section 2, many works hypothesize,\nexplicitly or not, that the features from the same class\nare aligned with a speciﬁc distribution (often Gaussian-\nlike). But this aspect is rarely experimentally veriﬁed.\nIn fact, it is very likely that features obtained using\nthe backbone architecture are not Gaussian. Indeed,\nusually the features are obtained after applying a relu\nfunction, and exhibit a positive distribution mostly\nconcentrated around 0 (see details in the next section).\nMultiple works in the domain [38, 18] discuss the diﬀer-\nent statistical methods (e.g. normalization) to better\nﬁt the features into a model. Although these methods\nmay have provable assets for some distributions, they\ncould worsen the process if applied to an unexpected\ninput distribution. This is why we propose to prepro-\ncess the obtained feature vectors so that they better\nalign with typical distribution assumptions in the ﬁeld.\nNamely, we use a power transform as follows.\nPower transform (PT).\nDenote v = fϕ(x) ∈\n(R+)d, x ∈Dnovel as the obtained features on Dnovel.\nWe hereby perform a power transformation method,\nwhich is similar to Tukey’s Transformation Ladder [32],\non the features. We then follow a unit variance projec-\ntion, the formula is given by:\nf(v) =\n(\n(v+ϵ)β\n∥(v+ϵ)β∥2\nif β ̸= 0\nlog (v+ϵ)\n∥log (v+ϵ)∥2\nif β = 0\n,\n(1)\nwhere ϵ = 1e−6 is used to make sure that v+ϵ is strictly\npositive and β is a hyper-parameter. The rationales of\nthe preprocessing above are: (1) Power transforms have\nthe functionality of reducing the skew of a distribution,\nadjusted by β, (2) Unit variance projection scales the\nfeatures to the same area so that large variance features\ndo not predominate the others. This preprocessing step\nis often able to map data from any distribution to a\nclose-to-Gaussian distribution. We will analyse this\nability and the eﬀect of power transform in more details\nin Section 4.\nNote that β = 1 leads to almost no eﬀect. More gener-\nally, the skew of the obtained distribution changes when\nβ varies. For instance, if a raw distribution is right-\nskewed, decreasing β phases out the right skew, and\nphases into a left-skewed distribution when β becomes\nnegative. After experiments, we found that β = 0.5\ngives the most consistent results for our considered\nexperiments. More details based on our considered\nexperiments are available in Section 4.\nThis ﬁrst step of feature preprocessing can be performed\nin both inductive and transductive settings.\n3.4\nMAP\nLet us assume that the preprocessed feature distribu-\ntion for each class is Gaussian or Gaussian-like. As such,\na well-positioned class center is crucial to a good pre-\ndiction. In this section we discuss how to best estimate\nthe class centers when the number of samples is very\nlimited and classes are only partially labelled. In more\ndetails, we propose an Expectation–Maximization [7]-\nlike algorithm that will iteratively ﬁnd the Maximum\nA Posteriori (MAP) estimates of the class centers.\nWe ﬁrstly show that estimating these centers through\nMAP is similar to the minimization of Wasserstein\ndistance.\nThen, an iterative procedure based on a\nWasserstein distance estimation, using the sinkhorn\nalgorithm [5, 33, 14], is designed to estimate the optimal\ntransport from the initial distribution of the feature\nvectors to one that would correspond to the draw of\nsamples from Gaussian distributions.\nNote that in this step we consider what is called the\n“transductive” setting in many other few shot learning\n",
    "Running heading title breaks the line\nworks [20, 18, 19, 13, 17, 9, 16, 10, 39], where we exploit\nunlabelled samples during the procedure as well as\npriors about their relative proportions.\nIn the following, we denote by fS the set of feature\nvectors corresponding to labelled inputs and by fQ\nthe set of feature vectors corresponding to unlabelled\ninputs. For a feature vector f ∈fS ∪fQ, we denote\nby ℓ(f) the corresponding label. We use 0 < i ≤wq\nto denote the index of an unlabelled sample, so that\nfQ = (fi)i, and we denote cj, 0 < j ≤w the estimated\ncenter for feature vectors corresponding to class j.\nOur algorithm consists in several steps in which we\nestimate class centers from a soft allocation matrix\nM∗, then we update the allocation matrix based on\nthe newly found class centers and iterate the process.\nIn the following paragraphs, we detail these steps.\nSinkhorn mapping.\nConsidering using MAP esti-\nmation for the class centers, and assuming a Gaussian\ndistribution for each class, we typically aim at solving:\n{ˆl(fi)}, {ˆcj}\n=\narg max{ℓ(fi)}∈C,{cj}\nQ\ni P(fi|j = ℓ(fi))\n=\narg min{ℓ(fi)}∈C,{cj}\nP\ni(fi −cℓ(fi))2,\n(2)\nwhere C represents the set of admissible labelling sets.\nLet us point out that the last term corresponds ex-\nactly to the Wasserstein distance used in the Optimal\nTransport problem formulation [5].\nTherefore, in this step we ﬁnd the class mapping matrix\nthat minimizes the Wasserstein distance. Inspired by\nthe Sinkhorn algorithm [35, 5], we deﬁne the mapping\nmatrix M∗as follows:\nM∗= Sinkhorn(L, p, q, λ)\n= arg\nmin\nM∈U(p,q)\nX\nij\nMijLij + λH(M),\n(3)\nwhere U(p, q) ∈Rwq×w\n+\nis a set of positive matrices for\nwhich the rows sum to p and the columns sum to q.\nFormally, U(p, q) can be written as:\nU(p, q) = {M ∈Rwq×w\n+\n|M1w = p, MT 1wq = q}, (4)\np denotes the distribution of the amount that each\nunlabelled example uses for class allocation, and q\ndenotes the distribution of the amount of unlabelled\nexamples allocated to each class. Therefore, U(p, q)\ncontains all the possible ways of allocating examples to\nclasses. The cost function L ∈Rwq×w in Equation (3)\nconsists of the euclidean distances between unlabelled\nexamples and class centers, hence Lij denotes the eu-\nclidean distance between example i and class center j.\nHere we assume a soft class mapping, meaning that\neach example can be “sliced” into diﬀerent classes.\nThe second term on the right of Equation (3) denotes\nthe entropy of M: H(M) = −P\nij Mij log Mij, regu-\nlarized by a hyper-parameter λ. Increasing λ would\nforce the entropy to become smaller, so that the map-\nping is less homogeneous. This term also makes the\nobjective function strictly convex [5, 29] and thus a\npractical and eﬀective computation. From lemma 2\nin [5], the result of this Sinkhorn mapping has the\ntypical form M∗= diag(u) · exp(−L/λ) · diag(v).\nIterative center estimation.\nIn this step, our aim\nis to estimate class centers. As shown in Algorithm 1,\nwe initialize cj as the average of labelled samples be-\nlonging to class j. Then cj is iteratively re-estimated.\nAt each iteration, we compute a mapping matrix M∗\non the unlabelled examples using the sinkhorn map-\nping. Along with labelled examples, we re-estimate cj\n(temporarily denoted µj) by weighted-averaging the\nfeatures with their allocated portions for class j:\nµj = g(M∗, j) =\nPwq\ni=1 M∗ijfi + P\nf∈fS,ℓ(f)=j f\ns + Pwq\ni=1 M∗ij\n. (5)\nThis formula corresponds to the minimization of Equa-\ntion (3). Note that labelled examples do not participate\nin the mapping process. Since their labels are known,\nwe instead set allocations for their belonging classes to\nbe 1 and to the others to be 0. Therefore, labelled exam-\nples have the largest possible weight when re-estimating\nthe class centers.\nProportioned center update.\nIn order to avoid\ntaking risky harsh decisions in early iterations of the\nalgorithm, we propose to proportionate the update\nof class centers using an inertia parameter. In more\ndetails, we update the center with a learning rate 0 <\nα ≤1. When α is close to 0, the update becomes very\nslow, whereas α = 1 corresponds to directly allocating\nthe newly found class centers:\ncj ←cj + α(µj −cj).\n(6)\nFinal decision.\nAfter a ﬁxed number of steps nsteps,\nthe rows of M∗are interpreted as probabilities to be-\nlong to each class. The maximal value corresponds to\nthe decision of the algorithm.\nA summary of our proposed algorithm is presented in\nAlgorithm 1. In Table 1 we summarize the main param-\neters and hyperparameters of the considered problem\nand proposed solution. The code is available at XXX.\n4\nExperiments\n4.1\nDatasets\nWe evaluate the performance of the proposed method\nusing standardized few-shot classiﬁcation datasets:\nminiImageNet [36], tieredImageNet [24], CUB [37] and\n",
    "Yuqing Hu, Vincent Gripon, St´ephane Pateux\nAlgorithm 1: Proposed algorithm\nParameters\n: w, s, q, λ, α, nsteps\nInitialization : cj = 1\ns · P\nf∈fS,ℓ(f)=j f\nrepeat nsteps times:\nLij = ∥fi −cj∥2, ∀i, j\nM∗= Sinkhorn(L, p = 1wq, q = q1w, λ)\nµj = g(M∗, j)\ncj ←cj + α(µj −cj)\nend\nreturn ˆℓ(fi) = arg maxj(M∗[i, j])\nCIFAR-FS [1]. The miniImageNet dataset contains\n100 classes randomly chosen from ILSVRC- 2012 [25]\nand 600 images of size 84×84 pixels per class. It is split\ninto 64 base classes, 16 validation classes and 20 novel\nclasses. The tieredImageNet dataset is another sub-\nset of ImageNet, it consists of 34 high-level categories\nwith 608 classes in total. These categories are split into\n20 meta-training superclasses, 6 meta-validation super-\nclasses and 8 meta-test superclasses, which corresponds\nto 351 base classes, 97 validation classes and 160 novel\nclasses respectively. The CUB dataset contains 200\nclasses and has 11,788 images of size 84 × 84 pixels in\ntotal. Following [13], it is split into 100 base classes, 50\nvalidation classes and 50 novel classes. The CIFAR-\nFS dataset has 100 classes, each class contains 600\nimages of size 32 × 32 pixels. The splits of this dataset\nare the same as those in miniImageNet.\n4.2\nImplementation details\nIn order to stress the genericity of our proposed\nmethod with regards to the chosen backbone architec-\nture and training strategy, we perform experiments\nusing WRN [40], ResNet18 and ResNet12 [12],\nalong with some other pretrained backbones (e.g.\nDenseNet [15]). For each dataset we train the feature\nextractor with base classes, tune the hyperparameters\nwith validation classes and test the performance using\nnovel classes. Therefore, for each test run, w classes\nare drawn uniformly at random among novel classes.\nAmong these w classes, s labelled examples and q un-\nlabelled examples per class are uniformly drawn at\nrandom to form Dnovel. The WRN and ResNet are\ntrained following [21].\nIn the inductive setting, we\nuse our proposed Power Transform followed by a basic\nNearest Class Mean (NCM) classiﬁer. In the trans-\nductive setting, the MAP or an alternative is applied\nafter PT. In order to better segregate between feature\nvectors of corresponding classes for each task, we imple-\nment the “trans-mean-sub” [18] before MAP where we\nseparately subtract inputs by the means of labelled and\nunlabelled examples, followed by a unit hypersphere\nprojection. All our experiments are performed using\nw = 5, q = 15, s = 1 or 5. We run 10,000 random draws\nto obtain mean accuracy score and indicate conﬁdence\nscores (95%) when relevant. The tuned hyperparam-\neters for miniImageNet are β = 0.5, λ = 10, α = 0.4\nand nsteps = 30 for s = 1; β = 0.5, λ = 10, α = 0.2\nand nsteps = 20 for s = 5. Hyperparameters for other\ndatasets are detailed in the experiments below.\n4.3\nComparison with state-of-the-art\nmethods\nIn the ﬁrst experiment, we conduct our proposed\nmethod on diﬀerent benchmarks and compare the per-\nformance with other state-of-the-art solutions. The\nresults are presented in Table 2, we observe that our\nmethod with WRN as backbone reaches the state-of-\nthe-art performance for most cases in both inductive\nand transductive settings on all the benchmarks. In\nTable 3 we also implement our proposed method on\ntieredImageNet based on a pre-trained DenseNet121\nbackbone following the procedure described in [38].\nFrom these experiments we conclude that the proposed\nmethod can bring an increase of accuracy with a vari-\nety of backbones and datasets, leading to competitive\nperformance. In terms of execution time, we measured\nan average of 0.002s per run.\nPerformance on cross-domain settings.\nWe also\ntest our method in a cross-domain setting, where the\nbackbone is trained with the base classes in miniIma-\ngeNet but tested with the novel classes in CUB dataset.\nAs shown in Table 4, the proposed method gives the\nbest accuracy both in the case of 1-shot and 5-shot.\n4.4\nOther experiments\nAblation study.\nTo prove the interest of the ingre-\ndients on the proposed method in order to reach top\nperformance, we report in Tables 5 and 6 the results\nof ablation studies. In Table 5, we ﬁrst investigate\nthe impact of changing the backbone architecture. To-\ngether with previous experiments, we observe that the\nproposed method consistently achieves the best results\nfor any ﬁxed backbone architecture. We also report\nperformance in the case of inductive few-shot using a\nsimple Nearest-Class Mean (NCM) classiﬁer instead of\nthe iterative MAP procedure described in Section 3.\nWe perform another experiment where we replace the\nMAP algorithm with a standard K-Means where cen-\ntroids are initialized with the available labelled samples\nfor each class. We can observe signiﬁcant drops in ac-\ncuracy, emphasizing the interest of the proposed MAP\nprocedure to better estimate the class centers.\nIn Table 6 we show the impact of PT in the transductive\nsetting, where we can see about 6% gain for 1-shot and\n",
    "Running heading title breaks the line\nTable 1: Important parameters and hyperparameters.\nNovel dataset parameters\nNotation\nValue\nDescription\nw\ntypically 5\nnumber of classes\ns\ntypically 1 or 5\nnumber of labelled inputs per class\nq\ntypically 15\nnumber of unlabelled inputs per class\nProposed method hyperparameters\nNotation\nRange\nDescription\nβ\n{−2, −1, −0.5, 0, 0.5, 1, 2}\ncoeﬃcient to adjust distribution skew\nλ\nλ ∈R+\nregularization coeﬃcient for sinkhorn mapping\nα\n0 < α ≤1\nlearning rate for class center updates\n4% gain for 5-shot in terms of accuracy.\nEﬀect of Power Transform.\nTo visualize the eﬀect\nof PT on the feature distributions, we depict in Figure 2\nthe distributions of an arbitrarily selected feature for 5\nrandomly selected novel classes of miniImageNet when\nusing WRN, before and after applying PT. We observe\nquite clearly how PT is able to reshape the feature\ndistributions to close-to-gaussian distributions.\nWe\nobserved similar behaviors with other datasets as well.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0\n50\n100\n150\nClass 1\nClass 2\nClass 3\nClass 4\nClass 5\n(a)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0\n20\n40\n60\n80\nClass 1\nClass 2\nClass 3\nClass 4\nClass 5\n(b)\nFigure 2: Distributions of an arbitrarily chosen feature\nfor 5 novel classes before (a) and after (b) PT.\nInﬂuence of the number of unlabelled samples.\nSmall values of q lead to settings that are closer to the\ninductive case. In order to better understand the gain\nin accuracy due to having access to more unlabelled\nsamples, we depict in Figure 4 the evolution of accuracy\nas a function of q, when w = 5 is ﬁxed. Interestingly,\nthe accuracy quickly reaches a close-to-asymptotical\nplateau, emphasizing the ability of the method to soon\nexploit available information in the task.\nImpact of class imbalance.\nIn all previous trans-\nductive experiments, we assumed a balanced number of\nunlabelled samples per class. We now consider the case\nof 2 classes, where we vary the number of unlabelled\nexamples q1 of class 1 with respect to that of class 2\n(100 −q1). In Figure 3 we depict: 1) the performance\nof the inductive version of our method (PT-NCM),\nwhich is independent of q1, 2) the performance of the\nproposed transductive method when the vector q is\nappropriately deﬁned (knowing the proportion of ele-\nments in class 1 vs. class 2), and 3) a mixed case where\nwe expect at least 30 elements in both classes but do not\nknow exactly how many (q = [30, 30]). Interestingly,\nwe observe that the transductive setting still outper-\nforms the inductive ones even when the proportion of\nelements in both classes is only approximately known.\n0\n10\n20\n30\n40\n50\n85\n90\n95\n100\nq1\nAccuracy\nPT+NCM\nPT+MAP (q = [30, 30])\nPT+MAP (q = [q1, 100 −q1])\nFigure 3: Accuracy of 2-ways classiﬁcation on miniIma-\ngenet (1-shot) with unevenly distributed query data for\neach class in diﬀerent settings, where the total number\nof query inputs remains constant (total: 100 elements).\nWhen q1 = 1, we obtain the most imbalanced case,\nwhereas q1 = 50 corresponds to a balanced case.\nHyperparameter tuning.\nIn the next experiment\nwe tune β, λ and α on the validation classes of each\ndataset, and then apply them to test our model on\n",
    "Yuqing Hu, Vincent Gripon, St´ephane Pateux\nTable 2: 1-shot and 5-shot accuracy of state-of-the-art methods in the literature, compared with the proposed\nsolution. We present results using WRN as backbones for our proposed solutions.\nminiImageNet\nSetting\nMethod\nBackbone\n1-shot\n5-shot\nInductive\nBaseline++ [3]\nResNet18\n51.87 ± 0.77%\n75.68 ± 0.63%\nMAML [8]\nResNet18\n49.61 ± 0.92%\n65.72 ± 0.77%\nProtoNet [28]\nWRN\n62.60 ± 0.20%\n79.97 ± 0.14%\nMatching Networks [36]\nWRN\n64.03 ± 0.20%\n76.32 ± 0.16%\nSimpleShot [38]\nDenseNet121\n64.29 ± 0.20%\n81.50 ± 0.14%\nS2M2 R [21]\nWRN\n64.93 ± 0.18%\n83.18 ± 0.11%\nPT+NCM(ours)\nWRN\n65.35 ± 0.20%\n83.87 ± 0.13%\nTransductive\nBD-CSPN [19]\nWRN\n70.31 ± 0.93%\n81.89 ± 0.60%\nTransfer+SGC [13]\nWRN\n76.47 ± 0.23%\n85.23 ± 0.13%\nTAFSSL [18]\nDenseNet121\n77.06 ± 0.26%\n84.99 ± 0.14%\nDFMN-MCT [17]\nResNet12\n78.55 ± 0.86%\n86.03 ± 0.42%\nPT+MAP(ours)\nWRN\n82.92 ± 0.26%\n88.82 ± 0.13%\nCUB\nSetting\nMethod\nBackbone\n1-shot\n5-shot\nInductive\nBaseline++ [3]\nResNet10\n69.55 ± 0.89%\n85.17 ± 0.50%\nMAML [8]\nResNet10\n70.32 ± 0.99%\n80.93 ± 0.71%\nProtoNet [28]\nResNet18\n72.99 ± 0.88%\n86.64 ± 0.51%\nMatching Networks [36]\nResNet18\n73.49 ± 0.89%\n84.45 ± 0.58%\nS2M2 R [21]\nWRN\n80.68 ± 0.81%\n90.85 ± 0.44%\nPT+NCM(ours)\nWRN\n80.57 ± 0.20%\n91.15 ± 0.10%\nTransductive\nBD-CSPN [19]\nWRN\n87.45%\n91.74%\nTransfer+SGC [13]\nWRN\n88.35 ± 0.19%\n92.14 ± 0.10%\nPT+MAP(ours)\nWRN\n91.55 ± 0.19%\n93.99 ± 0.10%\nCIFAR-FS\nSetting\nMethod\nBackbone\n1-shot\n5-shot\nInductive\nProtoNet [28]\nConvNet64\n55.50 ± 0.70%\n72.00 ± 0.60%\nMAML [8]\nConvNet32\n58.90 ± 1.90%\n71.50 ± 1.00%\nS2M2 R [21]\nWRN\n74.81 ± 0.19%\n87.47 ± 0.13%\nPT+NCM(ours)\nWRN\n74.64 ± 0.21%\n87.64 ± 0.15%\nTransductive\nDSN-MR [27]\nResNet12\n78.00 ± 0.90%\n87.30 ± 0.60%\nTransfer+SGC [13]\nWRN\n83.90 ± 0.22%\n88.76 ± 0.15%\nPT+MAP(ours)\nWRN\n87.69 ± 0.23%\n90.68 ± 0.15%\nTable 3: 1-shot and 5-shot accuracy of state-of-the-art\nmethods on tieredImageNet.\ntieredImageNet\nMethod\nBackbone\n1-shot\n5-shot\nProtoNet [28]♭\nConvNet4\n53.31 ± 0.89%\n72.69 ± 0.74%\nLEO [26]♭\nWRN\n66.33 ± 0.05%\n81.44 ± 0.09%\nSimpleShot [38]♭\nDenseNet121\n71.32 ± 0.22%\n86.66 ± 0.15%\nPT+NCM(ours)♭\nDenseNet121\n69.96 ± 0.22%\n86.45 ± 0.15%\nDFMN-MCT [17]♯\nResNet12\n80.89 ± 0.84%\n87.30 ± 0.49%\nTAFSSL [18]♯\nDenseNet121\n84.29 ± 0.25%\n89.31 ± 0.15%\nPT+MAP(ours)♯\nDenseNet121\n85.67 ± 0.26%\n90.45 ± 0.14%\n♭: Inductive setting.\n♯: Transductive setting.\nnovel classes. We vary each hyperparamter in a certain\nrange and observe the evolution of accuracy to choose\nthe peak that corresponds to the highest prediction.\nFor example, the evolving curve for β, λ and α with\nminiImageNet are presented in Figure 4 (2) to (4). For\ncomparison purposes, we also trace the corresponding\ncurves on novel classes. We draw a dash line on the\nhyperparameter values where the accuracy on the vali-\nTable 4: 1-shot and 5-shot accuracy of state-of-the-art\nmethods when performing cross-domain classiﬁcation\n(backbone: WRN).\nMethod\n1-shot\n5-shot\nBaseline++ [3]♭\n40.44 ± 0.75%\n56.64 ± 0.72%\nManifold Mixup [34]♭\n46.21 ± 0.77%\n66.03 ± 0.71%\nS2M2 R [21]♭\n48.24 ± 0.84%\n70.44 ± 0.75%\nPT+NCM(ours)♭\n48.37 ± 0.19%\n70.22 ± 0.17%\nTransfer+SGC [13]♯\n58.63 ± 0.25%\n73.46 ± 0.17%\nPT+MAP(ours)♯\n62.49 ± 0.32%\n76.51 ± 0.18%\n♭: Inductive setting.\n♯: Transductive setting.\ndation classes peaks, meaning that this is the chosen\nvalue resulting in Table 2.\nThe following observations can be drawn from this ex-\nperiment: 1) The evolving curves on validation classes\n(red) and novel classes (blue) have generally similar\ntrend for each hyperparameter.\nIn particular, two\ncurves peak at the same β (β = 0.5) and λ (λ = 10),\n",
    "Running heading title breaks the line\nTable 5: Accuracy of the proposed method in inductive and transductive settings, with diﬀerent backbones, and\ncomparison with K-Means and NCM baselines.\nSetting\nInductive\nTransductive\n(NCM baseline) Proposed PT+NCM\nPT+K-Means\nProposed PT+MAP\nDataset\nBackbone\n1-shot\n5-shot\n1-shot\n5-shot\n1-shot\n5-shot\nminiImageNet\nResNet12\n(49.08) 62.68 ± 0.20%\n(70.85) 81.99 ± 0.14%\n72.73 ± 0.23%\n84.05 ± 0.14%\n78.47 ± 0.28%\n85.84 ± 0.15%\nResNet18\n(47.63) 62.50 ± 0.20%\n(72.89) 82.17 ± 0.14%\n73.08 ± 0.22%\n84.67 ± 0.14%\n80.00 ± 0.27%\n86.96 ± 0.14%\nWRN\n(55.31) 65.35 ± 0.20%\n(78.33) 83.87 ± 0.13%\n76.67 ± 0.22%\n86.73 ± 0.13%\n82.92 ± 0.26%\n88.82 ± 0.13%\nCUB\nResNet12\n(61.30) 78.40 ± 0.20%\n(82.83) 91.12 ± 0.10%\n87.35 ± 0.19%\n92.31 ± 0.10%\n90.96 ± 0.20%\n93.77 ± 0.09%\nResNet18\n(58.92) 76.98 ± 0.20%\n(82.69) 90.56 ± 0.10%\n87.16 ± 0.19%\n91.97 ± 0.09%\n91.10 ± 0.20%\n93.78 ± 0.09%\nWRN\n(69.21) 80.57 ± 0.20%\n(88.33) 91.15 ± 0.10%\n88.28 ± 0.19%\n92.37 ± 0.10%\n91.55 ± 0.19%\n93.99 ± 0.10%\nCIFAR-FS\nResNet12\n(52.50) 71.02 ± 0.22%\n(74.16) 84.68 ± 0.16%\n78.39 ± 0.24%\n85.73 ± 0.16%\n82.45 ± 0.27%\n87.33 ± 0.17%\nResNet18\n(56.40) 71.41 ± 0.22%\n(78.30) 85.50 ± 0.15%\n79.95 ± 0.23%\n86.74 ± 0.16%\n84.80 ± 0.25%\n88.55 ± 0.16%\nWRN\n(68.93) 74.64 ± 0.21%\n(86.81) 87.64 ± 0.15%\n83.69 ± 0.22%\n89.19 ± 0.15%\n87.69 ± 0.23%\n90.68 ± 0.15%\nTable 6: Inﬂuence of Power Transform in the transductive setting with diﬀerent backbones on miniImageNet.\nWRN\nResNet18\nResNet12\nPT\nMAP\n1-shot\n5-shot\n1-shot\n5-shot\n1-shot\n5-shot\n75.60 ± 0.29%\n84.13 ± 0.16%\n74.48 ± 0.29%\n82.88 ± 0.17%\n72.04 ± 0.30%\n80.98 ± 0.18%\n82.92 ± 0.26%\n88.82 ± 0.13%\n80.00 ± 0.27%\n86.96 ± 0.14%\n78.47 ± 0.28%\n85.84 ± 0.15%\nmeaning that validation classes and novel classes share\nthe same β and λ that reach the highest accuracy. 2) A\nsmall λ tends to lead to a homogeneous class partition\nfor M∗, where each sample are uniformly allocated\nto w classes. Hence the sharp drop on the accuracy\nwhen λ < 5. 3) A too small α results in an insuﬃcient\nclass center update. On the contrary, the impact on a\nlarge α is relatively mild. Overall, it is interesting to\npoint out the little sensitivity of the proposed method\naccuracy with regards to hyperparameter tuning.\nWe followed this procedure to ﬁnd the tuned hyper-\nparameters for each dataset. Therefore, we obtained\nthat working with CUB leads to the the same hyper-\nparameters as miniImageNet. For tieredImageNet and\nCIFAR-FS, the best accuracy are obtained on valida-\ntion classes when β = 0.5, λ = 10, α = 0.3 for s = 1;\nβ = 0.5, λ = 10, α = 0.2 for s = 5.\n5\nConclusion\nIn this paper we introduced a new pipeline to solve the\nfew-shot classiﬁcation problem. Namely, we proposed\nto ﬁrstly preprocess the raw feature vectors to better\nalign to a Gaussian distribution and then we designed\nan optimal-transport inspired iterative algorithm to\nestimate the class centers. Our experimental results\non standard vision benchmarks reach state-of-the-art\naccuracy, with important gains in both 1-shot and 5-\nshot classiﬁcation. Moreover, the proposed method can\nbring gains with a variety of feature extractors, with few\nhyperparameters. Thus we believe that the proposed\nmethod is applicable to many practical problems.\n0\n50\n100\n150\n80\n85\n90\n(1) 5q\nAccuracy\nmini\ncub\ncifar −fs\n−2\n−1 −0.5 0\n0.5\n1\n2\n40\n60\n80\n(2) β\nAccuracy\nnovel\nval\n0\n10\n20\n30\n70\n75\n80\n85\n(3) λ\nAccuracy\nnovel\nval\n0.2\n0.4\n0.6\n0.8\n1\n82\n84\n86\n(4) α\nAccuracy\nnovel\nval\nFigure 4: (1) represents 5-way 1-shot accuracy on\nminiImagenet, CUB and CIFAR-FS (backbone: WRN)\nas a function of q. (2), (3) and (4) represent 1-shot\naccuracy on miniImageNet (backbone: WRN) as a\nfunction of β, λ and α respectively.\n",
    "Yuqing Hu, Vincent Gripon, St´ephane Pateux\nReferences\n[1] L. Bertinetto, J. F. Henriques, P. H. Torr, and\nA. Vedaldi. Meta-learning with diﬀerentiable closed-\nform solvers. arXiv preprint arXiv:1805.08136, 2018.\n[2] O. Chapelle, B. Scholkopf, and A. Zien.\nSemi-\nsupervised learning (chapelle, o. et al., eds.; 2006)[book\nreviews].\nIEEE Transactions on Neural Networks,\n20(3):542–542, 2009.\n[3] W.-Y. Chen, Y.-C. Liu, Z. Kira, Y.-C. F. Wang, and\nJ.-B. Huang. A closer look at few-shot classiﬁcation,\n2019.\n[4] Z. Chen, Y. Fu, Y.-X. Wang, L. Ma, W. Liu, and\nM. Hebert. Image deformation meta-networks for one-\nshot learning. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages\n8680–8689, 2019.\n[5] M. Cuturi. Sinkhorn distances: Lightspeed computa-\ntion of optimal transport. In Advances in neural in-\nformation processing systems, pages 2292–2300, 2013.\n[6] D. Das and C. G. Lee. A two-stage approach to few-\nshot learning for image recognition. IEEE Transactions\non Image Processing, 2019.\n[7] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maxi-\nmum likelihood from incomplete data via the em algo-\nrithm. Journal of the Royal Statistical Society: Series\nB (Methodological), 39(1):1–22, 1977.\n[8] C. Finn, P. Abbeel, and S. Levine. Model-agnostic\nmeta-learning for fast adaptation of deep networks. In\nProceedings of the 34th International Conference on\nMachine Learning-Volume 70, pages 1126–1135. JMLR.\norg, 2017.\n[9] V. Garcia and J. Bruna. Few-shot learning with graph\nneural networks.\narXiv preprint arXiv:1711.04043,\n2017.\n[10] S. Gidaris and N. Komodakis. Generating classiﬁcation\nweights with gnn denoising autoencoders for few-shot\nlearning. arXiv preprint arXiv:1905.01102, 2019.\n[11] V. Gripon, G. B. Hacene, M. L¨owe, and F. Vermet.\nImproving accuracy of nonparametric transfer learning\nvia vector segmentation. In 2018 IEEE International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 2966–2970, 2018.\n[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep resid-\nual learning for image recognition. In Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016.\n[13] Y. Hu, V. Gripon, and S. Pateux. Exploiting unsuper-\nvised inputs for accurate few-shot classiﬁcation. arXiv\npreprint arXiv:2001.09849, 2020.\n[14] G. Huang, H. Larochelle, and S. Lacoste-Julien. Are\nfew-shot learning benchmarks too simple?\narXiv\npreprint arXiv:1902.08605, 2019.\n[15] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q.\nWeinberger. Densely connected convolutional networks.\nIn Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 4700–4708, 2017.\n[16] J. Kim, T. Kim, S. Kim, and C. D. Yoo. Edge-labeling\ngraph neural network for few-shot learning. In Pro-\nceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 11–20, 2019.\n[17] S. M. Kye, H. B. Lee, H. Kim, and S. J. Hwang. Trans-\nductive few-shot learning with meta-learned conﬁdence.\narXiv preprint arXiv:2002.12017, 2020.\n[18] M. Lichtenstein, P. Sattigeri, R. Feris, R. Giryes, and\nL. Karlinsky. Tafssl: Task-adaptive feature sub-space\nlearning for few-shot classiﬁcation.\narXiv preprint\narXiv:2003.06670, 2020.\n[19] J. Liu, L. Song, and Y. Qin. Prototype rectiﬁcation for\nfew-shot learning. arXiv preprint arXiv:1911.10713,\n2019.\n[20] Y. Liu, J. Lee, M. Park, S. Kim, E. Yang, S. J. Hwang,\nand Y. Yang. Learning to propagate labels: Transduc-\ntive propagation network for few-shot learning. arXiv\npreprint arXiv:1805.10002, 2018.\n[21] P. Mangla, N. Kumari, A. Sinha, M. Singh, B. Krish-\nnamurthy, and V. N. Balasubramanian. Charting the\nright manifold: Manifold mixup for few-shot learning.\nIn The IEEE Winter Conference on Applications of\nComputer Vision, pages 2218–2227, 2020.\n[22] T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka.\nMetric learning for large scale image classiﬁcation:\nGeneralizing to new classes at near-zero cost. In Euro-\npean Conference on Computer Vision, pages 488–501.\nSpringer, 2012.\n[23] S. Ravi and H. Larochelle. Optimization as a model\nfor few-shot learning. 2016.\n[24] M. Ren, E. Triantaﬁllou, S. Ravi, J. Snell, K. Swer-\nsky, J. B. Tenenbaum, H. Larochelle, and R. S. Zemel.\nMeta-learning for semi-supervised few-shot classiﬁca-\ntion. arXiv preprint arXiv:1803.00676, 2018.\n[25] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bern-\nstein, et al. Imagenet large scale visual recognition\nchallenge. International journal of computer vision,\n115(3):211–252, 2015.\n[26] A. A. Rusu, D. Rao, J. Sygnowski, O. Vinyals, R. Pas-\ncanu, S. Osindero, and R. Hadsell.\nMeta-learning\nwith latent embedding optimization. arXiv preprint\narXiv:1807.05960, 2018.\n[27] C. Simon, P. Koniusz, R. Nock, and M. Harandi. Adap-\ntive subspaces for few-shot learning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 4136–4145, 2020.\n[28] J. Snell, K. Swersky, and R. Zemel. Prototypical net-\nworks for few-shot learning. In Advances in Neural In-\nformation Processing Systems, pages 4077–4087, 2017.\n[29] J. Solomon, F. De Goes, G. Peyr´e, M. Cuturi,\nA. Butscher, A. Nguyen, T. Du, and L. Guibas. Convo-\nlutional wasserstein distances: Eﬃcient optimal trans-\nportation on geometric domains. ACM Transactions\non Graphics (TOG), 34(4):1–11, 2015.\n",
    "Running heading title breaks the line\n[30] S. Thrun and L. Pratt. Learning to learn. Springer\nScience & Business Media, 2012.\n[31] L. Torrey and J. Shavlik. Transfer learning. In Hand-\nbook of research on machine learning applications and\ntrends: algorithms, methods, and techniques, pages\n242–264. IGI Global, 2010.\n[32] J. W. Tukey.\nExploratory data analysis, volume 2.\nReading, Mass., 1977.\n[33] S. Vallender. Calculation of the wasserstein distance\nbetween probability distributions on the line. Theory\nof Probability & Its Applications, 18(4):784–786, 1974.\n[34] V.\nVerma,\nA.\nLamb,\nC.\nBeckham,\nA.\nNajaﬁ,\nI. Mitliagkas, A. Courville, D. Lopez-Paz, and Y. Ben-\ngio. Manifold mixup: Better representations by interpo-\nlating hidden states. arXiv preprint arXiv:1806.05236,\n2018.\n[35] C. Villani. Optimal transport: old and new, volume\n338. Springer Science & Business Media, 2008.\n[36] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al.\nMatching networks for one shot learning. In Advances\nin neural information processing systems, pages 3630–\n3638, 2016.\n[37] C. Wah, S. Branson, P. Welinder, P. Perona, and\nS. Belongie. The caltech-ucsd birds-200-2011 dataset.\n2011.\n[38] Y. Wang, W.-L. Chao, K. Q. Weinberger, and\nL. van der Maaten. Simpleshot: Revisiting nearest-\nneighbor classiﬁcation for few-shot learning. arXiv\npreprint arXiv:1911.04623, 2019.\n[39] H.-J. Ye, H. Hu, D.-C. Zhan, and F. Sha. Learning\nembedding adaptation for few-shot learning. arXiv\npreprint arXiv:1812.03664, 2018.\n[40] S. Zagoruyko and N. Komodakis. Wide residual net-\nworks. arXiv preprint arXiv:1605.07146, 2016.\n[41] H. Zhang, J. Zhang, and P. Koniusz. Few-shot learning\nvia saliency-guided hallucination of samples. In Pro-\nceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 2770–2779, 2019.\n",
    "Yuqing Hu, Vincent Gripon, St´ephane Pateux\nSupplementary Materials\n6\nADDITIONAL EXPERIMENTS\nIn this section, we provide additional experiments and results on our proposed method, including a combination of\nmulti-backbones in terms of features. We demonstrate that with PT-MAP, a direct concatenation of diﬀerent backbone\nfeatures can increase the performance.\n6.1\nEﬀect of PT-MAP on pre-trained backones\nIn the paper we trained the diﬀerent backbones following [21]. To evaluate the generosity of our proposed method, here we\ntested the performance of PT-MAP based on a set of pre-trained backbones [38] that follow a diﬀerent training procedure.\nAs in Table 7, we can see that our method is still able to bring a large accuracy increase on all backbones, no matter what\ntheir training procedure is. Therefore, this proves the generosity of PT-MAP, which can be applied in various applications.\nTable 7: 1-shot and 5-shot accuracy (dataset: miniImagenet) on baseline and our proposed PT-MAP.\nBaseline\nPT-MAP\nBackbone\n1-shot\n5-shot\n1-shot\n5-shot\nConv4\n33.17 ± 0.17%\n63.25 ± 0.17%\n58.18 ± 0.28%\n70.79 ± 0.18%\nMobilenet\n55.70 ± 0.20%\n77.46 ± 0.15%\n73.58 ± 0.29%\n82.81 ± 0.15%\nResNet10\n54.45 ± 0.21%\n76.98 ± 0.15%\n74.91 ± 0.29%\n83.73 ± 0.15%\nResNet18\n56.06 ± 0.20%\n78.63 ± 0.15%\n77.28 ± 0.28%\n85.13 ± 0.14%\nWRN\n57.26 ± 0.21%\n78.99 ± 0.14%\n78.86 ± 0.28%\n86.17 ± 0.14%\nDenseNet121\n57.81 ± 0.21%\n80.43 ± 0.15%\n79.98 ± 0.28%\n87.19 ± 0.13%\n6.2\nEﬀect of PT-MAP on multi-backbones\nTo further investigate the eﬀect of our proposed method on the features, we perform a direct concatenation of raw feature\nvectors extracted from multiple backbones before PT-MAP. In Table 8 we chose the feature vectors from three backbones\n(WRN, ResNet18 and ResNet12) and evaluated the performance with diﬀerent combinations. We observe that a direct\nconcatenation, depending on the backbones, can bring about 1% gain in both 1-shot and 5-shot settings.\nTable 8: 1-shot and 5-shot accuracy (datasets: miniImageNet, CUB and CIFAR-FS) on our proposed PT-MAP\nwith multi-backbones (’+’ denotes a concatenation of backbone features).\nminiImageNet\nCUB\nCIFAR-FS\nBackbone\n1-shot\n5-shot\n1-shot\n5-shot\n1-shot\n5-shot\nWRN\n82.92%\n88.82%\n91.55%\n93.99%\n87.69%\n90.68%\nRN18\n80.00%\n86.96%\n91.10%\n93.78%\n84.80%\n88.55%\nRN12\n78.47%\n85.84%\n90.96%\n93.77%\n82.45%\n87.33%\nRN18+RN12\n81.27%\n87.89%\n93.05%\n95.15%\n86.10%\n89.67%\nWRN+RN18\n83.87%\n89.64%\n93.28%\n95.27%\n88.05%\n91.18%\nWRN+RN12\n83.63%\n89.47%\n93.37%\n95.35%\n87.72%\n90.98%\nWRN+RN18+RN12\n83.79%\n89.63%\n94.04%\n95.76%\n88.15%\n91.25%\n"
  ],
  "full_text": "Leveraging the Feature Distribution\nin Transfer-based Few-Shot Learning\nYuqing Hu\nVincent Gripon\nSt´ephane Pateux\nIMT Atlantique\nOrange Labs\nIMT Atlantique\nOrange Labs\nAbstract\nFew-shot classiﬁcation is a challenging prob-\nlem due to the uncertainty caused by using\nfew labelled samples. In the past few years,\nmany methods have been proposed to solve\nfew-shot classiﬁcation, among which transfer-\nbased methods have proved to achieve the\nbest performance.\nFollowing this vein, in\nthis paper we propose a novel transfer-based\nmethod that builds on two successive steps:\n1) preprocessing the feature vectors so that\nthey become closer to Gaussian-like distri-\nbutions, and 2) leveraging this preprocess-\ning using an optimal-transport inspired algo-\nrithm (in the case of transductive settings).\nUsing standardized vision benchmarks, we\nprove the ability of the proposed methodol-\nogy to achieve state-of-the-art accuracy with\nvarious datasets, backbone architectures and\nfew-shot settings.\nThe code can be found\nat https://github.com/yhu01/PT-MAP.\n1\nIntroduction\nThanks to their outstanding performance, Deep Learn-\ning methods are widely considered for vision tasks such\nas object classiﬁcation or detection. To reach top per-\nformance, these systems are typically trained using very\nlarge labelled datasets that are representative enough\nof the inputs to be processed afterwards.\nHowever, in many applications, it is costly to acquire\nor to annotate data, resulting in the impossibility to\ncreate such large labelled datasets. In this context, it\nis challenging to optimize Deep Learning architectures\nconsidering the fact they typically are made of way\nmore parameters than the dataset contains. This is\nwhy in the past few years, few-shot learning (i.e. the\nproblem of learning with few labelled examples) has\nbecome a trending research subject in the ﬁeld. In\nmore details, there are two settings that authors often\nconsider: a) “inductive few-shot”, where only a few\nlabelled samples are available during training and pre-\ndiction is performed on each test input independently,\nand b) “transductive few-shot”, where prediction is\nperformed on a batch of (non-labelled) test inputs,\nallowing to take into account their joint distribution.\nMany works in the domain are built based on a “learn-\ning to learn” guidance, where the pipeline is to train\nan optimizer [8, 23, 30] with diﬀerent tasks of limited\ndata so that the model is able to learn generic experi-\nence for novel tasks. Namely, the model learns a set of\ninitialization parameters that are in an advantageous\nposition for the model to adapt to a new (small) dataset.\nRecently, the trend evolved towards using well-thought-\nout transfer architectures (called backbones) [31, 6]\ntrained one time on the same training data, but seen\nas a unique large dataset.\nA main problem of using feature vectors extracted us-\ning a backbone architecture is that their distribution\nis likely to be complex, as the problem the backbone\nhas been optimized for most of the time diﬀers from\nthe considered task. As such, methods that rely on\nstrong assumptions about the data distributions are\nlikely to fail in leveraging the quality of features. In\nthis paper, we tackle the problem of transfer-based few-\nshot learning with a twofold strategy: 1) preprocessing\nthe data extracted from the backbone so that it ﬁts a\nparticular distribution (i.e. Gaussian-like) and 2) lever-\naging this speciﬁc distribution thanks to a well-thought\nproposed algorithm based on maximum a posteriori\nand optimal transport (only in the case of transductive\nfew-shot). Using standardized benchmarks in the ﬁeld,\nwe demonstrate the ability of the proposed method to\nobtain state-of-the-art accuracy, for various problems\nand backbone architectures in some inductive settings\nand most transductive ones.\narXiv:2006.03806v3  [cs.LG]  26 Jan 2021\n\n\nRunning heading title breaks the line\npreprocessing\nMAP\nfϕ\nPT\nS ∪Q\nSinkhorn mapping\nInitialized cj\nCenter update\nnsteps\nPrediction\nfQ\nfS ∪fQ\nM∗\ncj\nAccuracy\nhj(k)\n˜hj(k)\nPT\nFigure 1: Illustration of the proposed method. First we extract feature vectors of all the inputs in Dnovel and\npreprocess them to obtain fS ∪fQ. Note that the Power transform (PT) has the eﬀect of mapping a skewed\nfeature distribution into a gaussian-like distribution (hj(k) denotes the histogram of feature k in class j). In\nMAP, we perform Sinkhorn mapping with class center cj initialized on fS to obtain the class allocation matrix\nM∗for fQ, and we update the class centers for the next iteration. After nsteps we evaluate the accuracy on fQ.\n2\nRelated work\nA large volume of works in few-shot classiﬁcation is\nbased on meta learning [30] methods, where the training\ndata is transformed into few-shot learning episodes\nto better ﬁt in the context of few examples. In this\nbranch, optimization based methods [30, 8, 23] train a\nwell-initialized optimizer so that it quickly adapts to\nunseen classes with a few epochs of training. Other\nworks [41, 4] utilize data augmentation techniques to\nartiﬁcially increase the size of the training datasets.\nIn the past few years, there have been a growing interest\nin transfer-based methods. The main idea consists in\ntraining feature extractors able to eﬃciently segregate\nnovel classes it never saw before. For example, in [3]\nthe authors train the backbone with a distance-based\nclassiﬁer [22] that takes into account the inter-class\ndistance. In [21], the authors utilize self-supervised\nlearning techniques [2] to co-train an extra rotation\nclassiﬁer for the output features, improving the accu-\nracy in few-shot settings. Many approaches are built\non top of a feature extractor. For instance, in [38] the\nauthors implement a nearest class mean classiﬁer to\nassociate an input with a class whose centroid is the\nclosest in terms of the ℓ2 distance. In [18] an iterative\napproach is used to adjust the class centers. In [13] the\nauthors build a graph neural neural network to gather\nthe feature information from similar samples. Transfer-\nbased techniques typically reach the best performance\non standardized benchmarks.\nAlthough many works involve feature extraction, few\nhave explored the features in terms of their distribu-\ntion [11]. Often, assumptions are made that the fea-\ntures in a class align to a certain distribution, even\nthough these assumptions are rarely experimentally\ndiscussed. In our work, we analyze the impact of the\nfeatures distributions and how they can be transformed\nfor better processing and accuracy. We also introduce a\nnew algorithm to improve the quality of the association\nbetween input features and corresponding classes in\ntypical few-shot settings.\nContributions.\nLet us highlight the main contribu-\ntions of this work. (1) We propose to preprocess the\nraw extracted features in order to make them more\naligned with Gaussian assumptions. Namely we intro-\nduce transforms of the features so that they become\nless skewed. (2) We use a wasserstein-based method\nto better align the distribution of features with that of\nthe considered classes. (3) We show that the proposed\nmethod can bring large increase in accuracy with a\nvariety of feature extractors and datasets, leading to\nstate-of-the-art results in the considered benchmarks.\n3\nMethodology\nIn this section we introduce the problem settings. We\ndiscuss the training of the feature extractors, the pre-\nprocessing steps that we apply on the trained features\nand the ﬁnal classiﬁcation algorithm. A summary of\nour proposed method is depicted in Figure 1.\n3.1\nProblem statement\nWe consider a typical few-shot learning problem. We\nare given a base dataset Dbase and a novel dataset\nDnovel such that Dbase ∩Dnovel = ∅. Dbase contains\na large number of labelled examples from K diﬀerent\nclasses. Dnovel, also referred to as a task in other works,\n\n\nYuqing Hu, Vincent Gripon, St´ephane Pateux\ncontains a small number of labelled examples (support\nset S), along with some unlabelled ones (query set Q),\nall from w new classes. Our goal is to predict the\nclass of the unlabelled examples in the query set. The\nfollowing parameters are of particular importance to\ndeﬁne such a few-shot problem: the number of classes\nin the novel dataset w (called w-way), the number of\nlabelled samples per class s (called s-shot) and the\nnumber of unlabelled samples per class q. So the novel\ndataset contains a total of w(s + q) samples, ws of\nthem being labelled, and wq of them being those to\nclassify. In the case of inductive few-shot, the prediction\nis performed independently on each one of the wq\nsamples. In the case of transductive few-shot [20, 18],\nthe prediction is performed considering all wq samples\ntogether. In the latter case, most works exploit the\ninformation that there are exactly q samples in each\nclass. We discuss this point in the experiments.\n3.2\nFeature extraction\nThe ﬁrst step is to train a neural network backbone\nmodel using only the base dataset. In this work we\nconsider multiple backbones, with various training pro-\ncedures. Once the considered backbone is trained, we\nobtain robust embeddings that should generalize well to\nnovel classes. We denote by fϕ the backbone function,\nobtained by extracting the output of the penultimate\nlayer from the considered architecture, with ϕ being\nthe trained architecture parameters. Note that im-\nportantly, in all backbone architectures used in the\nexperiments of this work, the penultimate layers are\nobtained by applying a ReLU function, so that all\nfeature components coming out of fϕ are nonnegative.\n3.3\nFeature preprocessing\nAs mentioned in Section 2, many works hypothesize,\nexplicitly or not, that the features from the same class\nare aligned with a speciﬁc distribution (often Gaussian-\nlike). But this aspect is rarely experimentally veriﬁed.\nIn fact, it is very likely that features obtained using\nthe backbone architecture are not Gaussian. Indeed,\nusually the features are obtained after applying a relu\nfunction, and exhibit a positive distribution mostly\nconcentrated around 0 (see details in the next section).\nMultiple works in the domain [38, 18] discuss the diﬀer-\nent statistical methods (e.g. normalization) to better\nﬁt the features into a model. Although these methods\nmay have provable assets for some distributions, they\ncould worsen the process if applied to an unexpected\ninput distribution. This is why we propose to prepro-\ncess the obtained feature vectors so that they better\nalign with typical distribution assumptions in the ﬁeld.\nNamely, we use a power transform as follows.\nPower transform (PT).\nDenote v = fϕ(x) ∈\n(R+)d, x ∈Dnovel as the obtained features on Dnovel.\nWe hereby perform a power transformation method,\nwhich is similar to Tukey’s Transformation Ladder [32],\non the features. We then follow a unit variance projec-\ntion, the formula is given by:\nf(v) =\n(\n(v+ϵ)β\n∥(v+ϵ)β∥2\nif β ̸= 0\nlog (v+ϵ)\n∥log (v+ϵ)∥2\nif β = 0\n,\n(1)\nwhere ϵ = 1e−6 is used to make sure that v+ϵ is strictly\npositive and β is a hyper-parameter. The rationales of\nthe preprocessing above are: (1) Power transforms have\nthe functionality of reducing the skew of a distribution,\nadjusted by β, (2) Unit variance projection scales the\nfeatures to the same area so that large variance features\ndo not predominate the others. This preprocessing step\nis often able to map data from any distribution to a\nclose-to-Gaussian distribution. We will analyse this\nability and the eﬀect of power transform in more details\nin Section 4.\nNote that β = 1 leads to almost no eﬀect. More gener-\nally, the skew of the obtained distribution changes when\nβ varies. For instance, if a raw distribution is right-\nskewed, decreasing β phases out the right skew, and\nphases into a left-skewed distribution when β becomes\nnegative. After experiments, we found that β = 0.5\ngives the most consistent results for our considered\nexperiments. More details based on our considered\nexperiments are available in Section 4.\nThis ﬁrst step of feature preprocessing can be performed\nin both inductive and transductive settings.\n3.4\nMAP\nLet us assume that the preprocessed feature distribu-\ntion for each class is Gaussian or Gaussian-like. As such,\na well-positioned class center is crucial to a good pre-\ndiction. In this section we discuss how to best estimate\nthe class centers when the number of samples is very\nlimited and classes are only partially labelled. In more\ndetails, we propose an Expectation–Maximization [7]-\nlike algorithm that will iteratively ﬁnd the Maximum\nA Posteriori (MAP) estimates of the class centers.\nWe ﬁrstly show that estimating these centers through\nMAP is similar to the minimization of Wasserstein\ndistance.\nThen, an iterative procedure based on a\nWasserstein distance estimation, using the sinkhorn\nalgorithm [5, 33, 14], is designed to estimate the optimal\ntransport from the initial distribution of the feature\nvectors to one that would correspond to the draw of\nsamples from Gaussian distributions.\nNote that in this step we consider what is called the\n“transductive” setting in many other few shot learning\n\n\nRunning heading title breaks the line\nworks [20, 18, 19, 13, 17, 9, 16, 10, 39], where we exploit\nunlabelled samples during the procedure as well as\npriors about their relative proportions.\nIn the following, we denote by fS the set of feature\nvectors corresponding to labelled inputs and by fQ\nthe set of feature vectors corresponding to unlabelled\ninputs. For a feature vector f ∈fS ∪fQ, we denote\nby ℓ(f) the corresponding label. We use 0 < i ≤wq\nto denote the index of an unlabelled sample, so that\nfQ = (fi)i, and we denote cj, 0 < j ≤w the estimated\ncenter for feature vectors corresponding to class j.\nOur algorithm consists in several steps in which we\nestimate class centers from a soft allocation matrix\nM∗, then we update the allocation matrix based on\nthe newly found class centers and iterate the process.\nIn the following paragraphs, we detail these steps.\nSinkhorn mapping.\nConsidering using MAP esti-\nmation for the class centers, and assuming a Gaussian\ndistribution for each class, we typically aim at solving:\n{ˆl(fi)}, {ˆcj}\n=\narg max{ℓ(fi)}∈C,{cj}\nQ\ni P(fi|j = ℓ(fi))\n=\narg min{ℓ(fi)}∈C,{cj}\nP\ni(fi −cℓ(fi))2,\n(2)\nwhere C represents the set of admissible labelling sets.\nLet us point out that the last term corresponds ex-\nactly to the Wasserstein distance used in the Optimal\nTransport problem formulation [5].\nTherefore, in this step we ﬁnd the class mapping matrix\nthat minimizes the Wasserstein distance. Inspired by\nthe Sinkhorn algorithm [35, 5], we deﬁne the mapping\nmatrix M∗as follows:\nM∗= Sinkhorn(L, p, q, λ)\n= arg\nmin\nM∈U(p,q)\nX\nij\nMijLij + λH(M),\n(3)\nwhere U(p, q) ∈Rwq×w\n+\nis a set of positive matrices for\nwhich the rows sum to p and the columns sum to q.\nFormally, U(p, q) can be written as:\nU(p, q) = {M ∈Rwq×w\n+\n|M1w = p, MT 1wq = q}, (4)\np denotes the distribution of the amount that each\nunlabelled example uses for class allocation, and q\ndenotes the distribution of the amount of unlabelled\nexamples allocated to each class. Therefore, U(p, q)\ncontains all the possible ways of allocating examples to\nclasses. The cost function L ∈Rwq×w in Equation (3)\nconsists of the euclidean distances between unlabelled\nexamples and class centers, hence Lij denotes the eu-\nclidean distance between example i and class center j.\nHere we assume a soft class mapping, meaning that\neach example can be “sliced” into diﬀerent classes.\nThe second term on the right of Equation (3) denotes\nthe entropy of M: H(M) = −P\nij Mij log Mij, regu-\nlarized by a hyper-parameter λ. Increasing λ would\nforce the entropy to become smaller, so that the map-\nping is less homogeneous. This term also makes the\nobjective function strictly convex [5, 29] and thus a\npractical and eﬀective computation. From lemma 2\nin [5], the result of this Sinkhorn mapping has the\ntypical form M∗= diag(u) · exp(−L/λ) · diag(v).\nIterative center estimation.\nIn this step, our aim\nis to estimate class centers. As shown in Algorithm 1,\nwe initialize cj as the average of labelled samples be-\nlonging to class j. Then cj is iteratively re-estimated.\nAt each iteration, we compute a mapping matrix M∗\non the unlabelled examples using the sinkhorn map-\nping. Along with labelled examples, we re-estimate cj\n(temporarily denoted µj) by weighted-averaging the\nfeatures with their allocated portions for class j:\nµj = g(M∗, j) =\nPwq\ni=1 M∗ijfi + P\nf∈fS,ℓ(f)=j f\ns + Pwq\ni=1 M∗ij\n. (5)\nThis formula corresponds to the minimization of Equa-\ntion (3). Note that labelled examples do not participate\nin the mapping process. Since their labels are known,\nwe instead set allocations for their belonging classes to\nbe 1 and to the others to be 0. Therefore, labelled exam-\nples have the largest possible weight when re-estimating\nthe class centers.\nProportioned center update.\nIn order to avoid\ntaking risky harsh decisions in early iterations of the\nalgorithm, we propose to proportionate the update\nof class centers using an inertia parameter. In more\ndetails, we update the center with a learning rate 0 <\nα ≤1. When α is close to 0, the update becomes very\nslow, whereas α = 1 corresponds to directly allocating\nthe newly found class centers:\ncj ←cj + α(µj −cj).\n(6)\nFinal decision.\nAfter a ﬁxed number of steps nsteps,\nthe rows of M∗are interpreted as probabilities to be-\nlong to each class. The maximal value corresponds to\nthe decision of the algorithm.\nA summary of our proposed algorithm is presented in\nAlgorithm 1. In Table 1 we summarize the main param-\neters and hyperparameters of the considered problem\nand proposed solution. The code is available at XXX.\n4\nExperiments\n4.1\nDatasets\nWe evaluate the performance of the proposed method\nusing standardized few-shot classiﬁcation datasets:\nminiImageNet [36], tieredImageNet [24], CUB [37] and\n\n\nYuqing Hu, Vincent Gripon, St´ephane Pateux\nAlgorithm 1: Proposed algorithm\nParameters\n: w, s, q, λ, α, nsteps\nInitialization : cj = 1\ns · P\nf∈fS,ℓ(f)=j f\nrepeat nsteps times:\nLij = ∥fi −cj∥2, ∀i, j\nM∗= Sinkhorn(L, p = 1wq, q = q1w, λ)\nµj = g(M∗, j)\ncj ←cj + α(µj −cj)\nend\nreturn ˆℓ(fi) = arg maxj(M∗[i, j])\nCIFAR-FS [1]. The miniImageNet dataset contains\n100 classes randomly chosen from ILSVRC- 2012 [25]\nand 600 images of size 84×84 pixels per class. It is split\ninto 64 base classes, 16 validation classes and 20 novel\nclasses. The tieredImageNet dataset is another sub-\nset of ImageNet, it consists of 34 high-level categories\nwith 608 classes in total. These categories are split into\n20 meta-training superclasses, 6 meta-validation super-\nclasses and 8 meta-test superclasses, which corresponds\nto 351 base classes, 97 validation classes and 160 novel\nclasses respectively. The CUB dataset contains 200\nclasses and has 11,788 images of size 84 × 84 pixels in\ntotal. Following [13], it is split into 100 base classes, 50\nvalidation classes and 50 novel classes. The CIFAR-\nFS dataset has 100 classes, each class contains 600\nimages of size 32 × 32 pixels. The splits of this dataset\nare the same as those in miniImageNet.\n4.2\nImplementation details\nIn order to stress the genericity of our proposed\nmethod with regards to the chosen backbone architec-\nture and training strategy, we perform experiments\nusing WRN [40], ResNet18 and ResNet12 [12],\nalong with some other pretrained backbones (e.g.\nDenseNet [15]). For each dataset we train the feature\nextractor with base classes, tune the hyperparameters\nwith validation classes and test the performance using\nnovel classes. Therefore, for each test run, w classes\nare drawn uniformly at random among novel classes.\nAmong these w classes, s labelled examples and q un-\nlabelled examples per class are uniformly drawn at\nrandom to form Dnovel. The WRN and ResNet are\ntrained following [21].\nIn the inductive setting, we\nuse our proposed Power Transform followed by a basic\nNearest Class Mean (NCM) classiﬁer. In the trans-\nductive setting, the MAP or an alternative is applied\nafter PT. In order to better segregate between feature\nvectors of corresponding classes for each task, we imple-\nment the “trans-mean-sub” [18] before MAP where we\nseparately subtract inputs by the means of labelled and\nunlabelled examples, followed by a unit hypersphere\nprojection. All our experiments are performed using\nw = 5, q = 15, s = 1 or 5. We run 10,000 random draws\nto obtain mean accuracy score and indicate conﬁdence\nscores (95%) when relevant. The tuned hyperparam-\neters for miniImageNet are β = 0.5, λ = 10, α = 0.4\nand nsteps = 30 for s = 1; β = 0.5, λ = 10, α = 0.2\nand nsteps = 20 for s = 5. Hyperparameters for other\ndatasets are detailed in the experiments below.\n4.3\nComparison with state-of-the-art\nmethods\nIn the ﬁrst experiment, we conduct our proposed\nmethod on diﬀerent benchmarks and compare the per-\nformance with other state-of-the-art solutions. The\nresults are presented in Table 2, we observe that our\nmethod with WRN as backbone reaches the state-of-\nthe-art performance for most cases in both inductive\nand transductive settings on all the benchmarks. In\nTable 3 we also implement our proposed method on\ntieredImageNet based on a pre-trained DenseNet121\nbackbone following the procedure described in [38].\nFrom these experiments we conclude that the proposed\nmethod can bring an increase of accuracy with a vari-\nety of backbones and datasets, leading to competitive\nperformance. In terms of execution time, we measured\nan average of 0.002s per run.\nPerformance on cross-domain settings.\nWe also\ntest our method in a cross-domain setting, where the\nbackbone is trained with the base classes in miniIma-\ngeNet but tested with the novel classes in CUB dataset.\nAs shown in Table 4, the proposed method gives the\nbest accuracy both in the case of 1-shot and 5-shot.\n4.4\nOther experiments\nAblation study.\nTo prove the interest of the ingre-\ndients on the proposed method in order to reach top\nperformance, we report in Tables 5 and 6 the results\nof ablation studies. In Table 5, we ﬁrst investigate\nthe impact of changing the backbone architecture. To-\ngether with previous experiments, we observe that the\nproposed method consistently achieves the best results\nfor any ﬁxed backbone architecture. We also report\nperformance in the case of inductive few-shot using a\nsimple Nearest-Class Mean (NCM) classiﬁer instead of\nthe iterative MAP procedure described in Section 3.\nWe perform another experiment where we replace the\nMAP algorithm with a standard K-Means where cen-\ntroids are initialized with the available labelled samples\nfor each class. We can observe signiﬁcant drops in ac-\ncuracy, emphasizing the interest of the proposed MAP\nprocedure to better estimate the class centers.\nIn Table 6 we show the impact of PT in the transductive\nsetting, where we can see about 6% gain for 1-shot and\n\n\nRunning heading title breaks the line\nTable 1: Important parameters and hyperparameters.\nNovel dataset parameters\nNotation\nValue\nDescription\nw\ntypically 5\nnumber of classes\ns\ntypically 1 or 5\nnumber of labelled inputs per class\nq\ntypically 15\nnumber of unlabelled inputs per class\nProposed method hyperparameters\nNotation\nRange\nDescription\nβ\n{−2, −1, −0.5, 0, 0.5, 1, 2}\ncoeﬃcient to adjust distribution skew\nλ\nλ ∈R+\nregularization coeﬃcient for sinkhorn mapping\nα\n0 < α ≤1\nlearning rate for class center updates\n4% gain for 5-shot in terms of accuracy.\nEﬀect of Power Transform.\nTo visualize the eﬀect\nof PT on the feature distributions, we depict in Figure 2\nthe distributions of an arbitrarily selected feature for 5\nrandomly selected novel classes of miniImageNet when\nusing WRN, before and after applying PT. We observe\nquite clearly how PT is able to reshape the feature\ndistributions to close-to-gaussian distributions.\nWe\nobserved similar behaviors with other datasets as well.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0\n50\n100\n150\nClass 1\nClass 2\nClass 3\nClass 4\nClass 5\n(a)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0\n20\n40\n60\n80\nClass 1\nClass 2\nClass 3\nClass 4\nClass 5\n(b)\nFigure 2: Distributions of an arbitrarily chosen feature\nfor 5 novel classes before (a) and after (b) PT.\nInﬂuence of the number of unlabelled samples.\nSmall values of q lead to settings that are closer to the\ninductive case. In order to better understand the gain\nin accuracy due to having access to more unlabelled\nsamples, we depict in Figure 4 the evolution of accuracy\nas a function of q, when w = 5 is ﬁxed. Interestingly,\nthe accuracy quickly reaches a close-to-asymptotical\nplateau, emphasizing the ability of the method to soon\nexploit available information in the task.\nImpact of class imbalance.\nIn all previous trans-\nductive experiments, we assumed a balanced number of\nunlabelled samples per class. We now consider the case\nof 2 classes, where we vary the number of unlabelled\nexamples q1 of class 1 with respect to that of class 2\n(100 −q1). In Figure 3 we depict: 1) the performance\nof the inductive version of our method (PT-NCM),\nwhich is independent of q1, 2) the performance of the\nproposed transductive method when the vector q is\nappropriately deﬁned (knowing the proportion of ele-\nments in class 1 vs. class 2), and 3) a mixed case where\nwe expect at least 30 elements in both classes but do not\nknow exactly how many (q = [30, 30]). Interestingly,\nwe observe that the transductive setting still outper-\nforms the inductive ones even when the proportion of\nelements in both classes is only approximately known.\n0\n10\n20\n30\n40\n50\n85\n90\n95\n100\nq1\nAccuracy\nPT+NCM\nPT+MAP (q = [30, 30])\nPT+MAP (q = [q1, 100 −q1])\nFigure 3: Accuracy of 2-ways classiﬁcation on miniIma-\ngenet (1-shot) with unevenly distributed query data for\neach class in diﬀerent settings, where the total number\nof query inputs remains constant (total: 100 elements).\nWhen q1 = 1, we obtain the most imbalanced case,\nwhereas q1 = 50 corresponds to a balanced case.\nHyperparameter tuning.\nIn the next experiment\nwe tune β, λ and α on the validation classes of each\ndataset, and then apply them to test our model on\n\n\nYuqing Hu, Vincent Gripon, St´ephane Pateux\nTable 2: 1-shot and 5-shot accuracy of state-of-the-art methods in the literature, compared with the proposed\nsolution. We present results using WRN as backbones for our proposed solutions.\nminiImageNet\nSetting\nMethod\nBackbone\n1-shot\n5-shot\nInductive\nBaseline++ [3]\nResNet18\n51.87 ± 0.77%\n75.68 ± 0.63%\nMAML [8]\nResNet18\n49.61 ± 0.92%\n65.72 ± 0.77%\nProtoNet [28]\nWRN\n62.60 ± 0.20%\n79.97 ± 0.14%\nMatching Networks [36]\nWRN\n64.03 ± 0.20%\n76.32 ± 0.16%\nSimpleShot [38]\nDenseNet121\n64.29 ± 0.20%\n81.50 ± 0.14%\nS2M2 R [21]\nWRN\n64.93 ± 0.18%\n83.18 ± 0.11%\nPT+NCM(ours)\nWRN\n65.35 ± 0.20%\n83.87 ± 0.13%\nTransductive\nBD-CSPN [19]\nWRN\n70.31 ± 0.93%\n81.89 ± 0.60%\nTransfer+SGC [13]\nWRN\n76.47 ± 0.23%\n85.23 ± 0.13%\nTAFSSL [18]\nDenseNet121\n77.06 ± 0.26%\n84.99 ± 0.14%\nDFMN-MCT [17]\nResNet12\n78.55 ± 0.86%\n86.03 ± 0.42%\nPT+MAP(ours)\nWRN\n82.92 ± 0.26%\n88.82 ± 0.13%\nCUB\nSetting\nMethod\nBackbone\n1-shot\n5-shot\nInductive\nBaseline++ [3]\nResNet10\n69.55 ± 0.89%\n85.17 ± 0.50%\nMAML [8]\nResNet10\n70.32 ± 0.99%\n80.93 ± 0.71%\nProtoNet [28]\nResNet18\n72.99 ± 0.88%\n86.64 ± 0.51%\nMatching Networks [36]\nResNet18\n73.49 ± 0.89%\n84.45 ± 0.58%\nS2M2 R [21]\nWRN\n80.68 ± 0.81%\n90.85 ± 0.44%\nPT+NCM(ours)\nWRN\n80.57 ± 0.20%\n91.15 ± 0.10%\nTransductive\nBD-CSPN [19]\nWRN\n87.45%\n91.74%\nTransfer+SGC [13]\nWRN\n88.35 ± 0.19%\n92.14 ± 0.10%\nPT+MAP(ours)\nWRN\n91.55 ± 0.19%\n93.99 ± 0.10%\nCIFAR-FS\nSetting\nMethod\nBackbone\n1-shot\n5-shot\nInductive\nProtoNet [28]\nConvNet64\n55.50 ± 0.70%\n72.00 ± 0.60%\nMAML [8]\nConvNet32\n58.90 ± 1.90%\n71.50 ± 1.00%\nS2M2 R [21]\nWRN\n74.81 ± 0.19%\n87.47 ± 0.13%\nPT+NCM(ours)\nWRN\n74.64 ± 0.21%\n87.64 ± 0.15%\nTransductive\nDSN-MR [27]\nResNet12\n78.00 ± 0.90%\n87.30 ± 0.60%\nTransfer+SGC [13]\nWRN\n83.90 ± 0.22%\n88.76 ± 0.15%\nPT+MAP(ours)\nWRN\n87.69 ± 0.23%\n90.68 ± 0.15%\nTable 3: 1-shot and 5-shot accuracy of state-of-the-art\nmethods on tieredImageNet.\ntieredImageNet\nMethod\nBackbone\n1-shot\n5-shot\nProtoNet [28]♭\nConvNet4\n53.31 ± 0.89%\n72.69 ± 0.74%\nLEO [26]♭\nWRN\n66.33 ± 0.05%\n81.44 ± 0.09%\nSimpleShot [38]♭\nDenseNet121\n71.32 ± 0.22%\n86.66 ± 0.15%\nPT+NCM(ours)♭\nDenseNet121\n69.96 ± 0.22%\n86.45 ± 0.15%\nDFMN-MCT [17]♯\nResNet12\n80.89 ± 0.84%\n87.30 ± 0.49%\nTAFSSL [18]♯\nDenseNet121\n84.29 ± 0.25%\n89.31 ± 0.15%\nPT+MAP(ours)♯\nDenseNet121\n85.67 ± 0.26%\n90.45 ± 0.14%\n♭: Inductive setting.\n♯: Transductive setting.\nnovel classes. We vary each hyperparamter in a certain\nrange and observe the evolution of accuracy to choose\nthe peak that corresponds to the highest prediction.\nFor example, the evolving curve for β, λ and α with\nminiImageNet are presented in Figure 4 (2) to (4). For\ncomparison purposes, we also trace the corresponding\ncurves on novel classes. We draw a dash line on the\nhyperparameter values where the accuracy on the vali-\nTable 4: 1-shot and 5-shot accuracy of state-of-the-art\nmethods when performing cross-domain classiﬁcation\n(backbone: WRN).\nMethod\n1-shot\n5-shot\nBaseline++ [3]♭\n40.44 ± 0.75%\n56.64 ± 0.72%\nManifold Mixup [34]♭\n46.21 ± 0.77%\n66.03 ± 0.71%\nS2M2 R [21]♭\n48.24 ± 0.84%\n70.44 ± 0.75%\nPT+NCM(ours)♭\n48.37 ± 0.19%\n70.22 ± 0.17%\nTransfer+SGC [13]♯\n58.63 ± 0.25%\n73.46 ± 0.17%\nPT+MAP(ours)♯\n62.49 ± 0.32%\n76.51 ± 0.18%\n♭: Inductive setting.\n♯: Transductive setting.\ndation classes peaks, meaning that this is the chosen\nvalue resulting in Table 2.\nThe following observations can be drawn from this ex-\nperiment: 1) The evolving curves on validation classes\n(red) and novel classes (blue) have generally similar\ntrend for each hyperparameter.\nIn particular, two\ncurves peak at the same β (β = 0.5) and λ (λ = 10),\n\n\nRunning heading title breaks the line\nTable 5: Accuracy of the proposed method in inductive and transductive settings, with diﬀerent backbones, and\ncomparison with K-Means and NCM baselines.\nSetting\nInductive\nTransductive\n(NCM baseline) Proposed PT+NCM\nPT+K-Means\nProposed PT+MAP\nDataset\nBackbone\n1-shot\n5-shot\n1-shot\n5-shot\n1-shot\n5-shot\nminiImageNet\nResNet12\n(49.08) 62.68 ± 0.20%\n(70.85) 81.99 ± 0.14%\n72.73 ± 0.23%\n84.05 ± 0.14%\n78.47 ± 0.28%\n85.84 ± 0.15%\nResNet18\n(47.63) 62.50 ± 0.20%\n(72.89) 82.17 ± 0.14%\n73.08 ± 0.22%\n84.67 ± 0.14%\n80.00 ± 0.27%\n86.96 ± 0.14%\nWRN\n(55.31) 65.35 ± 0.20%\n(78.33) 83.87 ± 0.13%\n76.67 ± 0.22%\n86.73 ± 0.13%\n82.92 ± 0.26%\n88.82 ± 0.13%\nCUB\nResNet12\n(61.30) 78.40 ± 0.20%\n(82.83) 91.12 ± 0.10%\n87.35 ± 0.19%\n92.31 ± 0.10%\n90.96 ± 0.20%\n93.77 ± 0.09%\nResNet18\n(58.92) 76.98 ± 0.20%\n(82.69) 90.56 ± 0.10%\n87.16 ± 0.19%\n91.97 ± 0.09%\n91.10 ± 0.20%\n93.78 ± 0.09%\nWRN\n(69.21) 80.57 ± 0.20%\n(88.33) 91.15 ± 0.10%\n88.28 ± 0.19%\n92.37 ± 0.10%\n91.55 ± 0.19%\n93.99 ± 0.10%\nCIFAR-FS\nResNet12\n(52.50) 71.02 ± 0.22%\n(74.16) 84.68 ± 0.16%\n78.39 ± 0.24%\n85.73 ± 0.16%\n82.45 ± 0.27%\n87.33 ± 0.17%\nResNet18\n(56.40) 71.41 ± 0.22%\n(78.30) 85.50 ± 0.15%\n79.95 ± 0.23%\n86.74 ± 0.16%\n84.80 ± 0.25%\n88.55 ± 0.16%\nWRN\n(68.93) 74.64 ± 0.21%\n(86.81) 87.64 ± 0.15%\n83.69 ± 0.22%\n89.19 ± 0.15%\n87.69 ± 0.23%\n90.68 ± 0.15%\nTable 6: Inﬂuence of Power Transform in the transductive setting with diﬀerent backbones on miniImageNet.\nWRN\nResNet18\nResNet12\nPT\nMAP\n1-shot\n5-shot\n1-shot\n5-shot\n1-shot\n5-shot\n75.60 ± 0.29%\n84.13 ± 0.16%\n74.48 ± 0.29%\n82.88 ± 0.17%\n72.04 ± 0.30%\n80.98 ± 0.18%\n82.92 ± 0.26%\n88.82 ± 0.13%\n80.00 ± 0.27%\n86.96 ± 0.14%\n78.47 ± 0.28%\n85.84 ± 0.15%\nmeaning that validation classes and novel classes share\nthe same β and λ that reach the highest accuracy. 2) A\nsmall λ tends to lead to a homogeneous class partition\nfor M∗, where each sample are uniformly allocated\nto w classes. Hence the sharp drop on the accuracy\nwhen λ < 5. 3) A too small α results in an insuﬃcient\nclass center update. On the contrary, the impact on a\nlarge α is relatively mild. Overall, it is interesting to\npoint out the little sensitivity of the proposed method\naccuracy with regards to hyperparameter tuning.\nWe followed this procedure to ﬁnd the tuned hyper-\nparameters for each dataset. Therefore, we obtained\nthat working with CUB leads to the the same hyper-\nparameters as miniImageNet. For tieredImageNet and\nCIFAR-FS, the best accuracy are obtained on valida-\ntion classes when β = 0.5, λ = 10, α = 0.3 for s = 1;\nβ = 0.5, λ = 10, α = 0.2 for s = 5.\n5\nConclusion\nIn this paper we introduced a new pipeline to solve the\nfew-shot classiﬁcation problem. Namely, we proposed\nto ﬁrstly preprocess the raw feature vectors to better\nalign to a Gaussian distribution and then we designed\nan optimal-transport inspired iterative algorithm to\nestimate the class centers. Our experimental results\non standard vision benchmarks reach state-of-the-art\naccuracy, with important gains in both 1-shot and 5-\nshot classiﬁcation. Moreover, the proposed method can\nbring gains with a variety of feature extractors, with few\nhyperparameters. Thus we believe that the proposed\nmethod is applicable to many practical problems.\n0\n50\n100\n150\n80\n85\n90\n(1) 5q\nAccuracy\nmini\ncub\ncifar −fs\n−2\n−1 −0.5 0\n0.5\n1\n2\n40\n60\n80\n(2) β\nAccuracy\nnovel\nval\n0\n10\n20\n30\n70\n75\n80\n85\n(3) λ\nAccuracy\nnovel\nval\n0.2\n0.4\n0.6\n0.8\n1\n82\n84\n86\n(4) α\nAccuracy\nnovel\nval\nFigure 4: (1) represents 5-way 1-shot accuracy on\nminiImagenet, CUB and CIFAR-FS (backbone: WRN)\nas a function of q. (2), (3) and (4) represent 1-shot\naccuracy on miniImageNet (backbone: WRN) as a\nfunction of β, λ and α respectively.\n\n\nYuqing Hu, Vincent Gripon, St´ephane Pateux\nReferences\n[1] L. Bertinetto, J. F. Henriques, P. H. Torr, and\nA. Vedaldi. Meta-learning with diﬀerentiable closed-\nform solvers. arXiv preprint arXiv:1805.08136, 2018.\n[2] O. Chapelle, B. Scholkopf, and A. Zien.\nSemi-\nsupervised learning (chapelle, o. et al., eds.; 2006)[book\nreviews].\nIEEE Transactions on Neural Networks,\n20(3):542–542, 2009.\n[3] W.-Y. Chen, Y.-C. Liu, Z. Kira, Y.-C. F. Wang, and\nJ.-B. Huang. A closer look at few-shot classiﬁcation,\n2019.\n[4] Z. Chen, Y. Fu, Y.-X. Wang, L. Ma, W. Liu, and\nM. Hebert. Image deformation meta-networks for one-\nshot learning. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages\n8680–8689, 2019.\n[5] M. Cuturi. Sinkhorn distances: Lightspeed computa-\ntion of optimal transport. In Advances in neural in-\nformation processing systems, pages 2292–2300, 2013.\n[6] D. Das and C. G. Lee. A two-stage approach to few-\nshot learning for image recognition. IEEE Transactions\non Image Processing, 2019.\n[7] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maxi-\nmum likelihood from incomplete data via the em algo-\nrithm. Journal of the Royal Statistical Society: Series\nB (Methodological), 39(1):1–22, 1977.\n[8] C. Finn, P. Abbeel, and S. Levine. Model-agnostic\nmeta-learning for fast adaptation of deep networks. In\nProceedings of the 34th International Conference on\nMachine Learning-Volume 70, pages 1126–1135. JMLR.\norg, 2017.\n[9] V. Garcia and J. Bruna. Few-shot learning with graph\nneural networks.\narXiv preprint arXiv:1711.04043,\n2017.\n[10] S. Gidaris and N. Komodakis. Generating classiﬁcation\nweights with gnn denoising autoencoders for few-shot\nlearning. arXiv preprint arXiv:1905.01102, 2019.\n[11] V. Gripon, G. B. Hacene, M. L¨owe, and F. Vermet.\nImproving accuracy of nonparametric transfer learning\nvia vector segmentation. In 2018 IEEE International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 2966–2970, 2018.\n[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep resid-\nual learning for image recognition. In Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016.\n[13] Y. Hu, V. Gripon, and S. Pateux. Exploiting unsuper-\nvised inputs for accurate few-shot classiﬁcation. arXiv\npreprint arXiv:2001.09849, 2020.\n[14] G. Huang, H. Larochelle, and S. Lacoste-Julien. Are\nfew-shot learning benchmarks too simple?\narXiv\npreprint arXiv:1902.08605, 2019.\n[15] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q.\nWeinberger. Densely connected convolutional networks.\nIn Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 4700–4708, 2017.\n[16] J. Kim, T. Kim, S. Kim, and C. D. Yoo. Edge-labeling\ngraph neural network for few-shot learning. In Pro-\nceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 11–20, 2019.\n[17] S. M. Kye, H. B. Lee, H. Kim, and S. J. Hwang. Trans-\nductive few-shot learning with meta-learned conﬁdence.\narXiv preprint arXiv:2002.12017, 2020.\n[18] M. Lichtenstein, P. Sattigeri, R. Feris, R. Giryes, and\nL. Karlinsky. Tafssl: Task-adaptive feature sub-space\nlearning for few-shot classiﬁcation.\narXiv preprint\narXiv:2003.06670, 2020.\n[19] J. Liu, L. Song, and Y. Qin. Prototype rectiﬁcation for\nfew-shot learning. arXiv preprint arXiv:1911.10713,\n2019.\n[20] Y. Liu, J. Lee, M. Park, S. Kim, E. Yang, S. J. Hwang,\nand Y. Yang. Learning to propagate labels: Transduc-\ntive propagation network for few-shot learning. arXiv\npreprint arXiv:1805.10002, 2018.\n[21] P. Mangla, N. Kumari, A. Sinha, M. Singh, B. Krish-\nnamurthy, and V. N. Balasubramanian. Charting the\nright manifold: Manifold mixup for few-shot learning.\nIn The IEEE Winter Conference on Applications of\nComputer Vision, pages 2218–2227, 2020.\n[22] T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka.\nMetric learning for large scale image classiﬁcation:\nGeneralizing to new classes at near-zero cost. In Euro-\npean Conference on Computer Vision, pages 488–501.\nSpringer, 2012.\n[23] S. Ravi and H. Larochelle. Optimization as a model\nfor few-shot learning. 2016.\n[24] M. Ren, E. Triantaﬁllou, S. Ravi, J. Snell, K. Swer-\nsky, J. B. Tenenbaum, H. Larochelle, and R. S. Zemel.\nMeta-learning for semi-supervised few-shot classiﬁca-\ntion. arXiv preprint arXiv:1803.00676, 2018.\n[25] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bern-\nstein, et al. Imagenet large scale visual recognition\nchallenge. International journal of computer vision,\n115(3):211–252, 2015.\n[26] A. A. Rusu, D. Rao, J. Sygnowski, O. Vinyals, R. Pas-\ncanu, S. Osindero, and R. Hadsell.\nMeta-learning\nwith latent embedding optimization. arXiv preprint\narXiv:1807.05960, 2018.\n[27] C. Simon, P. Koniusz, R. Nock, and M. Harandi. Adap-\ntive subspaces for few-shot learning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 4136–4145, 2020.\n[28] J. Snell, K. Swersky, and R. Zemel. Prototypical net-\nworks for few-shot learning. In Advances in Neural In-\nformation Processing Systems, pages 4077–4087, 2017.\n[29] J. Solomon, F. De Goes, G. Peyr´e, M. Cuturi,\nA. Butscher, A. Nguyen, T. Du, and L. Guibas. Convo-\nlutional wasserstein distances: Eﬃcient optimal trans-\nportation on geometric domains. ACM Transactions\non Graphics (TOG), 34(4):1–11, 2015.\n\n\nRunning heading title breaks the line\n[30] S. Thrun and L. Pratt. Learning to learn. Springer\nScience & Business Media, 2012.\n[31] L. Torrey and J. Shavlik. Transfer learning. In Hand-\nbook of research on machine learning applications and\ntrends: algorithms, methods, and techniques, pages\n242–264. IGI Global, 2010.\n[32] J. W. Tukey.\nExploratory data analysis, volume 2.\nReading, Mass., 1977.\n[33] S. Vallender. Calculation of the wasserstein distance\nbetween probability distributions on the line. Theory\nof Probability & Its Applications, 18(4):784–786, 1974.\n[34] V.\nVerma,\nA.\nLamb,\nC.\nBeckham,\nA.\nNajaﬁ,\nI. Mitliagkas, A. Courville, D. Lopez-Paz, and Y. Ben-\ngio. Manifold mixup: Better representations by interpo-\nlating hidden states. arXiv preprint arXiv:1806.05236,\n2018.\n[35] C. Villani. Optimal transport: old and new, volume\n338. Springer Science & Business Media, 2008.\n[36] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al.\nMatching networks for one shot learning. In Advances\nin neural information processing systems, pages 3630–\n3638, 2016.\n[37] C. Wah, S. Branson, P. Welinder, P. Perona, and\nS. Belongie. The caltech-ucsd birds-200-2011 dataset.\n2011.\n[38] Y. Wang, W.-L. Chao, K. Q. Weinberger, and\nL. van der Maaten. Simpleshot: Revisiting nearest-\nneighbor classiﬁcation for few-shot learning. arXiv\npreprint arXiv:1911.04623, 2019.\n[39] H.-J. Ye, H. Hu, D.-C. Zhan, and F. Sha. Learning\nembedding adaptation for few-shot learning. arXiv\npreprint arXiv:1812.03664, 2018.\n[40] S. Zagoruyko and N. Komodakis. Wide residual net-\nworks. arXiv preprint arXiv:1605.07146, 2016.\n[41] H. Zhang, J. Zhang, and P. Koniusz. Few-shot learning\nvia saliency-guided hallucination of samples. In Pro-\nceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 2770–2779, 2019.\n\n\nYuqing Hu, Vincent Gripon, St´ephane Pateux\nSupplementary Materials\n6\nADDITIONAL EXPERIMENTS\nIn this section, we provide additional experiments and results on our proposed method, including a combination of\nmulti-backbones in terms of features. We demonstrate that with PT-MAP, a direct concatenation of diﬀerent backbone\nfeatures can increase the performance.\n6.1\nEﬀect of PT-MAP on pre-trained backones\nIn the paper we trained the diﬀerent backbones following [21]. To evaluate the generosity of our proposed method, here we\ntested the performance of PT-MAP based on a set of pre-trained backbones [38] that follow a diﬀerent training procedure.\nAs in Table 7, we can see that our method is still able to bring a large accuracy increase on all backbones, no matter what\ntheir training procedure is. Therefore, this proves the generosity of PT-MAP, which can be applied in various applications.\nTable 7: 1-shot and 5-shot accuracy (dataset: miniImagenet) on baseline and our proposed PT-MAP.\nBaseline\nPT-MAP\nBackbone\n1-shot\n5-shot\n1-shot\n5-shot\nConv4\n33.17 ± 0.17%\n63.25 ± 0.17%\n58.18 ± 0.28%\n70.79 ± 0.18%\nMobilenet\n55.70 ± 0.20%\n77.46 ± 0.15%\n73.58 ± 0.29%\n82.81 ± 0.15%\nResNet10\n54.45 ± 0.21%\n76.98 ± 0.15%\n74.91 ± 0.29%\n83.73 ± 0.15%\nResNet18\n56.06 ± 0.20%\n78.63 ± 0.15%\n77.28 ± 0.28%\n85.13 ± 0.14%\nWRN\n57.26 ± 0.21%\n78.99 ± 0.14%\n78.86 ± 0.28%\n86.17 ± 0.14%\nDenseNet121\n57.81 ± 0.21%\n80.43 ± 0.15%\n79.98 ± 0.28%\n87.19 ± 0.13%\n6.2\nEﬀect of PT-MAP on multi-backbones\nTo further investigate the eﬀect of our proposed method on the features, we perform a direct concatenation of raw feature\nvectors extracted from multiple backbones before PT-MAP. In Table 8 we chose the feature vectors from three backbones\n(WRN, ResNet18 and ResNet12) and evaluated the performance with diﬀerent combinations. We observe that a direct\nconcatenation, depending on the backbones, can bring about 1% gain in both 1-shot and 5-shot settings.\nTable 8: 1-shot and 5-shot accuracy (datasets: miniImageNet, CUB and CIFAR-FS) on our proposed PT-MAP\nwith multi-backbones (’+’ denotes a concatenation of backbone features).\nminiImageNet\nCUB\nCIFAR-FS\nBackbone\n1-shot\n5-shot\n1-shot\n5-shot\n1-shot\n5-shot\nWRN\n82.92%\n88.82%\n91.55%\n93.99%\n87.69%\n90.68%\nRN18\n80.00%\n86.96%\n91.10%\n93.78%\n84.80%\n88.55%\nRN12\n78.47%\n85.84%\n90.96%\n93.77%\n82.45%\n87.33%\nRN18+RN12\n81.27%\n87.89%\n93.05%\n95.15%\n86.10%\n89.67%\nWRN+RN18\n83.87%\n89.64%\n93.28%\n95.27%\n88.05%\n91.18%\nWRN+RN12\n83.63%\n89.47%\n93.37%\n95.35%\n87.72%\n90.98%\nWRN+RN18+RN12\n83.79%\n89.63%\n94.04%\n95.76%\n88.15%\n91.25%\n"
}