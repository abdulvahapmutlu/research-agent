{
  "filename": "1805.08136v3.pdf",
  "num_pages": 15,
  "pages": [
    "Published as a conference paper at ICLR 2019\nMETA-LEARNING WITH\nDIFFERENTIABLE CLOSED-FORM SOLVERS\nLuca Bertinetto\nJoão Henriques\nFiveAI & University of Oxford\nUniversity of Oxford\nluca@robots.ox.ac.uk\njoao@robots.ox.ac.uk\nPhilip H.S. Torr\nAndrea Vedaldi\nFiveAI & University of Oxford\nUniversity of Oxford\nphilip.torr@eng.ox.ac.uk\nvedaldi@robots.ox.ac.uk\nABSTRACT\nAdapting deep networks to new concepts from a few examples is challenging,\ndue to the high computational requirements of standard ﬁne-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques\nfor adaptation, such as nearest neighbours or gradient descent. Nonetheless, the\nmachine learning literature contains a wealth of methods that learn non-deep models\nvery efﬁciently. In this paper, we propose to use these fast convergent methods as\nthe main adaptation mechanism for few-shot learning. The main idea is to teach\na deep network to use standard machine learning tools, such as ridge regression,\nas part of its own internal model, enabling it to quickly adapt to novel data. This\nrequires back-propagating errors through the solver steps. While normally the\ncost of the matrix operations involved in such a process would be signiﬁcant, by\nusing the Woodbury identity we can make the small number of examples work to\nour advantage. We propose both closed-form and iterative solvers, based on ridge\nregression and logistic regression components. Our methods constitute a simple\nand novel approach to the problem of few-shot learning and achieve performance\ncompetitive with or superior to the state of the art on three benchmarks.\n1\nINTRODUCTION\nHumans can efﬁciently perform fast mapping (Carey, 1978; Carey & Bartlett, 1978), i.e. learning\na new concept after a single exposure. By contrast, supervised learning algorithms — and neural\nnetworks in particular — typically need to be trained using a vast amount of data in order to generalize\nwell. This requirement is problematic, as the availability of large labelled datasets cannot always be\ntaken for granted. Labels can be costly to acquire: in drug discovery, for instance, campaign budgets\noften limits researchers to only operate with a small amount of biological data that can be used to\nform predictions about properties and activities of compounds (Altae-Tran et al., 2017). In other\ncircumstances, data itself can be scarce, as it can happen for example with the problem of classifying\nrare animal species, whose exemplars are not easy to observe. Such a scenario, in which just one or\na handful of training examples is provided, is referred to as one-shot or few-shot learning (Miller\net al., 2000; Fei-Fei et al., 2006; Lake et al., 2015; Hariharan & Girshick, 2017) and has recently seen\na tremendous surge in interest within the machine learning community (e.g.Vinyals et al. (2016);\nBertinetto et al. (2016); Ravi & Larochelle (2017); Finn et al. (2017)).\nCurrently, most methods tackling few-shot learning operate within the general paradigm of meta-\nlearning, which allows one to develop algorithms in which the process of learning can improve\nwith the number of training episodes (Thrun, 1998; Vilalta & Drissi, 2002). This can be achieved\nby distilling and transferring knowledge across episodes. In practice, for the problem of few-shot\nclassiﬁcation, meta-learning is often implemented using two “nested training loops”. The base learner\nworks at the level of individual episodes, which correspond to learning problems characterised by\nhaving only a small set of labelled training images available. The meta learner, by contrast, learns\nfrom a collection of such episodes, with the goal of improving the performance of the base learner\nacross episodes.\n1\narXiv:1805.08136v3  [cs.CV]  24 Jul 2019\n",
    "Published as a conference paper at ICLR 2019\nCNN\nΦ\nBase training-set\nBase\ntraining-set\nlabels Y\nBase test-set\n•\nCNN\nΦ\nω\nR.R.\nΛ\nCross-entropy\nLoss\nEpisode 1\nEpisode 3\nEpisode 2\n…\nW1\nX\nX’\nBase test-set\nlabels Y’\nEpisode N\nFigure 1: Diagram of the proposed method for one episode, of which several are seen during\nmeta-training. The task is to learn new classes given just a few sample images per class. In this\nillustrative example, there are 3 classes and 2 samples per class, making each episode a 3-way, 2-shot\nclassiﬁcation problem. At the base learning level, learning is accomplished by a differentiable ridge\nregression layer (R.R.), which computes episode-speciﬁc weights (referred to as wE in Section 3.1\nand as W in Section 3.2). At the meta-training level, by back-propagating errors through many of\nthese small learning problems, we train a network whose weights are shared across episodes, together\nwith the hyper-parameters of the R.R. layer. In this way, the R.R. base learner can improve its learning\ncapabilities as the number of experienced episodes increases.\nClearly, in any meta-learning algorithm, it is of paramount importance to choose the base learner\ncarefully. On one side of the spectrum, methods related to nearest-neighbours, such as learning\nsimilarity functions (Koch et al., 2015; Vinyals et al., 2016; Snell et al., 2017), are fast but rely solely\non the quality of the similarity metric, with no additional data-dependent adaptation at test-time. On\nthe other side of the spectrum, methods that optimize standard iterative learning algorithms, such as\nbackpropagating through gradient descent (Finn et al., 2017; Nichol et al., 2018) or explicitly learning\nthe learner’s update rule (Hochreiter et al., 2001; Andrychowicz et al., 2016; Ravi & Larochelle,\n2017), are slower but allow more adaptability to different problems/datasets.\nIn this paper, we take a different perspective. As base learners, we propose to adopt simple learning\nalgorithms that admit a closed-form solution such as ridge regression. Crucially, the simplicity and\ndifferentiability of these solutions allow us to backpropagate through learning problems. Moreover,\nthese algorithms are particularly suitable for use within a meta-learning framework for few-shot\nclassiﬁcation for two main reasons. First, their closed-form solution allows learning problems to be\nsolved efﬁciently. Second, in a data regime characterized by few examples of high dimensionality,\nthe Woodbury’s identity (Petersen et al., 2008, Chapter 3.2) can be used to obtain a very signiﬁcant\ngain in terms of computational speed.\nWe demonstrate the strength of our approach by performing extensive experiments on Omniglot (Lake\net al., 2015), CIFAR-100 (Krizhevsky & Hinton, 2009) (adapted to the few-shot problem) and\nminiImageNet (Vinyals et al., 2016). Our base learners are fast, simple to implement, and can achieve\nperformance that is competitive with or superior to the state of the art in terms of accuracy.\n2\nRELATED WORK\nThe topic of meta-learning gained importance in the machine learning community several decades ago,\nwith the ﬁrst examples already appearing in the eighties and early nineties (Utgoff, 1986; Schmidhuber,\n1987; Naik & Mammone, 1992; Bengio et al., 1992; Thrun & Pratt, 1998). Utgoff (1986) proposed a\nframework describing when and how it is useful to dynamically adjust the inductive bias of a learning\nalgorithm, thus implicitly “changing the ordering” of the elements of its hypothesis space (Vilalta &\nDrissi, 2002). Later, Bengio et al. (1992) interpreted the update rule of a neural network’s weights\nas a function that is learnable. Another seminal work is the one of Thrun (1996), which presents\n2\n",
    "Published as a conference paper at ICLR 2019\nthe so-called lifelong learning scenario, where a learning algorithm gradually encounters an ordered\nsequence of learning problems. Throughout this course, the learner can beneﬁt from re-using the\nknowledge accumulated during previous tasks. In later work, Thrun & Pratt (1998) stated that an\nalgorithm is learning to learn if “[...] its performance at each task improves with experience and with\nthe number of tasks”. This characterisation has been inspired by Mitchell et al. (1997)’s deﬁnition of\na learning algorithm as a computer program whose performance on a task improves with experience.\nSimilarly, Vilalta & Drissi (2002) explained meta-learning as organised in two “nested learning\nlevels”. At the base level, an algorithm is conﬁned within a limited hypothesis space while solving a\nsingle learning problem. Contrarily, the meta-level can “accrue knowledge” by spanning multiple\nproblems, so that the hypothesis space at the base level can be adapted effectively.\nArguably, the simplest approach to meta-learning is to train a similarity function by exposing it to\nmany matching problems (Bromley et al., 1993; Chopra et al., 2005; Koch et al., 2015). Despite\nits simplicity, this general strategy is particularly effective and it is at the core of several state-\nof-the-art few-shot classiﬁcation algorithms (Vinyals et al., 2016; Snell et al., 2017; Sung et al.,\n2018). Interestingly, Garcia & Bruna (2018) interpret learning as information propagation from\nsupport (training) to query (test) images and propose a graph neural network that can generalize\nmatching-based approaches. Since this line of work relies on learning a similarity metric, one\ndistinctive characteristic is that parameter updates only occur within the long time horizon of the\nouter training loop. While this can clearly spare costly computations, it also prevents these methods\nfrom performing adaptation at test time. A possible way to overcome the lack of adaptability is\nto train a neural network capable of predicting (some of) its own parameters. This technique has\nbeen ﬁrst introduced in Schmidhuber (1992; 1993) and recently revamped by Bertinetto et al. (2016)\nand Munkhdalai & Yu (2017). Rebufﬁet al. (2017) showed that a similar approach can be used to\nadapt a neural network, on the ﬂy, to entirely different visual domains.\nAnother popular approach to meta-learning is to interpret the gradient update of SGD as a parametric\nand learnable function rather than a ﬁxed ad-hoc routine. Younger et al. (2001) and Hochreiter et al.\n(2001) observed that, because of the sequential nature of a learning algorithm, a recurrent neural\nnetwork can be considered as a meta-learning system. They identify LSTMs as particularly apt\nfor the task because of their ability to span long-term dependencies, which are essential in order to\nmeta-learn. A modern take on this idea has been presented by Andrychowicz et al. (2016) and Ravi &\nLarochelle (2017), showing beneﬁts on large-scale classiﬁcation, style transfer and few-shot learning.\nA recent and promising research direction is the one set by Maclaurin et al. (2015) and by the MAML\nalgorithm (Finn et al., 2017; Finn & Levine, 2018). Instead of explicitly designing a meta-learner\nmodule for learning the update rule, they backpropagate through the very operation of gradient\ndescent to optimize for the hyperparameters or the initial parameters of the learner. However, back-\npropagation through gradient descent steps is costly in terms of memory, and thus the total number of\nsteps must be kept small.\nTo alleviate the drawback of catastrophic forgetting typical of deep neural networks (McCloskey &\nCohen, 1989), several recent methods (Santoro et al., 2016; Kaiser et al., 2017; Munkhdalai & Yu,\n2017; Sprechmann et al., 2018) make use of memory-augmented models, which can ﬁrst retain and\nthen access important and previously unseen information associated with newly encountered episodes.\nWhile such memory modules store and retrieve information in the long time range, approaches\nbased on attention like the one of Vinyals et al. (2016) are useful to specify the most relevant pieces\nof knowledge within an episode. Mishra et al. (2018) complemented soft attention with temporal\nconvolutions (Oord et al., 2016), thus allowing the attention mechanism to access information related\nto past episodes.\nIn this paper, we instead argue for simple, fast and differentiable base learners such as ridge regression.\nCompared to nearest-neighbour methods, they allow more ﬂexibility because they produce a different\nset of parameters for different episodes (Wi in Figure 1). Compared to methods that adapt SGD,\nthey exhibit an inherently fast rate of convergence, particularly in cases where a closed form solution\nexists. A similar idea has been discussed by Bengio (2000), where the analytic formulations of\nzero-gradient solutions are used to obtain meta-gradients analytically and optimize hyper-parameters.\nMore recently, Ionescu et al. (2015) and Valmadre et al. (2017) have derived backpropagation forms\nfor the SVD and Correlation Filter, so that SGD can be applied, respectively, to a deep neural network\nthat computes the solution to either an eigenvalue problem or a system of linear equations where the\ndata matrix has a circulant structure.\n3\n",
    "Published as a conference paper at ICLR 2019\n3\nMETHOD\n3.1\nMETA-LEARNING\nAccording to widely accepted deﬁnitions of learning (Mitchell, 1980) and meta-learning (Vilalta &\nDrissi, 2002; Vinyals et al., 2016), an algorithm is “learning to learn” if it can improve its learning\nskills with the number of experienced episodes (by progressively and dynamically modifying its\ninductive bias). There are two main components in a meta-learning algorithm: a base learner and a\nmeta-learner (Vilalta & Drissi, 2002). The base learner works at the level of individual episodes (or\ntasks), which in the few-shot scenario correspond to learning problems characterised by having only\na small set of labelled training images available. The meta-learner learns from several such episodes\nin sequence with the goal of improving the performance of the base learner across episodes.\nIn other words, the goal of meta-learning is to enable a base learning algorithm to adapt to new\nepisodes efﬁciently by generalizing from a set of training episodes E ∈E. E can be modelled as\na probability distribution of example inputs x ∈Rm and outputs y ∈Ro, such that we can write\n(x, y) ∼E.\nIn the case of few-shot classiﬁcation, the inputs are represented by few images belonging to different\nunseen classes, while the outputs are the (episode-speciﬁc) class labels. It is important not to confuse\nthe small sets that are used in an episode E with the super-set E (such as Omniglot or miniImageNet,\nSection 4.1) from which they are drawn.\nConsider a generic feature extractor, such as commonly used pre-trained networks 1 φ(x) : Rm →Re.\nThen, a much simpler episode-speciﬁc predictor f(φ(x); wE) : Re × Rp →Ro can be trained to\nmap input embeddings to outputs. The predictor is parameterized by a set of parameters wE ∈Rp,\nwhich are speciﬁc to the episode E.\nTo train and assess the predictor on one episode, we are given access to training samples ZE =\n{(xi, yi)} ∼E and test samples Z′\nE = {(x′\ni, y′\ni)} ∼E, sampled independently from the distribution\nE. We can then use a learning algorithm Λ to obtain the parameters wE = Λ(φ(ZE)), where\nφ(ZE) ≜{(φ(xi), yi)}. The expected quality of the trained predictor is then computed by a standard\nloss or error function L : Ro × Ro →R, which is evaluated on the test samples Z′\nE:\nq(E) =\n1\n|Z′\nE|\nX\n(x′,y′)∈Z′\nE\nL (f (φ (x′) ; wE) , y′) ,\nwith wE = Λ(φ(ZE)).\n(1)\nOther than abstracting away the complexities of the learning algorithm as Λ, eq. (1) corresponds to the\nstandard train-test protocol commonly employed in machine learning, here applied to a single episode\nE. However, simply re-training a predictor for each episode ignores potentially useful knowledge\nthat can be transferred between them. For this reason, we now take the step of parameterizing φ\nand Λ with two sets of meta-parameters, respectively ω and ρ, which can aid the training procedure.\nIn particular, ω affects the representation of the input of the base learner algorithm Λ, while ρ\ncorresponds to its hyper-parameters, which here can be learnt by the meta-learner loop instead of\nbeing manually set, as it usually happens in a standard training scenario. These meta-parameters will\naffect the generalization properties of the learned predictors. This motivates evaluating the result of\ntraining on a held-out test set Z′\nE (eq. (1)). In order to learn ω and ρ, we minimize the expected loss\non held-out test sets over all episodes E ∈E:\nmin\nω,ρ\n1\n|E| · |Z′\nE|\nX\nE∈E\nX\n(x′,y′)∈Z′\nE\nL (f (φ (x′ ; ω) ; wE) , y′) ,\nwith wE = Λ(φ(ZE ; ω) ; ρ).\n(2)\nSince eq. (2) consists of a composition of non-linear functions, we can leverage the same tools used\nsuccessfully in deep learning, namely back-propagation and stochastic gradient descent (SGD), to\noptimize it. The main obstacle is to choose a learning algorithm Λ that is amenable to optimization\nwith such tools. This means that, in practice, Λ must be quite simple.\nExamples of meta-learning algorithms. Using eq. 2, it is possible to describe several of the meta-\nlearning methods in the literature, which mostly differ for the choice of Λ. The feature extractor\nφ is typically a standard CNN, whose intermediate layers are trained jointly as ω (and thus are not\n1Note that in practice we do not use pre-trained networks, but are able to train them from scratch.\n4\n",
    "Published as a conference paper at ICLR 2019\nepisode-speciﬁc). The last layer represents the linear predictor f, with episode-speciﬁc parameters\nwE. In Siamese networks (Bromley et al., 1993; Chopra et al., 2005; Koch et al., 2015), f is a nearest\nneighbour classiﬁer, which becomes soft k-means in the semi-supervised setting proposed by Ren\net al. (2018). Ravi & Larochelle (2017) and Andrychowicz et al. (2016) used an LSTM to implement\nΛ, while the Learnet (Bertinetto et al., 2016) uses a factorized CNN and MAML (Finn et al., 2017)\nimplements it using SGD (and furthermore adapts all parameters of the CNN).\nInstead, we use simple and fast-converging methods as base learner Λ, namely least-squares based\nsolutions for ridge regression and logistic regression. In the outer loop, we allow SGD to learn both\nthe parameters ω of the feature representation of Λ and its hyper-parameters ρ.\n3.2\nEFFICIENT RIDGE REGRESSION BASE LEARNERS\nSimilarly to the methods discussed in Section 3.1, over the course of a single episode we adapt a\nlinear predictor f, which can be considered as the ﬁnal layer of a CNN. The remaining layers φ\nare trained from scratch (within the outer loop of meta-learning) to generalize between episodes,\nbut for the purposes of one episode they are considered ﬁxed. In this section, we assume that the\ninputs were pre-processed by the CNN φ, and that we are dealing only with the ﬁnal linear predictor\nf(φ(x)) = φ(x)W ∈Ro, where the parameters wE are reorganized into a matrix W ∈Re×o.\nThe motivation for our work is that, while not quite as simple as nearest neighbours, least-squares\nregressors admit closed-form solutions. Although simple least-squares is prone to overﬁtting, it is\neasy to augment it with L2 regularization (controlled by a positive hyper-parameter λ), in what is\nknown as ridge regression:\nΛ(Z) = arg min\nW\n∥XW −Y ∥2 + λ ∥W∥2\n(3)\n= (XT X + λI)\n−1XT Y,\n(4)\nwhere X ∈Rn×e and Y ∈Rn×o contain the n sample pairs of input embeddings and outputs from\nZ, stacked as rows.\nBecause ridge regression admits a closed form solution (eq. (4)), it is relatively easy to integrate into\nmeta-learning (eq. (2)) using standard automatic differentiation packages. The only element that\nmay have to be treated more carefully is the matrix inversion. When the matrix to invert is close to\nsingular (which we do not expect when λ > 0), it is possible to achieve more numerically accurate\nresults by replacing the matrix inverse and vector product with a linear system solver (Murphy, 2012,\n7.5.2). In our experiments, the matrices were not close to singular and we did not ﬁnd this necessary.\nAnother concern about eq. (4) is that the intermediate matrix XT X ∈Re×e grows quadratically\nwith the embedding size e. Given the high dimensionality of features typically used in deep networks,\nthe inversion could come at a very expensive cost. To alleviate this, we rely on the Woodbury\nformula (Petersen et al., 2008, Chapter 3.2), obtaining:\nW = Λ(Z) = XT (XXT + λI)\n−1Y.\n(5)\nThe main advantage of eq. (5) is that the intermediate matrix XXT ∈Rn×n now grows quadratically\nwith the number of samples in the episode, n. As we are interested in one or few-shot learning, this is\ntypically very small. The overall cost of eq. (5) is only linear in the embedding size e.\nAlthough this method was originally designed for regression, we found that it works well also in a\n(few-shot) classiﬁcation scenario, where the target outputs are one-hot vectors representing classes.\nHowever, since eq. 4 does not directly produce classiﬁcation labels, it is important to calibrate its\noutput for the cross-entropy loss, which is used to evaluate the episode’s test samples (L in eq. 2).\nThis can be done by simply adjusting our prediction X′W with a scale and a bias α, β ∈R:\nbY = αX′W + β.\n(6)\nNote that λ, α and β are hyper-parameters of the base learner Λ and can be learnt by the outer learning\nloop represented by the meta-learner, together with the CNN parameters ω.\n3.3\nITERATIVE BASE LEARNERS AND LOGISTIC REGRESSION\nIt is natural to ask whether other learning algorithms can be integrated as efﬁciently as ridge regression\nwithin our meta-learning framework. In general, a similar derivation is possible for iterative solvers,\n5\n",
    "Published as a conference paper at ICLR 2019\nas long as the operations are differentiable. For linear models with convex loss functions, a better\nchoice than gradient descent is Newton’s method, which uses curvature (second-order) information to\nreach the solution in very few steps. One learning objective of particular interest is logistic regression,\nwhich unlike ridge regression directly produces classiﬁcation labels, and thus does not require the use\nof calibration before the (binary) cross-entropy loss.\nWhen one applies Newton’s method to logistic regression, the resulting algorithm takes a familiar\nform — it consists of a series of weighted least squares (or ridge regression) problems, giving it the\nname Iteratively Reweighted Least Squares (IRLS) (Murphy, 2012, Chapter 8.3.4). Given inputs\nX ∈Rn×e and binary outputs y ∈{−1, 1}n, the i-th iteration updates the parameters wi ∈Re as:\nwi =\n\u0000XT diag(si)X + λI\n\u0001−1 XT diag(si)zi,\n(7)\nwhere I is an identity matrix, si = µi(1 −µi), zi = wT\ni−1X + (y −µi)/si, and µi = σ(wT\ni−1X)\napplies a sigmoid function σ to the predictions using the previous parameters wi−1.\nSince eq. (7) takes a similar form to ridge regression, we can use it for meta-learning in the same\nway as in section 3.2, with the difference that a small number of steps (eq. (7)) must be performed in\norder to obtain the ﬁnal parameters wE. Similarly, at each step i, we obtain a solution with a cost\nwhich is linear rather than quadratic in the embedding size by employing the Woodbury formula:\nwi = XT \u0010\nXXT + λdiag(si)−1\u0011−1\nzi,\nwhere the inner inverse has negligible cost since it is a diagonal matrix. Note that a similar strategy\ncould be followed for other learning algorithms based on IRLS, such as L1 minimization and LASSO.\nWe take logistic regression to be a sufﬁciently illustrative example, of particular interest for binary\nclassiﬁcation in one/few-shot learning, leaving the exploration of other variants for future work.\n3.4\nTRAINING POLICY\nFigure 1 illustrates our overall framework. Like most meta-learning techniques, we organize our\ntraining procedure into episodes, each of which corresponds to a few-shot classiﬁcation problem. In\nstandard classiﬁcation, training requires sampling from a distribution of images and labels. Instead,\nin our case we sample from a distribution of episodes, each containing its own training set and test\nset, with just a few samples per image. Each episode also contains two sets of labels: Y and Y ′. The\nformer is used to train the base learner, while the latter to compute the error of the just-trained base\nlearner, enabling back-propagation in order to learn ω, λ, α and β.\nIn our implementation, one episode corresponds to a mini-batch of size S = N(K + Q), where N is\nthe number of different classes (“ways”), K the number of samples per classes (“shots”) and Q the\nnumber of query (or test) images per class.\n4\nEXPERIMENTS\nIn this section, we provide practical details for the two novel methods introduced in Section 3.2\nand 3.3, which we dub R2-D2 (Ridge Regression Differentiable Discriminator) and LR-D2 (Lo-\ngistic Regression Differentiable Discriminator). We analyze their performance against the recent\nliterature on multi-class and binary classiﬁcation problems using three few-shot learning bench-\nmarks: Omniglot (Lake et al., 2015), miniImageNet (Vinyals et al., 2016) and CIFAR-FS, which\nwe introduce in this paper. The code for both our methods and the splits of CIFAR-FS are available\nat http://www.robots.ox.ac.uk/~luca/r2d2.html.\n4.1\nFEW-SHOT LEARNING BENCHMARKS\nLet I⋆and C⋆be respectively the set of images and the set of classes belonging to a certain data split\n⋆. In standard classiﬁcation datasets, Itrain ∩Itest = ∅and Ctrain = Ctest. Instead, the few-shot setup\nrequires both Imeta-train ∩Imeta-test = ∅and Cmeta-train ∩Cmeta-test = ∅, while within an episode we\nhave Ctask-train = Ctask-test.\nOmniglot (Lake et al., 2015) is a dataset of handwritten characters that has been referred to as the\n“MNIST transpose” for its high number of classes and small number of instances per class. It contains\n6\n",
    "Published as a conference paper at ICLR 2019\n20 examples of 1623 characters, grouped in 50 different alphabets. In order to be able to compare\nagainst the state of the art, we adopt the same setup and data split used in Vinyals et al. (2016).\nHence, we resize images to 28×28 and we augment the dataset using four rotated versions of the\neach instance (0°, 90°, 180°, 270°). Including rotations, we use 4800 classes for meta-training and\nmeta-validation and 1692 for meta-testing.\nminiImageNet (Vinyals et al., 2016) aims at representing a challenging dataset without demanding\nconsiderable computational resources. It is randomly sampled from ImageNet (Russakovsky et al.,\n2015) and it is constituted by a total of 60,000 images from 100 different classes, each with 600\ninstances. All images are RGB and have been downsampled to 84×84. As all recent work, we\nadopt the same splits of Ravi & Larochelle (2017), who employ 64 classes for meta-training, 16 for\nmeta-validation and 20 for meta-testing.\nCIFAR-FS. On the one hand, despite being lightweight, Omniglot is becoming too simple for\nmodern few-shot learning methods, especially with the splits of Vinyals et al. (2016). On the other,\nminiImageNet is more challenging, but it might still require a model to train for several hours before\nconvergence. Thus, we propose CIFAR-FS (CIFAR100 few-shots), which is randomly sampled from\nCIFAR-100 (Krizhevsky & Hinton, 2009) by using the same criteria with which miniImageNet has\nbeen generated. We observed that the average inter-class similarity is sufﬁciently high to represent a\nchallenge for the current state of the art. Moreover, the limited original resolution of 32×32 makes\nthe task harder and at the same time allows fast prototyping.\n4.2\nEXPERIMENTAL RESULTS\nIn order to produce the features X for the base learners (eq. 4 and 7), as many recent methods we\nuse a shallow network of four convolutional “blocks”, each consisting of the following sequence: a\n3×3 convolution (padding=1, stride=1), batch-normalization, 2×2 max-pooling, and a leaky-ReLU\nwith a factor of 0.1. Max pooling’s stride is 2 for the ﬁrst three layers and 1 for the last one. The\nfour convolutional layers have [96, 192, 384, 512] ﬁlters. Dropout is applied to the last two blocks\nfor the experiments on miniImageNet and CIFAR-FS, respectively with probabilities 0.1 and 0.4. We\ndo not use any fully connected layer. Instead, we ﬂatten and concatenate the output of the third and\nfourth convolutional blocks and feed it to the base learner. Doing so, we obtain high-dimensional\nfeatures of size 3584, 72576 and 8064 for Omniglot, miniImageNet and CIFAR-FS respectively. It is\nimportant to mention that the use of the Woodbury formula (section 3.2) allows us to make use of\nhigh-dimensional features without incurring burdensome computations. In fact, in few-shot problems\nthe data matrix X is particularly “large and short”. As an example, with a 5-way/1-shot problem\nfrom miniImageNet we have X ∈R5×72576. Applying the Woodbury identity, we obtain signiﬁcant\ngains in computation, as in eq. 5 we invert a matrix that is only 5×5 instead of 72576×72576.\nAs Snell et al. (2017), we observe that using a higher number of classes during training is important.\nHence, despite the few-shot problem at test time being 5 or 20-way, in our multi-class classiﬁcation\nexperiments we train using 60 classes for Omniglot, 16 for miniImageNet and 20 for CIFAR-FS.\nMoreover, in order not to train a different model for every single conﬁguration (two for miniImageNet\nand CIFAR-FS, four for Omniglot), similarly to (Mishra et al., 2018) and differently from previous\nwork, we train our models with a random number of shots, which does not deteriorate the performance\nand allow us to simply train one model per dataset. We then choose Q (the size of the query or test\nset) accordingly, so that the batch size S remains constant throughout the episodes. We set S to 600\nfor Omniglot and 240 for both miniImageNet and CIFAR-FS.\nAt the meta-learning level, we train our methods with Adam (Kingma & Ba, 2015) with an initial\nlearning rate of 0.005, dampened by 0.5 every 2,000 episodes. Training is stopped when the error on\nthe meta-validation set does not decrease meaningfully for 20,000 episodes.\nAs for the base learner, we let SGD learn the parameters ω of the CNN, as well as the regularization\nfactor λ and the scale α and bias β of the calibration layer of R2-D2 (end of Section 3.2). In practice,\nwe observed that it is important to use SGD to adapt α and β, while it is indifferent whether λ is\nlearnt or not. A more detailed analysis can be found in Appendix C.\nMulti-class classiﬁcation. Tables 1 and 2 show the performance of our closed-form base learner\nR2-D2 against the current state of the art for shallow architectures of four convolutional layers.\nValues represent average classiﬁcation accuracies obtained by sampling 10,000 episodes from the\n7\n",
    "Published as a conference paper at ICLR 2019\nTable 1: Few-shot multi-class classiﬁcation accuracies on miniImageNet and CIFAR-FS.\nminiImageNet, 5-way\nCIFAR-FS, 5-way\nMethod\n1-shot\n5-shot\n1-shot\n5-shot\nMATCHING NET (Vinyals et al., 2016)\n44.2%\n57%\n—\n—\nMAML (Finn et al., 2017)\n48.7±1.8%\n63.1±0.9%\n58.9±1.9%\n71.5±1.0%\nMAML ∗\n40.9±1.5%\n58.9±0.9%\n53.8±1.8%\n67.6±1.0%\nMETA-LSTM (Ravi & Larochelle, 2017)\n43.4±0.8%\n60.6±0.7%\n—\n—\nPROTO NET (Snell et al., 2017)\n47.4±0.6%\n65.4±0.5%\n55.5±0.7%\n72.0±0.6%\nPROTO NET ∗\n42.9±0.6%\n65.9±0.6%\n57.9±0.8%\n76.7±0.6%\nRELATION NET (Sung et al., 2018)\n50.4±0.8%\n65.3±0.7%\n55.0±1.0%\n69.3±0.8%\nSNAIL (with ResNet) (Mishra et al., 2018)\n55.7±1.0%\n68.9±0.9%\n—\n—\nSNAIL (with 32C) (Mishra et al., 2018)\n45.1%\n55.2%\n—\n—\nGNN (Garcia & Bruna, 2018)\n50.3%\n66.4%\n61.9%\n75.3%\nGNN∗\n50.3%\n68.2%\n56.0%\n72.5%\nOURS/R2-D2 (with 64C)\n49.5±0.2%\n65.4±0.2%\n62.3±0.2%\n77.4±0.2%\nOURS/R2-D2\n51.8±0.2%\n68.4±0.2%\n65.4±0.2%\n79.4±0.2%\nOURS/LR-D2 (1 iter.)\n51.0±0.2%\n65.6±0.2%\n64.5±0.2%\n75.8±0.2%\nOURS/LR-D2 (5 iter.)\n51.9±0.2%\n68.7±0.2%\n65.3±0.2%\n78.3±0.2%\nmeta test-set and are presented with 95% conﬁdence intervals. For each column, the best performance\nis in bold. If more than one value is outlined, it means their intervals overlap. For prototypical\nnetworks, we report the results reproduced by the code provided by the authors. For our comparison,\nwe report the results of methods which train their models from scratch for few-shot classiﬁcation,\nomitting very recent work of Qiao et al. (2018) and Gidaris & Komodakis (2018), which instead\nmake use of pre-trained embeddings.\nIn terms of feature embeddings, Vinyals et al. (2016); Finn et al. (2017); Snell et al. (2017); Ravi &\nLarochelle (2017) use 64 ﬁlters per layer (which become 32 for miniImageNet in (Ravi & Larochelle,\n2017; Finn et al., 2017) to limit overﬁtting). On top of this, Sung et al. (2018) also uses a relation\nmodule of two convolutional and two fully connected layers. GNN (Garcia & Bruna, 2018) employs\nan embedding with [64, 96, 128, 256] ﬁlters, a fully connected layer and a graph neural network (with\nits own extra parameters). In order to ensure a fair comparison, we increased the capacity of the\narchitectures of three representative methods (MAML, prototypical networks and GNN) to match\nours. The results of these experiments are reported with a ∗on Table 1. We make use of dropout on\nthe last two layers for all the experiments on baselines with ∗, as we veriﬁed it is helpful to reduce\noverﬁtting. Moreover, we report results for experiments on our R2-D2 in which we use a 64 channels\nembedding.\nDespite its simplicity, our proposed method achieves an average accuracy that, on miniImageNet\nand CIFAR-FS, is superior to the state of the art with shallow architectures. For example, on the\nfour problems of Table 1, R2-D2 improves on average of a relative 4.3% w.r.t. GNN (the second\nbest method). R2-D2 shows competitive results also on Omniglot (Table 2), achieving among the\nbest performance for all problems. Furthermore, when we use the “lighter” embedding, we can still\nobserve a performance which is in line with the state of the art. Interestingly, increasing the capacity\nof the other methods it is not particularly helpful. It is beneﬁcial only for GNN on miniImageNet and\nprototypical networks on CIFAR-FS, while being detrimental in all the other cases.\nOur R2-D2 is also competitive against SNAIL, which uses a much deeper architecture (a ResNet\nwith a total of 14 convolutional layers). Despite being outperformed for the 1-shot case, we can\nmatch its results on the 5-shot one. Moreover, it is paramount for SNAIL to make use of such deep\nembedding, as its performance drops signiﬁcantly with a shallow one.\nLR-D2 performance on multi-class classiﬁcation. In order to be able to compare our binary\nclassiﬁer LR-D2 with the state-of-the-art in few-shot N-class classiﬁcation, it is possible to jointly\nconsider N binary classiﬁers, each of which discriminates between a speciﬁc class and all the\nremaining ones (Bishop, 2006, Chapter 4.1). In our framework, this can be easily implemented\nby concatenating together the outputs of N instances of LR-D2, resulting in a single multi-class\nprediction.\n8\n",
    "Published as a conference paper at ICLR 2019\nTable 2: Few-shot multi-class classiﬁcation accuracies on Omniglot.\nOmniglot, 5-way\nOmniglot, 20-way\nMethod\n1-shot\n5-shot\n1-shot\n5-shot\nSIAMESE NET (Koch et al., 2015)\n96.7%\n98.4%\n88%\n96.5%\nMATCHING NET (Vinyals et al., 2016)\n98.1%\n98.9%\n93.8%\n98.5%\nMAML (Finn et al., 2017)\n98.7±0.4%\n99.9±0.1%\n95.8±0.3%\n98.9±0.2%\nPROTO NET (Snell et al., 2017)\n98.5±0.2%\n99.5±0.1%\n95.3±0.2%\n98.7±0.1%\nSNAIL (Mishra et al., 2018)\n99.07±0.16%\n99.77±0.09%\n97.64±0.30%\n99.36±0.18%\nGNN (Garcia & Bruna, 2018)\n99.2%\n99.7%\n97.4%\n99.0%\nOURS/R2-D2 (with 64C)\n98.55±0.05%\n99.66±0.02%\n94.70±0.05%\n98.91±0.02%\nOURS/R2-D2\n98.91±0.05%\n99.74±0.02%\n96.24±0.05%\n99.20±0.02%\nTable 3: Few-shot binary classiﬁcation accuracies on miniImageNet and CIFAR-FS.\nminiImageNet, 2-way\nCIFAR-FS, 2-way\nMethod\n1-shot\n5-shot\n1-shot\n5-shot\nMAML (Finn et al., 2017)\n74.9±3.0%\n84.4±1.2%\n82.8±2.7%\n88.3±1.1%\nPROTO NETS (Snell et al., 2017)\n71.7±1.0%\n84.8±0.7%\n76.4±0.9%\n88.5±0.6%\nRELATION NET (Sung et al., 2018)\n76.2±1.2%\n86.8±1.0%\n75.0±1.5%\n86.7±0.9%\nGNN (Garcia & Bruna, 2018)\n78.4%\n87.1%\n79.3%\n89.1%\nOURS/R2-D2\n77.4±0.3%\n86.8±0.2%\n84.1±0.3%\n91.7±0.2%\nOURS/LR-D2 (10 iter.)\n78.1±0.3%\n86.5±0.2%\n84.7±0.3%\n91.5±0.2%\nWe use the same setup and hyper-parameters of R2-D2 (Section 4), except for the number of\nclasses/ways used at training, which we limit to 10. Interestingly, with ﬁve IRLS iterations the\naccuracy of the 1-vs-all variant of LR-D2 is similar to the one of R2-D2 (Table 1): 51.9% and\n68.7% for miniImageNet (1-shot and 5-shot); 65.3% and 78.3% for CIFAR-FS. With a single iteration,\nperformance is still very competitive: 51.0% and 65.6% for miniImageNet; 64.5% and 75.8% for\nCIFAR-FS. However, the requirement of solving N binary problems per iteration makes it much less\nefﬁcient than R2-D2, as evident in Table 4.\nBinary classiﬁcation. Finally, in Table 3 we report the performance of both our ridge regression\nand logistic regression base learners, together with four representative methods. Since LR-D2 is\nlimited to operate in a binary classiﬁcation setup, we run our R2-D2 and prototypical network\nwithout oversampling the number of ways. For both methods and prototypical networks, we report\nthe performance obtained annealing the learning rate by a factor of 0.99, which works better than the\nschedule used for multi-class classiﬁcation. Moreover, motivated by the small size of the mini-batches,\nwe replace Batch Normalization with Group Normalization (Wu & He, 2018). For this table, we\nuse the default setup found in the code of MAML, which uses 5 SGD iterations during training and\n10 during testing. Table 3 conﬁrms the validity of both our approaches on the binary classiﬁcation\nproblem.\nAlthough different in nature, both MAML and our LR-D2 make use of iterative base learners:\nthe former is based on SGD, while the latter on Newton’s method (under the form of Iteratively\nReweighted Least Squares). The use of second-order optimization might suggest that LR-D2 is\ncharacterized by computationally demanding steps. However, we can apply the Woodbury identity at\nevery iteration and obtain a signiﬁcant speedup. In Figure 2 we compare the performance of LR-D2\nvs the one of MAML for a different number of steps of the base learner (kept constant between\ntraining and testing). LR-D2 is superior to MAML, especially for a higher number of steps.\nEfﬁciency. In Table 4 we compare the amount of time required by two representative methods and\nours to solve 10,000 episodes (each with 10 images) on a single NVIDIA GTX 1080 GPU. We use\nminiImageNet (5-way, 1-shot) and adopt, for the lower part of the table, a lightweight embedding\nnetwork of 4 layers and 32 channels per layer. For reference, in the upper part of the table we also\nreport the timings for R2-D2 with [64, 64, 64, 64] and [96, 192, 384, 512] embeddings.\n9\n",
    "Published as a conference paper at ICLR 2019\n0\n1\n2\n5\n10\nNum iterations\n74.0\n74.5\n75.0\n75.5\n76.0\n76.5\n77.0\n77.5\n78.0\n78.5\n79.0\nAccuracy\nminiImageNet 2-way, 1-shot\nMAML\nOurs/LR-D2\nOurs/R2-D2\n0\n1\n2\n5\n10\nNum iterations\n83.0\n83.5\n84.0\n84.5\n85.0\n85.5\n86.0\n86.5\n87.0\nAccuracy\nminiImageNet 2-way, 5-shot\nMAML\nOurs/LR-D2\nOurs/R2-D2\n0\n1\n2\n5\n10\nNum iterations\n79\n80\n81\n82\n83\n84\n85\n86\nAccuracy\nCIFAR-FS 2-way, 1-shot\nMAML\nOurs/LR-D2\nOurs/R2-D2\n0\n1\n2\n5\n10\nNum iterations\n87.0\n87.5\n88.0\n88.5\n89.0\n89.5\n90.0\n90.5\n91.0\n91.5\n92.0\nAccuracy\nCIFAR-FS 2-way, 5-shot\nMAML\nOurs/LR-D2\nOurs/R2-D2\nFigure 2: Binary classiﬁcation accuracy on two datasets and two setups at different number of steps of\nthe base learner for MAML, R2-D2 and LR-D2. Shaded areas represent 95% conﬁdence intervals.\nInterestingly, we can observe how R2-D2 allows us to achieve an efﬁciency that is comparable to\nthe one of prototypical networks and signiﬁcantly higher than MAML. Notably, unlike prototypical\nnetworks, our methods do allow per-episode adaptation through the weights W of the solver.\nTable 4: Time required to solve 10,000 miniImageNet episodes of 10 samples each.\nminiImageNet, 5-way, 1-shot\nOURS/R2-D2\n1 min 23 sec\nOURS/R2-D2 (with 64C)\n1 min 4 sec\nMAML (Finn et al., 2017) (with 32C)\n6 min 35 sec\nOURS/LR-D2 (1-vs-all) (1 iter.) (with 32C)\n5 min 48 sec\nOURS/R2-D2 (with 32C)\n57 sec\nPROTO NETS (Snell et al., 2017) (with 32C)\n24 sec\n5\nCONCLUSIONS\nWith the aim of allowing efﬁcient adaptation to unseen learning problems, in this paper we explored\nthe feasibility of incorporating fast solvers with closed-form solutions as the base learning component\nof a meta-learning system. Importantly, the use of the Woodbury identity allows signiﬁcant computa-\ntional gains in a scenario presenting only a few samples with high dimensionality, like one-shot of\nfew-shot learning. R2-D2, the differentiable ridge regression base learner we introduce, is almost as\nfast as prototypical networks and strikes a useful compromise between not performing adaptation\nfor new episodes (like metric-learning-based approaches) and conducting a costly iterative approach\n(like MAML or LSTM-based meta-learners). In general, we showed that our base learners work\nremarkably well, with excellent results on few-shot learning benchmarks, generalizing to episodes\nwith new classes that were not seen during training. We believe that our ﬁndings point in an exciting\ndirection of more sophisticated yet efﬁcient online adaptation methods, able to leverage the potential\nof prior knowledge distilled in an ofﬂine training phase. In future work, we would like to explore\nNewton’s methods with more complicated second-order structure than ridge regression.\nACKNOWLEDGMENTS\nWe would like to thank Jack Valmadre, Namhoon Lee and the anonymous reviewers for their insightful\ncomments, which have been useful to improve the manuscript. This work was partially supported by\nthe ERC grant 638009-IDIU.\n10\n",
    "Published as a conference paper at ICLR 2019\nREFERENCES\nHan Altae-Tran, Bharath Ramsundar, Aneesh S Pappu, and Vijay Pande. Low data drug discovery\nwith one-shot learning. ACS central science, 2017.\nMarcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,\nand Nando de Freitas. Learning to learn by gradient descent by gradient descent. In Advances in\nNeural Information Processing Systems, 2016.\nSamy Bengio, Yoshua Bengio, Jocelyn Cloutier, and Jan Gecsei. On the optimization of a synaptic\nlearning rule. In Preprints Conf. Optimality in Artiﬁcial and Biological Neural Networks, pp. 6–8.\nUniv. of Texas, 1992.\nYoshua Bengio. Gradient-based optimization of hyperparameters. Neural computation, 2000.\nLuca Bertinetto, João F Henriques, Jack Valmadre, Philip Torr, and Andrea Vedaldi. Learning\nfeed-forward one-shot learners. In Advances in Neural Information Processing Systems, 2016.\nChristopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and\nStatistics). Springer-Verlag, Berlin, Heidelberg, 2006.\nJane Bromley, James W Bentz, Léon Bottou, Isabelle Guyon, Yann LeCun, Cliff Moore, Eduard\nSäckinger, and Roopak Shah. Signature veriﬁcation using a “Siamese” time delay neural network.\nInternational Journal of Pattern Recognition and Artiﬁcial Intelligence, 1993.\nSusan Carey. Less may never mean more. Recent advances in the psychology of language, 1978.\nSusan Carey and Elsa Bartlett. Acquiring a single new word. 1978.\nRich Caruana. Multitask learning. In Learning to learn. Springer, 1998.\nSumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity metric discriminatively, with\napplication to face veriﬁcation. In IEEE Conference on Computer Vision and Pattern Recognition,\n2005.\nBrian Chu, Vashisht Madhavan, Oscar Beijbom, Judy Hoffman, and Trevor Darrell. Best practices\nfor ﬁne-tuning visual classiﬁers to new domains. In European Conference on Computer Vision\nworkshops. Springer, 2016.\nLi Fei-Fei, Rob Fergus, and Pietro Perona. One-shot learning of object categories. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 2006.\nChelsea Finn and Sergey Levine. Meta-learning and universality: Deep representations and gradient\ndescent can approximate any learning algorithm. 2018.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of\ndeep networks. In International Conference on Machine Learning, 2017.\nVictor Garcia and Joan Bruna. Few-shot learning with graph neural networks. In International\nConference on Learning Representations, 2018.\nSpyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In IEEE\nConference on Computer Vision and Pattern Recognition, 2018.\nBoqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic ﬂow kernel for unsupervised\ndomain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition, 2012.\nBharath Hariharan and Ross B Girshick. Low-shot visual recognition by shrinking and hallucinating\nfeatures. In IEEE International Conference on Computer Vision, 2017.\nSepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent.\nIn International Conference on Artiﬁcial Neural Networks, pp. 87–94. Springer, 2001.\nCatalin Ionescu, Orestis Vantzos, and Cristian Sminchisescu. Training deep networks with structured\nlayers by matrix backpropagation. arXiv preprint arXiv:1509.07838, 2015.\n11\n",
    "Published as a conference paper at ICLR 2019\nŁukasz Kaiser, Oﬁr Nachum, Aurko Roy, and Samy Bengio. Learning to remember rare events. In\nInternational Conference on Learning Representations, 2017.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. 2015.\nGregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot\nimage recognition. In International Conference on Machine Learning workshops, 2015.\nAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.\nBrenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning\nthrough probabilistic program induction. Science, 2015.\nDougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization\nthrough reversible learning. In International Conference on Machine Learning, pp. 2113–2122,\n2015.\nMichael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The\nsequential learning problem. In Psychology of learning and motivation. 1989.\nErik G Miller, Nicholas E Matsakis, and Paul A Viola. Learning from one example through shared\ndensities on transforms. In IEEE Conference on Computer Vision and Pattern Recognition. IEEE,\n2000.\nNikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-\nlearner. In International Conference on Learning Representations, 2018.\nTom M Mitchell. The need for biases in learning generalizations. Department of Computer Science,\nLaboratory for Computer Science Research, Rutgers Univ. New Jersey, 1980.\nTom M Mitchell et al. Machine learning. 1997. Burr Ridge, IL: McGraw Hill, 1997.\nTsendsuren Munkhdalai and Hong Yu. Meta networks. In International Conference on Machine\nLearning, 2017.\nKevin P. Murphy. Machine Learning: A Probabilistic Perspective. The MIT Press, 2012.\nDevang K Naik and RJ Mammone. Meta-neural networks that learn by learning. In Neural Networks,\n1992. IJCNN., International Joint Conference on. IEEE, 1992.\nAlex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-order meta-learning algorithms. CoRR,\n2018. URL http://arxiv.org/abs/1803.02999.\nAaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,\nNal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw\naudio. arXiv preprint arXiv:1609.03499, 2016.\nKaare Brandt Petersen, Michael Syskind Pedersen, et al. The matrix cookbook. Technical University\nof Denmark, 2008.\nSiyuan Qiao, Chenxi Liu, Wei Shen, and Alan L. Yuille. Few-shot image recognition by predicting\nparameters from activations. In IEEE Conference on Computer Vision and Pattern Recognition,\n2018.\nSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International\nConference on Learning Representations, 2017.\nSylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with\nresidual adapters. In Advances in Neural Information Processing Systems, 2017.\nMengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum,\nHugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classiﬁcation.\nIn International Conference on Learning Representations, 2018.\n12\n",
    "Published as a conference paper at ICLR 2019\nSebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint\narXiv:1706.05098, 2017.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition\nchallenge. 2015.\nAdam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-\nlearning with memory-augmented neural networks. In International Conference on Machine\nLearning, 2016.\nJürgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn:\nthe meta-meta-... hook. PhD thesis, Technische Universität München, 1987.\nJürgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent\nnetworks. Neural Computation, 1992.\nJürgen Schmidhuber. A neural network that embeds its own meta-levels. In Neural Networks, 1993.,\nIEEE International Conference on. IEEE, 1993.\nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In\nAdvances in Neural Information Processing Systems, 2017.\nPablo Sprechmann, Siddhant M Jayakumar, Jack W Rae, Alexander Pritzel, Adrià Puigdomènech\nBadia, Benigno Uria, Oriol Vinyals, Demis Hassabis, Razvan Pascanu, and Charles Blundell.\nMemory-based parameter adaptation. 2018.\nFlood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.\nLearning to compare: Relation network for few-shot learning. In IEEE Conference on Computer\nVision and Pattern Recognition, 2018.\nAlbert Tarantola. Inverse problem theory and methods for model parameter estimation, volume 89.\nsiam, 2005.\nSebastian Thrun. Is learning the n-th thing any easier than learning the ﬁrst? In Advances in Neural\nInformation Processing Systems, 1996.\nSebastian Thrun. Lifelong learning algorithms. In Learning to learn. Springer, 1998.\nSebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media, 1998.\nPaul E Utgoff. Shift of bias for inductive concept learning. Machine learning: An artiﬁcial intelligence\napproach, 1986.\nJack Valmadre, Luca Bertinetto, João Henriques, Andrea Vedaldi, and Philip HS Torr. End-to-end\nrepresentation learning for correlation ﬁlter based tracking. In IEEE Conference on Computer\nVision and Pattern Recognition, 2017.\nRicardo Vilalta and Youssef Drissi. A perspective view and survey of meta-learning. Artiﬁcial\nIntelligence Review, 2002.\nOriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one shot\nlearning. In Advances in Neural Information Processing Systems, 2016.\nYuxin Wu and Kaiming He. Group normalization. CoRR, 2018. URL http://arxiv.org/\nabs/1803.08494.\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep\nneural networks? In Advances in Neural Information Processing Systems, 2014.\nA Steven Younger, Sepp Hochreiter, and Peter R Conwell. Meta-learning with backpropagation. In\nNeural Networks, 2001. Proceedings. IJCNN’01. International Joint Conference on. IEEE, 2001.\n13\n",
    "Published as a conference paper at ICLR 2019\nA\nEXTENDED DISCUSSION\nContributions within the few-shot learning paradigm. In this work, we evaluated our proposed\nmethods R2-D2 and LR-D2 in the few-shot learning scenario (Fei-Fei et al., 2006; Lake et al., 2015;\nVinyals et al., 2016; Ravi & Larochelle, 2017; Hariharan & Girshick, 2017), which consists in\nlearning how to discriminate between images given one or very few examples. For methods tackling\nthis problem, it is common practice to organise the training procedure in two nested loops. The\ninner loop is used to solve the actual few-shot classiﬁcation problem, while the outer loop serves as\na guidance for the former by gradually modifying the inductive bias of the base learner (Vilalta &\nDrissi, 2002). Differently from standard classiﬁcation benchmarks, the few-shot ones enforce that\nclasses are disjoint between dataset splits.\nIn the literature (e.g. Vinyals et al. (2016)), the very small classiﬁcation problems with unseen classes\nsolved within the inner loop have often been referred to as episodes or tasks. Considering the general\nfew-shot learning paradigm just described, methods in the recent literature mostly differ for the type\nof learner they use in the inner loop and the amount of per-episode adaptability they allow. For\nexample, at the one end of the spectrum in terms of “amount of adaptability”, we can ﬁnd methods\nsuch as MAML Finn et al. (2017), which learns how to efﬁciently ﬁne-tune the parameters of a\nneural-network with few iterations of SGD. On the other end, we have methods based on metric\nlearning such as prototypical networks Snell et al. (2017) and relation network Sung et al. (2018),\nwhich are fast but do not perform adaptation. Note that the amount of adaptation to a new episode\n(i.e.a new classiﬁcation problem with unseen classes) is not at all indicative of the performance in\nfew-shot learning benchmarks. As a matter of fact, both Snell et al. (2017) and Sung et al. (2018)\nachieve higher accuracy than MAML. Nonetheless, adaptability is a desirable property, as it allows\nmore design ﬂexibility.\nWithin this landscape, our work proposes a novel technique (R2-D2) that does allow per-episode\nadaptation while at the same time being fast (Table 4) and achieving strong performance (Table 1).\nThe key innovation is to use a simple (and differentiable) solver such as ridge regression within the\ninner loop, which requires back-propagating through the solution of a learning problem. Crucially,\nits closed-form solution and the use of the Woodbury identity (particularly advantageous in the low\ndata regime) allow this non-trivial endeavour to be efﬁcient. We further demonstrate that this strategy\nis not limited to the ridge regression case, but it can also be extended to other solvers (LR-D2) by\ndividing the problem into a short series of weighted least squares problems ((Murphy, 2012, Chapter\n8.3.4)).\nDisambiguation from the multi-task learning paradigm. Our work – and more generally the\nfew-shot learning literature as a whole – is related to the multi-task learning paradigm (Caruana,\n1998; Ruder, 2017). However, several crucial differences exist. In terms of setup, multi-task learning\nmethods are trained to solve a ﬁxed set of T tasks (or domains). At test time, the same T tasks or\ndomains are encountered. For instance, the popular Ofﬁce-Caltech (Gong et al., 2012) dataset is\nconstructed by considering all the images from 10 classes present in 4 different datasets (the domains).\nFor multi-task learning, the splits span the domains but contain all the 10 classes. Conversely, few-shot\nlearning datasets have splits with disjoint sets of classes (i.e. each split’s classes are not contained in\nother splits). Moreover, only a few examples (shots) can be used as training data within one episode,\nwhile in multi-task learning this limitation is not present. For this reason, meta-learning methods\napplied to few-shot learning (e.g.ours, (Vinyals et al., 2016; Finn et al., 2017; Ravi & Larochelle,\n2017; Mishra et al., 2018)) crucially take into account adaptation already during the training process\nto mimic the test-time setting, de facto learning how to learn from limited data.\nThe importance of considering adaptation during training. Considering adaptation during train-\ning is also one of the main traits that differentiate our approach from basic transfer learning approaches\nin which a neural network is ﬁrst pre-trained on one dataset/task and then adapted to a different\ndataset/task by simply adapting the ﬁnal layer(s) (e.g. Yosinski et al. (2014); Chu et al. (2016)).\nTo better illustrate this point, we conducted a baseline experiment. First, we pre-trained for a standard\nclassiﬁcation problem the same 4-layers CNN architecture using the same training datasets. We\nsimply added a ﬁnal fully-connected layer (with 64 outputs, like the number of classes in the training\nsplits) and used the cross-entropy loss. Then, we used the convolutional part of this trained network\nas a feature extractor and fed its activations to our ridge-regression layer to produce a per-episode\nset of weights W. On miniImagenet, the drop in performance w.r.t. our proposed R2-D2 is very\n14\n",
    "Published as a conference paper at ICLR 2019\nsigniﬁcant: −13.8% and −11.6% accuracy for the 1 and 5 shot problems respectively. The drop in\nperformance is consistent on CIFAR, though a bit less drastic: −11.5% and −5.9%.\nThese results empirically conﬁrm that simply using basic transfer learning techniques with a shared\nfeature representation and task-speciﬁc ﬁnal layers is not a good strategy to obtain results competitive\nwith the state-of-the-art in few-shot learning. Instead, it is necessary to enforce the generality of\nthe underlying features during training explicitly, which we do by back-propagating through the\nadaptation procedure (the regressors R2-D2 and LR-D2).\nB\nDIFFERENT GAUSSIAN PRIORS FOR REGULARIZATION\nThe regularization term can be seen as a prior gaussian distribution of the parameters in a Bayesian\ninterpretation, or more simply Tikhonov regularization (Tarantola, 2005). In the most common case\nof λI, it corresponds to an isotropic gaussian prior on the parameters.\nIn addition to the case in which λ is a scalar, we also experiment with the variant diag(λ), corre-\nsponding to an axis-aligned gaussian prior with an independent variance for each parameter, which\ncan potentially exploit the fact that the parameters have different scales. Replacing λI with diag(λ)\nin 4, the ﬁnal expression for W after having applied the Woodbury identity becomes:\nW = Λ(Z) = diag(λ)−1XT (Xdiag(λ)−1XT + I)−1Y.\n(8)\nC\nBASE LEARNER HYPER-PARAMETERS\nFigure 3 illustrates the effect of using SGD to learn, together with the parameters ω of the CNN,\nalso the hyper-parameters (ρ in eq. 2) of the base learner Λ. We ﬁnd that it is very important to\nlearn the scalar α (right plot of Figure 3) used to calibrate the output of R2-D2 in eq. 6, while it is\nindifferent whether or not to learn λ. Note that, by using SGD to update α, it is possible (e.g.in the\nrange [10−3, 100]) to recover from poor initial values and suffer just a little performance loss w.r.t.\nthe optimal value of α = 10.\nThe left plot of Figure 3 also shows the performance of R2-D2 with the variant diag(λ) introduced\nin Appendix B. Unfortunately, despite this formulation allows us to make use of a more expressive\nprior, it does not improve the results compared to using a simple scalar λ. Moreover, performance\nabruptly deteriorate for λ > 0.01.\n10\n4 10\n3 10\n2 10\n1 100\n101\n102\n103\n104\n105\nInitial value of\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\nAccuracy\nCIFAR-FS 5-way, 1-shot\nFixed \nLearnt \nLearnt diag( )\n10\n3\n10\n2\n10\n1\n100\n101\n102\n103\n104\n105\nInitial value of\n61\n62\n63\n64\n65\n66\n67\nAccuracy\nCIFAR-FS 5-way, 1-shot\nFixed \nLearnt \nFigure 3: Shaded areas represent 95% conﬁdence intervals.\n15\n"
  ],
  "full_text": "Published as a conference paper at ICLR 2019\nMETA-LEARNING WITH\nDIFFERENTIABLE CLOSED-FORM SOLVERS\nLuca Bertinetto\nJoão Henriques\nFiveAI & University of Oxford\nUniversity of Oxford\nluca@robots.ox.ac.uk\njoao@robots.ox.ac.uk\nPhilip H.S. Torr\nAndrea Vedaldi\nFiveAI & University of Oxford\nUniversity of Oxford\nphilip.torr@eng.ox.ac.uk\nvedaldi@robots.ox.ac.uk\nABSTRACT\nAdapting deep networks to new concepts from a few examples is challenging,\ndue to the high computational requirements of standard ﬁne-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques\nfor adaptation, such as nearest neighbours or gradient descent. Nonetheless, the\nmachine learning literature contains a wealth of methods that learn non-deep models\nvery efﬁciently. In this paper, we propose to use these fast convergent methods as\nthe main adaptation mechanism for few-shot learning. The main idea is to teach\na deep network to use standard machine learning tools, such as ridge regression,\nas part of its own internal model, enabling it to quickly adapt to novel data. This\nrequires back-propagating errors through the solver steps. While normally the\ncost of the matrix operations involved in such a process would be signiﬁcant, by\nusing the Woodbury identity we can make the small number of examples work to\nour advantage. We propose both closed-form and iterative solvers, based on ridge\nregression and logistic regression components. Our methods constitute a simple\nand novel approach to the problem of few-shot learning and achieve performance\ncompetitive with or superior to the state of the art on three benchmarks.\n1\nINTRODUCTION\nHumans can efﬁciently perform fast mapping (Carey, 1978; Carey & Bartlett, 1978), i.e. learning\na new concept after a single exposure. By contrast, supervised learning algorithms — and neural\nnetworks in particular — typically need to be trained using a vast amount of data in order to generalize\nwell. This requirement is problematic, as the availability of large labelled datasets cannot always be\ntaken for granted. Labels can be costly to acquire: in drug discovery, for instance, campaign budgets\noften limits researchers to only operate with a small amount of biological data that can be used to\nform predictions about properties and activities of compounds (Altae-Tran et al., 2017). In other\ncircumstances, data itself can be scarce, as it can happen for example with the problem of classifying\nrare animal species, whose exemplars are not easy to observe. Such a scenario, in which just one or\na handful of training examples is provided, is referred to as one-shot or few-shot learning (Miller\net al., 2000; Fei-Fei et al., 2006; Lake et al., 2015; Hariharan & Girshick, 2017) and has recently seen\na tremendous surge in interest within the machine learning community (e.g.Vinyals et al. (2016);\nBertinetto et al. (2016); Ravi & Larochelle (2017); Finn et al. (2017)).\nCurrently, most methods tackling few-shot learning operate within the general paradigm of meta-\nlearning, which allows one to develop algorithms in which the process of learning can improve\nwith the number of training episodes (Thrun, 1998; Vilalta & Drissi, 2002). This can be achieved\nby distilling and transferring knowledge across episodes. In practice, for the problem of few-shot\nclassiﬁcation, meta-learning is often implemented using two “nested training loops”. The base learner\nworks at the level of individual episodes, which correspond to learning problems characterised by\nhaving only a small set of labelled training images available. The meta learner, by contrast, learns\nfrom a collection of such episodes, with the goal of improving the performance of the base learner\nacross episodes.\n1\narXiv:1805.08136v3  [cs.CV]  24 Jul 2019\n\n\nPublished as a conference paper at ICLR 2019\nCNN\nΦ\nBase training-set\nBase\ntraining-set\nlabels Y\nBase test-set\n•\nCNN\nΦ\nω\nR.R.\nΛ\nCross-entropy\nLoss\nEpisode 1\nEpisode 3\nEpisode 2\n…\nW1\nX\nX’\nBase test-set\nlabels Y’\nEpisode N\nFigure 1: Diagram of the proposed method for one episode, of which several are seen during\nmeta-training. The task is to learn new classes given just a few sample images per class. In this\nillustrative example, there are 3 classes and 2 samples per class, making each episode a 3-way, 2-shot\nclassiﬁcation problem. At the base learning level, learning is accomplished by a differentiable ridge\nregression layer (R.R.), which computes episode-speciﬁc weights (referred to as wE in Section 3.1\nand as W in Section 3.2). At the meta-training level, by back-propagating errors through many of\nthese small learning problems, we train a network whose weights are shared across episodes, together\nwith the hyper-parameters of the R.R. layer. In this way, the R.R. base learner can improve its learning\ncapabilities as the number of experienced episodes increases.\nClearly, in any meta-learning algorithm, it is of paramount importance to choose the base learner\ncarefully. On one side of the spectrum, methods related to nearest-neighbours, such as learning\nsimilarity functions (Koch et al., 2015; Vinyals et al., 2016; Snell et al., 2017), are fast but rely solely\non the quality of the similarity metric, with no additional data-dependent adaptation at test-time. On\nthe other side of the spectrum, methods that optimize standard iterative learning algorithms, such as\nbackpropagating through gradient descent (Finn et al., 2017; Nichol et al., 2018) or explicitly learning\nthe learner’s update rule (Hochreiter et al., 2001; Andrychowicz et al., 2016; Ravi & Larochelle,\n2017), are slower but allow more adaptability to different problems/datasets.\nIn this paper, we take a different perspective. As base learners, we propose to adopt simple learning\nalgorithms that admit a closed-form solution such as ridge regression. Crucially, the simplicity and\ndifferentiability of these solutions allow us to backpropagate through learning problems. Moreover,\nthese algorithms are particularly suitable for use within a meta-learning framework for few-shot\nclassiﬁcation for two main reasons. First, their closed-form solution allows learning problems to be\nsolved efﬁciently. Second, in a data regime characterized by few examples of high dimensionality,\nthe Woodbury’s identity (Petersen et al., 2008, Chapter 3.2) can be used to obtain a very signiﬁcant\ngain in terms of computational speed.\nWe demonstrate the strength of our approach by performing extensive experiments on Omniglot (Lake\net al., 2015), CIFAR-100 (Krizhevsky & Hinton, 2009) (adapted to the few-shot problem) and\nminiImageNet (Vinyals et al., 2016). Our base learners are fast, simple to implement, and can achieve\nperformance that is competitive with or superior to the state of the art in terms of accuracy.\n2\nRELATED WORK\nThe topic of meta-learning gained importance in the machine learning community several decades ago,\nwith the ﬁrst examples already appearing in the eighties and early nineties (Utgoff, 1986; Schmidhuber,\n1987; Naik & Mammone, 1992; Bengio et al., 1992; Thrun & Pratt, 1998). Utgoff (1986) proposed a\nframework describing when and how it is useful to dynamically adjust the inductive bias of a learning\nalgorithm, thus implicitly “changing the ordering” of the elements of its hypothesis space (Vilalta &\nDrissi, 2002). Later, Bengio et al. (1992) interpreted the update rule of a neural network’s weights\nas a function that is learnable. Another seminal work is the one of Thrun (1996), which presents\n2\n\n\nPublished as a conference paper at ICLR 2019\nthe so-called lifelong learning scenario, where a learning algorithm gradually encounters an ordered\nsequence of learning problems. Throughout this course, the learner can beneﬁt from re-using the\nknowledge accumulated during previous tasks. In later work, Thrun & Pratt (1998) stated that an\nalgorithm is learning to learn if “[...] its performance at each task improves with experience and with\nthe number of tasks”. This characterisation has been inspired by Mitchell et al. (1997)’s deﬁnition of\na learning algorithm as a computer program whose performance on a task improves with experience.\nSimilarly, Vilalta & Drissi (2002) explained meta-learning as organised in two “nested learning\nlevels”. At the base level, an algorithm is conﬁned within a limited hypothesis space while solving a\nsingle learning problem. Contrarily, the meta-level can “accrue knowledge” by spanning multiple\nproblems, so that the hypothesis space at the base level can be adapted effectively.\nArguably, the simplest approach to meta-learning is to train a similarity function by exposing it to\nmany matching problems (Bromley et al., 1993; Chopra et al., 2005; Koch et al., 2015). Despite\nits simplicity, this general strategy is particularly effective and it is at the core of several state-\nof-the-art few-shot classiﬁcation algorithms (Vinyals et al., 2016; Snell et al., 2017; Sung et al.,\n2018). Interestingly, Garcia & Bruna (2018) interpret learning as information propagation from\nsupport (training) to query (test) images and propose a graph neural network that can generalize\nmatching-based approaches. Since this line of work relies on learning a similarity metric, one\ndistinctive characteristic is that parameter updates only occur within the long time horizon of the\nouter training loop. While this can clearly spare costly computations, it also prevents these methods\nfrom performing adaptation at test time. A possible way to overcome the lack of adaptability is\nto train a neural network capable of predicting (some of) its own parameters. This technique has\nbeen ﬁrst introduced in Schmidhuber (1992; 1993) and recently revamped by Bertinetto et al. (2016)\nand Munkhdalai & Yu (2017). Rebufﬁet al. (2017) showed that a similar approach can be used to\nadapt a neural network, on the ﬂy, to entirely different visual domains.\nAnother popular approach to meta-learning is to interpret the gradient update of SGD as a parametric\nand learnable function rather than a ﬁxed ad-hoc routine. Younger et al. (2001) and Hochreiter et al.\n(2001) observed that, because of the sequential nature of a learning algorithm, a recurrent neural\nnetwork can be considered as a meta-learning system. They identify LSTMs as particularly apt\nfor the task because of their ability to span long-term dependencies, which are essential in order to\nmeta-learn. A modern take on this idea has been presented by Andrychowicz et al. (2016) and Ravi &\nLarochelle (2017), showing beneﬁts on large-scale classiﬁcation, style transfer and few-shot learning.\nA recent and promising research direction is the one set by Maclaurin et al. (2015) and by the MAML\nalgorithm (Finn et al., 2017; Finn & Levine, 2018). Instead of explicitly designing a meta-learner\nmodule for learning the update rule, they backpropagate through the very operation of gradient\ndescent to optimize for the hyperparameters or the initial parameters of the learner. However, back-\npropagation through gradient descent steps is costly in terms of memory, and thus the total number of\nsteps must be kept small.\nTo alleviate the drawback of catastrophic forgetting typical of deep neural networks (McCloskey &\nCohen, 1989), several recent methods (Santoro et al., 2016; Kaiser et al., 2017; Munkhdalai & Yu,\n2017; Sprechmann et al., 2018) make use of memory-augmented models, which can ﬁrst retain and\nthen access important and previously unseen information associated with newly encountered episodes.\nWhile such memory modules store and retrieve information in the long time range, approaches\nbased on attention like the one of Vinyals et al. (2016) are useful to specify the most relevant pieces\nof knowledge within an episode. Mishra et al. (2018) complemented soft attention with temporal\nconvolutions (Oord et al., 2016), thus allowing the attention mechanism to access information related\nto past episodes.\nIn this paper, we instead argue for simple, fast and differentiable base learners such as ridge regression.\nCompared to nearest-neighbour methods, they allow more ﬂexibility because they produce a different\nset of parameters for different episodes (Wi in Figure 1). Compared to methods that adapt SGD,\nthey exhibit an inherently fast rate of convergence, particularly in cases where a closed form solution\nexists. A similar idea has been discussed by Bengio (2000), where the analytic formulations of\nzero-gradient solutions are used to obtain meta-gradients analytically and optimize hyper-parameters.\nMore recently, Ionescu et al. (2015) and Valmadre et al. (2017) have derived backpropagation forms\nfor the SVD and Correlation Filter, so that SGD can be applied, respectively, to a deep neural network\nthat computes the solution to either an eigenvalue problem or a system of linear equations where the\ndata matrix has a circulant structure.\n3\n\n\nPublished as a conference paper at ICLR 2019\n3\nMETHOD\n3.1\nMETA-LEARNING\nAccording to widely accepted deﬁnitions of learning (Mitchell, 1980) and meta-learning (Vilalta &\nDrissi, 2002; Vinyals et al., 2016), an algorithm is “learning to learn” if it can improve its learning\nskills with the number of experienced episodes (by progressively and dynamically modifying its\ninductive bias). There are two main components in a meta-learning algorithm: a base learner and a\nmeta-learner (Vilalta & Drissi, 2002). The base learner works at the level of individual episodes (or\ntasks), which in the few-shot scenario correspond to learning problems characterised by having only\na small set of labelled training images available. The meta-learner learns from several such episodes\nin sequence with the goal of improving the performance of the base learner across episodes.\nIn other words, the goal of meta-learning is to enable a base learning algorithm to adapt to new\nepisodes efﬁciently by generalizing from a set of training episodes E ∈E. E can be modelled as\na probability distribution of example inputs x ∈Rm and outputs y ∈Ro, such that we can write\n(x, y) ∼E.\nIn the case of few-shot classiﬁcation, the inputs are represented by few images belonging to different\nunseen classes, while the outputs are the (episode-speciﬁc) class labels. It is important not to confuse\nthe small sets that are used in an episode E with the super-set E (such as Omniglot or miniImageNet,\nSection 4.1) from which they are drawn.\nConsider a generic feature extractor, such as commonly used pre-trained networks 1 φ(x) : Rm →Re.\nThen, a much simpler episode-speciﬁc predictor f(φ(x); wE) : Re × Rp →Ro can be trained to\nmap input embeddings to outputs. The predictor is parameterized by a set of parameters wE ∈Rp,\nwhich are speciﬁc to the episode E.\nTo train and assess the predictor on one episode, we are given access to training samples ZE =\n{(xi, yi)} ∼E and test samples Z′\nE = {(x′\ni, y′\ni)} ∼E, sampled independently from the distribution\nE. We can then use a learning algorithm Λ to obtain the parameters wE = Λ(φ(ZE)), where\nφ(ZE) ≜{(φ(xi), yi)}. The expected quality of the trained predictor is then computed by a standard\nloss or error function L : Ro × Ro →R, which is evaluated on the test samples Z′\nE:\nq(E) =\n1\n|Z′\nE|\nX\n(x′,y′)∈Z′\nE\nL (f (φ (x′) ; wE) , y′) ,\nwith wE = Λ(φ(ZE)).\n(1)\nOther than abstracting away the complexities of the learning algorithm as Λ, eq. (1) corresponds to the\nstandard train-test protocol commonly employed in machine learning, here applied to a single episode\nE. However, simply re-training a predictor for each episode ignores potentially useful knowledge\nthat can be transferred between them. For this reason, we now take the step of parameterizing φ\nand Λ with two sets of meta-parameters, respectively ω and ρ, which can aid the training procedure.\nIn particular, ω affects the representation of the input of the base learner algorithm Λ, while ρ\ncorresponds to its hyper-parameters, which here can be learnt by the meta-learner loop instead of\nbeing manually set, as it usually happens in a standard training scenario. These meta-parameters will\naffect the generalization properties of the learned predictors. This motivates evaluating the result of\ntraining on a held-out test set Z′\nE (eq. (1)). In order to learn ω and ρ, we minimize the expected loss\non held-out test sets over all episodes E ∈E:\nmin\nω,ρ\n1\n|E| · |Z′\nE|\nX\nE∈E\nX\n(x′,y′)∈Z′\nE\nL (f (φ (x′ ; ω) ; wE) , y′) ,\nwith wE = Λ(φ(ZE ; ω) ; ρ).\n(2)\nSince eq. (2) consists of a composition of non-linear functions, we can leverage the same tools used\nsuccessfully in deep learning, namely back-propagation and stochastic gradient descent (SGD), to\noptimize it. The main obstacle is to choose a learning algorithm Λ that is amenable to optimization\nwith such tools. This means that, in practice, Λ must be quite simple.\nExamples of meta-learning algorithms. Using eq. 2, it is possible to describe several of the meta-\nlearning methods in the literature, which mostly differ for the choice of Λ. The feature extractor\nφ is typically a standard CNN, whose intermediate layers are trained jointly as ω (and thus are not\n1Note that in practice we do not use pre-trained networks, but are able to train them from scratch.\n4\n\n\nPublished as a conference paper at ICLR 2019\nepisode-speciﬁc). The last layer represents the linear predictor f, with episode-speciﬁc parameters\nwE. In Siamese networks (Bromley et al., 1993; Chopra et al., 2005; Koch et al., 2015), f is a nearest\nneighbour classiﬁer, which becomes soft k-means in the semi-supervised setting proposed by Ren\net al. (2018). Ravi & Larochelle (2017) and Andrychowicz et al. (2016) used an LSTM to implement\nΛ, while the Learnet (Bertinetto et al., 2016) uses a factorized CNN and MAML (Finn et al., 2017)\nimplements it using SGD (and furthermore adapts all parameters of the CNN).\nInstead, we use simple and fast-converging methods as base learner Λ, namely least-squares based\nsolutions for ridge regression and logistic regression. In the outer loop, we allow SGD to learn both\nthe parameters ω of the feature representation of Λ and its hyper-parameters ρ.\n3.2\nEFFICIENT RIDGE REGRESSION BASE LEARNERS\nSimilarly to the methods discussed in Section 3.1, over the course of a single episode we adapt a\nlinear predictor f, which can be considered as the ﬁnal layer of a CNN. The remaining layers φ\nare trained from scratch (within the outer loop of meta-learning) to generalize between episodes,\nbut for the purposes of one episode they are considered ﬁxed. In this section, we assume that the\ninputs were pre-processed by the CNN φ, and that we are dealing only with the ﬁnal linear predictor\nf(φ(x)) = φ(x)W ∈Ro, where the parameters wE are reorganized into a matrix W ∈Re×o.\nThe motivation for our work is that, while not quite as simple as nearest neighbours, least-squares\nregressors admit closed-form solutions. Although simple least-squares is prone to overﬁtting, it is\neasy to augment it with L2 regularization (controlled by a positive hyper-parameter λ), in what is\nknown as ridge regression:\nΛ(Z) = arg min\nW\n∥XW −Y ∥2 + λ ∥W∥2\n(3)\n= (XT X + λI)\n−1XT Y,\n(4)\nwhere X ∈Rn×e and Y ∈Rn×o contain the n sample pairs of input embeddings and outputs from\nZ, stacked as rows.\nBecause ridge regression admits a closed form solution (eq. (4)), it is relatively easy to integrate into\nmeta-learning (eq. (2)) using standard automatic differentiation packages. The only element that\nmay have to be treated more carefully is the matrix inversion. When the matrix to invert is close to\nsingular (which we do not expect when λ > 0), it is possible to achieve more numerically accurate\nresults by replacing the matrix inverse and vector product with a linear system solver (Murphy, 2012,\n7.5.2). In our experiments, the matrices were not close to singular and we did not ﬁnd this necessary.\nAnother concern about eq. (4) is that the intermediate matrix XT X ∈Re×e grows quadratically\nwith the embedding size e. Given the high dimensionality of features typically used in deep networks,\nthe inversion could come at a very expensive cost. To alleviate this, we rely on the Woodbury\nformula (Petersen et al., 2008, Chapter 3.2), obtaining:\nW = Λ(Z) = XT (XXT + λI)\n−1Y.\n(5)\nThe main advantage of eq. (5) is that the intermediate matrix XXT ∈Rn×n now grows quadratically\nwith the number of samples in the episode, n. As we are interested in one or few-shot learning, this is\ntypically very small. The overall cost of eq. (5) is only linear in the embedding size e.\nAlthough this method was originally designed for regression, we found that it works well also in a\n(few-shot) classiﬁcation scenario, where the target outputs are one-hot vectors representing classes.\nHowever, since eq. 4 does not directly produce classiﬁcation labels, it is important to calibrate its\noutput for the cross-entropy loss, which is used to evaluate the episode’s test samples (L in eq. 2).\nThis can be done by simply adjusting our prediction X′W with a scale and a bias α, β ∈R:\nbY = αX′W + β.\n(6)\nNote that λ, α and β are hyper-parameters of the base learner Λ and can be learnt by the outer learning\nloop represented by the meta-learner, together with the CNN parameters ω.\n3.3\nITERATIVE BASE LEARNERS AND LOGISTIC REGRESSION\nIt is natural to ask whether other learning algorithms can be integrated as efﬁciently as ridge regression\nwithin our meta-learning framework. In general, a similar derivation is possible for iterative solvers,\n5\n\n\nPublished as a conference paper at ICLR 2019\nas long as the operations are differentiable. For linear models with convex loss functions, a better\nchoice than gradient descent is Newton’s method, which uses curvature (second-order) information to\nreach the solution in very few steps. One learning objective of particular interest is logistic regression,\nwhich unlike ridge regression directly produces classiﬁcation labels, and thus does not require the use\nof calibration before the (binary) cross-entropy loss.\nWhen one applies Newton’s method to logistic regression, the resulting algorithm takes a familiar\nform — it consists of a series of weighted least squares (or ridge regression) problems, giving it the\nname Iteratively Reweighted Least Squares (IRLS) (Murphy, 2012, Chapter 8.3.4). Given inputs\nX ∈Rn×e and binary outputs y ∈{−1, 1}n, the i-th iteration updates the parameters wi ∈Re as:\nwi =\n\u0000XT diag(si)X + λI\n\u0001−1 XT diag(si)zi,\n(7)\nwhere I is an identity matrix, si = µi(1 −µi), zi = wT\ni−1X + (y −µi)/si, and µi = σ(wT\ni−1X)\napplies a sigmoid function σ to the predictions using the previous parameters wi−1.\nSince eq. (7) takes a similar form to ridge regression, we can use it for meta-learning in the same\nway as in section 3.2, with the difference that a small number of steps (eq. (7)) must be performed in\norder to obtain the ﬁnal parameters wE. Similarly, at each step i, we obtain a solution with a cost\nwhich is linear rather than quadratic in the embedding size by employing the Woodbury formula:\nwi = XT \u0010\nXXT + λdiag(si)−1\u0011−1\nzi,\nwhere the inner inverse has negligible cost since it is a diagonal matrix. Note that a similar strategy\ncould be followed for other learning algorithms based on IRLS, such as L1 minimization and LASSO.\nWe take logistic regression to be a sufﬁciently illustrative example, of particular interest for binary\nclassiﬁcation in one/few-shot learning, leaving the exploration of other variants for future work.\n3.4\nTRAINING POLICY\nFigure 1 illustrates our overall framework. Like most meta-learning techniques, we organize our\ntraining procedure into episodes, each of which corresponds to a few-shot classiﬁcation problem. In\nstandard classiﬁcation, training requires sampling from a distribution of images and labels. Instead,\nin our case we sample from a distribution of episodes, each containing its own training set and test\nset, with just a few samples per image. Each episode also contains two sets of labels: Y and Y ′. The\nformer is used to train the base learner, while the latter to compute the error of the just-trained base\nlearner, enabling back-propagation in order to learn ω, λ, α and β.\nIn our implementation, one episode corresponds to a mini-batch of size S = N(K + Q), where N is\nthe number of different classes (“ways”), K the number of samples per classes (“shots”) and Q the\nnumber of query (or test) images per class.\n4\nEXPERIMENTS\nIn this section, we provide practical details for the two novel methods introduced in Section 3.2\nand 3.3, which we dub R2-D2 (Ridge Regression Differentiable Discriminator) and LR-D2 (Lo-\ngistic Regression Differentiable Discriminator). We analyze their performance against the recent\nliterature on multi-class and binary classiﬁcation problems using three few-shot learning bench-\nmarks: Omniglot (Lake et al., 2015), miniImageNet (Vinyals et al., 2016) and CIFAR-FS, which\nwe introduce in this paper. The code for both our methods and the splits of CIFAR-FS are available\nat http://www.robots.ox.ac.uk/~luca/r2d2.html.\n4.1\nFEW-SHOT LEARNING BENCHMARKS\nLet I⋆and C⋆be respectively the set of images and the set of classes belonging to a certain data split\n⋆. In standard classiﬁcation datasets, Itrain ∩Itest = ∅and Ctrain = Ctest. Instead, the few-shot setup\nrequires both Imeta-train ∩Imeta-test = ∅and Cmeta-train ∩Cmeta-test = ∅, while within an episode we\nhave Ctask-train = Ctask-test.\nOmniglot (Lake et al., 2015) is a dataset of handwritten characters that has been referred to as the\n“MNIST transpose” for its high number of classes and small number of instances per class. It contains\n6\n\n\nPublished as a conference paper at ICLR 2019\n20 examples of 1623 characters, grouped in 50 different alphabets. In order to be able to compare\nagainst the state of the art, we adopt the same setup and data split used in Vinyals et al. (2016).\nHence, we resize images to 28×28 and we augment the dataset using four rotated versions of the\neach instance (0°, 90°, 180°, 270°). Including rotations, we use 4800 classes for meta-training and\nmeta-validation and 1692 for meta-testing.\nminiImageNet (Vinyals et al., 2016) aims at representing a challenging dataset without demanding\nconsiderable computational resources. It is randomly sampled from ImageNet (Russakovsky et al.,\n2015) and it is constituted by a total of 60,000 images from 100 different classes, each with 600\ninstances. All images are RGB and have been downsampled to 84×84. As all recent work, we\nadopt the same splits of Ravi & Larochelle (2017), who employ 64 classes for meta-training, 16 for\nmeta-validation and 20 for meta-testing.\nCIFAR-FS. On the one hand, despite being lightweight, Omniglot is becoming too simple for\nmodern few-shot learning methods, especially with the splits of Vinyals et al. (2016). On the other,\nminiImageNet is more challenging, but it might still require a model to train for several hours before\nconvergence. Thus, we propose CIFAR-FS (CIFAR100 few-shots), which is randomly sampled from\nCIFAR-100 (Krizhevsky & Hinton, 2009) by using the same criteria with which miniImageNet has\nbeen generated. We observed that the average inter-class similarity is sufﬁciently high to represent a\nchallenge for the current state of the art. Moreover, the limited original resolution of 32×32 makes\nthe task harder and at the same time allows fast prototyping.\n4.2\nEXPERIMENTAL RESULTS\nIn order to produce the features X for the base learners (eq. 4 and 7), as many recent methods we\nuse a shallow network of four convolutional “blocks”, each consisting of the following sequence: a\n3×3 convolution (padding=1, stride=1), batch-normalization, 2×2 max-pooling, and a leaky-ReLU\nwith a factor of 0.1. Max pooling’s stride is 2 for the ﬁrst three layers and 1 for the last one. The\nfour convolutional layers have [96, 192, 384, 512] ﬁlters. Dropout is applied to the last two blocks\nfor the experiments on miniImageNet and CIFAR-FS, respectively with probabilities 0.1 and 0.4. We\ndo not use any fully connected layer. Instead, we ﬂatten and concatenate the output of the third and\nfourth convolutional blocks and feed it to the base learner. Doing so, we obtain high-dimensional\nfeatures of size 3584, 72576 and 8064 for Omniglot, miniImageNet and CIFAR-FS respectively. It is\nimportant to mention that the use of the Woodbury formula (section 3.2) allows us to make use of\nhigh-dimensional features without incurring burdensome computations. In fact, in few-shot problems\nthe data matrix X is particularly “large and short”. As an example, with a 5-way/1-shot problem\nfrom miniImageNet we have X ∈R5×72576. Applying the Woodbury identity, we obtain signiﬁcant\ngains in computation, as in eq. 5 we invert a matrix that is only 5×5 instead of 72576×72576.\nAs Snell et al. (2017), we observe that using a higher number of classes during training is important.\nHence, despite the few-shot problem at test time being 5 or 20-way, in our multi-class classiﬁcation\nexperiments we train using 60 classes for Omniglot, 16 for miniImageNet and 20 for CIFAR-FS.\nMoreover, in order not to train a different model for every single conﬁguration (two for miniImageNet\nand CIFAR-FS, four for Omniglot), similarly to (Mishra et al., 2018) and differently from previous\nwork, we train our models with a random number of shots, which does not deteriorate the performance\nand allow us to simply train one model per dataset. We then choose Q (the size of the query or test\nset) accordingly, so that the batch size S remains constant throughout the episodes. We set S to 600\nfor Omniglot and 240 for both miniImageNet and CIFAR-FS.\nAt the meta-learning level, we train our methods with Adam (Kingma & Ba, 2015) with an initial\nlearning rate of 0.005, dampened by 0.5 every 2,000 episodes. Training is stopped when the error on\nthe meta-validation set does not decrease meaningfully for 20,000 episodes.\nAs for the base learner, we let SGD learn the parameters ω of the CNN, as well as the regularization\nfactor λ and the scale α and bias β of the calibration layer of R2-D2 (end of Section 3.2). In practice,\nwe observed that it is important to use SGD to adapt α and β, while it is indifferent whether λ is\nlearnt or not. A more detailed analysis can be found in Appendix C.\nMulti-class classiﬁcation. Tables 1 and 2 show the performance of our closed-form base learner\nR2-D2 against the current state of the art for shallow architectures of four convolutional layers.\nValues represent average classiﬁcation accuracies obtained by sampling 10,000 episodes from the\n7\n\n\nPublished as a conference paper at ICLR 2019\nTable 1: Few-shot multi-class classiﬁcation accuracies on miniImageNet and CIFAR-FS.\nminiImageNet, 5-way\nCIFAR-FS, 5-way\nMethod\n1-shot\n5-shot\n1-shot\n5-shot\nMATCHING NET (Vinyals et al., 2016)\n44.2%\n57%\n—\n—\nMAML (Finn et al., 2017)\n48.7±1.8%\n63.1±0.9%\n58.9±1.9%\n71.5±1.0%\nMAML ∗\n40.9±1.5%\n58.9±0.9%\n53.8±1.8%\n67.6±1.0%\nMETA-LSTM (Ravi & Larochelle, 2017)\n43.4±0.8%\n60.6±0.7%\n—\n—\nPROTO NET (Snell et al., 2017)\n47.4±0.6%\n65.4±0.5%\n55.5±0.7%\n72.0±0.6%\nPROTO NET ∗\n42.9±0.6%\n65.9±0.6%\n57.9±0.8%\n76.7±0.6%\nRELATION NET (Sung et al., 2018)\n50.4±0.8%\n65.3±0.7%\n55.0±1.0%\n69.3±0.8%\nSNAIL (with ResNet) (Mishra et al., 2018)\n55.7±1.0%\n68.9±0.9%\n—\n—\nSNAIL (with 32C) (Mishra et al., 2018)\n45.1%\n55.2%\n—\n—\nGNN (Garcia & Bruna, 2018)\n50.3%\n66.4%\n61.9%\n75.3%\nGNN∗\n50.3%\n68.2%\n56.0%\n72.5%\nOURS/R2-D2 (with 64C)\n49.5±0.2%\n65.4±0.2%\n62.3±0.2%\n77.4±0.2%\nOURS/R2-D2\n51.8±0.2%\n68.4±0.2%\n65.4±0.2%\n79.4±0.2%\nOURS/LR-D2 (1 iter.)\n51.0±0.2%\n65.6±0.2%\n64.5±0.2%\n75.8±0.2%\nOURS/LR-D2 (5 iter.)\n51.9±0.2%\n68.7±0.2%\n65.3±0.2%\n78.3±0.2%\nmeta test-set and are presented with 95% conﬁdence intervals. For each column, the best performance\nis in bold. If more than one value is outlined, it means their intervals overlap. For prototypical\nnetworks, we report the results reproduced by the code provided by the authors. For our comparison,\nwe report the results of methods which train their models from scratch for few-shot classiﬁcation,\nomitting very recent work of Qiao et al. (2018) and Gidaris & Komodakis (2018), which instead\nmake use of pre-trained embeddings.\nIn terms of feature embeddings, Vinyals et al. (2016); Finn et al. (2017); Snell et al. (2017); Ravi &\nLarochelle (2017) use 64 ﬁlters per layer (which become 32 for miniImageNet in (Ravi & Larochelle,\n2017; Finn et al., 2017) to limit overﬁtting). On top of this, Sung et al. (2018) also uses a relation\nmodule of two convolutional and two fully connected layers. GNN (Garcia & Bruna, 2018) employs\nan embedding with [64, 96, 128, 256] ﬁlters, a fully connected layer and a graph neural network (with\nits own extra parameters). In order to ensure a fair comparison, we increased the capacity of the\narchitectures of three representative methods (MAML, prototypical networks and GNN) to match\nours. The results of these experiments are reported with a ∗on Table 1. We make use of dropout on\nthe last two layers for all the experiments on baselines with ∗, as we veriﬁed it is helpful to reduce\noverﬁtting. Moreover, we report results for experiments on our R2-D2 in which we use a 64 channels\nembedding.\nDespite its simplicity, our proposed method achieves an average accuracy that, on miniImageNet\nand CIFAR-FS, is superior to the state of the art with shallow architectures. For example, on the\nfour problems of Table 1, R2-D2 improves on average of a relative 4.3% w.r.t. GNN (the second\nbest method). R2-D2 shows competitive results also on Omniglot (Table 2), achieving among the\nbest performance for all problems. Furthermore, when we use the “lighter” embedding, we can still\nobserve a performance which is in line with the state of the art. Interestingly, increasing the capacity\nof the other methods it is not particularly helpful. It is beneﬁcial only for GNN on miniImageNet and\nprototypical networks on CIFAR-FS, while being detrimental in all the other cases.\nOur R2-D2 is also competitive against SNAIL, which uses a much deeper architecture (a ResNet\nwith a total of 14 convolutional layers). Despite being outperformed for the 1-shot case, we can\nmatch its results on the 5-shot one. Moreover, it is paramount for SNAIL to make use of such deep\nembedding, as its performance drops signiﬁcantly with a shallow one.\nLR-D2 performance on multi-class classiﬁcation. In order to be able to compare our binary\nclassiﬁer LR-D2 with the state-of-the-art in few-shot N-class classiﬁcation, it is possible to jointly\nconsider N binary classiﬁers, each of which discriminates between a speciﬁc class and all the\nremaining ones (Bishop, 2006, Chapter 4.1). In our framework, this can be easily implemented\nby concatenating together the outputs of N instances of LR-D2, resulting in a single multi-class\nprediction.\n8\n\n\nPublished as a conference paper at ICLR 2019\nTable 2: Few-shot multi-class classiﬁcation accuracies on Omniglot.\nOmniglot, 5-way\nOmniglot, 20-way\nMethod\n1-shot\n5-shot\n1-shot\n5-shot\nSIAMESE NET (Koch et al., 2015)\n96.7%\n98.4%\n88%\n96.5%\nMATCHING NET (Vinyals et al., 2016)\n98.1%\n98.9%\n93.8%\n98.5%\nMAML (Finn et al., 2017)\n98.7±0.4%\n99.9±0.1%\n95.8±0.3%\n98.9±0.2%\nPROTO NET (Snell et al., 2017)\n98.5±0.2%\n99.5±0.1%\n95.3±0.2%\n98.7±0.1%\nSNAIL (Mishra et al., 2018)\n99.07±0.16%\n99.77±0.09%\n97.64±0.30%\n99.36±0.18%\nGNN (Garcia & Bruna, 2018)\n99.2%\n99.7%\n97.4%\n99.0%\nOURS/R2-D2 (with 64C)\n98.55±0.05%\n99.66±0.02%\n94.70±0.05%\n98.91±0.02%\nOURS/R2-D2\n98.91±0.05%\n99.74±0.02%\n96.24±0.05%\n99.20±0.02%\nTable 3: Few-shot binary classiﬁcation accuracies on miniImageNet and CIFAR-FS.\nminiImageNet, 2-way\nCIFAR-FS, 2-way\nMethod\n1-shot\n5-shot\n1-shot\n5-shot\nMAML (Finn et al., 2017)\n74.9±3.0%\n84.4±1.2%\n82.8±2.7%\n88.3±1.1%\nPROTO NETS (Snell et al., 2017)\n71.7±1.0%\n84.8±0.7%\n76.4±0.9%\n88.5±0.6%\nRELATION NET (Sung et al., 2018)\n76.2±1.2%\n86.8±1.0%\n75.0±1.5%\n86.7±0.9%\nGNN (Garcia & Bruna, 2018)\n78.4%\n87.1%\n79.3%\n89.1%\nOURS/R2-D2\n77.4±0.3%\n86.8±0.2%\n84.1±0.3%\n91.7±0.2%\nOURS/LR-D2 (10 iter.)\n78.1±0.3%\n86.5±0.2%\n84.7±0.3%\n91.5±0.2%\nWe use the same setup and hyper-parameters of R2-D2 (Section 4), except for the number of\nclasses/ways used at training, which we limit to 10. Interestingly, with ﬁve IRLS iterations the\naccuracy of the 1-vs-all variant of LR-D2 is similar to the one of R2-D2 (Table 1): 51.9% and\n68.7% for miniImageNet (1-shot and 5-shot); 65.3% and 78.3% for CIFAR-FS. With a single iteration,\nperformance is still very competitive: 51.0% and 65.6% for miniImageNet; 64.5% and 75.8% for\nCIFAR-FS. However, the requirement of solving N binary problems per iteration makes it much less\nefﬁcient than R2-D2, as evident in Table 4.\nBinary classiﬁcation. Finally, in Table 3 we report the performance of both our ridge regression\nand logistic regression base learners, together with four representative methods. Since LR-D2 is\nlimited to operate in a binary classiﬁcation setup, we run our R2-D2 and prototypical network\nwithout oversampling the number of ways. For both methods and prototypical networks, we report\nthe performance obtained annealing the learning rate by a factor of 0.99, which works better than the\nschedule used for multi-class classiﬁcation. Moreover, motivated by the small size of the mini-batches,\nwe replace Batch Normalization with Group Normalization (Wu & He, 2018). For this table, we\nuse the default setup found in the code of MAML, which uses 5 SGD iterations during training and\n10 during testing. Table 3 conﬁrms the validity of both our approaches on the binary classiﬁcation\nproblem.\nAlthough different in nature, both MAML and our LR-D2 make use of iterative base learners:\nthe former is based on SGD, while the latter on Newton’s method (under the form of Iteratively\nReweighted Least Squares). The use of second-order optimization might suggest that LR-D2 is\ncharacterized by computationally demanding steps. However, we can apply the Woodbury identity at\nevery iteration and obtain a signiﬁcant speedup. In Figure 2 we compare the performance of LR-D2\nvs the one of MAML for a different number of steps of the base learner (kept constant between\ntraining and testing). LR-D2 is superior to MAML, especially for a higher number of steps.\nEfﬁciency. In Table 4 we compare the amount of time required by two representative methods and\nours to solve 10,000 episodes (each with 10 images) on a single NVIDIA GTX 1080 GPU. We use\nminiImageNet (5-way, 1-shot) and adopt, for the lower part of the table, a lightweight embedding\nnetwork of 4 layers and 32 channels per layer. For reference, in the upper part of the table we also\nreport the timings for R2-D2 with [64, 64, 64, 64] and [96, 192, 384, 512] embeddings.\n9\n\n\nPublished as a conference paper at ICLR 2019\n0\n1\n2\n5\n10\nNum iterations\n74.0\n74.5\n75.0\n75.5\n76.0\n76.5\n77.0\n77.5\n78.0\n78.5\n79.0\nAccuracy\nminiImageNet 2-way, 1-shot\nMAML\nOurs/LR-D2\nOurs/R2-D2\n0\n1\n2\n5\n10\nNum iterations\n83.0\n83.5\n84.0\n84.5\n85.0\n85.5\n86.0\n86.5\n87.0\nAccuracy\nminiImageNet 2-way, 5-shot\nMAML\nOurs/LR-D2\nOurs/R2-D2\n0\n1\n2\n5\n10\nNum iterations\n79\n80\n81\n82\n83\n84\n85\n86\nAccuracy\nCIFAR-FS 2-way, 1-shot\nMAML\nOurs/LR-D2\nOurs/R2-D2\n0\n1\n2\n5\n10\nNum iterations\n87.0\n87.5\n88.0\n88.5\n89.0\n89.5\n90.0\n90.5\n91.0\n91.5\n92.0\nAccuracy\nCIFAR-FS 2-way, 5-shot\nMAML\nOurs/LR-D2\nOurs/R2-D2\nFigure 2: Binary classiﬁcation accuracy on two datasets and two setups at different number of steps of\nthe base learner for MAML, R2-D2 and LR-D2. Shaded areas represent 95% conﬁdence intervals.\nInterestingly, we can observe how R2-D2 allows us to achieve an efﬁciency that is comparable to\nthe one of prototypical networks and signiﬁcantly higher than MAML. Notably, unlike prototypical\nnetworks, our methods do allow per-episode adaptation through the weights W of the solver.\nTable 4: Time required to solve 10,000 miniImageNet episodes of 10 samples each.\nminiImageNet, 5-way, 1-shot\nOURS/R2-D2\n1 min 23 sec\nOURS/R2-D2 (with 64C)\n1 min 4 sec\nMAML (Finn et al., 2017) (with 32C)\n6 min 35 sec\nOURS/LR-D2 (1-vs-all) (1 iter.) (with 32C)\n5 min 48 sec\nOURS/R2-D2 (with 32C)\n57 sec\nPROTO NETS (Snell et al., 2017) (with 32C)\n24 sec\n5\nCONCLUSIONS\nWith the aim of allowing efﬁcient adaptation to unseen learning problems, in this paper we explored\nthe feasibility of incorporating fast solvers with closed-form solutions as the base learning component\nof a meta-learning system. Importantly, the use of the Woodbury identity allows signiﬁcant computa-\ntional gains in a scenario presenting only a few samples with high dimensionality, like one-shot of\nfew-shot learning. R2-D2, the differentiable ridge regression base learner we introduce, is almost as\nfast as prototypical networks and strikes a useful compromise between not performing adaptation\nfor new episodes (like metric-learning-based approaches) and conducting a costly iterative approach\n(like MAML or LSTM-based meta-learners). In general, we showed that our base learners work\nremarkably well, with excellent results on few-shot learning benchmarks, generalizing to episodes\nwith new classes that were not seen during training. We believe that our ﬁndings point in an exciting\ndirection of more sophisticated yet efﬁcient online adaptation methods, able to leverage the potential\nof prior knowledge distilled in an ofﬂine training phase. In future work, we would like to explore\nNewton’s methods with more complicated second-order structure than ridge regression.\nACKNOWLEDGMENTS\nWe would like to thank Jack Valmadre, Namhoon Lee and the anonymous reviewers for their insightful\ncomments, which have been useful to improve the manuscript. This work was partially supported by\nthe ERC grant 638009-IDIU.\n10\n\n\nPublished as a conference paper at ICLR 2019\nREFERENCES\nHan Altae-Tran, Bharath Ramsundar, Aneesh S Pappu, and Vijay Pande. Low data drug discovery\nwith one-shot learning. ACS central science, 2017.\nMarcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,\nand Nando de Freitas. Learning to learn by gradient descent by gradient descent. In Advances in\nNeural Information Processing Systems, 2016.\nSamy Bengio, Yoshua Bengio, Jocelyn Cloutier, and Jan Gecsei. On the optimization of a synaptic\nlearning rule. In Preprints Conf. Optimality in Artiﬁcial and Biological Neural Networks, pp. 6–8.\nUniv. of Texas, 1992.\nYoshua Bengio. Gradient-based optimization of hyperparameters. Neural computation, 2000.\nLuca Bertinetto, João F Henriques, Jack Valmadre, Philip Torr, and Andrea Vedaldi. Learning\nfeed-forward one-shot learners. In Advances in Neural Information Processing Systems, 2016.\nChristopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and\nStatistics). Springer-Verlag, Berlin, Heidelberg, 2006.\nJane Bromley, James W Bentz, Léon Bottou, Isabelle Guyon, Yann LeCun, Cliff Moore, Eduard\nSäckinger, and Roopak Shah. Signature veriﬁcation using a “Siamese” time delay neural network.\nInternational Journal of Pattern Recognition and Artiﬁcial Intelligence, 1993.\nSusan Carey. Less may never mean more. Recent advances in the psychology of language, 1978.\nSusan Carey and Elsa Bartlett. Acquiring a single new word. 1978.\nRich Caruana. Multitask learning. In Learning to learn. Springer, 1998.\nSumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity metric discriminatively, with\napplication to face veriﬁcation. In IEEE Conference on Computer Vision and Pattern Recognition,\n2005.\nBrian Chu, Vashisht Madhavan, Oscar Beijbom, Judy Hoffman, and Trevor Darrell. Best practices\nfor ﬁne-tuning visual classiﬁers to new domains. In European Conference on Computer Vision\nworkshops. Springer, 2016.\nLi Fei-Fei, Rob Fergus, and Pietro Perona. One-shot learning of object categories. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 2006.\nChelsea Finn and Sergey Levine. Meta-learning and universality: Deep representations and gradient\ndescent can approximate any learning algorithm. 2018.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of\ndeep networks. In International Conference on Machine Learning, 2017.\nVictor Garcia and Joan Bruna. Few-shot learning with graph neural networks. In International\nConference on Learning Representations, 2018.\nSpyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In IEEE\nConference on Computer Vision and Pattern Recognition, 2018.\nBoqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic ﬂow kernel for unsupervised\ndomain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition, 2012.\nBharath Hariharan and Ross B Girshick. Low-shot visual recognition by shrinking and hallucinating\nfeatures. In IEEE International Conference on Computer Vision, 2017.\nSepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent.\nIn International Conference on Artiﬁcial Neural Networks, pp. 87–94. Springer, 2001.\nCatalin Ionescu, Orestis Vantzos, and Cristian Sminchisescu. Training deep networks with structured\nlayers by matrix backpropagation. arXiv preprint arXiv:1509.07838, 2015.\n11\n\n\nPublished as a conference paper at ICLR 2019\nŁukasz Kaiser, Oﬁr Nachum, Aurko Roy, and Samy Bengio. Learning to remember rare events. In\nInternational Conference on Learning Representations, 2017.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. 2015.\nGregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot\nimage recognition. In International Conference on Machine Learning workshops, 2015.\nAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.\nBrenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning\nthrough probabilistic program induction. Science, 2015.\nDougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization\nthrough reversible learning. In International Conference on Machine Learning, pp. 2113–2122,\n2015.\nMichael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The\nsequential learning problem. In Psychology of learning and motivation. 1989.\nErik G Miller, Nicholas E Matsakis, and Paul A Viola. Learning from one example through shared\ndensities on transforms. In IEEE Conference on Computer Vision and Pattern Recognition. IEEE,\n2000.\nNikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-\nlearner. In International Conference on Learning Representations, 2018.\nTom M Mitchell. The need for biases in learning generalizations. Department of Computer Science,\nLaboratory for Computer Science Research, Rutgers Univ. New Jersey, 1980.\nTom M Mitchell et al. Machine learning. 1997. Burr Ridge, IL: McGraw Hill, 1997.\nTsendsuren Munkhdalai and Hong Yu. Meta networks. In International Conference on Machine\nLearning, 2017.\nKevin P. Murphy. Machine Learning: A Probabilistic Perspective. The MIT Press, 2012.\nDevang K Naik and RJ Mammone. Meta-neural networks that learn by learning. In Neural Networks,\n1992. IJCNN., International Joint Conference on. IEEE, 1992.\nAlex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-order meta-learning algorithms. CoRR,\n2018. URL http://arxiv.org/abs/1803.02999.\nAaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,\nNal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw\naudio. arXiv preprint arXiv:1609.03499, 2016.\nKaare Brandt Petersen, Michael Syskind Pedersen, et al. The matrix cookbook. Technical University\nof Denmark, 2008.\nSiyuan Qiao, Chenxi Liu, Wei Shen, and Alan L. Yuille. Few-shot image recognition by predicting\nparameters from activations. In IEEE Conference on Computer Vision and Pattern Recognition,\n2018.\nSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International\nConference on Learning Representations, 2017.\nSylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with\nresidual adapters. In Advances in Neural Information Processing Systems, 2017.\nMengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum,\nHugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classiﬁcation.\nIn International Conference on Learning Representations, 2018.\n12\n\n\nPublished as a conference paper at ICLR 2019\nSebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint\narXiv:1706.05098, 2017.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition\nchallenge. 2015.\nAdam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-\nlearning with memory-augmented neural networks. In International Conference on Machine\nLearning, 2016.\nJürgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn:\nthe meta-meta-... hook. PhD thesis, Technische Universität München, 1987.\nJürgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent\nnetworks. Neural Computation, 1992.\nJürgen Schmidhuber. A neural network that embeds its own meta-levels. In Neural Networks, 1993.,\nIEEE International Conference on. IEEE, 1993.\nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In\nAdvances in Neural Information Processing Systems, 2017.\nPablo Sprechmann, Siddhant M Jayakumar, Jack W Rae, Alexander Pritzel, Adrià Puigdomènech\nBadia, Benigno Uria, Oriol Vinyals, Demis Hassabis, Razvan Pascanu, and Charles Blundell.\nMemory-based parameter adaptation. 2018.\nFlood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.\nLearning to compare: Relation network for few-shot learning. In IEEE Conference on Computer\nVision and Pattern Recognition, 2018.\nAlbert Tarantola. Inverse problem theory and methods for model parameter estimation, volume 89.\nsiam, 2005.\nSebastian Thrun. Is learning the n-th thing any easier than learning the ﬁrst? In Advances in Neural\nInformation Processing Systems, 1996.\nSebastian Thrun. Lifelong learning algorithms. In Learning to learn. Springer, 1998.\nSebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media, 1998.\nPaul E Utgoff. Shift of bias for inductive concept learning. Machine learning: An artiﬁcial intelligence\napproach, 1986.\nJack Valmadre, Luca Bertinetto, João Henriques, Andrea Vedaldi, and Philip HS Torr. End-to-end\nrepresentation learning for correlation ﬁlter based tracking. In IEEE Conference on Computer\nVision and Pattern Recognition, 2017.\nRicardo Vilalta and Youssef Drissi. A perspective view and survey of meta-learning. Artiﬁcial\nIntelligence Review, 2002.\nOriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one shot\nlearning. In Advances in Neural Information Processing Systems, 2016.\nYuxin Wu and Kaiming He. Group normalization. CoRR, 2018. URL http://arxiv.org/\nabs/1803.08494.\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep\nneural networks? In Advances in Neural Information Processing Systems, 2014.\nA Steven Younger, Sepp Hochreiter, and Peter R Conwell. Meta-learning with backpropagation. In\nNeural Networks, 2001. Proceedings. IJCNN’01. International Joint Conference on. IEEE, 2001.\n13\n\n\nPublished as a conference paper at ICLR 2019\nA\nEXTENDED DISCUSSION\nContributions within the few-shot learning paradigm. In this work, we evaluated our proposed\nmethods R2-D2 and LR-D2 in the few-shot learning scenario (Fei-Fei et al., 2006; Lake et al., 2015;\nVinyals et al., 2016; Ravi & Larochelle, 2017; Hariharan & Girshick, 2017), which consists in\nlearning how to discriminate between images given one or very few examples. For methods tackling\nthis problem, it is common practice to organise the training procedure in two nested loops. The\ninner loop is used to solve the actual few-shot classiﬁcation problem, while the outer loop serves as\na guidance for the former by gradually modifying the inductive bias of the base learner (Vilalta &\nDrissi, 2002). Differently from standard classiﬁcation benchmarks, the few-shot ones enforce that\nclasses are disjoint between dataset splits.\nIn the literature (e.g. Vinyals et al. (2016)), the very small classiﬁcation problems with unseen classes\nsolved within the inner loop have often been referred to as episodes or tasks. Considering the general\nfew-shot learning paradigm just described, methods in the recent literature mostly differ for the type\nof learner they use in the inner loop and the amount of per-episode adaptability they allow. For\nexample, at the one end of the spectrum in terms of “amount of adaptability”, we can ﬁnd methods\nsuch as MAML Finn et al. (2017), which learns how to efﬁciently ﬁne-tune the parameters of a\nneural-network with few iterations of SGD. On the other end, we have methods based on metric\nlearning such as prototypical networks Snell et al. (2017) and relation network Sung et al. (2018),\nwhich are fast but do not perform adaptation. Note that the amount of adaptation to a new episode\n(i.e.a new classiﬁcation problem with unseen classes) is not at all indicative of the performance in\nfew-shot learning benchmarks. As a matter of fact, both Snell et al. (2017) and Sung et al. (2018)\nachieve higher accuracy than MAML. Nonetheless, adaptability is a desirable property, as it allows\nmore design ﬂexibility.\nWithin this landscape, our work proposes a novel technique (R2-D2) that does allow per-episode\nadaptation while at the same time being fast (Table 4) and achieving strong performance (Table 1).\nThe key innovation is to use a simple (and differentiable) solver such as ridge regression within the\ninner loop, which requires back-propagating through the solution of a learning problem. Crucially,\nits closed-form solution and the use of the Woodbury identity (particularly advantageous in the low\ndata regime) allow this non-trivial endeavour to be efﬁcient. We further demonstrate that this strategy\nis not limited to the ridge regression case, but it can also be extended to other solvers (LR-D2) by\ndividing the problem into a short series of weighted least squares problems ((Murphy, 2012, Chapter\n8.3.4)).\nDisambiguation from the multi-task learning paradigm. Our work – and more generally the\nfew-shot learning literature as a whole – is related to the multi-task learning paradigm (Caruana,\n1998; Ruder, 2017). However, several crucial differences exist. In terms of setup, multi-task learning\nmethods are trained to solve a ﬁxed set of T tasks (or domains). At test time, the same T tasks or\ndomains are encountered. For instance, the popular Ofﬁce-Caltech (Gong et al., 2012) dataset is\nconstructed by considering all the images from 10 classes present in 4 different datasets (the domains).\nFor multi-task learning, the splits span the domains but contain all the 10 classes. Conversely, few-shot\nlearning datasets have splits with disjoint sets of classes (i.e. each split’s classes are not contained in\nother splits). Moreover, only a few examples (shots) can be used as training data within one episode,\nwhile in multi-task learning this limitation is not present. For this reason, meta-learning methods\napplied to few-shot learning (e.g.ours, (Vinyals et al., 2016; Finn et al., 2017; Ravi & Larochelle,\n2017; Mishra et al., 2018)) crucially take into account adaptation already during the training process\nto mimic the test-time setting, de facto learning how to learn from limited data.\nThe importance of considering adaptation during training. Considering adaptation during train-\ning is also one of the main traits that differentiate our approach from basic transfer learning approaches\nin which a neural network is ﬁrst pre-trained on one dataset/task and then adapted to a different\ndataset/task by simply adapting the ﬁnal layer(s) (e.g. Yosinski et al. (2014); Chu et al. (2016)).\nTo better illustrate this point, we conducted a baseline experiment. First, we pre-trained for a standard\nclassiﬁcation problem the same 4-layers CNN architecture using the same training datasets. We\nsimply added a ﬁnal fully-connected layer (with 64 outputs, like the number of classes in the training\nsplits) and used the cross-entropy loss. Then, we used the convolutional part of this trained network\nas a feature extractor and fed its activations to our ridge-regression layer to produce a per-episode\nset of weights W. On miniImagenet, the drop in performance w.r.t. our proposed R2-D2 is very\n14\n\n\nPublished as a conference paper at ICLR 2019\nsigniﬁcant: −13.8% and −11.6% accuracy for the 1 and 5 shot problems respectively. The drop in\nperformance is consistent on CIFAR, though a bit less drastic: −11.5% and −5.9%.\nThese results empirically conﬁrm that simply using basic transfer learning techniques with a shared\nfeature representation and task-speciﬁc ﬁnal layers is not a good strategy to obtain results competitive\nwith the state-of-the-art in few-shot learning. Instead, it is necessary to enforce the generality of\nthe underlying features during training explicitly, which we do by back-propagating through the\nadaptation procedure (the regressors R2-D2 and LR-D2).\nB\nDIFFERENT GAUSSIAN PRIORS FOR REGULARIZATION\nThe regularization term can be seen as a prior gaussian distribution of the parameters in a Bayesian\ninterpretation, or more simply Tikhonov regularization (Tarantola, 2005). In the most common case\nof λI, it corresponds to an isotropic gaussian prior on the parameters.\nIn addition to the case in which λ is a scalar, we also experiment with the variant diag(λ), corre-\nsponding to an axis-aligned gaussian prior with an independent variance for each parameter, which\ncan potentially exploit the fact that the parameters have different scales. Replacing λI with diag(λ)\nin 4, the ﬁnal expression for W after having applied the Woodbury identity becomes:\nW = Λ(Z) = diag(λ)−1XT (Xdiag(λ)−1XT + I)−1Y.\n(8)\nC\nBASE LEARNER HYPER-PARAMETERS\nFigure 3 illustrates the effect of using SGD to learn, together with the parameters ω of the CNN,\nalso the hyper-parameters (ρ in eq. 2) of the base learner Λ. We ﬁnd that it is very important to\nlearn the scalar α (right plot of Figure 3) used to calibrate the output of R2-D2 in eq. 6, while it is\nindifferent whether or not to learn λ. Note that, by using SGD to update α, it is possible (e.g.in the\nrange [10−3, 100]) to recover from poor initial values and suffer just a little performance loss w.r.t.\nthe optimal value of α = 10.\nThe left plot of Figure 3 also shows the performance of R2-D2 with the variant diag(λ) introduced\nin Appendix B. Unfortunately, despite this formulation allows us to make use of a more expressive\nprior, it does not improve the results compared to using a simple scalar λ. Moreover, performance\nabruptly deteriorate for λ > 0.01.\n10\n4 10\n3 10\n2 10\n1 100\n101\n102\n103\n104\n105\nInitial value of\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\nAccuracy\nCIFAR-FS 5-way, 1-shot\nFixed \nLearnt \nLearnt diag( )\n10\n3\n10\n2\n10\n1\n100\n101\n102\n103\n104\n105\nInitial value of\n61\n62\n63\n64\n65\n66\n67\nAccuracy\nCIFAR-FS 5-way, 1-shot\nFixed \nLearnt \nFigure 3: Shaded areas represent 95% conﬁdence intervals.\n15\n"
}