{
  "filename": "1812.01187v2.pdf",
  "num_pages": 10,
  "pages": [
    "Bag of Tricks for Image Classiﬁcation with Convolutional Neural Networks\nTong He\nZhi Zhang\nHang Zhang\nZhongyue Zhang\nJunyuan Xie\nMu Li\nAmazon Web Services\n{htong,zhiz,hzaws,zhongyue,junyuanx,mli}@amazon.com\nAbstract\nMuch of the recent progress made in image classiﬁcation\nresearch can be credited to training procedure reﬁnements,\nsuch as changes in data augmentations and optimization\nmethods. In the literature, however, most reﬁnements are ei-\nther brieﬂy mentioned as implementation details or only vis-\nible in source code. In this paper, we will examine a collec-\ntion of such reﬁnements and empirically evaluate their im-\npact on the ﬁnal model accuracy through ablation study. We\nwill show that, by combining these reﬁnements together, we\nare able to improve various CNN models signiﬁcantly. For\nexample, we raise ResNet-50’s top-1 validation accuracy\nfrom 75.3% to 79.29% on ImageNet. We will also demon-\nstrate that improvement on image classiﬁcation accuracy\nleads to better transfer learning performance in other ap-\nplication domains such as object detection and semantic\nsegmentation.\n1. Introduction\nSince the introduction of AlexNet [15] in 2012, deep\nconvolutional neural networks have become the dominat-\ning approach for image classiﬁcation. Various new architec-\ntures have been proposed since then, including VGG [24],\nNiN [16], Inception [1], ResNet [9], DenseNet [13], and\nNASNet [34]. At the same time, we have seen a steady\ntrend of model accuracy improvement. For example, the\ntop-1 validation accuracy on ImageNet [23] has been raised\nfrom 62.5% (AlexNet) to 82.7% (NASNet-A).\nHowever, these advancements did not solely come from\nimproved model architecture. Training procedure reﬁne-\nments, including changes in loss functions, data preprocess-\ning, and optimization methods also played a major role. A\nlarge number of such reﬁnements has been proposed in the\npast years, but has received relatively less attention. In the\nliterature, most were only brieﬂy mentioned as implemen-\ntation details while others can only be found in source code.\nIn this paper, we will examine a collection of training\nModel\nFLOPs\ntop-1\ntop-5\nResNet-50 [9]\n3.9 G\n75.3\n92.2\nResNeXt-50 [27]\n4.2 G\n77.8\n-\nSE-ResNet-50 [12]\n3.9 G\n76.71\n93.38\nSE-ResNeXt-50 [12]\n4.3 G\n78.90\n94.51\nDenseNet-201 [13]\n4.3 G\n77.42\n93.66\nResNet-50 + tricks (ours)\n4.3 G\n79.29\n94.63\nTable 1: Computational costs and validation accuracy of\nvarious models. ResNet, trained with our “tricks”, is able\nto outperform newer and improved architectures trained\nwith standard pipeline.\nprocedure and model architecture reﬁnements that improve\nmodel accuracy but barely change computational complex-\nity. Many of them are minor “tricks” like modifying the\nstride size of a particular convolution layer or adjusting\nlearning rate schedule. Collectively, however, they make a\nbig difference. We will evaluate them on multiple network\narchitectures and datasets and report their impact to the ﬁnal\nmodel accuracy.\nOur empirical evaluation shows that several tricks lead\nto signiﬁcant accuracy improvement and combining them\ntogether can further boost the model accuracy. We com-\npare ResNet-50, after applying all tricks, to other related\nnetworks in Table 1. Note that these tricks raises ResNet-\n50’s top-1 validation accuracy from 75.3% to 79.29% on\nImageNet. It also outperforms other newer and improved\nnetwork architectures, such as SE-ResNeXt-50. In addi-\ntion, we show that our approach can generalize to other net-\nworks (Inception V3 [1] and MobileNet [11]) and datasets\n(Place365 [32]). We further show that models trained with\nour tricks bring better transfer learning performance in other\napplication domains such as object detection and semantic\nsegmentation.\nPaper Outline.\nWe ﬁrst set up a baseline training proce-\ndure in Section 2, and then discuss several tricks that are\n1\narXiv:1812.01187v2  [cs.CV]  5 Dec 2018\n",
    "Algorithm 1 Train a neural network with mini-batch\nstochastic gradient descent.\ninitialize(net)\nfor epoch = 1, . . . , K do\nfor batch = 1, . . . , #images/b do\nimages ←uniformly random sample b images\nX, y ←preprocess(images)\nz ←forward(net, X)\nℓ←loss(z, y)\ngrad ←backward(ℓ)\nupdate(net, grad)\nend for\nend for\nuseful for efﬁcient training on new hardware in Section 3. In\nSection 4 we review three minor model architecture tweaks\nfor ResNet and propose a new one. Four additional train-\ning procedure reﬁnements are then discussed in Section 5.\nAt last, we study if these more accurate models can help\ntransfer learning in Section 6.\nOur model implementations and training scripts are pub-\nlicly available in GluonCV 1.\n2. Training Procedures\nThe template of training a neural network with mini-\nbatch stochastic gradient descent is shown in Algorithm 1.\nIn each iteration, we randomly sample b images to com-\npute the gradients and then update the network parameters.\nIt stops after K passes through the dataset. All functions\nand hyper-parameters in Algorithm 1 can be implemented\nin many different ways. In this section, we ﬁrst specify a\nbaseline implementation of Algorithm 1.\n2.1. Baseline Training Procedure\nWe follow a widely used implementation [8] of ResNet\nas our baseline. The preprocessing pipelines between train-\ning and validation are different. During training, we per-\nform the following steps one-by-one:\n1. Randomly sample an image and decode it into 32-bit\nﬂoating point raw pixel values in [0, 255].\n2. Randomly crop a rectangular region whose aspect ratio\nis randomly sampled in [3/4, 4/3] and area randomly\nsampled in [8%, 100%], then resize the cropped region\ninto a 224-by-224 square image.\n3. Flip horizontally with 0.5 probability.\n4. Scale hue, saturation, and brightness with coefﬁcients\nuniformly drawn from [0.6, 1.4].\n5. Add PCA noise with a coefﬁcient sampled from a nor-\nmal distribution N(0, 0.1).\n1https://github.com/dmlc/gluon-cv\nModel\nBaseline\nReference\nTop-1\nTop-5\nTop-1\nTop-5\nResNet-50 [9]\n75.87\n92.70\n75.3\n92.2\nInception-V3 [26]\n77.32\n93.43\n78.8\n94.4\nMobileNet [11]\n69.03\n88.71\n70.6\n-\nTable 2: Validation accuracy of reference implementa-\ntions and our baseline. Note that the numbers for Incep-\ntion V3 are obtained with 299-by-299 input images.\n6. Normalize RGB channels by subtracting 123.68,\n116.779, 103.939 and dividing by 58.393, 57.12,\n57.375, respectively.\nDuring validation, we resize each image’s shorter edge\nto 256 pixels while keeping its aspect ratio. Next, we crop\nout the 224-by-224 region in the center and normalize RGB\nchannels similar to training. We do not perform any random\naugmentations during validation.\nThe weights of both convolutional and fully-connected\nlayers are initialized with the Xavier algorithm [6]. In par-\nticular, we set the parameter to random values uniformly\ndrawn from [−a, a], where a =\np\n6/(din + dout). Here\ndin and dout are the input and output channel sizes, respec-\ntively. All biases are initialized to 0. For batch normaliza-\ntion layers, γ vectors are initialized to 1 and β vectors to\n0.\nNesterov Accelerated Gradient (NAG) descent [20] is\nused for training. Each model is trained for 120 epochs on\n8 Nvidia V100 GPUs with a total batch size of 256. The\nlearning rate is initialized to 0.1 and divided by 10 at the\n30th, 60th, and 90th epochs.\n2.2. Experiment Results\nWe evaluate three CNNs:\nResNet-50 [9], Inception-\nV3 [1], and MobileNet [11]. For Inception-V3 we resize the\ninput images into 299x299. We use the ISLVRC2012 [23]\ndataset, which has 1.3 million images for training and 1000\nclasses. The validation accuracies are shown in Table 2. As\ncan be seen, our ResNet-50 results are slightly better than\nthe reference results, while our baseline Inception-V3 and\nMobileNet are slightly lower in accuracy due to different\ntraining procedure.\n3. Efﬁcient Training\nHardware, especially GPUs, has been rapidly evolving\nin recent years. As a result, the optimal choices for many\nperformance related trade-offs have changed. For example,\nit is now more efﬁcient to use lower numerical precision and\nlarger batch sizes during training. In this section, we review\nvarious techniques that enable low precision and large batch\n",
    "training without sacriﬁcing model accuracy.\nSome tech-\nniques can even improve both accuracy and training speed.\n3.1. Large-batch training\nMini-batch SGD groups multiple samples to a mini-\nbatch to increase parallelism and decrease communication\ncosts. Using large batch size, however, may slow down\nthe training progress. For convex problems, convergence\nrate decreases as batch size increases. Similar empirical re-\nsults have been reported for neural networks [25]. In other\nwords, for the same number of epochs, training with a large\nbatch size results in a model with degraded validation accu-\nracy compared to the ones trained with smaller batch sizes.\nMultiple works [7, 14] have proposed heuristics to solve\nthis issue. In the following paragraphs, we will examine\nfour heuristics that help scale the batch size up for single\nmachine training.\nLinear scaling learning rate.\nIn mini-batch SGD, gradi-\nent descending is a random process because the examples\nare randomly selected in each batch. Increasing the batch\nsize does not change the expectation of the stochastic gra-\ndient but reduces its variance. In other words, a large batch\nsize reduces the noise in the gradient, so we may increase\nthe learning rate to make a larger progress along the op-\nposite of the gradient direction. Goyal et al. [7] reports\nthat linearly increasing the learning rate with the batch size\nworks empirically for ResNet-50 training. In particular, if\nwe follow He et al. [9] to choose 0.1 as the initial learn-\ning rate for batch size 256, then when changing to a larger\nbatch size b, we will increase the initial learning rate to\n0.1 × b/256.\nLearning rate warmup.\nAt the beginning of the training,\nall parameters are typically random values and therefore far\naway from the ﬁnal solution. Using a too large learning rate\nmay result in numerical instability. In the warmup heuristic,\nwe use a small learning rate at the beginning and then switch\nback to the initial learning rate when the training process\nis stable [9]. Goyal et al. [7] proposes a gradual warmup\nstrategy that increases the learning rate from 0 to the initial\nlearning rate linearly. In other words, assume we will use\nthe ﬁrst m batches (e.g. 5 data epochs) to warm up, and the\ninitial learning rate is η, then at batch i, 1 ≤i ≤m, we will\nset the learning rate to be iη/m.\nZero γ.\nA ResNet network consists of multiple residual\nblocks, each block consists of several convolutional lay-\ners. Given input x, assume block(x) is the output for the\nlast layer in the block, this residual block then outputs\nx + block(x). Note that the last layer of a block could\nbe a batch normalization (BN) layer. The BN layer ﬁrst\nstandardizes its input, denoted by ˆx, and then performs a\nscale transformation γˆx + β. Both γ and β are learnable\nparameters whose elements are initialized to 1s and 0s, re-\nspectively. In the zero γ initialization heuristic, we initialize\nγ = 0 for all BN layers that sit at the end of a residual block.\nTherefore, all residual blocks just return their inputs, mim-\nics network that has less number of layers and is easier to\ntrain at the initial stage.\nNo bias decay.\nThe weight decay is often applied to all\nlearnable parameters including both weights and bias. It’s\nequivalent to applying an L2 regularization to all parame-\nters to drive their values towards 0. As pointed out by Jia et\nal. [14], however, it’s recommended to only apply the reg-\nularization to weights to avoid overﬁtting. The no bias de-\ncay heuristic follows this recommendation, it only applies\nthe weight decay to the weights in convolution and fully-\nconnected layers. Other parameters, including the biases\nand γ and β in BN layers, are left unregularized.\nNote that LARS [4] offers layer-wise adaptive learning\nrate and is reported to be effective for extremely large batch\nsizes (beyond 16K). While in this paper we limit ourselves\nto methods that are sufﬁcient for single machine training,\nin which case a batch size no more than 2K often leads to\ngood system efﬁciency.\n3.2. Low-precision training\nNeural networks are commonly trained with 32-bit ﬂoat-\ning point (FP32) precision. That is, all numbers are stored in\nFP32 format and both inputs and outputs of arithmetic oper-\nations are FP32 numbers as well. New hardware, however,\nmay have enhanced arithmetic logic unit for lower precision\ndata types. For example, the previously mentioned Nvidia\nV100 offers 14 TFLOPS in FP32 but over 100 TFLOPS in\nFP16. As in Table 3, the overall training speed is acceler-\nated by 2 to 3 times after switching from FP32 to FP16 on\nV100.\nDespite the performance beneﬁt, a reduced precision has\na narrower range that makes results more likely to be out-of-\nrange and then disturb the training progress. Micikevicius et\nal. [19] proposes to store all parameters and activations in\nFP16 and use FP16 to compute gradients. At the same time,\nall parameters have an copy in FP32 for parameter updat-\ning. In addition, multiplying a scalar to the loss to better\nalign the range of the gradient into FP16 is also a practical\nsolution.\n3.3. Experiment Results\nThe evaluation results for ResNet-50 are shown in Ta-\nble 3. Compared to the baseline with batch size 256 and\nFP32, using a larger 1024 batch size and FP16 reduces the\ntraining time for ResNet-50 from 13.3-min per epoch to 4.4-\nmin per epoch. In addition, by stacking all heuristics for\n",
    "Input stem\nStage 1\nStage 2\nStage 3\nStage 4\nOutput\nInput\nOutput\nConv \n7x7, 64, s=2\nInput\nMaxPool \n3x3, s=2\nDown\nsampling\nResidual\nResidual\nConv\n1[\u0014\u000f\u0003\u0018\u0014\u0015\u000f\u0003V \u0015\nConv\n3x3, 512\nConv\n1x1, 2048\nConv\n1[\u0014\u000f\u0003\u0015\u0013\u0017\u001b\u000f\u0003V \u0015\nOutput\nInput\nInput\nOutput\nOutput\n+\nPath A\nPath B\nFigure 1: The architecture of ResNet-50. The convolution\nkernel size, output channel size and stride size (default is 1)\nare illustrated, similar for pooling layers.\nlarge-batch training, the model trained with 1024 batch size\nand FP16 even slightly increased 0.5% top-1 accuracy com-\npared to the baseline model.\nThe ablation study of all heuristics is shown in Table 4.\nIncreasing batch size from 256 to 1024 by linear scaling\nlearning rate alone leads to a 0.9% decrease of the top-1\naccuracy while stacking the rest three heuristics bridges the\ngap. Switching from FP32 to FP16 at the end of training\ndoes not affect the accuracy.\n4. Model Tweaks\nA model tweak is a minor adjustment to the network ar-\nchitecture, such as changing the stride of a particular convo-\nlution layer. Such a tweak often barely changes the compu-\ntational complexity but might have a non-negligible effect\non the model accuracy. In this section, we will use ResNet\nas an example to investigate the effects of model tweaks.\n4.1. ResNet Architecture\nWe will brieﬂy present the ResNet architecture, espe-\ncially its modules related to the model tweaks. For detailed\ninformation please refer to He et al. [9]. A ResNet network\nconsists of an input stem, four subsequent stages and a ﬁnal\noutput layer, which is illustrated in Figure 1. The input stem\nhas a 7 × 7 convolution with an output channel of 64 and a\nstride of 2, followed by a 3 × 3 max pooling layer also with\na stride of 2. The input stem reduces the input width and\nheight by 4 times and increases its channel size to 64.\nStarting from stage 2, each stage begins with a down-\nsampling block, which is then followed by several residual\nblocks. In the downsampling block, there are path A and\nConv\n\u000b1[\u0014\f\nConv\n\u000b3x3, s=2\f\nConv\n\u000b1x1\f\nConv\n\u000b1[\u0014\u000f\u0003V \u0015\f\nInput\nOutput\n+\n(a) ResNet-B\nConv \n(3x3)\nInput\nMaxPool \n(3x3, s=2)\nOutput\nConv \n(3x3, s=2)\nConv \n(3x3)\n(b) ResNet-C\nConv\n\u000b1[\u0014\f\nConv\n\u000b3x3, s=2\f\nConv\n\u000b1x1\f\nConv\n\u000b1[\u0014\f\nInput\nOutput\n+\nAvgPool\n\u000b2[\u0015\u000f\u0003V \u0015\f\n(c) ResNet-D\nFigure 2: Three ResNet tweaks. ResNet-B modiﬁes the\ndownsampling block of Resnet. ResNet-C further modiﬁes\nthe input stem. On top of that, ResNet-D again modiﬁes the\ndownsampling block.\npath B. Path A has three convolutions, whose kernel sizes\nare 1×1, 3×3 and 1×1, respectively. The ﬁrst convolution\nhas a stride of 2 to halve the input width and height, and the\nlast convolution’s output channel is 4 times larger than the\nprevious two, which is called the bottleneck structure. Path\nB uses a 1×1 convolution with a stride of 2 to transform the\ninput shape to be the output shape of path A, so we can sum\noutputs of both paths to obtain the output of the downsam-\npling block. A residual block is similar to a downsampling\nblock except for only using convolutions with a stride of 1.\nOne can vary the number of residual blocks in each stage\nto obtain different ResNet models, such as ResNet-50 and\nResNet-152, where the number presents the number of con-\nvolutional layers in the network.\n4.2. ResNet Tweaks\nNext, we revisit two popular ResNet tweaks, we call\nthem ResNet-B and ResNet-C, respectively. We propose\na new model tweak ResNet-D afterwards.\nResNet-B.\nThis tweak ﬁrst appeared in a Torch imple-\nmentation of ResNet [8] and then adopted by multiple\nworks [7, 12, 27]. It changes the downsampling block of\nResNet. The observation is that the convolution in path A\nignores three-quarters of the input feature map because it\nuses a kernel size 1×1 with a stride of 2. ResNet-B switches\nthe strides size of the ﬁrst two convolutions in path A, as\nshown in Figure 2a, so no information is ignored. Because\nthe second convolution has a kernel size 3 × 3, the output\nshape of path A remains unchanged.\nResNet-C.\nThis tweak was proposed in Inception-v2 [26]\noriginally, and it can be found on the implementations\n",
    "Model\nEfﬁcient\nBaseline\nTime/epoch\nTop-1\nTop-5\nTime/epoch\nTop-1\nTop-5\nResNet-50\n4.4 min\n76.21\n92.97\n13.3 min\n75.87\n92.70\nInception-V3\n8 min\n77.50\n93.60\n19.8 min\n77.32\n93.43\nMobileNet\n3.7 min\n71.90\n90.47\n6.2 min\n69.03\n88.71\nTable 3: Comparison of the training time and validation accuracy for ResNet-50 between the baseline (BS=256 with FP32)\nand a more hardware efﬁcient setting (BS=1024 with FP16).\nHeuristic\nBS=256\nBS=1024\nTop-1\nTop-5\nTop-1\nTop-5\nLinear scaling\n75.87\n92.70\n75.17\n92.54\n+ LR warmup\n76.03\n92.81\n75.93\n92.84\n+ Zero γ\n76.19\n93.03\n76.37\n92.96\n+ No bias decay\n76.16\n92.97\n76.03\n92.86\n+ FP16\n76.15\n93.09\n76.21\n92.97\nTable 4: The breakdown effect for each effective training\nheuristic on ResNet-50.\nof other models, such as SENet [12], PSPNet [31],\nDeepLabV3 [1], and ShufﬂeNetV2 [21]. The observation\nis that the computational cost of a convolution is quadratic\nto the kernel width or height. A 7 × 7 convolution is 5.4\ntimes more expensive than a 3 × 3 convolution. So this\ntweak replacing the 7 × 7 convolution in the input stem\nwith three conservative 3 × 3 convolutions, which is shown\nin Figure 2b, with the ﬁrst and second convolutions have\ntheir output channel of 32 and a stride of 2, while the last\nconvolution uses a 64 output channel.\nResNet-D.\nInspired by ResNet-B, we note that the 1 × 1\nconvolution in the path B of the downsampling block also\nignores 3/4 of input feature maps, we would like to modify\nit so no information will be ignored. Empirically, we found\nadding a 2×2 average pooling layer with a stride of 2 before\nthe convolution, whose stride is changed to 1, works well\nin practice and impacts the computational cost little. This\ntweak is illustrated in Figure 2c.\n4.3. Experiment Results\nWe evaluate ResNet-50 with the three tweaks and set-\ntings described in Section 3, namely the batch size is 1024\nand precision is FP16. The results are shown in Table 5.\nSuggested by the results, ResNet-B receives more infor-\nmation in path A of the downsampling blocks and improves\nvalidation accuracy by around 0.5% compared to ResNet-\n50. Replacing the 7 × 7 convolution with three 3 × 3 ones\ngives another 0.2% improvement. Taking more information\nin path B of the downsampling blocks improves the vali-\nModel\n#params\nFLOPs\nTop-1\nTop-5\nResNet-50\n25 M\n3.8 G\n76.21\n92.97\nResNet-50-B\n25 M\n4.1 G\n76.66\n93.28\nResNet-50-C\n25 M\n4.3 G\n76.87\n93.48\nResNet-50-D\n25 M\n4.3 G\n77.16\n93.52\nTable 5: Compare ResNet-50 with three model tweaks on\nmodel size, FLOPs and ImageNet validation accuracy.\ndation accuracy by another 0.3%. In total, ResNet-50-D\nimproves ResNet-50 by 1%.\nOn the other hand, these four models have the same\nmodel size. ResNet-D has the largest computational cost,\nbut its difference compared to ResNet-50 is within 15% in\nterms of ﬂoating point operations. In practice, we observed\nResNet-50-D is only 3% slower in training throughput com-\npared to ResNet-50.\n5. Training Reﬁnements\nIn this section, we will describe four training reﬁnements\nthat aim to further improve the model accuracy.\n5.1. Cosine Learning Rate Decay\nLearning rate adjustment is crucial to the training. Af-\nter the learning rate warmup described in Section 3.1, we\ntypically steadily decrease the value from the initial learn-\ning rate. The widely used strategy is exponentially decaying\nthe learning rate. He et al. [9] decreases rate at 0.1 for ev-\nery 30 epochs, we call it “step decay”. Szegedy et al. [26]\ndecreases rate at 0.94 for every two epochs.\nIn contrast to it, Loshchilov et al. [18] propose a cosine\nannealing strategy. An simpliﬁed version is decreasing the\nlearning rate from the initial value to 0 by following the\ncosine function. Assume the total number of batches is T\n(the warmup stage is ignored), then at batch t, the learning\nrate ηt is computed as:\nηt = 1\n2\n\u0012\n1 + cos\n\u0012tπ\nT\n\u0013\u0013\nη,\n(1)\nwhere η is the initial learning rate. We call this scheduling\nas “cosine” decay.\n",
    "0.0\n0.1\n0.2\n0.3\n0.4\n0\n20\n40\n60\n80\n100\n120\nEpoch\nLearning Rate\nCosine Decay\nStep Decay\n(a) Learning Rate Schedule\n0.0\n0.2\n0.4\n0.6\n0.8\n0\n20\n40\n60\n80\n100\n120\nEpoch\nTop−1 Accuracy\nCosine Decay\nStep Decay\n(b) Validation Accuracy\nFigure 3:\nVisualization of learning rate schedules with\nwarm-up. Top: cosine and step schedules for batch size\n1024. Bottom: Top-1 validation accuracy curve with regard\nto the two schedules.\nThe comparison between step decay and cosine decay\nare illustrated in Figure 3a. As can be seen, the cosine decay\ndecreases the learning rate slowly at the beginning, and then\nbecomes almost linear decreasing in the middle, and slows\ndown again at the end. Compared to the step decay, the\ncosine decay starts to decay the learning since the beginning\nbut remains large until step decay reduces the learning rate\nby 10x, which potentially improves the training progress.\n5.2. Label Smoothing\nThe last layer of a image classiﬁcation network is often a\nfully-connected layer with a hidden size being equal to the\nnumber of labels, denote by K, to output the predicted con-\nﬁdence scores. Given an image, denote by zi the predicted\nscore for class i. These scores can be normalized by the\nsoftmax operator to obtain predicted probabilities. Denote\nby q the output of the softmax operator q = softmax(z), the\nprobability for class i, qi, can be computed by:\nqi =\nexp(zi)\nPK\nj=1 exp(zj)\n.\n(2)\nIt’s easy to see qi > 0 and PK\ni=1 qi = 1, so q is a valid\nprobability distribution.\nOn the other hand, assume the true label of this image\nis y, we can construct a truth probability distribution to be\npi = 1 if i = y and 0 otherwise. During training, we mini-\nmize the negative cross entropy loss\nℓ(p, q) = −\nK\nX\ni=1\nqi log pi\n(3)\nto update model parameters to make these two probabil-\nity distributions similar to each other. In particular, by the\nway how p is constructed, we know ℓ(p, q) = −log py =\n−zy + log\n\u0010PK\ni=1 exp(zi)\n\u0011\n. The optimal solution is z∗\ny =\ninf while keeping others small enough. In other words, it\nencourages the output scores dramatically distinctive which\npotentially leads to overﬁtting.\nThe idea of label smoothing was ﬁrst proposed to train\nInception-v2 [26]. It changes the construction of the true\nprobability to\nqi =\n(\n1 −ε\nif i = y,\nε/(K −1)\notherwise,\n(4)\nwhere ε is a small constant. Now the optimal solution\nbecomes\nz∗\ni =\n(\nlog((K −1)(1 −ε)/ε) + α\nif i = y,\nα\notherwise,\n(5)\nwhere α can be an arbitrary real number. This encour-\nages a ﬁnite output from the fully-connected layer and can\ngeneralize better.\nWhen ε = 0, the gap log((K −1)(1 −ε)/ε) will be\n∞and as ε increases, the gap decreases. Speciﬁcally when\nε = (K −1)/K, all optimal z∗\ni will be identical. Figure 4a\nshows how the gap changes as we move ε, given K = 1000\nfor ImageNet dataset.\nWe empirically compare the output value from two\nResNet-50-D models that are trained with and without la-\nbel smoothing respectively and calculate the gap between\nthe maximum prediction value and the average of the rest.\nUnder ε = 0.1 and K = 1000, the theoretical gap is around\n9.1. Figure 4b demonstrate the gap distributions from the\ntwo models predicting over the validation set of ImageNet.\nIt is clear that with label smoothing the distribution centers\nat the theoretical value and has fewer extreme values.\n5.3. Knowledge Distillation\nIn knowledge distillation [10], we use a teacher model\nto help train the current model, which is called the student\nmodel. The teacher model is often a pre-trained model with\nhigher accuracy, so by imitation, the student model is able\nto improve its own accuracy while keeping the model com-\nplexity the same. One example is using a ResNet-152 as the\nteacher model to help training ResNet-50.\nDuring training, we add a distillation loss to penalize\nthe difference between the softmax outputs from the teacher\nmodel and the learner model. Given an input, assume p is\nthe true probability distribution, and z and r are outputs of\nthe last fully-connected layer of the student model and the\nteacher model, respectively. Remember previously we use a\n",
    "−5\n0\n5\n10\n15\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\neps\nGap\n(a) Theoretical gap\n0.0\n0.1\n0.2\n0.3\n0\n10\n20\n30\nGap\nDensityy\none−hot\nsmoothed\n(b) Empirical gap from ImageNet validation set\nFigure 4: Visualization of the effectiveness of label smooth-\ning on ImageNet. Top: theoretical gap between z∗\np and oth-\ners decreases when increasing ε. Bottom: The empirical\ndistributions of the gap between the maximum prediction\nand the average of the rest.\nnegative cross entropy loss ℓ(p, softmax(z)) to measure the\ndifference between p and z, here we use the same loss again\nfor the distillation. Therefore, the loss is changed to\nℓ(p, softmax(z)) + T 2ℓ(softmax(r/T), softmax(z/T)),\n(6)\nwhere T is the temperature hyper-parameter to make the\nsoftmax outputs smoother thus distill the knowledge of la-\nbel distribution from teacher’s prediction.\n5.4. Mixup Training\nIn Section 2.1 we described how images are augmented\nbefore training. Here we consider another augmentation\nmethod called mixup [29]. In mixup, each time we ran-\ndomly sample two examples (xi, yi) and (xj, yj). Then we\nform a new example by a weighted linear interpolation of\nthese two examples:\nˆx\n=\nλxi + (1 −λ)xj,\n(7)\nˆy\n=\nλyi + (1 −λ)yj,\n(8)\nwhere λ ∈[0, 1] is a random number drawn from the\nBeta(α, α) distribution. In mixup training, we only use\nthe new example (ˆx, ˆy).\n5.5. Experiment Results\nNow we evaluate the four training reﬁnements.\nWe\nset ε = 0.1 for label smoothing by following Szegedy et\nal. [26]. For the model distillation we use T = 20, specif-\nically a pretrained ResNet-152-D model with both cosine\ndecay and label smoothing applied is used as the teacher.\nIn the mixup training, we choose α = 0.2 in the Beta dis-\ntribution and increase the number of epochs from 120 to\n200 because the mixed examples ask for a longer training\nprogress to converge better. When combining the mixup\ntraining with distillation, we train the teacher model with\nmixup as well.\nWe demonstrate that the reﬁnements are not only lim-\nited to ResNet architecture or the ImageNet dataset. First,\nwe train ResNet-50-D, Inception-V3 and MobileNet on Im-\nageNet dataset with reﬁnements.\nThe validation accura-\ncies for applying these training reﬁnements one-by-one are\nshown in Table 6. By stacking cosine decay, label smooth-\ning and mixup, we have steadily improving ResNet, Incep-\ntionV3 and MobileNet models. Distillation works well on\nResNet, however, it does not work well on Inception-V3\nand MobileNet. Our interpretation is that the teacher model\nis not from the same family of the student, therefore has\ndifferent distribution in the prediction, and brings negative\nimpact to the model.\nTo support our tricks is transferable to other dataset, we\ntrain a ResNet-50-D model on MIT Places365 dataset with\nand without the reﬁnements. Results are reported in Ta-\nble 7. We see the reﬁnements improve the top-5 accuracy\nconsistently on both the validation and test set.\n6. Transfer Learning\nTransfer learning is one major down-streaming use case\nof trained image classiﬁcation models. In this section, we\nwill investigate if these improvements discussed so far can\nbeneﬁt transfer learning. In particular, we pick two impor-\ntant computer vision tasks, object detection and semantic\nsegmentation, and evaluate their performance by varying\nbase models.\n6.1. Object Detection\nThe goal of object detection is to locate bounding boxes\nof objects in an image.\nWe evaluate performance using\nPASCAL VOC [3]. Similar to Ren et al. [22], we use union\nset of VOC 2007 trainval and VOC 2012 trainval for train-\ning, and VOC 2007 test for evaluation, respectively. We\ntrain Faster-RCNN [22] on this dataset, with reﬁnements\nfrom Detectron [5] such as linear warmup and long train-\ning schedule. The VGG-19 base model in Faster-RCNN\nis replaced with various pretrained models in the previous\ndiscussion. We keep other settings the same so the gain is\nsolely from the base models.\nMean average precision (mAP) results are reported in\nTable 8. We can observe that a base model with a higher\nvalidation accuracy leads to a higher mAP for Faster-RNN\nin a consistent manner. In particular, the best base model\nwith accuracy 79.29% on ImageNet leads to the best mAP\n",
    "Reﬁnements\nResNet-50-D\nInception-V3\nMobileNet\nTop-1\nTop-5\nTop-1\nTop-5\nTop-1\nTop-5\nEfﬁcient\n77.16\n93.52\n77.50\n93.60\n71.90\n90.53\n+ cosine decay\n77.91\n93.81\n78.19\n94.06\n72.83\n91.00\n+ label smoothing\n78.31\n94.09\n78.40\n94.13\n72.93\n91.14\n+ distill w/o mixup\n78.67\n94.36\n78.26\n94.01\n71.97\n90.89\n+ mixup w/o distill\n79.15\n94.58\n78.77\n94.39\n73.28\n91.30\n+ distill w/ mixup\n79.29\n94.63\n78.34\n94.16\n72.51\n91.02\nTable 6: The validation accuracies on ImageNet for stacking training reﬁnements one by one. The baseline models are\nobtained from Section 3.\nModel\nVal Top-1 Acc\nVal Top-5 Acc\nTest Top-1 Acc\nTest Top-5 Acc\nResNet-50-D Efﬁcient\n56.34\n86.87\n57.18\n87.28\nResNet-50-D Best\n56.70\n87.33\n57.63\n87.82\nTable 7: Results on both the validation set and the test set of MIT Places 365 dataset. Prediction are generated as stated\nin Section 2.1. ResNet-50-D Efﬁcient refers to ResNet-50-D trained with settings from Section 3, and ResNet-50-D Best\nfurther incorporate cosine scheduling, label smoothing and mixup.\nReﬁnement\nTop-1\nmAP\nB-standard\n76.14\n77.54\nD-efﬁcient\n77.16\n78.30\n+ cosine\n77.91\n79.23\n+ smooth\n78.34\n80.71\n+ distill w/o mixup\n78.67\n80.96\n+ mixup w/o distill\n79.16\n81.10\n+ distill w/ mixup\n79.29\n81.33\nTable 8:\nFaster-RCNN performance with various pre-\ntrained base networks evaluated on Pascal VOC.\nReﬁnement\nTop-1\nPixAcc\nmIoU\nB-standard\n76.14\n78.08\n37.05\nD-efﬁcient\n77.16\n78.88\n38.88\n+ cosine\n77.91\n79.25\n39.33\n+ smooth\n78.34\n78.64\n38.75\n+ distill w/o mixup\n78.67\n78.97\n38.90\n+ mixup w/o distill\n79.16\n78.47\n37.99\n+ mixup w/ distill\n79.29\n78.72\n38.40\nTable 9: FCN performance with various base networks eval-\nuated on ADE20K.\nat 81.33% on VOC, which outperforms the standard model\nby 4%.\n6.2. Semantic Segmentation\nSemantic segmentation predicts the category for every\npixel from the input images. We use Fully Convolutional\nNetwork (FCN) [17] for this task and train models on the\nADE20K [33] dataset. Following PSPNet [31] and Zhang et\nal. [30], we replace the base network with various pre-\ntrained models discussed in previous sections and apply di-\nlation network strategy [2, 28] on stage-3 and stage-4. A\nfully convolutional decoder is built on top of the base net-\nwork to make the ﬁnal prediction.\nBoth pixel accuracy (pixAcc) and mean intersection over\nunion (mIoU) are reported in Table 9.\nIn contradiction\nto our results on object detection, the cosine learning rate\nschedule effectively improves the accuracy of the FCN per-\nformance, while other reﬁnements provide suboptimal re-\nsults. A potential explanation to the phenomenon is that\nsemantic segmentation predicts in the pixel level. While\nmodels trained with label smoothing, distillation and mixup\nfavor soften labels, blurred pixel-level information may be\nblurred and degrade overall pixel-level accuracy.\n7. Conclusion\nIn this paper, we survey a dozen tricks to train deep\nconvolutional neural networks to improve model accuracy.\nThese tricks introduce minor modiﬁcations to the model\narchitecture, data preprocessing, loss function, and learn-\ning rate schedule.\nOur empirical results on ResNet-50,\nInception-V3 and MobileNet indicate that these tricks im-\nprove model accuracy consistently. More excitingly, stack-\ning all of them together leads to a signiﬁcantly higher accu-\nracy. In addition, these improved pre-trained models show\n",
    "strong advantages in transfer learning, which improve both\nobject detection and semantic segmentation. We believe the\nbeneﬁts can extend to broader domains where classiﬁcation\nbase models are favored.\nReferences\n[1] L. Chen, G. Papandreou, F. Schroff, and H. Adam.\nRe-\nthinking atrous convolution for semantic image segmenta-\ntion. CoRR, abs/1706.05587, 2017. 1, 2, 5\n[2] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and\nA. L. Yuille. Deeplab: Semantic image segmentation with\ndeep convolutional nets, atrous convolution, and fully con-\nnected crfs. IEEE transactions on pattern analysis and ma-\nchine intelligence, 40(4):834–848, 2018. 8\n[3] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,\nand A. Zisserman.\nThe PASCAL Visual Object Classes\nChallenge 2007 (VOC2007) Results.\nhttp://www.pascal-\nnetwork.org/challenges/VOC/voc2007/workshop/index.html.\n7\n[4] B. Ginsburg, I. Gitman, and Y. You. Large batch training of\nconvolutional networks with layer-wise adaptive rate scaling.\n2018. 3\n[5] R. Girshick,\nI. Radosavovic,\nG. Gkioxari,\nP. Doll´ar,\nand K. He.\nDetectron.\nhttps://github.com/\nfacebookresearch/detectron, 2018. 7\n[6] X. Glorot and Y. Bengio.\nUnderstanding the difﬁculty of\ntraining deep feedforward neural networks. In Proceedings\nof the thirteenth international conference on artiﬁcial intel-\nligence and statistics, pages 249–256, 2010. 2\n[7] P. Goyal,\nP. Doll´ar,\nR. B. Girshick,\nP. Noordhuis,\nL. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He.\nAccurate, large minibatch SGD: training imagenet in 1 hour.\nCoRR, abs/1706.02677, 2017. 3, 4\n[8] S. Gross and M. Wilber. Training and investigating residual\nnets. http://torch.ch/blog/2016/02/04/resnets.html. 2, 4\n[9] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n770–778, 2016. 1, 2, 3, 4, 5\n[10] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge\nin a neural network. arXiv preprint arXiv:1503.02531, 2015.\n6\n[11] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\nT. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efﬁ-\ncient convolutional neural networks for mobile vision appli-\ncations. arXiv preprint arXiv:1704.04861, 2017. 1, 2\n[12] J. Hu, L. Shen, and G. Sun.\nSqueeze-and-excitation net-\nworks. arXiv preprint arXiv:1709.01507, 7, 2017. 1, 4, 5\n[13] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Wein-\nberger. Densely connected convolutional networks. In 2017\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 2261–2269. IEEE, 2017. 1\n[14] X. Jia, S. Song, W. He, Y. Wang, H. Rong, F. Zhou, L. Xie,\nZ. Guo, Y. Yang, L. Yu, et al. Highly scalable deep learning\ntraining system with mixed-precision: Training imagenet in\nfour minutes. arXiv preprint arXiv:1807.11205, 2018. 3\n[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImagenet\nclassiﬁcation with deep convolutional neural networks. In\nAdvances in neural information processing systems, pages\n1097–1105, 2012. 1\n[16] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv\npreprint arXiv:1312.4400, 2013. 1\n[17] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional\nnetworks for semantic segmentation. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 3431–3440, 2015. 8\n[18] I. Loshchilov and F. Hutter. SGDR: stochastic gradient de-\nscent with restarts. CoRR, abs/1608.03983, 2016. 5\n[19] P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen,\nD. Garcia,\nB. Ginsburg,\nM. Houston,\nO. Kuchaev,\nG. Venkatesh, et al. Mixed precision training. arXiv preprint\narXiv:1710.03740, 2017. 3\n[20] Y. E. Nesterov. A method for solving the convex program-\nming problem with convergence rate o (1/kˆ 2).\nIn Dokl.\nAkad. Nauk SSSR, volume 269, pages 543–547, 1983. 2\n[21] H.-T. Z. Ningning Ma, Xiangyu Zhang and J. Sun. Shufﬂenet\nv2: Practical guidelines for efﬁcient cnn architecture design.\narXiv preprint arXiv:1807.11164, 2018. 5\n[22] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards\nreal-time object detection with region proposal networks. In\nAdvances in neural information processing systems, pages\n91–99, 2015. 7\n[23] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\net al.\nImagenet large scale visual recognition challenge.\nInternational Journal of Computer Vision, 115(3):211–252,\n2015. 1, 2\n[24] K. Simonyan and A. Zisserman.\nVery deep convolu-\ntional networks for large-scale image recognition.\nCoRR,\nabs/1409.1556, 2014. 1\n[25] S. L. Smith, P.-J. Kindermans, C. Ying, and Q. V. Le. Don’t\ndecay the learning rate, increase the batch size.\narXiv\npreprint arXiv:1711.00489, 2017. 3\n[26] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.\nRethinking the inception architecture for computer vision.\nCoRR, abs/1512.00567, 2015. 2, 4, 5, 6, 7\n[27] S. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He. Aggregated\nresidual transformations for deep neural networks. In Com-\nputer Vision and Pattern Recognition (CVPR), 2017 IEEE\nConference on, pages 5987–5995. IEEE, 2017. 1, 4\n[28] F. Yu and V. Koltun. Multi-scale context aggregation by di-\nlated convolutions. arXiv preprint arXiv:1511.07122, 2015.\n8\n[29] H. Zhang, M. Ciss´e, Y. N. Dauphin, and D. Lopez-\nPaz. mixup: Beyond empirical risk minimization. CoRR,\nabs/1710.09412, 2017. 7\n[30] H. Zhang, K. Dana, J. Shi, Z. Zhang, X. Wang, A. Tyagi, and\nA. Agrawal. Context encoding for semantic segmentation.\nIn The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2018. 8\n[31] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene\nparsing network. In Computer Vision and Pattern Recogni-\ntion (CVPR), 2017 IEEE Conference on, pages 6230–6239.\nIEEE, 2017. 5, 8\n",
    "[32] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba.\nPlaces: A 10 million image database for scene recognition.\nIEEE transactions on pattern analysis and machine intelli-\ngence, 2017. 1\n[33] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Tor-\nralba. Scene parsing through ade20k dataset. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2017. 8\n[34] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le.\nLearn-\ning transferable architectures for scalable image recognition.\nCoRR, abs/1707.07012, 2017. 1\n"
  ],
  "full_text": "Bag of Tricks for Image Classiﬁcation with Convolutional Neural Networks\nTong He\nZhi Zhang\nHang Zhang\nZhongyue Zhang\nJunyuan Xie\nMu Li\nAmazon Web Services\n{htong,zhiz,hzaws,zhongyue,junyuanx,mli}@amazon.com\nAbstract\nMuch of the recent progress made in image classiﬁcation\nresearch can be credited to training procedure reﬁnements,\nsuch as changes in data augmentations and optimization\nmethods. In the literature, however, most reﬁnements are ei-\nther brieﬂy mentioned as implementation details or only vis-\nible in source code. In this paper, we will examine a collec-\ntion of such reﬁnements and empirically evaluate their im-\npact on the ﬁnal model accuracy through ablation study. We\nwill show that, by combining these reﬁnements together, we\nare able to improve various CNN models signiﬁcantly. For\nexample, we raise ResNet-50’s top-1 validation accuracy\nfrom 75.3% to 79.29% on ImageNet. We will also demon-\nstrate that improvement on image classiﬁcation accuracy\nleads to better transfer learning performance in other ap-\nplication domains such as object detection and semantic\nsegmentation.\n1. Introduction\nSince the introduction of AlexNet [15] in 2012, deep\nconvolutional neural networks have become the dominat-\ning approach for image classiﬁcation. Various new architec-\ntures have been proposed since then, including VGG [24],\nNiN [16], Inception [1], ResNet [9], DenseNet [13], and\nNASNet [34]. At the same time, we have seen a steady\ntrend of model accuracy improvement. For example, the\ntop-1 validation accuracy on ImageNet [23] has been raised\nfrom 62.5% (AlexNet) to 82.7% (NASNet-A).\nHowever, these advancements did not solely come from\nimproved model architecture. Training procedure reﬁne-\nments, including changes in loss functions, data preprocess-\ning, and optimization methods also played a major role. A\nlarge number of such reﬁnements has been proposed in the\npast years, but has received relatively less attention. In the\nliterature, most were only brieﬂy mentioned as implemen-\ntation details while others can only be found in source code.\nIn this paper, we will examine a collection of training\nModel\nFLOPs\ntop-1\ntop-5\nResNet-50 [9]\n3.9 G\n75.3\n92.2\nResNeXt-50 [27]\n4.2 G\n77.8\n-\nSE-ResNet-50 [12]\n3.9 G\n76.71\n93.38\nSE-ResNeXt-50 [12]\n4.3 G\n78.90\n94.51\nDenseNet-201 [13]\n4.3 G\n77.42\n93.66\nResNet-50 + tricks (ours)\n4.3 G\n79.29\n94.63\nTable 1: Computational costs and validation accuracy of\nvarious models. ResNet, trained with our “tricks”, is able\nto outperform newer and improved architectures trained\nwith standard pipeline.\nprocedure and model architecture reﬁnements that improve\nmodel accuracy but barely change computational complex-\nity. Many of them are minor “tricks” like modifying the\nstride size of a particular convolution layer or adjusting\nlearning rate schedule. Collectively, however, they make a\nbig difference. We will evaluate them on multiple network\narchitectures and datasets and report their impact to the ﬁnal\nmodel accuracy.\nOur empirical evaluation shows that several tricks lead\nto signiﬁcant accuracy improvement and combining them\ntogether can further boost the model accuracy. We com-\npare ResNet-50, after applying all tricks, to other related\nnetworks in Table 1. Note that these tricks raises ResNet-\n50’s top-1 validation accuracy from 75.3% to 79.29% on\nImageNet. It also outperforms other newer and improved\nnetwork architectures, such as SE-ResNeXt-50. In addi-\ntion, we show that our approach can generalize to other net-\nworks (Inception V3 [1] and MobileNet [11]) and datasets\n(Place365 [32]). We further show that models trained with\nour tricks bring better transfer learning performance in other\napplication domains such as object detection and semantic\nsegmentation.\nPaper Outline.\nWe ﬁrst set up a baseline training proce-\ndure in Section 2, and then discuss several tricks that are\n1\narXiv:1812.01187v2  [cs.CV]  5 Dec 2018\n\n\nAlgorithm 1 Train a neural network with mini-batch\nstochastic gradient descent.\ninitialize(net)\nfor epoch = 1, . . . , K do\nfor batch = 1, . . . , #images/b do\nimages ←uniformly random sample b images\nX, y ←preprocess(images)\nz ←forward(net, X)\nℓ←loss(z, y)\ngrad ←backward(ℓ)\nupdate(net, grad)\nend for\nend for\nuseful for efﬁcient training on new hardware in Section 3. In\nSection 4 we review three minor model architecture tweaks\nfor ResNet and propose a new one. Four additional train-\ning procedure reﬁnements are then discussed in Section 5.\nAt last, we study if these more accurate models can help\ntransfer learning in Section 6.\nOur model implementations and training scripts are pub-\nlicly available in GluonCV 1.\n2. Training Procedures\nThe template of training a neural network with mini-\nbatch stochastic gradient descent is shown in Algorithm 1.\nIn each iteration, we randomly sample b images to com-\npute the gradients and then update the network parameters.\nIt stops after K passes through the dataset. All functions\nand hyper-parameters in Algorithm 1 can be implemented\nin many different ways. In this section, we ﬁrst specify a\nbaseline implementation of Algorithm 1.\n2.1. Baseline Training Procedure\nWe follow a widely used implementation [8] of ResNet\nas our baseline. The preprocessing pipelines between train-\ning and validation are different. During training, we per-\nform the following steps one-by-one:\n1. Randomly sample an image and decode it into 32-bit\nﬂoating point raw pixel values in [0, 255].\n2. Randomly crop a rectangular region whose aspect ratio\nis randomly sampled in [3/4, 4/3] and area randomly\nsampled in [8%, 100%], then resize the cropped region\ninto a 224-by-224 square image.\n3. Flip horizontally with 0.5 probability.\n4. Scale hue, saturation, and brightness with coefﬁcients\nuniformly drawn from [0.6, 1.4].\n5. Add PCA noise with a coefﬁcient sampled from a nor-\nmal distribution N(0, 0.1).\n1https://github.com/dmlc/gluon-cv\nModel\nBaseline\nReference\nTop-1\nTop-5\nTop-1\nTop-5\nResNet-50 [9]\n75.87\n92.70\n75.3\n92.2\nInception-V3 [26]\n77.32\n93.43\n78.8\n94.4\nMobileNet [11]\n69.03\n88.71\n70.6\n-\nTable 2: Validation accuracy of reference implementa-\ntions and our baseline. Note that the numbers for Incep-\ntion V3 are obtained with 299-by-299 input images.\n6. Normalize RGB channels by subtracting 123.68,\n116.779, 103.939 and dividing by 58.393, 57.12,\n57.375, respectively.\nDuring validation, we resize each image’s shorter edge\nto 256 pixels while keeping its aspect ratio. Next, we crop\nout the 224-by-224 region in the center and normalize RGB\nchannels similar to training. We do not perform any random\naugmentations during validation.\nThe weights of both convolutional and fully-connected\nlayers are initialized with the Xavier algorithm [6]. In par-\nticular, we set the parameter to random values uniformly\ndrawn from [−a, a], where a =\np\n6/(din + dout). Here\ndin and dout are the input and output channel sizes, respec-\ntively. All biases are initialized to 0. For batch normaliza-\ntion layers, γ vectors are initialized to 1 and β vectors to\n0.\nNesterov Accelerated Gradient (NAG) descent [20] is\nused for training. Each model is trained for 120 epochs on\n8 Nvidia V100 GPUs with a total batch size of 256. The\nlearning rate is initialized to 0.1 and divided by 10 at the\n30th, 60th, and 90th epochs.\n2.2. Experiment Results\nWe evaluate three CNNs:\nResNet-50 [9], Inception-\nV3 [1], and MobileNet [11]. For Inception-V3 we resize the\ninput images into 299x299. We use the ISLVRC2012 [23]\ndataset, which has 1.3 million images for training and 1000\nclasses. The validation accuracies are shown in Table 2. As\ncan be seen, our ResNet-50 results are slightly better than\nthe reference results, while our baseline Inception-V3 and\nMobileNet are slightly lower in accuracy due to different\ntraining procedure.\n3. Efﬁcient Training\nHardware, especially GPUs, has been rapidly evolving\nin recent years. As a result, the optimal choices for many\nperformance related trade-offs have changed. For example,\nit is now more efﬁcient to use lower numerical precision and\nlarger batch sizes during training. In this section, we review\nvarious techniques that enable low precision and large batch\n\n\ntraining without sacriﬁcing model accuracy.\nSome tech-\nniques can even improve both accuracy and training speed.\n3.1. Large-batch training\nMini-batch SGD groups multiple samples to a mini-\nbatch to increase parallelism and decrease communication\ncosts. Using large batch size, however, may slow down\nthe training progress. For convex problems, convergence\nrate decreases as batch size increases. Similar empirical re-\nsults have been reported for neural networks [25]. In other\nwords, for the same number of epochs, training with a large\nbatch size results in a model with degraded validation accu-\nracy compared to the ones trained with smaller batch sizes.\nMultiple works [7, 14] have proposed heuristics to solve\nthis issue. In the following paragraphs, we will examine\nfour heuristics that help scale the batch size up for single\nmachine training.\nLinear scaling learning rate.\nIn mini-batch SGD, gradi-\nent descending is a random process because the examples\nare randomly selected in each batch. Increasing the batch\nsize does not change the expectation of the stochastic gra-\ndient but reduces its variance. In other words, a large batch\nsize reduces the noise in the gradient, so we may increase\nthe learning rate to make a larger progress along the op-\nposite of the gradient direction. Goyal et al. [7] reports\nthat linearly increasing the learning rate with the batch size\nworks empirically for ResNet-50 training. In particular, if\nwe follow He et al. [9] to choose 0.1 as the initial learn-\ning rate for batch size 256, then when changing to a larger\nbatch size b, we will increase the initial learning rate to\n0.1 × b/256.\nLearning rate warmup.\nAt the beginning of the training,\nall parameters are typically random values and therefore far\naway from the ﬁnal solution. Using a too large learning rate\nmay result in numerical instability. In the warmup heuristic,\nwe use a small learning rate at the beginning and then switch\nback to the initial learning rate when the training process\nis stable [9]. Goyal et al. [7] proposes a gradual warmup\nstrategy that increases the learning rate from 0 to the initial\nlearning rate linearly. In other words, assume we will use\nthe ﬁrst m batches (e.g. 5 data epochs) to warm up, and the\ninitial learning rate is η, then at batch i, 1 ≤i ≤m, we will\nset the learning rate to be iη/m.\nZero γ.\nA ResNet network consists of multiple residual\nblocks, each block consists of several convolutional lay-\ners. Given input x, assume block(x) is the output for the\nlast layer in the block, this residual block then outputs\nx + block(x). Note that the last layer of a block could\nbe a batch normalization (BN) layer. The BN layer ﬁrst\nstandardizes its input, denoted by ˆx, and then performs a\nscale transformation γˆx + β. Both γ and β are learnable\nparameters whose elements are initialized to 1s and 0s, re-\nspectively. In the zero γ initialization heuristic, we initialize\nγ = 0 for all BN layers that sit at the end of a residual block.\nTherefore, all residual blocks just return their inputs, mim-\nics network that has less number of layers and is easier to\ntrain at the initial stage.\nNo bias decay.\nThe weight decay is often applied to all\nlearnable parameters including both weights and bias. It’s\nequivalent to applying an L2 regularization to all parame-\nters to drive their values towards 0. As pointed out by Jia et\nal. [14], however, it’s recommended to only apply the reg-\nularization to weights to avoid overﬁtting. The no bias de-\ncay heuristic follows this recommendation, it only applies\nthe weight decay to the weights in convolution and fully-\nconnected layers. Other parameters, including the biases\nand γ and β in BN layers, are left unregularized.\nNote that LARS [4] offers layer-wise adaptive learning\nrate and is reported to be effective for extremely large batch\nsizes (beyond 16K). While in this paper we limit ourselves\nto methods that are sufﬁcient for single machine training,\nin which case a batch size no more than 2K often leads to\ngood system efﬁciency.\n3.2. Low-precision training\nNeural networks are commonly trained with 32-bit ﬂoat-\ning point (FP32) precision. That is, all numbers are stored in\nFP32 format and both inputs and outputs of arithmetic oper-\nations are FP32 numbers as well. New hardware, however,\nmay have enhanced arithmetic logic unit for lower precision\ndata types. For example, the previously mentioned Nvidia\nV100 offers 14 TFLOPS in FP32 but over 100 TFLOPS in\nFP16. As in Table 3, the overall training speed is acceler-\nated by 2 to 3 times after switching from FP32 to FP16 on\nV100.\nDespite the performance beneﬁt, a reduced precision has\na narrower range that makes results more likely to be out-of-\nrange and then disturb the training progress. Micikevicius et\nal. [19] proposes to store all parameters and activations in\nFP16 and use FP16 to compute gradients. At the same time,\nall parameters have an copy in FP32 for parameter updat-\ning. In addition, multiplying a scalar to the loss to better\nalign the range of the gradient into FP16 is also a practical\nsolution.\n3.3. Experiment Results\nThe evaluation results for ResNet-50 are shown in Ta-\nble 3. Compared to the baseline with batch size 256 and\nFP32, using a larger 1024 batch size and FP16 reduces the\ntraining time for ResNet-50 from 13.3-min per epoch to 4.4-\nmin per epoch. In addition, by stacking all heuristics for\n\n\nInput stem\nStage 1\nStage 2\nStage 3\nStage 4\nOutput\nInput\nOutput\nConv \n7x7, 64, s=2\nInput\nMaxPool \n3x3, s=2\nDown\nsampling\nResidual\nResidual\nConv\n1[\u0014\u000f\u0003\u0018\u0014\u0015\u000f\u0003V \u0015\nConv\n3x3, 512\nConv\n1x1, 2048\nConv\n1[\u0014\u000f\u0003\u0015\u0013\u0017\u001b\u000f\u0003V \u0015\nOutput\nInput\nInput\nOutput\nOutput\n+\nPath A\nPath B\nFigure 1: The architecture of ResNet-50. The convolution\nkernel size, output channel size and stride size (default is 1)\nare illustrated, similar for pooling layers.\nlarge-batch training, the model trained with 1024 batch size\nand FP16 even slightly increased 0.5% top-1 accuracy com-\npared to the baseline model.\nThe ablation study of all heuristics is shown in Table 4.\nIncreasing batch size from 256 to 1024 by linear scaling\nlearning rate alone leads to a 0.9% decrease of the top-1\naccuracy while stacking the rest three heuristics bridges the\ngap. Switching from FP32 to FP16 at the end of training\ndoes not affect the accuracy.\n4. Model Tweaks\nA model tweak is a minor adjustment to the network ar-\nchitecture, such as changing the stride of a particular convo-\nlution layer. Such a tweak often barely changes the compu-\ntational complexity but might have a non-negligible effect\non the model accuracy. In this section, we will use ResNet\nas an example to investigate the effects of model tweaks.\n4.1. ResNet Architecture\nWe will brieﬂy present the ResNet architecture, espe-\ncially its modules related to the model tweaks. For detailed\ninformation please refer to He et al. [9]. A ResNet network\nconsists of an input stem, four subsequent stages and a ﬁnal\noutput layer, which is illustrated in Figure 1. The input stem\nhas a 7 × 7 convolution with an output channel of 64 and a\nstride of 2, followed by a 3 × 3 max pooling layer also with\na stride of 2. The input stem reduces the input width and\nheight by 4 times and increases its channel size to 64.\nStarting from stage 2, each stage begins with a down-\nsampling block, which is then followed by several residual\nblocks. In the downsampling block, there are path A and\nConv\n\u000b1[\u0014\f\nConv\n\u000b3x3, s=2\f\nConv\n\u000b1x1\f\nConv\n\u000b1[\u0014\u000f\u0003V \u0015\f\nInput\nOutput\n+\n(a) ResNet-B\nConv \n(3x3)\nInput\nMaxPool \n(3x3, s=2)\nOutput\nConv \n(3x3, s=2)\nConv \n(3x3)\n(b) ResNet-C\nConv\n\u000b1[\u0014\f\nConv\n\u000b3x3, s=2\f\nConv\n\u000b1x1\f\nConv\n\u000b1[\u0014\f\nInput\nOutput\n+\nAvgPool\n\u000b2[\u0015\u000f\u0003V \u0015\f\n(c) ResNet-D\nFigure 2: Three ResNet tweaks. ResNet-B modiﬁes the\ndownsampling block of Resnet. ResNet-C further modiﬁes\nthe input stem. On top of that, ResNet-D again modiﬁes the\ndownsampling block.\npath B. Path A has three convolutions, whose kernel sizes\nare 1×1, 3×3 and 1×1, respectively. The ﬁrst convolution\nhas a stride of 2 to halve the input width and height, and the\nlast convolution’s output channel is 4 times larger than the\nprevious two, which is called the bottleneck structure. Path\nB uses a 1×1 convolution with a stride of 2 to transform the\ninput shape to be the output shape of path A, so we can sum\noutputs of both paths to obtain the output of the downsam-\npling block. A residual block is similar to a downsampling\nblock except for only using convolutions with a stride of 1.\nOne can vary the number of residual blocks in each stage\nto obtain different ResNet models, such as ResNet-50 and\nResNet-152, where the number presents the number of con-\nvolutional layers in the network.\n4.2. ResNet Tweaks\nNext, we revisit two popular ResNet tweaks, we call\nthem ResNet-B and ResNet-C, respectively. We propose\na new model tweak ResNet-D afterwards.\nResNet-B.\nThis tweak ﬁrst appeared in a Torch imple-\nmentation of ResNet [8] and then adopted by multiple\nworks [7, 12, 27]. It changes the downsampling block of\nResNet. The observation is that the convolution in path A\nignores three-quarters of the input feature map because it\nuses a kernel size 1×1 with a stride of 2. ResNet-B switches\nthe strides size of the ﬁrst two convolutions in path A, as\nshown in Figure 2a, so no information is ignored. Because\nthe second convolution has a kernel size 3 × 3, the output\nshape of path A remains unchanged.\nResNet-C.\nThis tweak was proposed in Inception-v2 [26]\noriginally, and it can be found on the implementations\n\n\nModel\nEfﬁcient\nBaseline\nTime/epoch\nTop-1\nTop-5\nTime/epoch\nTop-1\nTop-5\nResNet-50\n4.4 min\n76.21\n92.97\n13.3 min\n75.87\n92.70\nInception-V3\n8 min\n77.50\n93.60\n19.8 min\n77.32\n93.43\nMobileNet\n3.7 min\n71.90\n90.47\n6.2 min\n69.03\n88.71\nTable 3: Comparison of the training time and validation accuracy for ResNet-50 between the baseline (BS=256 with FP32)\nand a more hardware efﬁcient setting (BS=1024 with FP16).\nHeuristic\nBS=256\nBS=1024\nTop-1\nTop-5\nTop-1\nTop-5\nLinear scaling\n75.87\n92.70\n75.17\n92.54\n+ LR warmup\n76.03\n92.81\n75.93\n92.84\n+ Zero γ\n76.19\n93.03\n76.37\n92.96\n+ No bias decay\n76.16\n92.97\n76.03\n92.86\n+ FP16\n76.15\n93.09\n76.21\n92.97\nTable 4: The breakdown effect for each effective training\nheuristic on ResNet-50.\nof other models, such as SENet [12], PSPNet [31],\nDeepLabV3 [1], and ShufﬂeNetV2 [21]. The observation\nis that the computational cost of a convolution is quadratic\nto the kernel width or height. A 7 × 7 convolution is 5.4\ntimes more expensive than a 3 × 3 convolution. So this\ntweak replacing the 7 × 7 convolution in the input stem\nwith three conservative 3 × 3 convolutions, which is shown\nin Figure 2b, with the ﬁrst and second convolutions have\ntheir output channel of 32 and a stride of 2, while the last\nconvolution uses a 64 output channel.\nResNet-D.\nInspired by ResNet-B, we note that the 1 × 1\nconvolution in the path B of the downsampling block also\nignores 3/4 of input feature maps, we would like to modify\nit so no information will be ignored. Empirically, we found\nadding a 2×2 average pooling layer with a stride of 2 before\nthe convolution, whose stride is changed to 1, works well\nin practice and impacts the computational cost little. This\ntweak is illustrated in Figure 2c.\n4.3. Experiment Results\nWe evaluate ResNet-50 with the three tweaks and set-\ntings described in Section 3, namely the batch size is 1024\nand precision is FP16. The results are shown in Table 5.\nSuggested by the results, ResNet-B receives more infor-\nmation in path A of the downsampling blocks and improves\nvalidation accuracy by around 0.5% compared to ResNet-\n50. Replacing the 7 × 7 convolution with three 3 × 3 ones\ngives another 0.2% improvement. Taking more information\nin path B of the downsampling blocks improves the vali-\nModel\n#params\nFLOPs\nTop-1\nTop-5\nResNet-50\n25 M\n3.8 G\n76.21\n92.97\nResNet-50-B\n25 M\n4.1 G\n76.66\n93.28\nResNet-50-C\n25 M\n4.3 G\n76.87\n93.48\nResNet-50-D\n25 M\n4.3 G\n77.16\n93.52\nTable 5: Compare ResNet-50 with three model tweaks on\nmodel size, FLOPs and ImageNet validation accuracy.\ndation accuracy by another 0.3%. In total, ResNet-50-D\nimproves ResNet-50 by 1%.\nOn the other hand, these four models have the same\nmodel size. ResNet-D has the largest computational cost,\nbut its difference compared to ResNet-50 is within 15% in\nterms of ﬂoating point operations. In practice, we observed\nResNet-50-D is only 3% slower in training throughput com-\npared to ResNet-50.\n5. Training Reﬁnements\nIn this section, we will describe four training reﬁnements\nthat aim to further improve the model accuracy.\n5.1. Cosine Learning Rate Decay\nLearning rate adjustment is crucial to the training. Af-\nter the learning rate warmup described in Section 3.1, we\ntypically steadily decrease the value from the initial learn-\ning rate. The widely used strategy is exponentially decaying\nthe learning rate. He et al. [9] decreases rate at 0.1 for ev-\nery 30 epochs, we call it “step decay”. Szegedy et al. [26]\ndecreases rate at 0.94 for every two epochs.\nIn contrast to it, Loshchilov et al. [18] propose a cosine\nannealing strategy. An simpliﬁed version is decreasing the\nlearning rate from the initial value to 0 by following the\ncosine function. Assume the total number of batches is T\n(the warmup stage is ignored), then at batch t, the learning\nrate ηt is computed as:\nηt = 1\n2\n\u0012\n1 + cos\n\u0012tπ\nT\n\u0013\u0013\nη,\n(1)\nwhere η is the initial learning rate. We call this scheduling\nas “cosine” decay.\n\n\n0.0\n0.1\n0.2\n0.3\n0.4\n0\n20\n40\n60\n80\n100\n120\nEpoch\nLearning Rate\nCosine Decay\nStep Decay\n(a) Learning Rate Schedule\n0.0\n0.2\n0.4\n0.6\n0.8\n0\n20\n40\n60\n80\n100\n120\nEpoch\nTop−1 Accuracy\nCosine Decay\nStep Decay\n(b) Validation Accuracy\nFigure 3:\nVisualization of learning rate schedules with\nwarm-up. Top: cosine and step schedules for batch size\n1024. Bottom: Top-1 validation accuracy curve with regard\nto the two schedules.\nThe comparison between step decay and cosine decay\nare illustrated in Figure 3a. As can be seen, the cosine decay\ndecreases the learning rate slowly at the beginning, and then\nbecomes almost linear decreasing in the middle, and slows\ndown again at the end. Compared to the step decay, the\ncosine decay starts to decay the learning since the beginning\nbut remains large until step decay reduces the learning rate\nby 10x, which potentially improves the training progress.\n5.2. Label Smoothing\nThe last layer of a image classiﬁcation network is often a\nfully-connected layer with a hidden size being equal to the\nnumber of labels, denote by K, to output the predicted con-\nﬁdence scores. Given an image, denote by zi the predicted\nscore for class i. These scores can be normalized by the\nsoftmax operator to obtain predicted probabilities. Denote\nby q the output of the softmax operator q = softmax(z), the\nprobability for class i, qi, can be computed by:\nqi =\nexp(zi)\nPK\nj=1 exp(zj)\n.\n(2)\nIt’s easy to see qi > 0 and PK\ni=1 qi = 1, so q is a valid\nprobability distribution.\nOn the other hand, assume the true label of this image\nis y, we can construct a truth probability distribution to be\npi = 1 if i = y and 0 otherwise. During training, we mini-\nmize the negative cross entropy loss\nℓ(p, q) = −\nK\nX\ni=1\nqi log pi\n(3)\nto update model parameters to make these two probabil-\nity distributions similar to each other. In particular, by the\nway how p is constructed, we know ℓ(p, q) = −log py =\n−zy + log\n\u0010PK\ni=1 exp(zi)\n\u0011\n. The optimal solution is z∗\ny =\ninf while keeping others small enough. In other words, it\nencourages the output scores dramatically distinctive which\npotentially leads to overﬁtting.\nThe idea of label smoothing was ﬁrst proposed to train\nInception-v2 [26]. It changes the construction of the true\nprobability to\nqi =\n(\n1 −ε\nif i = y,\nε/(K −1)\notherwise,\n(4)\nwhere ε is a small constant. Now the optimal solution\nbecomes\nz∗\ni =\n(\nlog((K −1)(1 −ε)/ε) + α\nif i = y,\nα\notherwise,\n(5)\nwhere α can be an arbitrary real number. This encour-\nages a ﬁnite output from the fully-connected layer and can\ngeneralize better.\nWhen ε = 0, the gap log((K −1)(1 −ε)/ε) will be\n∞and as ε increases, the gap decreases. Speciﬁcally when\nε = (K −1)/K, all optimal z∗\ni will be identical. Figure 4a\nshows how the gap changes as we move ε, given K = 1000\nfor ImageNet dataset.\nWe empirically compare the output value from two\nResNet-50-D models that are trained with and without la-\nbel smoothing respectively and calculate the gap between\nthe maximum prediction value and the average of the rest.\nUnder ε = 0.1 and K = 1000, the theoretical gap is around\n9.1. Figure 4b demonstrate the gap distributions from the\ntwo models predicting over the validation set of ImageNet.\nIt is clear that with label smoothing the distribution centers\nat the theoretical value and has fewer extreme values.\n5.3. Knowledge Distillation\nIn knowledge distillation [10], we use a teacher model\nto help train the current model, which is called the student\nmodel. The teacher model is often a pre-trained model with\nhigher accuracy, so by imitation, the student model is able\nto improve its own accuracy while keeping the model com-\nplexity the same. One example is using a ResNet-152 as the\nteacher model to help training ResNet-50.\nDuring training, we add a distillation loss to penalize\nthe difference between the softmax outputs from the teacher\nmodel and the learner model. Given an input, assume p is\nthe true probability distribution, and z and r are outputs of\nthe last fully-connected layer of the student model and the\nteacher model, respectively. Remember previously we use a\n\n\n−5\n0\n5\n10\n15\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\neps\nGap\n(a) Theoretical gap\n0.0\n0.1\n0.2\n0.3\n0\n10\n20\n30\nGap\nDensityy\none−hot\nsmoothed\n(b) Empirical gap from ImageNet validation set\nFigure 4: Visualization of the effectiveness of label smooth-\ning on ImageNet. Top: theoretical gap between z∗\np and oth-\ners decreases when increasing ε. Bottom: The empirical\ndistributions of the gap between the maximum prediction\nand the average of the rest.\nnegative cross entropy loss ℓ(p, softmax(z)) to measure the\ndifference between p and z, here we use the same loss again\nfor the distillation. Therefore, the loss is changed to\nℓ(p, softmax(z)) + T 2ℓ(softmax(r/T), softmax(z/T)),\n(6)\nwhere T is the temperature hyper-parameter to make the\nsoftmax outputs smoother thus distill the knowledge of la-\nbel distribution from teacher’s prediction.\n5.4. Mixup Training\nIn Section 2.1 we described how images are augmented\nbefore training. Here we consider another augmentation\nmethod called mixup [29]. In mixup, each time we ran-\ndomly sample two examples (xi, yi) and (xj, yj). Then we\nform a new example by a weighted linear interpolation of\nthese two examples:\nˆx\n=\nλxi + (1 −λ)xj,\n(7)\nˆy\n=\nλyi + (1 −λ)yj,\n(8)\nwhere λ ∈[0, 1] is a random number drawn from the\nBeta(α, α) distribution. In mixup training, we only use\nthe new example (ˆx, ˆy).\n5.5. Experiment Results\nNow we evaluate the four training reﬁnements.\nWe\nset ε = 0.1 for label smoothing by following Szegedy et\nal. [26]. For the model distillation we use T = 20, specif-\nically a pretrained ResNet-152-D model with both cosine\ndecay and label smoothing applied is used as the teacher.\nIn the mixup training, we choose α = 0.2 in the Beta dis-\ntribution and increase the number of epochs from 120 to\n200 because the mixed examples ask for a longer training\nprogress to converge better. When combining the mixup\ntraining with distillation, we train the teacher model with\nmixup as well.\nWe demonstrate that the reﬁnements are not only lim-\nited to ResNet architecture or the ImageNet dataset. First,\nwe train ResNet-50-D, Inception-V3 and MobileNet on Im-\nageNet dataset with reﬁnements.\nThe validation accura-\ncies for applying these training reﬁnements one-by-one are\nshown in Table 6. By stacking cosine decay, label smooth-\ning and mixup, we have steadily improving ResNet, Incep-\ntionV3 and MobileNet models. Distillation works well on\nResNet, however, it does not work well on Inception-V3\nand MobileNet. Our interpretation is that the teacher model\nis not from the same family of the student, therefore has\ndifferent distribution in the prediction, and brings negative\nimpact to the model.\nTo support our tricks is transferable to other dataset, we\ntrain a ResNet-50-D model on MIT Places365 dataset with\nand without the reﬁnements. Results are reported in Ta-\nble 7. We see the reﬁnements improve the top-5 accuracy\nconsistently on both the validation and test set.\n6. Transfer Learning\nTransfer learning is one major down-streaming use case\nof trained image classiﬁcation models. In this section, we\nwill investigate if these improvements discussed so far can\nbeneﬁt transfer learning. In particular, we pick two impor-\ntant computer vision tasks, object detection and semantic\nsegmentation, and evaluate their performance by varying\nbase models.\n6.1. Object Detection\nThe goal of object detection is to locate bounding boxes\nof objects in an image.\nWe evaluate performance using\nPASCAL VOC [3]. Similar to Ren et al. [22], we use union\nset of VOC 2007 trainval and VOC 2012 trainval for train-\ning, and VOC 2007 test for evaluation, respectively. We\ntrain Faster-RCNN [22] on this dataset, with reﬁnements\nfrom Detectron [5] such as linear warmup and long train-\ning schedule. The VGG-19 base model in Faster-RCNN\nis replaced with various pretrained models in the previous\ndiscussion. We keep other settings the same so the gain is\nsolely from the base models.\nMean average precision (mAP) results are reported in\nTable 8. We can observe that a base model with a higher\nvalidation accuracy leads to a higher mAP for Faster-RNN\nin a consistent manner. In particular, the best base model\nwith accuracy 79.29% on ImageNet leads to the best mAP\n\n\nReﬁnements\nResNet-50-D\nInception-V3\nMobileNet\nTop-1\nTop-5\nTop-1\nTop-5\nTop-1\nTop-5\nEfﬁcient\n77.16\n93.52\n77.50\n93.60\n71.90\n90.53\n+ cosine decay\n77.91\n93.81\n78.19\n94.06\n72.83\n91.00\n+ label smoothing\n78.31\n94.09\n78.40\n94.13\n72.93\n91.14\n+ distill w/o mixup\n78.67\n94.36\n78.26\n94.01\n71.97\n90.89\n+ mixup w/o distill\n79.15\n94.58\n78.77\n94.39\n73.28\n91.30\n+ distill w/ mixup\n79.29\n94.63\n78.34\n94.16\n72.51\n91.02\nTable 6: The validation accuracies on ImageNet for stacking training reﬁnements one by one. The baseline models are\nobtained from Section 3.\nModel\nVal Top-1 Acc\nVal Top-5 Acc\nTest Top-1 Acc\nTest Top-5 Acc\nResNet-50-D Efﬁcient\n56.34\n86.87\n57.18\n87.28\nResNet-50-D Best\n56.70\n87.33\n57.63\n87.82\nTable 7: Results on both the validation set and the test set of MIT Places 365 dataset. Prediction are generated as stated\nin Section 2.1. ResNet-50-D Efﬁcient refers to ResNet-50-D trained with settings from Section 3, and ResNet-50-D Best\nfurther incorporate cosine scheduling, label smoothing and mixup.\nReﬁnement\nTop-1\nmAP\nB-standard\n76.14\n77.54\nD-efﬁcient\n77.16\n78.30\n+ cosine\n77.91\n79.23\n+ smooth\n78.34\n80.71\n+ distill w/o mixup\n78.67\n80.96\n+ mixup w/o distill\n79.16\n81.10\n+ distill w/ mixup\n79.29\n81.33\nTable 8:\nFaster-RCNN performance with various pre-\ntrained base networks evaluated on Pascal VOC.\nReﬁnement\nTop-1\nPixAcc\nmIoU\nB-standard\n76.14\n78.08\n37.05\nD-efﬁcient\n77.16\n78.88\n38.88\n+ cosine\n77.91\n79.25\n39.33\n+ smooth\n78.34\n78.64\n38.75\n+ distill w/o mixup\n78.67\n78.97\n38.90\n+ mixup w/o distill\n79.16\n78.47\n37.99\n+ mixup w/ distill\n79.29\n78.72\n38.40\nTable 9: FCN performance with various base networks eval-\nuated on ADE20K.\nat 81.33% on VOC, which outperforms the standard model\nby 4%.\n6.2. Semantic Segmentation\nSemantic segmentation predicts the category for every\npixel from the input images. We use Fully Convolutional\nNetwork (FCN) [17] for this task and train models on the\nADE20K [33] dataset. Following PSPNet [31] and Zhang et\nal. [30], we replace the base network with various pre-\ntrained models discussed in previous sections and apply di-\nlation network strategy [2, 28] on stage-3 and stage-4. A\nfully convolutional decoder is built on top of the base net-\nwork to make the ﬁnal prediction.\nBoth pixel accuracy (pixAcc) and mean intersection over\nunion (mIoU) are reported in Table 9.\nIn contradiction\nto our results on object detection, the cosine learning rate\nschedule effectively improves the accuracy of the FCN per-\nformance, while other reﬁnements provide suboptimal re-\nsults. A potential explanation to the phenomenon is that\nsemantic segmentation predicts in the pixel level. While\nmodels trained with label smoothing, distillation and mixup\nfavor soften labels, blurred pixel-level information may be\nblurred and degrade overall pixel-level accuracy.\n7. Conclusion\nIn this paper, we survey a dozen tricks to train deep\nconvolutional neural networks to improve model accuracy.\nThese tricks introduce minor modiﬁcations to the model\narchitecture, data preprocessing, loss function, and learn-\ning rate schedule.\nOur empirical results on ResNet-50,\nInception-V3 and MobileNet indicate that these tricks im-\nprove model accuracy consistently. More excitingly, stack-\ning all of them together leads to a signiﬁcantly higher accu-\nracy. In addition, these improved pre-trained models show\n\n\nstrong advantages in transfer learning, which improve both\nobject detection and semantic segmentation. We believe the\nbeneﬁts can extend to broader domains where classiﬁcation\nbase models are favored.\nReferences\n[1] L. Chen, G. Papandreou, F. Schroff, and H. Adam.\nRe-\nthinking atrous convolution for semantic image segmenta-\ntion. CoRR, abs/1706.05587, 2017. 1, 2, 5\n[2] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and\nA. L. Yuille. Deeplab: Semantic image segmentation with\ndeep convolutional nets, atrous convolution, and fully con-\nnected crfs. IEEE transactions on pattern analysis and ma-\nchine intelligence, 40(4):834–848, 2018. 8\n[3] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,\nand A. Zisserman.\nThe PASCAL Visual Object Classes\nChallenge 2007 (VOC2007) Results.\nhttp://www.pascal-\nnetwork.org/challenges/VOC/voc2007/workshop/index.html.\n7\n[4] B. Ginsburg, I. Gitman, and Y. You. Large batch training of\nconvolutional networks with layer-wise adaptive rate scaling.\n2018. 3\n[5] R. Girshick,\nI. Radosavovic,\nG. Gkioxari,\nP. Doll´ar,\nand K. He.\nDetectron.\nhttps://github.com/\nfacebookresearch/detectron, 2018. 7\n[6] X. Glorot and Y. Bengio.\nUnderstanding the difﬁculty of\ntraining deep feedforward neural networks. In Proceedings\nof the thirteenth international conference on artiﬁcial intel-\nligence and statistics, pages 249–256, 2010. 2\n[7] P. Goyal,\nP. Doll´ar,\nR. B. Girshick,\nP. Noordhuis,\nL. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He.\nAccurate, large minibatch SGD: training imagenet in 1 hour.\nCoRR, abs/1706.02677, 2017. 3, 4\n[8] S. Gross and M. Wilber. Training and investigating residual\nnets. http://torch.ch/blog/2016/02/04/resnets.html. 2, 4\n[9] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n770–778, 2016. 1, 2, 3, 4, 5\n[10] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge\nin a neural network. arXiv preprint arXiv:1503.02531, 2015.\n6\n[11] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\nT. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efﬁ-\ncient convolutional neural networks for mobile vision appli-\ncations. arXiv preprint arXiv:1704.04861, 2017. 1, 2\n[12] J. Hu, L. Shen, and G. Sun.\nSqueeze-and-excitation net-\nworks. arXiv preprint arXiv:1709.01507, 7, 2017. 1, 4, 5\n[13] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Wein-\nberger. Densely connected convolutional networks. In 2017\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 2261–2269. IEEE, 2017. 1\n[14] X. Jia, S. Song, W. He, Y. Wang, H. Rong, F. Zhou, L. Xie,\nZ. Guo, Y. Yang, L. Yu, et al. Highly scalable deep learning\ntraining system with mixed-precision: Training imagenet in\nfour minutes. arXiv preprint arXiv:1807.11205, 2018. 3\n[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImagenet\nclassiﬁcation with deep convolutional neural networks. In\nAdvances in neural information processing systems, pages\n1097–1105, 2012. 1\n[16] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv\npreprint arXiv:1312.4400, 2013. 1\n[17] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional\nnetworks for semantic segmentation. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 3431–3440, 2015. 8\n[18] I. Loshchilov and F. Hutter. SGDR: stochastic gradient de-\nscent with restarts. CoRR, abs/1608.03983, 2016. 5\n[19] P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen,\nD. Garcia,\nB. Ginsburg,\nM. Houston,\nO. Kuchaev,\nG. Venkatesh, et al. Mixed precision training. arXiv preprint\narXiv:1710.03740, 2017. 3\n[20] Y. E. Nesterov. A method for solving the convex program-\nming problem with convergence rate o (1/kˆ 2).\nIn Dokl.\nAkad. Nauk SSSR, volume 269, pages 543–547, 1983. 2\n[21] H.-T. Z. Ningning Ma, Xiangyu Zhang and J. Sun. Shufﬂenet\nv2: Practical guidelines for efﬁcient cnn architecture design.\narXiv preprint arXiv:1807.11164, 2018. 5\n[22] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards\nreal-time object detection with region proposal networks. In\nAdvances in neural information processing systems, pages\n91–99, 2015. 7\n[23] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\net al.\nImagenet large scale visual recognition challenge.\nInternational Journal of Computer Vision, 115(3):211–252,\n2015. 1, 2\n[24] K. Simonyan and A. Zisserman.\nVery deep convolu-\ntional networks for large-scale image recognition.\nCoRR,\nabs/1409.1556, 2014. 1\n[25] S. L. Smith, P.-J. Kindermans, C. Ying, and Q. V. Le. Don’t\ndecay the learning rate, increase the batch size.\narXiv\npreprint arXiv:1711.00489, 2017. 3\n[26] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.\nRethinking the inception architecture for computer vision.\nCoRR, abs/1512.00567, 2015. 2, 4, 5, 6, 7\n[27] S. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He. Aggregated\nresidual transformations for deep neural networks. In Com-\nputer Vision and Pattern Recognition (CVPR), 2017 IEEE\nConference on, pages 5987–5995. IEEE, 2017. 1, 4\n[28] F. Yu and V. Koltun. Multi-scale context aggregation by di-\nlated convolutions. arXiv preprint arXiv:1511.07122, 2015.\n8\n[29] H. Zhang, M. Ciss´e, Y. N. Dauphin, and D. Lopez-\nPaz. mixup: Beyond empirical risk minimization. CoRR,\nabs/1710.09412, 2017. 7\n[30] H. Zhang, K. Dana, J. Shi, Z. Zhang, X. Wang, A. Tyagi, and\nA. Agrawal. Context encoding for semantic segmentation.\nIn The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2018. 8\n[31] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene\nparsing network. In Computer Vision and Pattern Recogni-\ntion (CVPR), 2017 IEEE Conference on, pages 6230–6239.\nIEEE, 2017. 5, 8\n\n\n[32] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba.\nPlaces: A 10 million image database for scene recognition.\nIEEE transactions on pattern analysis and machine intelli-\ngence, 2017. 1\n[33] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Tor-\nralba. Scene parsing through ade20k dataset. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2017. 8\n[34] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le.\nLearn-\ning transferable architectures for scalable image recognition.\nCoRR, abs/1707.07012, 2017. 1\n"
}