{
  "filename": "2104.10972v4.pdf",
  "num_pages": 20,
  "pages": [
    "arXiv:2104.10972v4  [cs.CV]  5 Aug 2021\nImageNet-21K Pretraining for the Masses\nTal Ridnik\nDAMO Academy, Alibaba Group\ntal.ridnik@alibaba-inc.com\nEmanuel Ben-Baruch\nDAMO Academy, Alibaba Group\nemanuel.benbaruch@alibaba-inc.com\nAsaf Noy\nDAMO Academy, Alibaba Group\nasaf.noy@alibaba-inc.com\nLihi Zelnik-Manor\nDAMO Academy, Alibaba Group\nlihi.zelnik@alibaba-inc.com\nAbstract\nImageNet-1K serves as the primary dataset for pretraining deep learning models\nfor computer vision tasks. ImageNet-21K dataset, which is bigger and more di-\nverse, is used less frequently for pretraining, mainly due to its complexity, low\naccessibility, and underestimation of its added value. This paper aims to close\nthis gap, and make high-quality efﬁcient pretraining on ImageNet-21K available\nfor everyone. Via a dedicated preprocessing stage, utilization of WordNet hi-\nerarchical structure, and a novel training scheme called semantic softmax, we\nshow that various models signiﬁcantly beneﬁt from ImageNet-21K pretraining\non numerous datasets and tasks, including small mobile-oriented models. We\nalso show that we outperform previous ImageNet-21K pretraining schemes for\nprominent new models like ViT and Mixer. Our proposed pretraining pipeline\nis efﬁcient, accessible, and leads to SoTA reproducible results, from a publicly\navailable dataset.\nThe training code and pretrained models are available at:\nhttps://github.com/Alibaba-MIIL/ImageNet21K\n1\nIntroduction\nImageNet-1K dataset, introduced for the ILSVRC2012 visual recognition challenge [45], has been\nat the center of modern advances in deep learning [30, 20, 46]. ImageNet-1K serves as the main\ndataset for pretraining of models for computer-vision transfer learning [51, 33, 21], and improving\nperformances on ImageNet-1K is often seen as a litmus test for general applicability on downstream\ntasks [28, 62, 44]. ImageNet-1K is a subset of the full ImageNet dataset [11], which consists of\n14,197,122 images, divided into 21,841 classes. We shall refer to the full dataset as ImageNet-21K,\nfollowing [27] (although other papers sometimes described it as ImageNet-22K [8]). ImageNet-1K\nwas created by selecting a subset of 1.2M images from ImageNet-21K, that belong to 1000 mutually\nexclusive classes.\nEven though some previous works showed that pretraining on ImageNet-21K could provide better\ndownstream results for large models [27, 14], pretraining on ImageNet-1K remained far more popu-\nlar. A main reason for this discrepancy is that ImageNet-21K labels are not mutually exclusive - the\nlabels are taken from WordNet [38], where each image is labeled with one label only, not necessar-\nily at the highest possible hierarchy of WordNet semantic tree. For example, ImageNet-21K dataset\ncontains the labels \"chair\" and \"furniture\". A picture, with an actual chair, can sometimes be labeled\nas \"chair\", but sometimes be labeled as the semantic parent of \"chair\", \"furniture\". This kind of tag-\nging methodology complicates the training process, and makes evaluating models on ImageNet-21K\nless accurate. Other challenges of ImageNet-21K dataset are the lack of ofﬁcial train-validation split,\nthe fact that training is longer than ImageNet-1K and requires highly efﬁcient training schemes, and\nthat the raw dataset is large - 1.3TB.\n",
    "Figure 1: Our end-to-end pretraining pipeline on ImageNet-21K. We start with a dataset preparation and\npreprocessing stage. Via WordNet’s synsets, we convert all the single-label inputs to semantic multi-labels,\nresulting in a semantic structure for ImageNet-21K, with 11 possible hierarchies. For each hierarchy, we apply\na dedicated softmax activation, and aggregate the losses with hierarchy balancing.\nSeveral past works have used ImageNet-21K for pretraining, mostly in comparison to larger datasets,\nwhich are not publicly available, such as JFT-300M [49]. [40] and [43] used ImageNet-21K and JFT-\n300M to train expert models according to the datasets hierarchies, and combined them to ensembles\non downstream tasks; [27] and [14] compared pretraining JFT-300M to ImageNet-21K on large\nmodels such as ViT and ResNet-50x4. Many papers used these pretrained models for downstream\ntasks (e.g., [63, 41, 36, 1]). There are also works on ImageNet-21K that did not focus on pretraining:\n[61] used extra (unlabled) data from ImageNet-21K to improve knowledge-distillation training on\nImageNet-1K; [13] used ImageNet-21k for testing few-shot learning; [56] tested efﬁcient softmax\nschemes on ImageNet-21k; [17] tested pooling operations schemes on animal-oriented subset of\nImageNet-21k.\nHowever, previous works have not methodologically studied and optimized a pretraining process\nspeciﬁcally for ImageNet-21K. Since this is a large-scale, high-quality, publicly available dataset,\nthis kind of study can be highly beneﬁcial to the community. We wish to close this gap in this\nwork, and make efﬁcient top-quality pretraining on ImageNet-21K accessible to all deep learning\npractitioners.\nOur pretraining pipeline starts by preprocessing ImageNet-21K to ensure all classes have enough\nimages for a meaningful learning, splitting the dataset to a standardized train-validation split, and\nresizing all images to reduce memory footprint. Using WordNet semantic tree [38], we show that\nImageNet-21K can be transformed into a (semantic) multi-label dataset. We thoroughly analyze\nthe advantages and disadvantages of single-label and multi-label training. Extensive tests on down-\nstream tasks show that multi-label pretraining does not improve results on downstream tasks, despite\nhaving more information per image. To effectively utilize the semantic data, we develop a novel\ntraining method, called semantic softmax, which exploits the hierarchical structure of ImageNet-\n21K tagging to train the network over several semantic softmax layers, instead of the single layer.\nUsing semantic softmax pretraining, we consistently outperform both single-label and multi-label\npretraining on downstream tasks. By integrating semantic softmax into a dedicated semantic knowl-\nedge distillation loss, we further improved results. The complete end-to-end pretraining pipeline\nappears in Figure 1.\nUsing semantic softmax pretraining on ImageNet-21K we achieve signiﬁcant improvement on nu-\nmerous downstream tasks, compared to standard ImageNet-1K pretraining. Unlike previous works,\nwhich focused on pretraining of large models only [27], we show that ImageNet-21K pretraining\nbeneﬁts a wide variety of models, from larger models like TResNet-L [44], through medium-sized\nmodels like ResNet50 [20], and even small mobile-dedicated models like OFA-595 [5] and Mo-\nbileNetV3 [21]. Our proposed pretraining scheme also outperforms previous ImageNet-21K pre-\ntraining schemes that were used to trained MLP-based models like Vision-Transformer (ViT) [14]\nand Mixer [53].\nThe paper’s contribution can be summarized as follows:\n• We develop a methodical preprocess procedure to transform raw ImageNet-21K into a viable\ndataset for efﬁcient, high-quality pretraining.\n• Using WordNet semantic tree, we convert each (single) label to semantic multi labels, and com-\npare the pretrain quality of two baseline methods: single-label and multi-label pretraining. We\n2\n",
    "show that while a multi-label approach provides more information per image, it can have signiﬁ-\ncant optimization drawbacks, resulting in inferior results on downstream tasks.\n• We develop a novel training scheme called semantic softmax, which exploits the hierarchical\nstructure of ImageNet-21K. With semantic softmax pretraining, we outperform both single-label\nand multi-label pretraining on downstream tasks. We further improve results by integrating se-\nmantic softmax into a dedicated semantic knowledge distillation scheme.\n• Via extensive experimentations, we show that compared to ImageNet-1K pretraining, ImageNet-\n21K pretraining signiﬁcantly improves downstream results for a wide variety of architectures,\ninclude mobile-oriented ones. In addition, our ImageNet-21K pretraining scheme consistently\noutperforms previous ImageNet-21K pretraining schemes for prominent new models like ViT\nand Mixer.\n2\nDataset Preparation\n2.1\nPreprocessing ImageNet-21K\nOur preprocessing stage consists of three steps, as described in Figure 1 (leftmost image): (1) invalid\nclasses cleaning, (2) creating a validation set, (3) image resizing. Details are as follows:\nStep 1 - cleaning invalid classes: the original ImageNet-21K dataset [11] consists of 14,197,122\nimages, each tagged in a single-label fashion by one of 21,841 possible classes. The dataset has no\nofﬁcial train-validation split, and the classes are not well-balanced - some classes contain only 1-10\nsamples, while others contain thousands of samples. Classes with few samples cannot be learned\nefﬁciently, and may hinder the entire training process and hurt the pretrain quality [23]. Hence we\nstart our preprocessing stage by removing infrequent classes, with less than 500 labels. After this\nstage, the dataset contains 12,358,688 images from 11,221 classes. Notice that the cleaning process\nreduced the number of total classes by half, but removed only 13% of the original pictures.\nStep 2 - validation split: we allocate 50 images per class for a standardized validation split, that\ncan be used for future benchmarks and comparisons.\nStep 3 - image resizing: ImageNet-1K training usually uses crop-resizing [22] which favours load-\ning the original images at full resolution and resizing them on-the-ﬂy. To make ImageNet-21K\ndataset more accessible and accelerate training, we resized during the preprocessing stage all the\nimages to 224 resolution (equivalent to squish-resizing [22]). While somewhat limiting scale aug-\nmentations, this stage signiﬁcantly reduces the dataset’s memory footprint, from 1.3TB to 250GB,\nand makes loading the data during training faster.\nAfter ﬁnishing the preprocessing stage, we kept only valid classes, produced a standardized train-\nvalidation split, and signiﬁcantly reduced the dataset size. We shall name this processed dataset\nImageNet-21K-P (P for Processed).\n2.2\nUtilizing Semantic Data\nWe now wish to analyze the semantic structure of ImageNet-21K-P dataset. This structure will\nenable us to better understand ImageNet-21K-P tagging methodology, and employ and compare\ndifferent pretraining schemes.\nFrom single labels to semantic multi labels\nEach image in the original ImageNet-21K dataset\nwas labeled with a single label, that belongs to WordNet synset [38]. Using the WordNet synset\nhyponym (subtype) and hypernym (supertype) relations, we can obtain for each class its parent class,\nif exists, and a list of child classes, if exists. When applying the parenthood relation recursively, we\ncan build a semantic tree, that enables us to transform ImageNet-21K-P dataset into a multi-label\ndataset, where each image is associated with several labels - the original label, and also its parent\nclass, parent-of-parent class, and so on. Example is given in Figure 1 (middle image) - the original\nimage was labeled as ’swan’, but by utilizing the semantic tree, we can produce a list of semantic\nlabels for the image - ’animal, vertebrate, bird, aquatic bird, swan’. Notice that the labels are sorted\nby hierarchy: ’animal’ label belongs to hierarchy 0, while ’swan’ label belongs to hierarchy 4. A\nlabel from hierarchy k has k ancestors.\nUnderstanding the inconsistent tagging methodology\nThe semantic structure of ImageNet-21K\nenables us to understand its tagging methodology better. According to the stated tagging method-\nology of ImageNet-21K [11], we are not guaranteed that each image was labeled at the highest\n3\n",
    "Figure 2:\nExample of inconsistent tagging in\nImageNet-21K dataset. Two pictures containing the\nsame animal were labeled differently.\nHierarchy\nExample Classes\n0\nperson, animal, plant,\nfood, artifact\n1\ndomestic animal,\nbasketball court, clothing\n...\n6\nwhitetip shark, ortolan,\ngrey kingbird\nTable 1:\nExamples of classes from different\nImageNet-21K-P hierarchies.\npossible hierarchy. An example is given in Figure 2. Two pictures, that contain the animal cow,\nwere labeled differently - one with the label ’animal’, the other with the label ’cow’. Notice that\n’animal’ is a semantic ancestor of ’cow’ (cow →placental →mammal →vertebrate →animal).\nThis kind of incomplete tagging methodology, which is common in large datasets [32, 42], hinders\nand complicates the training process. A dedicated scheme that tackles this tagging methodology will\nbe presented in section 3.3.\nSemantic statistics\nBy using WordNet synsets, we can calculate for each class the number of\nancestors it has - its hierarchy. In total, our processed dataset, ImageNet-21K-P, has 11 possible\nhierarchies. Example of classes from different hierarchies appears in Table 1. In Figure 4 in appendix\nA we present the number of classes per hierarchy. We see that while there are 11 possible hierarchies,\nthe vast majority of classes belong to the lower hierarchies.\n3\nPretraining Schemes\nIn this section, we will review and analyze two baseline schemes for pretraining on ImageNet-21K-\nP: single-label and multi-label training. We will also develop a novel new scheme for pretraining on\nImageNet-21K-P, semantic softmax, and analyze its advantages over the baseline schemes.\n3.1\nSingle-label Training Scheme\nThe straightforward way to pretrain on ImageNet-21K-P is to use the original (single) labels, apply\nsoftmax on the output logits, and use cross-entropy loss. Our single-label training scheme is similar\nto common efﬁcient training schemes on ImageNet-1K [44], with minor adaptations to better handle\nthe inconsistent tagging (Full training details appear in appendix B.1). Since we aim for an efﬁcient\nscheme with maximal throughput, we don’t incorporate any tricks that might signiﬁcantly increase\ntraining times. To further shorten the training times, we propose to initialize the models from\nstandard ImageNet-1K training, and train on ImageNet-21K-P for 80 epochs. On 8xV100 NVIDIA\nGPU machine, mixed-precision training takes 40 minutes per epoch for ResNet50 and TResNet-M\narchitectures (∼5000 img\nsec ), leading to a total training time of 54 hours. Similar accuracies are\nobtained when doing random initialization, but training the models longer - 140 epochs.\nPros of using single-label training\n• Well-balanced dataset - with single-label training on ImageNet-21K-P, the dataset is well-\nbalanced, meaning each class appears, roughly, the same number of times.\n• Single-loss training - training with a softmax (a single loss) makes convergence easy and efﬁ-\ncient, and avoids many optimization problems associated with multi-loss learning, such as differ-\nent gradient magnitudes and gradient interference [60, 7, 9].\nCons of using single-label training\n• Inconsistent tagging - due to the tagging methodology of ImageNet-21K-P, where we are not\nguaranteed that an image was labeled at the highest possible hierarchy, ground-truth labels are\ninherently inconsistent. Pictures, containing the same object, can appear with different single-\nlabel tagging (see Figure 2 for example).\n4\n",
    "• No semantic data - during training, we are not presenting semantic data via the single-label\nground-truth.\n3.2\nMulti-label Training Scheme\nUsing the semantic tree, we can convert any (single) label to semantic multi labels, and train our\nmodels on ImageNet-21K-P in a multi-label fashion, expecting that the additional semantic infor-\nmation per image will improve the pretrain quality. As commonly done in multi-label classiﬁcation\n[3], we reduce the problem to a series of binary classiﬁcation tasks. Given N labels, the base net-\nwork outputs one logit per label, zn, and each logit is independently activated by a sigmoid function\nσ(zn). Let’s denote yn as the ground-truth for class n. The total classiﬁcation loss, Ltot, is obtained\nby aggregating a binary loss from the N labels:\nLtot =\nN\nX\nn=1\nL (σ(zn), yn) .\n(1)\nEq. 1 formalizes multi-label classiﬁcation as a multi-task problem. Since we have a large number\nof classes (11, 221), this is an extreme multi-task case. For training, we adopted the high-quality\ntraining scheme described in [3], that provided state-of-the-art results on large-scale multi-label\ndatasets such as Open Images [32]. Full training details appear in appendix B.2.\nPros of using multi-label training\n• More information per image - we present for each image all the available semantic labels.\n• Tagging and metrics are more accurate - if an image was originally given a single label at\nhierarchy k, with multi-label training we are guaranteed that all ground-truth labels at hierarchies\n0 to k are accurate. Hence, multi-label training partly mitigates the inconsistent tagging problem,\nand makes training metrics more accurate and reﬂective than single-label training.\nCons of using multi-label training\n• Extreme multi-tasking - with multi-label training, each class is learned separately (sigmoids\ninstead of softmax). This extreme multi-task learning makes the optimization process harder and\nless efﬁcient, and may cause convergences to a local minimum [60, 7, 15].\n• Extreme imbalancing - as a multi-label dataset with many classes, ImageNet-21K-P suffers from\na large positive-negative imbalance [3]. In addition, due to the semantic structure, multi-label\ntraining is hindered by a large class imbalance [24] - on average, classes from a lower hierarchy\nwill appear far more frequent than classes from a higher hierarchy.\nIn appendices C.2 and E we show that for multi-label training, ASL loss [3], that was designed to\ncope with large positive-negative imbalancing, signiﬁcantly outperforms cross-entropy loss, both on\nupstream and downstream tasks. This supports our analysis of extreme imbalancing as a major opti-\nmization challenge of multi-label training. Notice that we also list extreme multi-tasking as another\noptimization pitfall of multi-label training, and a dedicated scheme for dealing with it might further\nimprove results. However, most methods that tackle multi-task learning, such as GradNorm [7] and\nPCGrad [60], require computation of gradients for each class separately. This is computationally\ninfeasible for a dataset with a large number of classes, such as ImageNet-21K-P.\n3.3\nSemantic Softmax Training Scheme\nOur goal is to develop a dedicated training scheme that utilizes the advantages of both the single-\nlabel and the multi-label training. Speciﬁcally, our scheme should present for each input image all\nthe available semantic labels, but use softmax activations instead of independent sigmoids to avoid\nextreme multi-tasking. We also want to have fully accurate ground-truth and training metrics, and\nprovide the network direct data on the semantic hierarchies (this is not achieved even in multi-label\ntraining, the hierarchical structure there is implicit). In addition, the scheme should remain efﬁcient\nin terms of training times.\nSemantic softmax formulation\nTo meet these goals, we develop a new training scheme called\nsemantic softmax training. As we saw in section 2.2, each label in ImageNet-21K-P can belong to\n5\n",
    "one of 11 possible hierarchies. By deﬁnition, for each hierarchy there can be only one ground-truth\nlabel per input image. Hence, instead of single-label training with a single softmax, we shall have 11\nsoftmax layers, for the 11 different hierarchies. Each softmax will sample the relevant logits from\nthe corresponding hierarchy, as shown in Figure 1 (rightmost image). To deal with the partial tagging\nof ImageNet-21K-P, not all softmax layers will propagate gradients from each sample. Instead, we\nwill activate only softmax layers from the relevant hierarchies. An example is given in Figure 3 -\nthe original image had a label from hierarchy 5. We transform it to 6 semantic ground-truth labels,\nfor hierarchies 0-5, and activate only the 6 ﬁrst semantic softmax layers (only activated layers will\npropagate gradients). Compared to single-label and multi-label schemes, semantic softmax training\nscheme has the following advantages:\n1. We avoid extreme multi-tasking (11, 221 uncoupled losses in multi-label training). Instead,\nwe have only 11 losses, as the number of softmax layers.\n2. We present for each input image all the possible semantic labels. The loss scheme even\nprovides direct data on the hierarchical structure.\n3. Unlike single-label and multi-label training, semantic softmax ground-truth and training met-\nrics are fully accurate. If a sample has no labels at hierarchy k, we don’t propagate gradients\nfrom the kth softmax during training, and ignore that hierarchy for metrics calculation (A\ndedicated metrics for semantic softmax training is deﬁned in appendix C.3).\n4. Calculating several softmax activations instead of a single one has negligible overhead, and\nin practice training times are similar to single-label training.\nFigure 3: Gradient propagation logic of semantic\nsoftmax training.\nWeighting the different softmax layers\nFor\neach input image we have K losses (11). As\ncommonly done in multi-task training [7], we\nneed to aggregate them to a single loss. A naive\nsolution will be to sum them: Ltot = PK−1\nk=0 Lk\nwhere Lk, the loss per softmax layer, is zero\nwhen the layer is not activated. However, this\nformulation ignores the fact that softmax lay-\ners at lower hierarchies will be activated much\nmore frequently than softmax layers at higher\nhierarchies, resulting in over-emphasizing of\nclasses from lower hierarchies. To account for\nthis imbalancing, we propose a balancing logic:\nlet Nj be the total number of classes in hierar-\nchy j (as presented in Figure 4). Due to the semantic structure, the relative number of occurrences\nof hierarchy k in the loss function will be:\nOk =\nk−1\nX\nj=0\nNj\n(2)\nHence, to balance the contribution of different hierarchies we can use a normalization factor Wk =\n1\nOk , and obtain a balanced aggregation loss, that will be used for semantic softmax training:\nLtot =\nK−1\nX\nk=0\nWkLk\n(3)\n3.4\nSemantic Knowledge Distillation\nKnowledge distillation (KD) is a known method to improve not only upstream, but also downstream\nresults [58, 59, 61]. We want to combine our semantic softmax scheme with KD training - semantic\nKD. In addition to the general beneﬁt from KD training [19], for ImageNet-21K-P semantic KD has\nan additional beneﬁt - it can predict the missing tags that arise from the inconsistent tagging. For\nexample, for the left picture in Figure 2, the teacher model can predict the missing labels - ’cow,\nplacental, mammal, vertebrate’. To implement semantic KD loss, for each hierarchy we will calcu-\nlate both the teacher and the student the corresponding probability distributions {Ti}K−1\ni=0 , {Si}K−1\ni=0 .\nThe KD loss of hierarchy i will be:\nLKDi = KDLoss(Ti, Si)\n(4)\n6\n",
    "where KDLoss is a standard measurement for the distance between distributions, that can be chosen\nas Kullback-Leibler divergence [19, 58], or as MSE loss [2, 52]. We have found that the latter\nconverges faster, and used it. A vanilla implementation for the total loss will be a simple sum of\nthe losses from different hierarchies: LKD = PK−1\ni=0 LKDi. However, this formulation assumes\nthat all the hierarchies are relevant for each image. This is inaccurate - usually higher hierarchies\nrepresent subspecies of animals or plants, and are not applicable for a picture of a chair, for example.\nSo we need to determine from the teacher predictions which hierarchies are relevant, and weigh\nthe different losses accordingly. Let’s assume that for each hierarchy we can calculate the teacher\nconﬁdence level, Pi. A conﬁdence-weighted KD loss will be:\nLKD =\nK−1\nX\ni=0\nPiLKDi\n(5)\nEq. 5 is our proposed semantic KD loss. In appendix F we present a method to calculate the teacher\nconﬁdence level, Pi, from the teacher predictions, similar to [58].\n4\nExperimental Study\nIn this section, we will present upstream and downstream results for the different training schemes,\nand show that semantic softmax pretraining outperforms single-label and multi-label pretraining.\nWe will also demonstrate how semantic KD further improves results on downstream tasks.\n4.1\nUpstream Results\nIn appendix C we provide upstream results for the three training schemes. Since each scheme has\ndifferent training metrics, we cannot use these results to directly compare (pre)training quality.\n4.2\nDownstream Results\nTo compare the pretrain quality of different training schemes, we will test our models via transfer\nlearning. To ensure that we are not overﬁtting a speciﬁc dataset or task, we chose a wide variety of\ndownstream datasets, from different computer-vision tasks. We also ensured that our downstream\ndatasets represent a variety of domains, and have diverse sizes - from small datasets of thousands\nof images, to larger datasets with more than a million images. For single-label classiﬁcation, we\ntransferred our models to ImageNet-1K [30], iNaturalist 2019 [55], CIFAR-100 [29] and Food 251\n[25]. For multi-label classiﬁcation, we transferred our models to MS-COCO [34] and Pascal-VOC\n[16] datasets. For video action recognition, we transferred our models to Kinetics 200 dataset [26].\nIn appendix D we provide full training details on all downstream datasets.\nComparing different pretraining schemes\nIn Table 2 we compare downstream results for three\npretraining schemes: single-label, multi-label and semantic softmax. We see that on 6 out of 7\nDataset\nSingle\nLabel\nPretrain\nMutli\nLabel\nPretrain\nSemantic\nSoftmax\nPretrain\nSemantic\nSoftmax\nPretrain + KD\nImageNet1K(1)\n81.1\n81.0\n81.4\n82.2\niNaturalist(1)\n71.5\n71.0\n72.0\n72.7\nFood 251(1)\n75.4\n75.2\n75.8\n76.1\nCIFAR 100(1)\n89.5\n90.6\n90.4\n91.7\nMS-COCO(2)\n80.8\n80.6\n81.3\n82.2\nPascal-VOC(2)\n88.1\n87.9\n89.7\n89.8\nKinetics 200(3)\n81.9\n81.9\n83.0\n84.4\nTable 2: Comparing downstream results for different pretraining schemes. Darker cell color means better\nscore. Dataset types and metrics: (1) - single-label, top-1 Acc.[%] ; (2) - multi-label, mAP [%]; (3) - action\nrecognition, top-1 Acc. [%].\ndatasets tested, semantic softmax pretraining outperforms both single-label and multi-label pretrain-\n7\n",
    "ing. In addition, we see from Table 2 that single-label pretraining performs better than multi-label\npretraining (scores are higher on 5 out of 7 datasets tested).\nThese results support our analysis of the pros and cons of the different pretraining schemes from Sec-\ntion 3: with multi-label training, we have more information per input image, but the optimization\nprocess is less efﬁcient due to extreme multi-tasking and extreme imbalancing. All-in-all, multi-\nlabel training does not improve downstream results. Single-label training, despite its shortcomings\nfrom the partial tagging methodology and the minimal information per image, provides a better pre-\ntraining baseline. Semantic softmax scheme, which utilizes semantic data without the optimization\npitfalls of extreme multi-label training, outperforms both single-label and multi-label training.\nSemantic KD\nIn Table 2 we also compare the downstream results of semantic softmax pretraining,\nwith and without semantic KD. We see that on all tasks and datasets tested, adding semantic KD to\nour pretraining process improves downstream results. Indeed the ability of semantic KD to ﬁll in\nthe missing tags and provide a smoother and more informative ground-truth is translated to better\ndownstream results. In appendix G we compare single-label pretraining with KD, to semantic soft-\nmax pretraining with semantic KD, and show that the latter achieves better results on downstream\ntasks.\n5\nResults\nIn the previous chapters we developed a dedicated pretraining scheme for ImageNet-21K-P dataset,\nsemantic softmax, and showed that it outperforms two baseline pretraining schemes, single-label\nand multi-label, in terms of downstream results. Now we wish to compare our semantic softmax\npretraining on ImageNet-21K-P to other known pretraining schemes and pretraining datasets.\n5.1\nComparison to Other ImageNet-21K Pretraining Schemes\nWe want to compare our proposed training scheme to other ImageNet-21K training schemes from the\nliterature. However, to the best of our knowledge, no previous works have published their upstream\nresults on ImageNet-21K, or shared thorough details about their training scheme or preprocessing\nstage. Recently, prominent new models called ViT [14] and Mixer [53] were published, and ofﬁcial\npretrained weights were released [18]. In Table 3 we compare downstream results when using the\nofﬁcial ImageNet-21K weights, and when using weights from semantic softmax pretraining.\nViT-B-16\nMixer-B-16\nDataset\nOfﬁcial\nImageNet-21K\nPretrain\nOur\nImageNet-21K\nPretrain\nOfﬁcial\nImageNet-21K\nPretrain\nOur\nImageNet-21K\nPretrain\nImageNet1K(1)\n83.3\n83.9\n79.7\n82.0\niNaturalist(1)\n71.7\n73.1\n62.2\n66.6\nFood 251(1)\n74.6\n76.0\n69.9\n74.5\nCIFAR 100(1)\n92.7\n94.2\n85.5\n92.3\nMS-COCO(2)\n81.1\n82.6\n74.1\n80.9\nPascal-VOC(2)\n78.7\n93.1\n63.1\n88.6\nKinetics 200(3)\n82.7\n84.1\n79.3\n82.1\nTable 3: Comparing downstream results for different pretraining schemes. Dataset types and metrics: (1)\n- single-label, top-1 Acc.[%] ; (2) - multi-label, mAP [%]; (3) - action recognition, top-1 Acc. [%].\nWe see from Table 3 that our pretraining scheme signiﬁcantly outperforms the ofﬁcial pretrain, on\nall downstream tasks tested. Previous works have observed that MLP-based models can be harder\nand less stable to use in transfer learning since they don’t have inherent translation inductive bias\n[6, 39, 35]. When using the ofﬁcial weights, we also noticed this phenomenon on some datasets\n(Pascal-VOC, for example). Using semantic softmax pretraining, the transfer learning training was\nmore stable and robust, and reached higher accuracy.\n8\n",
    "Dataset\nMobileNetV3\nOFA595\nResNet50\nTResNet-M\nTResNet-L\n1K\n21K\n1K\n21K\n1K\n21K\n1K\n21K\n1K\n21K\niNaturalist(1)\n62.4\n65.0\n69.0\n71.5\n66.8\n71.4\n70.1\n72.7\n72.4\n74.8\nCIFAR100(1)\n86.7\n88.5\n88.3\n90.3\n86.8\n90.3\n89.5\n91.7\n90.2\n92.5\nFood 251(1)\n70.1\n70.3\n72.9\n73.5\n72.2\n74.0\n75.1\n76.1\n76.3\n77.0\nMS-COCO(2)\n73.0\n74.9\n74.9\n77.7\n76.7\n80.5\n79.5\n82.2\n81.1\n83.7\nPascal-VOC(2)\n72.1\n72.4\n72.4\n81.5\n86.9\n87.9\n85.8\n89.8\n88.2\n92.5\nKinetics200(3)\n72.2\n74.3\n73.2\n78.1\n78.2\n81.3\n80.5\n84.3\n82.1\n84.6\nTable 4: Comparing downstream results for ImageNet-1K standard pretraining, and our proposed\nImageNet-21K-P pretraining scheme. (1) - single-label dataset, top-1 Acc [%] metric; (2) - multi-label\ndataset, mAP [%] metric; (3) - action recognition dataset, top-1 Acc [%] metric.\n5.2\nComparison to ImageNet-1K Pretraining\nIn Table 4 we compare downstream results, for different models, when using ImageNet-1K pre-\ntraining (taken from [57]), and when using our ImageNet-21K-P pertraining. We can see that our\npretraining scheme signiﬁcantly outperforms standard ImageNet-1K pretraining on all datasets, for\nall models tested. For example, on iNaturalist dataset we improve the average top-1 accuracy by\n2.9%.\nNotice that some previous works stated that pretraining on a large dataset beneﬁts only large models\n[27, 49]. MobileNetV3 backbone, for example, has only 4.2M parameters, while ViT-B model has\n85.6M parameters. Previous works assumed that a large number of parameters, like ViT has, is\nneeded to properly utilize pretraining on large datasets. However, we show consistently and signif-\nicantly that even small mobile-oriented models, like MobileNetV3 and OFA-595, can beneﬁt from\npretraining on a large (publicly available) dataset like ImageNet-21K-P. Due to their fast inference\ntimes and reduced heating, mobile-oriented models are used frequently for deployment. Hence, im-\nproving their downstream results by using better pretrain weights can enhance real-world products,\nwithout increasing training complexity or inference times.\n5.3\nImageNet-1K SoTA Results\nIn Table 10 in appendix H we bring downstream results on ImageNet-1K for different models, when\nusing ImageNet-21K-P semantic softmax pretraining. To achieve top results, similar to previous\nworks [33, 61, 58], we added standard knowledge distillation loss into our ImageNet-1K training.\nTo the best of our knowledge, for all the models in Table 10 we achieve a new SoTA record (for\ninput resolution 224). Unlike previous top works, which used private datasets [49], we are using a\npublicly available dataset for pretraining. Note that the gap from the original reported accuracies\nis signiﬁcant. For example, MobileNetV3 reported accuracy was 75.2% [21] - we achieved 78.0%;\nResNet50 reported accuracy was 76.0% [20] - we achieved 82.0%.\n5.4\nAdditional Comparisons and Results\nIn appendix J we bring additional comparisons: (1) Comparison to Open Images pretraining; (2)\nDownstream results comparison on additional non-classiﬁcation computer-vision tasks; (3) Impact\nof different number of training samples on upstream results.\n6\nConclusion\nIn this paper, we presented an end-to-end scheme for high-quality efﬁcient pretraining on ImageNet-\n21K dataset. We start by standardizing the dataset preprocessing stage. Then we show how we can\ntransform ImageNet-21K dataset into a multi-label one, using WordNet semantics. Via extensive\ntests on downstream tasks, we demonstrate how single-label training outperforms multi-label train-\ning, despite having less information per image. We then develop a new training scheme, called se-\nmantic softmax, which utilizes ImageNet-21K hierarchical structure to outperform both single-label\nand multi-label training. We also integrate the semantic softmax scheme into a dedicated knowledge\ndistillation loss to further improve results. On a variety of computer vision datasets and tasks, dif-\n9\n",
    "ferent architectures signiﬁcantly and consistently beneﬁt from our pretraining scheme, compared to\nImageNet-1K pretraining and previous ImageNet-21K pretraining schemes.\nBroader Impact\nIn the past, pretraining on ImageNet-21K was out of scope for the common deep\nlearning practitioner. With our proposed pipeline, high-quality efﬁcient pretraining on ImageNet-\n21K will be more accessible to the deep learning community, enabling researchers to design new\narchitectures and pretrain them to top results, without the need for massive computing resources\nor large-scale private datasets. In addition, our ﬁndings that even small mobile-oriented models\nsigniﬁcantly beneﬁt from large-scale pretraining can be used to enhance real-world products. Finally,\nour improved pretraining scheme on ImageNet-21K can support prominent MLP-based models that\nrequire large-scale pretraining, like ViT and Mixer.\nReferences\n[1] Shekoofeh Azizi, Basil Mustafa, Fiona Ryan, Zachary Beaver, Jan Freyberg, Jonathan Deaton, Aaron\nLoh, Alan Karthikesalingam, Simon Kornblith, Ting Chen, Vivek Natarajan, and Mohammad Norouzi.\nBig self-supervised models advance medical image classiﬁcation, 2021.\n[2] Lei Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? arXiv preprint arXiv:1312.6184,\n2013.\n[3] Emanuel Ben-Baruch, Tal Ridnik, Nadav Zamir, Asaf Noy, Itamar Friedman, Matan Protter, and Lihi\nZelnik-Manor. Asymmetric loss for multi-label classiﬁcation. arXiv preprint arXiv:2009.14119, 2020.\n[4] Clemens-Alexander Brust and Joachim Denzler. Integrating domain knowledge: using hierarchies to\nimprove deep classiﬁers. In Asian Conference on Pattern Recognition, pages 3–16. Springer, 2019.\n[5] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once-for-all: Train one network\nand specialize it for efﬁcient deployment. arXiv preprint arXiv:1908.09791, 2019.\n[6] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised visual trans-\nformers. arXiv preprint arXiv:2104.02057, 2021.\n[7] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normal-\nization for adaptive loss balancing in deep multitask networks. In International Conference on Machine\nLearning, pages 794–803. PMLR, 2018.\n[8] Valeriu Codreanu, Damian Podareanu, and Vikram Saletore. Scale out for large minibatch sgd: Resid-\nual network training on imagenet-1k with improved accuracy and reduced time to train. arXiv preprint\narXiv:1711.04291, 2017.\n[9] Michael Crawshaw.\nMulti-task learning with deep neural networks:\nA survey.\narXiv preprint\narXiv:2009.09796, 2020.\n[10] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data\naugmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition Workshops, pages 702–703, 2020.\n[11] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical\nImage Database. In CVPR09, 2009.\n[12] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with\ncutout. arXiv preprint arXiv:1708.04552, 2017.\n[13] Guneet S Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline for few-shot\nimage classiﬁcation. arXiv preprint arXiv:1909.02729, 2019.\n[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-\nterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[15] Yunshu Du, Wojciech M Czarnecki, Siddhant M Jayakumar, Mehrdad Farajtabar, Razvan Pascanu,\nand Balaji Lakshminarayanan.\nAdapting auxiliary losses using gradient similarity.\narXiv preprint\narXiv:1812.02224, 2018.\n[16] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The\npascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303–338,\n2010.\n[17] Wonjoon Goo, Juyong Kim, Gunhee Kim, and Sung Ju Hwang. Taxonomy-regularized semantic deep\nconvolutional neural networks. In European Conference on Computer Vision, pages 86–101. Springer,\n2016.\n[18] Google. vit pretrained weights. https://console.cloud.google.com/storage/browser/vit_models,\n2021.\n[19] Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A survey.\nInternational Journal of Computer Vision, pages 1–31, 2021.\n[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.\n10\n",
    "[21] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang,\nYukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pages 1314–1324, 2019.\n[22] Jeremy Howard and Sylvain Gugger. Fastai: A layered api for deep learning. Information, 11(2):108,\n2020.\n[23] Minyoung Huh, Pulkit Agrawal, and Alexei A Efros. What makes imagenet good for transfer learning?\narXiv preprint arXiv:1608.08614, 2016.\n[24] Justin M Johnson and Taghi M Khoshgoftaar. Survey on deep learning with class imbalance. Journal of\nBig Data, 6(1):1–54, 2019.\n[25] Parneet Kaur, Karan Sikka, Weijun Wang, Serge Belongie, and Ajay Divakaran. Foodx-251: a dataset for\nﬁne-grained food classiﬁcation. arXiv preprint arXiv:1907.06167, 2019.\n[26] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan,\nFabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv\npreprint arXiv:1705.06950, 2017.\n[27] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and\nNeil Houlsby. Big transfer (bit): General visual representation learning. arXiv preprint arXiv:1912.11370,\n6(2):8, 2019.\n[28] Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better?\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2661–\n2671, 2019.\n[29] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. arXiv\npreprint, 2009.\n[30] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional\nneural networks. Advances in neural information processing systems, 25:1097–1105, 2012.\n[31] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Ka-\nmali, Stefan Popov, Matteo Malloci, Tom Duerig, et al. The open images dataset v4: Uniﬁed image clas-\nsiﬁcation, object detection, and visual relationship detection at scale. arXiv preprint arXiv:1811.00982,\n2018.\n[32] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab\nKamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4. Inter-\nnational Journal of Computer Vision, pages 1–26, 2020.\n[33] Jungkyu Lee, Taeryun Won, Tae Kwan Lee, Hyemin Lee, Geonmo Gu, and Kiho Hong. Compounding\nthe performance improvements of assembled techniques in a convolutional neural network. arXiv preprint\narXiv:2001.06268, 2020.\n[34] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. Microsoft coco: Common objects in context.\nIn European conference on\ncomputer vision, pages 740–755. Springer, 2014.\n[35] Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the difﬁculty of\ntraining transformers. arXiv preprint arXiv:2004.08249, 2020.\n[36] Wei Liu, Sihan Chen, Longteng Guo, Xinxin Zhu, and Jing Liu. Cptr: Full transformer network for image\ncaptioning. arXiv preprint arXiv:2101.10804, 2021.\n[37] Ilya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\n[38] George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41,\n1995.\n[39] Marius Mosbach, Maksym Andriushchenko, and Dietrich Klakow. On the stability of ﬁne-tuning bert:\nMisconceptions, explanations, and strong baselines. arXiv preprint arXiv:2006.04884, 2020.\n[40] Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Daniel Keysers, Neil Houlsby, et al. Deep ensembles\nfor low-data transfer learning. arXiv preprint arXiv:2010.06866, 2020.\n[41] Daniel Neimark, Omri Bar, Maya Zohar, and Dotan Asselmann.\nVideo transformer network.\narXiv\npreprint arXiv:2102.00719, 2021.\n[42] Jiquan Ngiam, Daiyi Peng, Vijay Vasudevan, Simon Kornblith, Quoc V Le, and Ruoming Pang. Domain\nadaptive transfer learning with specialist models. arXiv preprint arXiv:1811.07056, 2018.\n[43] Joan Puigcerver, Carlos Riquelme, Basil Mustafa, Cedric Renggli, André Susano Pinto, Sylvain Gelly,\nDaniel Keysers, and Neil Houlsby.\nScalable transfer learning with expert models.\narXiv preprint\narXiv:2009.13239, 2020.\n[44] Tal Ridnik, Hussam Lawen, Asaf Noy, Emanuel Ben Baruch, Gilad Sharir, and Itamar Friedman. Tresnet:\nHigh performance gpu-dedicated architecture. In Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision, pages 1400–1409, 2021.\n[45] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large\nScale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252,\n2015.\n11\n",
    "[46] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.\nMo-\nbilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 4510–4520, 2018.\n[47] Gilad Sharir, Asaf Noy, and Lihi Zelnik-Manor. An image is worth 16x16 words, what is a video worth?,\n2021.\n[48] Leslie N Smith. A disciplined approach to neural network hyper-parameters: Part 1–learning rate, batch\nsize, momentum, and weight decay. arXiv preprint arXiv:1803.09820, 2018.\n[49] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effective-\nness of data in deep learning era. In Proceedings of the IEEE international conference on computer vision,\npages 843–852, 2017.\n[50] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking\nthe inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n[51] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks.\nIn International Conference on Machine Learning, pages 6105–6114. PMLR, 2019.\n[52] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency\ntargets improve semi-supervised deep learning results. arXiv preprint arXiv:1703.01780, 2017.\n[53] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner,\nJessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. Mlp-mixer: An all-mlp architecture\nfor vision. arXiv preprint arXiv:2105.01601, 2021.\n[54] Michael J Trammell, Priyanka Oberoi, James Egenrieder, and John Kaufhold. Contextual label smoothing\nwith a phylogenetic tree on the inaturalist 2018 challenge dataset. Washington Academy of Sciences.\nJournal of the Washington Academy of Sciences, 105(1):23–45, 2019.\n[55] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro\nPerona, and Serge Belongie. The inaturalist species classiﬁcation and detection dataset. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pages 8769–8778, 2018.\n[56] Sudheendra Vijayanarasimhan, Jonathon Shlens, Rajat Monga, and Jay Yagnik. Deep networks with large\noutput spaces. arXiv preprint arXiv:1412.7479, 2014.\n[57] Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models,\n2019.\n[58] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves\nimagenet classiﬁcation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10687–10698, 2020.\n[59] Xueting Yan, Ishan Misra, Abhinav Gupta, Deepti Ghadiyaram, and Dhruv Mahajan. Clusterﬁt: Improv-\ning generalization of visual representations. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 6509–6518, 2020.\n[60] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient\nsurgery for multi-task learning. arXiv preprint arXiv:2001.06782, 2020.\n[61] Sangdoo Yun, Seong Joon Oh, Byeongho Heo, Dongyoon Han, Junsuk Choe, and Sanghyuk Chun.\nRe-labeling imagenet: from single to multi-labels, from global to localized labels.\narXiv preprint\narXiv:2101.05022, 2021.\n[62] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk\nminimization. arXiv preprint arXiv:1710.09412, 2017.\n[63] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jian-\nfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-\nsequence perspective with transformers. arXiv preprint arXiv:2012.15840, 2020.\n12\n",
    "Appendices\nA\nNumber of Classes in Different Hierarchies\n0\n2\n4\n6\n8\n10\nHierarchy Level\n0\n500\n1000\n1500\n2000\n2500\nNumber Of Classes\nFigure 4: Number of classes in different hierarchies.\nB\nTraining Details\nB.1\nSingle-label ImageNet-21K-P Training Details\nTo better handle the ground-truth inconsistencies of ImageNet-21K-P, we increase the label-smooth\nfactor from the common value of 0.1 to 0.2. As explained in section 2.1, we also use squish-resizing\ninstead of crop-resizing. We trained the models with input resolution 224, using an Adam optimizer\nwith learning rate of 3e-4 and one-cycle policy [48]. When initializing our models from standard\nImageNet-1K pretraining (pretraining weights taken from [57]), we found that 80 epochs are enough\nfor achieving strong pretrain results on ImageNet-21K-P. For regularization, we used RandAugment\n[10], Cutout [12], Label-smoothing [50] and True-weight-decay [37]. We observed that the common\nImageNet statistics normalization [33, 51] does not improve the training accuracy, and instead nor-\nmalized all the RGB channels to be between 0 and 1. Unless stated otherwise, all runs and tests\nwere done on TResNet-M architecture. On an 8xV100 NVIDIA GPU machine, training with mixed-\nprecision takes 40 minutes per epoch on ResNet50 and TResNet-M architectures (∼5000 img\nsec ).\nB.2\nMulti-label ImageNet-21K-P Training Details\nFor multi-label training, we convert each image single label input to semantic multi labels, as\ndescribed in section 2.2. Multi-label training details are similar to single-label training (number\nof epochs, optimizer, augmentations, learning rate, models initialization and so on), and training\ntimes are also similar. The main difference between single-label and multi-label training relies in\nthe loss function: for multi-label training we tested 3 loss functions, following [3]: cross-entropy\n(γ−= γ+ = 0), focal loss (γ−= γ+ = 2) and ASL (γ−= 4, γ+ = 0). For ASL, we tried different\nvalues of γ−to obtain the best mAP scores.\nC\nUpstream Results\nAs we have a standardized dataset with a ﬁxed train-validation split, the training metrics for each\npretraining method can be used for future benchmark and comparisons.\nC.1\nSinge-label Upstream Results\nFor single-label training, regular top-1 accuracy metric becomes somewhat irrelevant - if pictures\nwith similar content have different ground-truth labels, the network has no clear \"correct\" answer.\nTop-5 accuracy metric is more representative, but still limited. Upstream results of single-label\ntraining are given in Table 5. We can see that the top-1 accuracies obtained on ImageNet-21K-P,\n13\n",
    "37% −46%, are signiﬁcantly lower than the ones obtained on ImageNet-1K, 75% −85%. This\naccuracy drop is mainly due to the semantic structure and inconsistent tagging methodology of\nImageNet-21K-P. However, as we take bigger and better architectures, we see from Table 5 that the\naccuracies continue to improve, so we are not completely hindered by the inconsistent tagging.\nModel Name\nTop-1 Acc. [%]\nTop-5 Acc. [%]\nMobileNetV3\n37.8\n66.3\nResNet50\n42.2\n72.0\nTResNet-M\n45.3\n75.2\nTResNet-L\n45.5\n75.6\nTable 5: Accuracy of different models in single-label training.\nC.2\nMulti-label Upstream Results\nFor multi-label training, we will use the common micro and macro mAP accuracy [3] as training\nmetrics. However, due to the missing labels in the validation (and train) set, this metric also is not\nfully accurate. In Table 7 we compare the results for three possible loss functions for multi-label\nclassiﬁcation - cross-entropy, focal loss and ASL. We see that ASL loss [3], that was designed to\nLoss Type\nMicro-mAP [%]\nMacro-mAP [%]\nCross-Entropy\n47.3\n73.9\nFocal loss\n47.4\n74.1\nASL\n48.5\n74.7\nTable 6: Comparing different loss functions for multi-label classiﬁcation on ImageNet-21K-P.\ncope with large positive-negative imbalancing, outperform cross-entropy and focal loss. This is in\nagreement with our analysis in section 3.2, where we identify extreme imbalancing as one of the\noptimization challenges that stems from multi-label training.\nC.3\nSemantic Softmax Upstream Results\nWith semantic softmax training, we can calculate for each hierarchy its top-1 accuracy metric. We\ncan also calculate the total accuracy by weighting the different accuracies by the number of classes\nin each hierarchy (see Figure 4). Notice that we are not using classes above the maximal hierarchy\nfor our metrics calculation. Hence, and unlike single-label and multi-label training, with semantic\nsoftmax our training metrics are fully accurate.\nIn Figure 5 we present the top-1 accuracies achieved by different models on different hierarchy\nlevels, when trained with semantic softmax (with KD).\nFigure 5: Top-1 accuracies on different hierarchies.\n14\n",
    "D\nDownstream Datasets Training Details\nFor single-label classiﬁcation, our downstream datasets were ImageNet-1K [30], iNaturalist 2019\n[55], CIFAR-100 [29] and Food-251 [25]. For multi-label classiﬁcation, our downstream datasets\nwere MS-COCO [34] and Pascal-VOC [16]. For video action recognition, our downstream dataset\nwas Kinetics-200 [26].\nGeneral details:\n• To minimize statistical uncertainty, for datasets with less than 150, 000 images (CIFAR-100,\nFood-251, MS-COCO, Pascal-VOC), we report result of averaging 3 runs with different seeds.\n• All results are reported for input resolution 224.\n• For all downstream datasets we used cutout of 0.5, rand-Augment and true-weight-decay of 1e-4.\n• All single-label datasets are trained with label-smooth of 0.1\n• Unless stated otherwise, dataset was trained for 40 epochs with Adam optimizer, learning rate of\n3e-4, one-cycle policy and and squish-resizing.\nSpeciﬁc dataset details:\n• ImageNet-1K - Since the dataset is bigger than the others, we ﬁnetuned our networks for 100\nepochs using SGD optimizer, and learning rate of 4e-4. We used crop-resizing with the common\nminimal crop factor of 0.08.\n• MS-COCO - We used ASL loss with γ−= 4.\n• Pascal-VOC - We used ASL loss with γ−= 4, and learning rate of 5e-5.\n• Kinetics-200 - we trained for 30 epochs with learning rate of 8e-5. We used the training method\ndescribed in [47], with simple averaging of the embedding from each sample along the video.\nE\nDownstream Results for Different Multi-label Losses\nIn Table 7 we compare downstream results when using multi-label pretraining with vanilla cross-\nentropy (CE) loss and ASL loss. We see that on all downstream datasets, pretraining with ASL\nleads to signiﬁcantly better results\nDataset\nMulti\nLabel\nPretrain\n(CE)\nMulti\nLabel\nPretrain\n(ASL)\nImageNet1K(1)\n79.6\n81.0\niNaturalist(1)\n69.4\n71.0\nFood 251(1)\n74.3\n75.2\nCIFAR 100(1)\n89.9\n90.6\nMS-COCO(2)\n79.1\n80.6\nPascal-VOC(2)\n87.6\n87.9\nKinetics 200(3)\n81.1\n81.9\nTable 7: Comparing downstream results for different losses of multi-label pretraining. Dataset types and\nmetrics: (1) - single-label, top-1 Acc.[%] ; (2) - multi-label, mAP [%]; (3) - action recognition, top-1 Acc. [%].\nF\nCalculating Teacher Conﬁdence\nUsing the teacher prediction for hierarchy i and the semantic ground-truth, we want to evaluate the\nteacher conﬁdence level, Pi, so we can weight properly the contribution of different hierarchies in\nthe KD loss. Our proposed logic for calculating the teacher’s (semantic) conﬁdence is simple:\n- If the ground-truth highest hierarchy is higher than i, set Pi to 1.\n15\n",
    "- Else, calculate the sum probabilities of the top 5% classes in the teacher prediction (we deliberately\ndon’t take only the probability of the highest class, to account for class similarities).\nIn Figure 6 we present the teacher conﬁdence level for different hierarchies, averaged over an epoch.\nWe can see that lower hierarchies have, in average, higher conﬁdence levels. This stems from the\nFigure 6: Teacher average conﬁdence levels for different hierarchies.\nfact that not all hierarchies are relevant for each image. For the picture in Figure 3, for example,\nonly hierarchies 0-5 are relevant, so we expect the teacher will have low conﬁdence for hierarchies\nhigher than 5.\nG\nSemantic KD Vs Regular KD\nDataset\nSingle\nLabel\n+ KD\nPretrain\nSematic\nSoftmax\n+ Semanic KD\nPretrain\nImageNet1K(1)\n81.5\n82.2\niNaturalist(1)\n72.4\n72.7\nFood 251(1)\n76.0\n76.1\nCIFAR 100(1)\n91.0\n91.7\nMS-COCO(2)\n81.6\n82.2\nPascal-VOC(2)\n89.0\n89.8\nKinetics 200(3)\n83.6\n84.4\nTable 9: Comparing KD with different schemes. Dataset types and metrics: (1) - single-label, top-1 Acc.[%]\n; (2) - multi-label, mAP [%]; (3) - action recognition, top-1 Acc. [%].\nH\nImageNet-1K Transfer Learning Results\nModel Name\nImageNet-1K + KD\nTop-1 Acc. [%]\nMobileNetV3\n78.0\nOFA-595\n81.0\nResNet50\n82.0\nMixer-B-16\n82.2\nTResNet-M\n83.1\nTResNet-L\n83.9\nViT-B-16\n84.4\nTable 10: Transfer learning results On ImageNet-1K, when using ImageNet-21K-P pretraining.\n16\n",
    "I\nImageNet-21K-P - Winter21 Split\nFor a fair comparison to previous works, the results in the article are based on the original ImageNet-\n21K images, i.e. we are using Fall11 release of ImageNet-21K (fall11-whole.tar ﬁle), which con-\ntains all the original images and classes of ImageNet-21K. After we processed this release to create\nImageNet-21K-P, we are left with a dataset that contains 11221 classes, where the train set has\n11797632 samples and the test set has 561052 samples. We shall name this variant Fall11 ImageNet-\n21K-P.\nRecently, the ofﬁcial ImageNet site1 used our pre-processing methodology to offer direct download-\ning of ImageNet-21K-P, based on a new release of ImageNet-21K - Winter21 (winter21-whole.tar\nﬁle). Compared to the original dataset, the Winter21 release removed some classes and samples.\nThe Winter21 variant of ImageNet-21K-P is a dataset that contains 10450 classes, where the train\nset has 11060223 samples and the test set has 522500 samples. We shall name this variant Winter21\nImageNet-21K-P.\nFor enabling future comparison and benchmarking, we report the upstream accuracies also on this\nnew variant of ImageNet-21K-P:\nSingle-Label\nTraining Acc. [%]\nMulti-Label Training\nMacro-mAP [%]\nSemantic Softmax\nTraining Acc. [%]\nFall11\nImageNet-21K-P\n45.3\n74.7\n75.6\nWinter21\nImageNet-21K-P\n47.3\n78.7\n77.7\nTable 11: Upstream results, with different pretraining methods, for different variants of ImageNet-21K-P.\nTested model - TResNet-M.\nNote that the Winter21 variant of ImageNet-21K-P contains 10% fewer classes and 6% fewer images.\nIn Table 12 we compare downstream results when using Winter21 and Fall11 variants of ImageNet-\n21K-P\nDataset\nFall11 ImageNet-21K-P\nWinter21 ImageNet-21K-P\nImageNet1K(1)\n81.4\n81.2\niNaturalist(1)\n72.0\n71.8\nFood 251(1)\n75.8\n75.5\nCIFAR 100(1)\n90.4\n90.5\nMS-COCO(2)\n81.3\n81.1\nPascal-VOC(2)\n89.7\n90.1\nKinetics 200(3)\n83.0\n82.8\nTable 12: Comparing downstream results when using different variant of ImageNet-21K-P. All results are\nfor MTResNet model, with semantic softmax pretraining. Dataset types and metrics: (1) - single-label, top-1\nAcc.[%] ; (2) - multi-label, mAP [%]; (3) - action recognition, top-1 Acc. [%].\nWe can see that compared to Fall11 variant, using Winter21 variant leads to a minor reduction in\nperformances on downstream tasks.\nJ\nAdditional Ablation Tests\nIn this section we will bring additional ablation tests and comparisons.\n1www.image-net.org\n17\n",
    "J.1\nComparison to Pretraining on Open Images Dataset\nOpen Images (v6) [31] is a large scale multi-label dataset, which consists of 9 million training images\nand 9600 labels. In Table 13 we compare downstream results when using two different datasets for\npretraining: ImageNet-21K (semantic softmax training) and Open Images (multi-label training).\nDataset\nImageNet-21K\nPretrain\nOpen Images\nPretrain\nImageNet1K(1)\n81.4\n81.0\niNaturalist(1)\n72.0\n70.7\nFood 251(1)\n75.8\n74.8\nCIFAR 100(1)\n90.4\n89.4\nMS-COCO(2)\n81.3\n80.5\nPascal-VOC(2)\n89.7\n89.6\nKinetics 200(3)\n83.0\n81.6\nTable 13: Comparing ImageNet-21K pretraining to Open Images pretraining. Downstream dataset types\nand metrics: (1) - single-label, top-1 Acc.[%] ; (2) - multi-label, mAP [%]; (3) - action recognition, top-1 Acc.\n[%].\nAs we can see, ImageNet-21K pretraining consistently provides better downstream results than Open\nImages. A possible reason is that Open Images, as a multi-label dataset with large number of classes,\nsuffers from the same multi-label optimization pitfalls we described in section 3.2.\nJ.2\nComparison on Additional Non-Classiﬁcation Computer-Vision Tasks\nIn Table 14 and Table 15 we compare 1K and 21K pretraining on two additional computer-vision\ntasks: object detection (MS-COCO dataset) and image retrieval (INRIA holidays dataset).\n1K Pretraining\n21K Pretraining\nmAP [%]\n42.9\n44.3\nTable 14: Comparing downstream results on MS-COCO object detection dataset.\n1K Pretraining\n21K Pretraining\nmAP [%]\n81.1\n82.1\nTable 15: Comparing downstream results on on INRIA Holidays image retrieval dataset.\nWe can see that also on non-classiﬁcation tasks such as object detection and image retrieval, pre-\ntraining on ImageNet-21K translates to better downstream results than ImageNet-1K pretraining.\nJ.3\nImpact of Different Number of Training Samples\nIn Figure 7 we test the impact of the number of training samples on on the upstream accuracies. As\nwe can see, there is no saturation - more training images lead to better semantic accuracies.\nK\nPseudo-code\nIn the following sections we will bring pseudo-code (PyTorch-style) to some components in\nour semantic softmax training scheme: logits sampling, KD calculation and estimating teacher\nconﬁdence.\n18\n",
    "2\n4\n6\n8\n10\n12\nNumber Of Training Images [Millions]\n73.0\n73.5\n74.0\n74.5\n75.0\nSemantic Accuracy[%]\nFigure 7: Upstream results for different number of training images.\nK.1\nLogits Sampling\ndef split_logits_to_semantic_logits(logits, hierarchy_indices_list):\nsemantic_logit_list = []\nfor i, ind in enumerate (hierarchy_indices_list):\nlogits_i = logits[:, ind]\nsemantic_logit_list.append(logits_i)\nreturn semantic_logit_list\nK.2\nKD Logic\ndef calculate_KD_loss(input_student , input_teacher , hierarchy_indices_list):\nsemantic_input_student = split_logits_to_semantic_logits(\ninput_student , hierarchy_indices_list)\nsemantic_input_teacher = split_logits_to_semantic_logits(\ninput_teacher , hierarchy_indices_list)\nnumber_of_hierarchies = len(semantic_input_student)\nlosses_list = []\n# scanning hirarchy_level_list\nfor i in range(number_of_hierarchies):\n# converting to semantic logits\ninputs_student_i = semantic_input_student[i]\ninputs_teacher_i = semantic_input_teacher[i]\n# generating probs\npreds_student_i = stable_softmax(inputs_student_i)\npreds_teacher_i = stable_softmax(inputs_teacher_i)\n# weight MSE−KD distances according to teacher confidence\nloss_non_reduced = torch.nn.MSELoss(reduction =’none’)(preds_student_i ,\npreds_teacher_i)\nweights_batch = estimate_teacher_confidence(preds_teacher_i)\nloss_weighted = loss_non_reduced * weights_batch.unsqueeze (1)\nlosses_list.append(torch.sum(loss_weighted))\nreturn sum(losses_list)\nK.3\nTeacher Conﬁdence\n19\n",
    "def estimate_teacher_confidence(preds_teacher)\nwith torch.no_grad():\nnum_elements = preds_teacher.shape[1]\nnum_elements_topk = int(np.ceil(num_elements / 20))\n# top 5%\nweights_batch = torch.sum(torch.topk(preds_teacher ,\nnum_elements_topk).values, dim=1)\nreturn weights_batch\nL\nLimitations\nIn this section we will discuss some of the limitations of our proposed pipeline for pretraining on\nImageNet-21K:\n1) While our work did put a large emphasis on the efﬁciency of the proposed pretraining pipeline,\nfor reasonable training times we still need an 8-GPUs machine (1 GPU training will be quite long,\n2-3 weeks).\n2) For creating an efﬁcient pretraining scheme, and also to stay within our inner computing budget,\nwe did not incorporate training tricks that signiﬁcantly increase training times, although some of\nthese tricks might give additional beneﬁts and improve pretraining quality.\nAn example - techniques for dealing with extreme multi-tasking, such as GradNorm [7] and PCGrad\n[60], that would probably improve the pretrain quality of multi-label training, but would signiﬁcantly\nincrease training times.\nAnother example of methods from the literature we have not tested - general \"semantic\" techniques\nthat can be used for training neural networks ([4, 54] for example). We found that most of these\ntechniques are not feasible for large-scale efﬁcient training. In addition, we believe that since our\nnovel method, semantic softmax, is designed and tailored to the speciﬁc needs and characterizations\nof ImageNet-21K, it will signiﬁcantly outperform general semantic methods.\n3) When using private datasets which are larger than ImageNet-21K, such as JFT-300M [49], the\npretrain quality that can be achieved is probably still higher than the one we offer.\n20\n"
  ],
  "full_text": "arXiv:2104.10972v4  [cs.CV]  5 Aug 2021\nImageNet-21K Pretraining for the Masses\nTal Ridnik\nDAMO Academy, Alibaba Group\ntal.ridnik@alibaba-inc.com\nEmanuel Ben-Baruch\nDAMO Academy, Alibaba Group\nemanuel.benbaruch@alibaba-inc.com\nAsaf Noy\nDAMO Academy, Alibaba Group\nasaf.noy@alibaba-inc.com\nLihi Zelnik-Manor\nDAMO Academy, Alibaba Group\nlihi.zelnik@alibaba-inc.com\nAbstract\nImageNet-1K serves as the primary dataset for pretraining deep learning models\nfor computer vision tasks. ImageNet-21K dataset, which is bigger and more di-\nverse, is used less frequently for pretraining, mainly due to its complexity, low\naccessibility, and underestimation of its added value. This paper aims to close\nthis gap, and make high-quality efﬁcient pretraining on ImageNet-21K available\nfor everyone. Via a dedicated preprocessing stage, utilization of WordNet hi-\nerarchical structure, and a novel training scheme called semantic softmax, we\nshow that various models signiﬁcantly beneﬁt from ImageNet-21K pretraining\non numerous datasets and tasks, including small mobile-oriented models. We\nalso show that we outperform previous ImageNet-21K pretraining schemes for\nprominent new models like ViT and Mixer. Our proposed pretraining pipeline\nis efﬁcient, accessible, and leads to SoTA reproducible results, from a publicly\navailable dataset.\nThe training code and pretrained models are available at:\nhttps://github.com/Alibaba-MIIL/ImageNet21K\n1\nIntroduction\nImageNet-1K dataset, introduced for the ILSVRC2012 visual recognition challenge [45], has been\nat the center of modern advances in deep learning [30, 20, 46]. ImageNet-1K serves as the main\ndataset for pretraining of models for computer-vision transfer learning [51, 33, 21], and improving\nperformances on ImageNet-1K is often seen as a litmus test for general applicability on downstream\ntasks [28, 62, 44]. ImageNet-1K is a subset of the full ImageNet dataset [11], which consists of\n14,197,122 images, divided into 21,841 classes. We shall refer to the full dataset as ImageNet-21K,\nfollowing [27] (although other papers sometimes described it as ImageNet-22K [8]). ImageNet-1K\nwas created by selecting a subset of 1.2M images from ImageNet-21K, that belong to 1000 mutually\nexclusive classes.\nEven though some previous works showed that pretraining on ImageNet-21K could provide better\ndownstream results for large models [27, 14], pretraining on ImageNet-1K remained far more popu-\nlar. A main reason for this discrepancy is that ImageNet-21K labels are not mutually exclusive - the\nlabels are taken from WordNet [38], where each image is labeled with one label only, not necessar-\nily at the highest possible hierarchy of WordNet semantic tree. For example, ImageNet-21K dataset\ncontains the labels \"chair\" and \"furniture\". A picture, with an actual chair, can sometimes be labeled\nas \"chair\", but sometimes be labeled as the semantic parent of \"chair\", \"furniture\". This kind of tag-\nging methodology complicates the training process, and makes evaluating models on ImageNet-21K\nless accurate. Other challenges of ImageNet-21K dataset are the lack of ofﬁcial train-validation split,\nthe fact that training is longer than ImageNet-1K and requires highly efﬁcient training schemes, and\nthat the raw dataset is large - 1.3TB.\n\n\nFigure 1: Our end-to-end pretraining pipeline on ImageNet-21K. We start with a dataset preparation and\npreprocessing stage. Via WordNet’s synsets, we convert all the single-label inputs to semantic multi-labels,\nresulting in a semantic structure for ImageNet-21K, with 11 possible hierarchies. For each hierarchy, we apply\na dedicated softmax activation, and aggregate the losses with hierarchy balancing.\nSeveral past works have used ImageNet-21K for pretraining, mostly in comparison to larger datasets,\nwhich are not publicly available, such as JFT-300M [49]. [40] and [43] used ImageNet-21K and JFT-\n300M to train expert models according to the datasets hierarchies, and combined them to ensembles\non downstream tasks; [27] and [14] compared pretraining JFT-300M to ImageNet-21K on large\nmodels such as ViT and ResNet-50x4. Many papers used these pretrained models for downstream\ntasks (e.g., [63, 41, 36, 1]). There are also works on ImageNet-21K that did not focus on pretraining:\n[61] used extra (unlabled) data from ImageNet-21K to improve knowledge-distillation training on\nImageNet-1K; [13] used ImageNet-21k for testing few-shot learning; [56] tested efﬁcient softmax\nschemes on ImageNet-21k; [17] tested pooling operations schemes on animal-oriented subset of\nImageNet-21k.\nHowever, previous works have not methodologically studied and optimized a pretraining process\nspeciﬁcally for ImageNet-21K. Since this is a large-scale, high-quality, publicly available dataset,\nthis kind of study can be highly beneﬁcial to the community. We wish to close this gap in this\nwork, and make efﬁcient top-quality pretraining on ImageNet-21K accessible to all deep learning\npractitioners.\nOur pretraining pipeline starts by preprocessing ImageNet-21K to ensure all classes have enough\nimages for a meaningful learning, splitting the dataset to a standardized train-validation split, and\nresizing all images to reduce memory footprint. Using WordNet semantic tree [38], we show that\nImageNet-21K can be transformed into a (semantic) multi-label dataset. We thoroughly analyze\nthe advantages and disadvantages of single-label and multi-label training. Extensive tests on down-\nstream tasks show that multi-label pretraining does not improve results on downstream tasks, despite\nhaving more information per image. To effectively utilize the semantic data, we develop a novel\ntraining method, called semantic softmax, which exploits the hierarchical structure of ImageNet-\n21K tagging to train the network over several semantic softmax layers, instead of the single layer.\nUsing semantic softmax pretraining, we consistently outperform both single-label and multi-label\npretraining on downstream tasks. By integrating semantic softmax into a dedicated semantic knowl-\nedge distillation loss, we further improved results. The complete end-to-end pretraining pipeline\nappears in Figure 1.\nUsing semantic softmax pretraining on ImageNet-21K we achieve signiﬁcant improvement on nu-\nmerous downstream tasks, compared to standard ImageNet-1K pretraining. Unlike previous works,\nwhich focused on pretraining of large models only [27], we show that ImageNet-21K pretraining\nbeneﬁts a wide variety of models, from larger models like TResNet-L [44], through medium-sized\nmodels like ResNet50 [20], and even small mobile-dedicated models like OFA-595 [5] and Mo-\nbileNetV3 [21]. Our proposed pretraining scheme also outperforms previous ImageNet-21K pre-\ntraining schemes that were used to trained MLP-based models like Vision-Transformer (ViT) [14]\nand Mixer [53].\nThe paper’s contribution can be summarized as follows:\n• We develop a methodical preprocess procedure to transform raw ImageNet-21K into a viable\ndataset for efﬁcient, high-quality pretraining.\n• Using WordNet semantic tree, we convert each (single) label to semantic multi labels, and com-\npare the pretrain quality of two baseline methods: single-label and multi-label pretraining. We\n2\n\n\nshow that while a multi-label approach provides more information per image, it can have signiﬁ-\ncant optimization drawbacks, resulting in inferior results on downstream tasks.\n• We develop a novel training scheme called semantic softmax, which exploits the hierarchical\nstructure of ImageNet-21K. With semantic softmax pretraining, we outperform both single-label\nand multi-label pretraining on downstream tasks. We further improve results by integrating se-\nmantic softmax into a dedicated semantic knowledge distillation scheme.\n• Via extensive experimentations, we show that compared to ImageNet-1K pretraining, ImageNet-\n21K pretraining signiﬁcantly improves downstream results for a wide variety of architectures,\ninclude mobile-oriented ones. In addition, our ImageNet-21K pretraining scheme consistently\noutperforms previous ImageNet-21K pretraining schemes for prominent new models like ViT\nand Mixer.\n2\nDataset Preparation\n2.1\nPreprocessing ImageNet-21K\nOur preprocessing stage consists of three steps, as described in Figure 1 (leftmost image): (1) invalid\nclasses cleaning, (2) creating a validation set, (3) image resizing. Details are as follows:\nStep 1 - cleaning invalid classes: the original ImageNet-21K dataset [11] consists of 14,197,122\nimages, each tagged in a single-label fashion by one of 21,841 possible classes. The dataset has no\nofﬁcial train-validation split, and the classes are not well-balanced - some classes contain only 1-10\nsamples, while others contain thousands of samples. Classes with few samples cannot be learned\nefﬁciently, and may hinder the entire training process and hurt the pretrain quality [23]. Hence we\nstart our preprocessing stage by removing infrequent classes, with less than 500 labels. After this\nstage, the dataset contains 12,358,688 images from 11,221 classes. Notice that the cleaning process\nreduced the number of total classes by half, but removed only 13% of the original pictures.\nStep 2 - validation split: we allocate 50 images per class for a standardized validation split, that\ncan be used for future benchmarks and comparisons.\nStep 3 - image resizing: ImageNet-1K training usually uses crop-resizing [22] which favours load-\ning the original images at full resolution and resizing them on-the-ﬂy. To make ImageNet-21K\ndataset more accessible and accelerate training, we resized during the preprocessing stage all the\nimages to 224 resolution (equivalent to squish-resizing [22]). While somewhat limiting scale aug-\nmentations, this stage signiﬁcantly reduces the dataset’s memory footprint, from 1.3TB to 250GB,\nand makes loading the data during training faster.\nAfter ﬁnishing the preprocessing stage, we kept only valid classes, produced a standardized train-\nvalidation split, and signiﬁcantly reduced the dataset size. We shall name this processed dataset\nImageNet-21K-P (P for Processed).\n2.2\nUtilizing Semantic Data\nWe now wish to analyze the semantic structure of ImageNet-21K-P dataset. This structure will\nenable us to better understand ImageNet-21K-P tagging methodology, and employ and compare\ndifferent pretraining schemes.\nFrom single labels to semantic multi labels\nEach image in the original ImageNet-21K dataset\nwas labeled with a single label, that belongs to WordNet synset [38]. Using the WordNet synset\nhyponym (subtype) and hypernym (supertype) relations, we can obtain for each class its parent class,\nif exists, and a list of child classes, if exists. When applying the parenthood relation recursively, we\ncan build a semantic tree, that enables us to transform ImageNet-21K-P dataset into a multi-label\ndataset, where each image is associated with several labels - the original label, and also its parent\nclass, parent-of-parent class, and so on. Example is given in Figure 1 (middle image) - the original\nimage was labeled as ’swan’, but by utilizing the semantic tree, we can produce a list of semantic\nlabels for the image - ’animal, vertebrate, bird, aquatic bird, swan’. Notice that the labels are sorted\nby hierarchy: ’animal’ label belongs to hierarchy 0, while ’swan’ label belongs to hierarchy 4. A\nlabel from hierarchy k has k ancestors.\nUnderstanding the inconsistent tagging methodology\nThe semantic structure of ImageNet-21K\nenables us to understand its tagging methodology better. According to the stated tagging method-\nology of ImageNet-21K [11], we are not guaranteed that each image was labeled at the highest\n3\n\n\nFigure 2:\nExample of inconsistent tagging in\nImageNet-21K dataset. Two pictures containing the\nsame animal were labeled differently.\nHierarchy\nExample Classes\n0\nperson, animal, plant,\nfood, artifact\n1\ndomestic animal,\nbasketball court, clothing\n...\n6\nwhitetip shark, ortolan,\ngrey kingbird\nTable 1:\nExamples of classes from different\nImageNet-21K-P hierarchies.\npossible hierarchy. An example is given in Figure 2. Two pictures, that contain the animal cow,\nwere labeled differently - one with the label ’animal’, the other with the label ’cow’. Notice that\n’animal’ is a semantic ancestor of ’cow’ (cow →placental →mammal →vertebrate →animal).\nThis kind of incomplete tagging methodology, which is common in large datasets [32, 42], hinders\nand complicates the training process. A dedicated scheme that tackles this tagging methodology will\nbe presented in section 3.3.\nSemantic statistics\nBy using WordNet synsets, we can calculate for each class the number of\nancestors it has - its hierarchy. In total, our processed dataset, ImageNet-21K-P, has 11 possible\nhierarchies. Example of classes from different hierarchies appears in Table 1. In Figure 4 in appendix\nA we present the number of classes per hierarchy. We see that while there are 11 possible hierarchies,\nthe vast majority of classes belong to the lower hierarchies.\n3\nPretraining Schemes\nIn this section, we will review and analyze two baseline schemes for pretraining on ImageNet-21K-\nP: single-label and multi-label training. We will also develop a novel new scheme for pretraining on\nImageNet-21K-P, semantic softmax, and analyze its advantages over the baseline schemes.\n3.1\nSingle-label Training Scheme\nThe straightforward way to pretrain on ImageNet-21K-P is to use the original (single) labels, apply\nsoftmax on the output logits, and use cross-entropy loss. Our single-label training scheme is similar\nto common efﬁcient training schemes on ImageNet-1K [44], with minor adaptations to better handle\nthe inconsistent tagging (Full training details appear in appendix B.1). Since we aim for an efﬁcient\nscheme with maximal throughput, we don’t incorporate any tricks that might signiﬁcantly increase\ntraining times. To further shorten the training times, we propose to initialize the models from\nstandard ImageNet-1K training, and train on ImageNet-21K-P for 80 epochs. On 8xV100 NVIDIA\nGPU machine, mixed-precision training takes 40 minutes per epoch for ResNet50 and TResNet-M\narchitectures (∼5000 img\nsec ), leading to a total training time of 54 hours. Similar accuracies are\nobtained when doing random initialization, but training the models longer - 140 epochs.\nPros of using single-label training\n• Well-balanced dataset - with single-label training on ImageNet-21K-P, the dataset is well-\nbalanced, meaning each class appears, roughly, the same number of times.\n• Single-loss training - training with a softmax (a single loss) makes convergence easy and efﬁ-\ncient, and avoids many optimization problems associated with multi-loss learning, such as differ-\nent gradient magnitudes and gradient interference [60, 7, 9].\nCons of using single-label training\n• Inconsistent tagging - due to the tagging methodology of ImageNet-21K-P, where we are not\nguaranteed that an image was labeled at the highest possible hierarchy, ground-truth labels are\ninherently inconsistent. Pictures, containing the same object, can appear with different single-\nlabel tagging (see Figure 2 for example).\n4\n\n\n• No semantic data - during training, we are not presenting semantic data via the single-label\nground-truth.\n3.2\nMulti-label Training Scheme\nUsing the semantic tree, we can convert any (single) label to semantic multi labels, and train our\nmodels on ImageNet-21K-P in a multi-label fashion, expecting that the additional semantic infor-\nmation per image will improve the pretrain quality. As commonly done in multi-label classiﬁcation\n[3], we reduce the problem to a series of binary classiﬁcation tasks. Given N labels, the base net-\nwork outputs one logit per label, zn, and each logit is independently activated by a sigmoid function\nσ(zn). Let’s denote yn as the ground-truth for class n. The total classiﬁcation loss, Ltot, is obtained\nby aggregating a binary loss from the N labels:\nLtot =\nN\nX\nn=1\nL (σ(zn), yn) .\n(1)\nEq. 1 formalizes multi-label classiﬁcation as a multi-task problem. Since we have a large number\nof classes (11, 221), this is an extreme multi-task case. For training, we adopted the high-quality\ntraining scheme described in [3], that provided state-of-the-art results on large-scale multi-label\ndatasets such as Open Images [32]. Full training details appear in appendix B.2.\nPros of using multi-label training\n• More information per image - we present for each image all the available semantic labels.\n• Tagging and metrics are more accurate - if an image was originally given a single label at\nhierarchy k, with multi-label training we are guaranteed that all ground-truth labels at hierarchies\n0 to k are accurate. Hence, multi-label training partly mitigates the inconsistent tagging problem,\nand makes training metrics more accurate and reﬂective than single-label training.\nCons of using multi-label training\n• Extreme multi-tasking - with multi-label training, each class is learned separately (sigmoids\ninstead of softmax). This extreme multi-task learning makes the optimization process harder and\nless efﬁcient, and may cause convergences to a local minimum [60, 7, 15].\n• Extreme imbalancing - as a multi-label dataset with many classes, ImageNet-21K-P suffers from\na large positive-negative imbalance [3]. In addition, due to the semantic structure, multi-label\ntraining is hindered by a large class imbalance [24] - on average, classes from a lower hierarchy\nwill appear far more frequent than classes from a higher hierarchy.\nIn appendices C.2 and E we show that for multi-label training, ASL loss [3], that was designed to\ncope with large positive-negative imbalancing, signiﬁcantly outperforms cross-entropy loss, both on\nupstream and downstream tasks. This supports our analysis of extreme imbalancing as a major opti-\nmization challenge of multi-label training. Notice that we also list extreme multi-tasking as another\noptimization pitfall of multi-label training, and a dedicated scheme for dealing with it might further\nimprove results. However, most methods that tackle multi-task learning, such as GradNorm [7] and\nPCGrad [60], require computation of gradients for each class separately. This is computationally\ninfeasible for a dataset with a large number of classes, such as ImageNet-21K-P.\n3.3\nSemantic Softmax Training Scheme\nOur goal is to develop a dedicated training scheme that utilizes the advantages of both the single-\nlabel and the multi-label training. Speciﬁcally, our scheme should present for each input image all\nthe available semantic labels, but use softmax activations instead of independent sigmoids to avoid\nextreme multi-tasking. We also want to have fully accurate ground-truth and training metrics, and\nprovide the network direct data on the semantic hierarchies (this is not achieved even in multi-label\ntraining, the hierarchical structure there is implicit). In addition, the scheme should remain efﬁcient\nin terms of training times.\nSemantic softmax formulation\nTo meet these goals, we develop a new training scheme called\nsemantic softmax training. As we saw in section 2.2, each label in ImageNet-21K-P can belong to\n5\n\n\none of 11 possible hierarchies. By deﬁnition, for each hierarchy there can be only one ground-truth\nlabel per input image. Hence, instead of single-label training with a single softmax, we shall have 11\nsoftmax layers, for the 11 different hierarchies. Each softmax will sample the relevant logits from\nthe corresponding hierarchy, as shown in Figure 1 (rightmost image). To deal with the partial tagging\nof ImageNet-21K-P, not all softmax layers will propagate gradients from each sample. Instead, we\nwill activate only softmax layers from the relevant hierarchies. An example is given in Figure 3 -\nthe original image had a label from hierarchy 5. We transform it to 6 semantic ground-truth labels,\nfor hierarchies 0-5, and activate only the 6 ﬁrst semantic softmax layers (only activated layers will\npropagate gradients). Compared to single-label and multi-label schemes, semantic softmax training\nscheme has the following advantages:\n1. We avoid extreme multi-tasking (11, 221 uncoupled losses in multi-label training). Instead,\nwe have only 11 losses, as the number of softmax layers.\n2. We present for each input image all the possible semantic labels. The loss scheme even\nprovides direct data on the hierarchical structure.\n3. Unlike single-label and multi-label training, semantic softmax ground-truth and training met-\nrics are fully accurate. If a sample has no labels at hierarchy k, we don’t propagate gradients\nfrom the kth softmax during training, and ignore that hierarchy for metrics calculation (A\ndedicated metrics for semantic softmax training is deﬁned in appendix C.3).\n4. Calculating several softmax activations instead of a single one has negligible overhead, and\nin practice training times are similar to single-label training.\nFigure 3: Gradient propagation logic of semantic\nsoftmax training.\nWeighting the different softmax layers\nFor\neach input image we have K losses (11). As\ncommonly done in multi-task training [7], we\nneed to aggregate them to a single loss. A naive\nsolution will be to sum them: Ltot = PK−1\nk=0 Lk\nwhere Lk, the loss per softmax layer, is zero\nwhen the layer is not activated. However, this\nformulation ignores the fact that softmax lay-\ners at lower hierarchies will be activated much\nmore frequently than softmax layers at higher\nhierarchies, resulting in over-emphasizing of\nclasses from lower hierarchies. To account for\nthis imbalancing, we propose a balancing logic:\nlet Nj be the total number of classes in hierar-\nchy j (as presented in Figure 4). Due to the semantic structure, the relative number of occurrences\nof hierarchy k in the loss function will be:\nOk =\nk−1\nX\nj=0\nNj\n(2)\nHence, to balance the contribution of different hierarchies we can use a normalization factor Wk =\n1\nOk , and obtain a balanced aggregation loss, that will be used for semantic softmax training:\nLtot =\nK−1\nX\nk=0\nWkLk\n(3)\n3.4\nSemantic Knowledge Distillation\nKnowledge distillation (KD) is a known method to improve not only upstream, but also downstream\nresults [58, 59, 61]. We want to combine our semantic softmax scheme with KD training - semantic\nKD. In addition to the general beneﬁt from KD training [19], for ImageNet-21K-P semantic KD has\nan additional beneﬁt - it can predict the missing tags that arise from the inconsistent tagging. For\nexample, for the left picture in Figure 2, the teacher model can predict the missing labels - ’cow,\nplacental, mammal, vertebrate’. To implement semantic KD loss, for each hierarchy we will calcu-\nlate both the teacher and the student the corresponding probability distributions {Ti}K−1\ni=0 , {Si}K−1\ni=0 .\nThe KD loss of hierarchy i will be:\nLKDi = KDLoss(Ti, Si)\n(4)\n6\n\n\nwhere KDLoss is a standard measurement for the distance between distributions, that can be chosen\nas Kullback-Leibler divergence [19, 58], or as MSE loss [2, 52]. We have found that the latter\nconverges faster, and used it. A vanilla implementation for the total loss will be a simple sum of\nthe losses from different hierarchies: LKD = PK−1\ni=0 LKDi. However, this formulation assumes\nthat all the hierarchies are relevant for each image. This is inaccurate - usually higher hierarchies\nrepresent subspecies of animals or plants, and are not applicable for a picture of a chair, for example.\nSo we need to determine from the teacher predictions which hierarchies are relevant, and weigh\nthe different losses accordingly. Let’s assume that for each hierarchy we can calculate the teacher\nconﬁdence level, Pi. A conﬁdence-weighted KD loss will be:\nLKD =\nK−1\nX\ni=0\nPiLKDi\n(5)\nEq. 5 is our proposed semantic KD loss. In appendix F we present a method to calculate the teacher\nconﬁdence level, Pi, from the teacher predictions, similar to [58].\n4\nExperimental Study\nIn this section, we will present upstream and downstream results for the different training schemes,\nand show that semantic softmax pretraining outperforms single-label and multi-label pretraining.\nWe will also demonstrate how semantic KD further improves results on downstream tasks.\n4.1\nUpstream Results\nIn appendix C we provide upstream results for the three training schemes. Since each scheme has\ndifferent training metrics, we cannot use these results to directly compare (pre)training quality.\n4.2\nDownstream Results\nTo compare the pretrain quality of different training schemes, we will test our models via transfer\nlearning. To ensure that we are not overﬁtting a speciﬁc dataset or task, we chose a wide variety of\ndownstream datasets, from different computer-vision tasks. We also ensured that our downstream\ndatasets represent a variety of domains, and have diverse sizes - from small datasets of thousands\nof images, to larger datasets with more than a million images. For single-label classiﬁcation, we\ntransferred our models to ImageNet-1K [30], iNaturalist 2019 [55], CIFAR-100 [29] and Food 251\n[25]. For multi-label classiﬁcation, we transferred our models to MS-COCO [34] and Pascal-VOC\n[16] datasets. For video action recognition, we transferred our models to Kinetics 200 dataset [26].\nIn appendix D we provide full training details on all downstream datasets.\nComparing different pretraining schemes\nIn Table 2 we compare downstream results for three\npretraining schemes: single-label, multi-label and semantic softmax. We see that on 6 out of 7\nDataset\nSingle\nLabel\nPretrain\nMutli\nLabel\nPretrain\nSemantic\nSoftmax\nPretrain\nSemantic\nSoftmax\nPretrain + KD\nImageNet1K(1)\n81.1\n81.0\n81.4\n82.2\niNaturalist(1)\n71.5\n71.0\n72.0\n72.7\nFood 251(1)\n75.4\n75.2\n75.8\n76.1\nCIFAR 100(1)\n89.5\n90.6\n90.4\n91.7\nMS-COCO(2)\n80.8\n80.6\n81.3\n82.2\nPascal-VOC(2)\n88.1\n87.9\n89.7\n89.8\nKinetics 200(3)\n81.9\n81.9\n83.0\n84.4\nTable 2: Comparing downstream results for different pretraining schemes. Darker cell color means better\nscore. Dataset types and metrics: (1) - single-label, top-1 Acc.[%] ; (2) - multi-label, mAP [%]; (3) - action\nrecognition, top-1 Acc. [%].\ndatasets tested, semantic softmax pretraining outperforms both single-label and multi-label pretrain-\n7\n\n\ning. In addition, we see from Table 2 that single-label pretraining performs better than multi-label\npretraining (scores are higher on 5 out of 7 datasets tested).\nThese results support our analysis of the pros and cons of the different pretraining schemes from Sec-\ntion 3: with multi-label training, we have more information per input image, but the optimization\nprocess is less efﬁcient due to extreme multi-tasking and extreme imbalancing. All-in-all, multi-\nlabel training does not improve downstream results. Single-label training, despite its shortcomings\nfrom the partial tagging methodology and the minimal information per image, provides a better pre-\ntraining baseline. Semantic softmax scheme, which utilizes semantic data without the optimization\npitfalls of extreme multi-label training, outperforms both single-label and multi-label training.\nSemantic KD\nIn Table 2 we also compare the downstream results of semantic softmax pretraining,\nwith and without semantic KD. We see that on all tasks and datasets tested, adding semantic KD to\nour pretraining process improves downstream results. Indeed the ability of semantic KD to ﬁll in\nthe missing tags and provide a smoother and more informative ground-truth is translated to better\ndownstream results. In appendix G we compare single-label pretraining with KD, to semantic soft-\nmax pretraining with semantic KD, and show that the latter achieves better results on downstream\ntasks.\n5\nResults\nIn the previous chapters we developed a dedicated pretraining scheme for ImageNet-21K-P dataset,\nsemantic softmax, and showed that it outperforms two baseline pretraining schemes, single-label\nand multi-label, in terms of downstream results. Now we wish to compare our semantic softmax\npretraining on ImageNet-21K-P to other known pretraining schemes and pretraining datasets.\n5.1\nComparison to Other ImageNet-21K Pretraining Schemes\nWe want to compare our proposed training scheme to other ImageNet-21K training schemes from the\nliterature. However, to the best of our knowledge, no previous works have published their upstream\nresults on ImageNet-21K, or shared thorough details about their training scheme or preprocessing\nstage. Recently, prominent new models called ViT [14] and Mixer [53] were published, and ofﬁcial\npretrained weights were released [18]. In Table 3 we compare downstream results when using the\nofﬁcial ImageNet-21K weights, and when using weights from semantic softmax pretraining.\nViT-B-16\nMixer-B-16\nDataset\nOfﬁcial\nImageNet-21K\nPretrain\nOur\nImageNet-21K\nPretrain\nOfﬁcial\nImageNet-21K\nPretrain\nOur\nImageNet-21K\nPretrain\nImageNet1K(1)\n83.3\n83.9\n79.7\n82.0\niNaturalist(1)\n71.7\n73.1\n62.2\n66.6\nFood 251(1)\n74.6\n76.0\n69.9\n74.5\nCIFAR 100(1)\n92.7\n94.2\n85.5\n92.3\nMS-COCO(2)\n81.1\n82.6\n74.1\n80.9\nPascal-VOC(2)\n78.7\n93.1\n63.1\n88.6\nKinetics 200(3)\n82.7\n84.1\n79.3\n82.1\nTable 3: Comparing downstream results for different pretraining schemes. Dataset types and metrics: (1)\n- single-label, top-1 Acc.[%] ; (2) - multi-label, mAP [%]; (3) - action recognition, top-1 Acc. [%].\nWe see from Table 3 that our pretraining scheme signiﬁcantly outperforms the ofﬁcial pretrain, on\nall downstream tasks tested. Previous works have observed that MLP-based models can be harder\nand less stable to use in transfer learning since they don’t have inherent translation inductive bias\n[6, 39, 35]. When using the ofﬁcial weights, we also noticed this phenomenon on some datasets\n(Pascal-VOC, for example). Using semantic softmax pretraining, the transfer learning training was\nmore stable and robust, and reached higher accuracy.\n8\n\n\nDataset\nMobileNetV3\nOFA595\nResNet50\nTResNet-M\nTResNet-L\n1K\n21K\n1K\n21K\n1K\n21K\n1K\n21K\n1K\n21K\niNaturalist(1)\n62.4\n65.0\n69.0\n71.5\n66.8\n71.4\n70.1\n72.7\n72.4\n74.8\nCIFAR100(1)\n86.7\n88.5\n88.3\n90.3\n86.8\n90.3\n89.5\n91.7\n90.2\n92.5\nFood 251(1)\n70.1\n70.3\n72.9\n73.5\n72.2\n74.0\n75.1\n76.1\n76.3\n77.0\nMS-COCO(2)\n73.0\n74.9\n74.9\n77.7\n76.7\n80.5\n79.5\n82.2\n81.1\n83.7\nPascal-VOC(2)\n72.1\n72.4\n72.4\n81.5\n86.9\n87.9\n85.8\n89.8\n88.2\n92.5\nKinetics200(3)\n72.2\n74.3\n73.2\n78.1\n78.2\n81.3\n80.5\n84.3\n82.1\n84.6\nTable 4: Comparing downstream results for ImageNet-1K standard pretraining, and our proposed\nImageNet-21K-P pretraining scheme. (1) - single-label dataset, top-1 Acc [%] metric; (2) - multi-label\ndataset, mAP [%] metric; (3) - action recognition dataset, top-1 Acc [%] metric.\n5.2\nComparison to ImageNet-1K Pretraining\nIn Table 4 we compare downstream results, for different models, when using ImageNet-1K pre-\ntraining (taken from [57]), and when using our ImageNet-21K-P pertraining. We can see that our\npretraining scheme signiﬁcantly outperforms standard ImageNet-1K pretraining on all datasets, for\nall models tested. For example, on iNaturalist dataset we improve the average top-1 accuracy by\n2.9%.\nNotice that some previous works stated that pretraining on a large dataset beneﬁts only large models\n[27, 49]. MobileNetV3 backbone, for example, has only 4.2M parameters, while ViT-B model has\n85.6M parameters. Previous works assumed that a large number of parameters, like ViT has, is\nneeded to properly utilize pretraining on large datasets. However, we show consistently and signif-\nicantly that even small mobile-oriented models, like MobileNetV3 and OFA-595, can beneﬁt from\npretraining on a large (publicly available) dataset like ImageNet-21K-P. Due to their fast inference\ntimes and reduced heating, mobile-oriented models are used frequently for deployment. Hence, im-\nproving their downstream results by using better pretrain weights can enhance real-world products,\nwithout increasing training complexity or inference times.\n5.3\nImageNet-1K SoTA Results\nIn Table 10 in appendix H we bring downstream results on ImageNet-1K for different models, when\nusing ImageNet-21K-P semantic softmax pretraining. To achieve top results, similar to previous\nworks [33, 61, 58], we added standard knowledge distillation loss into our ImageNet-1K training.\nTo the best of our knowledge, for all the models in Table 10 we achieve a new SoTA record (for\ninput resolution 224). Unlike previous top works, which used private datasets [49], we are using a\npublicly available dataset for pretraining. Note that the gap from the original reported accuracies\nis signiﬁcant. For example, MobileNetV3 reported accuracy was 75.2% [21] - we achieved 78.0%;\nResNet50 reported accuracy was 76.0% [20] - we achieved 82.0%.\n5.4\nAdditional Comparisons and Results\nIn appendix J we bring additional comparisons: (1) Comparison to Open Images pretraining; (2)\nDownstream results comparison on additional non-classiﬁcation computer-vision tasks; (3) Impact\nof different number of training samples on upstream results.\n6\nConclusion\nIn this paper, we presented an end-to-end scheme for high-quality efﬁcient pretraining on ImageNet-\n21K dataset. We start by standardizing the dataset preprocessing stage. Then we show how we can\ntransform ImageNet-21K dataset into a multi-label one, using WordNet semantics. Via extensive\ntests on downstream tasks, we demonstrate how single-label training outperforms multi-label train-\ning, despite having less information per image. We then develop a new training scheme, called se-\nmantic softmax, which utilizes ImageNet-21K hierarchical structure to outperform both single-label\nand multi-label training. We also integrate the semantic softmax scheme into a dedicated knowledge\ndistillation loss to further improve results. On a variety of computer vision datasets and tasks, dif-\n9\n\n\nferent architectures signiﬁcantly and consistently beneﬁt from our pretraining scheme, compared to\nImageNet-1K pretraining and previous ImageNet-21K pretraining schemes.\nBroader Impact\nIn the past, pretraining on ImageNet-21K was out of scope for the common deep\nlearning practitioner. With our proposed pipeline, high-quality efﬁcient pretraining on ImageNet-\n21K will be more accessible to the deep learning community, enabling researchers to design new\narchitectures and pretrain them to top results, without the need for massive computing resources\nor large-scale private datasets. In addition, our ﬁndings that even small mobile-oriented models\nsigniﬁcantly beneﬁt from large-scale pretraining can be used to enhance real-world products. Finally,\nour improved pretraining scheme on ImageNet-21K can support prominent MLP-based models that\nrequire large-scale pretraining, like ViT and Mixer.\nReferences\n[1] Shekoofeh Azizi, Basil Mustafa, Fiona Ryan, Zachary Beaver, Jan Freyberg, Jonathan Deaton, Aaron\nLoh, Alan Karthikesalingam, Simon Kornblith, Ting Chen, Vivek Natarajan, and Mohammad Norouzi.\nBig self-supervised models advance medical image classiﬁcation, 2021.\n[2] Lei Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? arXiv preprint arXiv:1312.6184,\n2013.\n[3] Emanuel Ben-Baruch, Tal Ridnik, Nadav Zamir, Asaf Noy, Itamar Friedman, Matan Protter, and Lihi\nZelnik-Manor. Asymmetric loss for multi-label classiﬁcation. arXiv preprint arXiv:2009.14119, 2020.\n[4] Clemens-Alexander Brust and Joachim Denzler. Integrating domain knowledge: using hierarchies to\nimprove deep classiﬁers. In Asian Conference on Pattern Recognition, pages 3–16. Springer, 2019.\n[5] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once-for-all: Train one network\nand specialize it for efﬁcient deployment. arXiv preprint arXiv:1908.09791, 2019.\n[6] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised visual trans-\nformers. arXiv preprint arXiv:2104.02057, 2021.\n[7] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normal-\nization for adaptive loss balancing in deep multitask networks. In International Conference on Machine\nLearning, pages 794–803. PMLR, 2018.\n[8] Valeriu Codreanu, Damian Podareanu, and Vikram Saletore. Scale out for large minibatch sgd: Resid-\nual network training on imagenet-1k with improved accuracy and reduced time to train. arXiv preprint\narXiv:1711.04291, 2017.\n[9] Michael Crawshaw.\nMulti-task learning with deep neural networks:\nA survey.\narXiv preprint\narXiv:2009.09796, 2020.\n[10] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data\naugmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition Workshops, pages 702–703, 2020.\n[11] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical\nImage Database. In CVPR09, 2009.\n[12] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with\ncutout. arXiv preprint arXiv:1708.04552, 2017.\n[13] Guneet S Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline for few-shot\nimage classiﬁcation. arXiv preprint arXiv:1909.02729, 2019.\n[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-\nterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[15] Yunshu Du, Wojciech M Czarnecki, Siddhant M Jayakumar, Mehrdad Farajtabar, Razvan Pascanu,\nand Balaji Lakshminarayanan.\nAdapting auxiliary losses using gradient similarity.\narXiv preprint\narXiv:1812.02224, 2018.\n[16] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The\npascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303–338,\n2010.\n[17] Wonjoon Goo, Juyong Kim, Gunhee Kim, and Sung Ju Hwang. Taxonomy-regularized semantic deep\nconvolutional neural networks. In European Conference on Computer Vision, pages 86–101. Springer,\n2016.\n[18] Google. vit pretrained weights. https://console.cloud.google.com/storage/browser/vit_models,\n2021.\n[19] Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A survey.\nInternational Journal of Computer Vision, pages 1–31, 2021.\n[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.\n10\n\n\n[21] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang,\nYukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pages 1314–1324, 2019.\n[22] Jeremy Howard and Sylvain Gugger. Fastai: A layered api for deep learning. Information, 11(2):108,\n2020.\n[23] Minyoung Huh, Pulkit Agrawal, and Alexei A Efros. What makes imagenet good for transfer learning?\narXiv preprint arXiv:1608.08614, 2016.\n[24] Justin M Johnson and Taghi M Khoshgoftaar. Survey on deep learning with class imbalance. Journal of\nBig Data, 6(1):1–54, 2019.\n[25] Parneet Kaur, Karan Sikka, Weijun Wang, Serge Belongie, and Ajay Divakaran. Foodx-251: a dataset for\nﬁne-grained food classiﬁcation. arXiv preprint arXiv:1907.06167, 2019.\n[26] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan,\nFabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv\npreprint arXiv:1705.06950, 2017.\n[27] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and\nNeil Houlsby. Big transfer (bit): General visual representation learning. arXiv preprint arXiv:1912.11370,\n6(2):8, 2019.\n[28] Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better?\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2661–\n2671, 2019.\n[29] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. arXiv\npreprint, 2009.\n[30] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional\nneural networks. Advances in neural information processing systems, 25:1097–1105, 2012.\n[31] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Ka-\nmali, Stefan Popov, Matteo Malloci, Tom Duerig, et al. The open images dataset v4: Uniﬁed image clas-\nsiﬁcation, object detection, and visual relationship detection at scale. arXiv preprint arXiv:1811.00982,\n2018.\n[32] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab\nKamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4. Inter-\nnational Journal of Computer Vision, pages 1–26, 2020.\n[33] Jungkyu Lee, Taeryun Won, Tae Kwan Lee, Hyemin Lee, Geonmo Gu, and Kiho Hong. Compounding\nthe performance improvements of assembled techniques in a convolutional neural network. arXiv preprint\narXiv:2001.06268, 2020.\n[34] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. Microsoft coco: Common objects in context.\nIn European conference on\ncomputer vision, pages 740–755. Springer, 2014.\n[35] Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the difﬁculty of\ntraining transformers. arXiv preprint arXiv:2004.08249, 2020.\n[36] Wei Liu, Sihan Chen, Longteng Guo, Xinxin Zhu, and Jing Liu. Cptr: Full transformer network for image\ncaptioning. arXiv preprint arXiv:2101.10804, 2021.\n[37] Ilya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\n[38] George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41,\n1995.\n[39] Marius Mosbach, Maksym Andriushchenko, and Dietrich Klakow. On the stability of ﬁne-tuning bert:\nMisconceptions, explanations, and strong baselines. arXiv preprint arXiv:2006.04884, 2020.\n[40] Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Daniel Keysers, Neil Houlsby, et al. Deep ensembles\nfor low-data transfer learning. arXiv preprint arXiv:2010.06866, 2020.\n[41] Daniel Neimark, Omri Bar, Maya Zohar, and Dotan Asselmann.\nVideo transformer network.\narXiv\npreprint arXiv:2102.00719, 2021.\n[42] Jiquan Ngiam, Daiyi Peng, Vijay Vasudevan, Simon Kornblith, Quoc V Le, and Ruoming Pang. Domain\nadaptive transfer learning with specialist models. arXiv preprint arXiv:1811.07056, 2018.\n[43] Joan Puigcerver, Carlos Riquelme, Basil Mustafa, Cedric Renggli, André Susano Pinto, Sylvain Gelly,\nDaniel Keysers, and Neil Houlsby.\nScalable transfer learning with expert models.\narXiv preprint\narXiv:2009.13239, 2020.\n[44] Tal Ridnik, Hussam Lawen, Asaf Noy, Emanuel Ben Baruch, Gilad Sharir, and Itamar Friedman. Tresnet:\nHigh performance gpu-dedicated architecture. In Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision, pages 1400–1409, 2021.\n[45] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large\nScale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252,\n2015.\n11\n\n\n[46] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.\nMo-\nbilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 4510–4520, 2018.\n[47] Gilad Sharir, Asaf Noy, and Lihi Zelnik-Manor. An image is worth 16x16 words, what is a video worth?,\n2021.\n[48] Leslie N Smith. A disciplined approach to neural network hyper-parameters: Part 1–learning rate, batch\nsize, momentum, and weight decay. arXiv preprint arXiv:1803.09820, 2018.\n[49] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effective-\nness of data in deep learning era. In Proceedings of the IEEE international conference on computer vision,\npages 843–852, 2017.\n[50] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking\nthe inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n[51] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks.\nIn International Conference on Machine Learning, pages 6105–6114. PMLR, 2019.\n[52] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency\ntargets improve semi-supervised deep learning results. arXiv preprint arXiv:1703.01780, 2017.\n[53] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner,\nJessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. Mlp-mixer: An all-mlp architecture\nfor vision. arXiv preprint arXiv:2105.01601, 2021.\n[54] Michael J Trammell, Priyanka Oberoi, James Egenrieder, and John Kaufhold. Contextual label smoothing\nwith a phylogenetic tree on the inaturalist 2018 challenge dataset. Washington Academy of Sciences.\nJournal of the Washington Academy of Sciences, 105(1):23–45, 2019.\n[55] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro\nPerona, and Serge Belongie. The inaturalist species classiﬁcation and detection dataset. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pages 8769–8778, 2018.\n[56] Sudheendra Vijayanarasimhan, Jonathon Shlens, Rajat Monga, and Jay Yagnik. Deep networks with large\noutput spaces. arXiv preprint arXiv:1412.7479, 2014.\n[57] Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models,\n2019.\n[58] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves\nimagenet classiﬁcation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10687–10698, 2020.\n[59] Xueting Yan, Ishan Misra, Abhinav Gupta, Deepti Ghadiyaram, and Dhruv Mahajan. Clusterﬁt: Improv-\ning generalization of visual representations. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 6509–6518, 2020.\n[60] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient\nsurgery for multi-task learning. arXiv preprint arXiv:2001.06782, 2020.\n[61] Sangdoo Yun, Seong Joon Oh, Byeongho Heo, Dongyoon Han, Junsuk Choe, and Sanghyuk Chun.\nRe-labeling imagenet: from single to multi-labels, from global to localized labels.\narXiv preprint\narXiv:2101.05022, 2021.\n[62] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk\nminimization. arXiv preprint arXiv:1710.09412, 2017.\n[63] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jian-\nfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-\nsequence perspective with transformers. arXiv preprint arXiv:2012.15840, 2020.\n12\n\n\nAppendices\nA\nNumber of Classes in Different Hierarchies\n0\n2\n4\n6\n8\n10\nHierarchy Level\n0\n500\n1000\n1500\n2000\n2500\nNumber Of Classes\nFigure 4: Number of classes in different hierarchies.\nB\nTraining Details\nB.1\nSingle-label ImageNet-21K-P Training Details\nTo better handle the ground-truth inconsistencies of ImageNet-21K-P, we increase the label-smooth\nfactor from the common value of 0.1 to 0.2. As explained in section 2.1, we also use squish-resizing\ninstead of crop-resizing. We trained the models with input resolution 224, using an Adam optimizer\nwith learning rate of 3e-4 and one-cycle policy [48]. When initializing our models from standard\nImageNet-1K pretraining (pretraining weights taken from [57]), we found that 80 epochs are enough\nfor achieving strong pretrain results on ImageNet-21K-P. For regularization, we used RandAugment\n[10], Cutout [12], Label-smoothing [50] and True-weight-decay [37]. We observed that the common\nImageNet statistics normalization [33, 51] does not improve the training accuracy, and instead nor-\nmalized all the RGB channels to be between 0 and 1. Unless stated otherwise, all runs and tests\nwere done on TResNet-M architecture. On an 8xV100 NVIDIA GPU machine, training with mixed-\nprecision takes 40 minutes per epoch on ResNet50 and TResNet-M architectures (∼5000 img\nsec ).\nB.2\nMulti-label ImageNet-21K-P Training Details\nFor multi-label training, we convert each image single label input to semantic multi labels, as\ndescribed in section 2.2. Multi-label training details are similar to single-label training (number\nof epochs, optimizer, augmentations, learning rate, models initialization and so on), and training\ntimes are also similar. The main difference between single-label and multi-label training relies in\nthe loss function: for multi-label training we tested 3 loss functions, following [3]: cross-entropy\n(γ−= γ+ = 0), focal loss (γ−= γ+ = 2) and ASL (γ−= 4, γ+ = 0). For ASL, we tried different\nvalues of γ−to obtain the best mAP scores.\nC\nUpstream Results\nAs we have a standardized dataset with a ﬁxed train-validation split, the training metrics for each\npretraining method can be used for future benchmark and comparisons.\nC.1\nSinge-label Upstream Results\nFor single-label training, regular top-1 accuracy metric becomes somewhat irrelevant - if pictures\nwith similar content have different ground-truth labels, the network has no clear \"correct\" answer.\nTop-5 accuracy metric is more representative, but still limited. Upstream results of single-label\ntraining are given in Table 5. We can see that the top-1 accuracies obtained on ImageNet-21K-P,\n13\n\n\n37% −46%, are signiﬁcantly lower than the ones obtained on ImageNet-1K, 75% −85%. This\naccuracy drop is mainly due to the semantic structure and inconsistent tagging methodology of\nImageNet-21K-P. However, as we take bigger and better architectures, we see from Table 5 that the\naccuracies continue to improve, so we are not completely hindered by the inconsistent tagging.\nModel Name\nTop-1 Acc. [%]\nTop-5 Acc. [%]\nMobileNetV3\n37.8\n66.3\nResNet50\n42.2\n72.0\nTResNet-M\n45.3\n75.2\nTResNet-L\n45.5\n75.6\nTable 5: Accuracy of different models in single-label training.\nC.2\nMulti-label Upstream Results\nFor multi-label training, we will use the common micro and macro mAP accuracy [3] as training\nmetrics. However, due to the missing labels in the validation (and train) set, this metric also is not\nfully accurate. In Table 7 we compare the results for three possible loss functions for multi-label\nclassiﬁcation - cross-entropy, focal loss and ASL. We see that ASL loss [3], that was designed to\nLoss Type\nMicro-mAP [%]\nMacro-mAP [%]\nCross-Entropy\n47.3\n73.9\nFocal loss\n47.4\n74.1\nASL\n48.5\n74.7\nTable 6: Comparing different loss functions for multi-label classiﬁcation on ImageNet-21K-P.\ncope with large positive-negative imbalancing, outperform cross-entropy and focal loss. This is in\nagreement with our analysis in section 3.2, where we identify extreme imbalancing as one of the\noptimization challenges that stems from multi-label training.\nC.3\nSemantic Softmax Upstream Results\nWith semantic softmax training, we can calculate for each hierarchy its top-1 accuracy metric. We\ncan also calculate the total accuracy by weighting the different accuracies by the number of classes\nin each hierarchy (see Figure 4). Notice that we are not using classes above the maximal hierarchy\nfor our metrics calculation. Hence, and unlike single-label and multi-label training, with semantic\nsoftmax our training metrics are fully accurate.\nIn Figure 5 we present the top-1 accuracies achieved by different models on different hierarchy\nlevels, when trained with semantic softmax (with KD).\nFigure 5: Top-1 accuracies on different hierarchies.\n14\n\n\nD\nDownstream Datasets Training Details\nFor single-label classiﬁcation, our downstream datasets were ImageNet-1K [30], iNaturalist 2019\n[55], CIFAR-100 [29] and Food-251 [25]. For multi-label classiﬁcation, our downstream datasets\nwere MS-COCO [34] and Pascal-VOC [16]. For video action recognition, our downstream dataset\nwas Kinetics-200 [26].\nGeneral details:\n• To minimize statistical uncertainty, for datasets with less than 150, 000 images (CIFAR-100,\nFood-251, MS-COCO, Pascal-VOC), we report result of averaging 3 runs with different seeds.\n• All results are reported for input resolution 224.\n• For all downstream datasets we used cutout of 0.5, rand-Augment and true-weight-decay of 1e-4.\n• All single-label datasets are trained with label-smooth of 0.1\n• Unless stated otherwise, dataset was trained for 40 epochs with Adam optimizer, learning rate of\n3e-4, one-cycle policy and and squish-resizing.\nSpeciﬁc dataset details:\n• ImageNet-1K - Since the dataset is bigger than the others, we ﬁnetuned our networks for 100\nepochs using SGD optimizer, and learning rate of 4e-4. We used crop-resizing with the common\nminimal crop factor of 0.08.\n• MS-COCO - We used ASL loss with γ−= 4.\n• Pascal-VOC - We used ASL loss with γ−= 4, and learning rate of 5e-5.\n• Kinetics-200 - we trained for 30 epochs with learning rate of 8e-5. We used the training method\ndescribed in [47], with simple averaging of the embedding from each sample along the video.\nE\nDownstream Results for Different Multi-label Losses\nIn Table 7 we compare downstream results when using multi-label pretraining with vanilla cross-\nentropy (CE) loss and ASL loss. We see that on all downstream datasets, pretraining with ASL\nleads to signiﬁcantly better results\nDataset\nMulti\nLabel\nPretrain\n(CE)\nMulti\nLabel\nPretrain\n(ASL)\nImageNet1K(1)\n79.6\n81.0\niNaturalist(1)\n69.4\n71.0\nFood 251(1)\n74.3\n75.2\nCIFAR 100(1)\n89.9\n90.6\nMS-COCO(2)\n79.1\n80.6\nPascal-VOC(2)\n87.6\n87.9\nKinetics 200(3)\n81.1\n81.9\nTable 7: Comparing downstream results for different losses of multi-label pretraining. Dataset types and\nmetrics: (1) - single-label, top-1 Acc.[%] ; (2) - multi-label, mAP [%]; (3) - action recognition, top-1 Acc. [%].\nF\nCalculating Teacher Conﬁdence\nUsing the teacher prediction for hierarchy i and the semantic ground-truth, we want to evaluate the\nteacher conﬁdence level, Pi, so we can weight properly the contribution of different hierarchies in\nthe KD loss. Our proposed logic for calculating the teacher’s (semantic) conﬁdence is simple:\n- If the ground-truth highest hierarchy is higher than i, set Pi to 1.\n15\n\n\n- Else, calculate the sum probabilities of the top 5% classes in the teacher prediction (we deliberately\ndon’t take only the probability of the highest class, to account for class similarities).\nIn Figure 6 we present the teacher conﬁdence level for different hierarchies, averaged over an epoch.\nWe can see that lower hierarchies have, in average, higher conﬁdence levels. This stems from the\nFigure 6: Teacher average conﬁdence levels for different hierarchies.\nfact that not all hierarchies are relevant for each image. For the picture in Figure 3, for example,\nonly hierarchies 0-5 are relevant, so we expect the teacher will have low conﬁdence for hierarchies\nhigher than 5.\nG\nSemantic KD Vs Regular KD\nDataset\nSingle\nLabel\n+ KD\nPretrain\nSematic\nSoftmax\n+ Semanic KD\nPretrain\nImageNet1K(1)\n81.5\n82.2\niNaturalist(1)\n72.4\n72.7\nFood 251(1)\n76.0\n76.1\nCIFAR 100(1)\n91.0\n91.7\nMS-COCO(2)\n81.6\n82.2\nPascal-VOC(2)\n89.0\n89.8\nKinetics 200(3)\n83.6\n84.4\nTable 9: Comparing KD with different schemes. Dataset types and metrics: (1) - single-label, top-1 Acc.[%]\n; (2) - multi-label, mAP [%]; (3) - action recognition, top-1 Acc. [%].\nH\nImageNet-1K Transfer Learning Results\nModel Name\nImageNet-1K + KD\nTop-1 Acc. [%]\nMobileNetV3\n78.0\nOFA-595\n81.0\nResNet50\n82.0\nMixer-B-16\n82.2\nTResNet-M\n83.1\nTResNet-L\n83.9\nViT-B-16\n84.4\nTable 10: Transfer learning results On ImageNet-1K, when using ImageNet-21K-P pretraining.\n16\n\n\nI\nImageNet-21K-P - Winter21 Split\nFor a fair comparison to previous works, the results in the article are based on the original ImageNet-\n21K images, i.e. we are using Fall11 release of ImageNet-21K (fall11-whole.tar ﬁle), which con-\ntains all the original images and classes of ImageNet-21K. After we processed this release to create\nImageNet-21K-P, we are left with a dataset that contains 11221 classes, where the train set has\n11797632 samples and the test set has 561052 samples. We shall name this variant Fall11 ImageNet-\n21K-P.\nRecently, the ofﬁcial ImageNet site1 used our pre-processing methodology to offer direct download-\ning of ImageNet-21K-P, based on a new release of ImageNet-21K - Winter21 (winter21-whole.tar\nﬁle). Compared to the original dataset, the Winter21 release removed some classes and samples.\nThe Winter21 variant of ImageNet-21K-P is a dataset that contains 10450 classes, where the train\nset has 11060223 samples and the test set has 522500 samples. We shall name this variant Winter21\nImageNet-21K-P.\nFor enabling future comparison and benchmarking, we report the upstream accuracies also on this\nnew variant of ImageNet-21K-P:\nSingle-Label\nTraining Acc. [%]\nMulti-Label Training\nMacro-mAP [%]\nSemantic Softmax\nTraining Acc. [%]\nFall11\nImageNet-21K-P\n45.3\n74.7\n75.6\nWinter21\nImageNet-21K-P\n47.3\n78.7\n77.7\nTable 11: Upstream results, with different pretraining methods, for different variants of ImageNet-21K-P.\nTested model - TResNet-M.\nNote that the Winter21 variant of ImageNet-21K-P contains 10% fewer classes and 6% fewer images.\nIn Table 12 we compare downstream results when using Winter21 and Fall11 variants of ImageNet-\n21K-P\nDataset\nFall11 ImageNet-21K-P\nWinter21 ImageNet-21K-P\nImageNet1K(1)\n81.4\n81.2\niNaturalist(1)\n72.0\n71.8\nFood 251(1)\n75.8\n75.5\nCIFAR 100(1)\n90.4\n90.5\nMS-COCO(2)\n81.3\n81.1\nPascal-VOC(2)\n89.7\n90.1\nKinetics 200(3)\n83.0\n82.8\nTable 12: Comparing downstream results when using different variant of ImageNet-21K-P. All results are\nfor MTResNet model, with semantic softmax pretraining. Dataset types and metrics: (1) - single-label, top-1\nAcc.[%] ; (2) - multi-label, mAP [%]; (3) - action recognition, top-1 Acc. [%].\nWe can see that compared to Fall11 variant, using Winter21 variant leads to a minor reduction in\nperformances on downstream tasks.\nJ\nAdditional Ablation Tests\nIn this section we will bring additional ablation tests and comparisons.\n1www.image-net.org\n17\n\n\nJ.1\nComparison to Pretraining on Open Images Dataset\nOpen Images (v6) [31] is a large scale multi-label dataset, which consists of 9 million training images\nand 9600 labels. In Table 13 we compare downstream results when using two different datasets for\npretraining: ImageNet-21K (semantic softmax training) and Open Images (multi-label training).\nDataset\nImageNet-21K\nPretrain\nOpen Images\nPretrain\nImageNet1K(1)\n81.4\n81.0\niNaturalist(1)\n72.0\n70.7\nFood 251(1)\n75.8\n74.8\nCIFAR 100(1)\n90.4\n89.4\nMS-COCO(2)\n81.3\n80.5\nPascal-VOC(2)\n89.7\n89.6\nKinetics 200(3)\n83.0\n81.6\nTable 13: Comparing ImageNet-21K pretraining to Open Images pretraining. Downstream dataset types\nand metrics: (1) - single-label, top-1 Acc.[%] ; (2) - multi-label, mAP [%]; (3) - action recognition, top-1 Acc.\n[%].\nAs we can see, ImageNet-21K pretraining consistently provides better downstream results than Open\nImages. A possible reason is that Open Images, as a multi-label dataset with large number of classes,\nsuffers from the same multi-label optimization pitfalls we described in section 3.2.\nJ.2\nComparison on Additional Non-Classiﬁcation Computer-Vision Tasks\nIn Table 14 and Table 15 we compare 1K and 21K pretraining on two additional computer-vision\ntasks: object detection (MS-COCO dataset) and image retrieval (INRIA holidays dataset).\n1K Pretraining\n21K Pretraining\nmAP [%]\n42.9\n44.3\nTable 14: Comparing downstream results on MS-COCO object detection dataset.\n1K Pretraining\n21K Pretraining\nmAP [%]\n81.1\n82.1\nTable 15: Comparing downstream results on on INRIA Holidays image retrieval dataset.\nWe can see that also on non-classiﬁcation tasks such as object detection and image retrieval, pre-\ntraining on ImageNet-21K translates to better downstream results than ImageNet-1K pretraining.\nJ.3\nImpact of Different Number of Training Samples\nIn Figure 7 we test the impact of the number of training samples on on the upstream accuracies. As\nwe can see, there is no saturation - more training images lead to better semantic accuracies.\nK\nPseudo-code\nIn the following sections we will bring pseudo-code (PyTorch-style) to some components in\nour semantic softmax training scheme: logits sampling, KD calculation and estimating teacher\nconﬁdence.\n18\n\n\n2\n4\n6\n8\n10\n12\nNumber Of Training Images [Millions]\n73.0\n73.5\n74.0\n74.5\n75.0\nSemantic Accuracy[%]\nFigure 7: Upstream results for different number of training images.\nK.1\nLogits Sampling\ndef split_logits_to_semantic_logits(logits, hierarchy_indices_list):\nsemantic_logit_list = []\nfor i, ind in enumerate (hierarchy_indices_list):\nlogits_i = logits[:, ind]\nsemantic_logit_list.append(logits_i)\nreturn semantic_logit_list\nK.2\nKD Logic\ndef calculate_KD_loss(input_student , input_teacher , hierarchy_indices_list):\nsemantic_input_student = split_logits_to_semantic_logits(\ninput_student , hierarchy_indices_list)\nsemantic_input_teacher = split_logits_to_semantic_logits(\ninput_teacher , hierarchy_indices_list)\nnumber_of_hierarchies = len(semantic_input_student)\nlosses_list = []\n# scanning hirarchy_level_list\nfor i in range(number_of_hierarchies):\n# converting to semantic logits\ninputs_student_i = semantic_input_student[i]\ninputs_teacher_i = semantic_input_teacher[i]\n# generating probs\npreds_student_i = stable_softmax(inputs_student_i)\npreds_teacher_i = stable_softmax(inputs_teacher_i)\n# weight MSE−KD distances according to teacher confidence\nloss_non_reduced = torch.nn.MSELoss(reduction =’none’)(preds_student_i ,\npreds_teacher_i)\nweights_batch = estimate_teacher_confidence(preds_teacher_i)\nloss_weighted = loss_non_reduced * weights_batch.unsqueeze (1)\nlosses_list.append(torch.sum(loss_weighted))\nreturn sum(losses_list)\nK.3\nTeacher Conﬁdence\n19\n\n\ndef estimate_teacher_confidence(preds_teacher)\nwith torch.no_grad():\nnum_elements = preds_teacher.shape[1]\nnum_elements_topk = int(np.ceil(num_elements / 20))\n# top 5%\nweights_batch = torch.sum(torch.topk(preds_teacher ,\nnum_elements_topk).values, dim=1)\nreturn weights_batch\nL\nLimitations\nIn this section we will discuss some of the limitations of our proposed pipeline for pretraining on\nImageNet-21K:\n1) While our work did put a large emphasis on the efﬁciency of the proposed pretraining pipeline,\nfor reasonable training times we still need an 8-GPUs machine (1 GPU training will be quite long,\n2-3 weeks).\n2) For creating an efﬁcient pretraining scheme, and also to stay within our inner computing budget,\nwe did not incorporate training tricks that signiﬁcantly increase training times, although some of\nthese tricks might give additional beneﬁts and improve pretraining quality.\nAn example - techniques for dealing with extreme multi-tasking, such as GradNorm [7] and PCGrad\n[60], that would probably improve the pretrain quality of multi-label training, but would signiﬁcantly\nincrease training times.\nAnother example of methods from the literature we have not tested - general \"semantic\" techniques\nthat can be used for training neural networks ([4, 54] for example). We found that most of these\ntechniques are not feasible for large-scale efﬁcient training. In addition, we believe that since our\nnovel method, semantic softmax, is designed and tailored to the speciﬁc needs and characterizations\nof ImageNet-21K, it will signiﬁcantly outperform general semantic methods.\n3) When using private datasets which are larger than ImageNet-21K, such as JFT-300M [49], the\npretrain quality that can be achieved is probably still higher than the one we offer.\n20\n"
}