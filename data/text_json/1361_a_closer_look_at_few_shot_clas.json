{
  "filename": "1361_a_closer_look_at_few_shot_clas.pdf",
  "num_pages": 16,
  "pages": [
    "Published as a conference paper at ICLR 2019\nA CLOSER LOOK AT FEW-SHOT CLASSIFICATION\nWei-Yu Chen\nCarnegie Mellon University\nweiyuc@andrew.cmu.edu\nYen-Cheng Liu & Zsolt Kira\nGeorgia Tech\n{ycliu,zkira}@gatech.edu\nYu-Chiang Frank Wang\nNational Taiwan University\nycwang@ntu.edu.tw\nJia-Bin Huang\nVirginia Tech\njbhuang@vt.edu\nABSTRACT\nFew-shot classiﬁcation aims to learn a classiﬁer to recognize unseen classes during\ntraining with limited labeled examples. While signiﬁcant progress has been made,\nthe growing complexity of network designs, meta-learning algorithms, and differ-\nences in implementation details make a fair comparison difﬁcult. In this paper,\nwe present 1) a consistent comparative analysis of several representative few-shot\nclassiﬁcation algorithms, with results showing that deeper backbones signiﬁcantly\nreduce the performance differences among methods on datasets with limited do-\nmain differences, 2) a modiﬁed baseline method that surprisingly achieves com-\npetitive performance when compared with the state-of-the-art on both the mini-\nImageNet and the CUB datasets, and 3) a new experimental setting for evaluating\nthe cross-domain generalization ability for few-shot classiﬁcation algorithms. Our\nresults reveal that reducing intra-class variation is an important factor when the\nfeature backbone is shallow, but not as critical when using deeper backbones. In\na realistic cross-domain evaluation setting, we show that a baseline method with\na standard ﬁne-tuning practice compares favorably against other state-of-the-art\nfew-shot learning algorithms.\n1\nINTRODUCTION\nDeep learning models have achieved state-of-the-art performance on visual recognition tasks such\nas image classiﬁcation. The strong performance, however, heavily relies on training a network with\nabundant labeled instances with diverse visual variations (e.g., thousands of examples for each new\nclass even with pre-training on large-scale dataset with base classes). The human annotation cost as\nwell as the scarcity of data in some classes (e.g., rare species) signiﬁcantly limit the applicability of\ncurrent vision systems to learn new visual concepts efﬁciently. In contrast, the human visual systems\ncan recognize new classes with extremely few labeled examples. It is thus of great interest to learn\nto generalize to new classes with a limited amount of labeled examples for each novel class.\nThe problem of learning to generalize to unseen classes during training, known as few-shot clas-\nsiﬁcation, has attracted considerable attention Vinyals et al. (2016); Snell et al. (2017); Finn et al.\n(2017); Ravi & Larochelle (2017); Sung et al. (2018); Garcia & Bruna (2018); Qi et al. (2018).\nOne promising direction to few-shot classiﬁcation is the meta-learning paradigm where transfer-\nable knowledge is extracted and propagated from a collection of tasks to prevent overﬁtting and\nimprove generalization. Examples include model initialization based methods Ravi & Larochelle\n(2017); Finn et al. (2017), metric learning methods Vinyals et al. (2016); Snell et al. (2017); Sung\net al. (2018), and hallucination based methods Antoniou et al. (2018); Hariharan & Girshick (2017);\nWang et al. (2018). Another line of work Gidaris & Komodakis (2018); Qi et al. (2018) also demon-\nstrates promising results by directly predicting the weights of the classiﬁers for novel classes.\nLimitations.\nWhile many few-shot classiﬁcation algorithms have reported improved performance\nover the state-of-the-art, there are two main challenges that prevent us from making a fair compari-\nson and measuring the actual progress. First, the discrepancy of the implementation details among\nmultiple few-shot learning algorithms obscures the relative performance gain. The performance of\n1\n",
    "Published as a conference paper at ICLR 2019\nbaseline approaches can also be signiﬁcantly under-estimated (e.g., training without data augmenta-\ntion). Second, while the current evaluation focuses on recognizing novel class with limited training\nexamples, these novel classes are sampled from the same dataset. The lack of domain shift between\nthe base and novel classes makes the evaluation scenarios unrealistic.\nOur work.\nIn this paper, we present a detailed empirical study to shed new light on the few-shot\nclassiﬁcation problem. First, we conduct consistent comparative experiments to compare several\nrepresentative few-shot classiﬁcation methods on common ground. Our results show that using a\ndeep backbone shrinks the performance gap between different methods in the setting of limited do-\nmain differences between base and novel classes. Second, by replacing the linear classiﬁer with\na distance-based classiﬁer as used in Gidaris & Komodakis (2018); Qi et al. (2018), the baseline\nmethod is surprisingly competitive to current state-of-art meta-learning algorithms. Third, we intro-\nduce a practical evaluation setting where there exists domain shift between base and novel classes\n(e.g., sampling base classes from generic object categories and novel classes from ﬁne-grained cat-\negories). Our results show that sophisticated few-shot learning algorithms do not provide perfor-\nmance improvement over the baseline under this setting. Through making the source code and\nmodel implementations with a consistent evaluation setting publicly available, we hope to foster\nfuture progress in the ﬁeld.1\nOur contributions.\n1. We provide a uniﬁed testbed for several different few-shot classiﬁcation algorithms for a fair\ncomparison. Our empirical evaluation results reveal that the use of a shallow backbone commonly\nused in existing work leads to favorable results for methods that explicitly reduce intra-class\nvariation. Increasing the model capacity of the feature backbone reduces the performance gap\nbetween different methods when domain differences are limited.\n2. We show that a baseline method with a distance-based classiﬁer surprisingly achieves competitive\nperformance with the state-of-the-art meta-learning methods on both mini-ImageNet and CUB\ndatasets.\n3. We investigate a practical evaluation setting where base and novel classes are sampled from dif-\nferent domains. We show that current few-shot classiﬁcation algorithms fail to address such do-\nmain shifts and are inferior even to the baseline method, highlighting the importance of learning\nto adapt to domain differences in few-shot learning.\n2\nRELATED WORK\nGiven abundant training examples for the base classes, few-shot learning algorithms aim to learn\nto recognizing novel classes with a limited amount of labeled examples. Much efforts have been\ndevoted to overcome the data efﬁciency issue. In the following, we discuss representative few-shot\nlearning algorithms organized into three main categories: initialization based, metric learning based,\nand hallucination based methods.\nInitialization based methods\ntackle the few-shot learning problem by “learning to ﬁne-tune”.\nOne approach aims to learn good model initialization (i.e., the parameters of a network) so that the\nclassiﬁers for novel classes can be learned with a limited number of labeled examples and a small\nnumber of gradient update steps Finn et al. (2017; 2018); Nichol & Schulman (2018); Rusu et al.\n(2019). Another line of work focuses on learning an optimizer. Examples include the LSTM-based\nmeta-learner for replacing the stochastic gradient decent optimizer Ravi & Larochelle (2017) and\nthe weight-update mechanism with an external memory Munkhdalai & Yu (2017). While these ini-\ntialization based methods are capable of achieving rapid adaption with a limited number of training\nexamples for novel classes, our experiments show that these methods have difﬁculty in handling\ndomain shifts between base and novel classes.\nDistance metric learning based methods\naddress the few-shot classiﬁcation problem by “learn-\ning to compare”. The intuition is that if a model can determine the similarity of two images, it can\nclassify an unseen input image with the labeled instances Koch et al. (2015). To learn a sophisticated\ncomparison models, meta-learning based methods make their prediction conditioned on distance or\n1https://github.com/wyharveychen/CloserLookFewShot\n2\n",
    "Published as a conference paper at ICLR 2019\nmetric to few labeled instances during the training process. Examples of distance metrics include\ncosine similarity Vinyals et al. (2016), Euclidean distance to class-mean representation Snell et al.\n(2017), CNN-based relation module Sung et al. (2018), ridge regression Bertinetto et al. (2019), and\ngraph neural network Garcia & Bruna (2018). In this paper, we compare the performance of three\ndistance metric learning methods. Our results show that a simple baseline method with a distance-\nbased classiﬁer (without training over a collection of tasks/episodes as in meta-learning) achieves\ncompetitive performance with respect to other sophisticated algorithms.\nBesides meta-learning methods, both Gidaris & Komodakis (2018) and Qi et al. (2018) develop\na similar method to our Baseline++ (described later in Section 3.2). The method in Gidaris &\nKomodakis (2018) learns a weight generator to predict the novel class classiﬁer using an attention-\nbased mechanism (cosine similarity), and the Qi et al. (2018) directly use novel class features as\ntheir weights. Our Baseline++ can be viewed as a simpliﬁed architecture of these methods. Our\nfocus, however, is to show that simply reducing intra-class variation in a baseline method using the\nbase class data leads to competitive performance.\nHallucination based methods\ndirectly deal with data deﬁciency by “learning to augment”. This\nclass of methods learns a generator from data in the base classes and use the learned generator to\nhallucinate new novel class data for data augmentation. One type of generator aims at transferring\nappearance variations exhibited in the base classes. These generators either transfer variance in\nbase class data to novel classes Hariharan & Girshick (2017), or use GAN models Antoniou et al.\n(2018) to transfer the style. Another type of generators does not explicitly specify what to transfer,\nbut directly integrate the generator into a meta-learning algorithm for improving the classiﬁcation\naccuracy Wang et al. (2018). Since hallucination based methods often work with other few-shot\nmethods together (e.g. use hallucination based and metric learning based methods together) and\nlead to complicated comparison, we do not include these methods in our comparative study and\nleave it for future work.\nDomain adaptation\ntechniques aim to reduce the domain shifts between source and target domain\nPan et al. (2010); Ganin & Lempitsky (2015), as well as novel tasks in a different domain Hsu et al.\n(2018). Similar to domain adaptation, we also investigate the impact of domain difference on few-\nshot classiﬁcation algorithms in Section 4.5. In contrast to most domain adaptation problems where\na large amount of data is available in the target domain (either labeled or unlabeled), our problem\nsetting differs because we only have very few examples in the new domain. Very recently, the\nmethod in Dong & Xing (2018) addresses the one-shot novel category domain adaptation problem,\nwhere in the testing stage both the domain and the category to classify are changed. Similarly, our\nwork highlights the limitations of existing few-shot classiﬁcation algorithms problem in handling\ndomain shift. To put these problem settings in context, we provided a detailed comparison of setting\ndifference in the appendix A1.\n3\nOVERVIEW OF FEW-SHOT CLASSIFICATION ALGORITHMS\nIn this section, we ﬁrst outline the details of the baseline model (Section 3.1) and its variant (Sec-\ntion 3.2), followed by describing representative meta-learning algorithms (Section 3.3) studied in\nour experiments. Given abundant base class labeled data Xb and a small amount of novel class la-\nbeled data Xn, the goal of few-shot classiﬁcation algorithms is to train classiﬁers for novel classes\n(unseen during training) with few labeled examples.\n3.1\nBASELINE\nOur baseline model follows the standard transfer learning procedure of network pre-training and\nﬁne-tuning. Figure 1 illustrates the overall procedure.\nTraining stage.\nWe train a feature extractor fθ (parametrized by the network parameters θ) and\nthe classiﬁer C(·|Wb) (parametrized by the weight matrix Wb ∈Rd×c) from scratch by minimizing\na standard cross-entropy classiﬁcation loss Lpred using the training examples in the base classes\nxi ∈Xb. Here, we denote the dimension of the encoded feature as d and the number of output\nclasses as c. The classiﬁer C(.|Wb) consists of a linear layer W⊤\nb fθ(xi) followed by a softmax\nfunction σ.\n3\n",
    "Published as a conference paper at ICLR 2019\nBaseline++\nBaseline\nTraining stage\nClassifier\nFeature \nextractor\nNovel class data\n(Few) \nFine-tuning stage\nFixed\nFeature \nextractor\nBase class data\n(Many)\nLinear\nlayer  \nSoftmax\n𝜎\nSoftmax\n𝜎\nCosine \ndistance\nClassifier\nClassifier\n…\n…\nFigure 1: Baseline and Baseline++ few-shot classiﬁcation methods. Both the baseline and\nbaseline++ method train a feature extractor fθ and classiﬁer C(.|Wb) with base class data in the\ntraining stage In the ﬁne-tuning stage, we ﬁx the network parameters θ in the feature extractor fθ\nand train a new classiﬁer C(.|Wn) with the given labeled examples in novel classes. The\nbaseline++ method differs from the baseline model in the use of cosine distances between the input\nfeature and the weight vector for each class that aims to reduce intra-class variations.\nFine-tuning stage.\nTo adapt the model to recognize novel classes in the ﬁne-tuning stage, we ﬁx\nthe pre-trained network parameter θ in our feature extractor fθ and train a new classiﬁer C(.|Wn)\n(parametrized by the weight matrix Wn) by minimizing Lpred using the few labeled of examples (i.e.,\nthe support set) in the novel classes Xn.\n3.2\nBASELINE++\nIn addition to the baseline model, we also implement a variant of the baseline model, denoted as\nBaseline++, which explicitly reduces intra-class variation among features during training. The im-\nportance of reducing intra-class variations of features has been highlighted in deep metric learn-\ning Hu et al. (2015) and few-shot classiﬁcation methods Gidaris & Komodakis (2018).\nThe training procedure of Baseline++ is the same as the original Baseline model except for the\nclassiﬁer design. As shown in Figure 1, we still have a weight matrix Wb ∈Rd×c of the classiﬁer in\nthe training stage and a Wn in the ﬁne-tuning stage in Baseline++. The classiﬁer design, however, is\ndifferent from the linear classiﬁer used in the Baseline. Take the weight matrix Wb as an example.\nWe can write the weight matrix Wb as [w1,w2,...wc], where each class has a d-dimensional weight\nvector. In the training stage, for an input feature fθ(xi) where xi ∈Xb, we compute its cosine\nsimilarity to each weight vector [w1,··· ,wc] and obtain the similarity scores [si,1,si,2,··· ,si,c] for\nall classes, where si,j = fθ(xi)⊤w j/∥fθ(xi)∥∥w j∥. We can then obtain the prediction probability\nfor each class by normalizing these similarity scores with a softmax function. Here, the classiﬁer\nmakes a prediction based on the cosine distance between the input feature and the learned weight\nvectors representing each class. Consequently, training the model with this distance-based classiﬁer\nexplicitly reduce intra-class variations. Intuitively, the learned weight vectors [w1,··· ,wc] can be\ninterpreted as prototypes (similar to Snell et al. (2017); Vinyals et al. (2016)) for each class and the\nclassiﬁcation is based on the distance of the input feature to these learned prototypes. The softmax\nfunction prevents the learned weight vectors collapsing to zeros.\nWe clarify that the network design in Baseline++ is not our contribution. The concept of distance-\nbased classiﬁcation has been extensively studied in Mensink et al. (2012) and recently has been\nrevisited in the few-shot classiﬁcation setting Gidaris & Komodakis (2018); Qi et al. (2018).\n3.3\nMETA-LEARNING ALGORITHMS\nHere we describe the formulations of meta-learning methods used in our study. We consider three\ndistance metric learning based methods (MatchingNet Vinyals et al. (2016), ProtoNet Snell et al.\n4\n",
    "Published as a conference paper at ICLR 2019\nMeta-training stage\nMeta-testing stage\nSupport set\nconditioned model\nNovel support set    \n(Novel class data     )\nBase query set\nBase support set\n𝑌\"\nSampled 𝑁classes\nSupport set conditioned model\nFeature \nextractor\nMatchingNet\nCosine \ndistance\nRelationNet\nRelation\nModule\n𝜇\nProtoNet\nEuclidean\ndistance\n𝜇\nMAML\nGradient\nLinear\nLinear\nBase class data \n(Many)\nClass \nmean\nClass \nmean\nFigure 2: Meta-learning few-shot classiﬁcation algorithms. The meta-learning classiﬁer M(·|S)\nis conditioned on the support set S. (Top) In the meta-train stage, the support set Sb and the query\nset Qb are ﬁrst sampled from random N classes, and then train the parameters in M(.|Sb) to\nminimize the N-way prediction loss LN−way. In the meta-testing stage, the adapted classiﬁer\nM(.|Sn) can predict novel classes with the support set in the novel classes Sn. (Bottom) The design\nof M(·|S) in different meta-learning algorithms.\n(2017), and RelationNet Sung et al. (2018)) and one initialization based method (MAML Finn et al.\n(2017)). While meta-learning is not a clearly deﬁned, Vinyals et al. (2016) considers a few-shot\nclassiﬁcation method as meta-learning if the prediction is conditioned on a small support set S,\nbecause it makes the training procedure explicitly learn to learn from a given small support set.\nAs shown in Figure 2, meta-learning algorithms consist of a meta-training and a meta-testing stage.\nIn the meta-training stage, the algorithm ﬁrst randomly select N classes, and sample small base\nsupport set Sb and a base query set Qb from data samples within these classes. The objective is\nto train a classiﬁcation model M that minimizes N-way prediction loss LN−way of the samples in\nthe query set Qb. Here, the classiﬁer M is conditioned on provided support set Sb. By making\nprediction conditioned on the given support set, a meta-learning method can learn how to learn from\nlimited labeled data through training from a collection of tasks (episodes). In the meta-testing stage,\nall novel class data Xn are considered as the support set for novel classes Sn, and the classiﬁcation\nmodel M can be adapted to predict novel classes with the new support set Sn.\nDifferent meta-learning methods differ in their strategies to make prediction conditioned on support\nset (see Figure 2). For both MatchingNet Vinyals et al. (2016) and ProtoNet Snell et al. (2017), the\nprediction of the examples in a query set Q is based on comparing the distance between the query\nfeature and the support feature from each class. MatchingNet compares cosine distance between the\nquery feature and each support feature, and computes average cosine distance for each class, while\nProtoNet compares the Euclidean distance between query features and the class mean of support\nfeatures. RelationNet Sung et al. (2018) shares a similar idea, but it replaces distance with a learn-\nable relation module. The MAML method Finn et al. (2017) is an initialization based meta-learning\nalgorithm, where each support set is used to adapt the initial model parameters using few gradient\nupdates. As different support sets have different gradient updates, the adapted model is conditioned\non the support set. Note that when the query set instances are predicted by the adapted model in\nthe meta-training stage, the loss of the query set is used to update the initial model, not the adapted\nmodel.\n4\nEXPERIMENTAL RESULTS\n4.1\nEXPERIMENTAL SETUP\nDatasets and scenarios.\nWe address the few-shot classiﬁcation problem under three scenarios: 1)\ngeneric object recognition, 2) ﬁne-grained image classiﬁcation, and 3) cross-domain adaptation.\n5\n",
    "Published as a conference paper at ICLR 2019\nFor object recognition, we use the mini-ImageNet dataset commonly used in evaluating few-shot\nclassiﬁcation algorithms. The mini-ImageNet dataset consists of a subset of 100 classes from the\nImageNet dataset Deng et al. (2009) and contains 600 images for each class. The dataset was ﬁrst\nproposed by Vinyals et al. (2016), but recent works use the follow-up setting provided by Ravi &\nLarochelle (2017), which is composed of randomly selected 64 base, 16 validation, and 20 novel\nclasses.\nFor ﬁne-grained classiﬁcation, we use CUB-200-2011 dataset Wah et al. (2011) (referred to as the\nCUB hereafter). The CUB dataset contains 200 classes and 11,788 images in total. Following\nthe evaluation protocol of Hilliard et al. (2018), we randomly split the dataset into 100 base, 50\nvalidation, and 50 novel classes.\nFor the cross-domain scenario (mini-ImageNet →CUB), we use mini-ImageNet as our base class\nand the 50 validation and 50 novel class from CUB. Evaluating the cross-domain scenario allows us\nto understand the effects of domain shifts to existing few-shot classiﬁcation approaches.\nImplementation details.\nIn the training stage for the Baseline and the Baseline++ methods, we\ntrain 400 epochs with a batch size of 16. In the meta-training stage for meta-learning methods, we\ntrain 60,000 episodes for 1-shot and 40,000 episodes for 5-shot tasks. We use the validation set\nto select the training episodes with the best accuracy.2 In each episode, we sample N classes to\nform N-way classiﬁcation (N is 5 in both meta-training and meta-testing stages unless otherwise\nmentioned). For each class, we pick k labeled instances as our support set and 16 instances for the\nquery set for a k-shot task.\nIn the ﬁne-tuning or meta-testing stage for all methods, we average the results over 600 experiments.\nIn each experiment, we randomly sample 5 classes from novel classes, and in each class, we also\npick k instances for the support set and 16 for the query set. For Baseline and Baseline++, we use the\nentire support set to train a new classiﬁer for 100 iterations with a batch size of 4. For meta-learning\nmethods, we obtain the classiﬁcation model conditioned on the support set as in Section 3.3.\nAll methods are trained from scratch and use the Adam optimizer with initial learning rate 10−3.\nWe apply standard data augmentation including random crop, left-right ﬂip, and color jitter in both\nthe training or meta-training stage. Some implementation details have been adjusted individually\nfor each method. For Baseline++, we multiply the cosine similarity by a constant scalar 2 to adjust\noriginal value range [-1,1] to be more appropriate for subsequent softmax layer. For MatchingNet,\nwe use an FCE classiﬁcation layer without ﬁne-tuning in all experiments and also multiply cosine\nsimilarity by a constant scalar. For RelationNet, we replace the L2 norm with a softmax layer\nto expedite training. For MAML, we use a ﬁrst-order approximation in the gradient for memory\nefﬁciency. The approximation has been shown in the original paper and in our appendix to have\nnearly identical performance as the full version. We choose the ﬁrst-order approximation for its\nefﬁciency.\n4.2\nEVALUATION USING THE STANDARD SETTING\nWe now conduct experiments on the most common setting in few-shot classiﬁcation, 1-shot and\n5-shot classiﬁcation, i.e., 1 or 5 labeled instances are available from each novel class. We use a\nfour-layer convolution backbone (Conv-4) with an input size of 84x84 as in Snell et al. (2017) and\nperform 5-way classiﬁcation for only novel classes during the ﬁne-tuning or meta-testing stage.\nTo validate the correctness of our implementation, we ﬁrst compare our results to the reported num-\nbers for the mini-ImageNet dataset in Table 1. Note that we have a ProtoNet#, as we use 5-way\nclassiﬁcation in the meta-training and meta-testing stages for all meta-learning methods as men-\ntioned in Section 4.1; however, the ofﬁcial reported results from ProtoNet uses 30-way for one shot\nand 20-way for ﬁve shot in the meta-training stage in spite of using 5-way in the meta-testing stage.\nWe report this result for completeness.\nFrom Table 1, we can observe that all of our re-implementation for meta-learning methods do not\nfall more than 2% behind reported performance.\nThese minor differences can be attributed to our\n2For example, the exact episodes for experiments on the mini-ImageNet in the 5-shot setting with a four-\nlayer ConvNet are: ProtoNet: 24,600; MatchingNet: 35,300; RelationNet: 37,100; MAML: 36,700.\n3Reported results are from Ravi & Larochelle (2017)\n6\n",
    "Published as a conference paper at ICLR 2019\nTable 1: Validating our re-implementation. We validate our few-shot classiﬁcation\nimplementation on the mini-ImageNet dataset using a Conv-4 backbone. We report the mean of\n600 randomly generated test episodes as well as the 95% conﬁdence intervals. Our reproduced\nresults to all few-shot methods do not fall behind by more than 2% to the reported results in the\nliterature. We attribute the slight discrepancy to different random seeds and minor implementation\ndifferences in each method. “Baseline∗” denotes the results without applying data augmentation\nduring training. ProtoNet# indicates performing 30-way classiﬁcation in 1-shot and 20-way in\n5-shot during the meta-training stage.\n1-shot\n5-shot\nMethod\nReported\nOurs\nReported\nOurs\nBaseline\n-\n42.11 ± 0.71\n-\n62.53 ±0.69\nBaseline∗3\n41.08 ± 0.70\n36.35 ± 0.64\n51.04 ± 0.65\n54.50 ±0.66\nMatchingNet3 Vinyals et al. (2016)\n43.56 ± 0.84\n48.14 ± 0.78\n55.31 ±0.73\n63.48 ±0.66\nProtoNet\n-\n44.42 ± 0.84\n-\n64.24 ±0.72\nProtoNet# Snell et al. (2017)\n49.42 ± 0.78\n47.74 ± 0.84\n68.20 ±0.66\n66.68 ±0.68\nMAML Finn et al. (2017)\n48.07 ± 1.75\n46.47 ± 0.82\n63.15 ±0.91\n62.71 ±0.71\nRelationNet Sung et al. (2018)\n50.44 ± 0.82\n49.31 ± 0.85\n65.32 ±0.70\n66.60 ±0.69\nTable 2: Few-shot classiﬁcation results for both the mini-ImageNet and CUB datasets. The\nBaseline++ consistently improves the Baseline model by a large margin and is competitive with the\nstate-of-the-art meta-learning methods. All experiments are from 5-way classiﬁcation with a\nConv-4 backbone and data augmentation.\nCUB\nmini-ImageNet\nMethod\n1-shot\n5-shot\n1-shot\n5-shot\nBaseline\n47.12 ± 0.74\n64.16 ± 0.71\n42.11 ± 0.71\n62.53 ±0.69\nBaseline++\n60.53 ± 0.83\n79.34 ± 0.61\n48.24 ± 0.75\n66.43 ±0.63\nMatchingNet Vinyals et al. (2016)\n61.16 ± 0.89\n72.86 ± 0.70\n48.14 ± 0.78\n63.48 ±0.66\nProtoNet Snell et al. (2017)\n51.31 ± 0.91\n70.77 ± 0.69\n44.42 ± 0.84\n64.24 ±0.72\nMAML Finn et al. (2017)\n55.92 ± 0.95\n72.09 ± 0.76\n46.47 ± 0.82\n62.71 ±0.71\nRelationNet Sung et al. (2018)\n62.45 ± 0.98\n76.11 ± 0.69\n49.31 ± 0.85\n66.60 ±0.69\nmodiﬁcations of some implementation details to ensure a fair comparison among all methods, such\nas using the same optimizer for all methods.\nMoreover, our implementation of existing work also improves the performance of some of the meth-\nods. For example, our results show that the Baseline approach under 5-shot setting can be improved\nby a large margin since previous implementations of the Baseline do not include data augmentation\nin their training stage, thereby leads to over-ﬁtting. While our Baseline∗is not as good as reported\nin 1-shot, our Baseline with augmentation still improves on it, and could be even higher if our re-\nproduced Baseline∗matches the reported statistics. In either case, the performance of the Baseline\nmethod is severely underestimated. We also improve the results of MatchingNet by adjusting the\ninput score to the softmax layer to a more appropriate range as stated in Section 4.1. On the other\nhand, while ProtoNet# is not as good as ProtoNet, as mentioned in the original paper a more chal-\nlenging setting in the meta-training stage leads to better accuracy. We choose to use a consistent\n5-way classiﬁcation setting in subsequent experiments to have a fair comparison to other methods.\nThis issue can be resolved by using a deeper backbone as shown in Section 4.3.\nAfter validating our re-implementation, we now report the accuracy in Table 2. Besides additionally\nreporting results on the CUB dataset, we also compare Baseline++ to other methods. Here, we\nﬁnd that Baseline++ improves the Baseline by a large margin and becomes competitive even when\ncompared with other meta-learning methods. The results demonstrate that reducing intra-class\nvariation is an important factor in the current few-shot classiﬁcation problem setting.\n7\n",
    "Published as a conference paper at ICLR 2019\n45%\n55%\n65%\n75%\nConv-4\nConv-6\nResNet-10\nResNet-18\nResNet-34\n60% \n70% \n80% \n90% \nConv-4\nConv-6\nResNet-10\nResNet-18\nResNet-34\n40% \n45% \n50% \n55% \nConv-4\nConv-6\nResNet-10\nResNet-18\nResNet-34\n60% \n65% \n70% \n75% \n80% \nConv-4\nConv-6\nResNet-10\nResNet-18\nResNet-34\nBaseline\nBaseline++\nMatchingNet\nProtoNet\nMAML\nRelationNet\nCUB\n1-shot\n5-shot\nmini-ImageNet\n1-shot\n5-shot\nFigure 3: Few-shot classiﬁcation accuracy vs. backbone depth. In the CUB dataset, gaps among\ndifferent methods diminish as the backbone gets deeper. In mini-ImageNet 5-shot, some\nmeta-learning methods are even beaten by Baseline with a deeper backbone. (Please refer to\nFigure A3 and Table A5 for larger ﬁgure and detailed statistics.)\nHowever, note that our current setting only uses a 4-layer backbone, while a deeper backbone can\ninherently reduce intra-class variation. Thus, we conduct experiments to investigate the effects of\nbackbone depth in the next section.\n4.3\nEFFECT OF INCREASING THE NETWORK DEPTH\nIn this section, we change the depth of the feature backbone to reduce intra-class variation for all\nmethods. See appendix for statistics on how network depth correlates with intra-class variation.\nStarting from Conv-4, we gradually increase the feature backbone to Conv-6, ResNet-10, 18 and 34,\nwhere Conv-6 have two additional convolution blocks without pooling after Conv-4. ResNet-18 and\n34 are the same as described in He et al. (2016) with an input size of 224×224, while ResNet-10 is\na simpliﬁed version of ResNet-18 where only one residual building block is used in each layer. The\nstatistics of this experiment would also be helpful to other works to make a fair comparison under\ndifferent feature backbones.\nResults of the CUB dataset shows a clearer tendency in Figure 3. As the backbone gets deeper, the\ngap among different methods drastically reduces. Another observation is how ProtoNet improves\nrapidly as the backbone gets deeper. While using a consistent 5-way classiﬁcation as discussed in\nSection 4.2 degrades the accuracy of ProtoNet with Conv-4, it works well with a deeper backbone.\nThus, the two observations above demonstrate that in the CUB dataset, the gap among existing\nmethods would be reduced if their intra-class variation are all reduced by a deeper backbone.\nHowever, the result of mini-ImageNet in Figure 3 is much more complicated. In the 5-shot setting,\nboth Baseline and Baseline++ achieve good performance with a deeper backbone, but some meta-\nlearning methods become worse relative to them. Thus, other than intra-class variation, we can\nassume that the dataset is also important in few-shot classiﬁcation. One difference between CUB and\nmini-ImageNet is their domain difference in base and novel classes since classes in mini-ImageNet\nhave a larger divergence than CUB in a word-net hierarchy Miller (1995). To better understand the\neffect, below we discuss how domain differences between base and novel classes impact few-shot\nclassiﬁcation results.\n4.4\nEFFECT OF DOMAIN DIFFERENCES BETWEEN BASE AND NOVEL CLASSES\nTo further dig into the issue of domain difference, we design scenarios that provide such domain\nshifts. Besides the ﬁne-grained classiﬁcation and object recognition scenarios, we propose a new\ncross-domain scenario: mini-ImageNet →CUB as mentioned in\nSection 4.1. We believe that\nthis is practical scenario since collecting images from a general class may be relatively easy (e.g.\ndue to increased availability) but collecting images from ﬁne-grained classes might be more difﬁcult.\nWe conduct the experiments with a ResNet-18 feature backbone. As shown in Table 3, the Baseline\noutperforms all meta-learning methods under this scenario. While meta-learning methods learn to\nlearn from the support set during the meta-training stage, they are not able to adapt to novel classes\nthat are too different since all of the base support sets are within the same dataset. A similar concept\nis also mentioned in Vinyals et al. (2016). In contrast, the Baseline simply replaces and trains a\nnew classiﬁer based on the few given novel class data, which allows it to quickly adapt to a novel\n8\n",
    "Published as a conference paper at ICLR 2019\nmini-ImageNet →CUB\nBaseline\n65.57±0.70\nBaseline++\n62.04±0.76\nMatchingNet\n53.07±0.74\nProtoNet\n62.02±0.70\nMAML\n51.34±0.72\nRelationNet\n57.71±0.73\nTable 3: 5-shot accuracy under the\ncross-domain scenario with a ResNet-18\nbackbone. Baseline outperforms all other\nmethods under this scenario.\n40% \n50% \n60% \n70% \n80% \n90% \nCUB\nminiImageNet\nminiImageNet -> CUB \nBaseline\nBaseline++\nMatchingNet\nProtoNet\nMAML\nRelationNet\nDomain Difference\nLarge\nSmall\nFigure 4: 5-shot accuracy in different scenarios\nwith a ResNet-18 backbone. The Baseline\nmodel performs relative well with larger domain\ndifferences.\nCUB\nmini-ImageNet\nCUB→mini-ImageNet\nFigure 5: Meta-learning methods with further adaptation steps. Further adaptation improves\nMatchingNet and MAML, but has less improvement to RelationNet, and could instead harm\nProtoNet under the scenarios with little domain differences.All statistics are for 5-shot accuracy\nwith ResNet-18 backbone. Note that different methods use different further adaptation strategies.\nclass and is less affected by domain shift between the source and target domains. The Baseline\nalso performs better than the Baseline++ method, possibly because additionally reducing intra-class\nvariation compromises adaptability. In Figure 4, we can further observe how Baseline accuracy\nbecomes relatively higher as the domain difference gets larger. That is, as the domain difference\ngrows larger, the adaptation based on a few novel class instances becomes more important.\n4.5\nEFFECT OF FURTHER ADAPTATION\nTo further adapt meta-learning methods as in the Baseline method, an intuitive way is to ﬁx the\nfeatures and train a new softmax classiﬁer. We apply this simple adaptation scheme to MatchingNet\nand ProtoNet. For MAML, it is not feasible to ﬁx the feature as it is an initialization method. In\ncontrast, since it updates the model with the support set for only a few iterations, we can adapt\nfurther by updating for as many iterations as is required to train a new classiﬁcation layer, which\nis 100 updates as mentioned in Section 4.1. For RelationNet, the features are convolution maps\nrather than the feature vectors, so we are not able to replace it with a softmax. As an alternative, we\nrandomly split the few training data in novel class into 3 support and 2 query data to ﬁnetune the\nrelation module for 100 epochs.\nThe results of further adaptation are shown in Figure 5; we can observe that the performance of\nMatchingNet and MAML improves signiﬁcantly after further adaptation, particularly in the mini-\nImageNet →CUB scenario. The results demonstrate that lack of adaptation is the reason they fall\nbehind the Baseline. However, changing the setting in the meta-testing stage can lead to inconsis-\ntency with the meta-training stage. The ProtoNet result shows that performance can degrade in sce-\n9\n",
    "Published as a conference paper at ICLR 2019\nnarios with less domain difference. Thus, we believe that learning how to adapt in the meta-training\nstage is important future direction. In summary, as domain differences are likely to exist in many\nreal-world applications, we consider that learning to learn adaptation in the meta-training stage\nwould be an important direction for future meta-learning research in few-shot classiﬁcation.\n5\nCONCLUSIONS\nIn this paper, we have investigated the limits of the standard evaluation setting for few-shot classi-\nﬁcation. Through comparing methods on a common ground, our results show that the Baseline++\nmodel is competitive to state of art under standard conditions, and the Baseline model achieves\ncompetitive performance with recent state-of-the-art meta-learning algorithms on both CUB and\nmini-ImageNet benchmark datasets when using a deeper feature backbone. Surprisingly, the Base-\nline compares favorably against all the evaluated meta-learning algorithms under a realistic scenario\nwhere there exists domain shift between the base and novel classes. By making our source code\npublicly available, we believe that community can beneﬁt from the consistent comparative experi-\nments and move forward to tackle the challenge of potential domain shifts in the context of few-shot\nlearning.\nREFERENCES\nAntreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarial\nnetworks. In Proceedings of the International Conference on Learning Representations Work-\nshops (ICLR Workshops), 2018. 1, 3\nLuca Bertinetto, Jo˜ao F Henriques, Philip HS Torr, and Andrea Vedaldi. Meta-learning with dif-\nferentiable closed-form solvers. In Proceedings of the International Conference on Learning\nRepresentations (ICLR), 2019. 3\nGregory Cohen, Saeed Afshar, Jonathan Tapson, and Andr´e van Schaik. Emnist: an extension of\nmnist to handwritten letters. arXiv preprint arXiv:1702.05373, 2017. 13\nDavid L Davies and Donald W Bouldin. A cluster separation measure. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 1979. 14\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2009. 6\nNanqing Dong and Eric P Xing. Domain adaption in one-shot learning. In Joint European Con-\nference on Machine Learning and Knowledge Discovery in Databases. Springer, 2018. 3, 12,\n13\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation\nof deep networks. In Proceedings of the International Conference on Machine Learning (ICML),\n2017. 1, 2, 5, 7, 12\nChelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In Ad-\nvances in Neural Information Processing Systems (NIPS), 2018. 2\nYaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In\nProceedings of the International Conference on Machine Learning (ICML), 2015. 3\nVictor Garcia and Joan Bruna. Few-shot learning with graph neural networks. In Proceedings of the\nInternational Conference on Learning Representations (ICLR), 2018. 1, 3\nSpyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n1, 2, 3, 4\nBharath Hariharan and Ross Girshick. Low-shot visual recognition by shrinking and hallucinating\nfeatures. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017.\n1, 3\n10\n",
    "Published as a conference paper at ICLR 2019\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2016. 8\nNathan Hilliard, Lawrence Phillips, Scott Howland, Art¨em Yankov, Courtney D Corley, and\nNathan O Hodas. Few-shot learning with metric-agnostic conditional embeddings. arXiv preprint\narXiv:1802.04376, 2018. 6\nYen-Chang Hsu, Zhaoyang Lv, and Zsolt Kira. Learning to cluster in order to transfer across do-\nmains and tasks. 2018. 3, 12\nJunlin Hu, Jiwen Lu, and Yap-Peng Tan. Deep transfer metric learning. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), 2015. 4\nGregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot\nimage recognition. In Proceedings of the International Conference on Machine Learning Work-\nshops (ICML Workshops), 2015. 2\nBrenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. One shot learning of\nsimple visual concepts. In Cogsci, 2011. 13\nThomas Mensink, Jakob Verbeek, Florent Perronnin, and Gabriela Csurka. Metric learning for large\nscale image classiﬁcation: Generalizing to new classes at near-zero cost. In Proceedings of the\nEuropean Conference on Computer Vision (ECCV). Springer, 2012. 4\nGeorge A Miller. Wordnet: a lexical database for english. Communications of the ACM, 1995. 8\nSaeid Motiian, Quinn Jones, Seyed Iranmanesh, and Gianfranco Doretto.\nFew-shot adversarial\ndomain adaptation. In Advances in Neural Information Processing Systems (NIPS), 2017. 12\nTsendsuren Munkhdalai and Hong Yu. Meta networks. In Proceedings of the International Confer-\nence on Machine Learning (ICML), 2017. 2\nAlex Nichol and John Schulman.\nReptile: a scalable metalearning algorithm.\narXiv preprint\narXiv:1803.02999, 2018. 2\nSinno Jialin Pan, Qiang Yang, et al. A survey on transfer learning. IEEE Transactions on Knowledge\nand Data Engineering (TKDE), 2010. 3\nHang Qi, Matthew Brown, and David G Lowe. Low-shot learning with imprinted weights. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n1, 2, 3, 4\nSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In Proceedings\nof the International Conference on Learning Representations (ICLR), 2017. 1, 2, 6, 12, 13\nAndrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero,\nand Raia Hadsell. Meta-learning with latent embedding optimization. In Proceedings of the\nInternational Conference on Learning Representations (ICLR), 2019. 2\nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In\nAdvances in Neural Information Processing Systems (NIPS), 2017. 1, 3, 4, 5, 6, 7, 12, 13, 16\nFlood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.\nLearning to compare: Relation network for few-shot learning. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition (CVPR), 2018. 1, 3, 5, 7\nOriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one\nshot learning. In Advances in Neural Information Processing Systems (NIPS), 2016. 1, 3, 4, 5, 6,\n7, 8, 12, 13\nCatherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd\nbirds-200-2011 dataset. 2011. 6\nYu-Xiong Wang, Ross Girshick, Martial Hebert, and Bharath Hariharan. Low-shot learning from\nimaginary data. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2018. 1, 3, 12\n11\n",
    "Published as a conference paper at ICLR 2019\nAPPENDIX\nA1\nRELATIONSHIP BETWEEN DOMAIN ADAPTATION AND FEW-SHOT CLASSIFICATION\nAs mentioned in Section 2, here we discuss the relationship between domain adaptation and few-\nshot classiﬁcation to clarify different experimental settings. As shown in Table A1, in general,\ndomain adaptation aims at adapting source dataset knowledge to the same class in target dataset.\nOn the other hand, the goal of few-shot classiﬁcation is to learn from base classes to classify novel\nclasses in the same dataset.\nSeveral recent work tackle the problem at the intersection of the two ﬁelds of study. For example,\ncross-task domain adaptation Hsu et al. (2018) also discuss novel classes in the target dataset. In\ncontrast, while Motiian et al. (2017) has “few-shot” in the title, their evaluation setting focuses on\nclassifying the same class in the target dataset.\nIf base and novel classes are both drawn from the same dataset, minor domain shift exists between\nthe base and novel classes, as we demonstrated in Section 4.4. To highlight the impact of domain\nshift, we further propose the mini-ImageNet →CUB setting. The domain shift in few-shot classiﬁ-\ncation is also discussed in Dong & Xing (2018).\nTable A1: Relationship between domain adaptation and few-shot classiﬁcation. The two\nﬁeld-of-studies have overlapping in the development. Notation ”*” indicates minor domain shifts\nexist between base and novel classes.\nDomain shift\nSource to target dataset\nBase to novel class\nDomain adaptation\nMotiian et al. (2017)\nV\nV\n-\nCross-task domain adaptation\nHsu et al. (2018)\nV\nV\nV\nFew-shot classiﬁcation\nOurs (CUB, mini-ImageNet )\n*\n-\nV\nCross-domain few-shot\nOurs (mini-ImageNet →CUB)\nDong & Xing (2018)\nV\nV\nV\nA2\nTERMINOLOGY DIFFERENCE\nDifferent meta-learning works use different terminology in their works. We highlight their differ-\nences in appendix Table A2 to clarify the inconsistency.\nTable A2: Different terminology used in other works. Notation ”-” indicates the term is the same\nas in this paper.\nOur terms\nMatchingNet\nVinyals et al.\nProtoNet\nSnell et al.\nMAML\nFinn et al.\nMeta-learn LSTM\nRavi & Larochelle\nImaginary\nWang et al.\nmeta-training stage\ntraining\ntraining\n-\n-\n-\nmeta-testing stage\ntest\ntest\n-\n-\n-\nbase class\ntraining set\ntraining set\ntask\nmeta-training set\n-\nnovel class\ntest set\ntest set\nnew task\nmeta-testing set\n-\nsupport set\n-\n-\nsample\ntraining dataset\ntraining data\nquery set\nbatch\n-\ntest time sample\ntest dataset\ntest data\n12\n",
    "Published as a conference paper at ICLR 2019\nA3\nADDITIONAL RESULTS ON OMNIGLOT AND OMNIGLOT→EMNIST\nFor completeness, here we also show the results under two additional scenarios in 4) character\nrecognition 5) cross-domain character recognition.\nFor character recognition, we use the Omniglot dataset Lake et al. (2011) commonly used in eval-\nuating few-shot classiﬁcation algorithms. Omniglot contains 1,623 characters from 50 languages,\nand we follow the evaluation protocol of Vinyals et al. (2016) to ﬁrst augment the classes by rota-\ntions in 90, 180, 270 degrees, resulting in 6492 classes. We then follow Snell et al. (2017) to split\nthese classes into 4112 base, 688 validation, and 1692 novel classes. Unlike Snell et al. (2017), our\nvalidation classes are only used to monitor the performance during meta-training.\nFor cross-domain character recognition (Omniglot→EMNIST), we follow the setting of Dong &\nXing (2018) to use Omniglot without Latin characters and without rotation augmentation as base\nclasses, so there are 1597 base classes. On the other hand, EMNIST dataset Cohen et al. (2017)\ncontains 10-digits and upper and lower case alphabets in English, so there are 62 classes in total. We\nsplit these classes into 31 validation and 31 novel classes, and invert the white-on-black characters\nto black-on-white as in Omniglot.\nWe use a Conv-4 backbone with input size 28x28 for both settings. As Omniglot characters are\nblack-and-white, center-aligned and rotation sensitive, we do not use data augmentation in this ex-\nperiment. To reduce the risk of over-ﬁtting, we use the validation set to select the epoch or episode\nwith the best accuracy for all methods, including baseline and baseline++.4\nAs shown in Table A3, in both Omniglot and Omniglot→EMNIST settings, meta-learning methods\noutperform baseline and baseline++ in 1-shot. However, all methods reach comparable performance\nin the 5-shot classiﬁcation setting. We attribute this to the lack of data augmentation for the baseline\nand baseline++ methods as they tend to over-ﬁt base classes. When sufﬁcient examples in novel\nclasses are available, the negative impact of over-ﬁtting is reduced.\nTable A3: Few-shot classiﬁcation results for both the Omniglot and Omniglot→EMNIST. All\nexperiments are from 5-way classiﬁcation with a Conv-4 backbone and without data augmentation.\nOmniglot\nOmniglot→EMNIST\nMethod\n1-shot\n5-shot\n1-shot\n5-shot\nBaseline\n94.89 ± 0.45\n99.12 ± 0.13\n63.94 ± 0.87\n86.00 ± 0.59\nBaseline++\n95.41 ± 0.39\n99.38 ± 0.10\n64.74 ± 0.82\n87.31 ± 0.58\nMatchingNet\n97.78 ± 0.30\n99.37 ± 0.11\n72.71 ± 0.79\n87.60 ± 0.56\nProtoNet\n98.01 ± 0.30\n99.15 ± 0.12\n70.43 ± 0.80\n87.04 ± 0.55\nMAML\n98.57 ± 0.19\n99.53 ± 0.08\n72.04 ± 0.83\n88.24 ± 0.56\nRelationNet\n97.22 ± 0.33\n99.30 ± 0.10\n75.55 ± 0.87\n88.94 ± 0.54\nA4\nBASELINE WITH 1-NN CLASSIFIER\nSome prior work (Vinyals et al. (2016)) apply a Baseline with 1-NN classiﬁer in the test stage. We\ninclude our result as in Table A4. The result shows that using 1-NN classiﬁer has better performance\nthan that of using the softmax classiﬁer in 1-shot setting, but softmax classiﬁer performs better in\n5-shot setting. We note that the number here are not directly comparable to results in Vinyals et al.\n(2016) because we use a different mini-ImageNet as in Ravi & Larochelle (2017).\nTable A4: Baseline with softmax and 1-NN classiﬁer in test stage. We note that we use cosine\ndistance in 1-NN.\n1-shot\n5-shot\nsoftmax\n1-NN\nsoftmax\n1-NN\nBaseline\n42.11±0.71\n44.18±0.69\n62.53±0.69\n56.68±0.67\nBaseline++\n48.24±0.75\n49.57±0.73\n66.43±0.63\n61.93±0.65\n4The exact epoch of baseline and baseline++ on Omniglot and Omniglot→EMNIST is 5 epochs\n13\n",
    "Published as a conference paper at ICLR 2019\nA5\nMAML AND MAML WITH FIRST-ORDER APPROXIMATION\nAs discussed in Section 4.1, we use ﬁrst-order approximation MAML to improve memory efﬁciency\nin all of our experiments. To demonstrate this design choice does not affect the accuracy, we compare\ntheir validation accuracy trends on Omniglot with 5-shot as in Figure A1. We observe that while the\nfull version MAML converge faster, both versions reach similar accuracy in the end.\nThis phenomena is consistent with the difference of ﬁrst-order (e.g. gradient descent) and second-\norder methods (e.g. Newton) in convex optimization problems. Second-order methods converge\nfaster at the cost of memory, but they both converge to similar objective value.\nFigure A1: Validation accuracy trends of MAML and MAML with ﬁrst order approximation.\nBoth versions converge to the same validation accuracy. The experimental results are on Omniglot\nwith 5-shot with a Conv-4 backbone.\nA6\nINTRA-CLASS VARIATION AND BACKBONE DEPTH\nAs mentioned in Section 4.3, here we demonstrate decreased intra-class variation as the network\ndepth gets deeper as in Figure A2. We use the Davies-Bouldin index Davies & Bouldin (1979) to\nmeasure intra-class variation. The Davies-Bouldin index is a metric to evaluate the tightness in a\ncluster (or class, in our case). Our results show that both intra-class variation in the base and novel\nclass feature decrease using deeper backbones.\n2\n3\n4\n5\n6\n7\n8\nConv-4\nConv-6\nResnet-10\nResnet-18\nResnet-34\n3\n4\n5\n6\n7\nConv-4\nConv-6\nResnet-10\nResnet-18\nResnet-34\nBaseline\nBaseline++\nMatchingNet\nProtoNet\nBase class feature\nNovel class feature\nDavies-Bouldin index\n(Intra-class variation)\nFigure A2: Intra-class variation decreases as backbone gets deeper. Here we use\nDavies-Bouldin index to represent intra-class variation, which is a metric to evaluate the tightness\nin a cluster (or class, in our case). The statistics are Davies-Bouldin index for all base and novel\nclass feature (extracted by feature extractor learned after training or meta-training stage) for CUB\ndataset under different backbone.\n14\n",
    "Published as a conference paper at ICLR 2019\nA7\nDETAILED STATISTICS IN EFFECTS OF INCREASING BACKBONE DEPTH\nHere we show a high-resolution version of Figure 3 in Figure A3 and show detailed statistics in\nTable A5 for easier comparison.\n45% \n55% \n65% \n75% \n40% \n45% \n50% \n55% \n60% \n70% \n80% \n90% \n60% \n65% \n70% \n75% \n80% \nBaseline\nBaseline++\nMatchingNet\nProtoNet\nMAML\nRelationNet\nCUB\nmini-ImageNet\n1-shot\n5-shot\nFigure A3: Few-shot classiﬁcation accuracy vs. backbone depth. In the CUB dataset, gaps\namong different methods diminish as the backbone gets deeper. In mini-ImageNet 5-shot, some\nmeta-learning methods are even beaten by Baseline with a deeper backbone.\nTable A5: Detailed statistics in Figure 3. We put exact value here for reference.\nConv-4\nConv-6\nResnet-10\nResnet-18\nResnet-34\nCUB\n1-shot\nBaseline\n47.12±0.74\n55.77±0.86\n63.34±0.91\n65.51±0.87\n67.96±0.89\nBaseline++\n60.53±0.83\n66.00±0.89\n69.55±0.89\n67.02±0.90\n68.00±0.83\nMatchingNet\n61.16±0.89\n67.16±0.97\n71.29±0.90\n72.36±0.90\n71.44±0.96\nProtoNet\n51.31±0.91\n66.07±0.97\n70.13±0.94\n71.88±0.91\n72.03±0.91\nMAML\n55.92±0.95\n65.91±0.97\n71.29±0.95\n69.96±1.01\n67.28±1.08\nRelationNet\n62.45±0.98\n63.11±0.94\n68.65±0.91\n67.59±1.02\n66.20±0.99\nCUB\n5-shot\nBaseline\n64.16±0.71\n73.07±0.71\n81.27±0.57\n82.85±0.55\n84.27±0.53\nBaseline++\n79.34±0.61\n82.02±0.55\n85.17±0.50\n83.58±0.54\n84.50±0.51\nMatchingNet\n72.86±0.70\n77.08±0.66\n83.59±0.58\n83.64±0.60\n83.78±0.56\nProtoNet\n70.77±0.69\n78.14±0.67\n84.76±0.52\n87.42±0.48\n85.98±0.53\nMAML\n72.09±0.76\n76.31±0.74\n80.33±0.70\n82.70±0.65\n83.47±0.59\nRelationNet\n76.11±0.69\n77.81±0.66\n81.12±0.63\n82.75±0.58\n82.30±0.58\nmini-ImageNet\n1-shot\nBaseline\n42.11±0.71\n45.82±0.74\n52.37±0.79\n51.75±0.80\n49.82±0.73\nBaseline++\n48.24±0.75\n48.29±0.72\n53.97±0.79\n51.87±0.77\n52.65±0.83\nMatchingNet\n48.14±0.78\n50.47±0.86\n54.49±0.81\n52.91±0.88\n53.20±0.78\nProtoNet\n44.42±0.84\n50.37±0.83\n51.98±0.84\n54.16±0.82\n53.90±0.83\nMAML\n46.47±0.82\n50.96±0.92\n54.69±0.89\n49.61±0.92\n51.46±0.90\nRelationNet\n49.31±0.85\n51.84±0.88\n52.19±0.83\n52.48±0.86\n51.74±0.83\nmini-ImageNet\n5-shot\nBaseline\n62.53±0.69\n66.42±0.67\n74.69±0.64\n74.27±0.63\n73.45±0.65\nBaseline++\n66.43±0.63\n68.09±0.69\n75.90±0.61\n75.68±0.63\n76.16±0.63\nMatchingNet\n63.48±0.66\n63.19±0.70\n68.82±0.65\n68.88±0.69\n68.32±0.66\nProtoNet\n64.24±0.72\n67.33±0.67\n72.64±0.64\n73.68±0.65\n74.65±0.64\nMAML\n62.71±0.71\n66.09±0.71\n66.62±0.83\n65.72±0.77\n65.90±0.79\nRelationNet\n66.60±0.69\n64.55±0.70\n70.20±0.66\n69.83±0.68\n69.61±0.67\n15\n",
    "Published as a conference paper at ICLR 2019\nA8\nMORE-WAY IN META-TESTING STAGE\nWe experiment with a practical setting that handles different testing scenarios. Speciﬁcally, we\nconduct the experiments of 5-way meta-training and N-way meta-testing (where N = 5, 10, 20) to\nexamine the effect of testing scenarios that are different from training.\nAs in Table A6, we compare the methods Baseline, Baseline++, MatchingNet, ProtoNet, and Re-\nlationNet. Note that we are unable to apply the MAML method as MAML learns the initialization\nfor the classiﬁer and can thus only be updated to classify the same number of classes. Our results\nshow that for classiﬁcation with a larger N-way in the meta-testing stage, the proposed Baseline++\ncompares favorably against other methods in both shallow or deeper backbone settings.\nWe attribute the results to two reasons. First, to perform well in a larger N-way classiﬁcation setting,\none needs to further reduce the intra-class variation to avoid misclassiﬁcation. Thus, Baseline++ has\nbetter performance than Baseline in both backbone settings. Second, as meta-learning algorithms\nwere trained to perform 5-way classiﬁcation in the meta-training stage, the performance of these\nalgorithms may drop signiﬁcantly when increasing the N-way in the meta-testing stage because the\ntasks of 10-way or 20-way classiﬁcation are harder than that of 5-way one.\nOne may address this issue by performing a larger N-way classiﬁcation in the meta-training stage\n(as suggested in Snell et al. (2017)). However, it may encounter the issue of memory constraint.\nFor example, to perform a 20-way classiﬁcation with 5 support images and 15 query images in each\nclass, we need to ﬁt a batch size of 400 (20 x (5 + 15)) that must ﬁt into the GPUs. Without special\nhardware parallelization, the large batch size may prevent us from training models with deeper\nbackbones such as ResNet.\nTable A6: 5-way meta-training and N-way meta-testing experiment. The experimental results\nare on mini-ImageNet with 5-shot. We could see Baseline++ compares favorably against other\nmethods in both shallow or deeper backbone settings.\nConv-4\nResNet-18\nN-way test\n5-way\n10-way\n20-way\n5-way\n10-way\n20-way\nBaseline\n62.53±0.69\n46.44±0.41\n32.27±0.24\n74.27±0.63\n55.00±0.46\n42.03±0.25\nBaseline++\n66.43±0.63\n52.26±0.40\n38.03±0.24\n75.68±0.63\n63.40±0.44\n50.85±0.25\nMatchingNet\n63.48±0.66\n47.61±0.44\n33.97±0.24\n68.88±0.69\n52.27±0.46\n36.78±0.25\nProtoNet\n64.24±0.68\n48.77±0.45\n34.58±0.23\n73.68±0.65\n59.22±0.44\n44.96±0.26\nRelationNet\n66.60±0.69\n47.77±0.43\n33.72±0.22\n69.83±0.68\n53.88±0.48\n39.17±0.25\n16\n"
  ],
  "full_text": "Published as a conference paper at ICLR 2019\nA CLOSER LOOK AT FEW-SHOT CLASSIFICATION\nWei-Yu Chen\nCarnegie Mellon University\nweiyuc@andrew.cmu.edu\nYen-Cheng Liu & Zsolt Kira\nGeorgia Tech\n{ycliu,zkira}@gatech.edu\nYu-Chiang Frank Wang\nNational Taiwan University\nycwang@ntu.edu.tw\nJia-Bin Huang\nVirginia Tech\njbhuang@vt.edu\nABSTRACT\nFew-shot classiﬁcation aims to learn a classiﬁer to recognize unseen classes during\ntraining with limited labeled examples. While signiﬁcant progress has been made,\nthe growing complexity of network designs, meta-learning algorithms, and differ-\nences in implementation details make a fair comparison difﬁcult. In this paper,\nwe present 1) a consistent comparative analysis of several representative few-shot\nclassiﬁcation algorithms, with results showing that deeper backbones signiﬁcantly\nreduce the performance differences among methods on datasets with limited do-\nmain differences, 2) a modiﬁed baseline method that surprisingly achieves com-\npetitive performance when compared with the state-of-the-art on both the mini-\nImageNet and the CUB datasets, and 3) a new experimental setting for evaluating\nthe cross-domain generalization ability for few-shot classiﬁcation algorithms. Our\nresults reveal that reducing intra-class variation is an important factor when the\nfeature backbone is shallow, but not as critical when using deeper backbones. In\na realistic cross-domain evaluation setting, we show that a baseline method with\na standard ﬁne-tuning practice compares favorably against other state-of-the-art\nfew-shot learning algorithms.\n1\nINTRODUCTION\nDeep learning models have achieved state-of-the-art performance on visual recognition tasks such\nas image classiﬁcation. The strong performance, however, heavily relies on training a network with\nabundant labeled instances with diverse visual variations (e.g., thousands of examples for each new\nclass even with pre-training on large-scale dataset with base classes). The human annotation cost as\nwell as the scarcity of data in some classes (e.g., rare species) signiﬁcantly limit the applicability of\ncurrent vision systems to learn new visual concepts efﬁciently. In contrast, the human visual systems\ncan recognize new classes with extremely few labeled examples. It is thus of great interest to learn\nto generalize to new classes with a limited amount of labeled examples for each novel class.\nThe problem of learning to generalize to unseen classes during training, known as few-shot clas-\nsiﬁcation, has attracted considerable attention Vinyals et al. (2016); Snell et al. (2017); Finn et al.\n(2017); Ravi & Larochelle (2017); Sung et al. (2018); Garcia & Bruna (2018); Qi et al. (2018).\nOne promising direction to few-shot classiﬁcation is the meta-learning paradigm where transfer-\nable knowledge is extracted and propagated from a collection of tasks to prevent overﬁtting and\nimprove generalization. Examples include model initialization based methods Ravi & Larochelle\n(2017); Finn et al. (2017), metric learning methods Vinyals et al. (2016); Snell et al. (2017); Sung\net al. (2018), and hallucination based methods Antoniou et al. (2018); Hariharan & Girshick (2017);\nWang et al. (2018). Another line of work Gidaris & Komodakis (2018); Qi et al. (2018) also demon-\nstrates promising results by directly predicting the weights of the classiﬁers for novel classes.\nLimitations.\nWhile many few-shot classiﬁcation algorithms have reported improved performance\nover the state-of-the-art, there are two main challenges that prevent us from making a fair compari-\nson and measuring the actual progress. First, the discrepancy of the implementation details among\nmultiple few-shot learning algorithms obscures the relative performance gain. The performance of\n1\n\n\nPublished as a conference paper at ICLR 2019\nbaseline approaches can also be signiﬁcantly under-estimated (e.g., training without data augmenta-\ntion). Second, while the current evaluation focuses on recognizing novel class with limited training\nexamples, these novel classes are sampled from the same dataset. The lack of domain shift between\nthe base and novel classes makes the evaluation scenarios unrealistic.\nOur work.\nIn this paper, we present a detailed empirical study to shed new light on the few-shot\nclassiﬁcation problem. First, we conduct consistent comparative experiments to compare several\nrepresentative few-shot classiﬁcation methods on common ground. Our results show that using a\ndeep backbone shrinks the performance gap between different methods in the setting of limited do-\nmain differences between base and novel classes. Second, by replacing the linear classiﬁer with\na distance-based classiﬁer as used in Gidaris & Komodakis (2018); Qi et al. (2018), the baseline\nmethod is surprisingly competitive to current state-of-art meta-learning algorithms. Third, we intro-\nduce a practical evaluation setting where there exists domain shift between base and novel classes\n(e.g., sampling base classes from generic object categories and novel classes from ﬁne-grained cat-\negories). Our results show that sophisticated few-shot learning algorithms do not provide perfor-\nmance improvement over the baseline under this setting. Through making the source code and\nmodel implementations with a consistent evaluation setting publicly available, we hope to foster\nfuture progress in the ﬁeld.1\nOur contributions.\n1. We provide a uniﬁed testbed for several different few-shot classiﬁcation algorithms for a fair\ncomparison. Our empirical evaluation results reveal that the use of a shallow backbone commonly\nused in existing work leads to favorable results for methods that explicitly reduce intra-class\nvariation. Increasing the model capacity of the feature backbone reduces the performance gap\nbetween different methods when domain differences are limited.\n2. We show that a baseline method with a distance-based classiﬁer surprisingly achieves competitive\nperformance with the state-of-the-art meta-learning methods on both mini-ImageNet and CUB\ndatasets.\n3. We investigate a practical evaluation setting where base and novel classes are sampled from dif-\nferent domains. We show that current few-shot classiﬁcation algorithms fail to address such do-\nmain shifts and are inferior even to the baseline method, highlighting the importance of learning\nto adapt to domain differences in few-shot learning.\n2\nRELATED WORK\nGiven abundant training examples for the base classes, few-shot learning algorithms aim to learn\nto recognizing novel classes with a limited amount of labeled examples. Much efforts have been\ndevoted to overcome the data efﬁciency issue. In the following, we discuss representative few-shot\nlearning algorithms organized into three main categories: initialization based, metric learning based,\nand hallucination based methods.\nInitialization based methods\ntackle the few-shot learning problem by “learning to ﬁne-tune”.\nOne approach aims to learn good model initialization (i.e., the parameters of a network) so that the\nclassiﬁers for novel classes can be learned with a limited number of labeled examples and a small\nnumber of gradient update steps Finn et al. (2017; 2018); Nichol & Schulman (2018); Rusu et al.\n(2019). Another line of work focuses on learning an optimizer. Examples include the LSTM-based\nmeta-learner for replacing the stochastic gradient decent optimizer Ravi & Larochelle (2017) and\nthe weight-update mechanism with an external memory Munkhdalai & Yu (2017). While these ini-\ntialization based methods are capable of achieving rapid adaption with a limited number of training\nexamples for novel classes, our experiments show that these methods have difﬁculty in handling\ndomain shifts between base and novel classes.\nDistance metric learning based methods\naddress the few-shot classiﬁcation problem by “learn-\ning to compare”. The intuition is that if a model can determine the similarity of two images, it can\nclassify an unseen input image with the labeled instances Koch et al. (2015). To learn a sophisticated\ncomparison models, meta-learning based methods make their prediction conditioned on distance or\n1https://github.com/wyharveychen/CloserLookFewShot\n2\n\n\nPublished as a conference paper at ICLR 2019\nmetric to few labeled instances during the training process. Examples of distance metrics include\ncosine similarity Vinyals et al. (2016), Euclidean distance to class-mean representation Snell et al.\n(2017), CNN-based relation module Sung et al. (2018), ridge regression Bertinetto et al. (2019), and\ngraph neural network Garcia & Bruna (2018). In this paper, we compare the performance of three\ndistance metric learning methods. Our results show that a simple baseline method with a distance-\nbased classiﬁer (without training over a collection of tasks/episodes as in meta-learning) achieves\ncompetitive performance with respect to other sophisticated algorithms.\nBesides meta-learning methods, both Gidaris & Komodakis (2018) and Qi et al. (2018) develop\na similar method to our Baseline++ (described later in Section 3.2). The method in Gidaris &\nKomodakis (2018) learns a weight generator to predict the novel class classiﬁer using an attention-\nbased mechanism (cosine similarity), and the Qi et al. (2018) directly use novel class features as\ntheir weights. Our Baseline++ can be viewed as a simpliﬁed architecture of these methods. Our\nfocus, however, is to show that simply reducing intra-class variation in a baseline method using the\nbase class data leads to competitive performance.\nHallucination based methods\ndirectly deal with data deﬁciency by “learning to augment”. This\nclass of methods learns a generator from data in the base classes and use the learned generator to\nhallucinate new novel class data for data augmentation. One type of generator aims at transferring\nappearance variations exhibited in the base classes. These generators either transfer variance in\nbase class data to novel classes Hariharan & Girshick (2017), or use GAN models Antoniou et al.\n(2018) to transfer the style. Another type of generators does not explicitly specify what to transfer,\nbut directly integrate the generator into a meta-learning algorithm for improving the classiﬁcation\naccuracy Wang et al. (2018). Since hallucination based methods often work with other few-shot\nmethods together (e.g. use hallucination based and metric learning based methods together) and\nlead to complicated comparison, we do not include these methods in our comparative study and\nleave it for future work.\nDomain adaptation\ntechniques aim to reduce the domain shifts between source and target domain\nPan et al. (2010); Ganin & Lempitsky (2015), as well as novel tasks in a different domain Hsu et al.\n(2018). Similar to domain adaptation, we also investigate the impact of domain difference on few-\nshot classiﬁcation algorithms in Section 4.5. In contrast to most domain adaptation problems where\na large amount of data is available in the target domain (either labeled or unlabeled), our problem\nsetting differs because we only have very few examples in the new domain. Very recently, the\nmethod in Dong & Xing (2018) addresses the one-shot novel category domain adaptation problem,\nwhere in the testing stage both the domain and the category to classify are changed. Similarly, our\nwork highlights the limitations of existing few-shot classiﬁcation algorithms problem in handling\ndomain shift. To put these problem settings in context, we provided a detailed comparison of setting\ndifference in the appendix A1.\n3\nOVERVIEW OF FEW-SHOT CLASSIFICATION ALGORITHMS\nIn this section, we ﬁrst outline the details of the baseline model (Section 3.1) and its variant (Sec-\ntion 3.2), followed by describing representative meta-learning algorithms (Section 3.3) studied in\nour experiments. Given abundant base class labeled data Xb and a small amount of novel class la-\nbeled data Xn, the goal of few-shot classiﬁcation algorithms is to train classiﬁers for novel classes\n(unseen during training) with few labeled examples.\n3.1\nBASELINE\nOur baseline model follows the standard transfer learning procedure of network pre-training and\nﬁne-tuning. Figure 1 illustrates the overall procedure.\nTraining stage.\nWe train a feature extractor fθ (parametrized by the network parameters θ) and\nthe classiﬁer C(·|Wb) (parametrized by the weight matrix Wb ∈Rd×c) from scratch by minimizing\na standard cross-entropy classiﬁcation loss Lpred using the training examples in the base classes\nxi ∈Xb. Here, we denote the dimension of the encoded feature as d and the number of output\nclasses as c. The classiﬁer C(.|Wb) consists of a linear layer W⊤\nb fθ(xi) followed by a softmax\nfunction σ.\n3\n\n\nPublished as a conference paper at ICLR 2019\nBaseline++\nBaseline\nTraining stage\nClassifier\nFeature \nextractor\nNovel class data\n(Few) \nFine-tuning stage\nFixed\nFeature \nextractor\nBase class data\n(Many)\nLinear\nlayer  \nSoftmax\n𝜎\nSoftmax\n𝜎\nCosine \ndistance\nClassifier\nClassifier\n…\n…\nFigure 1: Baseline and Baseline++ few-shot classiﬁcation methods. Both the baseline and\nbaseline++ method train a feature extractor fθ and classiﬁer C(.|Wb) with base class data in the\ntraining stage In the ﬁne-tuning stage, we ﬁx the network parameters θ in the feature extractor fθ\nand train a new classiﬁer C(.|Wn) with the given labeled examples in novel classes. The\nbaseline++ method differs from the baseline model in the use of cosine distances between the input\nfeature and the weight vector for each class that aims to reduce intra-class variations.\nFine-tuning stage.\nTo adapt the model to recognize novel classes in the ﬁne-tuning stage, we ﬁx\nthe pre-trained network parameter θ in our feature extractor fθ and train a new classiﬁer C(.|Wn)\n(parametrized by the weight matrix Wn) by minimizing Lpred using the few labeled of examples (i.e.,\nthe support set) in the novel classes Xn.\n3.2\nBASELINE++\nIn addition to the baseline model, we also implement a variant of the baseline model, denoted as\nBaseline++, which explicitly reduces intra-class variation among features during training. The im-\nportance of reducing intra-class variations of features has been highlighted in deep metric learn-\ning Hu et al. (2015) and few-shot classiﬁcation methods Gidaris & Komodakis (2018).\nThe training procedure of Baseline++ is the same as the original Baseline model except for the\nclassiﬁer design. As shown in Figure 1, we still have a weight matrix Wb ∈Rd×c of the classiﬁer in\nthe training stage and a Wn in the ﬁne-tuning stage in Baseline++. The classiﬁer design, however, is\ndifferent from the linear classiﬁer used in the Baseline. Take the weight matrix Wb as an example.\nWe can write the weight matrix Wb as [w1,w2,...wc], where each class has a d-dimensional weight\nvector. In the training stage, for an input feature fθ(xi) where xi ∈Xb, we compute its cosine\nsimilarity to each weight vector [w1,··· ,wc] and obtain the similarity scores [si,1,si,2,··· ,si,c] for\nall classes, where si,j = fθ(xi)⊤w j/∥fθ(xi)∥∥w j∥. We can then obtain the prediction probability\nfor each class by normalizing these similarity scores with a softmax function. Here, the classiﬁer\nmakes a prediction based on the cosine distance between the input feature and the learned weight\nvectors representing each class. Consequently, training the model with this distance-based classiﬁer\nexplicitly reduce intra-class variations. Intuitively, the learned weight vectors [w1,··· ,wc] can be\ninterpreted as prototypes (similar to Snell et al. (2017); Vinyals et al. (2016)) for each class and the\nclassiﬁcation is based on the distance of the input feature to these learned prototypes. The softmax\nfunction prevents the learned weight vectors collapsing to zeros.\nWe clarify that the network design in Baseline++ is not our contribution. The concept of distance-\nbased classiﬁcation has been extensively studied in Mensink et al. (2012) and recently has been\nrevisited in the few-shot classiﬁcation setting Gidaris & Komodakis (2018); Qi et al. (2018).\n3.3\nMETA-LEARNING ALGORITHMS\nHere we describe the formulations of meta-learning methods used in our study. We consider three\ndistance metric learning based methods (MatchingNet Vinyals et al. (2016), ProtoNet Snell et al.\n4\n\n\nPublished as a conference paper at ICLR 2019\nMeta-training stage\nMeta-testing stage\nSupport set\nconditioned model\nNovel support set    \n(Novel class data     )\nBase query set\nBase support set\n𝑌\"\nSampled 𝑁classes\nSupport set conditioned model\nFeature \nextractor\nMatchingNet\nCosine \ndistance\nRelationNet\nRelation\nModule\n𝜇\nProtoNet\nEuclidean\ndistance\n𝜇\nMAML\nGradient\nLinear\nLinear\nBase class data \n(Many)\nClass \nmean\nClass \nmean\nFigure 2: Meta-learning few-shot classiﬁcation algorithms. The meta-learning classiﬁer M(·|S)\nis conditioned on the support set S. (Top) In the meta-train stage, the support set Sb and the query\nset Qb are ﬁrst sampled from random N classes, and then train the parameters in M(.|Sb) to\nminimize the N-way prediction loss LN−way. In the meta-testing stage, the adapted classiﬁer\nM(.|Sn) can predict novel classes with the support set in the novel classes Sn. (Bottom) The design\nof M(·|S) in different meta-learning algorithms.\n(2017), and RelationNet Sung et al. (2018)) and one initialization based method (MAML Finn et al.\n(2017)). While meta-learning is not a clearly deﬁned, Vinyals et al. (2016) considers a few-shot\nclassiﬁcation method as meta-learning if the prediction is conditioned on a small support set S,\nbecause it makes the training procedure explicitly learn to learn from a given small support set.\nAs shown in Figure 2, meta-learning algorithms consist of a meta-training and a meta-testing stage.\nIn the meta-training stage, the algorithm ﬁrst randomly select N classes, and sample small base\nsupport set Sb and a base query set Qb from data samples within these classes. The objective is\nto train a classiﬁcation model M that minimizes N-way prediction loss LN−way of the samples in\nthe query set Qb. Here, the classiﬁer M is conditioned on provided support set Sb. By making\nprediction conditioned on the given support set, a meta-learning method can learn how to learn from\nlimited labeled data through training from a collection of tasks (episodes). In the meta-testing stage,\nall novel class data Xn are considered as the support set for novel classes Sn, and the classiﬁcation\nmodel M can be adapted to predict novel classes with the new support set Sn.\nDifferent meta-learning methods differ in their strategies to make prediction conditioned on support\nset (see Figure 2). For both MatchingNet Vinyals et al. (2016) and ProtoNet Snell et al. (2017), the\nprediction of the examples in a query set Q is based on comparing the distance between the query\nfeature and the support feature from each class. MatchingNet compares cosine distance between the\nquery feature and each support feature, and computes average cosine distance for each class, while\nProtoNet compares the Euclidean distance between query features and the class mean of support\nfeatures. RelationNet Sung et al. (2018) shares a similar idea, but it replaces distance with a learn-\nable relation module. The MAML method Finn et al. (2017) is an initialization based meta-learning\nalgorithm, where each support set is used to adapt the initial model parameters using few gradient\nupdates. As different support sets have different gradient updates, the adapted model is conditioned\non the support set. Note that when the query set instances are predicted by the adapted model in\nthe meta-training stage, the loss of the query set is used to update the initial model, not the adapted\nmodel.\n4\nEXPERIMENTAL RESULTS\n4.1\nEXPERIMENTAL SETUP\nDatasets and scenarios.\nWe address the few-shot classiﬁcation problem under three scenarios: 1)\ngeneric object recognition, 2) ﬁne-grained image classiﬁcation, and 3) cross-domain adaptation.\n5\n\n\nPublished as a conference paper at ICLR 2019\nFor object recognition, we use the mini-ImageNet dataset commonly used in evaluating few-shot\nclassiﬁcation algorithms. The mini-ImageNet dataset consists of a subset of 100 classes from the\nImageNet dataset Deng et al. (2009) and contains 600 images for each class. The dataset was ﬁrst\nproposed by Vinyals et al. (2016), but recent works use the follow-up setting provided by Ravi &\nLarochelle (2017), which is composed of randomly selected 64 base, 16 validation, and 20 novel\nclasses.\nFor ﬁne-grained classiﬁcation, we use CUB-200-2011 dataset Wah et al. (2011) (referred to as the\nCUB hereafter). The CUB dataset contains 200 classes and 11,788 images in total. Following\nthe evaluation protocol of Hilliard et al. (2018), we randomly split the dataset into 100 base, 50\nvalidation, and 50 novel classes.\nFor the cross-domain scenario (mini-ImageNet →CUB), we use mini-ImageNet as our base class\nand the 50 validation and 50 novel class from CUB. Evaluating the cross-domain scenario allows us\nto understand the effects of domain shifts to existing few-shot classiﬁcation approaches.\nImplementation details.\nIn the training stage for the Baseline and the Baseline++ methods, we\ntrain 400 epochs with a batch size of 16. In the meta-training stage for meta-learning methods, we\ntrain 60,000 episodes for 1-shot and 40,000 episodes for 5-shot tasks. We use the validation set\nto select the training episodes with the best accuracy.2 In each episode, we sample N classes to\nform N-way classiﬁcation (N is 5 in both meta-training and meta-testing stages unless otherwise\nmentioned). For each class, we pick k labeled instances as our support set and 16 instances for the\nquery set for a k-shot task.\nIn the ﬁne-tuning or meta-testing stage for all methods, we average the results over 600 experiments.\nIn each experiment, we randomly sample 5 classes from novel classes, and in each class, we also\npick k instances for the support set and 16 for the query set. For Baseline and Baseline++, we use the\nentire support set to train a new classiﬁer for 100 iterations with a batch size of 4. For meta-learning\nmethods, we obtain the classiﬁcation model conditioned on the support set as in Section 3.3.\nAll methods are trained from scratch and use the Adam optimizer with initial learning rate 10−3.\nWe apply standard data augmentation including random crop, left-right ﬂip, and color jitter in both\nthe training or meta-training stage. Some implementation details have been adjusted individually\nfor each method. For Baseline++, we multiply the cosine similarity by a constant scalar 2 to adjust\noriginal value range [-1,1] to be more appropriate for subsequent softmax layer. For MatchingNet,\nwe use an FCE classiﬁcation layer without ﬁne-tuning in all experiments and also multiply cosine\nsimilarity by a constant scalar. For RelationNet, we replace the L2 norm with a softmax layer\nto expedite training. For MAML, we use a ﬁrst-order approximation in the gradient for memory\nefﬁciency. The approximation has been shown in the original paper and in our appendix to have\nnearly identical performance as the full version. We choose the ﬁrst-order approximation for its\nefﬁciency.\n4.2\nEVALUATION USING THE STANDARD SETTING\nWe now conduct experiments on the most common setting in few-shot classiﬁcation, 1-shot and\n5-shot classiﬁcation, i.e., 1 or 5 labeled instances are available from each novel class. We use a\nfour-layer convolution backbone (Conv-4) with an input size of 84x84 as in Snell et al. (2017) and\nperform 5-way classiﬁcation for only novel classes during the ﬁne-tuning or meta-testing stage.\nTo validate the correctness of our implementation, we ﬁrst compare our results to the reported num-\nbers for the mini-ImageNet dataset in Table 1. Note that we have a ProtoNet#, as we use 5-way\nclassiﬁcation in the meta-training and meta-testing stages for all meta-learning methods as men-\ntioned in Section 4.1; however, the ofﬁcial reported results from ProtoNet uses 30-way for one shot\nand 20-way for ﬁve shot in the meta-training stage in spite of using 5-way in the meta-testing stage.\nWe report this result for completeness.\nFrom Table 1, we can observe that all of our re-implementation for meta-learning methods do not\nfall more than 2% behind reported performance.\nThese minor differences can be attributed to our\n2For example, the exact episodes for experiments on the mini-ImageNet in the 5-shot setting with a four-\nlayer ConvNet are: ProtoNet: 24,600; MatchingNet: 35,300; RelationNet: 37,100; MAML: 36,700.\n3Reported results are from Ravi & Larochelle (2017)\n6\n\n\nPublished as a conference paper at ICLR 2019\nTable 1: Validating our re-implementation. We validate our few-shot classiﬁcation\nimplementation on the mini-ImageNet dataset using a Conv-4 backbone. We report the mean of\n600 randomly generated test episodes as well as the 95% conﬁdence intervals. Our reproduced\nresults to all few-shot methods do not fall behind by more than 2% to the reported results in the\nliterature. We attribute the slight discrepancy to different random seeds and minor implementation\ndifferences in each method. “Baseline∗” denotes the results without applying data augmentation\nduring training. ProtoNet# indicates performing 30-way classiﬁcation in 1-shot and 20-way in\n5-shot during the meta-training stage.\n1-shot\n5-shot\nMethod\nReported\nOurs\nReported\nOurs\nBaseline\n-\n42.11 ± 0.71\n-\n62.53 ±0.69\nBaseline∗3\n41.08 ± 0.70\n36.35 ± 0.64\n51.04 ± 0.65\n54.50 ±0.66\nMatchingNet3 Vinyals et al. (2016)\n43.56 ± 0.84\n48.14 ± 0.78\n55.31 ±0.73\n63.48 ±0.66\nProtoNet\n-\n44.42 ± 0.84\n-\n64.24 ±0.72\nProtoNet# Snell et al. (2017)\n49.42 ± 0.78\n47.74 ± 0.84\n68.20 ±0.66\n66.68 ±0.68\nMAML Finn et al. (2017)\n48.07 ± 1.75\n46.47 ± 0.82\n63.15 ±0.91\n62.71 ±0.71\nRelationNet Sung et al. (2018)\n50.44 ± 0.82\n49.31 ± 0.85\n65.32 ±0.70\n66.60 ±0.69\nTable 2: Few-shot classiﬁcation results for both the mini-ImageNet and CUB datasets. The\nBaseline++ consistently improves the Baseline model by a large margin and is competitive with the\nstate-of-the-art meta-learning methods. All experiments are from 5-way classiﬁcation with a\nConv-4 backbone and data augmentation.\nCUB\nmini-ImageNet\nMethod\n1-shot\n5-shot\n1-shot\n5-shot\nBaseline\n47.12 ± 0.74\n64.16 ± 0.71\n42.11 ± 0.71\n62.53 ±0.69\nBaseline++\n60.53 ± 0.83\n79.34 ± 0.61\n48.24 ± 0.75\n66.43 ±0.63\nMatchingNet Vinyals et al. (2016)\n61.16 ± 0.89\n72.86 ± 0.70\n48.14 ± 0.78\n63.48 ±0.66\nProtoNet Snell et al. (2017)\n51.31 ± 0.91\n70.77 ± 0.69\n44.42 ± 0.84\n64.24 ±0.72\nMAML Finn et al. (2017)\n55.92 ± 0.95\n72.09 ± 0.76\n46.47 ± 0.82\n62.71 ±0.71\nRelationNet Sung et al. (2018)\n62.45 ± 0.98\n76.11 ± 0.69\n49.31 ± 0.85\n66.60 ±0.69\nmodiﬁcations of some implementation details to ensure a fair comparison among all methods, such\nas using the same optimizer for all methods.\nMoreover, our implementation of existing work also improves the performance of some of the meth-\nods. For example, our results show that the Baseline approach under 5-shot setting can be improved\nby a large margin since previous implementations of the Baseline do not include data augmentation\nin their training stage, thereby leads to over-ﬁtting. While our Baseline∗is not as good as reported\nin 1-shot, our Baseline with augmentation still improves on it, and could be even higher if our re-\nproduced Baseline∗matches the reported statistics. In either case, the performance of the Baseline\nmethod is severely underestimated. We also improve the results of MatchingNet by adjusting the\ninput score to the softmax layer to a more appropriate range as stated in Section 4.1. On the other\nhand, while ProtoNet# is not as good as ProtoNet, as mentioned in the original paper a more chal-\nlenging setting in the meta-training stage leads to better accuracy. We choose to use a consistent\n5-way classiﬁcation setting in subsequent experiments to have a fair comparison to other methods.\nThis issue can be resolved by using a deeper backbone as shown in Section 4.3.\nAfter validating our re-implementation, we now report the accuracy in Table 2. Besides additionally\nreporting results on the CUB dataset, we also compare Baseline++ to other methods. Here, we\nﬁnd that Baseline++ improves the Baseline by a large margin and becomes competitive even when\ncompared with other meta-learning methods. The results demonstrate that reducing intra-class\nvariation is an important factor in the current few-shot classiﬁcation problem setting.\n7\n\n\nPublished as a conference paper at ICLR 2019\n45%\n55%\n65%\n75%\nConv-4\nConv-6\nResNet-10\nResNet-18\nResNet-34\n60% \n70% \n80% \n90% \nConv-4\nConv-6\nResNet-10\nResNet-18\nResNet-34\n40% \n45% \n50% \n55% \nConv-4\nConv-6\nResNet-10\nResNet-18\nResNet-34\n60% \n65% \n70% \n75% \n80% \nConv-4\nConv-6\nResNet-10\nResNet-18\nResNet-34\nBaseline\nBaseline++\nMatchingNet\nProtoNet\nMAML\nRelationNet\nCUB\n1-shot\n5-shot\nmini-ImageNet\n1-shot\n5-shot\nFigure 3: Few-shot classiﬁcation accuracy vs. backbone depth. In the CUB dataset, gaps among\ndifferent methods diminish as the backbone gets deeper. In mini-ImageNet 5-shot, some\nmeta-learning methods are even beaten by Baseline with a deeper backbone. (Please refer to\nFigure A3 and Table A5 for larger ﬁgure and detailed statistics.)\nHowever, note that our current setting only uses a 4-layer backbone, while a deeper backbone can\ninherently reduce intra-class variation. Thus, we conduct experiments to investigate the effects of\nbackbone depth in the next section.\n4.3\nEFFECT OF INCREASING THE NETWORK DEPTH\nIn this section, we change the depth of the feature backbone to reduce intra-class variation for all\nmethods. See appendix for statistics on how network depth correlates with intra-class variation.\nStarting from Conv-4, we gradually increase the feature backbone to Conv-6, ResNet-10, 18 and 34,\nwhere Conv-6 have two additional convolution blocks without pooling after Conv-4. ResNet-18 and\n34 are the same as described in He et al. (2016) with an input size of 224×224, while ResNet-10 is\na simpliﬁed version of ResNet-18 where only one residual building block is used in each layer. The\nstatistics of this experiment would also be helpful to other works to make a fair comparison under\ndifferent feature backbones.\nResults of the CUB dataset shows a clearer tendency in Figure 3. As the backbone gets deeper, the\ngap among different methods drastically reduces. Another observation is how ProtoNet improves\nrapidly as the backbone gets deeper. While using a consistent 5-way classiﬁcation as discussed in\nSection 4.2 degrades the accuracy of ProtoNet with Conv-4, it works well with a deeper backbone.\nThus, the two observations above demonstrate that in the CUB dataset, the gap among existing\nmethods would be reduced if their intra-class variation are all reduced by a deeper backbone.\nHowever, the result of mini-ImageNet in Figure 3 is much more complicated. In the 5-shot setting,\nboth Baseline and Baseline++ achieve good performance with a deeper backbone, but some meta-\nlearning methods become worse relative to them. Thus, other than intra-class variation, we can\nassume that the dataset is also important in few-shot classiﬁcation. One difference between CUB and\nmini-ImageNet is their domain difference in base and novel classes since classes in mini-ImageNet\nhave a larger divergence than CUB in a word-net hierarchy Miller (1995). To better understand the\neffect, below we discuss how domain differences between base and novel classes impact few-shot\nclassiﬁcation results.\n4.4\nEFFECT OF DOMAIN DIFFERENCES BETWEEN BASE AND NOVEL CLASSES\nTo further dig into the issue of domain difference, we design scenarios that provide such domain\nshifts. Besides the ﬁne-grained classiﬁcation and object recognition scenarios, we propose a new\ncross-domain scenario: mini-ImageNet →CUB as mentioned in\nSection 4.1. We believe that\nthis is practical scenario since collecting images from a general class may be relatively easy (e.g.\ndue to increased availability) but collecting images from ﬁne-grained classes might be more difﬁcult.\nWe conduct the experiments with a ResNet-18 feature backbone. As shown in Table 3, the Baseline\noutperforms all meta-learning methods under this scenario. While meta-learning methods learn to\nlearn from the support set during the meta-training stage, they are not able to adapt to novel classes\nthat are too different since all of the base support sets are within the same dataset. A similar concept\nis also mentioned in Vinyals et al. (2016). In contrast, the Baseline simply replaces and trains a\nnew classiﬁer based on the few given novel class data, which allows it to quickly adapt to a novel\n8\n\n\nPublished as a conference paper at ICLR 2019\nmini-ImageNet →CUB\nBaseline\n65.57±0.70\nBaseline++\n62.04±0.76\nMatchingNet\n53.07±0.74\nProtoNet\n62.02±0.70\nMAML\n51.34±0.72\nRelationNet\n57.71±0.73\nTable 3: 5-shot accuracy under the\ncross-domain scenario with a ResNet-18\nbackbone. Baseline outperforms all other\nmethods under this scenario.\n40% \n50% \n60% \n70% \n80% \n90% \nCUB\nminiImageNet\nminiImageNet -> CUB \nBaseline\nBaseline++\nMatchingNet\nProtoNet\nMAML\nRelationNet\nDomain Difference\nLarge\nSmall\nFigure 4: 5-shot accuracy in different scenarios\nwith a ResNet-18 backbone. The Baseline\nmodel performs relative well with larger domain\ndifferences.\nCUB\nmini-ImageNet\nCUB→mini-ImageNet\nFigure 5: Meta-learning methods with further adaptation steps. Further adaptation improves\nMatchingNet and MAML, but has less improvement to RelationNet, and could instead harm\nProtoNet under the scenarios with little domain differences.All statistics are for 5-shot accuracy\nwith ResNet-18 backbone. Note that different methods use different further adaptation strategies.\nclass and is less affected by domain shift between the source and target domains. The Baseline\nalso performs better than the Baseline++ method, possibly because additionally reducing intra-class\nvariation compromises adaptability. In Figure 4, we can further observe how Baseline accuracy\nbecomes relatively higher as the domain difference gets larger. That is, as the domain difference\ngrows larger, the adaptation based on a few novel class instances becomes more important.\n4.5\nEFFECT OF FURTHER ADAPTATION\nTo further adapt meta-learning methods as in the Baseline method, an intuitive way is to ﬁx the\nfeatures and train a new softmax classiﬁer. We apply this simple adaptation scheme to MatchingNet\nand ProtoNet. For MAML, it is not feasible to ﬁx the feature as it is an initialization method. In\ncontrast, since it updates the model with the support set for only a few iterations, we can adapt\nfurther by updating for as many iterations as is required to train a new classiﬁcation layer, which\nis 100 updates as mentioned in Section 4.1. For RelationNet, the features are convolution maps\nrather than the feature vectors, so we are not able to replace it with a softmax. As an alternative, we\nrandomly split the few training data in novel class into 3 support and 2 query data to ﬁnetune the\nrelation module for 100 epochs.\nThe results of further adaptation are shown in Figure 5; we can observe that the performance of\nMatchingNet and MAML improves signiﬁcantly after further adaptation, particularly in the mini-\nImageNet →CUB scenario. The results demonstrate that lack of adaptation is the reason they fall\nbehind the Baseline. However, changing the setting in the meta-testing stage can lead to inconsis-\ntency with the meta-training stage. The ProtoNet result shows that performance can degrade in sce-\n9\n\n\nPublished as a conference paper at ICLR 2019\nnarios with less domain difference. Thus, we believe that learning how to adapt in the meta-training\nstage is important future direction. In summary, as domain differences are likely to exist in many\nreal-world applications, we consider that learning to learn adaptation in the meta-training stage\nwould be an important direction for future meta-learning research in few-shot classiﬁcation.\n5\nCONCLUSIONS\nIn this paper, we have investigated the limits of the standard evaluation setting for few-shot classi-\nﬁcation. Through comparing methods on a common ground, our results show that the Baseline++\nmodel is competitive to state of art under standard conditions, and the Baseline model achieves\ncompetitive performance with recent state-of-the-art meta-learning algorithms on both CUB and\nmini-ImageNet benchmark datasets when using a deeper feature backbone. Surprisingly, the Base-\nline compares favorably against all the evaluated meta-learning algorithms under a realistic scenario\nwhere there exists domain shift between the base and novel classes. By making our source code\npublicly available, we believe that community can beneﬁt from the consistent comparative experi-\nments and move forward to tackle the challenge of potential domain shifts in the context of few-shot\nlearning.\nREFERENCES\nAntreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarial\nnetworks. In Proceedings of the International Conference on Learning Representations Work-\nshops (ICLR Workshops), 2018. 1, 3\nLuca Bertinetto, Jo˜ao F Henriques, Philip HS Torr, and Andrea Vedaldi. Meta-learning with dif-\nferentiable closed-form solvers. In Proceedings of the International Conference on Learning\nRepresentations (ICLR), 2019. 3\nGregory Cohen, Saeed Afshar, Jonathan Tapson, and Andr´e van Schaik. Emnist: an extension of\nmnist to handwritten letters. arXiv preprint arXiv:1702.05373, 2017. 13\nDavid L Davies and Donald W Bouldin. A cluster separation measure. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 1979. 14\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2009. 6\nNanqing Dong and Eric P Xing. Domain adaption in one-shot learning. In Joint European Con-\nference on Machine Learning and Knowledge Discovery in Databases. Springer, 2018. 3, 12,\n13\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation\nof deep networks. In Proceedings of the International Conference on Machine Learning (ICML),\n2017. 1, 2, 5, 7, 12\nChelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In Ad-\nvances in Neural Information Processing Systems (NIPS), 2018. 2\nYaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In\nProceedings of the International Conference on Machine Learning (ICML), 2015. 3\nVictor Garcia and Joan Bruna. Few-shot learning with graph neural networks. In Proceedings of the\nInternational Conference on Learning Representations (ICLR), 2018. 1, 3\nSpyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n1, 2, 3, 4\nBharath Hariharan and Ross Girshick. Low-shot visual recognition by shrinking and hallucinating\nfeatures. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017.\n1, 3\n10\n\n\nPublished as a conference paper at ICLR 2019\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2016. 8\nNathan Hilliard, Lawrence Phillips, Scott Howland, Art¨em Yankov, Courtney D Corley, and\nNathan O Hodas. Few-shot learning with metric-agnostic conditional embeddings. arXiv preprint\narXiv:1802.04376, 2018. 6\nYen-Chang Hsu, Zhaoyang Lv, and Zsolt Kira. Learning to cluster in order to transfer across do-\nmains and tasks. 2018. 3, 12\nJunlin Hu, Jiwen Lu, and Yap-Peng Tan. Deep transfer metric learning. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), 2015. 4\nGregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot\nimage recognition. In Proceedings of the International Conference on Machine Learning Work-\nshops (ICML Workshops), 2015. 2\nBrenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. One shot learning of\nsimple visual concepts. In Cogsci, 2011. 13\nThomas Mensink, Jakob Verbeek, Florent Perronnin, and Gabriela Csurka. Metric learning for large\nscale image classiﬁcation: Generalizing to new classes at near-zero cost. In Proceedings of the\nEuropean Conference on Computer Vision (ECCV). Springer, 2012. 4\nGeorge A Miller. Wordnet: a lexical database for english. Communications of the ACM, 1995. 8\nSaeid Motiian, Quinn Jones, Seyed Iranmanesh, and Gianfranco Doretto.\nFew-shot adversarial\ndomain adaptation. In Advances in Neural Information Processing Systems (NIPS), 2017. 12\nTsendsuren Munkhdalai and Hong Yu. Meta networks. In Proceedings of the International Confer-\nence on Machine Learning (ICML), 2017. 2\nAlex Nichol and John Schulman.\nReptile: a scalable metalearning algorithm.\narXiv preprint\narXiv:1803.02999, 2018. 2\nSinno Jialin Pan, Qiang Yang, et al. A survey on transfer learning. IEEE Transactions on Knowledge\nand Data Engineering (TKDE), 2010. 3\nHang Qi, Matthew Brown, and David G Lowe. Low-shot learning with imprinted weights. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n1, 2, 3, 4\nSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In Proceedings\nof the International Conference on Learning Representations (ICLR), 2017. 1, 2, 6, 12, 13\nAndrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero,\nand Raia Hadsell. Meta-learning with latent embedding optimization. In Proceedings of the\nInternational Conference on Learning Representations (ICLR), 2019. 2\nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In\nAdvances in Neural Information Processing Systems (NIPS), 2017. 1, 3, 4, 5, 6, 7, 12, 13, 16\nFlood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.\nLearning to compare: Relation network for few-shot learning. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition (CVPR), 2018. 1, 3, 5, 7\nOriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one\nshot learning. In Advances in Neural Information Processing Systems (NIPS), 2016. 1, 3, 4, 5, 6,\n7, 8, 12, 13\nCatherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd\nbirds-200-2011 dataset. 2011. 6\nYu-Xiong Wang, Ross Girshick, Martial Hebert, and Bharath Hariharan. Low-shot learning from\nimaginary data. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2018. 1, 3, 12\n11\n\n\nPublished as a conference paper at ICLR 2019\nAPPENDIX\nA1\nRELATIONSHIP BETWEEN DOMAIN ADAPTATION AND FEW-SHOT CLASSIFICATION\nAs mentioned in Section 2, here we discuss the relationship between domain adaptation and few-\nshot classiﬁcation to clarify different experimental settings. As shown in Table A1, in general,\ndomain adaptation aims at adapting source dataset knowledge to the same class in target dataset.\nOn the other hand, the goal of few-shot classiﬁcation is to learn from base classes to classify novel\nclasses in the same dataset.\nSeveral recent work tackle the problem at the intersection of the two ﬁelds of study. For example,\ncross-task domain adaptation Hsu et al. (2018) also discuss novel classes in the target dataset. In\ncontrast, while Motiian et al. (2017) has “few-shot” in the title, their evaluation setting focuses on\nclassifying the same class in the target dataset.\nIf base and novel classes are both drawn from the same dataset, minor domain shift exists between\nthe base and novel classes, as we demonstrated in Section 4.4. To highlight the impact of domain\nshift, we further propose the mini-ImageNet →CUB setting. The domain shift in few-shot classiﬁ-\ncation is also discussed in Dong & Xing (2018).\nTable A1: Relationship between domain adaptation and few-shot classiﬁcation. The two\nﬁeld-of-studies have overlapping in the development. Notation ”*” indicates minor domain shifts\nexist between base and novel classes.\nDomain shift\nSource to target dataset\nBase to novel class\nDomain adaptation\nMotiian et al. (2017)\nV\nV\n-\nCross-task domain adaptation\nHsu et al. (2018)\nV\nV\nV\nFew-shot classiﬁcation\nOurs (CUB, mini-ImageNet )\n*\n-\nV\nCross-domain few-shot\nOurs (mini-ImageNet →CUB)\nDong & Xing (2018)\nV\nV\nV\nA2\nTERMINOLOGY DIFFERENCE\nDifferent meta-learning works use different terminology in their works. We highlight their differ-\nences in appendix Table A2 to clarify the inconsistency.\nTable A2: Different terminology used in other works. Notation ”-” indicates the term is the same\nas in this paper.\nOur terms\nMatchingNet\nVinyals et al.\nProtoNet\nSnell et al.\nMAML\nFinn et al.\nMeta-learn LSTM\nRavi & Larochelle\nImaginary\nWang et al.\nmeta-training stage\ntraining\ntraining\n-\n-\n-\nmeta-testing stage\ntest\ntest\n-\n-\n-\nbase class\ntraining set\ntraining set\ntask\nmeta-training set\n-\nnovel class\ntest set\ntest set\nnew task\nmeta-testing set\n-\nsupport set\n-\n-\nsample\ntraining dataset\ntraining data\nquery set\nbatch\n-\ntest time sample\ntest dataset\ntest data\n12\n\n\nPublished as a conference paper at ICLR 2019\nA3\nADDITIONAL RESULTS ON OMNIGLOT AND OMNIGLOT→EMNIST\nFor completeness, here we also show the results under two additional scenarios in 4) character\nrecognition 5) cross-domain character recognition.\nFor character recognition, we use the Omniglot dataset Lake et al. (2011) commonly used in eval-\nuating few-shot classiﬁcation algorithms. Omniglot contains 1,623 characters from 50 languages,\nand we follow the evaluation protocol of Vinyals et al. (2016) to ﬁrst augment the classes by rota-\ntions in 90, 180, 270 degrees, resulting in 6492 classes. We then follow Snell et al. (2017) to split\nthese classes into 4112 base, 688 validation, and 1692 novel classes. Unlike Snell et al. (2017), our\nvalidation classes are only used to monitor the performance during meta-training.\nFor cross-domain character recognition (Omniglot→EMNIST), we follow the setting of Dong &\nXing (2018) to use Omniglot without Latin characters and without rotation augmentation as base\nclasses, so there are 1597 base classes. On the other hand, EMNIST dataset Cohen et al. (2017)\ncontains 10-digits and upper and lower case alphabets in English, so there are 62 classes in total. We\nsplit these classes into 31 validation and 31 novel classes, and invert the white-on-black characters\nto black-on-white as in Omniglot.\nWe use a Conv-4 backbone with input size 28x28 for both settings. As Omniglot characters are\nblack-and-white, center-aligned and rotation sensitive, we do not use data augmentation in this ex-\nperiment. To reduce the risk of over-ﬁtting, we use the validation set to select the epoch or episode\nwith the best accuracy for all methods, including baseline and baseline++.4\nAs shown in Table A3, in both Omniglot and Omniglot→EMNIST settings, meta-learning methods\noutperform baseline and baseline++ in 1-shot. However, all methods reach comparable performance\nin the 5-shot classiﬁcation setting. We attribute this to the lack of data augmentation for the baseline\nand baseline++ methods as they tend to over-ﬁt base classes. When sufﬁcient examples in novel\nclasses are available, the negative impact of over-ﬁtting is reduced.\nTable A3: Few-shot classiﬁcation results for both the Omniglot and Omniglot→EMNIST. All\nexperiments are from 5-way classiﬁcation with a Conv-4 backbone and without data augmentation.\nOmniglot\nOmniglot→EMNIST\nMethod\n1-shot\n5-shot\n1-shot\n5-shot\nBaseline\n94.89 ± 0.45\n99.12 ± 0.13\n63.94 ± 0.87\n86.00 ± 0.59\nBaseline++\n95.41 ± 0.39\n99.38 ± 0.10\n64.74 ± 0.82\n87.31 ± 0.58\nMatchingNet\n97.78 ± 0.30\n99.37 ± 0.11\n72.71 ± 0.79\n87.60 ± 0.56\nProtoNet\n98.01 ± 0.30\n99.15 ± 0.12\n70.43 ± 0.80\n87.04 ± 0.55\nMAML\n98.57 ± 0.19\n99.53 ± 0.08\n72.04 ± 0.83\n88.24 ± 0.56\nRelationNet\n97.22 ± 0.33\n99.30 ± 0.10\n75.55 ± 0.87\n88.94 ± 0.54\nA4\nBASELINE WITH 1-NN CLASSIFIER\nSome prior work (Vinyals et al. (2016)) apply a Baseline with 1-NN classiﬁer in the test stage. We\ninclude our result as in Table A4. The result shows that using 1-NN classiﬁer has better performance\nthan that of using the softmax classiﬁer in 1-shot setting, but softmax classiﬁer performs better in\n5-shot setting. We note that the number here are not directly comparable to results in Vinyals et al.\n(2016) because we use a different mini-ImageNet as in Ravi & Larochelle (2017).\nTable A4: Baseline with softmax and 1-NN classiﬁer in test stage. We note that we use cosine\ndistance in 1-NN.\n1-shot\n5-shot\nsoftmax\n1-NN\nsoftmax\n1-NN\nBaseline\n42.11±0.71\n44.18±0.69\n62.53±0.69\n56.68±0.67\nBaseline++\n48.24±0.75\n49.57±0.73\n66.43±0.63\n61.93±0.65\n4The exact epoch of baseline and baseline++ on Omniglot and Omniglot→EMNIST is 5 epochs\n13\n\n\nPublished as a conference paper at ICLR 2019\nA5\nMAML AND MAML WITH FIRST-ORDER APPROXIMATION\nAs discussed in Section 4.1, we use ﬁrst-order approximation MAML to improve memory efﬁciency\nin all of our experiments. To demonstrate this design choice does not affect the accuracy, we compare\ntheir validation accuracy trends on Omniglot with 5-shot as in Figure A1. We observe that while the\nfull version MAML converge faster, both versions reach similar accuracy in the end.\nThis phenomena is consistent with the difference of ﬁrst-order (e.g. gradient descent) and second-\norder methods (e.g. Newton) in convex optimization problems. Second-order methods converge\nfaster at the cost of memory, but they both converge to similar objective value.\nFigure A1: Validation accuracy trends of MAML and MAML with ﬁrst order approximation.\nBoth versions converge to the same validation accuracy. The experimental results are on Omniglot\nwith 5-shot with a Conv-4 backbone.\nA6\nINTRA-CLASS VARIATION AND BACKBONE DEPTH\nAs mentioned in Section 4.3, here we demonstrate decreased intra-class variation as the network\ndepth gets deeper as in Figure A2. We use the Davies-Bouldin index Davies & Bouldin (1979) to\nmeasure intra-class variation. The Davies-Bouldin index is a metric to evaluate the tightness in a\ncluster (or class, in our case). Our results show that both intra-class variation in the base and novel\nclass feature decrease using deeper backbones.\n2\n3\n4\n5\n6\n7\n8\nConv-4\nConv-6\nResnet-10\nResnet-18\nResnet-34\n3\n4\n5\n6\n7\nConv-4\nConv-6\nResnet-10\nResnet-18\nResnet-34\nBaseline\nBaseline++\nMatchingNet\nProtoNet\nBase class feature\nNovel class feature\nDavies-Bouldin index\n(Intra-class variation)\nFigure A2: Intra-class variation decreases as backbone gets deeper. Here we use\nDavies-Bouldin index to represent intra-class variation, which is a metric to evaluate the tightness\nin a cluster (or class, in our case). The statistics are Davies-Bouldin index for all base and novel\nclass feature (extracted by feature extractor learned after training or meta-training stage) for CUB\ndataset under different backbone.\n14\n\n\nPublished as a conference paper at ICLR 2019\nA7\nDETAILED STATISTICS IN EFFECTS OF INCREASING BACKBONE DEPTH\nHere we show a high-resolution version of Figure 3 in Figure A3 and show detailed statistics in\nTable A5 for easier comparison.\n45% \n55% \n65% \n75% \n40% \n45% \n50% \n55% \n60% \n70% \n80% \n90% \n60% \n65% \n70% \n75% \n80% \nBaseline\nBaseline++\nMatchingNet\nProtoNet\nMAML\nRelationNet\nCUB\nmini-ImageNet\n1-shot\n5-shot\nFigure A3: Few-shot classiﬁcation accuracy vs. backbone depth. In the CUB dataset, gaps\namong different methods diminish as the backbone gets deeper. In mini-ImageNet 5-shot, some\nmeta-learning methods are even beaten by Baseline with a deeper backbone.\nTable A5: Detailed statistics in Figure 3. We put exact value here for reference.\nConv-4\nConv-6\nResnet-10\nResnet-18\nResnet-34\nCUB\n1-shot\nBaseline\n47.12±0.74\n55.77±0.86\n63.34±0.91\n65.51±0.87\n67.96±0.89\nBaseline++\n60.53±0.83\n66.00±0.89\n69.55±0.89\n67.02±0.90\n68.00±0.83\nMatchingNet\n61.16±0.89\n67.16±0.97\n71.29±0.90\n72.36±0.90\n71.44±0.96\nProtoNet\n51.31±0.91\n66.07±0.97\n70.13±0.94\n71.88±0.91\n72.03±0.91\nMAML\n55.92±0.95\n65.91±0.97\n71.29±0.95\n69.96±1.01\n67.28±1.08\nRelationNet\n62.45±0.98\n63.11±0.94\n68.65±0.91\n67.59±1.02\n66.20±0.99\nCUB\n5-shot\nBaseline\n64.16±0.71\n73.07±0.71\n81.27±0.57\n82.85±0.55\n84.27±0.53\nBaseline++\n79.34±0.61\n82.02±0.55\n85.17±0.50\n83.58±0.54\n84.50±0.51\nMatchingNet\n72.86±0.70\n77.08±0.66\n83.59±0.58\n83.64±0.60\n83.78±0.56\nProtoNet\n70.77±0.69\n78.14±0.67\n84.76±0.52\n87.42±0.48\n85.98±0.53\nMAML\n72.09±0.76\n76.31±0.74\n80.33±0.70\n82.70±0.65\n83.47±0.59\nRelationNet\n76.11±0.69\n77.81±0.66\n81.12±0.63\n82.75±0.58\n82.30±0.58\nmini-ImageNet\n1-shot\nBaseline\n42.11±0.71\n45.82±0.74\n52.37±0.79\n51.75±0.80\n49.82±0.73\nBaseline++\n48.24±0.75\n48.29±0.72\n53.97±0.79\n51.87±0.77\n52.65±0.83\nMatchingNet\n48.14±0.78\n50.47±0.86\n54.49±0.81\n52.91±0.88\n53.20±0.78\nProtoNet\n44.42±0.84\n50.37±0.83\n51.98±0.84\n54.16±0.82\n53.90±0.83\nMAML\n46.47±0.82\n50.96±0.92\n54.69±0.89\n49.61±0.92\n51.46±0.90\nRelationNet\n49.31±0.85\n51.84±0.88\n52.19±0.83\n52.48±0.86\n51.74±0.83\nmini-ImageNet\n5-shot\nBaseline\n62.53±0.69\n66.42±0.67\n74.69±0.64\n74.27±0.63\n73.45±0.65\nBaseline++\n66.43±0.63\n68.09±0.69\n75.90±0.61\n75.68±0.63\n76.16±0.63\nMatchingNet\n63.48±0.66\n63.19±0.70\n68.82±0.65\n68.88±0.69\n68.32±0.66\nProtoNet\n64.24±0.72\n67.33±0.67\n72.64±0.64\n73.68±0.65\n74.65±0.64\nMAML\n62.71±0.71\n66.09±0.71\n66.62±0.83\n65.72±0.77\n65.90±0.79\nRelationNet\n66.60±0.69\n64.55±0.70\n70.20±0.66\n69.83±0.68\n69.61±0.67\n15\n\n\nPublished as a conference paper at ICLR 2019\nA8\nMORE-WAY IN META-TESTING STAGE\nWe experiment with a practical setting that handles different testing scenarios. Speciﬁcally, we\nconduct the experiments of 5-way meta-training and N-way meta-testing (where N = 5, 10, 20) to\nexamine the effect of testing scenarios that are different from training.\nAs in Table A6, we compare the methods Baseline, Baseline++, MatchingNet, ProtoNet, and Re-\nlationNet. Note that we are unable to apply the MAML method as MAML learns the initialization\nfor the classiﬁer and can thus only be updated to classify the same number of classes. Our results\nshow that for classiﬁcation with a larger N-way in the meta-testing stage, the proposed Baseline++\ncompares favorably against other methods in both shallow or deeper backbone settings.\nWe attribute the results to two reasons. First, to perform well in a larger N-way classiﬁcation setting,\none needs to further reduce the intra-class variation to avoid misclassiﬁcation. Thus, Baseline++ has\nbetter performance than Baseline in both backbone settings. Second, as meta-learning algorithms\nwere trained to perform 5-way classiﬁcation in the meta-training stage, the performance of these\nalgorithms may drop signiﬁcantly when increasing the N-way in the meta-testing stage because the\ntasks of 10-way or 20-way classiﬁcation are harder than that of 5-way one.\nOne may address this issue by performing a larger N-way classiﬁcation in the meta-training stage\n(as suggested in Snell et al. (2017)). However, it may encounter the issue of memory constraint.\nFor example, to perform a 20-way classiﬁcation with 5 support images and 15 query images in each\nclass, we need to ﬁt a batch size of 400 (20 x (5 + 15)) that must ﬁt into the GPUs. Without special\nhardware parallelization, the large batch size may prevent us from training models with deeper\nbackbones such as ResNet.\nTable A6: 5-way meta-training and N-way meta-testing experiment. The experimental results\nare on mini-ImageNet with 5-shot. We could see Baseline++ compares favorably against other\nmethods in both shallow or deeper backbone settings.\nConv-4\nResNet-18\nN-way test\n5-way\n10-way\n20-way\n5-way\n10-way\n20-way\nBaseline\n62.53±0.69\n46.44±0.41\n32.27±0.24\n74.27±0.63\n55.00±0.46\n42.03±0.25\nBaseline++\n66.43±0.63\n52.26±0.40\n38.03±0.24\n75.68±0.63\n63.40±0.44\n50.85±0.25\nMatchingNet\n63.48±0.66\n47.61±0.44\n33.97±0.24\n68.88±0.69\n52.27±0.46\n36.78±0.25\nProtoNet\n64.24±0.68\n48.77±0.45\n34.58±0.23\n73.68±0.65\n59.22±0.44\n44.96±0.26\nRelationNet\n66.60±0.69\n47.77±0.43\n33.72±0.22\n69.83±0.68\n53.88±0.48\n39.17±0.25\n16\n"
}