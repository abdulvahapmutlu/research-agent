{
  "filename": "1605.07146v4.pdf",
  "num_pages": 15,
  "pages": [
    "SERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\n1\nWide Residual Networks\nSergey Zagoruyko\nsergey.zagoruyko@enpc.fr\nNikos Komodakis\nnikos.komodakis@enpc.fr\nUniversité Paris-Est, École des Ponts\nParisTech\nParis, France\nAbstract\nDeep residual networks were shown to be able to scale up to thousands of layers\nand still have improving performance. However, each fraction of a percent of improved\naccuracy costs nearly doubling the number of layers, and so training very deep resid-\nual networks has a problem of diminishing feature reuse, which makes these networks\nvery slow to train. To tackle these problems, in this paper we conduct a detailed exper-\nimental study on the architecture of ResNet blocks, based on which we propose a novel\narchitecture where we decrease depth and increase width of residual networks. We call\nthe resulting network structures wide residual networks (WRNs) and show that these are\nfar superior over their commonly used thin and very deep counterparts. For example,\nwe demonstrate that even a simple 16-layer-deep wide residual network outperforms in\naccuracy and efﬁciency all previous deep residual networks, including thousand-layer-\ndeep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and\nsigniﬁcant improvements on ImageNet. Our code and models are available at https:\n//github.com/szagoruyko/wide-residual-networks.\n1\nIntroduction\nConvolutional neural networks have seen a gradual increase of the number of layers in the\nlast few years, starting from AlexNet [16], VGG [26], Inception [30] to Residual [11] net-\nworks, corresponding to improvements in many image recognition tasks. The superiority\nof deep networks has been spotted in several works in the recent years [3, 22]. However,\ntraining deep neural networks has several difﬁculties, including exploding/vanishing gradi-\nents and degradation. Various techniques were suggested to enable training of deeper neural\nnetworks, such as well-designed initialization strategies [1, 12], better optimizers [29], skip\nconnections [19, 23], knowledge transfer [4, 24] and layer-wise training [25].\nThe latest residual networks [11] had a large success winning ImageNet and COCO 2015\ncompetition and achieving state-of-the-art in several benchmarks, including object classiﬁ-\ncation on ImageNet and CIFAR, object detection and segmentation on PASCAL VOC and\nMS COCO. Compared to Inception architectures they show better generalization, meaning\nthe features can be utilized in transfer learning with better efﬁciency. Also, follow-up work\nshowed that residual links speed up convergence of deep networks [31]. Recent follow-up\nwork explored the order of activations in residual networks, presenting identity mappings\nin residual blocks [13] and improving training of very deep networks. Successful training\nof very deep networks was also shown to be possible through the use of highway networks\nc⃝2016. The copyright of this document resides with its authors.\nIt may be distributed unchanged freely in print or electronic forms.\narXiv:1605.07146v4  [cs.CV]  14 Jun 2017\n",
    "2\nSERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\nconv3x3\nconv3x3\nxl\nxl+1\n(a) basic\nconv1x1\nconv3x3\nconv1x1\nxl\nxl+1\n(b) bottleneck\nconv3x3\nconv3x3\nxl\nxl+1\n(c) basic-wide\ndropout\nxl\nxl+1\nconv3x3\nconv3x3\n(d) wide-dropout\nFigure 1: Various residual blocks used in the paper. Batch normalization and ReLU precede\neach convolution (omitted for clarity)\n[28], which is an architecture that had been proposed prior to residual networks. The essen-\ntial difference between residual and highway networks is that in the latter residual links are\ngated and weights of these gates are learned.\nTherefore, up to this point, the study of residual networks has focused mainly on the\norder of activations inside a ResNet block and the depth of residual networks. In this work\nwe attempt to conduct an experimental study that goes beyond the above points. By doing\nso, our goal is to explore a much richer set of network architectures of ResNet blocks and\nthoroughly examine how several other different aspects besides the order of activations affect\nperformance. As we explain below, such an exploration of architectures has led to new\ninteresting ﬁndings with great practical importance concerning residual networks.\nWidth vs depth in residual networks. The problem of shallow vs deep networks has\nbeen in discussion for a long time in machine learning [2, 18] with pointers to the circuit\ncomplexity theory literature showing that shallow circuits can require exponentially more\ncomponents than deeper circuits. The authors of residual networks tried to make them as thin\nas possible in favor of increasing their depth and having less parameters, and even introduced\na «bottleneck» block which makes ResNet blocks even thinner.\nWe note, however, that the residual block with identity mapping that allows to train\nvery deep networks is at the same time a weakness of residual networks. As gradient ﬂows\nthrough the network there is nothing to force it to go through residual block weights and it\ncan avoid learning anything during training, so it is possible that there is either only a few\nblocks that learn useful representations, or many blocks share very little information with\nsmall contribution to the ﬁnal goal. This problem was formulated as diminishing feature\nreuse in [28]. The authors of [14] tried to address this problem with the idea of randomly\ndisabling residual blocks during training. This method can be viewed as a special case of\ndropout [27], where each residual block has an identity scalar weight on which dropout is\napplied. The effectiveness of this approach proves the hypothesis above.\nMotivated by the above observation, our work builds on top of [13] and tries to answer\nthe question of how wide deep residual networks should be and address the problem of train-\ning. In this context, we show that the widening of ResNet blocks (if done properly) provides\na much more effective way of improving performance of residual networks compared to in-\ncreasing their depth. In particular, we present wider deep residual networks that signiﬁcantly\nimprove over [13], having 50 times less layers and being more than 2 times faster. We call\nthe resulting network architectures wide residual networks. For instance, our wide 16-layer\ndeep network has the same accuracy as a 1000-layer thin deep network and a comparable\nnumber of parameters, although being several times faster to train. This type of experiments\n",
    "SERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\n3\nthus seem to indicate that the main power of deep residual networks is in residual blocks, and\nthat the effect of depth is supplementary. We note that one can train even better wide resid-\nual networks that have twice as many parameters (and more), which suggests that to further\nimprove performance by increasing depth of thin networks one needs to add thousands of\nlayers in this case.\nUse of dropout in ResNet blocks. Dropout was ﬁrst introduced in [27] and then was\nadopted by many successful architectures as [16, 26] etc. It was mostly applied on top layers\nthat had a large number of parameters to prevent feature coadaptation and overﬁtting. It was\nthen mainly substituted by batch normalization [15] which was introduced as a technique to\nreduce internal covariate shift in neural network activations by normalizing them to have spe-\nciﬁc distribution. It also works as a regularizer and the authors experimentally showed that a\nnetwork with batch normalization achieves better accuracy than a network with dropout. In\nour case, as widening of residual blocks results in an increase of the number of parameters,\nwe studied the effect of dropout to regularize training and prevent overﬁtting. Previously,\ndropout in residual networks was studied in [13] with dropout being inserted in the identity\npart of the block, and the authors showed negative effects of that. Instead, we argue here\nthat dropout should be inserted between convolutional layers. Experimental results on wide\nresidual networks show that this leads to consistent gains, yielding even new state-of-the-\nart results (e.g., 16-layer-deep wide residual network with dropout achieves 1.64% error on\nSVHN).\nIn summary, the contributions of this work are as follows:\n• We present a detailed experimental study of residual network architectures that thor-\noughly examines several important aspects of ResNet block structure.\n• We propose a novel widened architecture for ResNet blocks that allows for residual\nnetworks with signiﬁcantly improved performance.\n• We propose a new way of utilizing dropout within deep residual networks so as to\nproperly regularize them and prevent overﬁtting during training.\n• Last, we show that our proposed ResNet architectures achieve state-of-the-art results\non several datasets dramatically improving accuracy and speed of residual networks.\n2\nWide residual networks\nResidual block with identity mapping can be represented by the following formula:\nxl+1 = xl +F(xl,Wl)\n(1)\nwhere xl+1 and xl are input and output of the l-th unit in the network, F is a residual func-\ntion and Wl are parameters of the block. Residual network consists of sequentially stacked\nresidual blocks.\nIn [13] residual networks consisted of two type of blocks:\n• basic - with two consecutive 3 × 3 convolutions with batch normalization and ReLU\npreceding convolution: conv3×3-conv3×3 Fig.1(a)\n• bottleneck - with one 3 × 3 convolution surrounded by dimensionality reducing and\nexpanding 1×1 convolution layers: conv1×1-conv3×3-conv1×1 Fig.1(b)\n",
    "4\nSERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\ngroup name\noutput size\nblock type = B(3,3)\nconv1\n32×32\n[3×3, 16]\nconv2\n32×32\n\u0014\n3×3, 16×k\n3×3, 16×k\n\u0015\n×N\nconv3\n16×16\n\u0014\n3×3, 32×k\n3×3, 32×k\n\u0015\n×N\nconv4\n8×8\n\u0014\n3×3, 64×k\n3×3, 64×k\n\u0015\n×N\navg-pool\n1×1\n[8×8]\nTable 1: Structure of wide residual networks. Network width is determined by factor k.\nOriginal architecture [13] is equivalent to k = 1. Groups of convolutions are shown in brack-\nets where N is a number of blocks in group, downsampling performed by the ﬁrst layers\nin groups conv3 and conv4. Final classiﬁcation layer is omitted for clearance. In the\nparticular example shown, the network uses a ResNet block of type B(3,3).\nCompared to the original architecture [11] in [13] the order of batch normalization, ac-\ntivation and convolution in residual block was changed from conv-BN-ReLU to BN-ReLU-\nconv. As the latter was shown to train faster and achieve better results we don’t consider\nthe original version. Furthermore, so-called «bottleneck» blocks were initially used to make\nblocks less computationally expensive to increase the number of layers. As we want to study\nthe effect of widening and «bottleneck» is used to make networks thinner we don’t consider\nit too, focusing instead on «basic» residual architecture.\nThere are essentially three simple ways to increase representational power of residual\nblocks:\n• to add more convolutional layers per block\n• to widen the convolutional layers by adding more feature planes\n• to increase ﬁlter sizes in convolutional layers\nAs small ﬁlters were shown to be very effective in several works including [26, 31] we do\nnot consider using ﬁlters larger than 3×3. Let us also introduce two factors, deepening factor\nl and widening factor k, where l is the number of convolutions in a block and k multiplies\nthe number of features in convolutional layers, thus the baseline «basic» block corresponds\nto l = 2, k = 1. Figures 1(a) and 1(c) show schematic examples of «basic» and «basic-wide»\nblocks respectively.\nThe general structure of our residual networks is illustrated in table 1: it consists of an\ninitial convolutional layer conv1 that is followed by 3 groups (each of size N) of residual\nblocks conv2, conv3 and conv4, followed by average pooling and ﬁnal classiﬁcation\nlayer. The size of conv1 is ﬁxed in all of our experiments, while the introduced widen-\ning factor k scales the width of the residual blocks in the three groups conv2-4 (e.g., the\noriginal «basic» architecture is equivalent to k = 1). We want to study the effect of represen-\ntational power of residual block and, to that end, we perform and test several modiﬁcations\nto the «basic» architecture, which are detailed in the following subsections.\n",
    "SERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\n5\n2.1\nType of convolutions in residual block\nLet B(M) denote residual block structure, where M is a list with the kernel sizes of the\nconvolutional layers in a block. For example, B(3,1) denotes a residual block with 3×3 and\n1 × 1 convolutional layers (we always assume square spatial kernels). Note that, as we do\nnot consider «bottleneck» blocks as explained earlier, the number of feature planes is always\nkept the same across the block. We would like to answer the question of how important each\nof the 3 × 3 convolutional layers of the «basic» residual architecture is and if they can be\nsubstituted by a less computationally expensive 1 × 1 layer or even a combination of 1 × 1\nand 3 × 3 convolutional layers, e.g., B(1,3) or B(1,3). This can increase or decrease the\nrepresentational power of the block. We thus experiment with the following combinations\n(note that the last combination, i.e., B(3,1,1) is similar to effective Network-in-Network\n[20] architecture):\n1. B(3,3) - original «basic» block\n2. B(3,1,3) - with one extra 1×1 layer\n3. B(1,3,1) - with the same dimensionality of all convolutions, «straightened» bottleneck\n4. B(1,3) - the network has alternating 1×1 - 3×3 convolutions everywhere\n5. B(3,1) - similar idea to the previous block\n6. B(3,1,1) - Network-in-Network style block\n2.2\nNumber of convolutional layers per residual block\nWe also experiment with the block deepening factor l to see how it affects performance. The\ncomparison has to be done among networks with the same number of parameters, so in this\ncase we need to build networks with different l and d (where d denotes the total number of\nblocks) while ensuring that network complexity is kept roughly constant. This means, for\ninstance, that d should decrease whenever l increases.\n2.3\nWidth of residual blocks\nIn addition to the above modiﬁcations, we experiment with the widening factor k of a block.\nWhile the number of parameters increases linearly with l (the deepening factor) and d\n(the number of ResNet blocks), number of parameters and computational complexity are\nquadratic in k. However, it is more computationally effective to widen the layers than have\nthousands of small kernels as GPU is much more efﬁcient in parallel computations on large\ntensors, so we are interested in an optimal d to k ratio.\nOne argument for wider residual networks would be that almost all architectures before\nresidual networks, including the most successful Inception [30] and VGG [26], were much\nwider compared to [13]. For example, residual networks WRN-22-8 and WRN-16-10 (see\nnext paragraph for explanation of this notation) are very similar in width, depth and number\nof parameters to VGG architectures.\nWe further refer to original residual networks with k = 1 as «thin» and to networks with\nk > 1 as «wide». In the rest of the paper we use the following notation: WRN-n-k denotes\na residual network that has a total number of convolutional layers n and a widening factor k\n(for example, network with 40 layers and k = 2 times wider than original would be denoted\nas WRN-40-2). Also, when applicable we append block type, e.g. WRN-40-2-B(3,3).\n",
    "6\nSERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\nblock type\ndepth\n# params\ntime,s\nCIFAR-10\nB(1,3,1)\n40\n1.4M\n85.8\n6.06\nB(3,1)\n40\n1.2M\n67.5\n5.78\nB(1,3)\n40\n1.3M\n72.2\n6.42\nB(3,1,1)\n40\n1.3M\n82.2\n5.86\nB(3,3)\n28\n1.5M\n67.5\n5.73\nB(3,1,3)\n22\n1.1M\n59.9\n5.78\nTable 2: Test error (%, median over 5 runs) on CIFAR-10\nof residual networks with k = 2 and different block types.\nTime column measures one training epoch.\nl\nCIFAR-10\n1\n6.69\n2\n5.43\n3\n5.65\n4\n5.93\nTable 3: Test error (%, me-\ndian over 5 runs) on CIFAR-\n10 of WRN-40-2 (2.2M)\nwith various l.\n2.4\nDropout in residual blocks\nAs widening increases the number of parameters we would like to study ways of regular-\nization. Residual networks already have batch normalization that provides a regularization\neffect, however it requires heavy data augmentation, which we would like to avoid, and it’s\nnot always possible. We add a dropout layer into each residual block between convolutions\nas shown in ﬁg. 1(d) and after ReLU to perturb batch normalization in the next residual\nblock and prevent it from overﬁtting. In very deep residual networks that should help deal\nwith diminishing feature reuse problem enforcing learning in different residual blocks.\n3\nExperimental results\nFor experiments we chose well-known CIFAR-10, CIFAR-100, SVHN and ImageNet image\nclassiﬁcation datasets. CIFAR-10 and CIFAR-100 datasets [17] consist of 32 × 32 color\nimages drawn from 10 and 100 classes split into 50,000 train and 10,000 test images. For data\naugmentation we do horizontal ﬂips and take random crops from image padded by 4 pixels on\neach side, ﬁlling missing pixels with reﬂections of original image. We don’t use heavy data\naugmentation as proposed in [9]. SVHN is a dataset of Google’s Street View House Numbers\nimages and contains about 600,000 digit images, coming from a signiﬁcantly harder real\nworld problem. For experiments on SVHN we don’t do any image preprocessing, except\ndividing images by 255 to provide them in [0,1] range as input. All of our experiments\nexcept ImageNet are based on [13] architecture with pre-activation residual blocks and we\nuse it as baseline. For ImageNet, we ﬁnd that using pre-activation in networks with less\nthan 100 layers does not make any signiﬁcant difference and so we decide to use the original\nResNet architecture in this case. Unless mentioned otherwise, for CIFAR we follow the\nimage preprocessing of [8] with ZCA whitening. However, for some CIFAR experiments\nwe instead use simple mean/std normalization such that we can directly compare with [13]\nand other ResNet related works that make use of this type of preprocessing.\nIn the following we describe our ﬁndings w.r.t. the different ResNet block architectures\nand also analyze the performance of our proposed wide residual networks. We note that for\nall experiments related to «type of convolutions in a block» and «number of convolutions\nper block» we use k = 2 and reduced depth compared to [13] in order to speed up training.\n",
    "SERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\n7\nType of convolutions in a block\nWe start by reporting results using trained networks with different block types B (reported\nresults are on CIFAR-10). We used WRN-40-2 for blocks B(1,3,1), B(3,1), B(1,3) and\nB(3,1,1) as these blocks have only one 3×3 convolution. To keep the number of parameters\ncomparable we trained other networks with less layers: WRN-28-2-B(3,3) and WRN-22-2-\nB(3,1,3). We provide the results including test accuracy in median over 5 runs and time per\ntraining epoch in the table 2. Block B(3,3) turned out to be the best by a little margin, and\nB(3,1) with B(3,1,3) are very close to B(3,3) in accuracy having less parameters and less\nlayers. B(3,1,3) is faster than others by a small margin.\nBased on the above, blocks with comparable number of parameters turned out to give\nmore or less the same results. Due to this fact, we hereafter restrict our attention to only\nWRNs with 3×3 convolutions so as to be also consistent with other methods.\nNumber of convolutions per block\nWe next proceed with the experiments related to varying the deepening factor l (which rep-\nresents the number of convolutional layers per block). We show indicative results in table 3,\nwhere in this case we took WRN-40-2 with 3×3 convolutions and trained several networks\nwith different deepening factor l ∈[1,2,3,4], same number of parameters (2.2×106) and\nsame number of convolutional layers.\nAs can be noticed, B(3,3) turned out to be the best, whereas B(3,3,3) and B(3,3,3,3)\nhad the worst performance. We speculate that this is probably due to the increased difﬁculty\nin optimization as a result of the decreased number of residual connections in the last two\ncases. Furthermore, B(3) turned out to be quite worse. The conclusion is that B(3,3) is\noptimal in terms of number of convolutions per block. For this reason, in the remaining\nexperiments we only consider wide residual networks with a block of type B(3,3).\nWidth of residual blocks\nAs we try to increase widening parameter k we have to decrease total number of layers. To\nﬁnd an optimal ratio we experimented with k from 2 to 12 and depth from 16 to 40. The\nresults are presented in table 4. As can be seen, all networks with 40, 22 and 16 layers see\nconsistent gains when width is increased by 1 to 12 times. On the other hand, when keeping\nthe same ﬁxed widening factor k = 8 or k = 10 and varying depth from 16 to 28 there is a\nconsistent improvement, however when we further increase depth to 40 accuracy decreases\n(e.g., WRN-40-8 loses in accuracy to WRN-22-8).\nWe show additional results in table 5 where we compare thin and wide residual networks.\nAs can be observed, wide WRN-40-4 compares favorably to thin ResNet-1001 as it achieves\nbetter accuracy on both CIFAR-10 and CIFAR-100. Yet, it is interesting that these networks\nhave comparable number of parameters, 8.9×106 and 10.2×106, suggesting that depth does\nnot add regularization effects compared to width at this level. As we show further in bench-\nmarks, WRN-40-4 is 8 times faster to train, so evidently depth to width ratio in the original\nthin residual networks is far from optimal.\nAlso, wide WRN-28-10 outperforms thin ResNet-1001 by 0.92% (with the same mini-\nbatch size during training) on CIFAR-10 and 3.46% on CIFAR-100, having 36 times less\nlayers (see table 5). We note that the result of 4.64% with ResNet-1001 was obtained with\nbatch size 64, whereas we use a batch size 128 in all of our experiments (i.e., all other results\n",
    "8\nSERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\ndepth\nk\n# params\nCIFAR-10\nCIFAR-100\n40\n1\n0.6M\n6.85\n30.89\n40\n2\n2.2M\n5.33\n26.04\n40\n4\n8.9M\n4.97\n22.89\n40\n8\n35.7M\n4.66\n-\n28\n10\n36.5M\n4.17\n20.50\n28\n12\n52.5M\n4.33\n20.43\n22\n8\n17.2M\n4.38\n21.22\n22\n10\n26.8M\n4.44\n20.75\n16\n8\n11.0M\n4.81\n22.07\n16\n10\n17.1M\n4.56\n21.59\nTable 4: Test error (%) of various wide networks on CIFAR-10 and CIFAR-100 (ZCA pre-\nprocessing).\nreported in table 5 are with batch size 128). Training curves for these networks are presented\nin Figure 2.\nDespite previous arguments that depth gives regularization effects and width causes net-\nwork to overﬁt, we successfully train networks with several times more parameters than\nResNet-1001. For instance, wide WRN-28-10 (table 5) and wide WRN-40-10 (table 9) have\nrespectively 3.6 and 5 times more parameters than ResNet-1001 and both outperform it by a\nsigniﬁcant margin.\ndepth-k\n# params\nCIFAR-10\nCIFAR-100\nNIN [20]\n8.81\n35.67\nDSN [19]\n8.22\n34.57\nFitNet [24]\n8.39\n35.04\nHighway [28]\n7.72\n32.39\nELU [5]\n6.55\n24.28\noriginal-ResNet[11]\n110\n1.7M\n6.43\n25.16\n1202\n10.2M\n7.93\n27.82\nstoc-depth[14]\n110\n1.7M\n5.23\n24.58\n1202\n10.2M\n4.91\n-\npre-act-ResNet[13]\n110\n1.7M\n6.37\n-\n164\n1.7M\n5.46\n24.33\n1001\n10.2M\n4.92(4.64)\n22.71\nWRN (ours)\n40-4\n8.9M\n4.53\n21.18\n16-8\n11.0M\n4.27\n20.43\n28-10\n36.5M\n4.00\n19.25\nTable 5: Test error of different methods on CIFAR-10 and CIFAR-100 with moderate data\naugmentation (ﬂip/translation) and mean/std normalzation. We don’t use dropout for these\nresults. In the second column k is a widening factor. Results for [13] are shown with mini-\nbatch size 128 (as ours), and 64 in parenthesis. Our results were obtained by computing\nmedian over 5 runs.\nIn general, we observed that CIFAR mean/std preprocessing allows training wider and\ndeeper networks with better accuracy, and achieved 18.3% on CIFAR-100 using WRN-40-\n10 with 56×106 parameters (table 9), giving a total improvement of 4.4% over ResNet-1001\n",
    "SERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\n9\n0\n50\n100\n150\n200\n0\n5\n10\n15\n20\ntrain error (%)\n0\n5\n10\n15\n20\ntest error (%)\n0\n5\n10\n15\n20\ntest error (%)\nCIFAR-10\nResNet-164(error 5.46%)\nWRN-28-10(error 4.00%)\n0\n50\n100\n150\n200\n0\n10\n20\n30\n40\n50\ntrain error (%)\n0\n10\n20\n30\n40\n50\ntest error (%)\n0\n10\n20\n30\n40\n50\ntest error (%)\nCIFAR-100\nResNet-164(error 24.33%)\nWRN-28-10(error 19.25%)\nFigure 2: Training curves for thin and wide residual networks on CIFAR-10 and CIFAR-100.\nSolid lines denote test error (y-axis on the right), dashed lines denote training loss (y-axis on\nthe left).\nand establishing a new state-of-the-art result on this dataset.\nTo summarize:\n• widening consistently improves performance across residual networks of different\ndepth;\n• increasing both depth and width helps until the number of parameters becomes too\nhigh and stronger regularization is needed;\n• there doesn’t seem to be a regularization effect from very high depth in residual net-\nworks as wide networks with the same number of parameters as thin ones can learn\nsame or better representations. Furthermore, wide networks can successfully learn\nwith a 2 or more times larger number of parameters than thin ones, which would re-\nquire doubling the depth of thin networks, making them infeasibly expensive to train.\nDropout in residual blocks\nWe trained networks with dropout inserted into residual block between convolutions on all\ndatasets. We used cross-validation to determine dropout probability values, 0.3 on CIFAR\nand 0.4 on SVHN. Also, we didn’t have to increase number of training epochs compared to\nbaseline networks without dropout.\nDropout decreases test error on CIFAR-10 and CIFAR-100 by 0.11% and 0.4% corren-\nspondingly (over median of 5 runs and mean/std preprocessing) with WRN-28-10, and gives\nimprovements with other ResNets as well (table 6). To our knowledge, that was the ﬁrst\nresult to approach 20% error on CIFAR-100, even outperforming methods with heavy data\naugmentation. There is only a slight drop in accuracy with WRN-16-4 on CIFAR-10 which\nwe speculate is due to the relatively small number of parameters.\nWe notice a disturbing effect in residual network training after the ﬁrst learning rate drop\nwhen both loss and validation error suddenly start to go up and oscillate on high values until\nthe next learning rate drop. We found out that it is caused by weight decay, however making\nit lower leads to a signiﬁcant drop in accuracy. Interestingly, dropout partially removes this\neffect in most cases, see ﬁgures 2, 3.\nThe effect of dropout becomes more evident on SVHN. This is probably due to the fact\nthat we don’t do any data augmentation and batch normalization overﬁts, so dropout adds\n",
    "10\nSERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\ndepth\nk\ndropout\nCIFAR-10\nCIFAR-100\nSVHN\n16\n4\n5.02\n24.03\n1.85\n16\n4\n✓\n5.24\n23.91\n1.64\n28\n10\n4.00\n19.25\n-\n28\n10\n✓\n3.89\n18.85\n-\n52\n1\n6.43\n29.89\n2.08\n52\n1\n✓\n6.28\n29.78\n1.70\nTable 6: Effect of dropout in residual block. (mean/std preprocessing, CIFAR numbers are\nbased on median of 5 runs)\n0\n20\n40\n60\n80\n100\n120\n140\n160\n101\n102\ntraining loss\n0\n1\n2\n3\n4\n5\ntest error (%)\n0\n1\n2\n3\n4\n5\ntest error (%)\nSVHN\nResNet-50(error 2.07%)\nWRN-16-4(error 1.85%)\n0\n20\n40\n60\n80\n100\n120\n140\n160\n101\n102\ntraining loss\n0\n1\n2\n3\n4\n5\ntest error (%)\n0\n1\n2\n3\n4\n5\ntest error (%)\nSVHN\nWRN-16-4(error 1.85%)\nWRN-16-4-dropout(error 1.64%)\nFigure 3: Training curves for SVHN. On the left: thin and wide networks, on the right: effect\nof dropout. Solid lines denote test error (y-axis on the right), dashed lines denote training\nloss (y-axis on the left).\na regularization effect. Evidence for this can be found on training curves in ﬁgure 3 where\nthe loss without dropout drops to very low values. The results are presented in table 6. We\nobserve signiﬁcant improvements from using dropout on both thin and wide networks. Thin\n50-layer deep network even outperforms thin 152-layer deep network with stochastic depth\n[14]. We additionally trained WRN-16-8 with dropout on SVHN (table 9), which achieves\n1.54% on SVHN - the best published result to our knowledge. Without dropout it achieves\n1.81%.\nOverall, despite the arguments of combining with batch normalization, dropout shows\nitself as an effective techique of regularization of thin and wide networks. It can be used to\nfurther improve results from widening, while also being complementary to it.\nImageNet and COCO experiments\nFor ImageNet we ﬁrst experiment with non-bottleneck ResNet-18 and ResNet-34, trying to\ngradually increase their width from 1.0 to 3.0. The results are shown in table 7. Increas-\ning width gradually increases accuracy of both networks, and networks with a comparable\nnumber of parameters achieve similar results, despite having different depth. Althouth these\nnetworks have a large number of parameters, they are outperfomed by bottleneck networks,\nwhich is probably either due to that bottleneck architecture is simply better suited for Ima-\ngeNet classiﬁcation task, or due to that this more complex task needs a deeper network. To\ntest this, we took the ResNet-50, and tried to make it wider by increasing inner 3 × 3 layer\nwidth. With widening factor of 2.0 the resulting WRN-50-2-bottleneck outperforms ResNet-\n152 having 3 times less layers, and being signiﬁcantly faster. WRN-50-2-bottleneck is only\n",
    "SERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\n11\nslightly worse and almost 2× faster than the best-performing pre-activation ResNet-200, al-\nthouth having slightly more parameters (table 8). In general, we ﬁnd that, unlike CIFAR,\nImageNet networks need more width at the same depth to achieve the same accuracy. It is\nhowever clear that it is unnecessary to have residual networks with more than 50 layers due\nto computational reasons.\nWe didn’t try to train bigger bottleneck networks as 8-GPU machines are needed for that.\nwidth\n1.0\n1.5\n2.0\n3.0\nWRN-18\ntop1,top5\n30.4, 10.93\n27.06, 9.0\n25.58, 8.06\n24.06, 7.33\n#parameters\n11.7M\n25.9M\n45.6M\n101.8M\nWRN-34\ntop1,top5\n26.77, 8.67\n24.5, 7.58\n23.39, 7.00\n#parameters\n21.8M\n48.6M\n86.0M\nTable 7: ILSVRC-2012 validation error (single crop) of non-bottleneck ResNets for vari-\nous widening factors. Networks with a comparable number of parameters achieve similar\naccuracy, despite having 2 times less layers.\nModel\ntop-1 err, %\ntop-5 err, %\n#params\ntime/batch 16\nResNet-50\n24.01\n7.02\n25.6M\n49\nResNet-101\n22.44\n6.21\n44.5M\n82\nResNet-152\n22.16\n6.16\n60.2M\n115\nWRN-50-2-bottleneck\n21.9\n6.03\n68.9M\n93\npre-ResNet-200\n21.66\n5.79\n64.7M\n154\nTable 8: ILSVRC-2012 validation error (single crop) of bottleneck ResNets. Faster WRN-\n50-2-bottleneck outperforms ResNet-152 having 3 times less layers, and stands close to pre-\nResNet-200.\nWe also used WRN-34-2 to participate in COCO 2016 object detection challenge, using\na combination of MultiPathNet [32] and LocNet [7]. Despite having only 34 layers, this\nmodel achieves state-of-the-art single model performance, outperforming even ResNet-152\nand Inception-v4-based models.\nFinally, in table 9 we summarize our best WRN results over various commonly used\ndatasets.\nDataset\nmodel\ndropout\ntest perf.\nCIFAR-10\nWRN-40-10\n✓\n3.8%\nCIFAR-100\nWRN-40-10\n✓\n18.3%\nSVHN\nWRN-16-8\n✓\n1.54%\nImageNet (single crop)\nWRN-50-2-bottleneck\n21.9% top-1, 5.79% top-5\nCOCO test-std\nWRN-34-2\n35.2 mAP\nTable 9: Best WRN performance over various datasets, single run results. COCO model is\nbased on WRN-34-2 (wider basicblock), uses VGG-16-based AttractioNet proposals, and\nhas a LocNet-style localization part. To our knowledge, these are the best published results\nfor CIFAR-10, CIFAR-100, SVHN, and COCO (using non-ensemble models).\nComputational efﬁciency\nThin and deep residual networks with small kernels are against the nature of GPU com-\nputations because of their sequential structure. Increasing width helps effectively balance\n",
    "12\nSERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\ncomputations in much more optimal way, so that wide networks are many times more ef-\nﬁcient than thin ones as our benchmarks show. We use cudnn v5 and Titan X to measure\nforward+backward update times with minibatch size 32 for several networks, the results are\nin the ﬁgure 4. We show that our best CIFAR wide WRN-28-10 is 1.6 times faster than thin\nResNet-1001. Furthermore, wide WRN-40-4, which has approximately the same accuracy\nas ResNet-1001, is 8 times faster.\n164\n1004\n85\n512\nthin\n40-4 16-10 28-100\n100\n200\n300\n400\n500\n68\n164\n312\ntime (ms)\nwide\n5.46%\n4.64%\n4.66%\n4.56%\n4.38%\nFigure 4: Time of forward+backward update per minibatch of size 32 for wide and thin\nnetworks(x-axis denotes network depth and widening factor). Numbers beside bars indicate\ntest error on CIFAR-10, on top - time (ms). Test time is a proportional fraction of these\nbenchmarks. Note, for instance, that wide WRN-40-4 is 8 times faster than thin ResNet-\n1001 while having approximately the same accuracy.\nImplementation details\nIn all our experiments we use SGD with Nesterov momentum and cross-entropy loss. The\ninitial learning rate is set to 0.1, weight decay to 0.0005, dampening to 0, momentum to 0.9\nand minibatch size to 128. On CIFAR learning rate dropped by 0.2 at 60, 120 and 160 epochs\nand we train for total 200 epochs. On SVHN initial learning rate is set to 0.01 and we drop\nit at 80 and 120 epochs by 0.1, training for total 160 epochs. Our implementation is based\non Torch [6]. We use [21] to reduce memory footprints of all our networks. For ImageNet\nexperiments we used fb.resnet.torch implementation [10]. Our code and models are\navailable at https://github.com/szagoruyko/wide-residual-networks.\n4\nConclusions\nWe presented a study on the width of residual networks as well as on the use of dropout\nin residual architectures. Based on this study, we proposed a wide residual network archi-\ntecture that provides state-of-the-art results on several commonly used benchmark datasets\n(including CIFAR-10, CIFAR-100, SVHN and COCO), as well as signiﬁcant improvements\non ImageNet. We demonstrate that wide networks with only 16 layers can signiﬁcantly out-\nperform 1000-layer deep networks on CIFAR, as well as that 50-layer outperform 152-layer\non ImageNet, thus showing that the main power of residual networks is in residual blocks,\nand not in extreme depth as claimed earlier. Also, wide residual networks are several times\nfaster to train. We think that these intriguing ﬁndings will help further advances in research\nin deep neural networks.\n",
    "SERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\n13\n5\nAcknowledgements\nWe thank startup company VisionLabs and Eugenio Culurciello for giving us access to their\nclusters, without them ImageNet experiments wouldn’t be possible. We also thank Adam\nLerer and Sam Gross for helpful discussions. Work supported by EC project FP7-ICT-\n611145 ROBOSPECT.\nReferences\n[1] Yoshua Bengio and Xavier Glorot. Understanding the difﬁculty of training deep feed-\nforward neural networks. In Proceedings of AISTATS 2010, volume 9, pages 249–256,\nMay 2010.\n[2] Yoshua Bengio and Yann LeCun. Scaling learning algorithms towards AI. In Léon\nBottou, Olivier Chapelle, D. DeCoste, and J. Weston, editors, Large Scale Kernel Ma-\nchines. MIT Press, 2007.\n[3] Monica Bianchini and Franco Scarselli. On the complexity of shallow and deep neu-\nral network classiﬁers. In 22th European Symposium on Artiﬁcial Neural Networks,\nESANN 2014, Bruges, Belgium, April 23-25, 2014, 2014.\n[4] T. Chen, I. Goodfellow, and J. Shlens. Net2net: Accelerating learning via knowledge\ntransfer. In International Conference on Learning Representation, 2016.\n[5] Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep\nnetwork learning by exponential linear units (elus). CoRR, abs/1511.07289, 2015.\n[6] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A matlab-like environment for\nmachine learning. In BigLearn, NIPS Workshop, 2011.\n[7] Spyros Gidaris and Nikos Komodakis. Locnet: Improving localization accuracy for\nobject detection. In Computer Vision and Pattern Recognition (CVPR), 2016 IEEE\nConference on, 2016.\n[8] Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua\nBengio. Maxout networks. In Sanjoy Dasgupta and David McAllester, editors, Pro-\nceedings of the 30th International Conference on Machine Learning (ICML’13), pages\n1319–1327, 2013.\n[9] Benjamin Graham. Fractional max-pooling. arXiv:1412.6071, 2014.\n[10] Sam Gross and Michael Wilber. Training and investigating residual nets, 2016. URL\nhttps://github.com/facebook/fb.resnet.torch.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for\nimage recognition. CoRR, abs/1512.03385, 2015.\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDelving deep into\nrectiﬁers: Surpassing human-level performance on imagenet classiﬁcation.\nCoRR,\nabs/1502.01852, 2015.\n",
    "14\nSERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep\nresidual networks. CoRR, abs/1603.05027, 2016.\n[14] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger.\nDeep\nnetworks with stochastic depth. CoRR, abs/1603.09382, 2016.\n[15] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network\ntraining by reducing internal covariate shift. In David Blei and Francis Bach, editors,\nProceedings of the 32nd International Conference on Machine Learning (ICML-15),\npages 448–456. JMLR Workshop and Conference Proceedings, 2015.\n[16] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deep convo-\nlutional neural networks. In NIPS, 2012.\n[17] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.\nCifar-10 (canadian institute\nfor advanced research). 2012. URL http://www.cs.toronto.edu/~kriz/\ncifar.html.\n[18] Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Ben-\ngio. An empirical evaluation of deep architectures on problems with many factors of\nvariation. In Zoubin Ghahramani, editor, Proceedings of the 24th International Con-\nference on Machine Learning (ICML’07), pages 473–480. ACM, 2007.\n[19] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-Supervised Nets. 2014.\n[20] Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. CoRR, abs/1312.4400,\n2013.\n[21] Francisco Massa. Optnet - reducing memory usage in torch neural networks, 2016.\nURL https://github.com/fmassa/optimize-net.\n[22] Guido F. Montúfar, Razvan Pascanu, KyungHyun Cho, and Yoshua Bengio. On the\nnumber of linear regions of deep neural networks. In Advances in Neural Information\nProcessing Systems 27: Annual Conference on Neural Information Processing Systems\n2014, December 8-13 2014, Montreal, Quebec, Canada, pages 2924–2932, 2014.\n[23] Tapani Raiko, Harri Valpola, and Yann Lecun. Deep learning made easier by linear\ntransformations in perceptrons. In Neil D. Lawrence and Mark A. Girolami, editors,\nProceedings of the Fifteenth International Conference on Artiﬁcial Intelligence and\nStatistics (AISTATS-12), volume 22, pages 924–932, 2012.\n[24] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo\nGatta, and Yoshua Bengio. FitNets: Hints for thin deep nets. Technical Report Arxiv\nreport 1412.6550, arXiv, 2014.\n[25] J. Schmidhuber. Learning complex, extended sequences using the principle of history\ncompression. Neural Computation, 4(2):234–242, 1992.\n[26] K. Simonyan and A. Zisserman.\nVery deep convolutional networks for large-scale\nimage recognition. In ICLR, 2015.\n[27] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout:\nA simple way to prevent neural networks from overﬁtting. JMLR, 2014.\n",
    "SERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\n15\n[28] Rupesh Kumar Srivastava, Klaus Greff, and Jürgen Schmidhuber. Highway networks.\nCoRR, abs/1505.00387, 2015.\n[29] Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. On the im-\nportance of initialization and momentum in deep learning. In Sanjoy Dasgupta and\nDavid Mcallester, editors, Proceedings of the 30th International Conference on Ma-\nchine Learning (ICML-13), volume 28, pages 1139–1147. JMLR Workshop and Con-\nference Proceedings, May 2013.\n[30] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke,\nand A. Rabinovich. Going deeper with convolutions. In CVPR, 2015.\n[31] Christian Szegedy, Sergey Ioffe, and Vincent Vanhoucke.\nInception-v4, inception-\nresnet and the impact of residual connections on learning. abs/1602.07261, 2016.\n[32] S. Zagoruyko, A. Lerer, T.-Y. Lin, P. O. Pinheiro, S. Gross, S. Chintala, and P. Dollár.\nA multipath network for object detection. In BMVC, 2016.\n"
  ],
  "full_text": "SERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\n1\nWide Residual Networks\nSergey Zagoruyko\nsergey.zagoruyko@enpc.fr\nNikos Komodakis\nnikos.komodakis@enpc.fr\nUniversité Paris-Est, École des Ponts\nParisTech\nParis, France\nAbstract\nDeep residual networks were shown to be able to scale up to thousands of layers\nand still have improving performance. However, each fraction of a percent of improved\naccuracy costs nearly doubling the number of layers, and so training very deep resid-\nual networks has a problem of diminishing feature reuse, which makes these networks\nvery slow to train. To tackle these problems, in this paper we conduct a detailed exper-\nimental study on the architecture of ResNet blocks, based on which we propose a novel\narchitecture where we decrease depth and increase width of residual networks. We call\nthe resulting network structures wide residual networks (WRNs) and show that these are\nfar superior over their commonly used thin and very deep counterparts. For example,\nwe demonstrate that even a simple 16-layer-deep wide residual network outperforms in\naccuracy and efﬁciency all previous deep residual networks, including thousand-layer-\ndeep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and\nsigniﬁcant improvements on ImageNet. Our code and models are available at https:\n//github.com/szagoruyko/wide-residual-networks.\n1\nIntroduction\nConvolutional neural networks have seen a gradual increase of the number of layers in the\nlast few years, starting from AlexNet [16], VGG [26], Inception [30] to Residual [11] net-\nworks, corresponding to improvements in many image recognition tasks. The superiority\nof deep networks has been spotted in several works in the recent years [3, 22]. However,\ntraining deep neural networks has several difﬁculties, including exploding/vanishing gradi-\nents and degradation. Various techniques were suggested to enable training of deeper neural\nnetworks, such as well-designed initialization strategies [1, 12], better optimizers [29], skip\nconnections [19, 23], knowledge transfer [4, 24] and layer-wise training [25].\nThe latest residual networks [11] had a large success winning ImageNet and COCO 2015\ncompetition and achieving state-of-the-art in several benchmarks, including object classiﬁ-\ncation on ImageNet and CIFAR, object detection and segmentation on PASCAL VOC and\nMS COCO. Compared to Inception architectures they show better generalization, meaning\nthe features can be utilized in transfer learning with better efﬁciency. Also, follow-up work\nshowed that residual links speed up convergence of deep networks [31]. Recent follow-up\nwork explored the order of activations in residual networks, presenting identity mappings\nin residual blocks [13] and improving training of very deep networks. Successful training\nof very deep networks was also shown to be possible through the use of highway networks\nc⃝2016. The copyright of this document resides with its authors.\nIt may be distributed unchanged freely in print or electronic forms.\narXiv:1605.07146v4  [cs.CV]  14 Jun 2017\n\n\n2\nSERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\nconv3x3\nconv3x3\nxl\nxl+1\n(a) basic\nconv1x1\nconv3x3\nconv1x1\nxl\nxl+1\n(b) bottleneck\nconv3x3\nconv3x3\nxl\nxl+1\n(c) basic-wide\ndropout\nxl\nxl+1\nconv3x3\nconv3x3\n(d) wide-dropout\nFigure 1: Various residual blocks used in the paper. Batch normalization and ReLU precede\neach convolution (omitted for clarity)\n[28], which is an architecture that had been proposed prior to residual networks. The essen-\ntial difference between residual and highway networks is that in the latter residual links are\ngated and weights of these gates are learned.\nTherefore, up to this point, the study of residual networks has focused mainly on the\norder of activations inside a ResNet block and the depth of residual networks. In this work\nwe attempt to conduct an experimental study that goes beyond the above points. By doing\nso, our goal is to explore a much richer set of network architectures of ResNet blocks and\nthoroughly examine how several other different aspects besides the order of activations affect\nperformance. As we explain below, such an exploration of architectures has led to new\ninteresting ﬁndings with great practical importance concerning residual networks.\nWidth vs depth in residual networks. The problem of shallow vs deep networks has\nbeen in discussion for a long time in machine learning [2, 18] with pointers to the circuit\ncomplexity theory literature showing that shallow circuits can require exponentially more\ncomponents than deeper circuits. The authors of residual networks tried to make them as thin\nas possible in favor of increasing their depth and having less parameters, and even introduced\na «bottleneck» block which makes ResNet blocks even thinner.\nWe note, however, that the residual block with identity mapping that allows to train\nvery deep networks is at the same time a weakness of residual networks. As gradient ﬂows\nthrough the network there is nothing to force it to go through residual block weights and it\ncan avoid learning anything during training, so it is possible that there is either only a few\nblocks that learn useful representations, or many blocks share very little information with\nsmall contribution to the ﬁnal goal. This problem was formulated as diminishing feature\nreuse in [28]. The authors of [14] tried to address this problem with the idea of randomly\ndisabling residual blocks during training. This method can be viewed as a special case of\ndropout [27], where each residual block has an identity scalar weight on which dropout is\napplied. The effectiveness of this approach proves the hypothesis above.\nMotivated by the above observation, our work builds on top of [13] and tries to answer\nthe question of how wide deep residual networks should be and address the problem of train-\ning. In this context, we show that the widening of ResNet blocks (if done properly) provides\na much more effective way of improving performance of residual networks compared to in-\ncreasing their depth. In particular, we present wider deep residual networks that signiﬁcantly\nimprove over [13], having 50 times less layers and being more than 2 times faster. We call\nthe resulting network architectures wide residual networks. For instance, our wide 16-layer\ndeep network has the same accuracy as a 1000-layer thin deep network and a comparable\nnumber of parameters, although being several times faster to train. This type of experiments\n\n\nSERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\n3\nthus seem to indicate that the main power of deep residual networks is in residual blocks, and\nthat the effect of depth is supplementary. We note that one can train even better wide resid-\nual networks that have twice as many parameters (and more), which suggests that to further\nimprove performance by increasing depth of thin networks one needs to add thousands of\nlayers in this case.\nUse of dropout in ResNet blocks. Dropout was ﬁrst introduced in [27] and then was\nadopted by many successful architectures as [16, 26] etc. It was mostly applied on top layers\nthat had a large number of parameters to prevent feature coadaptation and overﬁtting. It was\nthen mainly substituted by batch normalization [15] which was introduced as a technique to\nreduce internal covariate shift in neural network activations by normalizing them to have spe-\nciﬁc distribution. It also works as a regularizer and the authors experimentally showed that a\nnetwork with batch normalization achieves better accuracy than a network with dropout. In\nour case, as widening of residual blocks results in an increase of the number of parameters,\nwe studied the effect of dropout to regularize training and prevent overﬁtting. Previously,\ndropout in residual networks was studied in [13] with dropout being inserted in the identity\npart of the block, and the authors showed negative effects of that. Instead, we argue here\nthat dropout should be inserted between convolutional layers. Experimental results on wide\nresidual networks show that this leads to consistent gains, yielding even new state-of-the-\nart results (e.g., 16-layer-deep wide residual network with dropout achieves 1.64% error on\nSVHN).\nIn summary, the contributions of this work are as follows:\n• We present a detailed experimental study of residual network architectures that thor-\noughly examines several important aspects of ResNet block structure.\n• We propose a novel widened architecture for ResNet blocks that allows for residual\nnetworks with signiﬁcantly improved performance.\n• We propose a new way of utilizing dropout within deep residual networks so as to\nproperly regularize them and prevent overﬁtting during training.\n• Last, we show that our proposed ResNet architectures achieve state-of-the-art results\non several datasets dramatically improving accuracy and speed of residual networks.\n2\nWide residual networks\nResidual block with identity mapping can be represented by the following formula:\nxl+1 = xl +F(xl,Wl)\n(1)\nwhere xl+1 and xl are input and output of the l-th unit in the network, F is a residual func-\ntion and Wl are parameters of the block. Residual network consists of sequentially stacked\nresidual blocks.\nIn [13] residual networks consisted of two type of blocks:\n• basic - with two consecutive 3 × 3 convolutions with batch normalization and ReLU\npreceding convolution: conv3×3-conv3×3 Fig.1(a)\n• bottleneck - with one 3 × 3 convolution surrounded by dimensionality reducing and\nexpanding 1×1 convolution layers: conv1×1-conv3×3-conv1×1 Fig.1(b)\n\n\n4\nSERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\ngroup name\noutput size\nblock type = B(3,3)\nconv1\n32×32\n[3×3, 16]\nconv2\n32×32\n\u0014\n3×3, 16×k\n3×3, 16×k\n\u0015\n×N\nconv3\n16×16\n\u0014\n3×3, 32×k\n3×3, 32×k\n\u0015\n×N\nconv4\n8×8\n\u0014\n3×3, 64×k\n3×3, 64×k\n\u0015\n×N\navg-pool\n1×1\n[8×8]\nTable 1: Structure of wide residual networks. Network width is determined by factor k.\nOriginal architecture [13] is equivalent to k = 1. Groups of convolutions are shown in brack-\nets where N is a number of blocks in group, downsampling performed by the ﬁrst layers\nin groups conv3 and conv4. Final classiﬁcation layer is omitted for clearance. In the\nparticular example shown, the network uses a ResNet block of type B(3,3).\nCompared to the original architecture [11] in [13] the order of batch normalization, ac-\ntivation and convolution in residual block was changed from conv-BN-ReLU to BN-ReLU-\nconv. As the latter was shown to train faster and achieve better results we don’t consider\nthe original version. Furthermore, so-called «bottleneck» blocks were initially used to make\nblocks less computationally expensive to increase the number of layers. As we want to study\nthe effect of widening and «bottleneck» is used to make networks thinner we don’t consider\nit too, focusing instead on «basic» residual architecture.\nThere are essentially three simple ways to increase representational power of residual\nblocks:\n• to add more convolutional layers per block\n• to widen the convolutional layers by adding more feature planes\n• to increase ﬁlter sizes in convolutional layers\nAs small ﬁlters were shown to be very effective in several works including [26, 31] we do\nnot consider using ﬁlters larger than 3×3. Let us also introduce two factors, deepening factor\nl and widening factor k, where l is the number of convolutions in a block and k multiplies\nthe number of features in convolutional layers, thus the baseline «basic» block corresponds\nto l = 2, k = 1. Figures 1(a) and 1(c) show schematic examples of «basic» and «basic-wide»\nblocks respectively.\nThe general structure of our residual networks is illustrated in table 1: it consists of an\ninitial convolutional layer conv1 that is followed by 3 groups (each of size N) of residual\nblocks conv2, conv3 and conv4, followed by average pooling and ﬁnal classiﬁcation\nlayer. The size of conv1 is ﬁxed in all of our experiments, while the introduced widen-\ning factor k scales the width of the residual blocks in the three groups conv2-4 (e.g., the\noriginal «basic» architecture is equivalent to k = 1). We want to study the effect of represen-\ntational power of residual block and, to that end, we perform and test several modiﬁcations\nto the «basic» architecture, which are detailed in the following subsections.\n\n\nSERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\n5\n2.1\nType of convolutions in residual block\nLet B(M) denote residual block structure, where M is a list with the kernel sizes of the\nconvolutional layers in a block. For example, B(3,1) denotes a residual block with 3×3 and\n1 × 1 convolutional layers (we always assume square spatial kernels). Note that, as we do\nnot consider «bottleneck» blocks as explained earlier, the number of feature planes is always\nkept the same across the block. We would like to answer the question of how important each\nof the 3 × 3 convolutional layers of the «basic» residual architecture is and if they can be\nsubstituted by a less computationally expensive 1 × 1 layer or even a combination of 1 × 1\nand 3 × 3 convolutional layers, e.g., B(1,3) or B(1,3). This can increase or decrease the\nrepresentational power of the block. We thus experiment with the following combinations\n(note that the last combination, i.e., B(3,1,1) is similar to effective Network-in-Network\n[20] architecture):\n1. B(3,3) - original «basic» block\n2. B(3,1,3) - with one extra 1×1 layer\n3. B(1,3,1) - with the same dimensionality of all convolutions, «straightened» bottleneck\n4. B(1,3) - the network has alternating 1×1 - 3×3 convolutions everywhere\n5. B(3,1) - similar idea to the previous block\n6. B(3,1,1) - Network-in-Network style block\n2.2\nNumber of convolutional layers per residual block\nWe also experiment with the block deepening factor l to see how it affects performance. The\ncomparison has to be done among networks with the same number of parameters, so in this\ncase we need to build networks with different l and d (where d denotes the total number of\nblocks) while ensuring that network complexity is kept roughly constant. This means, for\ninstance, that d should decrease whenever l increases.\n2.3\nWidth of residual blocks\nIn addition to the above modiﬁcations, we experiment with the widening factor k of a block.\nWhile the number of parameters increases linearly with l (the deepening factor) and d\n(the number of ResNet blocks), number of parameters and computational complexity are\nquadratic in k. However, it is more computationally effective to widen the layers than have\nthousands of small kernels as GPU is much more efﬁcient in parallel computations on large\ntensors, so we are interested in an optimal d to k ratio.\nOne argument for wider residual networks would be that almost all architectures before\nresidual networks, including the most successful Inception [30] and VGG [26], were much\nwider compared to [13]. For example, residual networks WRN-22-8 and WRN-16-10 (see\nnext paragraph for explanation of this notation) are very similar in width, depth and number\nof parameters to VGG architectures.\nWe further refer to original residual networks with k = 1 as «thin» and to networks with\nk > 1 as «wide». In the rest of the paper we use the following notation: WRN-n-k denotes\na residual network that has a total number of convolutional layers n and a widening factor k\n(for example, network with 40 layers and k = 2 times wider than original would be denoted\nas WRN-40-2). Also, when applicable we append block type, e.g. WRN-40-2-B(3,3).\n\n\n6\nSERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\nblock type\ndepth\n# params\ntime,s\nCIFAR-10\nB(1,3,1)\n40\n1.4M\n85.8\n6.06\nB(3,1)\n40\n1.2M\n67.5\n5.78\nB(1,3)\n40\n1.3M\n72.2\n6.42\nB(3,1,1)\n40\n1.3M\n82.2\n5.86\nB(3,3)\n28\n1.5M\n67.5\n5.73\nB(3,1,3)\n22\n1.1M\n59.9\n5.78\nTable 2: Test error (%, median over 5 runs) on CIFAR-10\nof residual networks with k = 2 and different block types.\nTime column measures one training epoch.\nl\nCIFAR-10\n1\n6.69\n2\n5.43\n3\n5.65\n4\n5.93\nTable 3: Test error (%, me-\ndian over 5 runs) on CIFAR-\n10 of WRN-40-2 (2.2M)\nwith various l.\n2.4\nDropout in residual blocks\nAs widening increases the number of parameters we would like to study ways of regular-\nization. Residual networks already have batch normalization that provides a regularization\neffect, however it requires heavy data augmentation, which we would like to avoid, and it’s\nnot always possible. We add a dropout layer into each residual block between convolutions\nas shown in ﬁg. 1(d) and after ReLU to perturb batch normalization in the next residual\nblock and prevent it from overﬁtting. In very deep residual networks that should help deal\nwith diminishing feature reuse problem enforcing learning in different residual blocks.\n3\nExperimental results\nFor experiments we chose well-known CIFAR-10, CIFAR-100, SVHN and ImageNet image\nclassiﬁcation datasets. CIFAR-10 and CIFAR-100 datasets [17] consist of 32 × 32 color\nimages drawn from 10 and 100 classes split into 50,000 train and 10,000 test images. For data\naugmentation we do horizontal ﬂips and take random crops from image padded by 4 pixels on\neach side, ﬁlling missing pixels with reﬂections of original image. We don’t use heavy data\naugmentation as proposed in [9]. SVHN is a dataset of Google’s Street View House Numbers\nimages and contains about 600,000 digit images, coming from a signiﬁcantly harder real\nworld problem. For experiments on SVHN we don’t do any image preprocessing, except\ndividing images by 255 to provide them in [0,1] range as input. All of our experiments\nexcept ImageNet are based on [13] architecture with pre-activation residual blocks and we\nuse it as baseline. For ImageNet, we ﬁnd that using pre-activation in networks with less\nthan 100 layers does not make any signiﬁcant difference and so we decide to use the original\nResNet architecture in this case. Unless mentioned otherwise, for CIFAR we follow the\nimage preprocessing of [8] with ZCA whitening. However, for some CIFAR experiments\nwe instead use simple mean/std normalization such that we can directly compare with [13]\nand other ResNet related works that make use of this type of preprocessing.\nIn the following we describe our ﬁndings w.r.t. the different ResNet block architectures\nand also analyze the performance of our proposed wide residual networks. We note that for\nall experiments related to «type of convolutions in a block» and «number of convolutions\nper block» we use k = 2 and reduced depth compared to [13] in order to speed up training.\n\n\nSERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\n7\nType of convolutions in a block\nWe start by reporting results using trained networks with different block types B (reported\nresults are on CIFAR-10). We used WRN-40-2 for blocks B(1,3,1), B(3,1), B(1,3) and\nB(3,1,1) as these blocks have only one 3×3 convolution. To keep the number of parameters\ncomparable we trained other networks with less layers: WRN-28-2-B(3,3) and WRN-22-2-\nB(3,1,3). We provide the results including test accuracy in median over 5 runs and time per\ntraining epoch in the table 2. Block B(3,3) turned out to be the best by a little margin, and\nB(3,1) with B(3,1,3) are very close to B(3,3) in accuracy having less parameters and less\nlayers. B(3,1,3) is faster than others by a small margin.\nBased on the above, blocks with comparable number of parameters turned out to give\nmore or less the same results. Due to this fact, we hereafter restrict our attention to only\nWRNs with 3×3 convolutions so as to be also consistent with other methods.\nNumber of convolutions per block\nWe next proceed with the experiments related to varying the deepening factor l (which rep-\nresents the number of convolutional layers per block). We show indicative results in table 3,\nwhere in this case we took WRN-40-2 with 3×3 convolutions and trained several networks\nwith different deepening factor l ∈[1,2,3,4], same number of parameters (2.2×106) and\nsame number of convolutional layers.\nAs can be noticed, B(3,3) turned out to be the best, whereas B(3,3,3) and B(3,3,3,3)\nhad the worst performance. We speculate that this is probably due to the increased difﬁculty\nin optimization as a result of the decreased number of residual connections in the last two\ncases. Furthermore, B(3) turned out to be quite worse. The conclusion is that B(3,3) is\noptimal in terms of number of convolutions per block. For this reason, in the remaining\nexperiments we only consider wide residual networks with a block of type B(3,3).\nWidth of residual blocks\nAs we try to increase widening parameter k we have to decrease total number of layers. To\nﬁnd an optimal ratio we experimented with k from 2 to 12 and depth from 16 to 40. The\nresults are presented in table 4. As can be seen, all networks with 40, 22 and 16 layers see\nconsistent gains when width is increased by 1 to 12 times. On the other hand, when keeping\nthe same ﬁxed widening factor k = 8 or k = 10 and varying depth from 16 to 28 there is a\nconsistent improvement, however when we further increase depth to 40 accuracy decreases\n(e.g., WRN-40-8 loses in accuracy to WRN-22-8).\nWe show additional results in table 5 where we compare thin and wide residual networks.\nAs can be observed, wide WRN-40-4 compares favorably to thin ResNet-1001 as it achieves\nbetter accuracy on both CIFAR-10 and CIFAR-100. Yet, it is interesting that these networks\nhave comparable number of parameters, 8.9×106 and 10.2×106, suggesting that depth does\nnot add regularization effects compared to width at this level. As we show further in bench-\nmarks, WRN-40-4 is 8 times faster to train, so evidently depth to width ratio in the original\nthin residual networks is far from optimal.\nAlso, wide WRN-28-10 outperforms thin ResNet-1001 by 0.92% (with the same mini-\nbatch size during training) on CIFAR-10 and 3.46% on CIFAR-100, having 36 times less\nlayers (see table 5). We note that the result of 4.64% with ResNet-1001 was obtained with\nbatch size 64, whereas we use a batch size 128 in all of our experiments (i.e., all other results\n\n\n8\nSERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\ndepth\nk\n# params\nCIFAR-10\nCIFAR-100\n40\n1\n0.6M\n6.85\n30.89\n40\n2\n2.2M\n5.33\n26.04\n40\n4\n8.9M\n4.97\n22.89\n40\n8\n35.7M\n4.66\n-\n28\n10\n36.5M\n4.17\n20.50\n28\n12\n52.5M\n4.33\n20.43\n22\n8\n17.2M\n4.38\n21.22\n22\n10\n26.8M\n4.44\n20.75\n16\n8\n11.0M\n4.81\n22.07\n16\n10\n17.1M\n4.56\n21.59\nTable 4: Test error (%) of various wide networks on CIFAR-10 and CIFAR-100 (ZCA pre-\nprocessing).\nreported in table 5 are with batch size 128). Training curves for these networks are presented\nin Figure 2.\nDespite previous arguments that depth gives regularization effects and width causes net-\nwork to overﬁt, we successfully train networks with several times more parameters than\nResNet-1001. For instance, wide WRN-28-10 (table 5) and wide WRN-40-10 (table 9) have\nrespectively 3.6 and 5 times more parameters than ResNet-1001 and both outperform it by a\nsigniﬁcant margin.\ndepth-k\n# params\nCIFAR-10\nCIFAR-100\nNIN [20]\n8.81\n35.67\nDSN [19]\n8.22\n34.57\nFitNet [24]\n8.39\n35.04\nHighway [28]\n7.72\n32.39\nELU [5]\n6.55\n24.28\noriginal-ResNet[11]\n110\n1.7M\n6.43\n25.16\n1202\n10.2M\n7.93\n27.82\nstoc-depth[14]\n110\n1.7M\n5.23\n24.58\n1202\n10.2M\n4.91\n-\npre-act-ResNet[13]\n110\n1.7M\n6.37\n-\n164\n1.7M\n5.46\n24.33\n1001\n10.2M\n4.92(4.64)\n22.71\nWRN (ours)\n40-4\n8.9M\n4.53\n21.18\n16-8\n11.0M\n4.27\n20.43\n28-10\n36.5M\n4.00\n19.25\nTable 5: Test error of different methods on CIFAR-10 and CIFAR-100 with moderate data\naugmentation (ﬂip/translation) and mean/std normalzation. We don’t use dropout for these\nresults. In the second column k is a widening factor. Results for [13] are shown with mini-\nbatch size 128 (as ours), and 64 in parenthesis. Our results were obtained by computing\nmedian over 5 runs.\nIn general, we observed that CIFAR mean/std preprocessing allows training wider and\ndeeper networks with better accuracy, and achieved 18.3% on CIFAR-100 using WRN-40-\n10 with 56×106 parameters (table 9), giving a total improvement of 4.4% over ResNet-1001\n\n\nSERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\n9\n0\n50\n100\n150\n200\n0\n5\n10\n15\n20\ntrain error (%)\n0\n5\n10\n15\n20\ntest error (%)\n0\n5\n10\n15\n20\ntest error (%)\nCIFAR-10\nResNet-164(error 5.46%)\nWRN-28-10(error 4.00%)\n0\n50\n100\n150\n200\n0\n10\n20\n30\n40\n50\ntrain error (%)\n0\n10\n20\n30\n40\n50\ntest error (%)\n0\n10\n20\n30\n40\n50\ntest error (%)\nCIFAR-100\nResNet-164(error 24.33%)\nWRN-28-10(error 19.25%)\nFigure 2: Training curves for thin and wide residual networks on CIFAR-10 and CIFAR-100.\nSolid lines denote test error (y-axis on the right), dashed lines denote training loss (y-axis on\nthe left).\nand establishing a new state-of-the-art result on this dataset.\nTo summarize:\n• widening consistently improves performance across residual networks of different\ndepth;\n• increasing both depth and width helps until the number of parameters becomes too\nhigh and stronger regularization is needed;\n• there doesn’t seem to be a regularization effect from very high depth in residual net-\nworks as wide networks with the same number of parameters as thin ones can learn\nsame or better representations. Furthermore, wide networks can successfully learn\nwith a 2 or more times larger number of parameters than thin ones, which would re-\nquire doubling the depth of thin networks, making them infeasibly expensive to train.\nDropout in residual blocks\nWe trained networks with dropout inserted into residual block between convolutions on all\ndatasets. We used cross-validation to determine dropout probability values, 0.3 on CIFAR\nand 0.4 on SVHN. Also, we didn’t have to increase number of training epochs compared to\nbaseline networks without dropout.\nDropout decreases test error on CIFAR-10 and CIFAR-100 by 0.11% and 0.4% corren-\nspondingly (over median of 5 runs and mean/std preprocessing) with WRN-28-10, and gives\nimprovements with other ResNets as well (table 6). To our knowledge, that was the ﬁrst\nresult to approach 20% error on CIFAR-100, even outperforming methods with heavy data\naugmentation. There is only a slight drop in accuracy with WRN-16-4 on CIFAR-10 which\nwe speculate is due to the relatively small number of parameters.\nWe notice a disturbing effect in residual network training after the ﬁrst learning rate drop\nwhen both loss and validation error suddenly start to go up and oscillate on high values until\nthe next learning rate drop. We found out that it is caused by weight decay, however making\nit lower leads to a signiﬁcant drop in accuracy. Interestingly, dropout partially removes this\neffect in most cases, see ﬁgures 2, 3.\nThe effect of dropout becomes more evident on SVHN. This is probably due to the fact\nthat we don’t do any data augmentation and batch normalization overﬁts, so dropout adds\n\n\n10\nSERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\ndepth\nk\ndropout\nCIFAR-10\nCIFAR-100\nSVHN\n16\n4\n5.02\n24.03\n1.85\n16\n4\n✓\n5.24\n23.91\n1.64\n28\n10\n4.00\n19.25\n-\n28\n10\n✓\n3.89\n18.85\n-\n52\n1\n6.43\n29.89\n2.08\n52\n1\n✓\n6.28\n29.78\n1.70\nTable 6: Effect of dropout in residual block. (mean/std preprocessing, CIFAR numbers are\nbased on median of 5 runs)\n0\n20\n40\n60\n80\n100\n120\n140\n160\n101\n102\ntraining loss\n0\n1\n2\n3\n4\n5\ntest error (%)\n0\n1\n2\n3\n4\n5\ntest error (%)\nSVHN\nResNet-50(error 2.07%)\nWRN-16-4(error 1.85%)\n0\n20\n40\n60\n80\n100\n120\n140\n160\n101\n102\ntraining loss\n0\n1\n2\n3\n4\n5\ntest error (%)\n0\n1\n2\n3\n4\n5\ntest error (%)\nSVHN\nWRN-16-4(error 1.85%)\nWRN-16-4-dropout(error 1.64%)\nFigure 3: Training curves for SVHN. On the left: thin and wide networks, on the right: effect\nof dropout. Solid lines denote test error (y-axis on the right), dashed lines denote training\nloss (y-axis on the left).\na regularization effect. Evidence for this can be found on training curves in ﬁgure 3 where\nthe loss without dropout drops to very low values. The results are presented in table 6. We\nobserve signiﬁcant improvements from using dropout on both thin and wide networks. Thin\n50-layer deep network even outperforms thin 152-layer deep network with stochastic depth\n[14]. We additionally trained WRN-16-8 with dropout on SVHN (table 9), which achieves\n1.54% on SVHN - the best published result to our knowledge. Without dropout it achieves\n1.81%.\nOverall, despite the arguments of combining with batch normalization, dropout shows\nitself as an effective techique of regularization of thin and wide networks. It can be used to\nfurther improve results from widening, while also being complementary to it.\nImageNet and COCO experiments\nFor ImageNet we ﬁrst experiment with non-bottleneck ResNet-18 and ResNet-34, trying to\ngradually increase their width from 1.0 to 3.0. The results are shown in table 7. Increas-\ning width gradually increases accuracy of both networks, and networks with a comparable\nnumber of parameters achieve similar results, despite having different depth. Althouth these\nnetworks have a large number of parameters, they are outperfomed by bottleneck networks,\nwhich is probably either due to that bottleneck architecture is simply better suited for Ima-\ngeNet classiﬁcation task, or due to that this more complex task needs a deeper network. To\ntest this, we took the ResNet-50, and tried to make it wider by increasing inner 3 × 3 layer\nwidth. With widening factor of 2.0 the resulting WRN-50-2-bottleneck outperforms ResNet-\n152 having 3 times less layers, and being signiﬁcantly faster. WRN-50-2-bottleneck is only\n\n\nSERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\n11\nslightly worse and almost 2× faster than the best-performing pre-activation ResNet-200, al-\nthouth having slightly more parameters (table 8). In general, we ﬁnd that, unlike CIFAR,\nImageNet networks need more width at the same depth to achieve the same accuracy. It is\nhowever clear that it is unnecessary to have residual networks with more than 50 layers due\nto computational reasons.\nWe didn’t try to train bigger bottleneck networks as 8-GPU machines are needed for that.\nwidth\n1.0\n1.5\n2.0\n3.0\nWRN-18\ntop1,top5\n30.4, 10.93\n27.06, 9.0\n25.58, 8.06\n24.06, 7.33\n#parameters\n11.7M\n25.9M\n45.6M\n101.8M\nWRN-34\ntop1,top5\n26.77, 8.67\n24.5, 7.58\n23.39, 7.00\n#parameters\n21.8M\n48.6M\n86.0M\nTable 7: ILSVRC-2012 validation error (single crop) of non-bottleneck ResNets for vari-\nous widening factors. Networks with a comparable number of parameters achieve similar\naccuracy, despite having 2 times less layers.\nModel\ntop-1 err, %\ntop-5 err, %\n#params\ntime/batch 16\nResNet-50\n24.01\n7.02\n25.6M\n49\nResNet-101\n22.44\n6.21\n44.5M\n82\nResNet-152\n22.16\n6.16\n60.2M\n115\nWRN-50-2-bottleneck\n21.9\n6.03\n68.9M\n93\npre-ResNet-200\n21.66\n5.79\n64.7M\n154\nTable 8: ILSVRC-2012 validation error (single crop) of bottleneck ResNets. Faster WRN-\n50-2-bottleneck outperforms ResNet-152 having 3 times less layers, and stands close to pre-\nResNet-200.\nWe also used WRN-34-2 to participate in COCO 2016 object detection challenge, using\na combination of MultiPathNet [32] and LocNet [7]. Despite having only 34 layers, this\nmodel achieves state-of-the-art single model performance, outperforming even ResNet-152\nand Inception-v4-based models.\nFinally, in table 9 we summarize our best WRN results over various commonly used\ndatasets.\nDataset\nmodel\ndropout\ntest perf.\nCIFAR-10\nWRN-40-10\n✓\n3.8%\nCIFAR-100\nWRN-40-10\n✓\n18.3%\nSVHN\nWRN-16-8\n✓\n1.54%\nImageNet (single crop)\nWRN-50-2-bottleneck\n21.9% top-1, 5.79% top-5\nCOCO test-std\nWRN-34-2\n35.2 mAP\nTable 9: Best WRN performance over various datasets, single run results. COCO model is\nbased on WRN-34-2 (wider basicblock), uses VGG-16-based AttractioNet proposals, and\nhas a LocNet-style localization part. To our knowledge, these are the best published results\nfor CIFAR-10, CIFAR-100, SVHN, and COCO (using non-ensemble models).\nComputational efﬁciency\nThin and deep residual networks with small kernels are against the nature of GPU com-\nputations because of their sequential structure. Increasing width helps effectively balance\n\n\n12\nSERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\ncomputations in much more optimal way, so that wide networks are many times more ef-\nﬁcient than thin ones as our benchmarks show. We use cudnn v5 and Titan X to measure\nforward+backward update times with minibatch size 32 for several networks, the results are\nin the ﬁgure 4. We show that our best CIFAR wide WRN-28-10 is 1.6 times faster than thin\nResNet-1001. Furthermore, wide WRN-40-4, which has approximately the same accuracy\nas ResNet-1001, is 8 times faster.\n164\n1004\n85\n512\nthin\n40-4 16-10 28-100\n100\n200\n300\n400\n500\n68\n164\n312\ntime (ms)\nwide\n5.46%\n4.64%\n4.66%\n4.56%\n4.38%\nFigure 4: Time of forward+backward update per minibatch of size 32 for wide and thin\nnetworks(x-axis denotes network depth and widening factor). Numbers beside bars indicate\ntest error on CIFAR-10, on top - time (ms). Test time is a proportional fraction of these\nbenchmarks. Note, for instance, that wide WRN-40-4 is 8 times faster than thin ResNet-\n1001 while having approximately the same accuracy.\nImplementation details\nIn all our experiments we use SGD with Nesterov momentum and cross-entropy loss. The\ninitial learning rate is set to 0.1, weight decay to 0.0005, dampening to 0, momentum to 0.9\nand minibatch size to 128. On CIFAR learning rate dropped by 0.2 at 60, 120 and 160 epochs\nand we train for total 200 epochs. On SVHN initial learning rate is set to 0.01 and we drop\nit at 80 and 120 epochs by 0.1, training for total 160 epochs. Our implementation is based\non Torch [6]. We use [21] to reduce memory footprints of all our networks. For ImageNet\nexperiments we used fb.resnet.torch implementation [10]. Our code and models are\navailable at https://github.com/szagoruyko/wide-residual-networks.\n4\nConclusions\nWe presented a study on the width of residual networks as well as on the use of dropout\nin residual architectures. Based on this study, we proposed a wide residual network archi-\ntecture that provides state-of-the-art results on several commonly used benchmark datasets\n(including CIFAR-10, CIFAR-100, SVHN and COCO), as well as signiﬁcant improvements\non ImageNet. We demonstrate that wide networks with only 16 layers can signiﬁcantly out-\nperform 1000-layer deep networks on CIFAR, as well as that 50-layer outperform 152-layer\non ImageNet, thus showing that the main power of residual networks is in residual blocks,\nand not in extreme depth as claimed earlier. Also, wide residual networks are several times\nfaster to train. We think that these intriguing ﬁndings will help further advances in research\nin deep neural networks.\n\n\nSERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\n13\n5\nAcknowledgements\nWe thank startup company VisionLabs and Eugenio Culurciello for giving us access to their\nclusters, without them ImageNet experiments wouldn’t be possible. We also thank Adam\nLerer and Sam Gross for helpful discussions. Work supported by EC project FP7-ICT-\n611145 ROBOSPECT.\nReferences\n[1] Yoshua Bengio and Xavier Glorot. Understanding the difﬁculty of training deep feed-\nforward neural networks. In Proceedings of AISTATS 2010, volume 9, pages 249–256,\nMay 2010.\n[2] Yoshua Bengio and Yann LeCun. Scaling learning algorithms towards AI. In Léon\nBottou, Olivier Chapelle, D. DeCoste, and J. Weston, editors, Large Scale Kernel Ma-\nchines. MIT Press, 2007.\n[3] Monica Bianchini and Franco Scarselli. On the complexity of shallow and deep neu-\nral network classiﬁers. In 22th European Symposium on Artiﬁcial Neural Networks,\nESANN 2014, Bruges, Belgium, April 23-25, 2014, 2014.\n[4] T. Chen, I. Goodfellow, and J. Shlens. Net2net: Accelerating learning via knowledge\ntransfer. In International Conference on Learning Representation, 2016.\n[5] Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep\nnetwork learning by exponential linear units (elus). CoRR, abs/1511.07289, 2015.\n[6] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A matlab-like environment for\nmachine learning. In BigLearn, NIPS Workshop, 2011.\n[7] Spyros Gidaris and Nikos Komodakis. Locnet: Improving localization accuracy for\nobject detection. In Computer Vision and Pattern Recognition (CVPR), 2016 IEEE\nConference on, 2016.\n[8] Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua\nBengio. Maxout networks. In Sanjoy Dasgupta and David McAllester, editors, Pro-\nceedings of the 30th International Conference on Machine Learning (ICML’13), pages\n1319–1327, 2013.\n[9] Benjamin Graham. Fractional max-pooling. arXiv:1412.6071, 2014.\n[10] Sam Gross and Michael Wilber. Training and investigating residual nets, 2016. URL\nhttps://github.com/facebook/fb.resnet.torch.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for\nimage recognition. CoRR, abs/1512.03385, 2015.\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDelving deep into\nrectiﬁers: Surpassing human-level performance on imagenet classiﬁcation.\nCoRR,\nabs/1502.01852, 2015.\n\n\n14\nSERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep\nresidual networks. CoRR, abs/1603.05027, 2016.\n[14] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger.\nDeep\nnetworks with stochastic depth. CoRR, abs/1603.09382, 2016.\n[15] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network\ntraining by reducing internal covariate shift. In David Blei and Francis Bach, editors,\nProceedings of the 32nd International Conference on Machine Learning (ICML-15),\npages 448–456. JMLR Workshop and Conference Proceedings, 2015.\n[16] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deep convo-\nlutional neural networks. In NIPS, 2012.\n[17] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.\nCifar-10 (canadian institute\nfor advanced research). 2012. URL http://www.cs.toronto.edu/~kriz/\ncifar.html.\n[18] Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Ben-\ngio. An empirical evaluation of deep architectures on problems with many factors of\nvariation. In Zoubin Ghahramani, editor, Proceedings of the 24th International Con-\nference on Machine Learning (ICML’07), pages 473–480. ACM, 2007.\n[19] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-Supervised Nets. 2014.\n[20] Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. CoRR, abs/1312.4400,\n2013.\n[21] Francisco Massa. Optnet - reducing memory usage in torch neural networks, 2016.\nURL https://github.com/fmassa/optimize-net.\n[22] Guido F. Montúfar, Razvan Pascanu, KyungHyun Cho, and Yoshua Bengio. On the\nnumber of linear regions of deep neural networks. In Advances in Neural Information\nProcessing Systems 27: Annual Conference on Neural Information Processing Systems\n2014, December 8-13 2014, Montreal, Quebec, Canada, pages 2924–2932, 2014.\n[23] Tapani Raiko, Harri Valpola, and Yann Lecun. Deep learning made easier by linear\ntransformations in perceptrons. In Neil D. Lawrence and Mark A. Girolami, editors,\nProceedings of the Fifteenth International Conference on Artiﬁcial Intelligence and\nStatistics (AISTATS-12), volume 22, pages 924–932, 2012.\n[24] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo\nGatta, and Yoshua Bengio. FitNets: Hints for thin deep nets. Technical Report Arxiv\nreport 1412.6550, arXiv, 2014.\n[25] J. Schmidhuber. Learning complex, extended sequences using the principle of history\ncompression. Neural Computation, 4(2):234–242, 1992.\n[26] K. Simonyan and A. Zisserman.\nVery deep convolutional networks for large-scale\nimage recognition. In ICLR, 2015.\n[27] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout:\nA simple way to prevent neural networks from overﬁtting. JMLR, 2014.\n\n\nSERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS\n15\n[28] Rupesh Kumar Srivastava, Klaus Greff, and Jürgen Schmidhuber. Highway networks.\nCoRR, abs/1505.00387, 2015.\n[29] Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. On the im-\nportance of initialization and momentum in deep learning. In Sanjoy Dasgupta and\nDavid Mcallester, editors, Proceedings of the 30th International Conference on Ma-\nchine Learning (ICML-13), volume 28, pages 1139–1147. JMLR Workshop and Con-\nference Proceedings, May 2013.\n[30] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke,\nand A. Rabinovich. Going deeper with convolutions. In CVPR, 2015.\n[31] Christian Szegedy, Sergey Ioffe, and Vincent Vanhoucke.\nInception-v4, inception-\nresnet and the impact of residual connections on learning. abs/1602.07261, 2016.\n[32] S. Zagoruyko, A. Lerer, T.-Y. Lin, P. O. Pinheiro, S. Gross, S. Chintala, and P. Dollár.\nA multipath network for object detection. In BMVC, 2016.\n"
}