{
  "filename": "2208.12398v1.pdf",
  "num_pages": 9,
  "pages": [
    "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n1\nFew-Shot Learning Meets Transformer: Uniﬁed\nQuery-Support Transformers for Few-Shot\nClassiﬁcation\nXixi Wang, Xiao Wang, Member, IEEE, Bo Jiang*, Bin Luo, Senior Member, IEEE\nAbstract—Few-shot classiﬁcation which aims to recognize un-\nseen classes using very limited samples has attracted more and\nmore attention. Usually, it is formulated as a metric learning\nproblem. The core issue of few-shot classiﬁcation is how to learn\n(1) consistent representations for images in both support and\nquery sets and (2) effective metric learning for images between\nsupport and query sets. In this paper, we show that the two\nchallenges can be well modeled simultaneously via a uniﬁed\nQuery-Support TransFormer (QSFormer) model. To be speciﬁc,\nthe proposed QSFormer involves global query-support sample\nTransformer (sampleFormer) branch and local patch Trans-\nformer (patchFormer) learning branch. sampleFormer aims to\ncapture the dependence of samples in support and query sets for\nimage representation. It adopts the Encoder, Decoder and Cross-\nAttention to respectively model the Support, Query (image) rep-\nresentation and Metric learning for few-shot classiﬁcation task.\nAlso, as a complementary to global learning branch, we adopt a\nlocal patch Transformer to extract structural representation for\neach image sample by capturing the long-range dependence of\nlocal image patches. In addition, a novel Cross-scale Interactive\nFeature Extractor (CIFE) is proposed to extract and fuse multi-\nscale CNN features as an effective backbone module for the\nproposed few-shot learning method. All modules are integrated\ninto a uniﬁed framework and trained in an end-to-end manner.\nExtensive experiments on four popular datasets demonstrate the\neffectiveness and superiority of the proposed QSFormer.\nIndex Terms—Few-Shot Learning, Transformer, Metric Learn-\ning, Deep Learning.\nI. INTRODUCTION\nC\nURRENT deep neural networks learn from large-scale\ntraining samples and achieve good performance on many\ntasks. However, in many scenarios, data collection and anno-\ntation is expensive and it is usually very challenging to collect\nenough data for the training of deep neural networks. The Few-\nshot classiﬁcation aims to recognize unseen/query classes by\nusing very limited seen/support samples has attracted more\nand more attention.\nMany deep learning methods [1]–[3] have been proposed\nto address few-shot learning problem. These methods can be\nroughly classiﬁed into three types, i.e., generation-based meth-\nods, optimization-based methods and metric-based methods.\nMetric-based methods are derived to distinguish support and\nquery samples by using some image representation and metric\nlearning techniques. As we know, the core issues for metric-\nbased few-shot classiﬁcation are two aspects: 1) How to learn\nThe authors are all from School of Computer Science and Technology,\nAnhui University, Hefei 230601, China\nCorresponding author: Bo Jiang\nQuery Set\nSupport Set\nWeight Sharing\nMetric Learning\nTransformer \nEncoder\nTransformer \nDecoder\nTransformer \nEncoder\nTransformer \nDecoder\nBackbone\n\nN\ncross-affinity\nFig. 1.\nIllustration of our proposed uniﬁed Query-Support Transformer for\nfew-shot learning. It models the feature engineering on query/support samples\nand metric learning simultaneously.\nconsistent representations for images in both support and query\nsets. 2) How to conduct effective metric learning for images\nbetween support and query sets. According to our observation,\nexisting works [3]–[7] usually ﬁrst employ Convolution Neural\nNetworks (CNNs) to learn image feature representation and\nthen use a metric function to directly compute the similarities\n(e.g., cosine) between query and support images for few-shot\nclassiﬁcation. The good performance can be achieved, how-\never, many recent studies [8], [9] demonstrate that CNN only\ncaptures the local relations well due to its limited receptive\nﬁeld. To address this issue, some researchers [10]–[12] propose\nto combine or replace CNN with Transformer networks to\nmodel the long-range relationships of local image patches and\nobtain better image representation results. However, they may\nstill obtain sub-optimal performance due to the following two\nreasons: 1) Existing works generally adopt Transformers (or\nCNN+Transformer) as the backbone network for engineering\neach image representation, which obviously ignores the in-\nherent relationships among samples in query and support sets\nfor image representation. 2) Existing works generally adopt\nthe two-stage learning scheme, i.e., ‘representation learning +\nmetric learning’. Although the two stages are usually learned\ntogether in an end-to-end manner, this decoupling way may\nlead to sub-optimal learning results.\nTo address these challenges, in this work, we propose a\nuniﬁed Query-Support Transformer architecture for few-shot\nlearning, termed QSFormer. The core of QSFormer is our\nnew design of query-support sample Transformer (named sam-\npleFormer) module, which aims to explore the relationships\nof samples for coupling sample representations and metric\nlearning of samples together in a uniﬁed module for few-\nshot classiﬁcation. To be speciﬁc, as shown in Figure 1, we\narXiv:2208.12398v1  [cs.CV]  26 Aug 2022\n",
    "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n2\ndexterously adopt the Encoder, Decoder and Cross-Attention\nin our sampleFormer architecture to model the Support, Query\n(image) representation and Metric learning in few-shot classi-\nﬁcation task, respectively. For the support branch, we represent\nall support images as a sequence of image tokens and feed\nthem into the Transformer encoder to enhance the support\nfeatures. For the query branch, it receives a sequence of\nquery image tokens to learn their representations. Meanwhile,\nit interacts with the previous support branch via the cross-\nattention for modeling the similarities/afﬁnities between query\nand support tokens, therefore, naturally achieving metric learn-\ning in the decoding procedure.\nBased on our newly proposed sampleFormer, we further\nextend it by introducing two additional new modules for high-\nperformance few-shot learning, including Cross-scale Interac-\ntive Feature Extractor (CIFE) and local patch Transformer\n(patchFormer) module. Speciﬁcally, as shown in Figure 2,\ngiven the query and support images, we ﬁrst use CIFE as\nthe backbone module to extract the image features. Then, the\nsampleFormer takes the embedded image tokens as input and\noutputs global metrics. Meanwhile, the local/patch correspon-\ndence of query-support image pairs is also considered using\nthe patchFormer. The global and local metrics are combined\nfor few-shot classiﬁcation. Note that, the whole network can\nbe optimized in an end-to-end way.\nTo sum up, the contributions of this paper can be summa-\nrized as follows:\n• We propose a uniﬁed Query-Support Transformer (termed\nQSFormer) for few-shot learning, which models the rep-\nresentation learning and metric learning simultaneously.\n• We propose a novel Sample Transformer module (sample-\nFormer) to capture the sample relationships in few-shot\nproblem setting. Also, we propose a patch Transformer\n(patchFormer) module for few-shot image representation\nand metric learning.\n• We propose a Cross-scale Interactive Feature Extractor\nfor image representation by considering the interaction\nof different CNN levels.\n• Extensive experiments on four widely used few-shot\nclassiﬁcation datasets demonstrate the effectiveness and\nsuperiority of our proposed method.\nII. RELATED WORK\nFew-shot Learning. Current few-shot learning algorithms\ncan be broadly divided into two categories: optimization-\nbased approaches [2], [6] and metric-based approaches [3],\n[4], [13], [14]. Our method is more relevant to the metric-\nbased approaches, which mainly focus on the representation\nlearning and metric learning of samples. Speciﬁcally, Sung\net al. [15] propose a Relation Network (RN) for few-shot\nlearning, which computes the relation scores between query\nexamples and the few examples of each new class to classify\nthe examples of new classes. Hou et al. [13] develop a Cross\nAttention Network, which highlights the target object regions\nto enhance the feature representation by producing cross at-\ntention maps for each feature. Zhang et al. [3] introduce Earth\nMover’s Distance to capture a structural distance between the\nlocal image representations for few-shot classiﬁcation. Xie\net al. [14] introduce a deep Brownian Distance Covariance\napproach to learn image representations and then use distance\nmetric for classiﬁcation.\nTransformer for Few-shot Classiﬁcation. Transformer [16]\nhas universal modeling capability because its core module self-\nattention learning mechanism. In recent years, Transformer has\nbeen employed by a large number of researchers for various\nvisual tasks, including object tracking [17], [18], object detec-\ntion [19], [20], object re-identiﬁcation [21], [22], multi-label\nclassiﬁcation [23], [24], Medical Image Segmentation [25],\n[26], and so on. For few-shot learning tasks, some works [10]–\n[12], [27]–[29] demonstrate that Transformer architecture is\nalso promising. For example, Ye et al. [27] develop a Few-\nShot Embedding Adaptation Transformer (FEAT) to instantiate\nset-to-set transformation and thus make instance embedding\ntask-speciﬁc for few-shot learning. Liu et al. [28] propose a\nUniversal Representation Transformer (URT) layer by combin-\ning feature representations from multiple domains together for\nmulti-domain few-shot classiﬁcation. Zhmoginov et al. [12] in-\ntroduce a transformer-based model, called HyperTransformer\n(HT), which encodes task-dependent variations in the weights\nof a small CNN model for few-shot learning. These works\nmainly employ Transformer architecture for representation\nlearning. Differently, in our work, we develop a Query-\nSupport Transformer (QSFormer) to accomplish both feature\nrepresentation and metric learning simultaneously.\nIII. THE PROPOSED METHOD\nThe purpose of few-shot classiﬁcation is to classify the\nunseen samples when only a small number of samples are\navailable. Many recent approaches [3], [13], [30], [31] indi-\ncate that the episode mechanism provides an effective way\nfor few-shot classiﬁcation task and we follow them in both\ntraining and testing phases. Formally, let Dtrain, Dval and\nDtest respectively represent meta-training, meta-validation and\nmeta-testing set, where Dtrain ∩Dval ∩Dtest = ∅. Taking C-\nway K-shot few-shot classiﬁcation task as an example, each\nepisode consists of support set X s = {(Xs\ni , Y s\ni )}ns\ni=1 and\nquery set X q = {(Xq\nj , Y q\nj )}nq\nj=1. Concretely, we randomly\nselect C classes and K labeled samples per class to form the\nsupport set X s, i.e., ns = C × K. Meanwhile, we randomly\nsample q samples per class to form the query set X q, i.e.,\nnq = C × q.\nAs shown in Figure 2, we propose a novel Query-Support\nTransformer (QSFormer) framework for few-shot learning,\nwhich contains the following four parts:\n• Cross-Scale Interactive Feature Extractor (CIFE):\nwe propose a cross-scale interactive feature extractor\nas backbone network to obtain the spatial enhanced\nsupport/query CNN feature representations.\n• Sample Transformer Module: we introduce a query-\nsupport sample Transformer (sampleFormer) module to\ncouple image sample representation and global metric\nlearning of samples together for few-shot learning.\n• Patch Transformer Module: we also propose a patch\nTransformer (patchFormer) module to model the context\n",
    "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n3\nSample Transformer Module\nEncoder\nDecoder\nScaled \nDot-Product \nAttention\nNorm\nScaled \nDot-Product \nAttention\nCross-\nAttention\nFFN\nNorm\nNorm\nFeed-Forward Network\nElement-wise Summation\nFFN\nNorm\nLayer Normalization\nMSA\nMulti-head Self-Attention\nImage Tokenize\nWeight Sharing\nCIFE\nCIFE\nPatch Transformer Module\nh\nConcat\nConcat\nLinear\nMSA\nPatch Tokenize\nScaled \nDot-Product \nAttention\nScaled \nDot-Product \nAttention\nConcat\nLinear\nScaled \nDot-Product \nAttention\nScaled \nDot-Product \nAttention\nh\nConcat\nLinear\nScaled \nDot-Product \nAttention\nh\nMSA\nCIFE\nCIFE\nLinear\nLinear Projection\ns\nq\ns\ns\ns\ns\nQ\ns\nK\ns\nV\nq\nQ\nq\nK\nq\nV\nq\nq\nq\nGAP\nPS\n......\n......\nGAP\nPS\nGlobal Average Pooling\nPartition Strategy\nWeight Sharing\nQuery Set\nSupport Set\nFew-shot \nClassification\n...\nGAP\nPS\n...\nq\nP\ns\nP\ng\nm\n   Global\nMetric  \n   Local\nMetric  l\nm\n(1\n)\ng\nl\nm\nm\nm\n\n\n=\n+\n−\nFig. 2. An overview of the proposed QSFormer framework, which mainly consists of Cross-scale Interactive Feature Extractor (CIFE), Sample Transformer\nModule, Patch Transformer Module, Metric Learning and Few-shot Classiﬁcation. More details can be found in Section III.\ncorrelation of patches in each image sample to conduct\nthe local metric learning between query-support sample\npairs.\n• Metric Learning and Few-shot Classiﬁcation: we ac-\nquire the ﬁnal metric by combining global metric ob-\ntained via sampleFormer and local metric obtained via\npatchFormer together and ﬁnal achieve few-shot classiﬁ-\ncation.\nBelow, we introduce the details of these modules.\nA. Cross-scale Interactive Feature Extractor\nWe introduce a novel Cross-scale Interactive Feature Ex-\ntractor (CIFE) as backbone module, which aims to obtain the\nego-context CNN feature representations for support and query\nsamples.\nAs shown in Figure 3, taking the support image set\nX s = {Xs\n1, Xs\n2, ..., Xs\nns} as inputs, we ﬁrst use the pre-\ntrained ResNet-12 to generate the initial multi-scale feature\nrepresentations Fs\nl ∈Rns×cl×hl×wl, l ∈{1, 2, 3, 4}, where\nns represents the number of support samples in each episode\nand cl, hl and wl denote the channel, height and width of\nsupport feature map in the l-th level respectively. Then, we\nemploy a Transformer architecture [16] consisting of multi-\nhead self-attention (MSA), layer normalization (LN), feed-\nforward network (FFN) and residual connection to achieve the\ninteraction of multi-scale features. Finally, we can obtain the\nInputs\nConvolution operation\n4\n4\nFeed-Forward Network\nElement-wise Summation\nFFN\nNorm Layer Normalization\nMSA\nMSA\nConv\nNorm\nConv\nMSA Multi-head Self-Attention\nNorm\nFFN\nNorm\nFFN\nNorm\nOutputs\nFig. 3.\nIllustration of Cross-scale Interactive Feature Extractor (CIFE) for\nfeature extraction.\nspatial enhanced feature representations for support samples\nas eFs = { eF s\n1 , eF s\n2 , · · · , eF s\nns} ∈Rns×c×h×w. Similarly, we\nobtain the spatial enhanced features for query samples as\neFq = { eF q\n1 , eF q\n2 , · · · , eF q\nnq} ∈Rnq×c×h×w. The parameters of\nCIFE are shared for support and query branches. In practice,\nwe empirically set c = 640 and h = w = 5.\nB. Sample Transformer Module\nTo achieve both image sample representation and metric\nlearning of samples in a uniﬁed module, we design a novel\nquery-support sample Transformer module, named sample-\nFormer. The proposed sampleFormer mainly consists of En-\ncoder and Decoder, as shown in Figure 2.\n",
    "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n4\nEncoder. The purpose of the Encoder is to mine the\nrelationships of samples in support set to obtain better support\nfeature representations. To this end, based on the aforemen-\ntioned support features eFs ∈Rns×c×h×w, we ﬁrst intro-\nduce image tokenize, which utilizes a global average pooling\nand reshape operation to gain the token sequence Hs =\n{Hs\n1, Hs\n2, · · · , Hs\nns} ∈Rns×c of support samples, where each\ntoken Hs\ni denotes a support image sample. As shown in\nFigure 2, we can see that the main component of encoder is\nattention mechanism, whose inputs are Query Qs ∈Rns×c,\nKey Ks ∈Rns×c, and Value Vs ∈Rns×c obtained by\nconducting three linear projections on Hs respectively. Next, it\nemploys dot-product operation to obtain a correlation/afﬁnity\nmatrix Attns→s(Qs, Ks) of different support samples as\nAttns→s(Qs, Ks) = Softmax(Qs(Ks)T\n√c\n)\n(1)\nwhere c denotes the dimension of support features. It learns the\nrepresentations for support samples by conducting the message\npassing operation as\nb\nHs = LN(Hs + Attns→s(Qs, Ks)Vs)\n(2)\nwhere LN(·) refers to layer normalization. Besides, we add\nFeed-Forward Network (FFN) [8] and residual operation to\nobtain the ﬁnal support sample representations as,\ne\nHs = LN( b\nHs + FFN( b\nHs))\n(3)\nwhere e\nHs = { eHs\n1, eHs\n2 · · · , eHs\nns} ∈Rns×c. ns denotes the\nnumber of support samples and c is the feature dimension.\nFFN consists of two fully-connection layers.\nDecoder. The Decoder aims to explore the dependence of\nsamples in query set to learn the representations for query\nsamples and also mines the intrinsic metrics of samples in\nquery and support sets. To be speciﬁc, it takes the afore-\nmentioned encoded support features e\nHs ∈Rns×c and query\nfeature embeddings\neFq ∈Rnq×c×h×w as its inputs. The\nimage tokenize is applied on eFq to obtain the initial query\ntoken sequence Hq = {Hq\n1, Hq\n2, · · · , Hq\nnq} ∈Rnq×c, where\neach token Hq\nj denotes a query image sample. Similar to\nthe Encoder branch, we ﬁrst leverage self-attention message\npassing mechanism to model the relationships among query\nsamples and learn representations for query samples as\nAttnq→q(Qq, Kq) = Softmax(Qq(Kq)T\n√c\n)\n(4)\nb\nHq = LN(Hq + Attnq→q(Qq, Kq)Vq)\n(5)\nwhere LN(·) denotes layer normalization.\nAfterward, based on the support features e\nHs and query\nfeatures\nb\nHq, we employ a cross-attention mechanism to\nexplore the relationships between support and query samples\nfor query sample representations. Speciﬁcally, it ﬁrst computes\nthe cross-afﬁnities between support and query samples as\nfollows\nAttnq→s(Qq, Ks) = Softmax(Qq(Ks)T )\n(6)\nThen, it learns query sample representations by aggregating\nthe information from support samples as follows\ne\nHq = b\nHq + LN(Attnq→s(Qq, Ks)Vs)\n(7)\nwhere e\nHq ∈Rnq×c and LN(·) denotes layer normalization.\nQq ∈Rnq×c is computed by conducting a linear projection on\nb\nHq. Ks ∈Rns×c and Vs ∈Rns×c are obtained by conducting\ntwo different linear projections on e\nHs, respectively.\nRemark. The above cross-afﬁnities Attnq→s(Qq, Ks) nat-\nurally reﬂect the similarities/afﬁnities between support and\nquery samples. In our work, we regard them as global metric\nmg for all support and query samples, i.e.,\nmg(X s, X q) = Attnq→s(Qq, Ks)\n(8)\nwhere mg(X s, X q) contains the similarities for all query-\nsupport sample pairs in each episode. For convenience, in\nthe following, we also use mg(Xs, Xq) to denote the metric\nbetween image Xs and Xq, where Xs ∈X s, Xq ∈X q.\nWe can utilize mg(Xs, Xq) for query sample classiﬁcation,\nas discussed in the following Section Metric Learning and\nFew-shot Classiﬁcation. Therefore, we can note that both\nquery/support sample representation and metric learning in\nfew-shot learning task are conducted simultaneously in our\nsampleFormer architecture. This is one main aspect of the\nproposed sampleFormer module.\nC. Patch Transformer Module\nAs a complementary to the above sampleFormer branch,\nwe also develop a query-support Patch Transformer Module\n(patchFormer) to capture the more visual content of each\nimage sample for local metric. As shown in Figure 2, patch-\nFormer mainly consists of multi-head self-attention (MSA)\nand residual connection. Here, we omit Feed-Forward Network\nused in regular Transformer [8] for simplicity consideration.\nThe parameters of MSA are shared on both support and query\nbranches.\nConcretely, for each input support sample Xs and query\nsample Xq, we ﬁrst obtain their feature embedding eF s ∈\nRc×h×w and\neF q\n∈Rc×h×w by using the above CIFE,\nfollowed by the patch tokenize [8] to obtain the initial\npatch token sequence for each support and query image,\ni.e., P s\n=\n{ps\n1, ps\n2, · · · , ps\nhw}\n∈\nRhw×c\nand P q\n=\n{pq\n1, pq\n2, · · · , pq\nhw} ∈Rhw×c. Then, we employ multi-head\nself-attention (MSA) [16] with shared weights and residual\noperation to transform the support and query image patch\nfeatures as\neP s = LN(P s + MSA(P s))\neP q = LN(P q + MSA(P q))\n(9)\nwhere LN(·) denotes layer normalization.\nBased\non\nthe\nabove\npatch\nrepresentations\neP s\n=\n{eps\n1, eps\n2, · · · , eps\nhw} and eP q\n= {epq\n1, epq\n2, · · · , epq\nhw}, we then\nadopt the Earth Mover’s Distance (EMD) [3], [32] to com-\npute their structural similarity. It ﬁrst computes the distance\nbetween all patch pairs (eps\ni, epq\nj) and then acquires the optimal\nmatching between patches of two images that have the min-\nimum distance cost. Finally, it returns the image-level metric\n",
    "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n5\nby aggregating the metrics of all matched patch pairs. In this\npaper, we denote this metric as local metric between support\nsample Xs and query sample Xq, i.e.,\nml(Xs, Xq) = EMD( eP q, eP s)\n(10)\nD. Metric Learning and Few-Shot Classiﬁcation\nGiven the support samples (Xs, Y s) ∈X s with known\nlabels and input query sample Xq ∈X q, few-shot classi-\nﬁcation aims to determine the label of the query sample.\nTo achieve this task, we ﬁrst obtain the sample-based global\nmetric mg(Xs, Xq) via Equ. (8) and patch-based local metric\nml(Xs, Xq) via Equ. (10) respectively and combine them\ntogether to obtain the ﬁnal metric/similarity between Xs and\nXq as\nm(Xs, Xq) = λmg(Xs, Xq) + (1 −λ)ml(Xs, Xq)\n(11)\nwhere λ ∈(0, 1) is a tradeoff parameter.\nThen, we can conduct few-shot classiﬁcation by using the\nnearest neighbor classiﬁcation strategy, i.e., the label of query\nXq is determined by the label Y s∗of the support sample\nXs∗that is most similar with query Xq, as used in previous\nworks [3], [4].\nLoss Function. In the training phase, we employ two loss\nfunctions for the proposed QSFormer. First, for the sample-\nFormer module, we speciﬁcally introduce a contrastive loss as\nsuggested in work [33], [34], which encourages the positive\nquery-support sample pairs with same label (i.e., Y s = Y q) to\nbe closing and the negative query-support sample pairs with\ndifferent labels (i.e., Y s ̸= Y q) are far away in each episode.\nThis loss function can be written as follows,\nLcl = −log\nP\nY s=Y q emg(Xs,Xq)\nP\nY s=Y q emg(Xs,Xq) +\nP\nY s̸=Y q emg(Xs,Xq)\n(12)\nwhere mg(Xs, Xq) is the global metric between query Xq and\nsupport sample Xs. The whole network is trained in an end-to-\nend way by minimizing the Cross-Entropy (CE) loss function\nLce [3]. Thus, the total loss function can be formulated as\nLtotal = αLce( ˆY q, Y q) + (1 −α)Lcl\n(13)\nwhere ˆY q is the label prediction obtained by our method and\nY q denotes the corresponding ground-truth label. α ∈(0, 1)\nis the balanced hyper-parameter.\nImplementation Details. To achieve a fair comparison,\nthe ResNet-12 [3], [6] with fully connected layers removed\nis adopted as the backbone module. It is ﬁrstly pre-trained\nfrom scratch and then use the episodic training based on\nmeta-learning framework by following works [3], [7]. We\nempirically conduct the feature interaction of the last two\nlevels in CIFE to obtain the enhanced sample features.\nWe randomly sample 50/1000/5000 episodes from the train-\ning/validation/testing set on four public datasets. We compute\nthe average accuracy and the corresponding 95% conﬁdence\ninterval to obtain the ﬁnal performances of four datasets.\nOur proposed method is implemented by using Python on\na server with a single 11G NVIDIA 2080Ti GPU. More\nhyper-parameter settings on four benchmarks for the proposed\nQSFormer are shown in Table VI.\nIV. EXPERIMENTS\nA. Datasets and Evaluation Metric\nTo verify our proposed QSFormer, we conduct extensive\nexperiments on four publicly popular datasets for few-shot\nclassiﬁcation task, including miniImageNet [4], tieredIm-\nageNet [44], Fewshot-CIFAR100 [36] and Caltech-UCSD\nBirds-200-2011 [45]. We also conduct cross-domain experi-\nments to evaluate the domain transfer ability of the proposed\nmodel. The recognition accuracy is adopted as the evaluation\nmetric for our experiments. More details of datasets descrip-\ntion are as follow.\nminiImageNet. This dataset is a sub-dataset of Ima-\ngeNet [46]. It contains a total of 100 classes with 600 samples\nin each class. As suggested in work [47], we divide these\nclasses into training set, validation set and testing set, which\nrespectively contains 64, 16 and 20 classes.\ntieredImageNet. It contains 608 classes from 34 super-\nclasses, with a total of 779,165 samples. Following [44], we\nsplit 34 super-classes into 20 super-classes (351 classes) for\nmeta-training, 6 super-classes (97 classes) for meta-validation\nand 8 super-classes (160 classes) for meta-testing.\nFC100. Fewshot-CIFAR100 is built upon the CIFAR100\ndataset for few-shot classiﬁcation task. It’s named FC100 for\nshort hereafter. It contains a total of 60,000 images from 100\nclasses. To reduce the information overlap, we group the 100\nclasses into 20 super-classes by following work [36]. Then,\nwe divide these super-classes into training set, validation set\nand testing set, which contains 12, 4 and 4 super-classes\nrespectively.\nCUB. Caltech-UCSD Birds-200-2011 dataset is an ex-\ntended vision of CUB-200 dataset. It’s termed CUB for short\nhereafter. CUB is originally presented in ﬁne-grained bird\nclassiﬁcation task. It contains the total of 11,788 images from\n200 classes. As suggested by [27], we divide 200 classes into\n100 classes for meta-training, 50 classes for meta-validation\nand 50 classes for meta-testing.\nminiImageNet →CUB. By following [6], we train a model\non miniImageNet dataset and evaluate on the CUB dataset\nto verify the transfer ability of model. In this experimental\nsetting, speciﬁcally, we use all 100 classes of miniImageNet,\nwith 600 samples per class for meta-training and use the meta-\ntesting set (50 classes) of CUB dataset for meta-testing.\nB. Comparison with State-of-the-art Methods\nAs shown in Table I, we report our results and compare with\nother state-of-the-art (SOTA) approaches on miniImageNet [4]\nand tieredImageNet [44] datasets. From this Table, we can ﬁnd\nthat the proposed QSFormer beats many SOTA models on the\nminiImageNet dataset. For example, QSFormer exceeds the\ntransformer-based HT [12] method by +11.14% and +11.46%\nin 1-shot and 5-shot tasks, respectively. For the attention\nmechanism based CAN [13], our model also outperforms it\non the 1-shot/5-shot task by +1.39%/+0.52%. Compared with\nFETA [27] that is also developed based on ResNet12 and\nTransformer, the proposed QSFormer has better results.\nFrom Table I, we can see that QSFormer achieves the best\nperformance on the tieredImageNet dataset, i.e., 72.47±0.31\n",
    "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n6\nTABLE I\n5-WAY RESULT COMPARISON OF OURS AND STATE-OF-THE-ART METHODS ON MINIIMAGENET AND TIEREDIMAGENET DATASETS. MOST RESULTS ARE\nFROM [3] OR THE ORIGINAL PAPERS. THE 1st, 2rd AND 3rd ARE RESPECTIVELY IN RED, BLUE AND GREEN. * DENOTES THIS METHOD IS REPRODUCED\nWITH OUR SETTINGS.\nMethod\nBackbone\nminiImagenet\ntieredImagenet\n1-shot\n5-shot\n1-shot\n5-shot\nDHL [35]\nConv4\n61.99 ± −\n78.71 ± −\n57.89 ± −\n73.62 ± −\ncosine classiﬁer [6]*\nResNet12\n59.64 ± 0.27\n75.80 ± 0.21\n55.87 ± 0.31\n80.92 ± 0.23\nTADAM [36]\nResNet12\n58.50 ± 0.30\n76.70 ± 0.30\n−\n−\nECM [37]\nResNet12\n59.00 ± −\n77.46 ± −\n63.99 ± −\n81.97 ± −\nTPN [38]\nResNet12\n59.46 ± −\n75.65 ± −\n59.91 ± 0.94\n73.30 ± 0.75\nProtoNet [5]*\nResNet12\n63.03 ± 0.29\n78.72 ± 0.21\n68.68 ± 0.34\n85.09 ± 0.23\nMTL [39]\nResNet12\n61.20 ± 1.80\n75.50 ± 0.80\n−\n−\nDC [40]\nResNet12\n62.53 ± 0.19\n79.77 ± 0.19\n−\n−\nMetaOptNet [41]\nResNet12\n62.64 ± 0.82\n78.63 ± 0.46\n65.99 ± 0.72\n81.56 ± 0.53\nMatchNet [4]*\nResNet12\n61.24 ± 0.29\n73.93 ± 0.23\n71.01 ± 0.33\n83.12 ± 0.24\nMeta-Baseline [7]\nResNet12\n63.17 ± 0.23\n79.26 ± 0.17\n68.62 ± 0.27\n83.74 ± 0.18\nCAN [13]\nResNet12\n63.85 ± 0.48\n79.44 ± 0.34\n69.89 ± 0.51\n84.23 ± 0.37\nPPA [42]\nWRN-28-10\n59.60 ± 0.41\n73.74 ± 0.19\n65.65 ± 0.92\n83.40 ± 0.65\nwDAE-GNN [43]\nWRN-28-10\n61.07 ± 0.15\n76.75 ± 0.11\n68.18 ± 0.16\n83.09 ± 0.12\nLEO [1]\nWRN-28-10\n61.76 ± 0.08\n77.59 ± 0.12\n66.33 ± 0.05\n81.44 ± 0.09\nFEAT [27]*\nResNet12\n64.75 ± 0.28\n79.96 ± 0.20\n71.34 ± 0.33\n85.28 ± 0.23\nHT [12]\nTransformer\n54.10 ± −\n68.50 ± −\n56.10 ± −\n73.30 ± −\nDeepEMD [3]*\nResNet12\n65.43 ± 0.28\n79.28 ± 0.20\n69.84 ± 0.32\n84.06 ± 0.23\nDeepBDC [14]*\nResNet12\n60.76 ± 0.28\n78.25 ± 0.20\n63.03 ± 0.31\n81.57 ± 0.22\nQSFormer (Ours)\nResNet12\n65.24 ± 0.28\n79.96 ± 0.20\n72.47 ± 0.31\n85.43 ± 0.22\nTABLE II\n5-WAY RESULT COMPARISON OF OURS AND STATE-OF-THE-ART METHODS\nON FEWSHOT-CIFAR100 DATASET. THE 1st, 2rd AND 3rd ARE\nRESPECTIVELY IN RED, BLUE AND GREEN. * DENOTES THIS METHOD IS\nREPRODUCED WITH OUR SETTINGS.\nMethod\n1-shot\n5-shot\ncosine classiﬁer [6]*\n39.47 ± 0.23\n56.29 ± 0.25\nFEAT [27]*\n42.28 ± 0.26\n56.37 ± 0.25\nTADAM [36]\n40.10 ± 0.40\n56.10 ± 0.40\nProtoNet [5]*\n40.91 ± 0.26\n56.66 ± 0.25\nMTL [39]\n45.10 ± 1.8\n57.60 ± 0.9\nDC [40]\n42.04 ± 0.17\n57.05 ± 0.16\nMetaOptNet [41]\n41.10 ± 0.60\n55.50 ± 0.60\nMatchNet [4]*\n41.90 ± 0.27\n54.41 ± 0.25\nTDE-FSL [48]\n44.61 ± 0.96\n57.93 ± 0.81\nDeepEMD [3]*\n45.58 ± 0.26\n62.08 ± 0.25\nDeepBDC [14]*\n43.57 ± 0.25\n59.49 ± 0.25\nQSFormer (Ours)\n46.51 ± 0.26\n61.58 ± 0.25\nand 85.43±0.22 in 1-shot and 5-shot tasks. It exceeds the\nCAN [13] by +2.58 and +1.2 points in 1-shot and 5-shot tasks.\nSimilar conclusions can also be drawn from the experimental\nresults of Fewshot-CIFAR100 [36] and CUB [45] datasets, as\nillustrated in Table II and Table III. All in all, the proposed QS-\nFormer attains SOTA performance on multiple FSL datasets,\nwhich fully demonstrates the effectiveness and advantages of\nour proposed QSFormer model.\nC. Ablation Study\nTo better understand the effectiveness of our proposed\nQSFormer, in this section, we conduct extensive ablation stud-\nies, including component analysis, similarity metric analysis,\ncross-domain analysis, etc.\nComponent Analysis. Our proposed QSFormer mainly\ncontains three components: Cross-scale Interactive Feature Ex-\ntractor (CIFE), Sample Transformer Module (sampleFormer)\nand Patch Transformer Module (patchFormer). The experi-\nmental results of ablation study are shown in Table V. We\nTABLE III\n5-WAY RESULT COMPARISON OF OURS AND STATE-OF-THE-ART METHODS\nON CALTECH-UCSD BIRDS-200-2011 DATASET. THE 1st, 2rd AND 3rd\nARE RESPECTIVELY IN RED, BLUE AND GREEN. * DENOTES THIS METHOD\nIS REPRODUCED WITH OUR SETTINGS.\nMethod\n1-shot\n5-shot\nMELR [30]\n70.26 ± 0.50\n85.01 ± 0.32\nIEPT [49]\n69.97 ± 0.49\n84.33 ± 0.33\nMVT [50]\n−\n85.35 ± 0.55\nFEAT [27]*\n75.00 ± 0.29\n86.24 ± 0.19\ncosine classiﬁer [6]*\n62.09 ± 0.29\n80.04 ± 0.21\nProtoNet [5]*\n70.93 ± 0.30\n85.55 ± 0.19\nMatchNet [4]*\n70.21 ± 0.30\n82.69 ± 0.22\nRelationNet [15]\n66.20 ± 0.99\n82.30 ± 0.58\nMAML [51]\n67.28 ± 1.08\n83.47 ± 0.59\nDEML [52]\n66.95 ± 1.06\n77.11 ± 0.78\nDeepEMD [3]*\n70.71 ± 0.30\n86.13 ± 0.19\nDeepBDC [14]*\n65.45 ± 0.29\n85.01 ± 0.19\nQSFormer (Ours)\n75.44 ± 0.29\n86.30 ± 0.19\nTABLE IV\nCROSS-DOMAIN EXPERIMENTS (miniImagenet →CUB). * DENOTES\nTHIS METHOD IS REPRODUCED WITH OUR SETTINGS. THE RED\nREPRESENTS THE BEST RESULTS AND BLUE DENOTES THE SECOND-BEST\nRESULTS.\nMethods\n1-shot\n5-shot\nProtoNet [5]\n50.01 ± 0.82\n72.02 ± 0.67\nMatchNet [4]\n51.65 ± 0.84\n69.14 ± 0.72\ncosine classiﬁer [6]\n44.17 ± 0.78\n69.01 ± 0.74\nBaseline [6]\n−\n65.57 ± 0.70\nBaseline++ [6]\n−\n62.04 ± 0.76\nFEAT [27]*\n52.67 ± 0.29\n72.65 ± 0.25\nDeepEMD [3]\n54.24 ± 0.86\n78.86 ± 0.65\nDeepBDC [14]*\n50.28 ± 0.27\n76.49 ± 0.23\nQSFormer (Ours)\n55.04 ± 0.29\n77.12 ± 0.24\nreproduce cosine classiﬁer method [6] consisting of CNN net-\nwork and cosine distance as the Baseline network for compari-\nson. From Table V, we can observe: (1) By comparing #1 with\n#2, the performance of Baseline network can be signiﬁcantly\nimproved with the help of CIFE, which demonstrates the\n",
    "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n7\nTABLE V\nABLATION STUDY FOR THE DIFFERENT COMPONENTS OF THE PROPOSED QSFORMER. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD.\nDifferent Components\nDatasets\n#\nBaseline\nCIFE\nsampleFormer\npatchFormer\nminiImageNet\ntieredImageNet\nFC100\nCUB\n1\n✓\n59.64 ± 0.27\n55.87 ± 0.31\n39.47 ± 0.23\n62.09 ± 0.29\n2\n✓\n✓\n61.15 ± 0.28\n70.73 ± 0.32\n41.54 ± 0.25\n65.95 ± 0.30\n3\n✓\n✓\n✓\n63.97 ± 0.28\n71.64 ± 0.32\n45.46 ± 0.26\n72.93 ± 0.29\n4\n✓\n✓\n✓\n✓\n65.24 ± 0.28\n72.47 ± 0.31\n46.51 ± 0.26\n75.44 ± 0.29\nTABLE VI\nHYPERPARAMETER SETTINGS OF OUR PROPOSED QSFORMER.\nHyper-parameters\nDatasets\nminiImageNet\ntieredImageNet\nFC100\nCUB\nminiImageNet →CUB\nOptimizer\nSGD\nSGD\nSGD\nSGD\nSGD\nInitial LR\n5e-4\n5e-4\n1e-4\n5e-4\n5e-4\nSteps of LR decay\n10\n10\n10\n10\n10\nCoefﬁcient of LR decay\n0.9\n0.5\n0.9\n0.95\n0.9\nN\n3\n3\n4\n2\n3\nNumber of Head\n10,8\n8,8\n8,1\n8,1\n10,8\ndropout rates\n0.5,0.5,0.5,0.1\n0.5,0.5,0.5,0.1\n0.5,0.5,0.5,0.1\n0.1,0.5,0.5,0.1\n0.5,0.5,0.5,0.1\nα\n0.7\n0.5\n0.5\n0.05\n0.7\nλ\n0.1\n0.1\n0.4\n0.3\n0.1\nEpochs\n100\n100\n50\n150\n100\nTABLE VII\nPERFORMANCE COMPARISON OF THE CLASSICAL METHODS BASED ON DIFFERENT METRIC LEARNING. * DENOTES THE COMPARISON METHODS IS\nREPRODUCED WITH OUR SETTING. THE BOLD BLACK REPRESENTS THE BEST RESULTS.\nMethods\nMetric\nminiImageNet\ntieredImageNet\nFC100\nCUB\ncosine classiﬁer [6]*\nCosine\n59.64 ± 0.27\n55.87 ± 0.31\n39.47 ± 0.23\n62.09 ± 0.29\nMatchNet [4]*\nCosine\n61.24 ± 0.29\n71.01 ± 0.33\n41.90 ± 0.27\n70.20 ± 0.30\nProtoNet [5]*\nEuclidean\n63.03 ± 0.29\n68.68 ± 0.34\n40.91 ± 0.26\n70.93 ± 0.30\nDeepEMD [3]*\nEMD\n65.43 ± 0.28\n69.84 ± 0.32\n45.58 ± 0.26\n70.71 ± 0.30\nQSFormer\nOurs\n65.24 ± 0.28\n72.47 ± 0.31\n46.51 ± 0.26\n75.44 ± 0.29\neffectiveness of CIFE. (2) By comparing #2 with #3, we can\nﬁnd that sampleFormer signiﬁcantly improves the performance\nof model based on #2, which indicates the effectiveness of\nsampleFormer module. (3) By adding patchFormer into #3,\nwe further improve the performance of whole network, which\nshows the effectiveness of patchFormer module. All these\nexperiments fully validate the effectiveness of each component\nin our proposed QSFormer framework.\nSimilarity Metric Analysis. To verify the effectiveness of\nthe proposed QSFormer on metric learning, we visualize the\nsimilarity distribution of Baseline and QSFormer on the more\nchallenging 5-way 1-shot task, as shown in Figure 4. For 5-\nway 1-shot task, each query sample generates the similarity\nresults of one positive query-support sample pair (i.e., “Q-\nS pos”) and four negative query-support sample pairs (i.e.,\n“Q-S neg”) during the metric learning process. To facilitate\nthe comparison of the similarity results of “Q-S pos” and\n“Q-S neg”, we average the similarity values of four “Q-S\nneg” corresponding to each query sample. For this experiment,\nwe perform 10 episodes, where each episode random selects\n15 × 5 = 75 query samples for classiﬁcation, i.e., we can get\nthe 75 × 10 = 750 similarity values of “Q-S pos” and “Q-\nS neg”, respectively. Subsequently, we count the number of\n“Q-S pos” and “Q-S neg” within a certain range according\nto the normalized similarity values and thus produce the\nsimilarity distribution as shown in Figure 4. We can observe\nthat: (1) the similarity values of “Q-S pos” obtained by the\nBaseline method are generally below 0.5, while “Q-S neg”\nare above 0.25. (2) In our proposed QSFormer, the similarity\nvalues of “Q-S pos” are mostly above 0.5, while “Q-S neg”\nare mostly below 0.25. Therefore, our proposed QSFormer\ncan separate positive and negative query-support sample pairs\nmore accurately.\nIn addition, we also compare our QSFormer with other\nmetric learning algorithms, including cosine classiﬁer [6],\nMatchNet [4], ProtoNet [5] and DeepEMD [3]. These com-\npared methods are reproduced with the same settings and\ntraining schemes as ours for a more fair comparison. As shown\nin Table VII, we can observe that our proposed method obtains\nthe best performance on four publicly popular datasets, which\nfully demonstrates the effectiveness and superiority of our\nproposed QSFormer. These experiments fully demonstrate the\neffectiveness of our proposed QSFormer for metric learning.\nCross-domain Analysis. To validate the transferable abil-\nity of our proposed QSFormer, we conduct a cross-domain\nexperiment by following [3], [6]. The training and testing\nare implemented on miniImagenet dataset and CUB dataset,\nrespectively. As shown in Table IV, our proposed QSFormer\nachieves the best performance on the 1-shot setting (55.04 ±\n0.29) and the second-best results on the 5-shot, i.e., 77.12\n± 0.24. These results demonstrate that the proposed QS-\nFormer learns the discriminative information across domains,\nand adaptively explores the correspondence of query-support\nsamples.\n",
    "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n8\n(a) Baseline\n(b) QSFormer\nFig. 4.\nComparison of similarity distribution between Baseline and our\nQSFormer. The similarities of “Q-S pos” become larger while the similarities\nof “Q-S neg” become smaller, which indicates they are more easily separated.\nFig. 5. Ablation study of two parameters (i.e., λ and N).\nParameter Analysis. There are two important parameters\nin our model, including the balanced parameter λ in Equ. (11)\nfor local and global metric, and the number of sampleFormer\nlayers N. In this section, we conduct experiments on the\nFC100 dataset on 5-way 1-shot task to check their inﬂuence.\nAs shown in Figure 5, we can observe that the performance\nis relatively stable when we slightly adjust the balanced\nparameter λ in the range of (0.2, 0.6). For the number N\nof sampleFormer layers, we can ﬁnd that our performance is\nincreasing continuously when the N is changing from 2 to 4.\nTherefore, we set λ = 0.4 and N = 4 for our experiments.\nV. CONCLUSION\nIn this paper, we propose a novel uniﬁed Query-Support\nTransformer (QSFormer) to deeply exploit the sample re-\nlationships in query and support sets for few-shot classiﬁ-\ncation task. QSFormer mainly contains sample Transformer\n(sampleFormer) module and patch Transformer (patchFormer)\nmodule. sampleFormer is designed to meet the problem setting\nof few-shot classiﬁcation, i.e., it couples the sample repre-\nsentation and metric learning between query and support sets\ntogether via a single Transformer architecture. Meanwhile, as\na complementary, patchFormer is also adopted to model the\nlocal structural metric between query and support samples. A\nnew CNN feature extractor (CIFE) is also proposed to pro-\nvide an effective CNN backbone for our approach. Extensive\nexperiments demonstrate the effectiveness and superiority of\nour proposed QSFormer approach.\nREFERENCES\n[1] A. A. Rusu, D. Rao, J. Sygnowski, O. Vinyals, R. Pascanu, S. Osindero,\nand R. Hadsell, “Meta-learning with latent embedding optimization,” in\nProceedings of the IEEE/CVF International Conference on Learning\nRepresentations, 2018, pp. 6907–6917.\n[2] M. A. Jamal and G.-J. Qi, “Task agnostic meta-learning for few-shot\nlearning,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2019, pp. 11 719–11 727.\n[3] C. Zhang, Y. Cai, G. Lin, and C. Shen, “Deepemd: Few-shot image\nclassiﬁcation with differentiable earth mover’s distance and structured\nclassiﬁers,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2020, pp. 12 203–12 213.\n[4] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra et al., “Matching\nnetworks for one shot learning,” Advances in Neural Information Pro-\ncessing Systems, vol. 29, 2016.\n[5] J. Snell, K. Swersky, and R. Zemel, “Prototypical networks for few-shot\nlearning,” Advances in Neural Information Processing Systems, vol. 30,\npp. 4080–4090, 2017.\n[6] W.-Y. Chen, Y.-C. Liu, Z. Kira, Y.-C. F. Wang, and J.-B. Huang, “A\ncloser look at few-shot classiﬁcation,” in Proceedings of the IEEE/CVF\nInternational Conference on Learning Representations, 2019.\n[7] Y. Chen, Z. Liu, H. Xu, T. Darrell, and X. Wang, “Meta-baseline:\nexploring simple meta-learning for few-shot learning,” in Proceedings\nof the IEEE/CVF International Conference on Computer Vision, 2021,\npp. 9062–9071.\n[8] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,” arXiv preprint arXiv:2010.11929, 2020.\n[9] Z. Peng, W. Huang, S. Gu, L. Xie, Y. Wang, J. Jiao, and Q. Ye,\n“Conformer: Local features coupling global representations for visual\nrecognition,” in Proceedings of the IEEE/CVF International Conference\non Computer Vision, 2021, pp. 367–376.\n[10] Y. He, W. Liang, D. Zhao, H.-Y. Zhou, W. Ge, Y. Yu, and W. Zhang,\n“Attribute surrogates learning and spectral tokens pooling in transform-\ners for few-shot learning,” arXiv preprint arXiv:2203.09064, 2022.\n[11] B. Dong, P. Zhou, S. Yan, and W. Zuo, “Self-promoted supervision for\nfew-shot transformer,” arXiv preprint arXiv:2203.07057, 2022.\n[12] A. Zhmoginov, M. Sandler, and M. Vladymyrov, “Hypertransformer:\nModel generation for supervised and semi-supervised few-shot learning,”\narXiv preprint arXiv:2201.04182, 2022.\n[13] R. Hou, H. Chang, B. Ma, S. Shan, and X. Chen, “Cross attention\nnetwork for few-shot classiﬁcation,” Advances in Neural Information\nProcessing Systems, vol. 32, 2019.\n[14] J. Xie, F. Long, J. Lv, Q. Wang, and P. Li, “Joint distribution matters:\nDeep brownian distance covariance for few-shot classiﬁcation,” in Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2022, pp. 7972–7981.\n[15] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and T. M. Hospedales,\n“Learning to compare: Relation network for few-shot learning,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2018, pp. 1199–1208.\n[16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\nNeural Information Processing Systems, vol. 30, 2017.\n[17] N. Wang, W. Zhou, J. Wang, and H. Li, “Transformer meets tracker: Ex-\nploiting temporal context for robust visual tracking,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2021, pp. 1571–1580.\n[18] E. Yu, Z. Li, S. Han, and H. Wang, “Relationtrack: Relation-aware mul-\ntiple object tracking with decoupled representation,” IEEE Transactions\non Multimedia, pp. 1–12, 2022.\n[19] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” in\nProceedings of European Conference on Computer Vision, 2020, pp.\n213–229.\n[20] T. Guan, J. Wang, S. Lan, R. Chandra, Z. Wu, L. Davis, and D. Manocha,\n“M3detr: Multi-representation, multi-scale, mutual-relation 3d object\ndetection with transformers,” in Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision, 2022, pp. 772–782.\n[21] S. Liao and L. Shao, “Transmatcher: Deep image matching through\ntransformers for generalizable person re-identiﬁcation,” Advances in\nNeural Information Processing Systems, vol. 34, pp. 1992–2003, 2021.\n[22] M. Jia, X. Cheng, S. Lu, and J. Zhang, “Learning disentangled represen-\ntation implicitly via transformer for occluded person re-identiﬁcation,”\nIEEE Transactions on Multimedia, pp. 1–11, 2022.\n[23] S. Liu, L. Zhang, X. Yang, H. Su, and J. Zhu, “Query2label: A\nsimple transformer way to multi-label classiﬁcation,” arXiv preprint\narXiv:2107.10834, 2021.\n[24] Z.-M. Chen, Q. Cui, B. Zhao, R. Song, X. Zhang, and O. Yoshie, “Sst:\nSpatial and semantic transformers for multi-label image recognition,”\nIEEE Transactions on Image Processing, vol. 31, pp. 2570–2583, 2022.\n",
    "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n9\n[25] J. M. J. Valanarasu, P. Oza, I. Hacihaliloglu, and V. M. Patel, “Medical\ntransformer: Gated axial-attention for medical image segmentation,” in\nProceedings of International Conference on Medical Image Computing\nand Computer-Assisted Intervention, 2021, pp. 36–46.\n[26] O. Petit, N. Thome, C. Rambour, L. Themyr, T. Collins, and L. Soler,\n“U-net transformer: Self and cross attention for medical image segmen-\ntation,” in Proceedings of International Workshop on Machine Learning\nin Medical Imaging, 2021, pp. 267–276.\n[27] H.-J. Ye, H. Hu, D.-C. Zhan, and F. Sha, “Few-shot learning via\nembedding adaptation with set-to-set functions,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2020, pp. 8808–8817.\n[28] L. Liu, W. Hamilton, G. Long, J. Jiang, and H. Larochelle, “A universal\nrepresentation transformer layer for few-shot image classiﬁcation,” in\nProceedings of the IEEE/CVF International Conference on Learning\nRepresentations, 2021.\n[29] B. Jiang, K. Zhao, and J. Tang, “Rgtransformer: Region-graph trans-\nformer for image representation and few-shot classiﬁcation,” IEEE\nSignal Processing Letters, vol. 29, pp. 792–796, 2022.\n[30] N. Fei, Z. Lu, T. Xiang, and S. Huang, “Melr: Meta-learning via model-\ning episode-level relationships for few-shot learning,” in Proceedings of\nthe IEEE/CVF International Conference on Learning Representations,\n2021.\n[31] J. Wang, B. Song, D. Wang, and H. Qin, “Two-stream network with\nphase map for few-shot classiﬁcation,” Neurocomputing, vol. 472, pp.\n45–53, 2022.\n[32] F. L. Hitchcock, “The distribution of a product from several sources to\nnumerous localities,” Journal of Mathematics and Physics, vol. 20, no.\n1-4, pp. 224–230, 1941.\n[33] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with\ncontrastive predictive coding,” arXiv preprint arXiv:1807.03748, 2018.\n[34] C. Liu, Y. Fu, C. Xu, S. Yang, J. Li, C. Wang, and L. Zhang, “Learning\na few-shot embedding model with contrastive learning,” in Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence, vol. 35, no. 10, 2021,\npp. 8635–8643.\n[35] X. Zhang, Y. Zhang, Z. Zhang, and J. Liu, “Discriminative learning of\nimaginary data for few-shot classiﬁcation,” Neurocomputing, vol. 467,\npp. 406–417, 2022.\n[36] B. Oreshkin, P. Rodr´ıguez L´opez, and A. Lacoste, “Tadam: Task\ndependent adaptive metric for improved few-shot learning,” Advances\nin Neural Information Processing Systems, vol. 31, pp. 719–729, 2018.\n[37] A. Ravichandran, R. Bhotika, and S. Soatto, “Few-shot learning with\nembedded class models and shot-free meta training,” in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision, 2019, pp.\n331–339.\n[38] Y. Liu, J. Lee, M. Park, S. Kim, E. Yang, S. J. Hwang, and Y. Yang,\n“Learning to propagate labels: Transductive propagation network for\nfew-shot learning,” in Proceedings of the IEEE/CVF International\nConference on Learning Representations, 2019.\n[39] Q. Sun, Y. Liu, T.-S. Chua, and B. Schiele, “Meta-transfer learning\nfor few-shot learning,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2019, pp. 403–412.\n[40] Y. Lifchitz, Y. Avrithis, S. Picard, and A. Bursuc, “Dense classiﬁcation\nand implanting for few-shot learning,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2019, pp.\n9258–9267.\n[41] K. Lee, S. Maji, A. Ravichandran, and S. Soatto, “Meta-learning with\ndifferentiable convex optimization,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2019, pp.\n10 657–10 665.\n[42] S. Qiao, C. Liu, W. Shen, and A. L. Yuille, “Few-shot image recognition\nby predicting parameters from activations,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2018, pp.\n7229–7238.\n[43] S. Gidaris and N. Komodakis, “Generating classiﬁcation weights with\ngnn denoising autoencoders for few-shot learning,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2019, pp. 21–30.\n[44] M. Ren, E. Triantaﬁllou, S. Ravi, J. Snell, K. Swersky, J. B. Tenenbaum,\nH. Larochelle, and R. S. Zemel, “Meta-learning for semi-supervised\nfew-shot classiﬁcation,” in Proceedings of the IEEE/CVF International\nConference on Learning Representations, 2018.\n[45] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie, “The\ncaltech-ucsd birds-200-2011 dataset,” 2011.\n[46] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., “Imagenet large\nscale visual recognition challenge,” International Journal of Computer\nVision, vol. 115, no. 3, pp. 211–252, 2015.\n[47] S. Ravi and H. Larochelle, “Optimization as a model for few-shot\nlearning,” 2017.\n[48] L. Xing, S. Shao, W. Liu, A. Han, X. Pan, and B.-D. Liu, “Learning task-\nspeciﬁc discriminative embeddings for few-shot image classiﬁcation,”\nNeurocomputing, vol. 488, pp. 1–13, 2022.\n[49] M. Zhang, J. Zhang, Z. Lu, T. Xiang, M. Ding, and S. Huang, “Iept:\nInstance-level and episode-level pretext tasks for few-shot learning,” in\nProceedings of the IEEE/CVF International Conference on Learning\nRepresentations, 2021.\n[50] S.-J. Park, S. Han, J.-W. Baek, I. Kim, J. Song, H. B. Lee, J.-J. Han,\nand S. J. Hwang, “Meta variance transfer: Learning to augment from\nthe others,” in Proceedings of the IEEE/CVF International Conference\non Machine Learning, 2020, pp. 7510–7520.\n[51] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for\nfast adaptation of deep networks,” in Proceedings of the IEEE/CVF\nInternational Conference on Machine Learning, 2017, pp. 1126–1135.\n[52] F. Zhou, B. Wu, and Z. Li, “Deep meta-learning: Learning to learn in\nthe concept space,” arXiv preprint arXiv:1802.03596, 2018.\n"
  ],
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n1\nFew-Shot Learning Meets Transformer: Uniﬁed\nQuery-Support Transformers for Few-Shot\nClassiﬁcation\nXixi Wang, Xiao Wang, Member, IEEE, Bo Jiang*, Bin Luo, Senior Member, IEEE\nAbstract—Few-shot classiﬁcation which aims to recognize un-\nseen classes using very limited samples has attracted more and\nmore attention. Usually, it is formulated as a metric learning\nproblem. The core issue of few-shot classiﬁcation is how to learn\n(1) consistent representations for images in both support and\nquery sets and (2) effective metric learning for images between\nsupport and query sets. In this paper, we show that the two\nchallenges can be well modeled simultaneously via a uniﬁed\nQuery-Support TransFormer (QSFormer) model. To be speciﬁc,\nthe proposed QSFormer involves global query-support sample\nTransformer (sampleFormer) branch and local patch Trans-\nformer (patchFormer) learning branch. sampleFormer aims to\ncapture the dependence of samples in support and query sets for\nimage representation. It adopts the Encoder, Decoder and Cross-\nAttention to respectively model the Support, Query (image) rep-\nresentation and Metric learning for few-shot classiﬁcation task.\nAlso, as a complementary to global learning branch, we adopt a\nlocal patch Transformer to extract structural representation for\neach image sample by capturing the long-range dependence of\nlocal image patches. In addition, a novel Cross-scale Interactive\nFeature Extractor (CIFE) is proposed to extract and fuse multi-\nscale CNN features as an effective backbone module for the\nproposed few-shot learning method. All modules are integrated\ninto a uniﬁed framework and trained in an end-to-end manner.\nExtensive experiments on four popular datasets demonstrate the\neffectiveness and superiority of the proposed QSFormer.\nIndex Terms—Few-Shot Learning, Transformer, Metric Learn-\ning, Deep Learning.\nI. INTRODUCTION\nC\nURRENT deep neural networks learn from large-scale\ntraining samples and achieve good performance on many\ntasks. However, in many scenarios, data collection and anno-\ntation is expensive and it is usually very challenging to collect\nenough data for the training of deep neural networks. The Few-\nshot classiﬁcation aims to recognize unseen/query classes by\nusing very limited seen/support samples has attracted more\nand more attention.\nMany deep learning methods [1]–[3] have been proposed\nto address few-shot learning problem. These methods can be\nroughly classiﬁed into three types, i.e., generation-based meth-\nods, optimization-based methods and metric-based methods.\nMetric-based methods are derived to distinguish support and\nquery samples by using some image representation and metric\nlearning techniques. As we know, the core issues for metric-\nbased few-shot classiﬁcation are two aspects: 1) How to learn\nThe authors are all from School of Computer Science and Technology,\nAnhui University, Hefei 230601, China\nCorresponding author: Bo Jiang\nQuery Set\nSupport Set\nWeight Sharing\nMetric Learning\nTransformer \nEncoder\nTransformer \nDecoder\nTransformer \nEncoder\nTransformer \nDecoder\nBackbone\n\nN\ncross-affinity\nFig. 1.\nIllustration of our proposed uniﬁed Query-Support Transformer for\nfew-shot learning. It models the feature engineering on query/support samples\nand metric learning simultaneously.\nconsistent representations for images in both support and query\nsets. 2) How to conduct effective metric learning for images\nbetween support and query sets. According to our observation,\nexisting works [3]–[7] usually ﬁrst employ Convolution Neural\nNetworks (CNNs) to learn image feature representation and\nthen use a metric function to directly compute the similarities\n(e.g., cosine) between query and support images for few-shot\nclassiﬁcation. The good performance can be achieved, how-\never, many recent studies [8], [9] demonstrate that CNN only\ncaptures the local relations well due to its limited receptive\nﬁeld. To address this issue, some researchers [10]–[12] propose\nto combine or replace CNN with Transformer networks to\nmodel the long-range relationships of local image patches and\nobtain better image representation results. However, they may\nstill obtain sub-optimal performance due to the following two\nreasons: 1) Existing works generally adopt Transformers (or\nCNN+Transformer) as the backbone network for engineering\neach image representation, which obviously ignores the in-\nherent relationships among samples in query and support sets\nfor image representation. 2) Existing works generally adopt\nthe two-stage learning scheme, i.e., ‘representation learning +\nmetric learning’. Although the two stages are usually learned\ntogether in an end-to-end manner, this decoupling way may\nlead to sub-optimal learning results.\nTo address these challenges, in this work, we propose a\nuniﬁed Query-Support Transformer architecture for few-shot\nlearning, termed QSFormer. The core of QSFormer is our\nnew design of query-support sample Transformer (named sam-\npleFormer) module, which aims to explore the relationships\nof samples for coupling sample representations and metric\nlearning of samples together in a uniﬁed module for few-\nshot classiﬁcation. To be speciﬁc, as shown in Figure 1, we\narXiv:2208.12398v1  [cs.CV]  26 Aug 2022\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n2\ndexterously adopt the Encoder, Decoder and Cross-Attention\nin our sampleFormer architecture to model the Support, Query\n(image) representation and Metric learning in few-shot classi-\nﬁcation task, respectively. For the support branch, we represent\nall support images as a sequence of image tokens and feed\nthem into the Transformer encoder to enhance the support\nfeatures. For the query branch, it receives a sequence of\nquery image tokens to learn their representations. Meanwhile,\nit interacts with the previous support branch via the cross-\nattention for modeling the similarities/afﬁnities between query\nand support tokens, therefore, naturally achieving metric learn-\ning in the decoding procedure.\nBased on our newly proposed sampleFormer, we further\nextend it by introducing two additional new modules for high-\nperformance few-shot learning, including Cross-scale Interac-\ntive Feature Extractor (CIFE) and local patch Transformer\n(patchFormer) module. Speciﬁcally, as shown in Figure 2,\ngiven the query and support images, we ﬁrst use CIFE as\nthe backbone module to extract the image features. Then, the\nsampleFormer takes the embedded image tokens as input and\noutputs global metrics. Meanwhile, the local/patch correspon-\ndence of query-support image pairs is also considered using\nthe patchFormer. The global and local metrics are combined\nfor few-shot classiﬁcation. Note that, the whole network can\nbe optimized in an end-to-end way.\nTo sum up, the contributions of this paper can be summa-\nrized as follows:\n• We propose a uniﬁed Query-Support Transformer (termed\nQSFormer) for few-shot learning, which models the rep-\nresentation learning and metric learning simultaneously.\n• We propose a novel Sample Transformer module (sample-\nFormer) to capture the sample relationships in few-shot\nproblem setting. Also, we propose a patch Transformer\n(patchFormer) module for few-shot image representation\nand metric learning.\n• We propose a Cross-scale Interactive Feature Extractor\nfor image representation by considering the interaction\nof different CNN levels.\n• Extensive experiments on four widely used few-shot\nclassiﬁcation datasets demonstrate the effectiveness and\nsuperiority of our proposed method.\nII. RELATED WORK\nFew-shot Learning. Current few-shot learning algorithms\ncan be broadly divided into two categories: optimization-\nbased approaches [2], [6] and metric-based approaches [3],\n[4], [13], [14]. Our method is more relevant to the metric-\nbased approaches, which mainly focus on the representation\nlearning and metric learning of samples. Speciﬁcally, Sung\net al. [15] propose a Relation Network (RN) for few-shot\nlearning, which computes the relation scores between query\nexamples and the few examples of each new class to classify\nthe examples of new classes. Hou et al. [13] develop a Cross\nAttention Network, which highlights the target object regions\nto enhance the feature representation by producing cross at-\ntention maps for each feature. Zhang et al. [3] introduce Earth\nMover’s Distance to capture a structural distance between the\nlocal image representations for few-shot classiﬁcation. Xie\net al. [14] introduce a deep Brownian Distance Covariance\napproach to learn image representations and then use distance\nmetric for classiﬁcation.\nTransformer for Few-shot Classiﬁcation. Transformer [16]\nhas universal modeling capability because its core module self-\nattention learning mechanism. In recent years, Transformer has\nbeen employed by a large number of researchers for various\nvisual tasks, including object tracking [17], [18], object detec-\ntion [19], [20], object re-identiﬁcation [21], [22], multi-label\nclassiﬁcation [23], [24], Medical Image Segmentation [25],\n[26], and so on. For few-shot learning tasks, some works [10]–\n[12], [27]–[29] demonstrate that Transformer architecture is\nalso promising. For example, Ye et al. [27] develop a Few-\nShot Embedding Adaptation Transformer (FEAT) to instantiate\nset-to-set transformation and thus make instance embedding\ntask-speciﬁc for few-shot learning. Liu et al. [28] propose a\nUniversal Representation Transformer (URT) layer by combin-\ning feature representations from multiple domains together for\nmulti-domain few-shot classiﬁcation. Zhmoginov et al. [12] in-\ntroduce a transformer-based model, called HyperTransformer\n(HT), which encodes task-dependent variations in the weights\nof a small CNN model for few-shot learning. These works\nmainly employ Transformer architecture for representation\nlearning. Differently, in our work, we develop a Query-\nSupport Transformer (QSFormer) to accomplish both feature\nrepresentation and metric learning simultaneously.\nIII. THE PROPOSED METHOD\nThe purpose of few-shot classiﬁcation is to classify the\nunseen samples when only a small number of samples are\navailable. Many recent approaches [3], [13], [30], [31] indi-\ncate that the episode mechanism provides an effective way\nfor few-shot classiﬁcation task and we follow them in both\ntraining and testing phases. Formally, let Dtrain, Dval and\nDtest respectively represent meta-training, meta-validation and\nmeta-testing set, where Dtrain ∩Dval ∩Dtest = ∅. Taking C-\nway K-shot few-shot classiﬁcation task as an example, each\nepisode consists of support set X s = {(Xs\ni , Y s\ni )}ns\ni=1 and\nquery set X q = {(Xq\nj , Y q\nj )}nq\nj=1. Concretely, we randomly\nselect C classes and K labeled samples per class to form the\nsupport set X s, i.e., ns = C × K. Meanwhile, we randomly\nsample q samples per class to form the query set X q, i.e.,\nnq = C × q.\nAs shown in Figure 2, we propose a novel Query-Support\nTransformer (QSFormer) framework for few-shot learning,\nwhich contains the following four parts:\n• Cross-Scale Interactive Feature Extractor (CIFE):\nwe propose a cross-scale interactive feature extractor\nas backbone network to obtain the spatial enhanced\nsupport/query CNN feature representations.\n• Sample Transformer Module: we introduce a query-\nsupport sample Transformer (sampleFormer) module to\ncouple image sample representation and global metric\nlearning of samples together for few-shot learning.\n• Patch Transformer Module: we also propose a patch\nTransformer (patchFormer) module to model the context\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n3\nSample Transformer Module\nEncoder\nDecoder\nScaled \nDot-Product \nAttention\nNorm\nScaled \nDot-Product \nAttention\nCross-\nAttention\nFFN\nNorm\nNorm\nFeed-Forward Network\nElement-wise Summation\nFFN\nNorm\nLayer Normalization\nMSA\nMulti-head Self-Attention\nImage Tokenize\nWeight Sharing\nCIFE\nCIFE\nPatch Transformer Module\nh\nConcat\nConcat\nLinear\nMSA\nPatch Tokenize\nScaled \nDot-Product \nAttention\nScaled \nDot-Product \nAttention\nConcat\nLinear\nScaled \nDot-Product \nAttention\nScaled \nDot-Product \nAttention\nh\nConcat\nLinear\nScaled \nDot-Product \nAttention\nh\nMSA\nCIFE\nCIFE\nLinear\nLinear Projection\ns\nq\ns\ns\ns\ns\nQ\ns\nK\ns\nV\nq\nQ\nq\nK\nq\nV\nq\nq\nq\nGAP\nPS\n......\n......\nGAP\nPS\nGlobal Average Pooling\nPartition Strategy\nWeight Sharing\nQuery Set\nSupport Set\nFew-shot \nClassification\n...\nGAP\nPS\n...\nq\nP\ns\nP\ng\nm\n   Global\nMetric  \n   Local\nMetric  l\nm\n(1\n)\ng\nl\nm\nm\nm\n\n\n=\n+\n−\nFig. 2. An overview of the proposed QSFormer framework, which mainly consists of Cross-scale Interactive Feature Extractor (CIFE), Sample Transformer\nModule, Patch Transformer Module, Metric Learning and Few-shot Classiﬁcation. More details can be found in Section III.\ncorrelation of patches in each image sample to conduct\nthe local metric learning between query-support sample\npairs.\n• Metric Learning and Few-shot Classiﬁcation: we ac-\nquire the ﬁnal metric by combining global metric ob-\ntained via sampleFormer and local metric obtained via\npatchFormer together and ﬁnal achieve few-shot classiﬁ-\ncation.\nBelow, we introduce the details of these modules.\nA. Cross-scale Interactive Feature Extractor\nWe introduce a novel Cross-scale Interactive Feature Ex-\ntractor (CIFE) as backbone module, which aims to obtain the\nego-context CNN feature representations for support and query\nsamples.\nAs shown in Figure 3, taking the support image set\nX s = {Xs\n1, Xs\n2, ..., Xs\nns} as inputs, we ﬁrst use the pre-\ntrained ResNet-12 to generate the initial multi-scale feature\nrepresentations Fs\nl ∈Rns×cl×hl×wl, l ∈{1, 2, 3, 4}, where\nns represents the number of support samples in each episode\nand cl, hl and wl denote the channel, height and width of\nsupport feature map in the l-th level respectively. Then, we\nemploy a Transformer architecture [16] consisting of multi-\nhead self-attention (MSA), layer normalization (LN), feed-\nforward network (FFN) and residual connection to achieve the\ninteraction of multi-scale features. Finally, we can obtain the\nInputs\nConvolution operation\n4\n4\nFeed-Forward Network\nElement-wise Summation\nFFN\nNorm Layer Normalization\nMSA\nMSA\nConv\nNorm\nConv\nMSA Multi-head Self-Attention\nNorm\nFFN\nNorm\nFFN\nNorm\nOutputs\nFig. 3.\nIllustration of Cross-scale Interactive Feature Extractor (CIFE) for\nfeature extraction.\nspatial enhanced feature representations for support samples\nas eFs = { eF s\n1 , eF s\n2 , · · · , eF s\nns} ∈Rns×c×h×w. Similarly, we\nobtain the spatial enhanced features for query samples as\neFq = { eF q\n1 , eF q\n2 , · · · , eF q\nnq} ∈Rnq×c×h×w. The parameters of\nCIFE are shared for support and query branches. In practice,\nwe empirically set c = 640 and h = w = 5.\nB. Sample Transformer Module\nTo achieve both image sample representation and metric\nlearning of samples in a uniﬁed module, we design a novel\nquery-support sample Transformer module, named sample-\nFormer. The proposed sampleFormer mainly consists of En-\ncoder and Decoder, as shown in Figure 2.\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n4\nEncoder. The purpose of the Encoder is to mine the\nrelationships of samples in support set to obtain better support\nfeature representations. To this end, based on the aforemen-\ntioned support features eFs ∈Rns×c×h×w, we ﬁrst intro-\nduce image tokenize, which utilizes a global average pooling\nand reshape operation to gain the token sequence Hs =\n{Hs\n1, Hs\n2, · · · , Hs\nns} ∈Rns×c of support samples, where each\ntoken Hs\ni denotes a support image sample. As shown in\nFigure 2, we can see that the main component of encoder is\nattention mechanism, whose inputs are Query Qs ∈Rns×c,\nKey Ks ∈Rns×c, and Value Vs ∈Rns×c obtained by\nconducting three linear projections on Hs respectively. Next, it\nemploys dot-product operation to obtain a correlation/afﬁnity\nmatrix Attns→s(Qs, Ks) of different support samples as\nAttns→s(Qs, Ks) = Softmax(Qs(Ks)T\n√c\n)\n(1)\nwhere c denotes the dimension of support features. It learns the\nrepresentations for support samples by conducting the message\npassing operation as\nb\nHs = LN(Hs + Attns→s(Qs, Ks)Vs)\n(2)\nwhere LN(·) refers to layer normalization. Besides, we add\nFeed-Forward Network (FFN) [8] and residual operation to\nobtain the ﬁnal support sample representations as,\ne\nHs = LN( b\nHs + FFN( b\nHs))\n(3)\nwhere e\nHs = { eHs\n1, eHs\n2 · · · , eHs\nns} ∈Rns×c. ns denotes the\nnumber of support samples and c is the feature dimension.\nFFN consists of two fully-connection layers.\nDecoder. The Decoder aims to explore the dependence of\nsamples in query set to learn the representations for query\nsamples and also mines the intrinsic metrics of samples in\nquery and support sets. To be speciﬁc, it takes the afore-\nmentioned encoded support features e\nHs ∈Rns×c and query\nfeature embeddings\neFq ∈Rnq×c×h×w as its inputs. The\nimage tokenize is applied on eFq to obtain the initial query\ntoken sequence Hq = {Hq\n1, Hq\n2, · · · , Hq\nnq} ∈Rnq×c, where\neach token Hq\nj denotes a query image sample. Similar to\nthe Encoder branch, we ﬁrst leverage self-attention message\npassing mechanism to model the relationships among query\nsamples and learn representations for query samples as\nAttnq→q(Qq, Kq) = Softmax(Qq(Kq)T\n√c\n)\n(4)\nb\nHq = LN(Hq + Attnq→q(Qq, Kq)Vq)\n(5)\nwhere LN(·) denotes layer normalization.\nAfterward, based on the support features e\nHs and query\nfeatures\nb\nHq, we employ a cross-attention mechanism to\nexplore the relationships between support and query samples\nfor query sample representations. Speciﬁcally, it ﬁrst computes\nthe cross-afﬁnities between support and query samples as\nfollows\nAttnq→s(Qq, Ks) = Softmax(Qq(Ks)T )\n(6)\nThen, it learns query sample representations by aggregating\nthe information from support samples as follows\ne\nHq = b\nHq + LN(Attnq→s(Qq, Ks)Vs)\n(7)\nwhere e\nHq ∈Rnq×c and LN(·) denotes layer normalization.\nQq ∈Rnq×c is computed by conducting a linear projection on\nb\nHq. Ks ∈Rns×c and Vs ∈Rns×c are obtained by conducting\ntwo different linear projections on e\nHs, respectively.\nRemark. The above cross-afﬁnities Attnq→s(Qq, Ks) nat-\nurally reﬂect the similarities/afﬁnities between support and\nquery samples. In our work, we regard them as global metric\nmg for all support and query samples, i.e.,\nmg(X s, X q) = Attnq→s(Qq, Ks)\n(8)\nwhere mg(X s, X q) contains the similarities for all query-\nsupport sample pairs in each episode. For convenience, in\nthe following, we also use mg(Xs, Xq) to denote the metric\nbetween image Xs and Xq, where Xs ∈X s, Xq ∈X q.\nWe can utilize mg(Xs, Xq) for query sample classiﬁcation,\nas discussed in the following Section Metric Learning and\nFew-shot Classiﬁcation. Therefore, we can note that both\nquery/support sample representation and metric learning in\nfew-shot learning task are conducted simultaneously in our\nsampleFormer architecture. This is one main aspect of the\nproposed sampleFormer module.\nC. Patch Transformer Module\nAs a complementary to the above sampleFormer branch,\nwe also develop a query-support Patch Transformer Module\n(patchFormer) to capture the more visual content of each\nimage sample for local metric. As shown in Figure 2, patch-\nFormer mainly consists of multi-head self-attention (MSA)\nand residual connection. Here, we omit Feed-Forward Network\nused in regular Transformer [8] for simplicity consideration.\nThe parameters of MSA are shared on both support and query\nbranches.\nConcretely, for each input support sample Xs and query\nsample Xq, we ﬁrst obtain their feature embedding eF s ∈\nRc×h×w and\neF q\n∈Rc×h×w by using the above CIFE,\nfollowed by the patch tokenize [8] to obtain the initial\npatch token sequence for each support and query image,\ni.e., P s\n=\n{ps\n1, ps\n2, · · · , ps\nhw}\n∈\nRhw×c\nand P q\n=\n{pq\n1, pq\n2, · · · , pq\nhw} ∈Rhw×c. Then, we employ multi-head\nself-attention (MSA) [16] with shared weights and residual\noperation to transform the support and query image patch\nfeatures as\neP s = LN(P s + MSA(P s))\neP q = LN(P q + MSA(P q))\n(9)\nwhere LN(·) denotes layer normalization.\nBased\non\nthe\nabove\npatch\nrepresentations\neP s\n=\n{eps\n1, eps\n2, · · · , eps\nhw} and eP q\n= {epq\n1, epq\n2, · · · , epq\nhw}, we then\nadopt the Earth Mover’s Distance (EMD) [3], [32] to com-\npute their structural similarity. It ﬁrst computes the distance\nbetween all patch pairs (eps\ni, epq\nj) and then acquires the optimal\nmatching between patches of two images that have the min-\nimum distance cost. Finally, it returns the image-level metric\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n5\nby aggregating the metrics of all matched patch pairs. In this\npaper, we denote this metric as local metric between support\nsample Xs and query sample Xq, i.e.,\nml(Xs, Xq) = EMD( eP q, eP s)\n(10)\nD. Metric Learning and Few-Shot Classiﬁcation\nGiven the support samples (Xs, Y s) ∈X s with known\nlabels and input query sample Xq ∈X q, few-shot classi-\nﬁcation aims to determine the label of the query sample.\nTo achieve this task, we ﬁrst obtain the sample-based global\nmetric mg(Xs, Xq) via Equ. (8) and patch-based local metric\nml(Xs, Xq) via Equ. (10) respectively and combine them\ntogether to obtain the ﬁnal metric/similarity between Xs and\nXq as\nm(Xs, Xq) = λmg(Xs, Xq) + (1 −λ)ml(Xs, Xq)\n(11)\nwhere λ ∈(0, 1) is a tradeoff parameter.\nThen, we can conduct few-shot classiﬁcation by using the\nnearest neighbor classiﬁcation strategy, i.e., the label of query\nXq is determined by the label Y s∗of the support sample\nXs∗that is most similar with query Xq, as used in previous\nworks [3], [4].\nLoss Function. In the training phase, we employ two loss\nfunctions for the proposed QSFormer. First, for the sample-\nFormer module, we speciﬁcally introduce a contrastive loss as\nsuggested in work [33], [34], which encourages the positive\nquery-support sample pairs with same label (i.e., Y s = Y q) to\nbe closing and the negative query-support sample pairs with\ndifferent labels (i.e., Y s ̸= Y q) are far away in each episode.\nThis loss function can be written as follows,\nLcl = −log\nP\nY s=Y q emg(Xs,Xq)\nP\nY s=Y q emg(Xs,Xq) +\nP\nY s̸=Y q emg(Xs,Xq)\n(12)\nwhere mg(Xs, Xq) is the global metric between query Xq and\nsupport sample Xs. The whole network is trained in an end-to-\nend way by minimizing the Cross-Entropy (CE) loss function\nLce [3]. Thus, the total loss function can be formulated as\nLtotal = αLce( ˆY q, Y q) + (1 −α)Lcl\n(13)\nwhere ˆY q is the label prediction obtained by our method and\nY q denotes the corresponding ground-truth label. α ∈(0, 1)\nis the balanced hyper-parameter.\nImplementation Details. To achieve a fair comparison,\nthe ResNet-12 [3], [6] with fully connected layers removed\nis adopted as the backbone module. It is ﬁrstly pre-trained\nfrom scratch and then use the episodic training based on\nmeta-learning framework by following works [3], [7]. We\nempirically conduct the feature interaction of the last two\nlevels in CIFE to obtain the enhanced sample features.\nWe randomly sample 50/1000/5000 episodes from the train-\ning/validation/testing set on four public datasets. We compute\nthe average accuracy and the corresponding 95% conﬁdence\ninterval to obtain the ﬁnal performances of four datasets.\nOur proposed method is implemented by using Python on\na server with a single 11G NVIDIA 2080Ti GPU. More\nhyper-parameter settings on four benchmarks for the proposed\nQSFormer are shown in Table VI.\nIV. EXPERIMENTS\nA. Datasets and Evaluation Metric\nTo verify our proposed QSFormer, we conduct extensive\nexperiments on four publicly popular datasets for few-shot\nclassiﬁcation task, including miniImageNet [4], tieredIm-\nageNet [44], Fewshot-CIFAR100 [36] and Caltech-UCSD\nBirds-200-2011 [45]. We also conduct cross-domain experi-\nments to evaluate the domain transfer ability of the proposed\nmodel. The recognition accuracy is adopted as the evaluation\nmetric for our experiments. More details of datasets descrip-\ntion are as follow.\nminiImageNet. This dataset is a sub-dataset of Ima-\ngeNet [46]. It contains a total of 100 classes with 600 samples\nin each class. As suggested in work [47], we divide these\nclasses into training set, validation set and testing set, which\nrespectively contains 64, 16 and 20 classes.\ntieredImageNet. It contains 608 classes from 34 super-\nclasses, with a total of 779,165 samples. Following [44], we\nsplit 34 super-classes into 20 super-classes (351 classes) for\nmeta-training, 6 super-classes (97 classes) for meta-validation\nand 8 super-classes (160 classes) for meta-testing.\nFC100. Fewshot-CIFAR100 is built upon the CIFAR100\ndataset for few-shot classiﬁcation task. It’s named FC100 for\nshort hereafter. It contains a total of 60,000 images from 100\nclasses. To reduce the information overlap, we group the 100\nclasses into 20 super-classes by following work [36]. Then,\nwe divide these super-classes into training set, validation set\nand testing set, which contains 12, 4 and 4 super-classes\nrespectively.\nCUB. Caltech-UCSD Birds-200-2011 dataset is an ex-\ntended vision of CUB-200 dataset. It’s termed CUB for short\nhereafter. CUB is originally presented in ﬁne-grained bird\nclassiﬁcation task. It contains the total of 11,788 images from\n200 classes. As suggested by [27], we divide 200 classes into\n100 classes for meta-training, 50 classes for meta-validation\nand 50 classes for meta-testing.\nminiImageNet →CUB. By following [6], we train a model\non miniImageNet dataset and evaluate on the CUB dataset\nto verify the transfer ability of model. In this experimental\nsetting, speciﬁcally, we use all 100 classes of miniImageNet,\nwith 600 samples per class for meta-training and use the meta-\ntesting set (50 classes) of CUB dataset for meta-testing.\nB. Comparison with State-of-the-art Methods\nAs shown in Table I, we report our results and compare with\nother state-of-the-art (SOTA) approaches on miniImageNet [4]\nand tieredImageNet [44] datasets. From this Table, we can ﬁnd\nthat the proposed QSFormer beats many SOTA models on the\nminiImageNet dataset. For example, QSFormer exceeds the\ntransformer-based HT [12] method by +11.14% and +11.46%\nin 1-shot and 5-shot tasks, respectively. For the attention\nmechanism based CAN [13], our model also outperforms it\non the 1-shot/5-shot task by +1.39%/+0.52%. Compared with\nFETA [27] that is also developed based on ResNet12 and\nTransformer, the proposed QSFormer has better results.\nFrom Table I, we can see that QSFormer achieves the best\nperformance on the tieredImageNet dataset, i.e., 72.47±0.31\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n6\nTABLE I\n5-WAY RESULT COMPARISON OF OURS AND STATE-OF-THE-ART METHODS ON MINIIMAGENET AND TIEREDIMAGENET DATASETS. MOST RESULTS ARE\nFROM [3] OR THE ORIGINAL PAPERS. THE 1st, 2rd AND 3rd ARE RESPECTIVELY IN RED, BLUE AND GREEN. * DENOTES THIS METHOD IS REPRODUCED\nWITH OUR SETTINGS.\nMethod\nBackbone\nminiImagenet\ntieredImagenet\n1-shot\n5-shot\n1-shot\n5-shot\nDHL [35]\nConv4\n61.99 ± −\n78.71 ± −\n57.89 ± −\n73.62 ± −\ncosine classiﬁer [6]*\nResNet12\n59.64 ± 0.27\n75.80 ± 0.21\n55.87 ± 0.31\n80.92 ± 0.23\nTADAM [36]\nResNet12\n58.50 ± 0.30\n76.70 ± 0.30\n−\n−\nECM [37]\nResNet12\n59.00 ± −\n77.46 ± −\n63.99 ± −\n81.97 ± −\nTPN [38]\nResNet12\n59.46 ± −\n75.65 ± −\n59.91 ± 0.94\n73.30 ± 0.75\nProtoNet [5]*\nResNet12\n63.03 ± 0.29\n78.72 ± 0.21\n68.68 ± 0.34\n85.09 ± 0.23\nMTL [39]\nResNet12\n61.20 ± 1.80\n75.50 ± 0.80\n−\n−\nDC [40]\nResNet12\n62.53 ± 0.19\n79.77 ± 0.19\n−\n−\nMetaOptNet [41]\nResNet12\n62.64 ± 0.82\n78.63 ± 0.46\n65.99 ± 0.72\n81.56 ± 0.53\nMatchNet [4]*\nResNet12\n61.24 ± 0.29\n73.93 ± 0.23\n71.01 ± 0.33\n83.12 ± 0.24\nMeta-Baseline [7]\nResNet12\n63.17 ± 0.23\n79.26 ± 0.17\n68.62 ± 0.27\n83.74 ± 0.18\nCAN [13]\nResNet12\n63.85 ± 0.48\n79.44 ± 0.34\n69.89 ± 0.51\n84.23 ± 0.37\nPPA [42]\nWRN-28-10\n59.60 ± 0.41\n73.74 ± 0.19\n65.65 ± 0.92\n83.40 ± 0.65\nwDAE-GNN [43]\nWRN-28-10\n61.07 ± 0.15\n76.75 ± 0.11\n68.18 ± 0.16\n83.09 ± 0.12\nLEO [1]\nWRN-28-10\n61.76 ± 0.08\n77.59 ± 0.12\n66.33 ± 0.05\n81.44 ± 0.09\nFEAT [27]*\nResNet12\n64.75 ± 0.28\n79.96 ± 0.20\n71.34 ± 0.33\n85.28 ± 0.23\nHT [12]\nTransformer\n54.10 ± −\n68.50 ± −\n56.10 ± −\n73.30 ± −\nDeepEMD [3]*\nResNet12\n65.43 ± 0.28\n79.28 ± 0.20\n69.84 ± 0.32\n84.06 ± 0.23\nDeepBDC [14]*\nResNet12\n60.76 ± 0.28\n78.25 ± 0.20\n63.03 ± 0.31\n81.57 ± 0.22\nQSFormer (Ours)\nResNet12\n65.24 ± 0.28\n79.96 ± 0.20\n72.47 ± 0.31\n85.43 ± 0.22\nTABLE II\n5-WAY RESULT COMPARISON OF OURS AND STATE-OF-THE-ART METHODS\nON FEWSHOT-CIFAR100 DATASET. THE 1st, 2rd AND 3rd ARE\nRESPECTIVELY IN RED, BLUE AND GREEN. * DENOTES THIS METHOD IS\nREPRODUCED WITH OUR SETTINGS.\nMethod\n1-shot\n5-shot\ncosine classiﬁer [6]*\n39.47 ± 0.23\n56.29 ± 0.25\nFEAT [27]*\n42.28 ± 0.26\n56.37 ± 0.25\nTADAM [36]\n40.10 ± 0.40\n56.10 ± 0.40\nProtoNet [5]*\n40.91 ± 0.26\n56.66 ± 0.25\nMTL [39]\n45.10 ± 1.8\n57.60 ± 0.9\nDC [40]\n42.04 ± 0.17\n57.05 ± 0.16\nMetaOptNet [41]\n41.10 ± 0.60\n55.50 ± 0.60\nMatchNet [4]*\n41.90 ± 0.27\n54.41 ± 0.25\nTDE-FSL [48]\n44.61 ± 0.96\n57.93 ± 0.81\nDeepEMD [3]*\n45.58 ± 0.26\n62.08 ± 0.25\nDeepBDC [14]*\n43.57 ± 0.25\n59.49 ± 0.25\nQSFormer (Ours)\n46.51 ± 0.26\n61.58 ± 0.25\nand 85.43±0.22 in 1-shot and 5-shot tasks. It exceeds the\nCAN [13] by +2.58 and +1.2 points in 1-shot and 5-shot tasks.\nSimilar conclusions can also be drawn from the experimental\nresults of Fewshot-CIFAR100 [36] and CUB [45] datasets, as\nillustrated in Table II and Table III. All in all, the proposed QS-\nFormer attains SOTA performance on multiple FSL datasets,\nwhich fully demonstrates the effectiveness and advantages of\nour proposed QSFormer model.\nC. Ablation Study\nTo better understand the effectiveness of our proposed\nQSFormer, in this section, we conduct extensive ablation stud-\nies, including component analysis, similarity metric analysis,\ncross-domain analysis, etc.\nComponent Analysis. Our proposed QSFormer mainly\ncontains three components: Cross-scale Interactive Feature Ex-\ntractor (CIFE), Sample Transformer Module (sampleFormer)\nand Patch Transformer Module (patchFormer). The experi-\nmental results of ablation study are shown in Table V. We\nTABLE III\n5-WAY RESULT COMPARISON OF OURS AND STATE-OF-THE-ART METHODS\nON CALTECH-UCSD BIRDS-200-2011 DATASET. THE 1st, 2rd AND 3rd\nARE RESPECTIVELY IN RED, BLUE AND GREEN. * DENOTES THIS METHOD\nIS REPRODUCED WITH OUR SETTINGS.\nMethod\n1-shot\n5-shot\nMELR [30]\n70.26 ± 0.50\n85.01 ± 0.32\nIEPT [49]\n69.97 ± 0.49\n84.33 ± 0.33\nMVT [50]\n−\n85.35 ± 0.55\nFEAT [27]*\n75.00 ± 0.29\n86.24 ± 0.19\ncosine classiﬁer [6]*\n62.09 ± 0.29\n80.04 ± 0.21\nProtoNet [5]*\n70.93 ± 0.30\n85.55 ± 0.19\nMatchNet [4]*\n70.21 ± 0.30\n82.69 ± 0.22\nRelationNet [15]\n66.20 ± 0.99\n82.30 ± 0.58\nMAML [51]\n67.28 ± 1.08\n83.47 ± 0.59\nDEML [52]\n66.95 ± 1.06\n77.11 ± 0.78\nDeepEMD [3]*\n70.71 ± 0.30\n86.13 ± 0.19\nDeepBDC [14]*\n65.45 ± 0.29\n85.01 ± 0.19\nQSFormer (Ours)\n75.44 ± 0.29\n86.30 ± 0.19\nTABLE IV\nCROSS-DOMAIN EXPERIMENTS (miniImagenet →CUB). * DENOTES\nTHIS METHOD IS REPRODUCED WITH OUR SETTINGS. THE RED\nREPRESENTS THE BEST RESULTS AND BLUE DENOTES THE SECOND-BEST\nRESULTS.\nMethods\n1-shot\n5-shot\nProtoNet [5]\n50.01 ± 0.82\n72.02 ± 0.67\nMatchNet [4]\n51.65 ± 0.84\n69.14 ± 0.72\ncosine classiﬁer [6]\n44.17 ± 0.78\n69.01 ± 0.74\nBaseline [6]\n−\n65.57 ± 0.70\nBaseline++ [6]\n−\n62.04 ± 0.76\nFEAT [27]*\n52.67 ± 0.29\n72.65 ± 0.25\nDeepEMD [3]\n54.24 ± 0.86\n78.86 ± 0.65\nDeepBDC [14]*\n50.28 ± 0.27\n76.49 ± 0.23\nQSFormer (Ours)\n55.04 ± 0.29\n77.12 ± 0.24\nreproduce cosine classiﬁer method [6] consisting of CNN net-\nwork and cosine distance as the Baseline network for compari-\nson. From Table V, we can observe: (1) By comparing #1 with\n#2, the performance of Baseline network can be signiﬁcantly\nimproved with the help of CIFE, which demonstrates the\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n7\nTABLE V\nABLATION STUDY FOR THE DIFFERENT COMPONENTS OF THE PROPOSED QSFORMER. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD.\nDifferent Components\nDatasets\n#\nBaseline\nCIFE\nsampleFormer\npatchFormer\nminiImageNet\ntieredImageNet\nFC100\nCUB\n1\n✓\n59.64 ± 0.27\n55.87 ± 0.31\n39.47 ± 0.23\n62.09 ± 0.29\n2\n✓\n✓\n61.15 ± 0.28\n70.73 ± 0.32\n41.54 ± 0.25\n65.95 ± 0.30\n3\n✓\n✓\n✓\n63.97 ± 0.28\n71.64 ± 0.32\n45.46 ± 0.26\n72.93 ± 0.29\n4\n✓\n✓\n✓\n✓\n65.24 ± 0.28\n72.47 ± 0.31\n46.51 ± 0.26\n75.44 ± 0.29\nTABLE VI\nHYPERPARAMETER SETTINGS OF OUR PROPOSED QSFORMER.\nHyper-parameters\nDatasets\nminiImageNet\ntieredImageNet\nFC100\nCUB\nminiImageNet →CUB\nOptimizer\nSGD\nSGD\nSGD\nSGD\nSGD\nInitial LR\n5e-4\n5e-4\n1e-4\n5e-4\n5e-4\nSteps of LR decay\n10\n10\n10\n10\n10\nCoefﬁcient of LR decay\n0.9\n0.5\n0.9\n0.95\n0.9\nN\n3\n3\n4\n2\n3\nNumber of Head\n10,8\n8,8\n8,1\n8,1\n10,8\ndropout rates\n0.5,0.5,0.5,0.1\n0.5,0.5,0.5,0.1\n0.5,0.5,0.5,0.1\n0.1,0.5,0.5,0.1\n0.5,0.5,0.5,0.1\nα\n0.7\n0.5\n0.5\n0.05\n0.7\nλ\n0.1\n0.1\n0.4\n0.3\n0.1\nEpochs\n100\n100\n50\n150\n100\nTABLE VII\nPERFORMANCE COMPARISON OF THE CLASSICAL METHODS BASED ON DIFFERENT METRIC LEARNING. * DENOTES THE COMPARISON METHODS IS\nREPRODUCED WITH OUR SETTING. THE BOLD BLACK REPRESENTS THE BEST RESULTS.\nMethods\nMetric\nminiImageNet\ntieredImageNet\nFC100\nCUB\ncosine classiﬁer [6]*\nCosine\n59.64 ± 0.27\n55.87 ± 0.31\n39.47 ± 0.23\n62.09 ± 0.29\nMatchNet [4]*\nCosine\n61.24 ± 0.29\n71.01 ± 0.33\n41.90 ± 0.27\n70.20 ± 0.30\nProtoNet [5]*\nEuclidean\n63.03 ± 0.29\n68.68 ± 0.34\n40.91 ± 0.26\n70.93 ± 0.30\nDeepEMD [3]*\nEMD\n65.43 ± 0.28\n69.84 ± 0.32\n45.58 ± 0.26\n70.71 ± 0.30\nQSFormer\nOurs\n65.24 ± 0.28\n72.47 ± 0.31\n46.51 ± 0.26\n75.44 ± 0.29\neffectiveness of CIFE. (2) By comparing #2 with #3, we can\nﬁnd that sampleFormer signiﬁcantly improves the performance\nof model based on #2, which indicates the effectiveness of\nsampleFormer module. (3) By adding patchFormer into #3,\nwe further improve the performance of whole network, which\nshows the effectiveness of patchFormer module. All these\nexperiments fully validate the effectiveness of each component\nin our proposed QSFormer framework.\nSimilarity Metric Analysis. To verify the effectiveness of\nthe proposed QSFormer on metric learning, we visualize the\nsimilarity distribution of Baseline and QSFormer on the more\nchallenging 5-way 1-shot task, as shown in Figure 4. For 5-\nway 1-shot task, each query sample generates the similarity\nresults of one positive query-support sample pair (i.e., “Q-\nS pos”) and four negative query-support sample pairs (i.e.,\n“Q-S neg”) during the metric learning process. To facilitate\nthe comparison of the similarity results of “Q-S pos” and\n“Q-S neg”, we average the similarity values of four “Q-S\nneg” corresponding to each query sample. For this experiment,\nwe perform 10 episodes, where each episode random selects\n15 × 5 = 75 query samples for classiﬁcation, i.e., we can get\nthe 75 × 10 = 750 similarity values of “Q-S pos” and “Q-\nS neg”, respectively. Subsequently, we count the number of\n“Q-S pos” and “Q-S neg” within a certain range according\nto the normalized similarity values and thus produce the\nsimilarity distribution as shown in Figure 4. We can observe\nthat: (1) the similarity values of “Q-S pos” obtained by the\nBaseline method are generally below 0.5, while “Q-S neg”\nare above 0.25. (2) In our proposed QSFormer, the similarity\nvalues of “Q-S pos” are mostly above 0.5, while “Q-S neg”\nare mostly below 0.25. Therefore, our proposed QSFormer\ncan separate positive and negative query-support sample pairs\nmore accurately.\nIn addition, we also compare our QSFormer with other\nmetric learning algorithms, including cosine classiﬁer [6],\nMatchNet [4], ProtoNet [5] and DeepEMD [3]. These com-\npared methods are reproduced with the same settings and\ntraining schemes as ours for a more fair comparison. As shown\nin Table VII, we can observe that our proposed method obtains\nthe best performance on four publicly popular datasets, which\nfully demonstrates the effectiveness and superiority of our\nproposed QSFormer. These experiments fully demonstrate the\neffectiveness of our proposed QSFormer for metric learning.\nCross-domain Analysis. To validate the transferable abil-\nity of our proposed QSFormer, we conduct a cross-domain\nexperiment by following [3], [6]. The training and testing\nare implemented on miniImagenet dataset and CUB dataset,\nrespectively. As shown in Table IV, our proposed QSFormer\nachieves the best performance on the 1-shot setting (55.04 ±\n0.29) and the second-best results on the 5-shot, i.e., 77.12\n± 0.24. These results demonstrate that the proposed QS-\nFormer learns the discriminative information across domains,\nand adaptively explores the correspondence of query-support\nsamples.\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n8\n(a) Baseline\n(b) QSFormer\nFig. 4.\nComparison of similarity distribution between Baseline and our\nQSFormer. The similarities of “Q-S pos” become larger while the similarities\nof “Q-S neg” become smaller, which indicates they are more easily separated.\nFig. 5. Ablation study of two parameters (i.e., λ and N).\nParameter Analysis. There are two important parameters\nin our model, including the balanced parameter λ in Equ. (11)\nfor local and global metric, and the number of sampleFormer\nlayers N. In this section, we conduct experiments on the\nFC100 dataset on 5-way 1-shot task to check their inﬂuence.\nAs shown in Figure 5, we can observe that the performance\nis relatively stable when we slightly adjust the balanced\nparameter λ in the range of (0.2, 0.6). For the number N\nof sampleFormer layers, we can ﬁnd that our performance is\nincreasing continuously when the N is changing from 2 to 4.\nTherefore, we set λ = 0.4 and N = 4 for our experiments.\nV. CONCLUSION\nIn this paper, we propose a novel uniﬁed Query-Support\nTransformer (QSFormer) to deeply exploit the sample re-\nlationships in query and support sets for few-shot classiﬁ-\ncation task. QSFormer mainly contains sample Transformer\n(sampleFormer) module and patch Transformer (patchFormer)\nmodule. sampleFormer is designed to meet the problem setting\nof few-shot classiﬁcation, i.e., it couples the sample repre-\nsentation and metric learning between query and support sets\ntogether via a single Transformer architecture. Meanwhile, as\na complementary, patchFormer is also adopted to model the\nlocal structural metric between query and support samples. A\nnew CNN feature extractor (CIFE) is also proposed to pro-\nvide an effective CNN backbone for our approach. Extensive\nexperiments demonstrate the effectiveness and superiority of\nour proposed QSFormer approach.\nREFERENCES\n[1] A. A. Rusu, D. Rao, J. Sygnowski, O. Vinyals, R. Pascanu, S. Osindero,\nand R. Hadsell, “Meta-learning with latent embedding optimization,” in\nProceedings of the IEEE/CVF International Conference on Learning\nRepresentations, 2018, pp. 6907–6917.\n[2] M. A. Jamal and G.-J. Qi, “Task agnostic meta-learning for few-shot\nlearning,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2019, pp. 11 719–11 727.\n[3] C. Zhang, Y. Cai, G. Lin, and C. Shen, “Deepemd: Few-shot image\nclassiﬁcation with differentiable earth mover’s distance and structured\nclassiﬁers,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2020, pp. 12 203–12 213.\n[4] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra et al., “Matching\nnetworks for one shot learning,” Advances in Neural Information Pro-\ncessing Systems, vol. 29, 2016.\n[5] J. Snell, K. Swersky, and R. Zemel, “Prototypical networks for few-shot\nlearning,” Advances in Neural Information Processing Systems, vol. 30,\npp. 4080–4090, 2017.\n[6] W.-Y. Chen, Y.-C. Liu, Z. Kira, Y.-C. F. Wang, and J.-B. Huang, “A\ncloser look at few-shot classiﬁcation,” in Proceedings of the IEEE/CVF\nInternational Conference on Learning Representations, 2019.\n[7] Y. Chen, Z. Liu, H. Xu, T. Darrell, and X. Wang, “Meta-baseline:\nexploring simple meta-learning for few-shot learning,” in Proceedings\nof the IEEE/CVF International Conference on Computer Vision, 2021,\npp. 9062–9071.\n[8] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,” arXiv preprint arXiv:2010.11929, 2020.\n[9] Z. Peng, W. Huang, S. Gu, L. Xie, Y. Wang, J. Jiao, and Q. Ye,\n“Conformer: Local features coupling global representations for visual\nrecognition,” in Proceedings of the IEEE/CVF International Conference\non Computer Vision, 2021, pp. 367–376.\n[10] Y. He, W. Liang, D. Zhao, H.-Y. Zhou, W. Ge, Y. Yu, and W. Zhang,\n“Attribute surrogates learning and spectral tokens pooling in transform-\ners for few-shot learning,” arXiv preprint arXiv:2203.09064, 2022.\n[11] B. Dong, P. Zhou, S. Yan, and W. Zuo, “Self-promoted supervision for\nfew-shot transformer,” arXiv preprint arXiv:2203.07057, 2022.\n[12] A. Zhmoginov, M. Sandler, and M. Vladymyrov, “Hypertransformer:\nModel generation for supervised and semi-supervised few-shot learning,”\narXiv preprint arXiv:2201.04182, 2022.\n[13] R. Hou, H. Chang, B. Ma, S. Shan, and X. Chen, “Cross attention\nnetwork for few-shot classiﬁcation,” Advances in Neural Information\nProcessing Systems, vol. 32, 2019.\n[14] J. Xie, F. Long, J. Lv, Q. Wang, and P. Li, “Joint distribution matters:\nDeep brownian distance covariance for few-shot classiﬁcation,” in Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2022, pp. 7972–7981.\n[15] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and T. M. Hospedales,\n“Learning to compare: Relation network for few-shot learning,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2018, pp. 1199–1208.\n[16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\nNeural Information Processing Systems, vol. 30, 2017.\n[17] N. Wang, W. Zhou, J. Wang, and H. Li, “Transformer meets tracker: Ex-\nploiting temporal context for robust visual tracking,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2021, pp. 1571–1580.\n[18] E. Yu, Z. Li, S. Han, and H. Wang, “Relationtrack: Relation-aware mul-\ntiple object tracking with decoupled representation,” IEEE Transactions\non Multimedia, pp. 1–12, 2022.\n[19] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” in\nProceedings of European Conference on Computer Vision, 2020, pp.\n213–229.\n[20] T. Guan, J. Wang, S. Lan, R. Chandra, Z. Wu, L. Davis, and D. Manocha,\n“M3detr: Multi-representation, multi-scale, mutual-relation 3d object\ndetection with transformers,” in Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision, 2022, pp. 772–782.\n[21] S. Liao and L. Shao, “Transmatcher: Deep image matching through\ntransformers for generalizable person re-identiﬁcation,” Advances in\nNeural Information Processing Systems, vol. 34, pp. 1992–2003, 2021.\n[22] M. Jia, X. Cheng, S. Lu, and J. Zhang, “Learning disentangled represen-\ntation implicitly via transformer for occluded person re-identiﬁcation,”\nIEEE Transactions on Multimedia, pp. 1–11, 2022.\n[23] S. Liu, L. Zhang, X. Yang, H. Su, and J. Zhu, “Query2label: A\nsimple transformer way to multi-label classiﬁcation,” arXiv preprint\narXiv:2107.10834, 2021.\n[24] Z.-M. Chen, Q. Cui, B. Zhao, R. Song, X. Zhang, and O. Yoshie, “Sst:\nSpatial and semantic transformers for multi-label image recognition,”\nIEEE Transactions on Image Processing, vol. 31, pp. 2570–2583, 2022.\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n9\n[25] J. M. J. Valanarasu, P. Oza, I. Hacihaliloglu, and V. M. Patel, “Medical\ntransformer: Gated axial-attention for medical image segmentation,” in\nProceedings of International Conference on Medical Image Computing\nand Computer-Assisted Intervention, 2021, pp. 36–46.\n[26] O. Petit, N. Thome, C. Rambour, L. Themyr, T. Collins, and L. Soler,\n“U-net transformer: Self and cross attention for medical image segmen-\ntation,” in Proceedings of International Workshop on Machine Learning\nin Medical Imaging, 2021, pp. 267–276.\n[27] H.-J. Ye, H. Hu, D.-C. Zhan, and F. Sha, “Few-shot learning via\nembedding adaptation with set-to-set functions,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2020, pp. 8808–8817.\n[28] L. Liu, W. Hamilton, G. Long, J. Jiang, and H. Larochelle, “A universal\nrepresentation transformer layer for few-shot image classiﬁcation,” in\nProceedings of the IEEE/CVF International Conference on Learning\nRepresentations, 2021.\n[29] B. Jiang, K. Zhao, and J. Tang, “Rgtransformer: Region-graph trans-\nformer for image representation and few-shot classiﬁcation,” IEEE\nSignal Processing Letters, vol. 29, pp. 792–796, 2022.\n[30] N. Fei, Z. Lu, T. Xiang, and S. Huang, “Melr: Meta-learning via model-\ning episode-level relationships for few-shot learning,” in Proceedings of\nthe IEEE/CVF International Conference on Learning Representations,\n2021.\n[31] J. Wang, B. Song, D. Wang, and H. Qin, “Two-stream network with\nphase map for few-shot classiﬁcation,” Neurocomputing, vol. 472, pp.\n45–53, 2022.\n[32] F. L. Hitchcock, “The distribution of a product from several sources to\nnumerous localities,” Journal of Mathematics and Physics, vol. 20, no.\n1-4, pp. 224–230, 1941.\n[33] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with\ncontrastive predictive coding,” arXiv preprint arXiv:1807.03748, 2018.\n[34] C. Liu, Y. Fu, C. Xu, S. Yang, J. Li, C. Wang, and L. Zhang, “Learning\na few-shot embedding model with contrastive learning,” in Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence, vol. 35, no. 10, 2021,\npp. 8635–8643.\n[35] X. Zhang, Y. Zhang, Z. Zhang, and J. Liu, “Discriminative learning of\nimaginary data for few-shot classiﬁcation,” Neurocomputing, vol. 467,\npp. 406–417, 2022.\n[36] B. Oreshkin, P. Rodr´ıguez L´opez, and A. Lacoste, “Tadam: Task\ndependent adaptive metric for improved few-shot learning,” Advances\nin Neural Information Processing Systems, vol. 31, pp. 719–729, 2018.\n[37] A. Ravichandran, R. Bhotika, and S. Soatto, “Few-shot learning with\nembedded class models and shot-free meta training,” in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision, 2019, pp.\n331–339.\n[38] Y. Liu, J. Lee, M. Park, S. Kim, E. Yang, S. J. Hwang, and Y. Yang,\n“Learning to propagate labels: Transductive propagation network for\nfew-shot learning,” in Proceedings of the IEEE/CVF International\nConference on Learning Representations, 2019.\n[39] Q. Sun, Y. Liu, T.-S. Chua, and B. Schiele, “Meta-transfer learning\nfor few-shot learning,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2019, pp. 403–412.\n[40] Y. Lifchitz, Y. Avrithis, S. Picard, and A. Bursuc, “Dense classiﬁcation\nand implanting for few-shot learning,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2019, pp.\n9258–9267.\n[41] K. Lee, S. Maji, A. Ravichandran, and S. Soatto, “Meta-learning with\ndifferentiable convex optimization,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2019, pp.\n10 657–10 665.\n[42] S. Qiao, C. Liu, W. Shen, and A. L. Yuille, “Few-shot image recognition\nby predicting parameters from activations,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2018, pp.\n7229–7238.\n[43] S. Gidaris and N. Komodakis, “Generating classiﬁcation weights with\ngnn denoising autoencoders for few-shot learning,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2019, pp. 21–30.\n[44] M. Ren, E. Triantaﬁllou, S. Ravi, J. Snell, K. Swersky, J. B. Tenenbaum,\nH. Larochelle, and R. S. Zemel, “Meta-learning for semi-supervised\nfew-shot classiﬁcation,” in Proceedings of the IEEE/CVF International\nConference on Learning Representations, 2018.\n[45] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie, “The\ncaltech-ucsd birds-200-2011 dataset,” 2011.\n[46] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., “Imagenet large\nscale visual recognition challenge,” International Journal of Computer\nVision, vol. 115, no. 3, pp. 211–252, 2015.\n[47] S. Ravi and H. Larochelle, “Optimization as a model for few-shot\nlearning,” 2017.\n[48] L. Xing, S. Shao, W. Liu, A. Han, X. Pan, and B.-D. Liu, “Learning task-\nspeciﬁc discriminative embeddings for few-shot image classiﬁcation,”\nNeurocomputing, vol. 488, pp. 1–13, 2022.\n[49] M. Zhang, J. Zhang, Z. Lu, T. Xiang, M. Ding, and S. Huang, “Iept:\nInstance-level and episode-level pretext tasks for few-shot learning,” in\nProceedings of the IEEE/CVF International Conference on Learning\nRepresentations, 2021.\n[50] S.-J. Park, S. Han, J.-W. Baek, I. Kim, J. Song, H. B. Lee, J.-J. Han,\nand S. J. Hwang, “Meta variance transfer: Learning to augment from\nthe others,” in Proceedings of the IEEE/CVF International Conference\non Machine Learning, 2020, pp. 7510–7520.\n[51] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for\nfast adaptation of deep networks,” in Proceedings of the IEEE/CVF\nInternational Conference on Machine Learning, 2017, pp. 1126–1135.\n[52] F. Zhou, B. Wu, and Z. Li, “Deep meta-learning: Learning to learn in\nthe concept space,” arXiv preprint arXiv:1802.03596, 2018.\n"
}