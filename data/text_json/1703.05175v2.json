{
  "filename": "1703.05175v2.pdf",
  "num_pages": 13,
  "pages": [
    "Prototypical Networks for Few-shot Learning\nJake Snell\nUniversity of Toronto∗\nKevin Swersky\nTwitter\nRichard S. Zemel\nUniversity of Toronto, Vector Institute\nAbstract\nWe propose prototypical networks for the problem of few-shot classiﬁcation, where\na classiﬁer must generalize to new classes not seen in the training set, given only\na small number of examples of each new class. Prototypical networks learn a\nmetric space in which classiﬁcation can be performed by computing distances\nto prototype representations of each class. Compared to recent approaches for\nfew-shot learning, they reﬂect a simpler inductive bias that is beneﬁcial in this\nlimited-data regime, and achieve excellent results. We provide an analysis showing\nthat some simple design decisions can yield substantial improvements over recent\napproaches involving complicated architectural choices and meta-learning. We\nfurther extend prototypical networks to zero-shot learning and achieve state-of-the-\nart results on the CU-Birds dataset.\n1\nIntroduction\nFew-shot classiﬁcation [20, 16, 13] is a task in which a classiﬁer must be adapted to accommodate\nnew classes not seen in training, given only a few examples of each of these classes. A naive approach,\nsuch as re-training the model on the new data, would severely overﬁt. While the problem is quite\ndifﬁcult, it has been demonstrated that humans have the ability to perform even one-shot classiﬁcation,\nwhere only a single example of each new class is given, with a high degree of accuracy [16].\nTwo recent approaches have made signiﬁcant progress in few-shot learning. Vinyals et al. [29]\nproposed matching networks, which uses an attention mechanism over a learned embedding of the\nlabeled set of examples (the support set) to predict classes for the unlabeled points (the query set).\nMatching networks can be interpreted as a weighted nearest-neighbor classiﬁer applied within an\nembedding space. Notably, this model utilizes sampled mini-batches called episodes during training,\nwhere each episode is designed to mimic the few-shot task by subsampling classes as well as data\npoints. The use of episodes makes the training problem more faithful to the test environment and\nthereby improves generalization. Ravi and Larochelle [22] take the episodic training idea further\nand propose a meta-learning approach to few-shot learning. Their approach involves training an\nLSTM [9] to produce the updates to a classiﬁer, given an episode, such that it will generalize well to\na test-set. Here, rather than training a single model over multiple episodes, the LSTM meta-learner\nlearns to train a custom model for each episode.\nWe attack the problem of few-shot learning by addressing the key issue of overﬁtting. Since data is\nseverely limited, we work under the assumption that a classiﬁer should have a very simple inductive\nbias. Our approach, prototypical networks, is based on the idea that there exists an embedding in\nwhich points cluster around a single prototype representation for each class. In order to do this,\nwe learn a non-linear mapping of the input into an embedding space using a neural network and\ntake a class’s prototype to be the mean of its support set in the embedding space. Classiﬁcation\nis then performed for an embedded query point by simply ﬁnding the nearest class prototype. We\nfollow the same approach to tackle zero-shot learning; here each class comes with meta-data giving\na high-level description of the class rather than a small number of labeled examples. We therefore\nlearn an embedding of the meta-data into a shared space to serve as the prototype for each class.\n*Initial work by ﬁrst author done while at Twitter.\narXiv:1703.05175v2  [cs.LG]  19 Jun 2017\n",
    "c1\nc2\nc3\nx\n(a) Few-shot\nv1\nv2\nv3\nc1\nc2\nc3\nx\n(b) Zero-shot\nFigure 1: Prototypical networks in the few-shot and zero-shot scenarios. Left: Few-shot prototypes\nck are computed as the mean of embedded support examples for each class. Right: Zero-shot\nprototypes ck are produced by embedding class meta-data vk. In either case, embedded query points\nare classiﬁed via a softmax over distances to class prototypes: pφ(y = k|x) ∝exp(−d(fφ(x), ck)).\nClassiﬁcation is performed, as in the few-shot scenario, by ﬁnding the nearest class prototype for an\nembedded query point.\nIn this paper, we formulate prototypical networks for both the few-shot and zero-shot settings. We\ndraw connections to matching networks in the one-shot setting, and analyze the underlying distance\nfunction used in the model. In particular, we relate prototypical networks to clustering [4] in order to\njustify the use of class means as prototypes when distances are computed with a Bregman divergence,\nsuch as squared Euclidean distance. We ﬁnd empirically that the choice of distance is vital, as\nEuclidean distance greatly outperforms the more commonly used cosine similarity. On several\nbenchmark tasks, we achieve state-of-the-art performance. Prototypical networks are simpler and\nmore efﬁcient than recent meta-learning algorithms, making them an appealing approach to few-shot\nand zero-shot learning.\n2\nPrototypical Networks\n2.1\nNotation\nIn few-shot classiﬁcation we are given a small support set of N labeled examples S\n=\n{(x1, y1), . . . , (xN, yN)} where each xi ∈RD is the D-dimensional feature vector of an example\nand yi ∈{1, . . . , K} is the corresponding label. Sk denotes the set of examples labeled with class k.\n2.2\nModel\nPrototypical networks compute an M-dimensional representation ck ∈RM, or prototype, of each\nclass through an embedding function fφ : RD →RM with learnable parameters φ. Each prototype\nis the mean vector of the embedded support points belonging to its class:\nck =\n1\n|Sk|\nX\n(xi,yi)∈Sk\nfφ(xi)\n(1)\nGiven a distance function d : RM × RM →[0, +∞), prototypical networks produce a distribution\nover classes for a query point x based on a softmax over distances to the prototypes in the embedding\nspace:\npφ(y = k | x) =\nexp(−d(fφ(x), ck))\nP\nk′ exp(−d(fφ(x), ck′))\n(2)\nLearning proceeds by minimizing the negative log-probability J(φ) = −log pφ(y = k | x) of the\ntrue class k via SGD. Training episodes are formed by randomly selecting a subset of classes from\nthe training set, then choosing a subset of examples within each class to act as the support set and a\nsubset of the remainder to serve as query points. Pseudocode to compute the loss J(φ) for a training\nepisode is provided in Algorithm 1.\n2\n",
    "Algorithm 1 Training episode loss computation for prototypical networks. N is the number of\nexamples in the training set, K is the number of classes in the training set, NC ≤K is the number\nof classes per episode, NS is the number of support examples per class, NQ is the number of query\nexamples per class. RANDOMSAMPLE(S, N) denotes a set of N elements chosen uniformly at\nrandom from set S, without replacement.\nInput: Training set D = {(x1, y1), . . . , (xN, yN)}, where each yi ∈{1, . . . , K}. Dk denotes the\nsubset of D containing all elements (xi, yi) such that yi = k.\nOutput: The loss J for a randomly generated training episode.\nV ←RANDOMSAMPLE({1, . . . , K}, NC)\n▷Select class indices for episode\nfor k in {1, . . . , NC} do\nSk ←RANDOMSAMPLE(DVk, NS)\n▷Select support examples\nQk ←RANDOMSAMPLE(DVk \\ Sk, NQ)\n▷Select query examples\nck ←\n1\nNC\nX\n(xi,yi)∈Sk\nfφ(xi)\n▷Compute prototype from support examples\nend for\nJ ←0\n▷Initialize loss\nfor k in {1, . . . , NC} do\nfor (x, y) in Qk do\nJ ←J +\n1\nNCNQ\n\"\nd(fφ(x), ck)) + log\nX\nk′\nexp(−d(fφ(x), ck))\n#\n▷Update loss\nend for\nend for\n2.3\nPrototypical Networks as Mixture Density Estimation\nFor a particular class of distance functions, known as regular Bregman divergences [4], the prototypi-\ncal networks algorithm is equivalent to performing mixture density estimation on the support set with\nan exponential family density. A regular Bregman divergence dϕ is deﬁned as:\ndϕ(z, z′) = ϕ(z) −ϕ(z′) −(z −z′)T ∇ϕ(z′),\n(3)\nwhere ϕ is a differentiable, strictly convex function of the Legendre type. Examples of Bregman\ndivergences include squared Euclidean distance ∥z −z′∥2 and Mahalanobis distance.\nPrototype computation can be viewed in terms of hard clustering on the support set, with one cluster\nper class and each support point assigned to its corresponding class cluster. It has been shown [4]\nfor Bregman divergences that the cluster representative achieving minimal distance to its assigned\npoints is the cluster mean. Thus the prototype computation in Equation (1) yields optimal cluster\nrepresentatives given the support set labels when a Bregman divergence is used.\nMoreover, any regular exponential family distribution pψ(z|θ) with parameters θ and cumulant\nfunction ψ can be written in terms of a uniquely determined regular Bregman divergence [4]:\npψ(z|θ) = exp{zT θ −ψ(θ) −gψ(z)} = exp{−dϕ(z, µ(θ)) −gϕ(z)}\n(4)\nConsider now a regular exponential family mixture model with parameters Γ = {θk, πk}K\nk=1:\np(z|Γ) =\nK\nX\nk=1\nπkpψ(z|θk) =\nK\nX\nk=1\nπk exp(−dϕ(z, µ(θk)) −gϕ(z))\n(5)\nGiven Γ, inference of the cluster assignment y for an unlabeled point z becomes:\np(y = k|z) =\nπk exp(−dϕ(z, µ(θk)))\nP\nk′ πk′ exp(−dϕ(z, µ(θk)))\n(6)\nFor an equally-weighted mixture model with one cluster per class, cluster assignment inference (6) is\nequivalent to query class prediction (2) with fφ(x) = z and ck = µ(θk). In this case, prototypical\nnetworks are effectively performing mixture density estimation with an exponential family distribution\ndetermined by dϕ. The choice of distance therefore speciﬁes modeling assumptions about the class-\nconditional data distribution in the embedding space.\n3\n",
    "2.4\nReinterpretation as a Linear Model\nA simple analysis is useful in gaining insight into the nature of the learned classiﬁer. When we use\nEuclidean distance d(z, z′) = ∥z −z′∥2, then the model in Equation (2) is equivalent to a linear\nmodel with a particular parameterization [19]. To see this, expand the term in the exponent:\n−∥fφ(x) −ck∥2 = −fφ(x)⊤fφ(x) + 2c⊤\nk fφ(x) −c⊤\nk ck\n(7)\nThe ﬁrst term in Equation (7) is constant with respect to the class k, so it does not affect the softmax\nprobabilities. We can write the remaining terms as a linear model as follows:\n2c⊤\nk fφ(x) −c⊤\nk ck = w⊤\nk fφ(x) + bk, where wk = 2ck and bk = −c⊤\nk ck\n(8)\nWe focus primarily on squared Euclidean distance (corresponding to spherical Gaussian densities) in\nthis work. Our results indicate that Euclidean distance is an effective choice despite the equivalence\nto a linear model. We hypothesize this is because all of the required non-linearity can be learned\nwithin the embedding function. Indeed, this is the approach that modern neural network classiﬁcation\nsystems currently use, e.g., [14, 28].\n2.5\nComparison to Matching Networks\nPrototypical networks differ from matching networks in the few-shot case with equivalence in the\none-shot scenario. Matching networks [29] produce a weighted nearest neighbor classiﬁer given the\nsupport set, while prototypical networks produce a linear classiﬁer when squared Euclidean distance\nis used. In the case of one-shot learning, ck = xk since there is only one support point per class, and\nmatching networks and prototypical networks become equivalent.\nA natural question is whether it makes sense to use multiple prototypes per class instead of just one.\nIf the number of prototypes per class is ﬁxed and greater than 1, then this would require a partitioning\nscheme to further cluster the support points within a class. This has been proposed in Mensink\net al. [19] and Rippel et al. [25]; however both methods require a separate partitioning phase that is\ndecoupled from the weight updates, while our approach is simple to learn with ordinary gradient\ndescent methods.\nVinyals et al. [29] propose a number of extensions, including decoupling the embedding functions of\nthe support and query points, and using a second-level, fully-conditional embedding (FCE) that takes\ninto account speciﬁc points in each episode. These could likewise be incorporated into prototypical\nnetworks, however they increase the number of learnable parameters, and FCE imposes an arbitrary\nordering on the support set using a bi-directional LSTM. Instead, we show that it is possible to\nachieve the same level of performance using simple design choices, which we outline next.\n2.6\nDesign Choices\nDistance metric\nVinyals et al. [29] and Ravi and Larochelle [22] apply matching networks using\ncosine distance. However for both prototypical and matching networks any distance is permissible,\nand we found that using squared Euclidean distance can greatly improve results for both. We\nconjecture this is primarily due to cosine distance not being a Bregman divergence, and thus the\nequivalence to mixture density estimation discussed in Section 2.3 does not hold.\nEpisode composition\nA straightforward way to construct episodes, used in Vinyals et al. [29] and\nRavi and Larochelle [22], is to choose Nc classes and NS support points per class in order to match\nthe expected situation at test-time. That is, if we expect at test-time to perform 5-way classiﬁcation\nand 1-shot learning, then training episodes could be comprised of Nc = 5, NS = 1. We have found,\nhowever, that it can be extremely beneﬁcial to train with a higher Nc, or “way”, than will be used\nat test-time. In our experiments, we tune the training Nc on a held-out validation set. Another\nconsideration is whether to match NS, or “shot”, at train and test-time. For prototypical networks,\nwe found that it is usually best to train and test with the same “shot” number.\n2.7\nZero-Shot Learning\nZero-shot learning differs from few-shot learning in that instead of being given a support set of\ntraining points, we are given a class meta-data vector vk for each class. These could be determined\n4\n",
    "Table 1: Few-shot classiﬁcation accuracies on Omniglot.\n5-way Acc.\n20-way Acc.\nModel\nDist.\nFine Tune\n1-shot\n5-shot\n1-shot\n5-shot\nMATCHING NETWORKS [29]\nCosine\nN\n98.1%\n98.9%\n93.8%\n98.5%\nMATCHING NETWORKS [29]\nCosine\nY\n97.9%\n98.7%\n93.5%\n98.7%\nNEURAL STATISTICIAN [6]\n-\nN\n98.1%\n99.5%\n93.2%\n98.1%\nPROTOTYPICAL NETWORKS (OURS)\nEuclid.\nN\n98.8%\n99.7%\n96.0%\n98.9%\nin advance, or they could be learned from e.g., raw text [7]. Modifying prototypical networks to deal\nwith the zero-shot case is straightforward: we simply deﬁne ck = gϑ(vk) to be a separate embedding\nof the meta-data vector. An illustration of the zero-shot procedure for prototypical networks as\nit relates to the few-shot procedure is shown in Figure 1. Since the meta-data vector and query\npoint come from different input domains, we found it was helpful empirically to ﬁx the prototype\nembedding g to have unit length, however we do not constrain the query embedding f.\n3\nExperiments\nFor few-shot learning, we performed experiments on Omniglot [16] and the miniImageNet version\nof ILSVRC-2012 [26] with the splits proposed by Ravi and Larochelle [22]. We perform zero-shot\nexperiments on the 2011 version of the Caltech UCSD bird dataset (CUB-200 2011) [31].\n3.1\nOmniglot Few-shot Classiﬁcation\nOmniglot [16] is a dataset of 1623 handwritten characters collected from 50 alphabets. There are 20\nexamples associated with each character, where each example is drawn by a different human subject.\nWe follow the procedure of Vinyals et al. [29] by resizing the grayscale images to 28 × 28 and\naugmenting the character classes with rotations in multiples of 90 degrees. We use 1200 characters\nplus rotations for training (4,800 classes in total) and the remaining classes, including rotations, for\ntest. Our embedding architecture mirrors that used by Vinyals et al. [29] and is composed of four\nconvolutional blocks. Each block comprises a 64-ﬁlter 3 × 3 convolution, batch normalization layer\n[10], a ReLU nonlinearity and a 2 × 2 max-pooling layer. When applied to the 28 × 28 Omniglot\nimages this architecture results in a 64-dimensional output space. We use the same encoder for\nembedding both support and query points. All of our models were trained via SGD with Adam [11].\nWe used an initial learning rate of 10−3 and cut the learning rate in half every 2000 episodes. No\nregularization was used other than batch normalization.\nWe trained prototypical networks using Euclidean distance in the 1-shot and 5-shot scenarios with\ntraining episodes containing 60 classes and 5 query points per class. We found that it is advantageous\nto match the training-shot with the test-shot, and to use more classes (higher “way”) per training\nepisode rather than fewer. We compare against various baselines, including the neural statistician\n[6] and both the ﬁne-tuned and non-ﬁne-tuned versions of matching networks [29]. We computed\nclassiﬁcation accuracy for our models averaged over 1000 randomly generated episodes from the test\nset. The results are shown in Table 1 and to our knowledge they represent the state-of-the-art on this\ndataset.\n3.2\nminiImageNet Few-shot Classiﬁcation\nThe miniImageNet dataset, originally proposed by Vinyals et al. [29], is derived from the larger\nILSVRC-12 dataset [26]. The splits used by Vinyals et al. [29] consist of 60,000 color images of size\n84 × 84 divided into 100 classes with 600 examples each. For our experiments, we use the splits\nintroduced by Ravi and Larochelle [22] in order to directly compare with state-of-the-art algorithms\nfor few-shot learning. Their splits use a different set of 100 classes, divided into 64 training, 16\nvalidation, and 20 test classes. We follow their procedure by training on the 64 training classes and\nusing the 16 validation classes for monitoring generalization performance only.\nWe use the same four-block embedding architecture as in our Omniglot experiments, though here\nit results in a 1600-dimensional output space due to the increased size of the images. We also\n5\n",
    "Table 2: Few-shot classiﬁcation accuracies on miniImageNet. All accuracy results are averaged over\n600 test episodes and are reported with 95% conﬁdence intervals. ∗Results reported by [22].\n5-way Acc.\nModel\nDist.\nFine Tune\n1-shot\n5-shot\nBASELINE NEAREST NEIGHBORS∗\nCosine\nN\n28.86 ± 0.54%\n49.79 ± 0.79%\nMATCHING NETWORKS [29]∗\nCosine\nN\n43.40 ± 0.78%\n51.09 ± 0.71%\nMATCHING NETWORKS FCE [29]∗\nCosine\nN\n43.56 ± 0.84%\n55.31 ± 0.73%\nMETA-LEARNER LSTM [22]∗\n-\nN\n43.44 ± 0.77%\n60.60 ± 0.71%\nPROTOTYPICAL NETWORKS (OURS)\nEuclid.\nN\n49.42 ± 0.78%\n68.20 ± 0.66%\n5-way\nCosine\n5-way\nEuclid.\n20-way\nCosine\n20-way\nEuclid.\n1-shot\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n1-shot Accuracy (5-way)\nMatching / Proto. Nets\n5-way\nCosine\n5-way\nEuclid.\n20-way\nCosine\n20-way\nEuclid.\n5-shot\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n5-shot Accuracy (5-way)\nMatching Nets\nProto. Nets\nFigure 2: Comparison showing the effect of distance metric and number of classes per training episode\non 5-way classiﬁcation accuracy for both matching and prototypical networks on miniImageNet.\nThe x-axis indicates conﬁguration of the training episodes (way, distance, and shot), and the y-axis\nindicates 5-way test accuracy for the corresponding shot. Error bars indicate 95% conﬁdence intervals\nas computed over 600 test episodes. Note that matching networks and prototypical networks are\nidentical in the 1-shot case.\nuse the same learning rate schedule as in our Omniglot experiments and train until validation loss\nstops improving. We train using 30-way episodes for 1-shot classiﬁcation and 20-way episodes for\n5-shot classiﬁcation. We match train shot to test shot and each class contains 15 query points per\nepisode. We compare to the baselines as reported by Ravi and Larochelle [22], which include a\nsimple nearest neighbor approach on top of features learned by a classiﬁcation network on the 64\ntraining classes. The other baselines are two non-ﬁne-tuned variants of matching networks (both\nordinary and FCE) and the Meta-Learner LSTM. As can be seen in Table 2, prototypical networks\nachieves state-of-the-art here by a wide margin.\nWe conducted further analysis, to determine the effect of distance metric and the number of training\nclasses per episode on the performance of prototypical networks and matching networks. To make\nthe methods comparable, we use our own implementation of matching networks that utilizes the\nsame embedding architecture as our prototypical networks. In Figure 2 we compare cosine vs.\nEuclidean distance and 5-way vs. 20-way training episodes in the 1-shot and 5-shot scenarios, with\n15 query points per class per episode. We note that 20-way achieves higher accuracy than 5-way\nand conjecture that the increased difﬁculty of 20-way classiﬁcation helps the network to generalize\nbetter, because it forces the model to make more ﬁne-grained decisions in the embedding space. Also,\nusing Euclidean distance improves performance substantially over cosine distance. This effect is even\nmore pronounced for prototypical networks, in which computing the class prototype as the mean of\nembedded support points is more naturally suited to Euclidean distances since cosine distance is not\na Bregman divergence.\n3.3\nCUB Zero-shot Classiﬁcation\nIn order to assess the suitability of our approach for zero-shot learning, we also run experiments on\nthe Caltech-UCSD Birds (CUB) 200-2011 dataset [31]. The CUB dataset contains 11,788 images of\n200 bird species. We closely follow the procedure of Reed et al. [23] in preparing the data. We use\n6\n",
    "Table 3: Zero-shot classiﬁcation accuracies on CUB-200.\nModel\nImage\nFeatures\n50-way Acc.\n0-shot\nALE [1]\nFisher\n26.9%\nSJE [2]\nAlexNet\n40.3%\nSAMPLE CLUSTERING [17]\nAlexNet\n44.3%\nSJE [2]\nGoogLeNet\n50.1%\nDS-SJE [23]\nGoogLeNet\n50.4%\nDA-SJE [23]\nGoogLeNet\n50.9%\nPROTO. NETS (OURS)\nGoogLeNet\n54.6%\ntheir splits to divide the classes into 100 training, 50 validation, and 50 test. For images we use 1,024-\ndimensional features extracted by applying GoogLeNet [28] to middle, upper left, upper right, lower\nleft, and lower right crops of the original and horizontally-ﬂipped image2. At test time we use only\nthe middle crop of the original image. For class meta-data we use the 312-dimensional continuous\nattribute vectors provided with the CUB dataset. These attributes encode various characteristics of\nthe bird species such as their color, shape, and feather patterns.\nWe learned a simple linear mapping on top of both the 1024-dimensional image features and the\n312-dimensional attribute vectors to produce a 1,024-dimensional output space. For this dataset we\nfound it helpful to normalize the class prototypes (embedded attribute vectors) to be of unit length,\nsince the attribute vectors come from a different domain than the images. Training episodes were\nconstructed with 50 classes and 10 query images per class. The embeddings were optimized via SGD\nwith Adam at a ﬁxed learning rate of 10−4 and weight decay of 10−5. Early stopping on validation\nloss was used to determine the optimal number of epochs for retraining on the training plus validation\nset.\nTable 3 shows that we achieve state-of-the-art results by a large margin when compared to methods\nutilizing attributes as class meta-data. We compare our method to other embedding approaches, such\nas ALE [1], SJE [2], and DS-SJE/DA-SJE [23]. We also compare to a recent clustering approach\n[17] which trains an SVM on a learned feature space obtained by ﬁne-tuning AlexNet [14]. These\nzero-shot classiﬁcation results demonstrate that our approach is general enough to be applied even\nwhen the data points (images) are from a different domain relative to the classes (attributes).\n4\nRelated Work\nThe literature on metric learning is vast [15, 5]; we summarize here the work most relevant to our\nproposed method. Neighborhood Components Analysis (NCA) [8] learns a Mahalanobis distance to\nmaximize K-nearest-neighbor’s (KNN) leave-one-out accuracy in the transformed space. Salakhutdi-\nnov and Hinton [27] extend NCA by using a neural network to perform the transformation. Large\nmargin nearest neighbor (LMNN) classiﬁcation [30] also attempts to optimize KNN accuracy but\ndoes so using a hinge loss that encourages the local neighborhood of a point to contain other points\nwith the same label. The DNet-KNN [21] is another margin-based method that improves upon LMNN\nby utilizing a neural network to perform the embedding instead of a simple linear transformation.\nOf these, our method is most similar to the non-linear extension of NCA [27] because we use a\nneural network to perform the embedding and we optimize a softmax based on Euclidean distances\nin the transformed space, as opposed to a margin loss. A key distinction between our approach\nand non-linear NCA is that we form a softmax directly over classes, rather than individual points,\ncomputed from distances to each class’s prototype representation. This allows each class to have a\nconcise representation independent of the number of data points and obviates the need to store the\nentire support set to make predictions.\nOur approach is also similar to the nearest class mean approach [19], where each class is represented\nby the mean of its examples. This approach was developed to rapidly incorporate new classes into\na classiﬁer without retraining, however it relies on a linear embedding and was designed to handle\n2Features downloaded from https://github.com/reedscot/cvpr2016.\n7\n",
    "the case where the novel classes come with a large number of examples. In contrast, our approach\nutilizes neural networks to non-linearly embed points and we couple this with episodic training\nin order to handle the few-shot scenario. Mensink et al. attempt to extend their approach to also\nperform non-linear classiﬁcation, but they do so by allowing classes to have multiple prototypes.\nThey ﬁnd these prototypes in a pre-processing step by using k-means on the input space and then\nperform a multi-modal variant of their linear embedding. Prototypical networks, on the other hand,\nlearn a non-linear embedding in an end-to-end manner with no such pre-processing, producing a\nnon-linear classiﬁer that still only requires one prototype per class. In addition, our approach naturally\ngeneralizes to other distance functions, particularly Bregman divergences.\nAnother relevant few-shot learning method is the meta-learning approach proposed in Ravi and\nLarochelle [22]. The key insight here is that LSTM dynamics and gradient descent can be written\nin effectively the same way. An LSTM can then be trained to itself train a model from a given\nepisode, with the performance goal of generalizing well on the query points. Matching networks\nand prototypical networks can also be seen as forms of meta-learning, in the sense that they produce\nsimple classiﬁers dynamically from new training episodes; however the core embeddings they rely\non are ﬁxed after training. The FCE extension to matching nets involves a secondary embedding that\ndepends on the support set. However, in the few-shot scenario the amount of data is so small that a\nsimple inductive bias seems to work well, without the need to learn a custom embedding for each\nepisode.\nPrototypical networks are also related to the neural statistician [6] from the generative modeling\nliterature, which extends the variational autoencoder [12, 24] to learn generative models of datasets\nrather than individual points. One component of the neural statistician is the “statistic network” which\nsummarizes a set of data points into a statistic vector. It does this by encoding each point within a\ndataset, taking a sample mean, and applying a post-processing network to obtain an approximate\nposterior over the statistic vector. Edwards and Storkey test their model for one-shot classiﬁcation on\nthe Omniglot dataset by considering each character to be a separate dataset and making predictions\nbased on the class whose approximate posterior over the statistic vector has minimal KL-divergence\nfrom the posterior inferred by the test point. Like the neural statistician, we also produce a summary\nstatistic for each class. However, ours is a discriminative model, as beﬁts our discriminative task of\nfew-shot classiﬁcation.\nWith respect to zero-shot learning, the use of embedded meta-data in prototypical networks resembles\nthe method of [3] in that both predict the weights of a linear classiﬁer. The DS-SJE and DA-SJE\napproach of [23] also learns deep multimodal embedding functions for images and class meta-data.\nUnlike ours, they learn using an empirical risk loss. Neither [3] nor [23] uses episodic training, which\nallows us to help speed up training and regularize the model.\n5\nConclusion\nWe have proposed a simple method called prototypical networks for few-shot learning based on the\nidea that we can represent each class by the mean of its examples in a representation space learned\nby a neural network. We train these networks to speciﬁcally perform well in the few-shot setting by\nusing episodic training. The approach is far simpler and more efﬁcient than recent meta-learning\napproaches, and produces state-of-the-art results even without sophisticated extensions developed\nfor matching networks (although these can be applied to prototypical nets as well). We show how\nperformance can be greatly improved by carefully considering the chosen distance metric, and by\nmodifying the episodic learning procedure. We further demonstrate how to generalize prototypical\nnetworks to the zero-shot setting, and achieve state-of-the-art results on the CUB-200 dataset. A\nnatural direction for future work is to utilize Bregman divergences other than squared Euclidean\ndistance, corresponding to class-conditional distributions beyond spherical Gaussians. We conducted\npreliminary explorations of this, including learning a variance per dimension for each class. This did\nnot lead to any empirical gains, suggesting that the embedding network has enough ﬂexibility on its\nown without requiring additional ﬁtted parameters per class. Overall, the simplicity and effectiveness\nof prototypical networks makes it a promising approach for few-shot learning.\n8\n",
    "Acknowledgements\nWe would like to thank Marc Law, Sachin Ravi, Hugo Larochelle, Renjie Liao, and Oriol Vinyals\nfor helpful discussions. This work was supported by the Samsung GRP project and the Canadian\nInstitute for Advanced Research.\nReferences\n[1] Zeynep Akata, Florent Perronnin, Zaid Harchaoui, and Cordelia Schmid. Label-embedding for attribute-\nbased classiﬁcation. In Computer Vision and Pattern Recognition, pages 819–826, 2013.\n[2] Zeynep Akata, Scott Reed, Daniel Walter, Honglak Lee, and Bernt Schiele. Evaluation of output embed-\ndings for ﬁne-grained image classiﬁcation. In Computer Vision and Pattern Recognition, pages 2927–2936,\n2015.\n[3] Jimmy Ba, Kevin Swersky, Sanja Fidler, and Ruslan Salakhutdinov. Predicting deep zero-shot convolutional\nneural networks using textual descriptions. In International Conference on Computer Vision, pages 4247–\n4255, 2015.\n[4] Arindam Banerjee, Srujana Merugu, Inderjit S Dhillon, and Joydeep Ghosh. Clustering with bregman\ndivergences. Journal of machine learning research, 6(Oct):1705–1749, 2005.\n[5] Aurélien Bellet, Amaury Habrard, and Marc Sebban. A survey on metric learning for feature vectors and\nstructured data. arXiv preprint arXiv:1306.6709, 2013.\n[6] Harrison Edwards and Amos Storkey. Towards a neural statistician. International Conference on Learning\nRepresentations, 2017.\n[7] Mohamed Elhoseiny, Babak Saleh, and Ahmed Elgammal. Write a classiﬁer: Zero-shot learning using\npurely textual descriptions. In International Conference on Computer Vision, pages 2584–2591, 2013.\n[8] Jacob Goldberger, Geoffrey E. Hinton, Sam T. Roweis, and Ruslan Salakhutdinov. Neighbourhood\ncomponents analysis. In Advances in Neural Information Processing Systems, pages 513–520, 2004.\n[9] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735–1780,\n1997.\n[10] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\n[11] Diederik Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization.\narXiv preprint\narXiv:1412.6980, 2014.\n[12] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,\n2013.\n[13] Gregory Koch. Siamese neural networks for one-shot image recognition. Master’s thesis, University of\nToronto, 2015.\n[14] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional\nneural networks. In Advances in Neural Information Processing Systems, pages 1097–1105, 2012.\n[15] Brian Kulis. Metric learning: A survey. Foundations and Trends in Machine Learning, 5(4):287–364,\n2012.\n[16] Brenden M. Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua B. Tenenbaum. One shot learning of\nsimple visual concepts. In CogSci, 2011.\n[17] Renjie Liao, Alexander Schwing, Richard Zemel, and Raquel Urtasun. Learning deep parsimonious\nrepresentations. Advances in Neural Information Processing Systems, 2016.\n[18] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning\nResearch, 9(Nov):2579–2605, 2008.\n[19] Thomas Mensink, Jakob Verbeek, Florent Perronnin, and Gabriela Csurka. Distance-based image classiﬁ-\ncation: Generalizing to new classes at near-zero cost. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 35(11):2624–2637, 2013.\n9\n",
    "[20] Erik G Miller, Nicholas E Matsakis, and Paul A Viola. Learning from one example through shared densities\non transforms. In CVPR, volume 1, pages 464–471, 2000.\n[21] Renqiang Min, David A Stanley, Zineng Yuan, Anthony Bonner, and Zhaolei Zhang. A deep non-linear\nfeature mapping for large-margin knn classiﬁcation. In IEEE International Conference on Data Mining,\npages 357–366, 2009.\n[22] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. International Conference\non Learning Representations, 2017.\n[23] Scott Reed, Zeynep Akata, Bernt Schiele, and Honglak Lee. Learning deep representations of ﬁne-grained\nvisual descriptions. arXiv preprint arXiv:1605.05395, 2016.\n[24] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approxi-\nmate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.\n[25] Oren Rippel, Manohar Paluri, Piotr Dollar, and Lubomir Bourdev. Metric learning with adaptive density\ndiscrimination. International Conference on Learning Representations, 2016.\n[26] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large\nscale visual recognition challenge. International Journal of Computer Vision, 115(3):211–252, 2015.\n[27] Ruslan Salakhutdinov and Geoffrey E. Hinton. Learning a nonlinear embedding by preserving class\nneighbourhood structure. In AISTATS, pages 412–419, 2007.\n[28] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru\nErhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, pages 1–9, 2015.\n[29] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one shot\nlearning. In Advances in Neural Information Processing Systems, pages 3630–3638, 2016.\n[30] Kilian Q Weinberger, John Blitzer, and Lawrence K Saul. Distance metric learning for large margin nearest\nneighbor classiﬁcation. In Advances in Neural Information Processing Systems, pages 1473–1480, 2005.\n[31] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD Birds 200.\nTechnical Report CNS-TR-2010-001, California Institute of Technology, 2010.\n10\n",
    "A\nAdditional Omniglot Results\nIn Table 4 we show test classiﬁcation accuracy for prototypical networks using Euclidean distance\ntrained with 5, 20, and 60 classes per episode.\nTable 4: Additional classiﬁcation accuracy results for prototypical networks on Omniglot. Conﬁgura-\ntion of training episodes is indicated by number of classes per episode (“way”), number of support\npoints per class (“shot”) and number of query points per class (“query”). Classiﬁcation accuracy was\naveraged over 1,000 randomly generated episodes from the test set.\nTrain Episodes\n5-way Acc.\n20-way Acc.\nModel\nDist.\nShot\nQuery\nWay\n1-shot\n5-shot\n1-shot\n5-shot\nPROTONETS\nEuclid.\n1\n15\n5\n97.4%\n99.3%\n92.0%\n97.8%\nPROTONETS\nEuclid.\n1\n15\n20\n98.7%\n99.6%\n95.4%\n98.8%\nPROTONETS\nEuclid.\n1\n5\n60\n98.8%\n99.7%\n96.0%\n99.0%\nPROTONETS\nEuclid.\n5\n15\n5\n96.9%\n99.3%\n90.7%\n97.8%\nPROTONETS\nEuclid.\n5\n15\n20\n98.1%\n99.6%\n94.1%\n98.7%\nPROTONETS\nEuclid.\n5\n5\n60\n98.5%\n99.7%\n94.7%\n98.9%\nFigure 3 shows a sample t-SNE visualization [18] of the embeddings learned by prototypical networks.\nWe visualize a subset of test characters from the same alphabet in order to gain better insight, despite\nthe fact that classes in actual test episodes are likely to come from different alphabets. Even though the\nvisualized characters are minor variations of each other, the network is able to cluster the hand-drawn\ncharacters closely around the class prototypes.\nB\nAdditional miniImageNet Results\nIn Table 5 we show the full results for the comparison of training episode conﬁguration in Figure 2 of\nthe main paper.\nWe also compared Euclidean-distance prototypical networks trained with a different number of\nclasses per episode. Here we vary the classes per training episode from 5 up to 30 while keeping the\nnumber of query points per class ﬁxed at 15. The results are shown in Figure 4. Our ﬁndings indicate\nthat construction of training episodes is an important consideration in order to achieve good results\nfor few-shot classiﬁcation. Table 6 contains the full results for this set of experiments.\n11\n",
    "Figure 3: A t-SNE visualization of the embeddings learned by prototypical networks on the Omniglot\ndataset. A subset of the Tengwar script is shown (an alphabet in the test set). Class prototypes are\nindicated in black. Several misclassiﬁed characters are highlighted in red along with arrows pointing\nto the correct prototype.\n5\n10\n15\n20\n25\n30\nTraining Classes per Episode\n45%\n46%\n47%\n48%\n49%\n50%\n51%\n1-shot Accuracy (5-way)\n1-shot\n5\n10\n15\n20\n25\n30\nTraining Classes per Episode\n65.0%\n65.5%\n66.0%\n66.5%\n67.0%\n67.5%\n68.0%\n68.5%\n69.0%\n5-shot Accuracy (5-way)\n5-shot\nFigure 4: Comparison of the effect of training “way” (number of classes per episode) for prototypical\nnetworks trained on miniImageNet. Each training episode contains 15 query points per class. Error\nbars indicate 95% conﬁdence intervals as computed over 600 test episodes.\n12\n",
    "Table 5: Comparison of matching and prototypical networks on miniImageNet under cosine vs.\nEuclidean distance, 5-way vs. 20-way, and 1-shot vs. 5-shot. All experiments use a shared encoder\nfor both support and query points with embedding dimension 1,600 (architecture and training details\nare provided in Section 3.2 of the main paper). Classiﬁcation accuracy is averaged over 600 randomly\ngenerated episodes from the test set and 95% conﬁdence intervals are shown.\nTrain Episodes\n5-way Acc.\nModel\nDist.\nShot\nQuery\nWay\n1-shot\n5-shot\nMATCHING NETS / PROTONETS\nCosine\n1\n15\n5\n38.82 ± 0.69%\n44.54 ± 0.56%\nMATCHING NETS / PROTONETS\nEuclid.\n1\n15\n5\n46.61 ± 0.78%\n59.84 ± 0.64%\nMATCHING NETS / PROTONETS\nCosine\n1\n15\n20\n43.63 ± 0.76%\n51.34 ± 0.64%\nMATCHING NETS / PROTONETS\nEuclid.\n1\n15\n20\n49.17 ± 0.83%\n62.66 ± 0.71%\nMATCHING NETS\nCosine\n5\n15\n5\n46.43 ± 0.74%\n54.60 ± 0.62%\nMATCHING NETS\nEuclid.\n5\n15\n5\n46.43 ± 0.78%\n60.97 ± 0.67%\nMATCHING NETS\nCosine\n5\n15\n20\n46.46 ± 0.79%\n55.77 ± 0.69%\nMATCHING NETS\nEuclid.\n5\n15\n20\n47.99 ± 0.79%\n63.66 ± 0.68%\nPROTONETS\nCosine\n5\n15\n5\n42.48 ± 0.74%\n51.23 ± 0.63%\nPROTONETS\nEuclid.\n5\n15\n5\n44.53 ± 0.76%\n65.77 ± 0.70%\nPROTONETS\nCosine\n5\n15\n20\n42.45 ± 0.73%\n51.48 ± 0.70%\nPROTONETS\nEuclid.\n5\n15\n20\n43.57 ± 0.82%\n68.20 ± 0.66%\nTable 6: Effect of training “way” (number of classes per training episode) for prototypical networks\nwith Euclidean distance on miniImageNet. The number of query points per class in training episodes\nwas ﬁxed at 15. Classiﬁcation accuracy is averaged over 600 randomly generated episodes from the\ntest set and 95% conﬁdence intervals are shown.\nTrain Episodes\n5-way Acc.\nModel\nDist.\nShot\nQuery\nWay\n1-shot\n5-shot\nPROTONETS\nEuclid.\n1\n15\n5\n46.14 ± 0.77%\n61.36 ± 0.68%\nPROTONETS\nEuclid.\n1\n15\n10\n48.27 ± 0.79%\n64.18 ± 0.68%\nPROTONETS\nEuclid.\n1\n15\n15\n48.60 ± 0.76%\n64.62 ± 0.66%\nPROTONETS\nEuclid.\n1\n15\n20\n48.57 ± 0.79%\n65.04 ± 0.69%\nPROTONETS\nEuclid.\n1\n15\n25\n48.51 ± 0.83%\n64.63 ± 0.69%\nPROTONETS\nEuclid.\n1\n15\n30\n49.42 ± 0.78%\n65.38 ± 0.68%\nPROTONETS\nEuclid.\n5\n15\n5\n44.53 ± 0.76%\n65.77 ± 0.70%\nPROTONETS\nEuclid.\n5\n15\n10\n45.09 ± 0.79%\n67.49 ± 0.70%\nPROTONETS\nEuclid.\n5\n15\n15\n44.07 ± 0.80%\n68.03 ± 0.66%\nPROTONETS\nEuclid.\n5\n15\n20\n43.57 ± 0.82%\n68.20 ± 0.66%\nPROTONETS\nEuclid.\n5\n15\n25\n43.32 ± 0.79%\n67.66 ± 0.68%\nPROTONETS\nEuclid.\n5\n15\n30\n41.38 ± 0.81%\n66.79 ± 0.66%\n13\n"
  ],
  "full_text": "Prototypical Networks for Few-shot Learning\nJake Snell\nUniversity of Toronto∗\nKevin Swersky\nTwitter\nRichard S. Zemel\nUniversity of Toronto, Vector Institute\nAbstract\nWe propose prototypical networks for the problem of few-shot classiﬁcation, where\na classiﬁer must generalize to new classes not seen in the training set, given only\na small number of examples of each new class. Prototypical networks learn a\nmetric space in which classiﬁcation can be performed by computing distances\nto prototype representations of each class. Compared to recent approaches for\nfew-shot learning, they reﬂect a simpler inductive bias that is beneﬁcial in this\nlimited-data regime, and achieve excellent results. We provide an analysis showing\nthat some simple design decisions can yield substantial improvements over recent\napproaches involving complicated architectural choices and meta-learning. We\nfurther extend prototypical networks to zero-shot learning and achieve state-of-the-\nart results on the CU-Birds dataset.\n1\nIntroduction\nFew-shot classiﬁcation [20, 16, 13] is a task in which a classiﬁer must be adapted to accommodate\nnew classes not seen in training, given only a few examples of each of these classes. A naive approach,\nsuch as re-training the model on the new data, would severely overﬁt. While the problem is quite\ndifﬁcult, it has been demonstrated that humans have the ability to perform even one-shot classiﬁcation,\nwhere only a single example of each new class is given, with a high degree of accuracy [16].\nTwo recent approaches have made signiﬁcant progress in few-shot learning. Vinyals et al. [29]\nproposed matching networks, which uses an attention mechanism over a learned embedding of the\nlabeled set of examples (the support set) to predict classes for the unlabeled points (the query set).\nMatching networks can be interpreted as a weighted nearest-neighbor classiﬁer applied within an\nembedding space. Notably, this model utilizes sampled mini-batches called episodes during training,\nwhere each episode is designed to mimic the few-shot task by subsampling classes as well as data\npoints. The use of episodes makes the training problem more faithful to the test environment and\nthereby improves generalization. Ravi and Larochelle [22] take the episodic training idea further\nand propose a meta-learning approach to few-shot learning. Their approach involves training an\nLSTM [9] to produce the updates to a classiﬁer, given an episode, such that it will generalize well to\na test-set. Here, rather than training a single model over multiple episodes, the LSTM meta-learner\nlearns to train a custom model for each episode.\nWe attack the problem of few-shot learning by addressing the key issue of overﬁtting. Since data is\nseverely limited, we work under the assumption that a classiﬁer should have a very simple inductive\nbias. Our approach, prototypical networks, is based on the idea that there exists an embedding in\nwhich points cluster around a single prototype representation for each class. In order to do this,\nwe learn a non-linear mapping of the input into an embedding space using a neural network and\ntake a class’s prototype to be the mean of its support set in the embedding space. Classiﬁcation\nis then performed for an embedded query point by simply ﬁnding the nearest class prototype. We\nfollow the same approach to tackle zero-shot learning; here each class comes with meta-data giving\na high-level description of the class rather than a small number of labeled examples. We therefore\nlearn an embedding of the meta-data into a shared space to serve as the prototype for each class.\n*Initial work by ﬁrst author done while at Twitter.\narXiv:1703.05175v2  [cs.LG]  19 Jun 2017\n\n\nc1\nc2\nc3\nx\n(a) Few-shot\nv1\nv2\nv3\nc1\nc2\nc3\nx\n(b) Zero-shot\nFigure 1: Prototypical networks in the few-shot and zero-shot scenarios. Left: Few-shot prototypes\nck are computed as the mean of embedded support examples for each class. Right: Zero-shot\nprototypes ck are produced by embedding class meta-data vk. In either case, embedded query points\nare classiﬁed via a softmax over distances to class prototypes: pφ(y = k|x) ∝exp(−d(fφ(x), ck)).\nClassiﬁcation is performed, as in the few-shot scenario, by ﬁnding the nearest class prototype for an\nembedded query point.\nIn this paper, we formulate prototypical networks for both the few-shot and zero-shot settings. We\ndraw connections to matching networks in the one-shot setting, and analyze the underlying distance\nfunction used in the model. In particular, we relate prototypical networks to clustering [4] in order to\njustify the use of class means as prototypes when distances are computed with a Bregman divergence,\nsuch as squared Euclidean distance. We ﬁnd empirically that the choice of distance is vital, as\nEuclidean distance greatly outperforms the more commonly used cosine similarity. On several\nbenchmark tasks, we achieve state-of-the-art performance. Prototypical networks are simpler and\nmore efﬁcient than recent meta-learning algorithms, making them an appealing approach to few-shot\nand zero-shot learning.\n2\nPrototypical Networks\n2.1\nNotation\nIn few-shot classiﬁcation we are given a small support set of N labeled examples S\n=\n{(x1, y1), . . . , (xN, yN)} where each xi ∈RD is the D-dimensional feature vector of an example\nand yi ∈{1, . . . , K} is the corresponding label. Sk denotes the set of examples labeled with class k.\n2.2\nModel\nPrototypical networks compute an M-dimensional representation ck ∈RM, or prototype, of each\nclass through an embedding function fφ : RD →RM with learnable parameters φ. Each prototype\nis the mean vector of the embedded support points belonging to its class:\nck =\n1\n|Sk|\nX\n(xi,yi)∈Sk\nfφ(xi)\n(1)\nGiven a distance function d : RM × RM →[0, +∞), prototypical networks produce a distribution\nover classes for a query point x based on a softmax over distances to the prototypes in the embedding\nspace:\npφ(y = k | x) =\nexp(−d(fφ(x), ck))\nP\nk′ exp(−d(fφ(x), ck′))\n(2)\nLearning proceeds by minimizing the negative log-probability J(φ) = −log pφ(y = k | x) of the\ntrue class k via SGD. Training episodes are formed by randomly selecting a subset of classes from\nthe training set, then choosing a subset of examples within each class to act as the support set and a\nsubset of the remainder to serve as query points. Pseudocode to compute the loss J(φ) for a training\nepisode is provided in Algorithm 1.\n2\n\n\nAlgorithm 1 Training episode loss computation for prototypical networks. N is the number of\nexamples in the training set, K is the number of classes in the training set, NC ≤K is the number\nof classes per episode, NS is the number of support examples per class, NQ is the number of query\nexamples per class. RANDOMSAMPLE(S, N) denotes a set of N elements chosen uniformly at\nrandom from set S, without replacement.\nInput: Training set D = {(x1, y1), . . . , (xN, yN)}, where each yi ∈{1, . . . , K}. Dk denotes the\nsubset of D containing all elements (xi, yi) such that yi = k.\nOutput: The loss J for a randomly generated training episode.\nV ←RANDOMSAMPLE({1, . . . , K}, NC)\n▷Select class indices for episode\nfor k in {1, . . . , NC} do\nSk ←RANDOMSAMPLE(DVk, NS)\n▷Select support examples\nQk ←RANDOMSAMPLE(DVk \\ Sk, NQ)\n▷Select query examples\nck ←\n1\nNC\nX\n(xi,yi)∈Sk\nfφ(xi)\n▷Compute prototype from support examples\nend for\nJ ←0\n▷Initialize loss\nfor k in {1, . . . , NC} do\nfor (x, y) in Qk do\nJ ←J +\n1\nNCNQ\n\"\nd(fφ(x), ck)) + log\nX\nk′\nexp(−d(fφ(x), ck))\n#\n▷Update loss\nend for\nend for\n2.3\nPrototypical Networks as Mixture Density Estimation\nFor a particular class of distance functions, known as regular Bregman divergences [4], the prototypi-\ncal networks algorithm is equivalent to performing mixture density estimation on the support set with\nan exponential family density. A regular Bregman divergence dϕ is deﬁned as:\ndϕ(z, z′) = ϕ(z) −ϕ(z′) −(z −z′)T ∇ϕ(z′),\n(3)\nwhere ϕ is a differentiable, strictly convex function of the Legendre type. Examples of Bregman\ndivergences include squared Euclidean distance ∥z −z′∥2 and Mahalanobis distance.\nPrototype computation can be viewed in terms of hard clustering on the support set, with one cluster\nper class and each support point assigned to its corresponding class cluster. It has been shown [4]\nfor Bregman divergences that the cluster representative achieving minimal distance to its assigned\npoints is the cluster mean. Thus the prototype computation in Equation (1) yields optimal cluster\nrepresentatives given the support set labels when a Bregman divergence is used.\nMoreover, any regular exponential family distribution pψ(z|θ) with parameters θ and cumulant\nfunction ψ can be written in terms of a uniquely determined regular Bregman divergence [4]:\npψ(z|θ) = exp{zT θ −ψ(θ) −gψ(z)} = exp{−dϕ(z, µ(θ)) −gϕ(z)}\n(4)\nConsider now a regular exponential family mixture model with parameters Γ = {θk, πk}K\nk=1:\np(z|Γ) =\nK\nX\nk=1\nπkpψ(z|θk) =\nK\nX\nk=1\nπk exp(−dϕ(z, µ(θk)) −gϕ(z))\n(5)\nGiven Γ, inference of the cluster assignment y for an unlabeled point z becomes:\np(y = k|z) =\nπk exp(−dϕ(z, µ(θk)))\nP\nk′ πk′ exp(−dϕ(z, µ(θk)))\n(6)\nFor an equally-weighted mixture model with one cluster per class, cluster assignment inference (6) is\nequivalent to query class prediction (2) with fφ(x) = z and ck = µ(θk). In this case, prototypical\nnetworks are effectively performing mixture density estimation with an exponential family distribution\ndetermined by dϕ. The choice of distance therefore speciﬁes modeling assumptions about the class-\nconditional data distribution in the embedding space.\n3\n\n\n2.4\nReinterpretation as a Linear Model\nA simple analysis is useful in gaining insight into the nature of the learned classiﬁer. When we use\nEuclidean distance d(z, z′) = ∥z −z′∥2, then the model in Equation (2) is equivalent to a linear\nmodel with a particular parameterization [19]. To see this, expand the term in the exponent:\n−∥fφ(x) −ck∥2 = −fφ(x)⊤fφ(x) + 2c⊤\nk fφ(x) −c⊤\nk ck\n(7)\nThe ﬁrst term in Equation (7) is constant with respect to the class k, so it does not affect the softmax\nprobabilities. We can write the remaining terms as a linear model as follows:\n2c⊤\nk fφ(x) −c⊤\nk ck = w⊤\nk fφ(x) + bk, where wk = 2ck and bk = −c⊤\nk ck\n(8)\nWe focus primarily on squared Euclidean distance (corresponding to spherical Gaussian densities) in\nthis work. Our results indicate that Euclidean distance is an effective choice despite the equivalence\nto a linear model. We hypothesize this is because all of the required non-linearity can be learned\nwithin the embedding function. Indeed, this is the approach that modern neural network classiﬁcation\nsystems currently use, e.g., [14, 28].\n2.5\nComparison to Matching Networks\nPrototypical networks differ from matching networks in the few-shot case with equivalence in the\none-shot scenario. Matching networks [29] produce a weighted nearest neighbor classiﬁer given the\nsupport set, while prototypical networks produce a linear classiﬁer when squared Euclidean distance\nis used. In the case of one-shot learning, ck = xk since there is only one support point per class, and\nmatching networks and prototypical networks become equivalent.\nA natural question is whether it makes sense to use multiple prototypes per class instead of just one.\nIf the number of prototypes per class is ﬁxed and greater than 1, then this would require a partitioning\nscheme to further cluster the support points within a class. This has been proposed in Mensink\net al. [19] and Rippel et al. [25]; however both methods require a separate partitioning phase that is\ndecoupled from the weight updates, while our approach is simple to learn with ordinary gradient\ndescent methods.\nVinyals et al. [29] propose a number of extensions, including decoupling the embedding functions of\nthe support and query points, and using a second-level, fully-conditional embedding (FCE) that takes\ninto account speciﬁc points in each episode. These could likewise be incorporated into prototypical\nnetworks, however they increase the number of learnable parameters, and FCE imposes an arbitrary\nordering on the support set using a bi-directional LSTM. Instead, we show that it is possible to\nachieve the same level of performance using simple design choices, which we outline next.\n2.6\nDesign Choices\nDistance metric\nVinyals et al. [29] and Ravi and Larochelle [22] apply matching networks using\ncosine distance. However for both prototypical and matching networks any distance is permissible,\nand we found that using squared Euclidean distance can greatly improve results for both. We\nconjecture this is primarily due to cosine distance not being a Bregman divergence, and thus the\nequivalence to mixture density estimation discussed in Section 2.3 does not hold.\nEpisode composition\nA straightforward way to construct episodes, used in Vinyals et al. [29] and\nRavi and Larochelle [22], is to choose Nc classes and NS support points per class in order to match\nthe expected situation at test-time. That is, if we expect at test-time to perform 5-way classiﬁcation\nand 1-shot learning, then training episodes could be comprised of Nc = 5, NS = 1. We have found,\nhowever, that it can be extremely beneﬁcial to train with a higher Nc, or “way”, than will be used\nat test-time. In our experiments, we tune the training Nc on a held-out validation set. Another\nconsideration is whether to match NS, or “shot”, at train and test-time. For prototypical networks,\nwe found that it is usually best to train and test with the same “shot” number.\n2.7\nZero-Shot Learning\nZero-shot learning differs from few-shot learning in that instead of being given a support set of\ntraining points, we are given a class meta-data vector vk for each class. These could be determined\n4\n\n\nTable 1: Few-shot classiﬁcation accuracies on Omniglot.\n5-way Acc.\n20-way Acc.\nModel\nDist.\nFine Tune\n1-shot\n5-shot\n1-shot\n5-shot\nMATCHING NETWORKS [29]\nCosine\nN\n98.1%\n98.9%\n93.8%\n98.5%\nMATCHING NETWORKS [29]\nCosine\nY\n97.9%\n98.7%\n93.5%\n98.7%\nNEURAL STATISTICIAN [6]\n-\nN\n98.1%\n99.5%\n93.2%\n98.1%\nPROTOTYPICAL NETWORKS (OURS)\nEuclid.\nN\n98.8%\n99.7%\n96.0%\n98.9%\nin advance, or they could be learned from e.g., raw text [7]. Modifying prototypical networks to deal\nwith the zero-shot case is straightforward: we simply deﬁne ck = gϑ(vk) to be a separate embedding\nof the meta-data vector. An illustration of the zero-shot procedure for prototypical networks as\nit relates to the few-shot procedure is shown in Figure 1. Since the meta-data vector and query\npoint come from different input domains, we found it was helpful empirically to ﬁx the prototype\nembedding g to have unit length, however we do not constrain the query embedding f.\n3\nExperiments\nFor few-shot learning, we performed experiments on Omniglot [16] and the miniImageNet version\nof ILSVRC-2012 [26] with the splits proposed by Ravi and Larochelle [22]. We perform zero-shot\nexperiments on the 2011 version of the Caltech UCSD bird dataset (CUB-200 2011) [31].\n3.1\nOmniglot Few-shot Classiﬁcation\nOmniglot [16] is a dataset of 1623 handwritten characters collected from 50 alphabets. There are 20\nexamples associated with each character, where each example is drawn by a different human subject.\nWe follow the procedure of Vinyals et al. [29] by resizing the grayscale images to 28 × 28 and\naugmenting the character classes with rotations in multiples of 90 degrees. We use 1200 characters\nplus rotations for training (4,800 classes in total) and the remaining classes, including rotations, for\ntest. Our embedding architecture mirrors that used by Vinyals et al. [29] and is composed of four\nconvolutional blocks. Each block comprises a 64-ﬁlter 3 × 3 convolution, batch normalization layer\n[10], a ReLU nonlinearity and a 2 × 2 max-pooling layer. When applied to the 28 × 28 Omniglot\nimages this architecture results in a 64-dimensional output space. We use the same encoder for\nembedding both support and query points. All of our models were trained via SGD with Adam [11].\nWe used an initial learning rate of 10−3 and cut the learning rate in half every 2000 episodes. No\nregularization was used other than batch normalization.\nWe trained prototypical networks using Euclidean distance in the 1-shot and 5-shot scenarios with\ntraining episodes containing 60 classes and 5 query points per class. We found that it is advantageous\nto match the training-shot with the test-shot, and to use more classes (higher “way”) per training\nepisode rather than fewer. We compare against various baselines, including the neural statistician\n[6] and both the ﬁne-tuned and non-ﬁne-tuned versions of matching networks [29]. We computed\nclassiﬁcation accuracy for our models averaged over 1000 randomly generated episodes from the test\nset. The results are shown in Table 1 and to our knowledge they represent the state-of-the-art on this\ndataset.\n3.2\nminiImageNet Few-shot Classiﬁcation\nThe miniImageNet dataset, originally proposed by Vinyals et al. [29], is derived from the larger\nILSVRC-12 dataset [26]. The splits used by Vinyals et al. [29] consist of 60,000 color images of size\n84 × 84 divided into 100 classes with 600 examples each. For our experiments, we use the splits\nintroduced by Ravi and Larochelle [22] in order to directly compare with state-of-the-art algorithms\nfor few-shot learning. Their splits use a different set of 100 classes, divided into 64 training, 16\nvalidation, and 20 test classes. We follow their procedure by training on the 64 training classes and\nusing the 16 validation classes for monitoring generalization performance only.\nWe use the same four-block embedding architecture as in our Omniglot experiments, though here\nit results in a 1600-dimensional output space due to the increased size of the images. We also\n5\n\n\nTable 2: Few-shot classiﬁcation accuracies on miniImageNet. All accuracy results are averaged over\n600 test episodes and are reported with 95% conﬁdence intervals. ∗Results reported by [22].\n5-way Acc.\nModel\nDist.\nFine Tune\n1-shot\n5-shot\nBASELINE NEAREST NEIGHBORS∗\nCosine\nN\n28.86 ± 0.54%\n49.79 ± 0.79%\nMATCHING NETWORKS [29]∗\nCosine\nN\n43.40 ± 0.78%\n51.09 ± 0.71%\nMATCHING NETWORKS FCE [29]∗\nCosine\nN\n43.56 ± 0.84%\n55.31 ± 0.73%\nMETA-LEARNER LSTM [22]∗\n-\nN\n43.44 ± 0.77%\n60.60 ± 0.71%\nPROTOTYPICAL NETWORKS (OURS)\nEuclid.\nN\n49.42 ± 0.78%\n68.20 ± 0.66%\n5-way\nCosine\n5-way\nEuclid.\n20-way\nCosine\n20-way\nEuclid.\n1-shot\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n1-shot Accuracy (5-way)\nMatching / Proto. Nets\n5-way\nCosine\n5-way\nEuclid.\n20-way\nCosine\n20-way\nEuclid.\n5-shot\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n5-shot Accuracy (5-way)\nMatching Nets\nProto. Nets\nFigure 2: Comparison showing the effect of distance metric and number of classes per training episode\non 5-way classiﬁcation accuracy for both matching and prototypical networks on miniImageNet.\nThe x-axis indicates conﬁguration of the training episodes (way, distance, and shot), and the y-axis\nindicates 5-way test accuracy for the corresponding shot. Error bars indicate 95% conﬁdence intervals\nas computed over 600 test episodes. Note that matching networks and prototypical networks are\nidentical in the 1-shot case.\nuse the same learning rate schedule as in our Omniglot experiments and train until validation loss\nstops improving. We train using 30-way episodes for 1-shot classiﬁcation and 20-way episodes for\n5-shot classiﬁcation. We match train shot to test shot and each class contains 15 query points per\nepisode. We compare to the baselines as reported by Ravi and Larochelle [22], which include a\nsimple nearest neighbor approach on top of features learned by a classiﬁcation network on the 64\ntraining classes. The other baselines are two non-ﬁne-tuned variants of matching networks (both\nordinary and FCE) and the Meta-Learner LSTM. As can be seen in Table 2, prototypical networks\nachieves state-of-the-art here by a wide margin.\nWe conducted further analysis, to determine the effect of distance metric and the number of training\nclasses per episode on the performance of prototypical networks and matching networks. To make\nthe methods comparable, we use our own implementation of matching networks that utilizes the\nsame embedding architecture as our prototypical networks. In Figure 2 we compare cosine vs.\nEuclidean distance and 5-way vs. 20-way training episodes in the 1-shot and 5-shot scenarios, with\n15 query points per class per episode. We note that 20-way achieves higher accuracy than 5-way\nand conjecture that the increased difﬁculty of 20-way classiﬁcation helps the network to generalize\nbetter, because it forces the model to make more ﬁne-grained decisions in the embedding space. Also,\nusing Euclidean distance improves performance substantially over cosine distance. This effect is even\nmore pronounced for prototypical networks, in which computing the class prototype as the mean of\nembedded support points is more naturally suited to Euclidean distances since cosine distance is not\na Bregman divergence.\n3.3\nCUB Zero-shot Classiﬁcation\nIn order to assess the suitability of our approach for zero-shot learning, we also run experiments on\nthe Caltech-UCSD Birds (CUB) 200-2011 dataset [31]. The CUB dataset contains 11,788 images of\n200 bird species. We closely follow the procedure of Reed et al. [23] in preparing the data. We use\n6\n\n\nTable 3: Zero-shot classiﬁcation accuracies on CUB-200.\nModel\nImage\nFeatures\n50-way Acc.\n0-shot\nALE [1]\nFisher\n26.9%\nSJE [2]\nAlexNet\n40.3%\nSAMPLE CLUSTERING [17]\nAlexNet\n44.3%\nSJE [2]\nGoogLeNet\n50.1%\nDS-SJE [23]\nGoogLeNet\n50.4%\nDA-SJE [23]\nGoogLeNet\n50.9%\nPROTO. NETS (OURS)\nGoogLeNet\n54.6%\ntheir splits to divide the classes into 100 training, 50 validation, and 50 test. For images we use 1,024-\ndimensional features extracted by applying GoogLeNet [28] to middle, upper left, upper right, lower\nleft, and lower right crops of the original and horizontally-ﬂipped image2. At test time we use only\nthe middle crop of the original image. For class meta-data we use the 312-dimensional continuous\nattribute vectors provided with the CUB dataset. These attributes encode various characteristics of\nthe bird species such as their color, shape, and feather patterns.\nWe learned a simple linear mapping on top of both the 1024-dimensional image features and the\n312-dimensional attribute vectors to produce a 1,024-dimensional output space. For this dataset we\nfound it helpful to normalize the class prototypes (embedded attribute vectors) to be of unit length,\nsince the attribute vectors come from a different domain than the images. Training episodes were\nconstructed with 50 classes and 10 query images per class. The embeddings were optimized via SGD\nwith Adam at a ﬁxed learning rate of 10−4 and weight decay of 10−5. Early stopping on validation\nloss was used to determine the optimal number of epochs for retraining on the training plus validation\nset.\nTable 3 shows that we achieve state-of-the-art results by a large margin when compared to methods\nutilizing attributes as class meta-data. We compare our method to other embedding approaches, such\nas ALE [1], SJE [2], and DS-SJE/DA-SJE [23]. We also compare to a recent clustering approach\n[17] which trains an SVM on a learned feature space obtained by ﬁne-tuning AlexNet [14]. These\nzero-shot classiﬁcation results demonstrate that our approach is general enough to be applied even\nwhen the data points (images) are from a different domain relative to the classes (attributes).\n4\nRelated Work\nThe literature on metric learning is vast [15, 5]; we summarize here the work most relevant to our\nproposed method. Neighborhood Components Analysis (NCA) [8] learns a Mahalanobis distance to\nmaximize K-nearest-neighbor’s (KNN) leave-one-out accuracy in the transformed space. Salakhutdi-\nnov and Hinton [27] extend NCA by using a neural network to perform the transformation. Large\nmargin nearest neighbor (LMNN) classiﬁcation [30] also attempts to optimize KNN accuracy but\ndoes so using a hinge loss that encourages the local neighborhood of a point to contain other points\nwith the same label. The DNet-KNN [21] is another margin-based method that improves upon LMNN\nby utilizing a neural network to perform the embedding instead of a simple linear transformation.\nOf these, our method is most similar to the non-linear extension of NCA [27] because we use a\nneural network to perform the embedding and we optimize a softmax based on Euclidean distances\nin the transformed space, as opposed to a margin loss. A key distinction between our approach\nand non-linear NCA is that we form a softmax directly over classes, rather than individual points,\ncomputed from distances to each class’s prototype representation. This allows each class to have a\nconcise representation independent of the number of data points and obviates the need to store the\nentire support set to make predictions.\nOur approach is also similar to the nearest class mean approach [19], where each class is represented\nby the mean of its examples. This approach was developed to rapidly incorporate new classes into\na classiﬁer without retraining, however it relies on a linear embedding and was designed to handle\n2Features downloaded from https://github.com/reedscot/cvpr2016.\n7\n\n\nthe case where the novel classes come with a large number of examples. In contrast, our approach\nutilizes neural networks to non-linearly embed points and we couple this with episodic training\nin order to handle the few-shot scenario. Mensink et al. attempt to extend their approach to also\nperform non-linear classiﬁcation, but they do so by allowing classes to have multiple prototypes.\nThey ﬁnd these prototypes in a pre-processing step by using k-means on the input space and then\nperform a multi-modal variant of their linear embedding. Prototypical networks, on the other hand,\nlearn a non-linear embedding in an end-to-end manner with no such pre-processing, producing a\nnon-linear classiﬁer that still only requires one prototype per class. In addition, our approach naturally\ngeneralizes to other distance functions, particularly Bregman divergences.\nAnother relevant few-shot learning method is the meta-learning approach proposed in Ravi and\nLarochelle [22]. The key insight here is that LSTM dynamics and gradient descent can be written\nin effectively the same way. An LSTM can then be trained to itself train a model from a given\nepisode, with the performance goal of generalizing well on the query points. Matching networks\nand prototypical networks can also be seen as forms of meta-learning, in the sense that they produce\nsimple classiﬁers dynamically from new training episodes; however the core embeddings they rely\non are ﬁxed after training. The FCE extension to matching nets involves a secondary embedding that\ndepends on the support set. However, in the few-shot scenario the amount of data is so small that a\nsimple inductive bias seems to work well, without the need to learn a custom embedding for each\nepisode.\nPrototypical networks are also related to the neural statistician [6] from the generative modeling\nliterature, which extends the variational autoencoder [12, 24] to learn generative models of datasets\nrather than individual points. One component of the neural statistician is the “statistic network” which\nsummarizes a set of data points into a statistic vector. It does this by encoding each point within a\ndataset, taking a sample mean, and applying a post-processing network to obtain an approximate\nposterior over the statistic vector. Edwards and Storkey test their model for one-shot classiﬁcation on\nthe Omniglot dataset by considering each character to be a separate dataset and making predictions\nbased on the class whose approximate posterior over the statistic vector has minimal KL-divergence\nfrom the posterior inferred by the test point. Like the neural statistician, we also produce a summary\nstatistic for each class. However, ours is a discriminative model, as beﬁts our discriminative task of\nfew-shot classiﬁcation.\nWith respect to zero-shot learning, the use of embedded meta-data in prototypical networks resembles\nthe method of [3] in that both predict the weights of a linear classiﬁer. The DS-SJE and DA-SJE\napproach of [23] also learns deep multimodal embedding functions for images and class meta-data.\nUnlike ours, they learn using an empirical risk loss. Neither [3] nor [23] uses episodic training, which\nallows us to help speed up training and regularize the model.\n5\nConclusion\nWe have proposed a simple method called prototypical networks for few-shot learning based on the\nidea that we can represent each class by the mean of its examples in a representation space learned\nby a neural network. We train these networks to speciﬁcally perform well in the few-shot setting by\nusing episodic training. The approach is far simpler and more efﬁcient than recent meta-learning\napproaches, and produces state-of-the-art results even without sophisticated extensions developed\nfor matching networks (although these can be applied to prototypical nets as well). We show how\nperformance can be greatly improved by carefully considering the chosen distance metric, and by\nmodifying the episodic learning procedure. We further demonstrate how to generalize prototypical\nnetworks to the zero-shot setting, and achieve state-of-the-art results on the CUB-200 dataset. A\nnatural direction for future work is to utilize Bregman divergences other than squared Euclidean\ndistance, corresponding to class-conditional distributions beyond spherical Gaussians. We conducted\npreliminary explorations of this, including learning a variance per dimension for each class. This did\nnot lead to any empirical gains, suggesting that the embedding network has enough ﬂexibility on its\nown without requiring additional ﬁtted parameters per class. Overall, the simplicity and effectiveness\nof prototypical networks makes it a promising approach for few-shot learning.\n8\n\n\nAcknowledgements\nWe would like to thank Marc Law, Sachin Ravi, Hugo Larochelle, Renjie Liao, and Oriol Vinyals\nfor helpful discussions. This work was supported by the Samsung GRP project and the Canadian\nInstitute for Advanced Research.\nReferences\n[1] Zeynep Akata, Florent Perronnin, Zaid Harchaoui, and Cordelia Schmid. Label-embedding for attribute-\nbased classiﬁcation. In Computer Vision and Pattern Recognition, pages 819–826, 2013.\n[2] Zeynep Akata, Scott Reed, Daniel Walter, Honglak Lee, and Bernt Schiele. Evaluation of output embed-\ndings for ﬁne-grained image classiﬁcation. In Computer Vision and Pattern Recognition, pages 2927–2936,\n2015.\n[3] Jimmy Ba, Kevin Swersky, Sanja Fidler, and Ruslan Salakhutdinov. Predicting deep zero-shot convolutional\nneural networks using textual descriptions. In International Conference on Computer Vision, pages 4247–\n4255, 2015.\n[4] Arindam Banerjee, Srujana Merugu, Inderjit S Dhillon, and Joydeep Ghosh. Clustering with bregman\ndivergences. Journal of machine learning research, 6(Oct):1705–1749, 2005.\n[5] Aurélien Bellet, Amaury Habrard, and Marc Sebban. A survey on metric learning for feature vectors and\nstructured data. arXiv preprint arXiv:1306.6709, 2013.\n[6] Harrison Edwards and Amos Storkey. Towards a neural statistician. International Conference on Learning\nRepresentations, 2017.\n[7] Mohamed Elhoseiny, Babak Saleh, and Ahmed Elgammal. Write a classiﬁer: Zero-shot learning using\npurely textual descriptions. In International Conference on Computer Vision, pages 2584–2591, 2013.\n[8] Jacob Goldberger, Geoffrey E. Hinton, Sam T. Roweis, and Ruslan Salakhutdinov. Neighbourhood\ncomponents analysis. In Advances in Neural Information Processing Systems, pages 513–520, 2004.\n[9] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735–1780,\n1997.\n[10] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\n[11] Diederik Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization.\narXiv preprint\narXiv:1412.6980, 2014.\n[12] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,\n2013.\n[13] Gregory Koch. Siamese neural networks for one-shot image recognition. Master’s thesis, University of\nToronto, 2015.\n[14] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional\nneural networks. In Advances in Neural Information Processing Systems, pages 1097–1105, 2012.\n[15] Brian Kulis. Metric learning: A survey. Foundations and Trends in Machine Learning, 5(4):287–364,\n2012.\n[16] Brenden M. Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua B. Tenenbaum. One shot learning of\nsimple visual concepts. In CogSci, 2011.\n[17] Renjie Liao, Alexander Schwing, Richard Zemel, and Raquel Urtasun. Learning deep parsimonious\nrepresentations. Advances in Neural Information Processing Systems, 2016.\n[18] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning\nResearch, 9(Nov):2579–2605, 2008.\n[19] Thomas Mensink, Jakob Verbeek, Florent Perronnin, and Gabriela Csurka. Distance-based image classiﬁ-\ncation: Generalizing to new classes at near-zero cost. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 35(11):2624–2637, 2013.\n9\n\n\n[20] Erik G Miller, Nicholas E Matsakis, and Paul A Viola. Learning from one example through shared densities\non transforms. In CVPR, volume 1, pages 464–471, 2000.\n[21] Renqiang Min, David A Stanley, Zineng Yuan, Anthony Bonner, and Zhaolei Zhang. A deep non-linear\nfeature mapping for large-margin knn classiﬁcation. In IEEE International Conference on Data Mining,\npages 357–366, 2009.\n[22] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. International Conference\non Learning Representations, 2017.\n[23] Scott Reed, Zeynep Akata, Bernt Schiele, and Honglak Lee. Learning deep representations of ﬁne-grained\nvisual descriptions. arXiv preprint arXiv:1605.05395, 2016.\n[24] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approxi-\nmate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.\n[25] Oren Rippel, Manohar Paluri, Piotr Dollar, and Lubomir Bourdev. Metric learning with adaptive density\ndiscrimination. International Conference on Learning Representations, 2016.\n[26] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large\nscale visual recognition challenge. International Journal of Computer Vision, 115(3):211–252, 2015.\n[27] Ruslan Salakhutdinov and Geoffrey E. Hinton. Learning a nonlinear embedding by preserving class\nneighbourhood structure. In AISTATS, pages 412–419, 2007.\n[28] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru\nErhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, pages 1–9, 2015.\n[29] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one shot\nlearning. In Advances in Neural Information Processing Systems, pages 3630–3638, 2016.\n[30] Kilian Q Weinberger, John Blitzer, and Lawrence K Saul. Distance metric learning for large margin nearest\nneighbor classiﬁcation. In Advances in Neural Information Processing Systems, pages 1473–1480, 2005.\n[31] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD Birds 200.\nTechnical Report CNS-TR-2010-001, California Institute of Technology, 2010.\n10\n\n\nA\nAdditional Omniglot Results\nIn Table 4 we show test classiﬁcation accuracy for prototypical networks using Euclidean distance\ntrained with 5, 20, and 60 classes per episode.\nTable 4: Additional classiﬁcation accuracy results for prototypical networks on Omniglot. Conﬁgura-\ntion of training episodes is indicated by number of classes per episode (“way”), number of support\npoints per class (“shot”) and number of query points per class (“query”). Classiﬁcation accuracy was\naveraged over 1,000 randomly generated episodes from the test set.\nTrain Episodes\n5-way Acc.\n20-way Acc.\nModel\nDist.\nShot\nQuery\nWay\n1-shot\n5-shot\n1-shot\n5-shot\nPROTONETS\nEuclid.\n1\n15\n5\n97.4%\n99.3%\n92.0%\n97.8%\nPROTONETS\nEuclid.\n1\n15\n20\n98.7%\n99.6%\n95.4%\n98.8%\nPROTONETS\nEuclid.\n1\n5\n60\n98.8%\n99.7%\n96.0%\n99.0%\nPROTONETS\nEuclid.\n5\n15\n5\n96.9%\n99.3%\n90.7%\n97.8%\nPROTONETS\nEuclid.\n5\n15\n20\n98.1%\n99.6%\n94.1%\n98.7%\nPROTONETS\nEuclid.\n5\n5\n60\n98.5%\n99.7%\n94.7%\n98.9%\nFigure 3 shows a sample t-SNE visualization [18] of the embeddings learned by prototypical networks.\nWe visualize a subset of test characters from the same alphabet in order to gain better insight, despite\nthe fact that classes in actual test episodes are likely to come from different alphabets. Even though the\nvisualized characters are minor variations of each other, the network is able to cluster the hand-drawn\ncharacters closely around the class prototypes.\nB\nAdditional miniImageNet Results\nIn Table 5 we show the full results for the comparison of training episode conﬁguration in Figure 2 of\nthe main paper.\nWe also compared Euclidean-distance prototypical networks trained with a different number of\nclasses per episode. Here we vary the classes per training episode from 5 up to 30 while keeping the\nnumber of query points per class ﬁxed at 15. The results are shown in Figure 4. Our ﬁndings indicate\nthat construction of training episodes is an important consideration in order to achieve good results\nfor few-shot classiﬁcation. Table 6 contains the full results for this set of experiments.\n11\n\n\nFigure 3: A t-SNE visualization of the embeddings learned by prototypical networks on the Omniglot\ndataset. A subset of the Tengwar script is shown (an alphabet in the test set). Class prototypes are\nindicated in black. Several misclassiﬁed characters are highlighted in red along with arrows pointing\nto the correct prototype.\n5\n10\n15\n20\n25\n30\nTraining Classes per Episode\n45%\n46%\n47%\n48%\n49%\n50%\n51%\n1-shot Accuracy (5-way)\n1-shot\n5\n10\n15\n20\n25\n30\nTraining Classes per Episode\n65.0%\n65.5%\n66.0%\n66.5%\n67.0%\n67.5%\n68.0%\n68.5%\n69.0%\n5-shot Accuracy (5-way)\n5-shot\nFigure 4: Comparison of the effect of training “way” (number of classes per episode) for prototypical\nnetworks trained on miniImageNet. Each training episode contains 15 query points per class. Error\nbars indicate 95% conﬁdence intervals as computed over 600 test episodes.\n12\n\n\nTable 5: Comparison of matching and prototypical networks on miniImageNet under cosine vs.\nEuclidean distance, 5-way vs. 20-way, and 1-shot vs. 5-shot. All experiments use a shared encoder\nfor both support and query points with embedding dimension 1,600 (architecture and training details\nare provided in Section 3.2 of the main paper). Classiﬁcation accuracy is averaged over 600 randomly\ngenerated episodes from the test set and 95% conﬁdence intervals are shown.\nTrain Episodes\n5-way Acc.\nModel\nDist.\nShot\nQuery\nWay\n1-shot\n5-shot\nMATCHING NETS / PROTONETS\nCosine\n1\n15\n5\n38.82 ± 0.69%\n44.54 ± 0.56%\nMATCHING NETS / PROTONETS\nEuclid.\n1\n15\n5\n46.61 ± 0.78%\n59.84 ± 0.64%\nMATCHING NETS / PROTONETS\nCosine\n1\n15\n20\n43.63 ± 0.76%\n51.34 ± 0.64%\nMATCHING NETS / PROTONETS\nEuclid.\n1\n15\n20\n49.17 ± 0.83%\n62.66 ± 0.71%\nMATCHING NETS\nCosine\n5\n15\n5\n46.43 ± 0.74%\n54.60 ± 0.62%\nMATCHING NETS\nEuclid.\n5\n15\n5\n46.43 ± 0.78%\n60.97 ± 0.67%\nMATCHING NETS\nCosine\n5\n15\n20\n46.46 ± 0.79%\n55.77 ± 0.69%\nMATCHING NETS\nEuclid.\n5\n15\n20\n47.99 ± 0.79%\n63.66 ± 0.68%\nPROTONETS\nCosine\n5\n15\n5\n42.48 ± 0.74%\n51.23 ± 0.63%\nPROTONETS\nEuclid.\n5\n15\n5\n44.53 ± 0.76%\n65.77 ± 0.70%\nPROTONETS\nCosine\n5\n15\n20\n42.45 ± 0.73%\n51.48 ± 0.70%\nPROTONETS\nEuclid.\n5\n15\n20\n43.57 ± 0.82%\n68.20 ± 0.66%\nTable 6: Effect of training “way” (number of classes per training episode) for prototypical networks\nwith Euclidean distance on miniImageNet. The number of query points per class in training episodes\nwas ﬁxed at 15. Classiﬁcation accuracy is averaged over 600 randomly generated episodes from the\ntest set and 95% conﬁdence intervals are shown.\nTrain Episodes\n5-way Acc.\nModel\nDist.\nShot\nQuery\nWay\n1-shot\n5-shot\nPROTONETS\nEuclid.\n1\n15\n5\n46.14 ± 0.77%\n61.36 ± 0.68%\nPROTONETS\nEuclid.\n1\n15\n10\n48.27 ± 0.79%\n64.18 ± 0.68%\nPROTONETS\nEuclid.\n1\n15\n15\n48.60 ± 0.76%\n64.62 ± 0.66%\nPROTONETS\nEuclid.\n1\n15\n20\n48.57 ± 0.79%\n65.04 ± 0.69%\nPROTONETS\nEuclid.\n1\n15\n25\n48.51 ± 0.83%\n64.63 ± 0.69%\nPROTONETS\nEuclid.\n1\n15\n30\n49.42 ± 0.78%\n65.38 ± 0.68%\nPROTONETS\nEuclid.\n5\n15\n5\n44.53 ± 0.76%\n65.77 ± 0.70%\nPROTONETS\nEuclid.\n5\n15\n10\n45.09 ± 0.79%\n67.49 ± 0.70%\nPROTONETS\nEuclid.\n5\n15\n15\n44.07 ± 0.80%\n68.03 ± 0.66%\nPROTONETS\nEuclid.\n5\n15\n20\n43.57 ± 0.82%\n68.20 ± 0.66%\nPROTONETS\nEuclid.\n5\n15\n25\n43.32 ± 0.79%\n67.66 ± 0.68%\nPROTONETS\nEuclid.\n5\n15\n30\n41.38 ± 0.81%\n66.79 ± 0.66%\n13\n"
}