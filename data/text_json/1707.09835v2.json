{
  "filename": "1707.09835v2.pdf",
  "num_pages": 11,
  "pages": [
    "Meta-SGD: Learning to Learn Quickly\nfor Few-Shot Learning\nZhenguo Li\nFengwei Zhou\nFei Chen\nHang Li\nHuawei Noah’s Ark Lab\n{li.zhenguo, zhou.fengwei, chenfei100, hangli.hl}@huawei.com\nAbstract\nFew-shot learning is challenging for learning algorithms that learn each task in\nisolation and from scratch. In contrast, meta-learning learns from many related\ntasks a meta-learner that can learn a new task more accurately and faster with\nfewer examples, where the choice of meta-learners is crucial. In this paper, we\ndevelop Meta-SGD, an SGD-like, easily trainable meta-learner that can initialize\nand adapt any differentiable learner in just one step, on both supervised learning and\nreinforcement learning. Compared to the popular meta-learner LSTM, Meta-SGD\nis conceptually simpler, easier to implement, and can be learned more efﬁciently.\nCompared to the latest meta-learner MAML, Meta-SGD has a much higher capacity\nby learning to learn not just the learner initialization, but also the learner update\ndirection and learning rate, all in a single meta-learning process. Meta-SGD shows\nhighly competitive performance for few-shot learning on regression, classiﬁcation,\nand reinforcement learning.\n1\nIntroduction\nThe ability to learn and adapt rapidly from small data is essential to intelligence. However, current\nsuccess of deep learning relies greatly on big labeled data. It learns each task in isolation and from\nscratch, by ﬁtting a deep neural network over data through extensive, incremental model updates\nusing stochastic gradient descent (SGD). The approach is inherently data-hungry and time-consuming,\nwith fundamental challenges for problems with limited data or in dynamic environments where fast\nadaptation is critical. In contrast, humans can learn quickly from a few examples by leveraging prior\nexperience. Such capacity in data efﬁciency and fast adaptation, if realized in machine learning, can\ngreatly expand its utility. This motivates the study of few-shot learning, which aims to learn quickly\nfrom only a few examples [15].\nSeveral existing ideas may be adapted for few-shot learning. In transfer learning, one often ﬁne-tunes\na pre-trained model using target data [22], where it is challenging not to unlearn the previously\nacquired knowledge. In multi-task learning, the target task is trained jointly with auxiliary ones to\ndistill inductive bias about the target problem [4]. It is tricky to decide what to share in the joint\nmodel. In semi-supervised learning, one augments labeled target data with massive unlabeled data\nto leverage a holistic distribution of the data [28]. Strong assumptions are required for this method\nto work. While these efforts can alleviate the issue of data scarcity to some extend, the way prior\nknowledge is used is speciﬁc and not generalizable. A principled approach for few-shot learning to\nrepresenting, extracting and leveraging prior knowledge is in need.\nMeta-learning offers a new perspective to machine learning, by lifting the learning level from data to\ntasks [3, 20, 24]. Consider supervised learning. The common practice learns from a set of labeled\nexamples, while meta-learning learns from a set of (labeled) tasks, each represented as a labeled\ntraining set and a labeled testing set. The hypothesis is that by being exposed to a broad scope of a\ntask space, a learning agent may ﬁgure out a learning strategy tailored to the tasks in that space.\narXiv:1707.09835v2  [cs.LG]  28 Sep 2017\n",
    "Figure 1: Illustrating the two-level learning process of Meta-SGD. Gradual learning is performed\nacross tasks at the meta-space (θ, α) that learns the meta-learner. Rapid learning is carried out by the\nmeta-learner in the learner space θ that learns task-speciﬁc learners.\nSpeciﬁcally, in meta-learning, a learner for a speciﬁc task is learned by a learning algorithm called\nmeta-learner, which is learned on a bunch of similar tasks to maximize the combined generalization\npower of the learners of all tasks. The learning occurs at two levels and in different time-scales.\nGradual learning is performed across tasks, which learns a meta-learner to carry out rapid learning\nwithin each task, whose feedback is used to adjust the learning strategy of the meta-learner. In-\nterestingly, the learning process can continue forever, thus enabling life-long learning, and at any\nmoment, the meta-learner can be applied to learn a learner for any new task. Such a two-tiered\nlearning to learn strategy for meta-learning has been applied successfully to few-shot learning on\nclassiﬁcation [7, 18, 19, 25], regression [7, 19], and reinforcement learning [6, 7, 17, 23, 26].\nThe key in meta-learning is in the design of meta-learners to be learned. In general terms, a meta-\nlearner is a trainable learning algorithm that can train a learner, inﬂuence its behavior, or itself function\nas a learner. Meta-learners developed so far include recurrent models [6, 10, 19, 26], metrics [12, 25],\nor optimizers [2, 7, 16, 18]. A recurrent model such as Long Short-Term Memory (LSTM) [9]\nprocesses data sequentially and ﬁgures out its own learning strategy from scratch in the course [19].\nSuch meta-learners are versatile but less comprehensible, with applications in classiﬁcation [19],\nregression [10, 19], and reinforcement learning [6, 26]. A metric inﬂuences a learner by modifying\ndistances between examples. Such meta-learners are more suitable for non-parametric learners such\nas the k-nearest neighbors algorithm or its variants [12, 25]. Meta-learners above do not learn an\nexplicit learner, which is typically done by an optimizer such as SGD. This suggests that optimizers,\nif trainable, can serve as meta-learners. The meta-learner perspective of optimizers, which is used to\nbe hand-designed, opens the door for learning optimizers via meta-learning.\nRecently, LSTM is used to update models such as Convolutional Neural Network (CNN) iteratively\nlike SGD [2, 18], where both initialization and update strategy are learned via meta-learning, thus\ncalled Meta-LSTM in what follows. This should be in sharp contrast to SGD where the initialization\nis randomly chosen, the learning rate is set manually, and the update direction simply follows the\ngradient. While Meta-LSTM shows promising results on few-shot learning [18] or as a generic\noptimizer [2], it is rather difﬁcult to train. In practice, each parameter of the learner is updated\nindependently in each step, which greatly limits its potential. In this paper, we develop a new\noptimizer that is very easy to train. Our proposed meta-learner acts like SGD, thus called Meta-SGD\n(Figure 1), but the initialization, update direction, and learning rates are learned via meta-learning,\nlike Meta-LSTM. Besides much easier to train than Meta-LSTM, Meta-SGD also learns much faster\nthan Meta-LSTM. It can learn effectively from a few examples even in one step. Experimental results\non regression, classiﬁcation, and reinforcement learning unanimously show that Meta-SGD is highly\ncompetitive on few-show learning.\n2\n",
    "2\nRelated Work\nOne popular approach to few-shot learning is with generative models, where one notable work is\nby [14]. It uses probabilistic programs to represent concepts of handwritten characters, and exploits\nthe speciﬁc knowledge of how pen strokes are composed to produce characters. This work shows\nhow knowledge of related concepts can ease learning of new concepts from even one example, using\nthe principles of compositionality and learning to learn [15].\nA more general approach to few-shot learning is by meta-learning, which trains a meta-learner\nfrom many related tasks to direct the learning of a learner for a new task, without relying on ad\nhoc knowledge about the problem. The key is in developing high-capacity yet trainable meta-\nlearners. [25] suggest metrics as meta-learners for non-parametric learners such as k-nearest neighbor\nclassiﬁers. Importantly, it matches training and testing conditions in meta-learning, which works\nwell for few-shot learning and is widely adopted afterwards. Note that a metric does not really train\na learner, but inﬂuences its behavior by modifying distances between examples. As such, metric\nmeta-learners mainly work for non-parametric learners.\nEarly studies show that a recurrent neural network (RNN) can model adaptive optimization algo-\nrithms [5, 29]. This suggests its potential as meta-learners. Interestingly, [10] ﬁnd that LSTM\nperforms best as meta-learner among various architectures of RNNs. [2] formulate LSTM as a\ngeneric, SGD-like optimizer which shows promising results compared to widely used hand-designed\noptimization algorithms. In [2], LSTM is used to imitate the model update process of the learner (e.g.,\nCNN) and output model increment at each timestep. [18] extend [2] for few-shot learning, where the\nLSTM cell state represents the parameters of the learner and the variation of the cell state corresponds\nto model update (like gradient descent) of the learner. Both initialization and update strategy are\nlearned jointly [18]. However, using LSTM as meta-learner to learn a learner such as CNN incurs\nprohibitively high complexity. In practice, each parameter of the learner is updated independently in\neach step, which may signiﬁcantly limit its potential. [19] adapt a memory-augmented LSTM [8] for\nfew-shot learning, where the learning strategy is ﬁgured out as the LSTM rolls out. [7] use SGD as\nmeta-learner, but only the initialization is learned. Despite its simplicity, it works well in practice.\n3\nMeta-SGD\n3.1\nMeta-Learner\nIn this section, we propose a new meta-learner that applies to both supervised learning (i.e., classi-\nﬁcation and regression) and reinforcement learning. For simplicity, we use supervised learning as\nrunning case and discuss reinforcement learning later. How can a meta-learner Mφ initialize and\nadapt a learner fθ for a new task from a few examples T = {(xi, yi)}? One standard way updates\nthe learner iteratively from random initialization using gradient descent:\nθt = θt−1 −α∇LT (θt−1),\n(1)\nwhere LT (θ) is the empirical loss\nLT (θ) =\n1\n|T |\nX\n(x,y)∈T\nℓ(fθ(x), y)\nwith some loss function ℓ, ∇LT (θ) is the gradient of LT (θ), and α denotes the learning rate that is\noften set manually.\nWith only a few examples, it is non-trivial to decide how to initialize and when to stop the learning\nprocess to avoid overﬁtting. Besides, while gradient is an effective direction for data ﬁtting, it may\nlead to overﬁtting under the few-shot regime. This also makes it tricky to choose the learning rate.\nWhile many ideas may be applied for regularization, it remains challenging to balance between\nthe induced prior and the few-shot ﬁtting. What in need is a principled approach that determines\nall learning factors in a way that maximizes generalization power rather than data ﬁtting. Another\nimportant aspect regards the speed of learning: can we learn within a couple of iterations? Besides an\ninteresting topic on its own [14], this will enable many emerging applications such as self-driving\ncars and autonomous robots that require to learn and react in a fast changing environment.\nThe idea of learning to learn appears to be promising for few-shot learning. Instead of hand-designing\na learning algorithm for the task of interest, it learns from many related tasks how to learn, which\n3\n",
    "Meta-SGD\ntrain(Ti)\ntest(Ti)\n{Ltest(Ti)(✓0\ni)}\n(✓, ↵)\n✓\n↵\nupdate (✓, ↵)\n✓0\ni\nbatch 1\nMeta-SGD\ntrain(Ti)\ntest(Ti)\n{Ltest(Ti)(✓0\ni)}\n(✓, ↵)\n✓\n↵\n✓0\ni\nMeta-SGD\ntrain(Ti)\ntest(Ti)\n{Ltest(Ti)(✓0\ni)}\n(✓, ↵)\n✓\n↵\n✓0\ni\nupdate (✓, ↵)\nbatch 2\nbatch n\nFigure 2: Meta-training process of Meta-SGD.\nmay include how to initialize and update a learner, among others, by training a meta-learner to do\nthe learning. The key here is in developing a high-capacity yet trainable meta-learner. While other\nmeta-learners are possible, here we consider meta-learners in the form of optimizers, given their\nbroad generality and huge success in machine learning. Speciﬁcally, we aim to learn an optimizer for\nfew-shot learning.\nThere are three key ingredients in deﬁning an optimizer: initialization, update direction, and learning\nrate. The initialization is often set randomly, the update direction often follows gradient or some\nvariant (e.g., conjugate gradient), and the learning rate is usually set to be small, or decayed over\niterations. While such rules of thumb work well with a huge amount of labeled data, they are unlikely\nreliable for few-shot learning. In this paper, we present a meta-learning approach that automatically\ndetermines all the ingredients of an optimizer in an end-to-end manner.\nMathematically, we propose the following meta-learner composed of an initialization term and an\nadaptation term:\nθ′ = θ −α ◦∇LT (θ),\n(2)\nwhere θ and α are (meta-)parameters of the meta-learner to be learned, and ◦denotes element-wise\nproduct. Speciﬁcally, θ represents the state of a learner that can be used to initialize the learner for\nany new task, and α is a vector of the same size as θ that decides both the update direction and\nlearning rate. The adaptation term α ◦∇LT (θ) is a vector whose direction represents the update\ndirection and whose length represents the learning rate. Since the direction of α ◦∇LT (θ) is usually\ndifferent from that of the gradient ∇LT (θ), it implies that the meta-learner does not follow the\ngradient direction to update the learner, as does by SGD. Interestingly, given α, the adaptation is\nindeed fully determined by the gradient, like SGD.\nIn summary, given a few examples T = {(xi, yi)} for a few-shot learning problem, our meta-\nlearner ﬁrst initializes the learner with θ and then adapts it to θ′ in just one step, in a new direction\nα ◦∇LT (θ) different from the gradient ∇LT (θ) and using a learning rate implicitly implemented\nin α ◦∇LT (θ). As our meta-learner also relies on the gradient as in SGD but it is learned via\nmeta-learning rather than being hand-designed like SGD, we call it Meta-SGD.\n3.2\nMeta-training\nWe aim to train the meta-learner to perform well on many related tasks. For this purpose, assume\nthere is a distribution p(T ) over the related task space, from which we can randomly sample tasks. A\ntask T consists of a training set train(T ) and a testing set test(T ). Our objective is to maximize\nthe expected generalization power of the meta-learner on the task space. Speciﬁcally, given a task T\nsampled from p(T ), the meta-learner learns the learner based on the training set train(T ), but the\ngeneralization loss is measured on the testing set test(T ). Our goal is to train the meta-learner to\nminimize the expected generalization loss.\nMathematically, the learning of our meta-learner is formulated as the optimization problem as follows:\nmin\nθ,α ET ∼p(T )[Ltest(T )(θ′)] = ET ∼p(T )[Ltest(T )(θ −α ◦∇Ltrain(T )(θ))].\n(3)\nThe above objective is differentiable w.r.t. both θ and α, which allows to use SGD to solve it\nefﬁciently, as shown in Algorithm 1 and illustrated in Figure 2.\n4\n",
    "Algorithm 1: Meta-SGD for Supervised Learning\nInput: task distribution p(T ), learning rate β\nOutput: θ, α\n1: Initialize θ, α;\n2: while not done do\n3:\nSample batch of tasks Ti ∼p(T );\n4:\nfor all Ti do\n5:\nLtrain(Ti)(θ) ←\n1\n|train(Ti)|\nP\n(x,y)∈train(Ti)\nℓ(fθ(x), y);\n6:\nθ′\ni ←θ −α ◦∇Ltrain(Ti)(θ);\n7:\nLtest(Ti)(θ′\ni) ←\n1\n|test(Ti)|\nP\n(x,y)∈test(Ti)\nℓ(fθ′\ni(x), y);\n8:\nend\n9:\n(θ, α) ←(θ, α) −β∇(θ,α)\nP\nTi Ltest(Ti)(θ′\ni);\n10: end\nReinforcement Learning. In reinforcement learning, we regard a task as a Markov decision process\n(MDP). Hence, a task T contains a tuple (S, A, q, q0, T, r, γ), where S is a set of states, A is a set\nof actions, q : S × A × S →[0, 1] is the transition probability distribution, q0 : S →[0, 1] is the\ninitial state distribution, T ∈N is the horizon, r : S × A →R is the reward function, and γ ∈[0, 1]\nis the discount factor. The learner fθ : S × A →[0, 1] is a stochastic policy, and the loss LT (θ) is\nthe negative expected discounted reward\nLT (θ) = −Est,at∼fθ,q,q0\n\" T\nX\nt=0\nγtr(st, at)\n#\n.\n(4)\nAs in supervised learning, we train the meta-learner to minimize the expected generalization loss.\nSpeciﬁcally, given a task T sampled from p(T ), we ﬁrst sample N1 trajectories according to the\npolicy fθ. Next, we use policy gradient methods to compute the empirical policy gradient ∇LT (θ)\nand then apply equation 2 to get the updated policy fθ′. After that, we sample N2 trajectories\naccording to fθ′ and compute the generalization loss.\nThe optimization problem for reinforcement learning can be rewritten as follows:\nmin\nθ,α ET ∼p(T )[LT (θ′)] = ET ∼p(T )[LT (θ −α ◦∇LT (θ))],\n(5)\nand the algorithm is summarized in Algorithm 2.\nAlgorithm 2: Meta-SGD for Reinforcement Learning\nInput: task distribution p(T ), learning rate β\nOutput: θ, α\n1: Initialize θ, α;\n2: while not done do\n3:\nSample batch of tasks Ti ∼p(T );\n4:\nfor all Ti do\n5:\nSample N1 trajectories according to fθ;\n6:\nCompute policy gradient ∇LTi(θ);\n7:\nθ′\ni ←θ −α ◦∇LTi(θ);\n8:\nSample N2 trajectories according to fθ′\ni;\n9:\nCompute policy gradient ∇(θ,α)LTi(θ′\ni);\n10:\nend\n11:\n(θ, α) ←(θ, α) −β∇(θ,α)\nP\nTi LTi(θ′\ni);\n12: end\n5\n",
    "3.3\nRelated Meta-Learners\nLet us compare Meta-SGD with other meta-learners in the form of optimizer. MAML [7] uses\nthe original SGD as meta-learner, but the initialization is learned via meta-learning. In contrast,\nMeta-SGD also learns the update direction and the learning rate, and may have a higher capacity.\nMeta-LSTM [18] relies on LSTM to learn all initialization, update direction, and learning rate,\nlike Meta-SGD, but it incurs a much higher complexity than Meta-SGD. In practice, it learns each\nparameter of the learner independently at each step, which may limit its potential.\n4\nExperimental Results\nWe evaluate the proposed meta-learner Meta-SGD on a variety of few-shot learning problems on\nregression, classiﬁcation, and reinforcement learning. We also compare its performance with the state-\nof-the-art results reported in previous work. Our results show that Meta-SGD can learn very quickly\nfrom a few examples with only one-step adaptation. All experiments are run on Tensorﬂow [1].\n4.1\nRegression\nIn this experiment, we evaluate Meta-SGD on the problem of K-shot regression, and compare it with\nthe state-of-the-art meta-learner MAML [7]. The target function is a sine curve y(x) = A sin(ωx+b),\nwhere the amplitude A, frequency ω, and phase b follow the uniform distribution on intervals [0.1, 5.0],\n[0.8, 1.2], and [0, π], respectively. The input range is restricted to the interval [−5.0, 5.0]. The K-shot\nregression task is to estimate the underlying sine curve from only K examples.\nFor meta-training, each task consists of K ∈{5, 10, 20} training examples and 10 testing examples\nwith inputs randomly chosen from [−5.0, 5.0]. The prediction loss is measured by the mean squared\nerror (MSE). For the regressor, we follow [7] to use a small neural network with an input layer of size\n1, followed by 2 hidden layers of size 40 with ReLU nonlinearities, and then an output layer of size\n1. All weight matrices use truncated normal initialization with mean 0 and standard deviation 0.01,\nand all bias vectors are initialized by 0. For Meta-SGD, all entries in α have the same initial value\nrandomly chosen from [0.005, 0.1]. For MAML, a ﬁxed learning rate α = 0.01 is used following [7].\nBoth meta-learners use one-step adaptation and are trained for 60000 iterations with meta batch-size\nof 4 tasks.\nFor performance evaluation (meta-testing), we randomly sample 100 sine curves. For each curve,\nwe sample K examples for training with inputs randomly chosen from [−5.0, 5.0], and another 100\nexamples for testing with inputs evenly distributed on [−5.0, 5.0]. We repeat this procedure 100\ntimes and take the average of MSE. The results averaged over the sampled 100 sine curves with 95%\nconﬁdence intervals are summarized in Table 1.\nBy Table 1, Meta-SGD performs consistently better than MAML on all cases with a wide margin,\nshowing that Meta-SGD does have a higher capacity than MAML by learning all the initialization,\nupdate direction, and learning rate simultaneously, rather than just the initialization as in MAML.\nBy learning all ingredients of an optimizer across many related tasks, Meta-SGD well captures the\nproblem structure and is able to learn a learner with very few examples. In contrast, MAML regards\nthe learning rate α as a hyper-parameter and just follows the gradient of empirical loss to learn the\nlearner, which may greatly limit its capacity. Indeed, if we change the learning rate α from 0.01\nto 0.1, and re-train MAML via 5-shot meta-training, the prediction losses for 5-shot, 10-shot, and\n20-shot meta-testing increase to 1.77 ± 0.30, 1.37 ± 0.23, and 1.15 ± 0.20, respectively.\nFigure 3 shows how the meta-learners perform on a random 5-shot regression task. From Figure 3\n(left), compared to MAML, Meta-SGD can adapt more quickly to the shape of the sine curve after\njust one step update with only 5 examples, even when these examples are all in one half of the\ninput range. This shows that Meta-SGD well captures the meta-level information across all tasks.\nMoreover, it continues to improve with additional training examples during meta-tesing, as shown\nin Figure 3 (right). While the performance of MAML also gets better with more training examples,\nthe regression results of Meta-SGD are still better than those of MAML (Table 1). This shows that\nour learned optimization strategy is better than gradient descent even when applied to solve the tasks\nwith large training data.\n6\n",
    "Table 1: Meta-SGD vs MAML on few-shot regression\nMeta-training\nModels\n5-shot testing\n10-shot testing\n20-shot testing\n5-shot training\nMAML\n1.13 ± 0.18\n0.85 ± 0.14\n0.71 ± 0.12\nMeta-SGD\n0.90 ± 0.16\n0.63 ± 0.12\n0.50 ± 0.10\n10-shot training\nMAML\n1.17 ± 0.16\n0.77 ± 0.11\n0.56 ± 0.08\nMeta-SGD\n0.88 ± 0.14\n0.53 ± 0.09\n0.35 ± 0.06\n20-shot training\nMAML\n1.29 ± 0.20\n0.76 ± 0.12\n0.48 ± 0.08\nMeta-SGD\n1.01 ± 0.17\n0.54 ± 0.08\n0.31 ± 0.05\n4\n2\n0\n2\n4\n4\n2\n0\n2\n4\n6\nGround Truth\nMAML\nMeta-SGD\n4\n2\n0\n2\n4\n4\n2\n0\n2\n4\n6\nGround Truth\n10-shot\n20-shot\n40-shot\nFigure 3: Left: Meta-SGD vs MAML on 5-shot regression. Both initialization (dotted) and result\nafter one-step adaptation (solid) are shown. Right: Meta-SGD (10-shot meta-training) performs\nbetter with more training examples in meta-testing.\n4.2\nClassiﬁcation\nWe evaluate Meta-SGD on few-shot classiﬁcation using two benchmark datasets Omniglot and\nMiniImagenet.\nOmniglot. The Omniglot dataset [13] consists of 1623 characters from 50 alphabets. Each character\ncontains 20 instances drawn by different individuals. We randomly select 1200 characters for\nmeta-training, and use the remaining characters for meta-testing. We consider 5-way and 20-way\nclassiﬁcation for both 1 shot and 5 shots.\nMiniImagenet. The MiniImagenet dataset consists of 60000 color images from 100 classes, each\nwith 600 images. The data is divided into three disjoint subsets: 64 classes for meta-training, 16\nclasses for meta-validation, and 20 classes for meta-testing [18]. We consider 5-way and 20-way\nclassiﬁcation for both 1 shot and 5 shots.\nWe train the model following [25]. For an N-way K-shot classiﬁcation task, we ﬁrst sample N\nclasses from the meta-training dataset, and then in each class sample K images for training and\n15 other images for testing. We update the meta-learner once for each batch of tasks. After meta-\ntraining, we test our model with unseen classes from the meta-testing dataset. Following [7], we use a\nconvolution architecture with 4 modules, where each module consists of 3 × 3 convolutions, followed\nby batch normalization [11], a ReLU nonlinearity, and 2 × 2 max-pooling. For Omniglot, the images\nare downsampled to 28 × 28, and we use 64 ﬁlters and add an additional fully-connected layer with\ndimensionality 32 after the convolution modules. For MiniImagenet, the images are downsampled to\n84 × 84, and we use 32 ﬁlters in the convolution modules.\nWe train and evaluate Meta-SGD that adapts the learner in one step. In each iteration of meta-training,\nMeta-SGD is updated once with one batch of tasks. We follow [7] for batch size settings. For\nOmniglot, the batch size is set to 32 and 16 for 5-way and 20-way classiﬁcation, respectively. For\nMiniImagenet, the batch size is set to 4 and 2 for 1-shot and 5-shot classiﬁcation, respectively. We\nadd a regularization term to the objective function.\nThe results of Meta-SGD are summarized in Table 2 and Table 3, together with results of other\nstate-of-the-art models, including Siamese Nets [12], Matching Nets [25], Meta-LSTM [18], and\nMAML [7]. The results of previous models for 5-way and 20-way classiﬁcation on Omniglot,\n7\n",
    "Table 2: Classiﬁcation accuracies on Omniglot\n5-way Accuracy\n20-way Accuracy\n1-shot\n5-shot\n1-shot\n5-shot\nSiamese Nets\n97.3%\n98.4%\n88.2%\n97.0%\nMatching Nets\n98.1%\n98.9%\n93.8%\n98.5%\nMAML\n98.7 ± 0.4%\n99.9 ± 0.1%\n95.8 ± 0.3%\n98.9 ± 0.2%\nMeta-SGD\n99.53 ± 0.26%\n99.93 ± 0.09%\n95.93 ± 0.38%\n98.97 ± 0.19%\nTable 3: Classiﬁcation accuracies on MiniImagenet\n5-way Accuracy\n20-way Accuracy\n1-shot\n5-shot\n1-shot\n5-shot\nMatching Nets\n43.56 ± 0.84%\n55.31 ± 0.73%\n17.31 ± 0.22%\n22.69 ± 0.20%\nMeta-LSTM\n43.44 ± 0.77%\n60.60 ± 0.71%\n16.70 ± 0.23%\n26.06 ± 0.25%\nMAML\n48.70 ± 1.84%\n63.11 ± 0.92%\n16.49 ± 0.58%\n19.29 ± 0.29%\nMeta-SGD\n50.47 ± 1.87%\n64.03 ± 0.94%\n17.56 ± 0.64%\n28.92 ± 0.35%\nand 5-way classiﬁcation on MiniImagenet are reported in previous work [7], while those for 20-\nway classiﬁcation on MiniImagenet are obtained in our experiment. For the 20-way results on\nMiniImagenet, we run Matching Nets and Meta-LSTM using the implementation by [18], and\nMAML using our own implementation1. For MAML, the learning rate α is set to 0.01 as in the 5-way\ncase, and the learner is updated with one gradient step for both meta-training and meta-testing tasks\nlike Meta-SGD. All models are trained for 60000 iterations. The results represent mean accuracies\nwith 95% conﬁdence intervals over tasks.\nFor Omniglot, our model Meta-SGD is slightly better than the state-of-the-art models on all classiﬁ-\ncation tasks. In our experiments we noted that for 5-shot classiﬁcation tasks, the model performs\nbetter when it is trained with 1-shot tasks during meta-training than trained with 5-shot tasks. This\nphenomenon was observed in both 5-way and 20-way classiﬁcation. The 5-shot (meta-testing) results\nof Meta-SGD in Table 2 are obtained via 1-shot meta-training.\nFor MiniImagenet, Meta-SGD outperforms all other models in all cases. Note that Meta-SGD learns\nthe learner in just one step, making it faster to train the model and to adapt to new tasks, while\nstill improving accuracies. In comparison, previous models often update the learner using SGD\nwith multiple gradient steps or using LSTM with multiple iterations. For 20-way classiﬁcation, the\nresults of Matching Nets shown in Table 3 are obtained when the model is trained with 10-way\nclassiﬁcation tasks. When trained with 20-way classiﬁcation tasks, its accuracies drop to 12.27±0.18\nand 21.30 ± 0.21 for 1-shot and 5-shot, respectively, suggesting that Matching Nets may need more\niterations for sufﬁcient training, especially for 1-shot. We also note that for 20-way classiﬁcation,\nMAML with the learner updated in one gradient step performs worse than Matching Nets and Meta-\nLSTM. In comparison, Meta-SGD has the highest accuracies for both 1-shot and 5-shot. We also run\nexperiments on MAML for 5-way classiﬁcation where the learner is updated with 1 gradient step\nfor both meta-training and meta-testing, the mean accuracies of which are 44.40% and 61.11% for\n1-shot and 5-shot classiﬁcation, respectively. These results show the capacity of Meta-SGD in terms\nof learning speed and performance for few-shot classiﬁcation.\n4.3\nReinforcement Learning\nIn this experiment, we evaluate Meta-SGD on 2D navigation tasks, and compare it with MAML [7].\nThe purpose of this reinforcement learning experiment is to enable a point agent in 2D to quickly\nacquire a policy for the task where the agent should move from a start position to a goal position. We\nexperiment with two sets of tasks separately. In the ﬁrst set of tasks, proposed by MAML, we ﬁx the\nstart position, which is the origin (0, 0), and randomly choose a goal position from the unit square\n[−0.5, 0.5] × [−0.5, 0.5] for each task. In the second set of tasks, both of the start and goal positions\nare randomly chosen from the unit square [−0.5, 0.5] × [−0.5, 0.5].\n1The code provided by [7] does not scale for this 5-shot 20-way problem in one GPU with 12G memory\nused in our experiment.\n8\n",
    "0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\nStart\nGoal\nMAML\nMeta-SGD\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\nStart\nGoal\nMAML\nMeta-SGD\nFigure 4: Left: Meta-SGD vs MAML on a 2D navigation task with ﬁxed start position and randomly\nsampled goal position. Right: Meta-SGD vs MAML on a 2D navigation task with randomly sampled\nstart and goal positions.\nGiven a task, the state is the position of the agent in the 2D plane and the action is the velocity of\nthe agent in the next step (unit time). The new state after the agent’s taking the action is the sum of\nthe previous state and the action. The action is sampled from a Gaussian distribution produced by a\npolicy network, which takes the current state as input and outputs the mean and log variance of the\nGaussian distribution. For the policy network, we follow [7]. The mean vector is created from the\nstate via a small neural network consisting of an input layer of size 2, followed by 2 hidden layers of\nsize 100 with ReLU nonlinearities, and then an output layer of size 2. The log variance is a diagonal\nmatrix with two trainable parameters. For the agent to move to the goal position, we deﬁne the reward\nas the negative distance between the state and the goal.\nFor meta-training, we sample 20 tasks as a mini-batch in each iteration. We ﬁrst sample 20 trajectories\nper task according to the policy network and each trajectory terminates when the agent is within 0.01\nof the goal or at the step of 100. Next, we use vanilla policy gradient [27] to compute the empirical\npolicy gradient ∇L(θ) and apply θ′ = θ −α ◦∇L(θ) to update the policy network. After that, we\nsample 20 trajectories according to the updated policy network. Finally, we use Trust Region Policy\nOptimization [21] to update θ and α for all 20 tasks. For additional optimization tricks, we follow\n[7]. We take 100 iterations in total.\nFor meta-testing, we randomly sample 600 tasks. For each task, we sample 20 trajectories according\nto the policy network initialized by the meta-learner, and then update the policy network by the vanilla\npolicy gradient and the meta-learner. To evaluate the performance of the updated policy network on\nthis task, 20 new trajectories are sampled and we calculate the return, the sum of the rewards, for each\ntrajectory and take average over these returns as the average return for this task. The results averaged\nover the sampled 600 tasks with 95% conﬁdence intervals are summarized in Table 4, which show\nthat Meta-SGD has relatively higher returns than MAML on both sets of tasks.\nTable 4: Meta-SGD vs MAML on 2D navigation\nﬁxed start position\nvarying start position\nMAML\n−9.12 ± 0.66\n−10.71 ± 0.76\nMeta-SGD\n−8.64 ± 0.68\n−10.15 ± 0.62\nFigure 4 shows some qualitative results of Meta-SGD and MAML. For the set of tasks with ﬁxed start\nposition, the initialized policies with Meta-SGD and MAML perform quite similar – the agents walk\naround near the start position. After one step update of the policies, both of the agents move to the\ngoal position, and the agent guided by Meta-SGD has a stronger perception of the target. For the set\nof tasks with different start positions, the initialized policies with Meta-SGD and MAML both lead\nthe agents to the origin. The updated policies conﬁdently take the agents to the goal position, and\nstill, the policy updated by Meta-SGD performs better. All these results show that our optimization\nstrategy is better than gradient descent.\n9\n",
    "5\nConclusions\nWe have developed a new, easily trainable, SGD-like meta-learner Meta-SGD that can learn faster\nand more accurately than existing meta-learners for few-shot learning. We learn all ingredients of\nan optimizer, namely initialization, update direction, and learning rate, via meta-learning in an end-\nto-end manner, resulting in a meta-learner with a higher capacity compared to other optimizer-like\nmeta-learners. Remarkably, in just one step adaptation, Meta-SGD leads to new state-of-the-art\nresults on few-shot regression, classiﬁcation, and reinforcement learning.\nOne important future work is large-scale meta-learning. As training a meta-learner involves training\na large number of learners, this entails a far more computational demand than traditional learning\napproaches, especially if a big learner is necessary when the data for each task increases far beyond\n“few shots”. Another important problem regards the versatility or generalization capacity of meta-\nlearner, including dealing with unseen situations such as new problem setups or new task domains,\nor even multi-tasking meta-learners. We believe these problems are important to greatly expand the\npractical value of meta-learning.\nReferences\n[1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S\nCorrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorﬂow: Large-scale machine learning on\nheterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.\n[2] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, and\nNando de Freitas. Learning to learn by gradient descent by gradient descent. In NIPS, 2016.\n[3] Y Bengio, S Bengio, and J Cloutier. Learning a synaptic learning rule. In IJCNN, 1991.\n[4] Rich Caruana. Multitask learning. In Learning to learn, pages 95–133. Springer, 1998.\n[5] N. E. Cotter and P. R. Conwell. Fixed-weight networks can learn. In IJCNN, June 1990.\n[6] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2: Fast\nreinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.\n[7] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep\nnetworks. arXiv preprint arXiv:1703.03400, 2017.\n[8] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401,\n2014.\n[9] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780,\n1997.\n[10] Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent. In\nInternational Conference on Artiﬁcial Neural Networks, pages 87–94, 2001.\n[11] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. In ICML, 2015.\n[12] Gregory Koch. Siamese neural networks for one-shot image recognition. PhD thesis, University of Toronto,\n2015.\n[13] Brenden M Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua B Tenenbaum. One shot learning of\nsimple visual concepts. In CogSci, 2011.\n[14] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning through\nprobabilistic program induction. Science, 350(6266), 2015.\n[15] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines\nthat learn and think like people. Behavioral and Brain Sciences, pages 1–101, 2016.\n[16] Ke Li and Jitendra Malik. Learning to optimize. arXiv preprint arXiv:1606.01885, 2016.\n[17] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. Meta-learning with temporal convolu-\ntions. arXiv preprint arXiv:1707.03141, 2017.\n[18] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In ICLR, 2017.\n[19] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning\nwith memory-augmented neural networks. In ICML, 2016.\n10\n",
    "[20] Jurgen Schmidhuber. Evolutionary principles in self-referential learning. On learning how to learn: The\nmeta-meta-... hook.) Diploma thesis, Institut f. Informatik, Tech. Univ. Munich, 1987.\n[21] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy\noptimization. In ICML, 2015.\n[22] Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. Cnn features off-the-shelf:\nan astounding baseline for recognition. In Proceedings of the IEEE conference on computer vision and\npattern recognition workshops, pages 806–813, 2014.\n[23] Flood Sung, Li Zhang, Tao Xiang, Timothy Hospedales, and Yongxin Yang. Learning to learn: Meta-critic\nnetworks for sample efﬁcient learning. arXiv preprint arXiv:1706.09529, 2017.\n[24] Sebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media, 2012.\n[25] Oriol Vinyals, Charles Blundell, Tim Lillicrap, and Daan Wierstra. Matching networks for one shot\nlearning. In NIPS, 2016.\n[26] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles\nBlundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv preprint\narXiv:1611.05763, 2016.\n[27] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine learning, 8(3-4):229–256, 1992.\n[28] Xiao-Ming Wu, Zhenguo Li, Anthony M So, John Wright, and Shih-Fu Chang. Learning with partially\nabsorbing random walks. In NIPS, 2012.\n[29] A Steven Younger, Peter R Conwell, and Neil E Cotter. Fixed-weight on-line learning. IEEE Transactions\non Neural Networks, 10(2), 1999.\n11\n"
  ],
  "full_text": "Meta-SGD: Learning to Learn Quickly\nfor Few-Shot Learning\nZhenguo Li\nFengwei Zhou\nFei Chen\nHang Li\nHuawei Noah’s Ark Lab\n{li.zhenguo, zhou.fengwei, chenfei100, hangli.hl}@huawei.com\nAbstract\nFew-shot learning is challenging for learning algorithms that learn each task in\nisolation and from scratch. In contrast, meta-learning learns from many related\ntasks a meta-learner that can learn a new task more accurately and faster with\nfewer examples, where the choice of meta-learners is crucial. In this paper, we\ndevelop Meta-SGD, an SGD-like, easily trainable meta-learner that can initialize\nand adapt any differentiable learner in just one step, on both supervised learning and\nreinforcement learning. Compared to the popular meta-learner LSTM, Meta-SGD\nis conceptually simpler, easier to implement, and can be learned more efﬁciently.\nCompared to the latest meta-learner MAML, Meta-SGD has a much higher capacity\nby learning to learn not just the learner initialization, but also the learner update\ndirection and learning rate, all in a single meta-learning process. Meta-SGD shows\nhighly competitive performance for few-shot learning on regression, classiﬁcation,\nand reinforcement learning.\n1\nIntroduction\nThe ability to learn and adapt rapidly from small data is essential to intelligence. However, current\nsuccess of deep learning relies greatly on big labeled data. It learns each task in isolation and from\nscratch, by ﬁtting a deep neural network over data through extensive, incremental model updates\nusing stochastic gradient descent (SGD). The approach is inherently data-hungry and time-consuming,\nwith fundamental challenges for problems with limited data or in dynamic environments where fast\nadaptation is critical. In contrast, humans can learn quickly from a few examples by leveraging prior\nexperience. Such capacity in data efﬁciency and fast adaptation, if realized in machine learning, can\ngreatly expand its utility. This motivates the study of few-shot learning, which aims to learn quickly\nfrom only a few examples [15].\nSeveral existing ideas may be adapted for few-shot learning. In transfer learning, one often ﬁne-tunes\na pre-trained model using target data [22], where it is challenging not to unlearn the previously\nacquired knowledge. In multi-task learning, the target task is trained jointly with auxiliary ones to\ndistill inductive bias about the target problem [4]. It is tricky to decide what to share in the joint\nmodel. In semi-supervised learning, one augments labeled target data with massive unlabeled data\nto leverage a holistic distribution of the data [28]. Strong assumptions are required for this method\nto work. While these efforts can alleviate the issue of data scarcity to some extend, the way prior\nknowledge is used is speciﬁc and not generalizable. A principled approach for few-shot learning to\nrepresenting, extracting and leveraging prior knowledge is in need.\nMeta-learning offers a new perspective to machine learning, by lifting the learning level from data to\ntasks [3, 20, 24]. Consider supervised learning. The common practice learns from a set of labeled\nexamples, while meta-learning learns from a set of (labeled) tasks, each represented as a labeled\ntraining set and a labeled testing set. The hypothesis is that by being exposed to a broad scope of a\ntask space, a learning agent may ﬁgure out a learning strategy tailored to the tasks in that space.\narXiv:1707.09835v2  [cs.LG]  28 Sep 2017\n\n\nFigure 1: Illustrating the two-level learning process of Meta-SGD. Gradual learning is performed\nacross tasks at the meta-space (θ, α) that learns the meta-learner. Rapid learning is carried out by the\nmeta-learner in the learner space θ that learns task-speciﬁc learners.\nSpeciﬁcally, in meta-learning, a learner for a speciﬁc task is learned by a learning algorithm called\nmeta-learner, which is learned on a bunch of similar tasks to maximize the combined generalization\npower of the learners of all tasks. The learning occurs at two levels and in different time-scales.\nGradual learning is performed across tasks, which learns a meta-learner to carry out rapid learning\nwithin each task, whose feedback is used to adjust the learning strategy of the meta-learner. In-\nterestingly, the learning process can continue forever, thus enabling life-long learning, and at any\nmoment, the meta-learner can be applied to learn a learner for any new task. Such a two-tiered\nlearning to learn strategy for meta-learning has been applied successfully to few-shot learning on\nclassiﬁcation [7, 18, 19, 25], regression [7, 19], and reinforcement learning [6, 7, 17, 23, 26].\nThe key in meta-learning is in the design of meta-learners to be learned. In general terms, a meta-\nlearner is a trainable learning algorithm that can train a learner, inﬂuence its behavior, or itself function\nas a learner. Meta-learners developed so far include recurrent models [6, 10, 19, 26], metrics [12, 25],\nor optimizers [2, 7, 16, 18]. A recurrent model such as Long Short-Term Memory (LSTM) [9]\nprocesses data sequentially and ﬁgures out its own learning strategy from scratch in the course [19].\nSuch meta-learners are versatile but less comprehensible, with applications in classiﬁcation [19],\nregression [10, 19], and reinforcement learning [6, 26]. A metric inﬂuences a learner by modifying\ndistances between examples. Such meta-learners are more suitable for non-parametric learners such\nas the k-nearest neighbors algorithm or its variants [12, 25]. Meta-learners above do not learn an\nexplicit learner, which is typically done by an optimizer such as SGD. This suggests that optimizers,\nif trainable, can serve as meta-learners. The meta-learner perspective of optimizers, which is used to\nbe hand-designed, opens the door for learning optimizers via meta-learning.\nRecently, LSTM is used to update models such as Convolutional Neural Network (CNN) iteratively\nlike SGD [2, 18], where both initialization and update strategy are learned via meta-learning, thus\ncalled Meta-LSTM in what follows. This should be in sharp contrast to SGD where the initialization\nis randomly chosen, the learning rate is set manually, and the update direction simply follows the\ngradient. While Meta-LSTM shows promising results on few-shot learning [18] or as a generic\noptimizer [2], it is rather difﬁcult to train. In practice, each parameter of the learner is updated\nindependently in each step, which greatly limits its potential. In this paper, we develop a new\noptimizer that is very easy to train. Our proposed meta-learner acts like SGD, thus called Meta-SGD\n(Figure 1), but the initialization, update direction, and learning rates are learned via meta-learning,\nlike Meta-LSTM. Besides much easier to train than Meta-LSTM, Meta-SGD also learns much faster\nthan Meta-LSTM. It can learn effectively from a few examples even in one step. Experimental results\non regression, classiﬁcation, and reinforcement learning unanimously show that Meta-SGD is highly\ncompetitive on few-show learning.\n2\n\n\n2\nRelated Work\nOne popular approach to few-shot learning is with generative models, where one notable work is\nby [14]. It uses probabilistic programs to represent concepts of handwritten characters, and exploits\nthe speciﬁc knowledge of how pen strokes are composed to produce characters. This work shows\nhow knowledge of related concepts can ease learning of new concepts from even one example, using\nthe principles of compositionality and learning to learn [15].\nA more general approach to few-shot learning is by meta-learning, which trains a meta-learner\nfrom many related tasks to direct the learning of a learner for a new task, without relying on ad\nhoc knowledge about the problem. The key is in developing high-capacity yet trainable meta-\nlearners. [25] suggest metrics as meta-learners for non-parametric learners such as k-nearest neighbor\nclassiﬁers. Importantly, it matches training and testing conditions in meta-learning, which works\nwell for few-shot learning and is widely adopted afterwards. Note that a metric does not really train\na learner, but inﬂuences its behavior by modifying distances between examples. As such, metric\nmeta-learners mainly work for non-parametric learners.\nEarly studies show that a recurrent neural network (RNN) can model adaptive optimization algo-\nrithms [5, 29]. This suggests its potential as meta-learners. Interestingly, [10] ﬁnd that LSTM\nperforms best as meta-learner among various architectures of RNNs. [2] formulate LSTM as a\ngeneric, SGD-like optimizer which shows promising results compared to widely used hand-designed\noptimization algorithms. In [2], LSTM is used to imitate the model update process of the learner (e.g.,\nCNN) and output model increment at each timestep. [18] extend [2] for few-shot learning, where the\nLSTM cell state represents the parameters of the learner and the variation of the cell state corresponds\nto model update (like gradient descent) of the learner. Both initialization and update strategy are\nlearned jointly [18]. However, using LSTM as meta-learner to learn a learner such as CNN incurs\nprohibitively high complexity. In practice, each parameter of the learner is updated independently in\neach step, which may signiﬁcantly limit its potential. [19] adapt a memory-augmented LSTM [8] for\nfew-shot learning, where the learning strategy is ﬁgured out as the LSTM rolls out. [7] use SGD as\nmeta-learner, but only the initialization is learned. Despite its simplicity, it works well in practice.\n3\nMeta-SGD\n3.1\nMeta-Learner\nIn this section, we propose a new meta-learner that applies to both supervised learning (i.e., classi-\nﬁcation and regression) and reinforcement learning. For simplicity, we use supervised learning as\nrunning case and discuss reinforcement learning later. How can a meta-learner Mφ initialize and\nadapt a learner fθ for a new task from a few examples T = {(xi, yi)}? One standard way updates\nthe learner iteratively from random initialization using gradient descent:\nθt = θt−1 −α∇LT (θt−1),\n(1)\nwhere LT (θ) is the empirical loss\nLT (θ) =\n1\n|T |\nX\n(x,y)∈T\nℓ(fθ(x), y)\nwith some loss function ℓ, ∇LT (θ) is the gradient of LT (θ), and α denotes the learning rate that is\noften set manually.\nWith only a few examples, it is non-trivial to decide how to initialize and when to stop the learning\nprocess to avoid overﬁtting. Besides, while gradient is an effective direction for data ﬁtting, it may\nlead to overﬁtting under the few-shot regime. This also makes it tricky to choose the learning rate.\nWhile many ideas may be applied for regularization, it remains challenging to balance between\nthe induced prior and the few-shot ﬁtting. What in need is a principled approach that determines\nall learning factors in a way that maximizes generalization power rather than data ﬁtting. Another\nimportant aspect regards the speed of learning: can we learn within a couple of iterations? Besides an\ninteresting topic on its own [14], this will enable many emerging applications such as self-driving\ncars and autonomous robots that require to learn and react in a fast changing environment.\nThe idea of learning to learn appears to be promising for few-shot learning. Instead of hand-designing\na learning algorithm for the task of interest, it learns from many related tasks how to learn, which\n3\n\n\nMeta-SGD\ntrain(Ti)\ntest(Ti)\n{Ltest(Ti)(✓0\ni)}\n(✓, ↵)\n✓\n↵\nupdate (✓, ↵)\n✓0\ni\nbatch 1\nMeta-SGD\ntrain(Ti)\ntest(Ti)\n{Ltest(Ti)(✓0\ni)}\n(✓, ↵)\n✓\n↵\n✓0\ni\nMeta-SGD\ntrain(Ti)\ntest(Ti)\n{Ltest(Ti)(✓0\ni)}\n(✓, ↵)\n✓\n↵\n✓0\ni\nupdate (✓, ↵)\nbatch 2\nbatch n\nFigure 2: Meta-training process of Meta-SGD.\nmay include how to initialize and update a learner, among others, by training a meta-learner to do\nthe learning. The key here is in developing a high-capacity yet trainable meta-learner. While other\nmeta-learners are possible, here we consider meta-learners in the form of optimizers, given their\nbroad generality and huge success in machine learning. Speciﬁcally, we aim to learn an optimizer for\nfew-shot learning.\nThere are three key ingredients in deﬁning an optimizer: initialization, update direction, and learning\nrate. The initialization is often set randomly, the update direction often follows gradient or some\nvariant (e.g., conjugate gradient), and the learning rate is usually set to be small, or decayed over\niterations. While such rules of thumb work well with a huge amount of labeled data, they are unlikely\nreliable for few-shot learning. In this paper, we present a meta-learning approach that automatically\ndetermines all the ingredients of an optimizer in an end-to-end manner.\nMathematically, we propose the following meta-learner composed of an initialization term and an\nadaptation term:\nθ′ = θ −α ◦∇LT (θ),\n(2)\nwhere θ and α are (meta-)parameters of the meta-learner to be learned, and ◦denotes element-wise\nproduct. Speciﬁcally, θ represents the state of a learner that can be used to initialize the learner for\nany new task, and α is a vector of the same size as θ that decides both the update direction and\nlearning rate. The adaptation term α ◦∇LT (θ) is a vector whose direction represents the update\ndirection and whose length represents the learning rate. Since the direction of α ◦∇LT (θ) is usually\ndifferent from that of the gradient ∇LT (θ), it implies that the meta-learner does not follow the\ngradient direction to update the learner, as does by SGD. Interestingly, given α, the adaptation is\nindeed fully determined by the gradient, like SGD.\nIn summary, given a few examples T = {(xi, yi)} for a few-shot learning problem, our meta-\nlearner ﬁrst initializes the learner with θ and then adapts it to θ′ in just one step, in a new direction\nα ◦∇LT (θ) different from the gradient ∇LT (θ) and using a learning rate implicitly implemented\nin α ◦∇LT (θ). As our meta-learner also relies on the gradient as in SGD but it is learned via\nmeta-learning rather than being hand-designed like SGD, we call it Meta-SGD.\n3.2\nMeta-training\nWe aim to train the meta-learner to perform well on many related tasks. For this purpose, assume\nthere is a distribution p(T ) over the related task space, from which we can randomly sample tasks. A\ntask T consists of a training set train(T ) and a testing set test(T ). Our objective is to maximize\nthe expected generalization power of the meta-learner on the task space. Speciﬁcally, given a task T\nsampled from p(T ), the meta-learner learns the learner based on the training set train(T ), but the\ngeneralization loss is measured on the testing set test(T ). Our goal is to train the meta-learner to\nminimize the expected generalization loss.\nMathematically, the learning of our meta-learner is formulated as the optimization problem as follows:\nmin\nθ,α ET ∼p(T )[Ltest(T )(θ′)] = ET ∼p(T )[Ltest(T )(θ −α ◦∇Ltrain(T )(θ))].\n(3)\nThe above objective is differentiable w.r.t. both θ and α, which allows to use SGD to solve it\nefﬁciently, as shown in Algorithm 1 and illustrated in Figure 2.\n4\n\n\nAlgorithm 1: Meta-SGD for Supervised Learning\nInput: task distribution p(T ), learning rate β\nOutput: θ, α\n1: Initialize θ, α;\n2: while not done do\n3:\nSample batch of tasks Ti ∼p(T );\n4:\nfor all Ti do\n5:\nLtrain(Ti)(θ) ←\n1\n|train(Ti)|\nP\n(x,y)∈train(Ti)\nℓ(fθ(x), y);\n6:\nθ′\ni ←θ −α ◦∇Ltrain(Ti)(θ);\n7:\nLtest(Ti)(θ′\ni) ←\n1\n|test(Ti)|\nP\n(x,y)∈test(Ti)\nℓ(fθ′\ni(x), y);\n8:\nend\n9:\n(θ, α) ←(θ, α) −β∇(θ,α)\nP\nTi Ltest(Ti)(θ′\ni);\n10: end\nReinforcement Learning. In reinforcement learning, we regard a task as a Markov decision process\n(MDP). Hence, a task T contains a tuple (S, A, q, q0, T, r, γ), where S is a set of states, A is a set\nof actions, q : S × A × S →[0, 1] is the transition probability distribution, q0 : S →[0, 1] is the\ninitial state distribution, T ∈N is the horizon, r : S × A →R is the reward function, and γ ∈[0, 1]\nis the discount factor. The learner fθ : S × A →[0, 1] is a stochastic policy, and the loss LT (θ) is\nthe negative expected discounted reward\nLT (θ) = −Est,at∼fθ,q,q0\n\" T\nX\nt=0\nγtr(st, at)\n#\n.\n(4)\nAs in supervised learning, we train the meta-learner to minimize the expected generalization loss.\nSpeciﬁcally, given a task T sampled from p(T ), we ﬁrst sample N1 trajectories according to the\npolicy fθ. Next, we use policy gradient methods to compute the empirical policy gradient ∇LT (θ)\nand then apply equation 2 to get the updated policy fθ′. After that, we sample N2 trajectories\naccording to fθ′ and compute the generalization loss.\nThe optimization problem for reinforcement learning can be rewritten as follows:\nmin\nθ,α ET ∼p(T )[LT (θ′)] = ET ∼p(T )[LT (θ −α ◦∇LT (θ))],\n(5)\nand the algorithm is summarized in Algorithm 2.\nAlgorithm 2: Meta-SGD for Reinforcement Learning\nInput: task distribution p(T ), learning rate β\nOutput: θ, α\n1: Initialize θ, α;\n2: while not done do\n3:\nSample batch of tasks Ti ∼p(T );\n4:\nfor all Ti do\n5:\nSample N1 trajectories according to fθ;\n6:\nCompute policy gradient ∇LTi(θ);\n7:\nθ′\ni ←θ −α ◦∇LTi(θ);\n8:\nSample N2 trajectories according to fθ′\ni;\n9:\nCompute policy gradient ∇(θ,α)LTi(θ′\ni);\n10:\nend\n11:\n(θ, α) ←(θ, α) −β∇(θ,α)\nP\nTi LTi(θ′\ni);\n12: end\n5\n\n\n3.3\nRelated Meta-Learners\nLet us compare Meta-SGD with other meta-learners in the form of optimizer. MAML [7] uses\nthe original SGD as meta-learner, but the initialization is learned via meta-learning. In contrast,\nMeta-SGD also learns the update direction and the learning rate, and may have a higher capacity.\nMeta-LSTM [18] relies on LSTM to learn all initialization, update direction, and learning rate,\nlike Meta-SGD, but it incurs a much higher complexity than Meta-SGD. In practice, it learns each\nparameter of the learner independently at each step, which may limit its potential.\n4\nExperimental Results\nWe evaluate the proposed meta-learner Meta-SGD on a variety of few-shot learning problems on\nregression, classiﬁcation, and reinforcement learning. We also compare its performance with the state-\nof-the-art results reported in previous work. Our results show that Meta-SGD can learn very quickly\nfrom a few examples with only one-step adaptation. All experiments are run on Tensorﬂow [1].\n4.1\nRegression\nIn this experiment, we evaluate Meta-SGD on the problem of K-shot regression, and compare it with\nthe state-of-the-art meta-learner MAML [7]. The target function is a sine curve y(x) = A sin(ωx+b),\nwhere the amplitude A, frequency ω, and phase b follow the uniform distribution on intervals [0.1, 5.0],\n[0.8, 1.2], and [0, π], respectively. The input range is restricted to the interval [−5.0, 5.0]. The K-shot\nregression task is to estimate the underlying sine curve from only K examples.\nFor meta-training, each task consists of K ∈{5, 10, 20} training examples and 10 testing examples\nwith inputs randomly chosen from [−5.0, 5.0]. The prediction loss is measured by the mean squared\nerror (MSE). For the regressor, we follow [7] to use a small neural network with an input layer of size\n1, followed by 2 hidden layers of size 40 with ReLU nonlinearities, and then an output layer of size\n1. All weight matrices use truncated normal initialization with mean 0 and standard deviation 0.01,\nand all bias vectors are initialized by 0. For Meta-SGD, all entries in α have the same initial value\nrandomly chosen from [0.005, 0.1]. For MAML, a ﬁxed learning rate α = 0.01 is used following [7].\nBoth meta-learners use one-step adaptation and are trained for 60000 iterations with meta batch-size\nof 4 tasks.\nFor performance evaluation (meta-testing), we randomly sample 100 sine curves. For each curve,\nwe sample K examples for training with inputs randomly chosen from [−5.0, 5.0], and another 100\nexamples for testing with inputs evenly distributed on [−5.0, 5.0]. We repeat this procedure 100\ntimes and take the average of MSE. The results averaged over the sampled 100 sine curves with 95%\nconﬁdence intervals are summarized in Table 1.\nBy Table 1, Meta-SGD performs consistently better than MAML on all cases with a wide margin,\nshowing that Meta-SGD does have a higher capacity than MAML by learning all the initialization,\nupdate direction, and learning rate simultaneously, rather than just the initialization as in MAML.\nBy learning all ingredients of an optimizer across many related tasks, Meta-SGD well captures the\nproblem structure and is able to learn a learner with very few examples. In contrast, MAML regards\nthe learning rate α as a hyper-parameter and just follows the gradient of empirical loss to learn the\nlearner, which may greatly limit its capacity. Indeed, if we change the learning rate α from 0.01\nto 0.1, and re-train MAML via 5-shot meta-training, the prediction losses for 5-shot, 10-shot, and\n20-shot meta-testing increase to 1.77 ± 0.30, 1.37 ± 0.23, and 1.15 ± 0.20, respectively.\nFigure 3 shows how the meta-learners perform on a random 5-shot regression task. From Figure 3\n(left), compared to MAML, Meta-SGD can adapt more quickly to the shape of the sine curve after\njust one step update with only 5 examples, even when these examples are all in one half of the\ninput range. This shows that Meta-SGD well captures the meta-level information across all tasks.\nMoreover, it continues to improve with additional training examples during meta-tesing, as shown\nin Figure 3 (right). While the performance of MAML also gets better with more training examples,\nthe regression results of Meta-SGD are still better than those of MAML (Table 1). This shows that\nour learned optimization strategy is better than gradient descent even when applied to solve the tasks\nwith large training data.\n6\n\n\nTable 1: Meta-SGD vs MAML on few-shot regression\nMeta-training\nModels\n5-shot testing\n10-shot testing\n20-shot testing\n5-shot training\nMAML\n1.13 ± 0.18\n0.85 ± 0.14\n0.71 ± 0.12\nMeta-SGD\n0.90 ± 0.16\n0.63 ± 0.12\n0.50 ± 0.10\n10-shot training\nMAML\n1.17 ± 0.16\n0.77 ± 0.11\n0.56 ± 0.08\nMeta-SGD\n0.88 ± 0.14\n0.53 ± 0.09\n0.35 ± 0.06\n20-shot training\nMAML\n1.29 ± 0.20\n0.76 ± 0.12\n0.48 ± 0.08\nMeta-SGD\n1.01 ± 0.17\n0.54 ± 0.08\n0.31 ± 0.05\n4\n2\n0\n2\n4\n4\n2\n0\n2\n4\n6\nGround Truth\nMAML\nMeta-SGD\n4\n2\n0\n2\n4\n4\n2\n0\n2\n4\n6\nGround Truth\n10-shot\n20-shot\n40-shot\nFigure 3: Left: Meta-SGD vs MAML on 5-shot regression. Both initialization (dotted) and result\nafter one-step adaptation (solid) are shown. Right: Meta-SGD (10-shot meta-training) performs\nbetter with more training examples in meta-testing.\n4.2\nClassiﬁcation\nWe evaluate Meta-SGD on few-shot classiﬁcation using two benchmark datasets Omniglot and\nMiniImagenet.\nOmniglot. The Omniglot dataset [13] consists of 1623 characters from 50 alphabets. Each character\ncontains 20 instances drawn by different individuals. We randomly select 1200 characters for\nmeta-training, and use the remaining characters for meta-testing. We consider 5-way and 20-way\nclassiﬁcation for both 1 shot and 5 shots.\nMiniImagenet. The MiniImagenet dataset consists of 60000 color images from 100 classes, each\nwith 600 images. The data is divided into three disjoint subsets: 64 classes for meta-training, 16\nclasses for meta-validation, and 20 classes for meta-testing [18]. We consider 5-way and 20-way\nclassiﬁcation for both 1 shot and 5 shots.\nWe train the model following [25]. For an N-way K-shot classiﬁcation task, we ﬁrst sample N\nclasses from the meta-training dataset, and then in each class sample K images for training and\n15 other images for testing. We update the meta-learner once for each batch of tasks. After meta-\ntraining, we test our model with unseen classes from the meta-testing dataset. Following [7], we use a\nconvolution architecture with 4 modules, where each module consists of 3 × 3 convolutions, followed\nby batch normalization [11], a ReLU nonlinearity, and 2 × 2 max-pooling. For Omniglot, the images\nare downsampled to 28 × 28, and we use 64 ﬁlters and add an additional fully-connected layer with\ndimensionality 32 after the convolution modules. For MiniImagenet, the images are downsampled to\n84 × 84, and we use 32 ﬁlters in the convolution modules.\nWe train and evaluate Meta-SGD that adapts the learner in one step. In each iteration of meta-training,\nMeta-SGD is updated once with one batch of tasks. We follow [7] for batch size settings. For\nOmniglot, the batch size is set to 32 and 16 for 5-way and 20-way classiﬁcation, respectively. For\nMiniImagenet, the batch size is set to 4 and 2 for 1-shot and 5-shot classiﬁcation, respectively. We\nadd a regularization term to the objective function.\nThe results of Meta-SGD are summarized in Table 2 and Table 3, together with results of other\nstate-of-the-art models, including Siamese Nets [12], Matching Nets [25], Meta-LSTM [18], and\nMAML [7]. The results of previous models for 5-way and 20-way classiﬁcation on Omniglot,\n7\n\n\nTable 2: Classiﬁcation accuracies on Omniglot\n5-way Accuracy\n20-way Accuracy\n1-shot\n5-shot\n1-shot\n5-shot\nSiamese Nets\n97.3%\n98.4%\n88.2%\n97.0%\nMatching Nets\n98.1%\n98.9%\n93.8%\n98.5%\nMAML\n98.7 ± 0.4%\n99.9 ± 0.1%\n95.8 ± 0.3%\n98.9 ± 0.2%\nMeta-SGD\n99.53 ± 0.26%\n99.93 ± 0.09%\n95.93 ± 0.38%\n98.97 ± 0.19%\nTable 3: Classiﬁcation accuracies on MiniImagenet\n5-way Accuracy\n20-way Accuracy\n1-shot\n5-shot\n1-shot\n5-shot\nMatching Nets\n43.56 ± 0.84%\n55.31 ± 0.73%\n17.31 ± 0.22%\n22.69 ± 0.20%\nMeta-LSTM\n43.44 ± 0.77%\n60.60 ± 0.71%\n16.70 ± 0.23%\n26.06 ± 0.25%\nMAML\n48.70 ± 1.84%\n63.11 ± 0.92%\n16.49 ± 0.58%\n19.29 ± 0.29%\nMeta-SGD\n50.47 ± 1.87%\n64.03 ± 0.94%\n17.56 ± 0.64%\n28.92 ± 0.35%\nand 5-way classiﬁcation on MiniImagenet are reported in previous work [7], while those for 20-\nway classiﬁcation on MiniImagenet are obtained in our experiment. For the 20-way results on\nMiniImagenet, we run Matching Nets and Meta-LSTM using the implementation by [18], and\nMAML using our own implementation1. For MAML, the learning rate α is set to 0.01 as in the 5-way\ncase, and the learner is updated with one gradient step for both meta-training and meta-testing tasks\nlike Meta-SGD. All models are trained for 60000 iterations. The results represent mean accuracies\nwith 95% conﬁdence intervals over tasks.\nFor Omniglot, our model Meta-SGD is slightly better than the state-of-the-art models on all classiﬁ-\ncation tasks. In our experiments we noted that for 5-shot classiﬁcation tasks, the model performs\nbetter when it is trained with 1-shot tasks during meta-training than trained with 5-shot tasks. This\nphenomenon was observed in both 5-way and 20-way classiﬁcation. The 5-shot (meta-testing) results\nof Meta-SGD in Table 2 are obtained via 1-shot meta-training.\nFor MiniImagenet, Meta-SGD outperforms all other models in all cases. Note that Meta-SGD learns\nthe learner in just one step, making it faster to train the model and to adapt to new tasks, while\nstill improving accuracies. In comparison, previous models often update the learner using SGD\nwith multiple gradient steps or using LSTM with multiple iterations. For 20-way classiﬁcation, the\nresults of Matching Nets shown in Table 3 are obtained when the model is trained with 10-way\nclassiﬁcation tasks. When trained with 20-way classiﬁcation tasks, its accuracies drop to 12.27±0.18\nand 21.30 ± 0.21 for 1-shot and 5-shot, respectively, suggesting that Matching Nets may need more\niterations for sufﬁcient training, especially for 1-shot. We also note that for 20-way classiﬁcation,\nMAML with the learner updated in one gradient step performs worse than Matching Nets and Meta-\nLSTM. In comparison, Meta-SGD has the highest accuracies for both 1-shot and 5-shot. We also run\nexperiments on MAML for 5-way classiﬁcation where the learner is updated with 1 gradient step\nfor both meta-training and meta-testing, the mean accuracies of which are 44.40% and 61.11% for\n1-shot and 5-shot classiﬁcation, respectively. These results show the capacity of Meta-SGD in terms\nof learning speed and performance for few-shot classiﬁcation.\n4.3\nReinforcement Learning\nIn this experiment, we evaluate Meta-SGD on 2D navigation tasks, and compare it with MAML [7].\nThe purpose of this reinforcement learning experiment is to enable a point agent in 2D to quickly\nacquire a policy for the task where the agent should move from a start position to a goal position. We\nexperiment with two sets of tasks separately. In the ﬁrst set of tasks, proposed by MAML, we ﬁx the\nstart position, which is the origin (0, 0), and randomly choose a goal position from the unit square\n[−0.5, 0.5] × [−0.5, 0.5] for each task. In the second set of tasks, both of the start and goal positions\nare randomly chosen from the unit square [−0.5, 0.5] × [−0.5, 0.5].\n1The code provided by [7] does not scale for this 5-shot 20-way problem in one GPU with 12G memory\nused in our experiment.\n8\n\n\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\nStart\nGoal\nMAML\nMeta-SGD\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\nStart\nGoal\nMAML\nMeta-SGD\nFigure 4: Left: Meta-SGD vs MAML on a 2D navigation task with ﬁxed start position and randomly\nsampled goal position. Right: Meta-SGD vs MAML on a 2D navigation task with randomly sampled\nstart and goal positions.\nGiven a task, the state is the position of the agent in the 2D plane and the action is the velocity of\nthe agent in the next step (unit time). The new state after the agent’s taking the action is the sum of\nthe previous state and the action. The action is sampled from a Gaussian distribution produced by a\npolicy network, which takes the current state as input and outputs the mean and log variance of the\nGaussian distribution. For the policy network, we follow [7]. The mean vector is created from the\nstate via a small neural network consisting of an input layer of size 2, followed by 2 hidden layers of\nsize 100 with ReLU nonlinearities, and then an output layer of size 2. The log variance is a diagonal\nmatrix with two trainable parameters. For the agent to move to the goal position, we deﬁne the reward\nas the negative distance between the state and the goal.\nFor meta-training, we sample 20 tasks as a mini-batch in each iteration. We ﬁrst sample 20 trajectories\nper task according to the policy network and each trajectory terminates when the agent is within 0.01\nof the goal or at the step of 100. Next, we use vanilla policy gradient [27] to compute the empirical\npolicy gradient ∇L(θ) and apply θ′ = θ −α ◦∇L(θ) to update the policy network. After that, we\nsample 20 trajectories according to the updated policy network. Finally, we use Trust Region Policy\nOptimization [21] to update θ and α for all 20 tasks. For additional optimization tricks, we follow\n[7]. We take 100 iterations in total.\nFor meta-testing, we randomly sample 600 tasks. For each task, we sample 20 trajectories according\nto the policy network initialized by the meta-learner, and then update the policy network by the vanilla\npolicy gradient and the meta-learner. To evaluate the performance of the updated policy network on\nthis task, 20 new trajectories are sampled and we calculate the return, the sum of the rewards, for each\ntrajectory and take average over these returns as the average return for this task. The results averaged\nover the sampled 600 tasks with 95% conﬁdence intervals are summarized in Table 4, which show\nthat Meta-SGD has relatively higher returns than MAML on both sets of tasks.\nTable 4: Meta-SGD vs MAML on 2D navigation\nﬁxed start position\nvarying start position\nMAML\n−9.12 ± 0.66\n−10.71 ± 0.76\nMeta-SGD\n−8.64 ± 0.68\n−10.15 ± 0.62\nFigure 4 shows some qualitative results of Meta-SGD and MAML. For the set of tasks with ﬁxed start\nposition, the initialized policies with Meta-SGD and MAML perform quite similar – the agents walk\naround near the start position. After one step update of the policies, both of the agents move to the\ngoal position, and the agent guided by Meta-SGD has a stronger perception of the target. For the set\nof tasks with different start positions, the initialized policies with Meta-SGD and MAML both lead\nthe agents to the origin. The updated policies conﬁdently take the agents to the goal position, and\nstill, the policy updated by Meta-SGD performs better. All these results show that our optimization\nstrategy is better than gradient descent.\n9\n\n\n5\nConclusions\nWe have developed a new, easily trainable, SGD-like meta-learner Meta-SGD that can learn faster\nand more accurately than existing meta-learners for few-shot learning. We learn all ingredients of\nan optimizer, namely initialization, update direction, and learning rate, via meta-learning in an end-\nto-end manner, resulting in a meta-learner with a higher capacity compared to other optimizer-like\nmeta-learners. Remarkably, in just one step adaptation, Meta-SGD leads to new state-of-the-art\nresults on few-shot regression, classiﬁcation, and reinforcement learning.\nOne important future work is large-scale meta-learning. As training a meta-learner involves training\na large number of learners, this entails a far more computational demand than traditional learning\napproaches, especially if a big learner is necessary when the data for each task increases far beyond\n“few shots”. Another important problem regards the versatility or generalization capacity of meta-\nlearner, including dealing with unseen situations such as new problem setups or new task domains,\nor even multi-tasking meta-learners. We believe these problems are important to greatly expand the\npractical value of meta-learning.\nReferences\n[1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S\nCorrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorﬂow: Large-scale machine learning on\nheterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.\n[2] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, and\nNando de Freitas. Learning to learn by gradient descent by gradient descent. In NIPS, 2016.\n[3] Y Bengio, S Bengio, and J Cloutier. Learning a synaptic learning rule. In IJCNN, 1991.\n[4] Rich Caruana. Multitask learning. In Learning to learn, pages 95–133. Springer, 1998.\n[5] N. E. Cotter and P. R. Conwell. Fixed-weight networks can learn. In IJCNN, June 1990.\n[6] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2: Fast\nreinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.\n[7] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep\nnetworks. arXiv preprint arXiv:1703.03400, 2017.\n[8] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401,\n2014.\n[9] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780,\n1997.\n[10] Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent. In\nInternational Conference on Artiﬁcial Neural Networks, pages 87–94, 2001.\n[11] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. In ICML, 2015.\n[12] Gregory Koch. Siamese neural networks for one-shot image recognition. PhD thesis, University of Toronto,\n2015.\n[13] Brenden M Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua B Tenenbaum. One shot learning of\nsimple visual concepts. In CogSci, 2011.\n[14] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning through\nprobabilistic program induction. Science, 350(6266), 2015.\n[15] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines\nthat learn and think like people. Behavioral and Brain Sciences, pages 1–101, 2016.\n[16] Ke Li and Jitendra Malik. Learning to optimize. arXiv preprint arXiv:1606.01885, 2016.\n[17] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. Meta-learning with temporal convolu-\ntions. arXiv preprint arXiv:1707.03141, 2017.\n[18] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In ICLR, 2017.\n[19] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning\nwith memory-augmented neural networks. In ICML, 2016.\n10\n\n\n[20] Jurgen Schmidhuber. Evolutionary principles in self-referential learning. On learning how to learn: The\nmeta-meta-... hook.) Diploma thesis, Institut f. Informatik, Tech. Univ. Munich, 1987.\n[21] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy\noptimization. In ICML, 2015.\n[22] Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. Cnn features off-the-shelf:\nan astounding baseline for recognition. In Proceedings of the IEEE conference on computer vision and\npattern recognition workshops, pages 806–813, 2014.\n[23] Flood Sung, Li Zhang, Tao Xiang, Timothy Hospedales, and Yongxin Yang. Learning to learn: Meta-critic\nnetworks for sample efﬁcient learning. arXiv preprint arXiv:1706.09529, 2017.\n[24] Sebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media, 2012.\n[25] Oriol Vinyals, Charles Blundell, Tim Lillicrap, and Daan Wierstra. Matching networks for one shot\nlearning. In NIPS, 2016.\n[26] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles\nBlundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv preprint\narXiv:1611.05763, 2016.\n[27] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine learning, 8(3-4):229–256, 1992.\n[28] Xiao-Ming Wu, Zhenguo Li, Anthony M So, John Wright, and Shih-Fu Chang. Learning with partially\nabsorbing random walks. In NIPS, 2012.\n[29] A Steven Younger, Peter R Conwell, and Neil E Cotter. Fixed-weight on-line learning. IEEE Transactions\non Neural Networks, 10(2), 1999.\n11\n"
}