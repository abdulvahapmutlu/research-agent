{
  "filename": "2310.10971v2.pdf",
  "num_pages": 22,
  "pages": [
    "Published as a conference paper at ICLR 2024\nCONTEXT-AWARE META-LEARNING\nChristopher Fifty1, Dennis Duan1,2, Ronald G. Junkins1,\nEhsan Amid3, Jure Leskovec1, Christopher Ré1, Sebastian Thrun1\n1Stanford University, 2Google, 3Google DeepMind\nfifty@cs.stanford.com\nABSTRACT\nLarge Language Models like ChatGPT demonstrate a remarkable capacity to learn\nnew concepts during inference without any fine-tuning. However, visual models\ntrained to detect new objects during inference have been unable to replicate this\nability, and instead either perform poorly or require meta-training and/or fine-\ntuning on similar objects. In this work, we propose a meta-learning algorithm\nthat emulates Large Language Models by learning new visual concepts during\ninference without fine-tuning. Our approach leverages a frozen pre-trained feature\nextractor, and analogous to in-context learning, recasts visual meta-learning as\nsequence modeling over datapoints with known labels and a test datapoint with\nan unknown label. On 8 out of 11 few-shot image classification benchmarks, our\napproach—without meta-training or fine-tuning—exceeds or matches the state-of-\nthe-art algorithm, P>M>F, which is meta-trained on these benchmarks. Our code\nis available at https://github.com/cfifty/CAML.\n1\nINTRODUCTION\nMeta-learning refers to a capacity to learn new concepts from a small number of demonstrations (Lake\net al., 2015). In a decade of remarkable advances to machine intelligence, it remains an area where\nhuman performance continues to surpass that of machines (Brown et al., 2020). To match human\ncapabilities, and towards developing machines that can learn and think like humans, we must develop\nmachine intelligence capable of learning novel concepts from only a few examples (Lake et al., 2017).\nMany applications of deep learning apply a learning algorithm to a large set of training data; however,\nlearning from a very small number of training examples poses a challenge (Lake et al., 2017; Garnelo\net al., 2018). This challenge led to two predominant evaluation settings: in-domain and cross-domain.\nThe in-domain setting evaluates a meta-learner’s ability to quickly adapt to new tasks after training\non similar tasks within a specific domain. Models designed for this setting are often extremely fast\nbut exhibit poor generalization to tasks outside the target domain (Chen et al., 2019). Meanwhile,\nthe cross-domain setting evaluates a meta-learner’s ability to adapt to tasks in previously unseen\ndomains. Methods designed for this setting are highly adaptable but slow during inference as they\nrequire fine-tuning on the support set (Guo et al., 2020; Oh et al., 2022; Hu et al., 2022). Critically,\nmeta-learners in both settings differ from a human’s capacity to quickly generalize to new tasks.\nThe problem of simultaneously fast and general meta-learning has recently been addressed in Natural\nLanguage by Large Language Models (LLMs). LLMs like ChatGPT can quickly generalize to new\ntasks through an ability termed in-context learning (Brown et al., 2020). However, it remains an open\nproblem in Computer Vision. Even the best visual meta-learning algorithms cannot be deployed to a\nChatGPT-like system because such systems require models that can (1) generalize to a broad set of\ntasks unknown at training time and (2) do so in real-time, without the time allowance for finetuning\nthe model. LLMs have shown a remarkable ability to do both; however, current visual meta-learners\nmay only satisfy one requirement or the other (Hu et al., 2022).\nTo measure progress towards this goal of fast and general visual meta-learners, we develop an\nevaluation paradigm that we call universal meta-learning. Universal meta-learning measures a\nmodel’s capacity to quickly learn new image classes. It evaluates models across a diverse set of\nmeta-learning benchmarks spanning many different image classification tasks without meta-training\non any of the benchmarks’ training sets or fine-tuning on the support set during inference. We focus on\n1\narXiv:2310.10971v2  [cs.LG]  25 Mar 2024\n",
    "Published as a conference paper at ICLR 2024\nthe application of few-shot image classification—as opposed to dense prediction tasks like in-painting\nor segmentation—as the universal setting has already been explored for these applications (Bar et al.,\n2022; Zhang et al., 2023; Wang et al., 2023; Kim et al., 2023; Butoi et al., 2023).\nBeyond benchmarking methods in the universal setting, we present a meta-learner that achieves\nstrong universal performance. Drawing inspiration from in-context learning in LLMs, we reformulate\nn-way-k-shot image classification as non-causal sequence modeling over the support set and an\nunknown query image. Specifically, given n-way classification with k-examples from each class,\nwe train a non-causal model over {(xi, yi)}nk\ni=1 (image, label) support set pairs, and an unlabeled\nquery image xnk+1, to predict the label of the query image. This formulation causes the meta-learner\nto extrapolate to new classes in its parameter space, enabling it to learn new visual concepts during\ninference without fine-tuning. Due to its capacity to learn visual information “in-context”, we term\nour approach Context-Aware Meta-Learning (CAML).\nIn summary, our contribution is two-fold. First, we develop a meta-learning evaluation paradigm that\napproximates the performance of visual meta-learners in a ChatGPT-like application. Second, we\ndesign a meta-learning algorithm that works well in this setting. Our empirical findings show that\nCAML outperforms other meta-learners in the universal setting. Remarkably, CAML’s performance\nin the universal setting often matches—and even exceeds—the in-domain performance of the state-of-\nthe-art meta-learning algorithm, P>M>F (Hu et al., 2022), that is directly trained on each down-stream\nbenchmark.\n2\nRELATED WORK\nMeta-Learning as Causal Sequence Modeling. Several of the earliest meta-learning algorithms were\nformulated as causal sequence modeling problems. Hochreiter et al. (2001) leverage a LSTM (Hochre-\niter & Schmidhuber, 1997) to model extensions to semi-linear and quadratic functions, and two\ndecades later, Graves et al. (2014); Santoro et al. (2016); Kaiser et al. (2017) build upon this approach\nby integrating a form of external memory that the LSTM can read to and write from memory to\ndevelop Neural Turing Machines. With the advent of self-attention (Vaswani et al., 2017), Mishra et al.\n(2017) predict the labels of query images by first composing a sequence of (image, label) pairs and\nthen feeding it through a stack of interleaved causal self-attention and temporal convolution layers.\nKirsch et al. (2022) replaces the stack of interleaved causal self-attention and temporal convolution\nlayers with a Transformer encoder; however, their approach is also causal in the input sequence by\ncomposing a sequence of (image, label of previous image) pairs. Both Mishra et al. (2017) and Kirsch\net al. (2022) are conceptually similar to our work; however, the causal property of both approaches\nbreaks an important symmetry in meta-learning, namely invariance to permutations of the support\nset (Garnelo et al., 2018; Müller et al., 2021). In Section 5.2, we observe a performance gap between\nboth approaches and CAML and hypothesize the causal approach actually forces a subtly more\ndifficult modeling problem by imposing a causality inductive bias on a fundamentally non-causal\nprediction task.\nCross-Domain Meta-Learning. Cross-domain meta-learning refers to a challenging evaluation\nparadigm where the meta-training and inference-time data distributions are significantly differ-\nent (Chen et al., 2019). Recent work finds that leveraging self-supervised pre-training—or foun-\ndational model feature extractors—can significantly improve cross-domain performance (Hu et al.,\n2022; Zhang et al., 2021). Moreover, fine-tuning with respect to the support set almost always\noutperforms meta-learning without fine-tuning in this setting (Guo et al., 2020; Oh et al., 2022; Phoo\n& Hariharan, 2020; Islam et al., 2021). While effective, fine-tuning is prohibitive to deploying visual\nmeta-learning models in a manner similar to LLMs like ChatGPT as the latency and memory cost\nto fine-tune a model’s parameters on each user query is untenable. Accordingly, we propose the\nuniversal setting to measure a meta-learner’s ability to learn to classify any task seen during inference\nwithout fine-tuning.\nIn-Context Learning for Dense Prediction Tasks. Many recent works have explored in-context\nlearning for other applications of computer vision. Bar et al. (2022) casts in-context learning as\nimage in-painting by first concatenating demonstration images with a query image and then using\na vision model to fill-in-the-blank within this concatenated image. Building on this work, Zhang\net al. (2023) explores what demonstrations lead to strong in-painting performance and Wang et al.\n(2023) generalizes the approach by formulating other visual applications like segmentation, depth\n2\n",
    "Published as a conference paper at ICLR 2024\nELMES Class Encoder\nMLP\nClass\nP(Class)\nPre-Trained Image Encoder\nConcatenate \nEmbeddings\nSupport Set Images\nQuery Image\n*\n Unknown \n[class] \nEmbedding\n[1, 0, 0]\n[0, 1, 0]\n[0, 0, 1]\n[1, 0, 0]\n[0, 1, 0]\n[0, 0, 1]\n0.03\n0.06\n0.91\n*\n*\nSupport Set Classes\nParameters Frozen\nParameters Trained\nNon-Causal Sequence Model\nFigure 1: Overview of CAML. Query and support set images are encoded with a pre-trained feature extractor\nand then concatenated with their corresponding ELMES label embeddings. We feed the resulting sequence of\nconcatenated vectors into a non-casual sequence model and extract the query vector from the output sequence to\npredict its class.\nestimation, etc. as in-painting. Other approaches explore in-context learning for applications like\nscene understanding (Balazevic et al., 2024), medical image segmentation (Butoi et al., 2023), and\nmore generally dense prediction tasks (Kim et al., 2023). Like these approaches, we study visual\nin-context learning; however, this work focuses on few-shot image classification rather than dense\nprediction tasks.\n3\nAPPROACH\nWe adapt the ideas underpinning in-context learning in LLMs—namely learning to classify a query\nfrom a context of support set demonstrations in a single forward pass—to image classification.\nHowever, dissimilar from in-context learning, visual meta-learners should be non-causal: placing one\nexample before another in the support set does not entail a causal relationship (Garnelo et al., 2018;\nMüller et al., 2021).\nArchitecture. An overview of CAML is shown in Figure 1. It consists of three different components:\n(1) a frozen pre-trained image encoder, (2) a fixed Equal Length and Maximally Equiangular Set\n(ELMES) class encoder, and (3) a non-causal sequence model. While pre-trained image encoders and\nnon-causal sequence models are well-known, to encode label information we introduce an ELMES\nencoder. An ELMES encoder is a bijective mapping between the labels and a set of vectors that are\nequal length and maximally equiangular. Historically, labels have been encoded with one-hot vectors;\nhowever in Section 4, we prove that an ELMES encoding of mutually exclusive classes allows the\nsequence model to maximally identify classes within the support set.\nAs visualized in Figure 1, CAML first encodes query and support set images using a frozen pre-trained\nfeature extractor. Crucially, the pre-trained image encoder’s embedding space distills images into\nlow-dimensional representations so that images with similar content and visual characteristics have\nsimilar embeddings. Classes of the support set are encoded with an ELMES class encoder; however\nas the class of the query is unknown, we use a special learnable “unknown token” embedding that is\nlearned during large-scale pre-training. CAML then concatenates each image embedding with its\ncorresponding query embedding to form an input sequence.\nProgressing through Figure 1, this sequence is fed into a non-causal sequence model, i.e. a Trans-\nformer encoder, to condition the output representations on the full context of query and support\nset points. This enables dynamic and real-time classification; visual characteristics from query and\nsupport set images can be compared with each other to determine the specific visual features—such\nas content, textures, etc.—used to classify the query. From the output sequence of the non-causal\nsequence model, we select the element at the same position as the query in the input sequence, and\npass this vector through a shallow MLP to predict the label of the query.\nLarge-Scale Pre-Training. As our focus is universal meta-learning—and CAML may encounter any\nnew visual concept during inference—we pre-train CAML’s non-causal sequence model on few-shot\n3\n",
    "Published as a conference paper at ICLR 2024\nimage classification tasks from ImageNet-1k (Deng et al., 2009), Fungi (Schroeder & Cui, 2018),\nMSCOCO (Lin et al., 2014), and WikiArt (Saleh & Elgammal, 2015). We chose these datasets because\nthey span generic object recognition (ImageNet-1k, MSCOCO), fine-grained image classification\n(Fungi), and unnatural image classification (WikiArt). To avoid distorting the pre-trained image\nencoder’s embedding space, we freeze this module and only update the sequence model’s parameters\nduring pretraining. Similarly, since an ELMES minimizes the entropy of detecting classes within\nthe support set, the label encoder is also frozen. In the context of pre-training, meta-training, and\nfine-tuning, CAML only requires pre-training and avoids meta-training on the train/validation splits\nof meta-learning benchmarks or fine-tuning on the support set during inference.\n4\nTHEORETICAL ANALYSIS\nIn this section, we motivate our choice of the ELMES Class Encoder by considering the symmetries\ndesirable in meta-learning algorithms. Two important symmetries are (1) invariance to the assignment\nof support set classes to numeric labels and (2) invariance to permutations in the ordering of the input\nsequence. The first invariance implies the class embeddings must be equiangular and equal norm,\nwith an ELMES configuration minimizing the entropy of learnable model parameters detecting any\ngiven class. Later, we show an ELMES also satisfies the second symmetry. Due to space constraints,\nall proofs and many definitions, properties, lemmas, and theorems are allocated to Appendix A.1. We\nbegin with a formal definition of an ELMES.\n4.1\nEQUAL LENGTH AND MAXIMALLY EQUIANGULAR SET OF VECTORS\nDefinition 1. An Equal Length and Maximally Equiangular Set (ELMES) is a set of non-zero\nvectors {ϕj}d\nj=1, ϕj ∈Rd+k for some k ≥0 and d > 1, such that ∀j ̸= j′, ∥ϕj∥= ∥ϕj′∥and\n⟨ϕj , ϕj′⟩=\n−1\nd−1. Simply, all vectors in this set are equal length and maximally equiangular.\nAn Equal Angle and Maximally Equiangular Set (ELMES) of vectors has connections to both\nEquiangular Tight Frames in representation theory (Welch, 1974; Fickus et al., 2018) as well as the\nSimplex Equiangular Tight Frames highlighted in recent neural collapse works exploring softmax-\nlayer geometry at the terminal phase of training (Papyan et al., 2020; Yang et al., 2022). We offer\nadditional discussion comparing these structures in Appendix A.1 as well as provide an intuitive view\nof an ELMES as a regular d-simplex immersed in Rd+k.\n4.2\nLABEL SYMMETRY\nSymmetry in the assignment of support classes to numeric labels is an important property of meta-\nlearning algorithms. For example, if we have the support set classes {tower, bear, tree}, the mapping\nof {bear -> 1, tower -> 2, tree -> 3} should produce the same prediction for a query point as a\ndifferent mapping {bear -> 2, tower -> 3, tree -> 1}. To explore this symmetry, we examine how\nclass embeddings may be used by the model.\nFrom our formulation in Section 3, we represent a demonstration vector as a concatenation of an\nimage embedding ρ and a label embedding ϕ: [ρ\nϕ]. This vector is directly fed into the self-attention\nmechanism, where we matrix multiply with key, query, and value self-attention heads. Taking only\none of these matrices for simplicity with head-dimension k:\n[ρ\nϕ]\n\u0014\nΓ1\n...\nΓk\nψ1\n...\nψk\n\u0015\n= [⟨ρ , Γ1⟩\n...\n⟨ρ , Γk⟩] + [⟨ϕ , ψ1⟩\n...\n⟨ϕ , ψk⟩]\n(1)\nThe output of this transformation will be the sum of two vectors: one composed of the inner products\nbetween the image embedding ρ and the learnable {Γi}k\ni=1 and the other composed of the class\nembedding ϕ and the learnable {ψi}k\ni=1. Note that Equation (1) implies that CAML is not invariant\nto the assignment of labels to support set classes due to the addition between ⟨ρ , Γi⟩and ⟨ϕ , ψi⟩;\nhowever, we can constrain the geometry of the class embeddings {ϕ}d\nj=1 to in principle respect label\nsymmetry. Specifically for i ̸= j ̸= k, ⟨ϕi , ϕj⟩= ⟨ϕi , ϕk⟩and ∥ϕi∥= ∥ϕj∥.\nSimilar to a convolutional filter learning to match a pattern within an image, our analysis assumes\nthe learnable [ψ1\n...\nψk] will converge to vectors that maximize the inner product with a single\n4\n",
    "Published as a conference paper at ICLR 2024\nclass embedding subject to certain constraints. Under this assumption, we ask what geometry of the\nd-class embeddings {ϕ}d\nj=1 allows a learnable ψi vector to most unambiguously detect a single class\nembedding. To answer this question, we define a probability mass function for each ψi over the set\nof d−classes so that maximizing the probability of the jth class aligns with maximizing ⟨ϕj , ψi⟩\nand equally minimizing ⟨ϕk , ψi⟩for k ̸= j.\nDefinition 2. Let X be a discrete Random Variable taking on values in {1, 2, ..., d}. For learnable vec-\ntor ψi, define probability mass function pψi(X = j) as the softmax over [⟨ϕ1 , ψi⟩\n...\n⟨ϕd , ψi⟩]\nso that:\npψi(X = j) =\ne∥ψi∥∥ϕj∥cos(θi,j)\nPd\nk=1 e∥ψi∥∥ϕj∥cos(θi,k)\nwhere θi,j is the angle between ϕj and ψi.\nWe say ψi learns to detect class j when pψi(X = j) > pψi(X = k) for 1 ≤k ≤d with k ̸= j. By\nsymmetry in the assignment of class embeddings to support classes, we can assume that the number\nof ψi learned to detect class i is similar to the number of ψj learned to detect class j for all pairs (i, j).\nWe also leverage symmetry in the assignment of labels to support set classes to make the following\nassumptions. A justification for each assumption is located in Appendix A.1.\nAssumption 1. Suppose {ψi}k\ni=1 are learnable class detectors of unit norm with at least one ψi\ndetecting each class 1 ≤i ≤d. The probability pψj(X = j) = pψi(X = i) for 1 ≤i, j ≤d.\nAssumption 2. Define pψi(X = i)\\{ϕl}d\nl=(m+1) as the probability of ψi detecting ϕi from the\nset of vectors {ϕj}m\nj=1, m < d. Then the probability pψj(X = j)\\{ϕl}d\nl=(m+1) = pψi(X =\ni)\\{ϕl}d\nl=(m+1) for 1 ≤i, j ≤m and m ≥2.\nAssumption 3. When ψi =\nϕi\n∥ϕi∥, pψi(X = i) is maximized.\nWhen Assumption 1, Assumption 2, and Assumption 3 hold, the set of class embeddings that\nmaximize the probability of a learnable ψi detecting class i is necessarily an ELMES.\nTheorem 1. The set of class embeddings {ϕj}d\nj=1 ∀j, 1 ≤j ≤d that maximizes pψj(X = j) is\nnecessarily an ELMES.\nAlternatively when viewed through the lens of information theory, we can interpret an ELMES as the\nclass embedding that minimizes the entropy of ψi detecting class i. Informally, ELMES causes ψi to\nhave the least uncertainty when detecting class i.\nProposition 1. Let Hψi(X) be the entropy of pψi(X). An ELMES minimizes Hψi(X).\n4.3\nPERMUTATION INVARIANCE.\nIn addition to label symmetry, it is also desirable for the output prediction of CAML to not depend on\nthe order of demonstrations in the sequence. For example, if we have the support set classes {tower,\nbear, tree}, the sequence {(bear -> 1), (tower -> 2), (tree -> 3)} should produce the same output as the\npermuted sequence {(tree -> 3), (bear -> 1), (tower -> 2)}. Building on the prior work of Kossen et al.\n(2021); Fifty et al. (2023), it suffices to show to show that the ELMES label encoder is equivariant to\npermutations in the input sequence to show that CAML is invariant to permutations.\nProposition 2. Consider an n-sequence of one-hot labels stacked into a matrix S ∈Rn×w, and an\nELMES label encoder denoted by W ∈Rw×d with w denoting “way” and d the dimension of the\nlabel embedding. The label embedding SW is equivariant to permutations.\n5\nEXPERIMENTS\nTo quantify universal image classification performance, we evaluate a diverse set of 11 meta-learning\nbenchmarks divided across 4 different categories:\n1. Generic Object Recognition: mini-ImageNet (Vinyals et al., 2016), tiered-ImageNet (Ren et al.,\n2018), CIFAR-fs (Bertinetto et al., 2018), and Pascal VOC (Everingham et al.)\n5\n",
    "Published as a conference paper at ICLR 2024\nTable 1: MiniImageNet & CIFAR-fs mean accuracy and standard error across 10,000 test epochs. †\nindicates the pre-trained image encoder backbone was frozen during training.\nMethod (Backbone)\nCIFAR-fs\nMiniImageNet\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nIn-Domain [Meta-Training]\nP>M>F Hu et al. (2022)\n84.3\n92.2\n95.3\n98.4\nUniversal Meta-Learning;\nNo Meta-Training or Finetuning\nProtoNet (Snell et al., 2017)\n62.9±.2\n79.7±.2\n92.1±.1\n97.1±.0\nProtoNet†\n57.7±.2\n81.0±.2\n85.3±.2\n96.0±.1\nMetaOpt (Lee et al., 2019)\n53.1±.3\n73.1±.2\n78.5±.2\n91.6±.1\nMetaOpt†\n61.7±.2\n83.1±.1\n86.9±.2\n96.5±.1\nMetaQDA (Zhang et al., 2021)\n60.4±.2\n83.2±.1\n88.2±.2\n97.4±.0\nGPICL (Kirsch et al., 2022)\n41.5±.4\n78.3±.2\n95.6±.1\n98.2±.1\nSNAIL (Mishra et al., 2017)\n62.1±.3\n71.1±.3\n93.6±.1\n98.1±.0\nCAML\n70.8±.2\n85.5±.1\n96.2±.1\n98.6±.0\nTable 2: Pascal & Paintings mean accuracy and standard error across 10,000 test epochs. † indicates\nthe the pre-trained image encoder backbone was frozen during training.\nMethod (Backbone)\nPascal + Paintings\nPaintings\nPascal\n5w-1s\n5w-5s\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nIn-Domain [Meta-Training]\nP>M>F\n60.7\n74.4\n53.2\n65.8\n72.2\n84.4\nUniversal Meta-Learning\nProtoNet\n49.6±.2\n63.5±.1\n38.3±.2\n48.2±.1\n77.9±.2\n87.3±.2\nProtoNet†\n52.2±.2\n70.6±.1\n48.3±.2\n64.1±.1\n72.2±.2\n84.3±.2\nMetaOpt\n38.2±.2\n58.2±.1\n31.6±.2\n48.0±.1\n63.7±.2\n81.7±.2\nMetaOpt†\n53.2±.2\n74.8±.1\n49.3±.2\n65.9±.1\n72.8±.2\n84.4±.2\nMetaQDA\n53.8±.2\n74.1±.1\n49.4±.2\n66.6±.1\n73.5±.2\n85.2±.2\nGPICL\n62.6±.2\n74.6±.1\n51.6±.2\n61.0±.1\n81.7±.2\n88.2±.2\nSNAIL\n62.5±.2\n77.6±.1\n51.9±.2\n65.8±.1\n79.7±.2\n88.0±.2\nCAML\n63.8±.2\n78.3±.1\n51.1±.2\n65.2±.1\n82.6±.2\n89.7±.1\n2. Fine-Grained Image Classification: CUB (Wah et al., 2011), Aircraft (Maji et al., 2013), meta-\niNat (Wertheimer & Hariharan, 2019), and tiered meta-iNat (Wertheimer & Hariharan, 2019)\n3. Unnatural Image Classification: ChestX (Guo et al., 2020) and Paintings (Crowley & Zisserman,\n2015)\n4. Inter-Domain Image Classification: Pascal+Paintings (Everingham et al.; Crowley & Zisserman,\n2015).\nGeneric object recognition, fine-grained image classification, and unnatural image classification\nare standard benchmarking tasks in meta-learning literature (Chen et al., 2020; Hu et al., 2022;\nWertheimer et al., 2020; Guo et al., 2020). Beyond this, we compose a challenging new inter-domain\ncategory by combining Pascal VOC with Paintings so that each class is composed of both natural\nimages and paintings. This allows us to evaluate the ability of meta-learning algorithms to generalize\nacross domains within the same class. For example, the support image for the class “tower” may be\nVan Gogh’s The Starry Night, while the query may be a picture of the Eiffel Tower. Humans have\nthe ability to generalize visual concepts between such domains; however, meta-learning algorithms\nstruggle with this formulation (Jankowski & Gr ˛abczewski, 2011).\n5.1\nBASELINES\nWe evaluate the performance of CAML, Prototypical Networks (ProtoNet) (Snell et al., 2017),\nMetaOpt (Lee et al., 2019), MetaQDA (Zhang et al., 2021), SNAIL (Mishra et al., 2017), and\nGPICL (Kirsch et al., 2022) in a universal meta-learning setting by pre-training them with a ViT-\nbase (Dosovitskiy et al., 2020) feature extractor initialized with weights from CLIP (Radford et al.,\n6\n",
    "Published as a conference paper at ICLR 2024\nTable 3: meta-iNat & tiered meta-iNat & ChestX mean accuracy and standard error across 10,000\ntest epochs. † indicates the the pre-trained image encoder backbone was frozen during training.\nMethod (Backbone)\nmeta-iNat\ntiered meta-iNat\nChestX\n5w-1s\n5w-5s\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nIn-Domain [Meta-Training]\nP>M>F\n91.2\n96.1\n74.8\n89.9\n27.0\n32.1\nUniversal Meta-Learning\nProtoNet\n78.4±.2\n89.4±.1\n66.3±.2\n82.2±.2\n22.4±.1\n25.3±.1\nProtoNet†\n84.5±.2\n94.8±.1\n73.8±.2\n89.5±.1\n22.7±.1\n25.8±.1\nMetaOpt\n53.0±.2\n77.7±.2\n37.3±.2\n63.0±.2\n20.8±.1\n23.0±.1\nMetaOpt†\n85.5±.2\n95.5±.1\n75.1±.2\n91.9±.1\n23.0±.1\n27.4±.1\nMetaQDA\n86.3±.2\n95.9±.1\n76.0±.2\n92.4±.1\n22.6±.1\n27.0±.1\nGPICL\n90.0±.2\n95.1±.1\n60.8±.5\n87.6±.2\n20.1±.1\n20.9±.1\nSNAIL\n89.1±.2\n94.8±.1\n77.3±.2\n86.5±.2\n20.2±.0\n20.0±.0\nCAML\n91.2±.2\n96.3±.1\n81.9±.2\n91.6±.1\n21.5±.1\n22.2±.1\nTable 4: CUB & tiered-ImageNet & Aircraft mean accuracy and standard error across 10,000 test\nepochs. † indicates the the pre-trained image encoder backbone was frozen during training.\nMethod (Backbone)\nCUB\ntiered-ImageNet\nAircraft\n5w-1s\n5w-5s\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nIn-Domain [Meta-Training]\nP>M>F\n92.3\n97.0\n93.5\n97.3\n79.8\n89.3\nUniversal Meta-Learning\nProtoNet\n59.4±.2\n77.3±.2\n93.5±.1\n97.4±.1\n37.9±.2\n52.5±.2\nProtoNet†\n87.0±.2\n97.1±.1\n87.3±.2\n96.1±.1\n62.4±.3\n82.0±.2\nMetaOpt\n71.5±.2\n41.2±.2\n76.6±.2\n89.6±.1\n41.6±.2\n26.7±.1\nMetaOpt †\n87.9±.2\n97.2±.1\n88.2±.2\n96.5±.1\n64.8±.2\n82.6±.2\nMetaQDA\n88.3±.2\n97.4±.1\n89.4±.2\n97.0±.1\n63.6±.3\n83.0±.2\nGPICL\n75.1±.5\n94.5±.1\n94.6±.1\n97.2±.1\n19.8±.2\n61.8±.3\nSNAIL\n87.5±.2\n92.8±.2\n93.1±.1\n97.3±.1\n48.9 ± .3\n35.8±.3\nCAML\n91.8±.2\n97.1±.1\n95.4±.1\n98.1±.1\n63.3±.3\n79.1±.2\n2021). Pre-training runs over few-shot classification tasks from ImageNet-1k, Fungi, MSCOCO,\nand WikiArt, and during evaluation on the set of 11 meta-learning benchmarks, models are not\nmeta-trained or fine-tuned. We compare with ProtoNet, MetaOpt, and MetaQDA as they achieve\nstate-of-the-art results when paired with a pre-trained feature extractor (Hu et al., 2022). As sequence\nmodeling underpins CAML, we also compare with SNAIL and GPICL to evaluate the performance\nof past formulations of causal sequence-based meta-learning algorithms in the universal setting.\nTo assess the gap between universal and in-domain meta-learning performance, we benchmark the\ncurrent state-of-the-art meta-learning algorithm P>M>F (Hu et al., 2022). Similar to the universal\nsetting, P>M>F uses a ViT-base feature extractor initialized with weights from DINO (Caron et al.,\n2021); however, it meta-trains on the training set of each benchmark before evaluating on that\nbenchmark’s test set.\nWhen pre-training all models in the universal setting, we set the learning rate to a fixed 1 × 10−5 and\ndo not perform any hyperparameter tuning in order to match the practices used by P>M>F. We use\nearly stopping with a window size of 10 epochs during pre-training and the code release of Hu et al.\n(2022) to benchmark P>M>F with the training settings and hyperparameters described in their work.\n5.2\nRESULTS\nOur findings are summarized in Table 1, Table 2, Table 3, and Table 4 and indicate that CAML sets a\nnew state-of-the-art for universal meta-learning by significantly outperforming other baselines on\n14 of 22 evaluation settings. For 5 of the other 8 evaluation settings, CAML matches—or nearly\nmatches—the best performing baseline. Remarkably, CAML also performs competitively with\n7\n",
    "Published as a conference paper at ICLR 2024\nP>M>F on 8 out of 11 meta-learning benchmarks, even though P>M>F meta-trains on the training\nset of each benchmark.\nThis result suggests that the amount of new visual information learned during inference through\nvisual in-context learning can be comparable to the amount learned when directly meta-training on\nin-domain data. This capacity may unlock new applications in the visual space, just as the emergence\nof in-context learning in LLMs has enabled many new applications in natural language.\nBenchmarks Where CAML Underperforms. The 3 datasets where P>M>F outperforms CAML\nare CIFAR-fs, Aircraft, and ChestX. CIFAR-fs is a generic object recognition benchmark containing\nCIFAR images downsampled to 32x32 resolution. As CAML and CLIP pre-train on 224x224\nresolution images, downsampling by a factor of 49 likely induces a distribution shift that was not\nlearned by CAML during large-scale pre-training. In the cases of Aircraft and ChestX, we postulate\nthat the CLIP embedding space—structured so images with similar captions have similar embeddings–\nstruggles to effectively differentiate between the fine-grained, specialized classes in these tasks. For\nexample, while a Boeing 737 and Airbus A380 have different labels in the Aircraft dataset, the\nscraped CLIP captions for those images may not reach that level of granularity. This corroborates the\nfindings from Radford et al. (2021), which found that in a zero-shot setting, CLIP underperforms in\nspecialized or complex tasks.\nOur ablation study into non-CLIP pre-trained feature extractors in Tables 5 to 8 of Appendix C shows\nCAML’s performance on Aircraft can drastically improve. Specifically, 5w-1s performance increases\nfrom 63.3 to 81.8 and 5w-5s performance increases from 79.1 to 92.1 when a ViT-Huge pre-trained\non Laion-2b (Schuhmann et al., 2022) initializes the weights of the image encoder rather than CLIP.\nFine-tuning CLIP Backbone. Our findings in Tables 1 to 4 indicate that updating the CLIP image\nencoder during pre-training hurts the performance of ProtoNet and MetaOpt. We observe that these\nmethods tend to overfit during pre-training, and our empirical results show a similar pattern: pre-\ntraining with these methods often helps the performance of benchmarks similar to ImageNet (i.e.\nPascal, MiniImageNet, tiered-ImageNet), but it significantly hurts the performance of out-of-domain\ntasks (i.e. Aircraft, CUB, Paintings) as shown in Tables 1 to 4. We believe that further training the\nCLIP backbone distorts the structure of its embedding space, leading to catastrophic forgetting on\nout-of-domain tasks. Conversely, CAML, MetaQDA, SNAIL, and GPICL—all of which freeze the\nparameters of the CLIP feature extractor—benefit greatly from large-scale episodic pre-training on\nImageNet-1k, Fungi, MSCOCO, and WikiArt.\n6\nANALYSIS\nTo better understand how CAML learns during inference, we analyze its ability to dynamically update\nits representations. Due to casting meta-learning as non-causal sequence modeling, CAML considers\nthe full context of query and support set to predict the label of the query. Specifically, the query\ndynamically influences the representation of support set points, and the support set points dynamically\ninfluence the representation of the query as this sequence is passed through the layers of a non-causal\nsequence model. This property enables universal meta-learning by allowing the model to update\nthe support and query representations based on the context of the task, not only the contents of the\nimages, within the parameter space of the sequence model.\nAn example where the query dynamically influences the support set is visualized in Figure 2. Given\nonly the 5 support examples, the prediction task is ambiguous. However, the nature of the query\ndetermines the prediction task. The query image of a tower in Figure 2a reduces the task to generic\nobject recognition: classify the query based on the object portrayed in the image. On the other hand,\nand as visualized in Figure 2b, the query image of embroidery reduces the prediction task to texture\nidentification: classify the query based on artistic medium.\nTo analyze how dynamic representations affect CAML, we examine the representations of the support\nset and query vectors at the input to and output of the non-causal sequence model. For both examples\nvisualized in Figure 2a and Figure 2b, the non-causal sequence model learns to separate support set\nvectors by class identity and group the query representation with the correct support set example.\nWe find the frozen CLIP image embeddings are actually antagonistic for the classification-by-texture\ntask visualized in Figure 2b: the query image embedding is closest to the support set example for\n8\n",
    "Published as a conference paper at ICLR 2024\nTower\nChicken\nSupport\nCow\nFlower\nHuman\nTower\nQuery\n(a) Left: An example task—classify images by the objects depicted. Center: image embeddings output from the\nImage Encoder (CLIP) in CAML . Right: joint image-label representations output by the non-causal sequence\nmodel in CAML for the same task.\nEmbroidery\n3D Model\nSupport\nPencil Sketch\nOil Painting\nStained Glass\nEmbroidery\nQuery\n(b) Left: An example task—classify images by the artistic medium used. Center: CLIP image embeddings output\nfrom the Image Encoder (CLIP) in CAML . Right: joint image-label representations output by the non-causal\nsequence model in CAML for the same task.\nFigure 2: Two sample tasks over the same support images but utilizing different criteria to define classes. The\nnature of the query image informs the task being presented, e.g. classification by object (top) vs. classification\nby texture (bottom). For both tasks, the output of the non-causal sequence model provides better separation\namong class representations than CLIP embeddings and groups the query representation with the proper task,\neven when projected into 2D space by PCA.\nthe second class, “oil painting”. Unsurprisingly, the baseline methods that rely on frozen CLIP\nembeddings—specificially MetaQDA, ProtoNet†, and MetaOpt†—group the query with “oil painting”\nand therefore misclassify this example. On the other hand, as CAML considers the full context of the\nquery and support set, it develops representations of the query in the context of the support set—and\nthe support set in the context of the query—to group the query with the “embroidery” support set\nimage as they share the same texture, thereby correctly classifying this example.\n7\nCONCLUSION\nIn this work, we develop universal meta-learning to approximate the performance of visual meta-\nlearners deployed to a ChatGPT-like application and present CAML: a meta-learning algorithm that\nemulates in-context learning in LLMs by learning new visual concepts during inference without\nfine-tuning. Our empirical findings show that CAML—without meta-training or fine-tuning—exceeds\nor matches the performance of the current state-of-the-art meta-learning algorithm on 8 out of 11\nbenchmarks. This result indicates visual meta-learning models are ready for deployment in a manner\nsimilar to LLMs, and we hope this work recalibrates our sense of limitations for the universal\nmeta-learning paradigm.\nNevertheless, there are areas where CAML struggles. Specifically, the performance of CAML on\nhighly out-of-distribution images—e.g. chest x-rays—and varying image resolutions—e.g. rescaled\nCIFAR images—lags behind that of the best in-domain approaches. Developing methods for the\nuniversal setting that are robust to these cases is a promising direction for future work.\n9\n",
    "Published as a conference paper at ICLR 2024\nACKNOWLEDGMENTS\nWe thank Mayee Chen, Dan Fu, Jerry Liu, and Benjamin Spector for their invaluable feedback\nand help during revisions of this work. We also thank Chelsea Finn for helping us improve the\nrelated work, Victor Butoi for constructive dialogue over Twitter, and the Hazy Group at Stanford\nas a whole their support throughout the research process. We gratefully acknowledge the support\nof NIH under No. U54EB020405 (Mobilize), DARPA under Nos. N660011924033 (MCS), NSF\nunder Nos. OAC-1835598 (CINES), CCF-1918940 (Expeditions), DMS-2327709 (IHBEM), Nos.\nCCF2247015 (Hardware-Aware), CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Veloc-\nity), and 1937301 (RTML); US DEVCOM ARL under Nos. W911NF-23-2-0184 (Long-context)\nand W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under Nos. N000142312633 (Deep\nSignal Processing); Stanford HAI under No. 247183; NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft,\nNEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices,\nGoogle Cloud, Salesforce, Total, Wu Tsai Neurosciences Institute, Chan Zuckerberg Initiative, Ama-\nzon, Genentech, GSK, Juniper Networks, KDDI, UCB, the HAI-GCP Cloud Credits for Research\nprogram, the Stanford Data Applications Initiative, and the Stanford Data Science Initiative (SDSI).\nThe U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes\nnotwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recom-\nmendations expressed in this material are those of the authors and do not necessarily reflect the views,\npolicies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government.\nREFERENCES\nIvana Balazevic, David Steiner, Nikhil Parthasarathy, Relja Arandjelovi´c, and Olivier Henaff. Towards\nin-context scene understanding. Advances in Neural Information Processing Systems, 36, 2024.\nAmir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting\nvia image inpainting. Advances in Neural Information Processing Systems, 35:25005–25017, 2022.\nLuca Bertinetto, Joao F Henriques, Philip HS Torr, and Andrea Vedaldi. Meta-learning with differen-\ntiable closed-form solvers. arXiv preprint arXiv:1805.08136, 2018.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\nVictor Ion Butoi, Jose Javier Gonzalez Ortiz, Tianyu Ma, Mert R Sabuncu, John Guttag, and Adrian V\nDalca. Universeg: Universal medical image segmentation. pp. 21438–21451, 2023.\nMathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the\nIEEE/CVF international conference on computer vision, pp. 9650–9660, 2021.\nWei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look\nat few-shot classification. arXiv preprint arXiv:1904.04232, 2019.\nWei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look\nat few-shot classification, 2020.\nElliot J Crowley and Andrew Zisserman. In search of art. In Computer Vision-ECCV 2014 Workshops:\nZurich, Switzerland, September 6-7 and 12, 2014, Proceedings, Part I 13, pp. 54–70. Springer,\n2015.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hier-\narchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,\npp. 248–255, 2009. doi: 10.1109/CVPR.2009.5206848.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020.\n10\n",
    "Published as a conference paper at ICLR 2024\nM. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman.\nThe\nPASCAL Visual Object Classes Challenge 2012 (VOC2012) Results.\nhttp://www.pascal-\nnetwork.org/challenges/VOC/voc2012/workshop/index.html.\nMatthew Fickus, John Jasper, Emily J King, and Dustin G Mixon. Equiangular tight frames that\ncontain regular simplices. Linear Algebra and its applications, 555:98–138, 2018.\nChristopher Fifty, Jure Leskovec, and Sebastian Thrun. In-context learning for few-shot molecular\nproperty prediction, 2023.\nMarta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray\nShanahan, Yee Whye Teh, Danilo Rezende, and SM Ali Eslami. Conditional neural processes. In\nInternational conference on machine learning, pp. 1704–1713. PMLR, 2018.\nAlex Graves, Greg Wayne, and Ivo Danihelka.\nNeural turing machines.\narXiv preprint\narXiv:1410.5401, 2014.\nYunhui Guo, Noel C Codella, Leonid Karlinsky, James V Codella, John R Smith, Kate Saenko, Tajana\nRosing, and Rogerio Feris. A broader study of cross-domain few-shot learning. In Computer\nVision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,\nPart XXVII 16, pp. 124–141. Springer, 2020.\nSepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n1735–1780, 1997.\nSepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent.\nIn Artificial Neural Networks—ICANN 2001: International Conference Vienna, Austria, August\n21–25, 2001 Proceedings 11, pp. 87–94. Springer, 2001.\nShell Xu Hu, Da Li, Jan Stühmer, Minyoung Kim, and Timothy M. Hospedales. Pushing the limits\nof simple pipelines for few-shot learning: External data and fine-tuning make a difference, 2022.\nAshraful Islam, Chun-Fu Richard Chen, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, and\nRichard J Radke. Dynamic distillation network for cross-domain few-shot recognition with\nunlabeled data. Advances in Neural Information Processing Systems, 34:3584–3595, 2021.\nNorbert Jankowski and Krzysztof Gr ˛abczewski. Universal meta-learning architecture and algorithms.\nMeta-learning in computational intelligence, pp. 1–76, 2011.\nŁukasz Kaiser, Ofir Nachum, Aurko Roy, and Samy Bengio. Learning to remember rare events.\narXiv preprint arXiv:1703.03129, 2017.\nDonggyun Kim, Jinwoo Kim, Seongwoong Cho, Chong Luo, and Seunghoon Hong.\nUniver-\nsal few-shot learning of dense prediction tasks with visual token matching.\narXiv preprint\narXiv:2303.14969, 2023.\nLouis Kirsch, James Harrison, Jascha Sohl-Dickstein, and Luke Metz. General-purpose in-context\nlearning by meta-learning transformers. arXiv preprint arXiv:2212.04458, 2022.\nJannik Kossen, Neil Band, Clare Lyle, Aidan N Gomez, Thomas Rainforth, and Yarin Gal. Self-\nattention between datapoints: Going beyond individual input-output pairs in deep learning. Ad-\nvances in Neural Information Processing Systems, 34:28742–28756, 2021.\nBrenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning\nthrough probabilistic program induction. Science, 350(6266):1332–1338, 2015.\nBrenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building\nmachines that learn and think like people. Behavioral and brain sciences, 40:e253, 2017.\nKwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with\ndifferentiable convex optimization, 2019.\n11\n",
    "Published as a conference paper at ICLR 2024\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–\nECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pp. 740–755. Springer, 2014.\nSubhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained\nvisual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.\nNikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-\nlearner. arXiv preprint arXiv:1707.03141, 2017.\nSamuel Müller, Noah Hollmann, Sebastian Pineda Arango, Josif Grabocka, and Frank Hutter.\nTransformers can do bayesian inference. arXiv preprint arXiv:2112.10510, 2021.\nJaehoon Oh, Sungnyun Kim, Namgyu Ho, Jin-Hwa Kim, Hwanjun Song, and Se-Young Yun.\nUnderstanding cross-domain few-shot learning based on domain similarity and few-shot difficulty.\nAdvances in Neural Information Processing Systems, 35:2622–2636, 2022.\nVardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the terminal\nphase of deep learning training. Proceedings of the National Academy of Sciences, 117(40):\n24652–24663, 2020.\nCheng Perng Phoo and Bharath Hariharan. Self-training for few-shot transfer across extreme task\ndifferences. arXiv preprint arXiv:2010.07734, 2020.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748–8763. PMLR, 2021.\nMengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum,\nHugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classification.\narXiv preprint arXiv:1803.00676, 2018.\nBabak Saleh and Ahmed Elgammal. Large-scale classification of fine-art paintings: Learning the\nright metric on the right feature. arXiv preprint arXiv:1505.00855, 2015.\nAdam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-\nlearning with memory-augmented neural networks. In International conference on machine\nlearning, pp. 1842–1850. PMLR, 2016.\nBrigit Schroeder and Yin Cui.\nFGVCx fungi classification challenge 2018.\ngithub.com/\nvisipedia/fgvcx_fungi_comp, 2018.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An\nopen large-scale dataset for training next generation image-text models. Advances in Neural\nInformation Processing Systems, 35:25278–25294, 2022.\nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning, 2017.\nAndreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas\nBeyer. How to train your vit? data, augmentation, and regularization in vision transformers. arXiv\npreprint arXiv:2106.10270, 2021.\nLaurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\nlearning research, 9(11), 2008.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing\nsystems, 30, 2017.\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one\nshot learning. Advances in neural information processing systems, 29, 2016.\n12\n",
    "Published as a conference paper at ICLR 2024\nCatherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd\nbirds-200-2011 dataset. 2011.\nXinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A\ngeneralist painter for in-context visual learning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 6830–6839, 2023.\nManfred K Warmuth and Dima Kuzmin. Randomized online pca algorithms with regret bounds\nthat are logarithmic in the dimension. Journal of Machine Learning Research, 9(Oct):2287–2320,\n2008.\nLloyd Welch. Lower bounds on the maximum cross correlation of signals (corresp.). IEEE Transac-\ntions on Information theory, 20(3):397–399, 1974.\nDavis Wertheimer and Bharath Hariharan. Few-shot learning with localization in realistic settings.\nIn Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.\n6558–6567, 2019.\nDavis Wertheimer, Luming Tang, and Bharath Hariharan. Few-shot classification with feature map\nreconstruction networks, 2020.\nYibo Yang, Liang Xie, Shixiang Chen, Xiangtai Li, Zhouchen Lin, and Dacheng Tao. Do we really\nneed a learnable classifier at the end of deep neural network? arXiv e-prints, pp. arXiv–2203,\n2022.\nXueting Zhang, Debin Meng, Henry Gouk, and Timothy Hospedales. Shallow bayesian meta learning\nfor real-world few-shot recognition, 2021.\nYuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. What makes good examples for visual in-context\nlearning? arXiv preprint arXiv:2301.13670, 2023.\n13\n",
    "Published as a conference paper at ICLR 2024\nA\nAPPENDIX\nA.1\nSUPPLEMENTARY THEORETICAL ANALYSIS\nWe offer additional insight into the theoretical analysis presented in Section 4 and provide the omitted\nremarks, properties, lemmas, and proofs.\nA.1.1\nEQUIANGULAR TIGHT FRAMES\nPapyan et al. (2020) coin the term Simplex Equianguar Tight Frame to describe a set of vectors\n{ϕj}d\nj=1 such that the minimum angle between any two pairs of vectors is maximized and all vectors\nhave equal norm. Formally,\nDefinition 3. Let Rd be a d−dimensional inner product space over R with the Euclidean inner\nproduct. A Simplex ETF is a set of d vectors {ϕj}d\nj=1, ϕj ∈Rd, specified by the columns of\nq\nd\nd−1(Id −1\nd11T )\nwhere Id ∈Rd×d is the identity matrix and 1 ∈Rd×1 is the ones vector. Somewhat contradictory, a\nSimplex Equiangular Tight Frame is not an Equiangular Tight Frame (Welch, 1974) as this set of\nvectors does not form a tight frame in Rd.\nDefinition 4. Let R be a d−dimensional space over R with the Euclidean inner product. An\nEquiangular Tight Frame (ETF) is a set of non-zero, equal norm vectors {ϕj}n\nj=1, n ≥d, that\nachieves the Welch lower bound:\nmax\nj̸=j′\n|⟨ϕj , ϕj′⟩|\n∥ϕj∥∥ϕj′∥=\ns\nn −d\nd(n −1)\nIt is well-known that a set of non-zero equal-norm vectors satisfies the Welch lower bound if and\nonly if that set of vectors is equiangular and also a tight frame for Rd (Fickus et al., 2018).\nDefinition 5. A set of non-zero, equal norm vectors {ϕj}n\nj=1 is equiangular if ∀j ̸= j′, |⟨ϕj , ϕj′⟩| =\nc for some c ∈R, c > 0.\nDefinition 6. {ϕj}n\nj=1 is a tight frame for Rd if, ∀v ∈Rd, ∃A > 0 such that A∥v∥2 =\nPn\nj=1 |⟨ϕj , v⟩|2.\nRemark 1. A Simplex Equiangular Tight Frame is not a tight frame.\nProof. Observe that for any finite d, for {ϕj}d\nj=1 equal to the columns of\nq\nd\nd−1(Id −1\nd11T ), it\nis the case that\nd−1\nP\nj=1\nϕj = −1 ∗ϕd. So {ϕj}n\nj=1 do not span Rd, and therefore, cannot be a tight\nframe.\nSimilarly, a Simplex ETF is not a d−simplex.\nRemark 2. A Simplex Equiangular Tight Frame is not a simplex.\nProof. A simplex in Rn requires n + 1 points.\nTo align terminology with properties, we generalize a Simplex ETF to an ELMES in Definition 1: a\nset of d vectors in a (d + k)-dimensional ambient space with k ≥0. Observe that a regular simplex\nis a special type of ETF in which the number of vectors in the set is one more than the dimension\nof the space that they span (Fickus et al., 2018). Building on this observation, an intuitive view of\nELMES is a regular d−simplex immersed in Rd+k.\nRemark 3. Consider a centered d-dimensional regular simplex with vertices {ϕj}d+1\nj=1, ϕj ∈Rd+1.\nLet ıcan be the canonical inclusion map: Rd →Rd+1, ıcan(x1, x2, ..., xd) = (x1, x2, ..., xd, 0d+1),\nthen {ıcan(ϕj)}d+1\nj=1 is an ELMES.\n14\n",
    "Published as a conference paper at ICLR 2024\nx\n1.0\n0.5\n0.0\n0.5\n1.0\ny\n1.0\n0.5\n0.0\n0.5\n1.0\nz\n1.0\n0.5\n0.0\n0.5\n1.0\nFigure 3: A visualization of a d = 4 ELMES in R3. Observe the endpoints of the vectors of an ELMES lie on\nthe vertices of a centered regular tetrahedron.\nProof. The two criteria of an ELMES are maximally equiangular and equal length. As all vertices\nof a centered regular d−simplex are equal length from the origin, {ϕj}d+1\nj=1 are equal length and\ntherefore {ıcan(ϕj)}d+1\nj=1 must also have equal length.\nSimilarly, from Lemma 10 of Papyan et al. (2020), we know the cosine of the angle between any\ntwo vectors in a (d + 1)−dimensional ELMES is −1\nd . It is known that for a d−dimensional regular\nsimplex in Rd centered at the origin, the angle subtended by any two verticies through the origin is\ncos(θ) = −1\nd . Immersing {ϕj}d+1\nj=1, ϕj ∈Rd, into Rd+1 via the canonical inclusion operator ıcan\ndoes not change the pairwise angle between vectors in this set: ⟨ϕj , ϕj′⟩= ⟨ıcan(ϕj) , ıcan(ϕj′)⟩.\nAs {ıcan(ϕj)}d+1\nj=1 are equal length and maximally equiangular, it forms an ELMES.\nWe now show that an ELMES immersed in a higher dimension remains an ELMES. Taken with\nRemark 3, we can view a high-dimensional ELMES in Rd composed of n + 1 vectors {ϕj}n+1\nj=1 ,\nd >> n + 1, as simply a n−simplex immersed in Rd via the canonical inclusion operator.\nLemma 1. Let ıcan : Rd →Rd+k. If {ϕj}n\nj=1 is an ELMES , then {ıcan(ϕj)}d\nj=1 is an ELMES.\nProof. This reduces to proving that the maximum angle between a set of d equiangular points in Rd\nis the maximum angle between a set of d equiangular points in Rd+k. Let {ϕj}d\nj=1 be an ELMES\nsuch that ϕj ∈Rd and {ψj}d\nj=1 be an ELMES such that ψj ∈Rd+k. Then {ψj}d\nj=1 lie in a\nd−dimensional subspace of Rd+k: ∃γ1, ..., γd and basis vectors e1, ..., ed such that ∀ψj ∈{ψj}d\nj=1,\nψj = Pd\ni=1 γiei. Therefore, ∀j ̸= j′, ⟨ψj , ψj′⟩≤⟨ϕj , ϕj′⟩as {ϕj}d\nj=1 are an ELMES for\nRd.\nA.1.2\nELMES ROTATIONAL SYMMETRY\nThere are infinitely many ELMES by rotating one such set of vectors about the origin.\nRemark 4. Let {ϕj}d\nj=1 be an ELMES in Rd+k for some k ≥0. Let o : Rd+k →Rd+k be an\noperator from the special orthogonal group SO(d + k). Then {o(ϕj)}d\nj=1 is also an ELMES .\n15\n",
    "Published as a conference paper at ICLR 2024\nProof. Length is preserved as operations in SO(d + k) have determinant 1 and angles are similarly\npreserved as operations in SO(d + k) are unitary (i.e. preserving inner product).\nA.1.3\nA SET OF ORTHONORMAL BASIS VECTORS IS NOT AN ELMES\nA final remark relates to the common misconception that a set of orthonormal basis vectors {ψj}d\nj=1\nis an ELMES. While {ψj}d\nj=1 is an ETF in Rd since this set realizes the Welch lower-bound\nin Definition 4, these vectors are not maximally equiangular: ⟨ψj , ψj′⟩= 0 >\n−1\nd−1.\nA.2\nELMES MAXIMIZES pψj(X = j)\nJustification of Assumption 1. This property is implied by symmetry in the assignment of class\nembeddings to support classes. As the assignment is arbitrary, all learnable ψi class detectors should\nhave equal probability of detecting their respective class. For simplicity of notation, we say ψi learns\nto detect class embedding ϕi rather another class embedding ϕk, k ̸= i.\nJustification of Assumption 2. Informally, this property states that, for any m-subset of classes\n{ϕj}m\nj=1, the probability of ψj detecting class j is equal to the probability of ψi detecting class\ni. This is again implied by symmetry in the assignment of class embeddings to support classes as\nmeta-learning algorithms may predict among a subset of m classes in the support set rather than the\nmaximum number of classes d.\nJustification of Assumption 3. Recall in Rd, ⟨ψ , ϕ⟩= ∥ψ∥∥ϕ∥cos(θ) where θ is the angle between\nψi and ϕi. Then this assumption constrains our set {ϕj}d\nj=1 so that relative norm of ϕi with respect\nto ϕj is lower bounded by cos(θi,j): ∥ϕi∥\n∥ϕj∥> cos(θi,j).\nInformally, the {ϕj}d\nj=1 are sufficiently spread out in the ambient space so that the learnable ψi that\nmaximizes pψi(X = i) is ϕi itself: ψi =\nϕi\n∥ϕi∥. This constraint helps us avoid degenerative cases like\n{ϕj}d\nj=1 all equal. For example, ϕj = αϕi, i ̸= j with α > 0 is one such degenerative case where\none class embedding vector is stacked on a different class embedding, but with higher norm.\nProof of Theorem 1. Taken with Assumption 1, Assumption 2, and Assumption 3, it suffices to show\nTheorem 2 and Lemma 4 to prove Theorem 1.\nTheorem 2. pψ1(X = 1) = pψ2(X = 2) = ... = pψd(X = d) ⇐⇒{ϕj}d\nj=1 are equiangular and\nequal norm.\nTo show the forward (⇒) direction, it suffices to first show pψ1(X = 1) = pψ2(X = 2) = ... =\npψd(X = d) ⇒{ϕj}d\nj=1 are equal norm and then show pψ1(X = 1) = pψ2(X = 2) = ... =\npψd(X = d) ⇒{ϕj}d\nj=1 are equiangular.\nLemma 2. pψ1 (X = 1) = pψ2 (X = 2) = ... = pψd(X = d) ⇒{ϕj}d\nj=1 are equal norm.\nProof. This implication holds when d = 2:\npψ1(X = 1) =\ne∥ϕ1∥\ne∥ϕ1∥+ e∥ϕ2∥cos(θ1,2) =\ne∥ϕ2∥\ne∥ϕ2∥+ e∥ϕ1∥cos(θ1,2) = pψ2(X = 2)\ne∥ϕ1∥(e∥ϕ2∥+ e∥ϕ1∥cos(θ1,2)) = e∥ϕ2∥(e∥ϕ1∥+ e∥ϕ2∥cos(θ1,2))\ne∥ϕ1∥+∥ϕ1∥cos(θ1,2) = e∥ϕ2∥+∥ϕ2∥cos(θ1,2)\n∥ϕ1∥(1 + cos(θ1,2)) = ∥ϕ2∥(1 + cos(θ1,2))\n∥ϕ1∥= ∥ϕ2∥\nSuppose d > 2 and pψ1(X = 1) = ... = pψd(X = d). By Assumption 2, all m−combinations\n\u0000 d\nm\n\u0001\nof {pψ1(X = 1), ..., pψd(X = d)} are equal. This implies all 2-combinations are equal:\npψi(X = i) = pψj(X = j) ⇒∥ϕi∥= ∥ϕj∥. Therefore, ∥ϕ1∥= ... = ∥ϕd∥.\n16\n",
    "Published as a conference paper at ICLR 2024\nLemma 3. pψ1(X = 1) = pψ2(X = 2) = ... = pψd(X = d) ⇒{ϕj}d\nj=1 are equiangular.\nProof. This implication is trivially true when d = 2 (see the proof of Lemma 2), and we show it is\nsimilarly true when d = 3. Following the steps in the proof of Lemma 2, we arrive at the following 3\npairs of equalities:\n(1) e∥ϕ1∥(1+cos(θ1,2)) + e∥ϕ1∥+∥ϕ3∥cos(θ2,3) = e∥ϕ2∥(1+cos(θ1,2)) + e∥ϕ2∥+∥ϕ3∥cos(θ1,3)\n(2) e∥ϕ1∥(1+cos(θ1,3)) + e∥ϕ1∥+∥ϕ2∥cos(θ2,3) = e∥ϕ3∥(1+cos(θ1,3)) + e∥ϕ3∥+∥ϕ2∥cos(θ1,3)\n(3) e∥ϕ2∥(1+cos(θ2,3)) + e∥ϕ2∥+∥ϕ1∥cos(θ1,3) = e∥ϕ3∥(1+cos(θ2,3)) + e∥ϕ3∥+∥ϕ1∥cos(θ1,2)\nFrom Lemma 2, pψ1(X = 1) = pψ2(X = 2) = pψ3(X = 3) ⇒∥ϕ1∥= ∥ϕ2∥= ∥ϕ3∥, so the\nabove pairs of equalities reduce to:\n(1) cos(θ2,3) = cos(θ1,3)\n(2) cos(θ2,3) = cos(θ1,3)\n(3) cos(θ1,3) = cos(θ1,2)\nand when d = 3, {ϕj}3\nj=1 are equiangular.\nSuppose d > 3 and pψ1(X = 1) = ... = pψd(X = d). By Assumption 2, all m−combinations\n\u0000 d\nm\n\u0001\nof {pψ1(X = 1), ..., pψd(X = d)} are equal. This implies all 3-combinations are equal:\npψi(X = i) = pψj(X = j) = pψk(X = k) ⇒θi,j = θi,k = θj,k. Therefore, all angles are equal\nθi,j = θl,m for 1 ≤i, j, l, m ≤d.\nProof of Theorem 2. (⇒) Suppose pψ1(X = 1) = pψ2(X = 2) = ... = pψd(X = d).\nBy Lemma 2 and Lemma 3, pψ1(X = 1) = pψ2(X = 2) = ... = pψd(X = d) ⇒{ϕj}d\nj=1 are\nequiangular and equal norm.\n(⇐) Suppose {ϕj}d\nj=1 are equiangular and equal norm. Let ∥ϕ∥be the norm of any vector in our set\nand cos(θ) be the pairwise angle between any two vectors. Then\npψi(X = i) =\ne∥ϕ∥\ne∥ϕ∥+ (d −1)e∥ϕ∥cos(θ) = pψj(X = j)\nfor any 1 ≤i, j ≤d.\nLemma 4. For a set of equiangular and equal norm vectors, maximum equiangularity maximizes\nP\nj\npψj(X = j).\nProof. The maximum pairwise angle between two vectors in Rd is π, and from Theorem 2\npψi(X = i) = pψj(X = j) =\ne∥ϕ∥\ne∥ϕ∥+ (d −1)e∥ϕ∥cos(θ)\nfor all 1 ≤i, j ≤d. Increasing the angle θ decreases cos(θ). Decreasing cos(θ) only decreases\nthe denominator, which in turn, increases pψi(X = i). Therefore, maximizing the pairwise angle\nbetween all vectors maximizes pψi(X = i) for all 1 ≤i ≤d.\nA.2.1\nAN ELMES MINIMIZES Hψi(X)\nProof of Lemma 1. Equal norm and equiangular {ϕj}d\nj=1 are bounded in norm, and thus, the set of\nprobability distributions we obtain {pψi(1), pψi(2), ..., pψi(d)} belong to a capped simplex (Warmuth\n& Kuzmin, 2008) ∆d\nc = {p ∈∆| maxk pψi(k) ≤c} where c =\ne∥ϕ∥2\ne∥ϕ∥2+(d−1)e∥ϕ∥2 cos(θ) . Clearly,\namong such probability vectors, the minimum entropy is achieved at the boundary where cos(θ) is\nminimized, i.e., when the {ϕj}d\nj=1 are maximally equiangular.\n17\n",
    "Published as a conference paper at ICLR 2024\nA.2.2\nAN ELMES MAINTAINS PERMUTATION INVARIANCE\nProof of Proposition 2. This follows from row-wise equivariance to permutations in matrix mul-\ntiplication. For any permutation π : [1, . . . , n] →[1, . . . , n] applied to the rows of Sn, we have\nπ(S)W = π(SW).\nB\nEXPERIMENTAL SETTINGS\nIn this section, we describe our experimental settings, and further, we direct readers interested in\nreproducing or using any of the methods we benchmark in this work to our released code. Unless\nstated otherwise, all universal meta-learning baselines use a CLIP feature extractor to encode images.\nLarge-Scale Pre-Training. All methods evaluated in the universal meta-learning setting adhere to\nthe same pre-training paradigm. For each large-scale image classification dataset, we reformulate the\nobjective from typical supervised image classification to both a 5-way-1-shot and a 5-way-5-shot\nepisodic prediction tasks. Within a dataset, examples from different classes are randomly sampled\nto compose a batch of episodes, and after exhausting iterating through every training example, this\nprocess is repeated with the next dataset. Iterating through each dataset in our set of ImageNet-1k,\nFungi, MSCOCO, and WikiArt then constitutes a single epoch of training.\nProtoNet and MetaOpt Implementations. For the ProtoNet and MetaOpt algorithms, we evaluate\ntwo settings. The first freezes the CLIP backbone and then applies the metric-learning objective—\ncosine distance for ProtoNet and SVM for MetaOpt—to classify the query image from the unmodified\nCLIP embeddings. The second emulates P>M>F Hu et al. (2022) by fine-tuning the CLIP backbone\nduring large-scale pre-training with the metric-learning objective function. During inference, the\nmetric-learning objective is applied to the fine-tuned CLIP embeddings to classify query images.\nMetaQDA Implementation. We follow the MetaQDA algorithm presented in Zhang et al. (2021).\nSpecifically, we freeze the CLIP feature extractor backbone and train the MetaQDA classifier during\nlarge-scale episodic pre-training.\nSNAIL Implementation. We use the architecture presented in Mishra et al. (2017) but with the\nhidden dimension of the Attention and Temporal Convolution Blocks adapted to CLIP embeddings\nrather than the ResNet embeddings used in the original implementation. As in this Mishra et al.\n(2017), we freeze the CLIP feature extractor and train the SNAIL model parameters during large-scale\npre-training.\nGPICL Implementation. We adapt the GPICL algorithm presented by Kirsch et al. (2022) for\nepisodic meta-training with an ELMES label encoder. Specifically, we represent image feature vectors\nas CLIP embeddings and the label embeddings with an ELMES. Following Kirsch et al. (2022), we\nform a sequence by concatening the current CLIP image embedding with the previous example’s\nELMES label embedding and add learnable positional embeddings so the model can use positional\ninformation of elements in the sequence to classify the query point in a causal-like fashion. We set the\nGeneral-Purpose In-Context Learning Transformer model to a ViT-Large (Dosovitskiy et al., 2020)\nwith leranable positional embeddings.\nCAML Implementation. The image encoder is set to CLIP and the label encoder is an ELMES.\nFor the non-causal sequence model, we use a ViT-Large as described in Table 1 of Dosovitskiy et al.\n(2020). This size is chosen as it has a hidden dimension of 1,024 and the CLIP output embedding\nvectors have hidden dimension of size 768. Choosing a non-causal sequence model with a large\nhidden dimension allows us to concatenate the label embedding to the CLIP embedding; in this case,\nthe label embedding is a 256 dimensional ELMES. In total, the implementation of CAML used for\nempirical evaluation has 302 million trainable parameters.\nOptimization Settings. Following the recommendation of training Vision Transformers (Steiner\net al., 2021) as well as standard practices, all universal meta-learning approaches use a cosine learning\nrate schedule with 9,600 warmup steps increasing linearly from 0 to 1e-5 followed by cosine decay\nto 1e−6 over the subsequent 360,000 steps. Given the size of our pre-training datasets, we do not\napply dropout, attention dropout, or weight decay regularization. We select a batch size of 525 so\nthe 5-way-1-shot episodes contain 520 query predictions and the 5-way-5-shot episodes contain 500\n18\n",
    "Published as a conference paper at ICLR 2024\nTable 5: MiniImageNet & CIFAR-fs mean accuracy and standard error across 10,000 test epochs.\nMethod\nCIFAR-fs\nMiniImageNet\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nCAML [ELMES Class Embedding]\n70.8±.2\n85.5±.1\n96.2±.1\n98.6±.0\nCAML [Learnable Class Embedding]\n71.1±.2\n85.9±.1\n96.1±.1\n98.7±.0\nTable 6: CUB & tiered-ImageNet & Aircraft mean accuracy and standard error across 10,000 test\nepochs.\nMethod\nCUB\ntiered-ImageNet\nAircraft\n5w-1s\n5w-5s\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nCAML [ELMES Class Embedding]\n91.8±.2\n97.1±.1\n95.4±.1\n98.1±.1\n63.3±.3\n79.1±.2\nCAML [Learnable Class Embedding]\n91.8±.2\n97.1±.1\n95.3±.1\n98.3±.1\n66.3±.2\n80.6±.2\nTable 7: Pascal & Paintings mean accuracy and standard error across 10,000 test epochs.\nMethod\nPascal + Paintings\nPaintings\nPascal\n5w-1s\n5w-5s\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nCAML [ELMES Class Embedding]\n63.8±.2\n78.3±.1\n51.1±.2\n65.2±.1\n82.6±.2\n89.7±.1\nCAML [Learnable Class Embedding]\n63.1±.2\n78.0±.1\n51.3±.2\n65.0±.1\n82.1±.2\n89.7±.1\nTable 8: meta-iNat & tiered meta-iNat & ChestX mean accuracy and standard error across 10,000\ntest epochs.\nMethod\nmeta-iNat\ntiered meta-iNat\nChestX\n5w-1s\n5w-5s\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nCAML [ELMES Class Embedding]\n91.2±.2\n96.3±.1\n81.9±.2\n91.6±.1\n21.5±.1\n22.2±.1\nCAML [Learnable Class Embedding]\n91.4±.2\n96.4±.1\n82.1±.2\n91.8±.1\n21.5±.1\n22.6±.1\nquery predictions. Given the scale of the pre-training datasets—and the computation to train a single\nmodel—we do not conduct any hyperparameter tuning.\nP>M>F Meta-Training. We follow the settings used by Hu et al. (2022) to evaluate P>M>F.\nSpecifically, P>M>F uses a DINO (Caron et al., 2021) feature extractor rather than a CLIP feature\nextractor as the authors of P>M>F found a DINO feature extractor to be preferrable. We refer\nreaders Hu et al. (2022) for this comparison. For meta-training, we use the code released by Hu et al.\n(2022) and simply switch out the datasets to evaluate the In-Domain setting. Both the in-domain and\nuniversal meta-learning settings use the same test-set data; the difference is that P>M>F meta-trains\non each training dataset before evaluating on the testing dataset of each benchmark.\nC\nSUPPLEMENTARY ANALYSIS\nELMES Ablation. To supplement our theoretical analysis in Section 4, we train a version of CAML\nwith learnable class embedding vectors in place of the fixed ELMES encoder. Given our analysis\nin Section 4, it is perhaps unsurprising we find that—without any constraints or limitations—the\nclass embeddings converge to an ELMES. The average pair-wise angle between embedding vectors\nis 1.77 ± 0.02 radians whereas the expected pairwise angle from an ELMES is 1.82. Similarly, the\naverage norm of the learnable class embeddings converges to 1.34 ± 0.02 whereas the learned norm\nof the ELMES model is 1.32.\nAn evaluation comparing CAML with learnable class embeddings to the approach with a fixed\nELMES encoder is presented in Table 5, Table 6, Table 7, and Table 8 of the Appendix. In summary,\nthe performance is approximately the same on each benchmark with the exception of Aircraft. In this\ncase, the learnable embedding model significantly outperforms the ELMES model, and moreover,\nsurpasses all other universal meta-learning baselines on the 5-way-1-shot split with an accuracy of\n66.3 ± .2. Nevertheless, given the similarity between both approaches on the remaining 10 datasets,\n19\n",
    "Published as a conference paper at ICLR 2024\nTable 9: MiniImageNet & CIFAR-fs mean accuracy and standard error across 10,000 test epochs. ◦\nindicates mean and standard error across 2,500 test epochs.\nMethod\nCIFAR-fs\nMiniImageNet\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nCAML (ResNet34)\n61.8 ± .2\n79.4 ± .2\n94.7 ± .1\n98.1 ± .0\nCAML (ViT-base)\n70.8±.2\n85.5±.1\n96.2±.1\n98.6±.0\nCAML (ViT-huge)◦\n83.3±.4\n93.5±.2\n98.6±.1\n99.6±.0\nTable 10: CUB & tiered-ImageNet & Aircraft mean accuracy and standard error across 10,000 test\nepochs. ◦indicates mean and standard error across 2,500 test epochs.\nMethod\nCUB\ntiered-ImageNet\nAircraft\n5w-1s\n5w-5s\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nCAML (ResNet34)\n75.4 ± .2\n88.3 ± .1\n96.1 ± .1\n98.5 ± .0\n45.1 ± .2\n58.7 ± .2\nCAML (ViT-base)\n91.8±.2\n97.1±.1\n95.4±.1\n98.1±.1\n63.3±.3\n79.1±.2\nCAML (ViT-huge)◦\n95.8±.2\n98.7±.1\n96.8±.2\n98.8±.1\n81.8±.4\n92.1±.3\nTable 11: Pascal & Paintings mean accuracy and standard error across 10,000 test epochs. ◦\nindicates mean and standard error across 2,500 test epochs.\nMethod\nPascal + Paintings\nPaintings\nPascal\n5w-1s\n5w-5s\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nCAML (ResNet34)\n57.5 ± .2\n71.0 ± .1\n46.1 ± .2\n57.3 ± .1\n77.4 ± .2\n86.8 ± .1\nCAML (ViT-base)\n63.8±.2\n78.3±.1\n51.1±.2\n65.2±.1\n82.6±.2\n89.7±.1\nCAML (ViT-huge)◦\n66.4±.4\n81.0±.2\n54.7±.3\n69.9±.2\n83.4±.4\n90.1±.3\nTable 12: meta-iNat & tiered meta-iNat & ChestX mean accuracy and standard error across 10,000\ntest epochs. ◦indicates mean and standard error across 2,500 test epochs.\nMethod\nmeta-iNat\ntiered meta-iNat\nChestX\n5w-1s\n5w-5s\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nCAML (ResNet34)\n82.4 ± .2\n91.4 ± .1\n72.3 ± .2\n84.6 ± .2\n21.8 ± .1\n23.6 ± .1\nCAML (ViT-base)\n91.2±.2\n96.3±.1\n81.9±.2\n91.6±.1\n21.5±.1\n22.2±.1\nCAML (ViT-huge)◦\n94.6±.3\n97.9±.1\n89.3±.4\n95.6±.2\n21.6±.2\n22.0±.2\nand the learnable class embeddings actually forming an ELMES, we attribute the difference in\nAircraft performance to stochasticity in training the model, suggesting that the fixed ELMES encoder\nis indeed optimal.\nImage Encoder Ablation. To evaluate how the performance of CAML is affected by the pre-trained\nimage encoder, we evaluate CAML with a ResNet-34 image encoder pre-trained on ImageNet-1k, a\nViT-base image encoder pre-trained with CLIP, and a ViT-huge image encoder that is pre-trained on\nLaion-2b (Schuhmann et al., 2022). We use the open source models released by Hugging Face in our\nevaluation.\nAs indicated in Table 9, Table 10, Table 11, and Table 12, the performance of CAML scales with the\nstrength of the feature extractor. Specifically, the performance with a ResNet-34 feature extractor\nis significantly worse than the performance with a CLIP ViT-base feature extractor, and in turn, the\nperformance with a CLIP ViT-base is significantly worse than the performance with a Laion-2b\nViT-huge feature extractor. However, its unclear what facet of the improved feature extractor is\nrelevant for CAML , especially on out-of-distribution tasks like Aircraft where the most benefit is\nseen. Moreover, it is unclear why there is no improvement on another out-of-distribution dataset,\nChestX.\n20\n",
    "Published as a conference paper at ICLR 2024\nt-SNE Plots of Image Encoder Embeddings \nViT-b CLIP ChestX\nViT-b CLIP Aircraft\nViT-h Laion-2b ChestX\nViT-h Laion-2b Aircraft\nFigure 4: t-SNE projections of different image embeddings of various benchmark datasets with embeddings\ncolored class identity. We see ViT-huge trained with Laion-2b better separates the Aircraft dataset than does\nViT-base trained with CLIP. However, both image encoders are unable to separate ChestX.\nTo investigate this dimension, we visualize the image embeddings of both Aircraft and ChestX\nusing t-sne (Van der Maaten & Hinton, 2008) dimensionality reduction. Figure 4 visualizes these\nembeddings colored by class identity. We find the ViT-huge model pre-trained on Laion-2b better\nseparates the Aircraft dataset than the ViT-base model pre-trained using the CLIP objective; however,\nboth models do not reasonably separate ChestX. We postulate that an image encoder that can capture\nthe axes of variability among image embeddings is crucial for strong CAML performance, and the\nreason we observe significantly improved results on Aircraft but not ChestX when using a Laion-2b\nViT-h image encoder.\nTaken together, these results indicate CAML is modular: as foundational model feature extractors\ncontinue to improve, CAML will be able to capture these advances to improve its own performance.\nAssignment of Labels to Support Set Classes Analysis. Symmetry to the assignment of labels\nto support set classes is a desirable property of few-shot learning algorithms. For instance, the\npredictions for [(bear, 1), (tower, 2), (tree, 3)] should be the same if the labels are permuted to [(bear,\n3), (tower 1), (tree, 2)]. CAML is not invariant to permutations in the assignment of classes to support\nset examples as implied by eq. (1) in Section 4.2; however, we empirically find it is robust to them.\nLabel symmetry is distinct from the permutation invariance property of CAML that is discussed in\nSection 4.3. Tangibly for the sequence [(bear, 1), (tower, 2), (tree, 3)], permutation invariance ensures\nthe predictions are the same as if the order of demonstrations is permuted to [(tower, 2), (tree, 3),\n(bear, 1)].\nIn Figure 5(left), we visualize the histogram of the correct class probability for the example presented\nin Figure 2a after permuting the assignment of labels to support-set images for all 120 permutations\n21\n",
    "Published as a conference paper at ICLR 2024\nHistogram Plots of Variability to Permutations in Label Assignment\nDistribution of P(Tower) after Permuting Label \nAssignment from Task in Figure 2a\nAverage Std. of Correct Class Probability after \nPermuting Label Assignment in mini-ImageNet\nFigure 5: (Left) histogram of the correct class probability for the example presented in Figure 2a after\npermuting the assignment of labels to support-set images for all 120 permutations of the 5-way-1-shot task.\n(Right) histogram of the average standard deviation of all 120 permutations of the 5-way-1-shot task for 1,000\nsamples from mini-ImageNet.\nof the 5-way-1-shot task. In Figure 5(right), we visualize the average standard deviation of all 120\npermutations of the 5-way-1-shot task for 1,000 samples from mini-ImageNet. The mean of this\nstatistic is 0.004±0.0004. Taken together, this indicates CAML is empirically robust to permutations\nin the assignment of labels to support set classes.\nD\nDISCUSSION\nWeaknesses of CAML. Despite its strong empirical performance, CAML presents several weaknesses.\nFirst, the maximum number of classes present in the support set at any point during inference must be\nknown at pre-training to instantiate a d-way ELMES. Further, at least one dataset during pre-training\nmust use a d-way classification setting so the ψi class detectors referenced in Section 4 are trained\nwithin the Transformer encoder’s attention layers.\nWhy does CAML not fine-tune the image encoder during pre-training? We do not fine-tune the\nimage encoder because it is not advantageous for universal meta-learning.\nOur goal is to develop a meta-learning algorithm that may function in a ChatGPT-like application; it\nshould be able to run in-context learning on any set of images. Foundational image models are trained\nfor exactly this purpose: they are pre-trained on billions of images to form a well-structured image\nembedding space that is robust to augmentations, occlusions, etc. Moreover, valuable characteristics\nsuch as the presence of objects, textures, etc. of an image are encoded into the structure of the\nembedding space so that the axes of variability among the embeddings encode variation in specific\nvisual attributes.\nFine-tuning the image encoder can corrupt this embedding space; especially since the datasets we\nuse for pre-training are orders of magnitude smaller than the ones used to train the Foundational\nmodel. This hypothesis is supported by our experiments with ProtoNet and MetaOpt in Tables 1 to 4.\nSpecifically, we find fine-tuning the backbone during pre-training leads to performance degradation\non many of our benchmarks when evaluated in the universal meta-learning setting.\n22\n"
  ],
  "full_text": "Published as a conference paper at ICLR 2024\nCONTEXT-AWARE META-LEARNING\nChristopher Fifty1, Dennis Duan1,2, Ronald G. Junkins1,\nEhsan Amid3, Jure Leskovec1, Christopher Ré1, Sebastian Thrun1\n1Stanford University, 2Google, 3Google DeepMind\nfifty@cs.stanford.com\nABSTRACT\nLarge Language Models like ChatGPT demonstrate a remarkable capacity to learn\nnew concepts during inference without any fine-tuning. However, visual models\ntrained to detect new objects during inference have been unable to replicate this\nability, and instead either perform poorly or require meta-training and/or fine-\ntuning on similar objects. In this work, we propose a meta-learning algorithm\nthat emulates Large Language Models by learning new visual concepts during\ninference without fine-tuning. Our approach leverages a frozen pre-trained feature\nextractor, and analogous to in-context learning, recasts visual meta-learning as\nsequence modeling over datapoints with known labels and a test datapoint with\nan unknown label. On 8 out of 11 few-shot image classification benchmarks, our\napproach—without meta-training or fine-tuning—exceeds or matches the state-of-\nthe-art algorithm, P>M>F, which is meta-trained on these benchmarks. Our code\nis available at https://github.com/cfifty/CAML.\n1\nINTRODUCTION\nMeta-learning refers to a capacity to learn new concepts from a small number of demonstrations (Lake\net al., 2015). In a decade of remarkable advances to machine intelligence, it remains an area where\nhuman performance continues to surpass that of machines (Brown et al., 2020). To match human\ncapabilities, and towards developing machines that can learn and think like humans, we must develop\nmachine intelligence capable of learning novel concepts from only a few examples (Lake et al., 2017).\nMany applications of deep learning apply a learning algorithm to a large set of training data; however,\nlearning from a very small number of training examples poses a challenge (Lake et al., 2017; Garnelo\net al., 2018). This challenge led to two predominant evaluation settings: in-domain and cross-domain.\nThe in-domain setting evaluates a meta-learner’s ability to quickly adapt to new tasks after training\non similar tasks within a specific domain. Models designed for this setting are often extremely fast\nbut exhibit poor generalization to tasks outside the target domain (Chen et al., 2019). Meanwhile,\nthe cross-domain setting evaluates a meta-learner’s ability to adapt to tasks in previously unseen\ndomains. Methods designed for this setting are highly adaptable but slow during inference as they\nrequire fine-tuning on the support set (Guo et al., 2020; Oh et al., 2022; Hu et al., 2022). Critically,\nmeta-learners in both settings differ from a human’s capacity to quickly generalize to new tasks.\nThe problem of simultaneously fast and general meta-learning has recently been addressed in Natural\nLanguage by Large Language Models (LLMs). LLMs like ChatGPT can quickly generalize to new\ntasks through an ability termed in-context learning (Brown et al., 2020). However, it remains an open\nproblem in Computer Vision. Even the best visual meta-learning algorithms cannot be deployed to a\nChatGPT-like system because such systems require models that can (1) generalize to a broad set of\ntasks unknown at training time and (2) do so in real-time, without the time allowance for finetuning\nthe model. LLMs have shown a remarkable ability to do both; however, current visual meta-learners\nmay only satisfy one requirement or the other (Hu et al., 2022).\nTo measure progress towards this goal of fast and general visual meta-learners, we develop an\nevaluation paradigm that we call universal meta-learning. Universal meta-learning measures a\nmodel’s capacity to quickly learn new image classes. It evaluates models across a diverse set of\nmeta-learning benchmarks spanning many different image classification tasks without meta-training\non any of the benchmarks’ training sets or fine-tuning on the support set during inference. We focus on\n1\narXiv:2310.10971v2  [cs.LG]  25 Mar 2024\n\n\nPublished as a conference paper at ICLR 2024\nthe application of few-shot image classification—as opposed to dense prediction tasks like in-painting\nor segmentation—as the universal setting has already been explored for these applications (Bar et al.,\n2022; Zhang et al., 2023; Wang et al., 2023; Kim et al., 2023; Butoi et al., 2023).\nBeyond benchmarking methods in the universal setting, we present a meta-learner that achieves\nstrong universal performance. Drawing inspiration from in-context learning in LLMs, we reformulate\nn-way-k-shot image classification as non-causal sequence modeling over the support set and an\nunknown query image. Specifically, given n-way classification with k-examples from each class,\nwe train a non-causal model over {(xi, yi)}nk\ni=1 (image, label) support set pairs, and an unlabeled\nquery image xnk+1, to predict the label of the query image. This formulation causes the meta-learner\nto extrapolate to new classes in its parameter space, enabling it to learn new visual concepts during\ninference without fine-tuning. Due to its capacity to learn visual information “in-context”, we term\nour approach Context-Aware Meta-Learning (CAML).\nIn summary, our contribution is two-fold. First, we develop a meta-learning evaluation paradigm that\napproximates the performance of visual meta-learners in a ChatGPT-like application. Second, we\ndesign a meta-learning algorithm that works well in this setting. Our empirical findings show that\nCAML outperforms other meta-learners in the universal setting. Remarkably, CAML’s performance\nin the universal setting often matches—and even exceeds—the in-domain performance of the state-of-\nthe-art meta-learning algorithm, P>M>F (Hu et al., 2022), that is directly trained on each down-stream\nbenchmark.\n2\nRELATED WORK\nMeta-Learning as Causal Sequence Modeling. Several of the earliest meta-learning algorithms were\nformulated as causal sequence modeling problems. Hochreiter et al. (2001) leverage a LSTM (Hochre-\niter & Schmidhuber, 1997) to model extensions to semi-linear and quadratic functions, and two\ndecades later, Graves et al. (2014); Santoro et al. (2016); Kaiser et al. (2017) build upon this approach\nby integrating a form of external memory that the LSTM can read to and write from memory to\ndevelop Neural Turing Machines. With the advent of self-attention (Vaswani et al., 2017), Mishra et al.\n(2017) predict the labels of query images by first composing a sequence of (image, label) pairs and\nthen feeding it through a stack of interleaved causal self-attention and temporal convolution layers.\nKirsch et al. (2022) replaces the stack of interleaved causal self-attention and temporal convolution\nlayers with a Transformer encoder; however, their approach is also causal in the input sequence by\ncomposing a sequence of (image, label of previous image) pairs. Both Mishra et al. (2017) and Kirsch\net al. (2022) are conceptually similar to our work; however, the causal property of both approaches\nbreaks an important symmetry in meta-learning, namely invariance to permutations of the support\nset (Garnelo et al., 2018; Müller et al., 2021). In Section 5.2, we observe a performance gap between\nboth approaches and CAML and hypothesize the causal approach actually forces a subtly more\ndifficult modeling problem by imposing a causality inductive bias on a fundamentally non-causal\nprediction task.\nCross-Domain Meta-Learning. Cross-domain meta-learning refers to a challenging evaluation\nparadigm where the meta-training and inference-time data distributions are significantly differ-\nent (Chen et al., 2019). Recent work finds that leveraging self-supervised pre-training—or foun-\ndational model feature extractors—can significantly improve cross-domain performance (Hu et al.,\n2022; Zhang et al., 2021). Moreover, fine-tuning with respect to the support set almost always\noutperforms meta-learning without fine-tuning in this setting (Guo et al., 2020; Oh et al., 2022; Phoo\n& Hariharan, 2020; Islam et al., 2021). While effective, fine-tuning is prohibitive to deploying visual\nmeta-learning models in a manner similar to LLMs like ChatGPT as the latency and memory cost\nto fine-tune a model’s parameters on each user query is untenable. Accordingly, we propose the\nuniversal setting to measure a meta-learner’s ability to learn to classify any task seen during inference\nwithout fine-tuning.\nIn-Context Learning for Dense Prediction Tasks. Many recent works have explored in-context\nlearning for other applications of computer vision. Bar et al. (2022) casts in-context learning as\nimage in-painting by first concatenating demonstration images with a query image and then using\na vision model to fill-in-the-blank within this concatenated image. Building on this work, Zhang\net al. (2023) explores what demonstrations lead to strong in-painting performance and Wang et al.\n(2023) generalizes the approach by formulating other visual applications like segmentation, depth\n2\n\n\nPublished as a conference paper at ICLR 2024\nELMES Class Encoder\nMLP\nClass\nP(Class)\nPre-Trained Image Encoder\nConcatenate \nEmbeddings\nSupport Set Images\nQuery Image\n*\n Unknown \n[class] \nEmbedding\n[1, 0, 0]\n[0, 1, 0]\n[0, 0, 1]\n[1, 0, 0]\n[0, 1, 0]\n[0, 0, 1]\n0.03\n0.06\n0.91\n*\n*\nSupport Set Classes\nParameters Frozen\nParameters Trained\nNon-Causal Sequence Model\nFigure 1: Overview of CAML. Query and support set images are encoded with a pre-trained feature extractor\nand then concatenated with their corresponding ELMES label embeddings. We feed the resulting sequence of\nconcatenated vectors into a non-casual sequence model and extract the query vector from the output sequence to\npredict its class.\nestimation, etc. as in-painting. Other approaches explore in-context learning for applications like\nscene understanding (Balazevic et al., 2024), medical image segmentation (Butoi et al., 2023), and\nmore generally dense prediction tasks (Kim et al., 2023). Like these approaches, we study visual\nin-context learning; however, this work focuses on few-shot image classification rather than dense\nprediction tasks.\n3\nAPPROACH\nWe adapt the ideas underpinning in-context learning in LLMs—namely learning to classify a query\nfrom a context of support set demonstrations in a single forward pass—to image classification.\nHowever, dissimilar from in-context learning, visual meta-learners should be non-causal: placing one\nexample before another in the support set does not entail a causal relationship (Garnelo et al., 2018;\nMüller et al., 2021).\nArchitecture. An overview of CAML is shown in Figure 1. It consists of three different components:\n(1) a frozen pre-trained image encoder, (2) a fixed Equal Length and Maximally Equiangular Set\n(ELMES) class encoder, and (3) a non-causal sequence model. While pre-trained image encoders and\nnon-causal sequence models are well-known, to encode label information we introduce an ELMES\nencoder. An ELMES encoder is a bijective mapping between the labels and a set of vectors that are\nequal length and maximally equiangular. Historically, labels have been encoded with one-hot vectors;\nhowever in Section 4, we prove that an ELMES encoding of mutually exclusive classes allows the\nsequence model to maximally identify classes within the support set.\nAs visualized in Figure 1, CAML first encodes query and support set images using a frozen pre-trained\nfeature extractor. Crucially, the pre-trained image encoder’s embedding space distills images into\nlow-dimensional representations so that images with similar content and visual characteristics have\nsimilar embeddings. Classes of the support set are encoded with an ELMES class encoder; however\nas the class of the query is unknown, we use a special learnable “unknown token” embedding that is\nlearned during large-scale pre-training. CAML then concatenates each image embedding with its\ncorresponding query embedding to form an input sequence.\nProgressing through Figure 1, this sequence is fed into a non-causal sequence model, i.e. a Trans-\nformer encoder, to condition the output representations on the full context of query and support\nset points. This enables dynamic and real-time classification; visual characteristics from query and\nsupport set images can be compared with each other to determine the specific visual features—such\nas content, textures, etc.—used to classify the query. From the output sequence of the non-causal\nsequence model, we select the element at the same position as the query in the input sequence, and\npass this vector through a shallow MLP to predict the label of the query.\nLarge-Scale Pre-Training. As our focus is universal meta-learning—and CAML may encounter any\nnew visual concept during inference—we pre-train CAML’s non-causal sequence model on few-shot\n3\n\n\nPublished as a conference paper at ICLR 2024\nimage classification tasks from ImageNet-1k (Deng et al., 2009), Fungi (Schroeder & Cui, 2018),\nMSCOCO (Lin et al., 2014), and WikiArt (Saleh & Elgammal, 2015). We chose these datasets because\nthey span generic object recognition (ImageNet-1k, MSCOCO), fine-grained image classification\n(Fungi), and unnatural image classification (WikiArt). To avoid distorting the pre-trained image\nencoder’s embedding space, we freeze this module and only update the sequence model’s parameters\nduring pretraining. Similarly, since an ELMES minimizes the entropy of detecting classes within\nthe support set, the label encoder is also frozen. In the context of pre-training, meta-training, and\nfine-tuning, CAML only requires pre-training and avoids meta-training on the train/validation splits\nof meta-learning benchmarks or fine-tuning on the support set during inference.\n4\nTHEORETICAL ANALYSIS\nIn this section, we motivate our choice of the ELMES Class Encoder by considering the symmetries\ndesirable in meta-learning algorithms. Two important symmetries are (1) invariance to the assignment\nof support set classes to numeric labels and (2) invariance to permutations in the ordering of the input\nsequence. The first invariance implies the class embeddings must be equiangular and equal norm,\nwith an ELMES configuration minimizing the entropy of learnable model parameters detecting any\ngiven class. Later, we show an ELMES also satisfies the second symmetry. Due to space constraints,\nall proofs and many definitions, properties, lemmas, and theorems are allocated to Appendix A.1. We\nbegin with a formal definition of an ELMES.\n4.1\nEQUAL LENGTH AND MAXIMALLY EQUIANGULAR SET OF VECTORS\nDefinition 1. An Equal Length and Maximally Equiangular Set (ELMES) is a set of non-zero\nvectors {ϕj}d\nj=1, ϕj ∈Rd+k for some k ≥0 and d > 1, such that ∀j ̸= j′, ∥ϕj∥= ∥ϕj′∥and\n⟨ϕj , ϕj′⟩=\n−1\nd−1. Simply, all vectors in this set are equal length and maximally equiangular.\nAn Equal Angle and Maximally Equiangular Set (ELMES) of vectors has connections to both\nEquiangular Tight Frames in representation theory (Welch, 1974; Fickus et al., 2018) as well as the\nSimplex Equiangular Tight Frames highlighted in recent neural collapse works exploring softmax-\nlayer geometry at the terminal phase of training (Papyan et al., 2020; Yang et al., 2022). We offer\nadditional discussion comparing these structures in Appendix A.1 as well as provide an intuitive view\nof an ELMES as a regular d-simplex immersed in Rd+k.\n4.2\nLABEL SYMMETRY\nSymmetry in the assignment of support classes to numeric labels is an important property of meta-\nlearning algorithms. For example, if we have the support set classes {tower, bear, tree}, the mapping\nof {bear -> 1, tower -> 2, tree -> 3} should produce the same prediction for a query point as a\ndifferent mapping {bear -> 2, tower -> 3, tree -> 1}. To explore this symmetry, we examine how\nclass embeddings may be used by the model.\nFrom our formulation in Section 3, we represent a demonstration vector as a concatenation of an\nimage embedding ρ and a label embedding ϕ: [ρ\nϕ]. This vector is directly fed into the self-attention\nmechanism, where we matrix multiply with key, query, and value self-attention heads. Taking only\none of these matrices for simplicity with head-dimension k:\n[ρ\nϕ]\n\u0014\nΓ1\n...\nΓk\nψ1\n...\nψk\n\u0015\n= [⟨ρ , Γ1⟩\n...\n⟨ρ , Γk⟩] + [⟨ϕ , ψ1⟩\n...\n⟨ϕ , ψk⟩]\n(1)\nThe output of this transformation will be the sum of two vectors: one composed of the inner products\nbetween the image embedding ρ and the learnable {Γi}k\ni=1 and the other composed of the class\nembedding ϕ and the learnable {ψi}k\ni=1. Note that Equation (1) implies that CAML is not invariant\nto the assignment of labels to support set classes due to the addition between ⟨ρ , Γi⟩and ⟨ϕ , ψi⟩;\nhowever, we can constrain the geometry of the class embeddings {ϕ}d\nj=1 to in principle respect label\nsymmetry. Specifically for i ̸= j ̸= k, ⟨ϕi , ϕj⟩= ⟨ϕi , ϕk⟩and ∥ϕi∥= ∥ϕj∥.\nSimilar to a convolutional filter learning to match a pattern within an image, our analysis assumes\nthe learnable [ψ1\n...\nψk] will converge to vectors that maximize the inner product with a single\n4\n\n\nPublished as a conference paper at ICLR 2024\nclass embedding subject to certain constraints. Under this assumption, we ask what geometry of the\nd-class embeddings {ϕ}d\nj=1 allows a learnable ψi vector to most unambiguously detect a single class\nembedding. To answer this question, we define a probability mass function for each ψi over the set\nof d−classes so that maximizing the probability of the jth class aligns with maximizing ⟨ϕj , ψi⟩\nand equally minimizing ⟨ϕk , ψi⟩for k ̸= j.\nDefinition 2. Let X be a discrete Random Variable taking on values in {1, 2, ..., d}. For learnable vec-\ntor ψi, define probability mass function pψi(X = j) as the softmax over [⟨ϕ1 , ψi⟩\n...\n⟨ϕd , ψi⟩]\nso that:\npψi(X = j) =\ne∥ψi∥∥ϕj∥cos(θi,j)\nPd\nk=1 e∥ψi∥∥ϕj∥cos(θi,k)\nwhere θi,j is the angle between ϕj and ψi.\nWe say ψi learns to detect class j when pψi(X = j) > pψi(X = k) for 1 ≤k ≤d with k ̸= j. By\nsymmetry in the assignment of class embeddings to support classes, we can assume that the number\nof ψi learned to detect class i is similar to the number of ψj learned to detect class j for all pairs (i, j).\nWe also leverage symmetry in the assignment of labels to support set classes to make the following\nassumptions. A justification for each assumption is located in Appendix A.1.\nAssumption 1. Suppose {ψi}k\ni=1 are learnable class detectors of unit norm with at least one ψi\ndetecting each class 1 ≤i ≤d. The probability pψj(X = j) = pψi(X = i) for 1 ≤i, j ≤d.\nAssumption 2. Define pψi(X = i)\\{ϕl}d\nl=(m+1) as the probability of ψi detecting ϕi from the\nset of vectors {ϕj}m\nj=1, m < d. Then the probability pψj(X = j)\\{ϕl}d\nl=(m+1) = pψi(X =\ni)\\{ϕl}d\nl=(m+1) for 1 ≤i, j ≤m and m ≥2.\nAssumption 3. When ψi =\nϕi\n∥ϕi∥, pψi(X = i) is maximized.\nWhen Assumption 1, Assumption 2, and Assumption 3 hold, the set of class embeddings that\nmaximize the probability of a learnable ψi detecting class i is necessarily an ELMES.\nTheorem 1. The set of class embeddings {ϕj}d\nj=1 ∀j, 1 ≤j ≤d that maximizes pψj(X = j) is\nnecessarily an ELMES.\nAlternatively when viewed through the lens of information theory, we can interpret an ELMES as the\nclass embedding that minimizes the entropy of ψi detecting class i. Informally, ELMES causes ψi to\nhave the least uncertainty when detecting class i.\nProposition 1. Let Hψi(X) be the entropy of pψi(X). An ELMES minimizes Hψi(X).\n4.3\nPERMUTATION INVARIANCE.\nIn addition to label symmetry, it is also desirable for the output prediction of CAML to not depend on\nthe order of demonstrations in the sequence. For example, if we have the support set classes {tower,\nbear, tree}, the sequence {(bear -> 1), (tower -> 2), (tree -> 3)} should produce the same output as the\npermuted sequence {(tree -> 3), (bear -> 1), (tower -> 2)}. Building on the prior work of Kossen et al.\n(2021); Fifty et al. (2023), it suffices to show to show that the ELMES label encoder is equivariant to\npermutations in the input sequence to show that CAML is invariant to permutations.\nProposition 2. Consider an n-sequence of one-hot labels stacked into a matrix S ∈Rn×w, and an\nELMES label encoder denoted by W ∈Rw×d with w denoting “way” and d the dimension of the\nlabel embedding. The label embedding SW is equivariant to permutations.\n5\nEXPERIMENTS\nTo quantify universal image classification performance, we evaluate a diverse set of 11 meta-learning\nbenchmarks divided across 4 different categories:\n1. Generic Object Recognition: mini-ImageNet (Vinyals et al., 2016), tiered-ImageNet (Ren et al.,\n2018), CIFAR-fs (Bertinetto et al., 2018), and Pascal VOC (Everingham et al.)\n5\n\n\nPublished as a conference paper at ICLR 2024\nTable 1: MiniImageNet & CIFAR-fs mean accuracy and standard error across 10,000 test epochs. †\nindicates the pre-trained image encoder backbone was frozen during training.\nMethod (Backbone)\nCIFAR-fs\nMiniImageNet\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nIn-Domain [Meta-Training]\nP>M>F Hu et al. (2022)\n84.3\n92.2\n95.3\n98.4\nUniversal Meta-Learning;\nNo Meta-Training or Finetuning\nProtoNet (Snell et al., 2017)\n62.9±.2\n79.7±.2\n92.1±.1\n97.1±.0\nProtoNet†\n57.7±.2\n81.0±.2\n85.3±.2\n96.0±.1\nMetaOpt (Lee et al., 2019)\n53.1±.3\n73.1±.2\n78.5±.2\n91.6±.1\nMetaOpt†\n61.7±.2\n83.1±.1\n86.9±.2\n96.5±.1\nMetaQDA (Zhang et al., 2021)\n60.4±.2\n83.2±.1\n88.2±.2\n97.4±.0\nGPICL (Kirsch et al., 2022)\n41.5±.4\n78.3±.2\n95.6±.1\n98.2±.1\nSNAIL (Mishra et al., 2017)\n62.1±.3\n71.1±.3\n93.6±.1\n98.1±.0\nCAML\n70.8±.2\n85.5±.1\n96.2±.1\n98.6±.0\nTable 2: Pascal & Paintings mean accuracy and standard error across 10,000 test epochs. † indicates\nthe the pre-trained image encoder backbone was frozen during training.\nMethod (Backbone)\nPascal + Paintings\nPaintings\nPascal\n5w-1s\n5w-5s\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nIn-Domain [Meta-Training]\nP>M>F\n60.7\n74.4\n53.2\n65.8\n72.2\n84.4\nUniversal Meta-Learning\nProtoNet\n49.6±.2\n63.5±.1\n38.3±.2\n48.2±.1\n77.9±.2\n87.3±.2\nProtoNet†\n52.2±.2\n70.6±.1\n48.3±.2\n64.1±.1\n72.2±.2\n84.3±.2\nMetaOpt\n38.2±.2\n58.2±.1\n31.6±.2\n48.0±.1\n63.7±.2\n81.7±.2\nMetaOpt†\n53.2±.2\n74.8±.1\n49.3±.2\n65.9±.1\n72.8±.2\n84.4±.2\nMetaQDA\n53.8±.2\n74.1±.1\n49.4±.2\n66.6±.1\n73.5±.2\n85.2±.2\nGPICL\n62.6±.2\n74.6±.1\n51.6±.2\n61.0±.1\n81.7±.2\n88.2±.2\nSNAIL\n62.5±.2\n77.6±.1\n51.9±.2\n65.8±.1\n79.7±.2\n88.0±.2\nCAML\n63.8±.2\n78.3±.1\n51.1±.2\n65.2±.1\n82.6±.2\n89.7±.1\n2. Fine-Grained Image Classification: CUB (Wah et al., 2011), Aircraft (Maji et al., 2013), meta-\niNat (Wertheimer & Hariharan, 2019), and tiered meta-iNat (Wertheimer & Hariharan, 2019)\n3. Unnatural Image Classification: ChestX (Guo et al., 2020) and Paintings (Crowley & Zisserman,\n2015)\n4. Inter-Domain Image Classification: Pascal+Paintings (Everingham et al.; Crowley & Zisserman,\n2015).\nGeneric object recognition, fine-grained image classification, and unnatural image classification\nare standard benchmarking tasks in meta-learning literature (Chen et al., 2020; Hu et al., 2022;\nWertheimer et al., 2020; Guo et al., 2020). Beyond this, we compose a challenging new inter-domain\ncategory by combining Pascal VOC with Paintings so that each class is composed of both natural\nimages and paintings. This allows us to evaluate the ability of meta-learning algorithms to generalize\nacross domains within the same class. For example, the support image for the class “tower” may be\nVan Gogh’s The Starry Night, while the query may be a picture of the Eiffel Tower. Humans have\nthe ability to generalize visual concepts between such domains; however, meta-learning algorithms\nstruggle with this formulation (Jankowski & Gr ˛abczewski, 2011).\n5.1\nBASELINES\nWe evaluate the performance of CAML, Prototypical Networks (ProtoNet) (Snell et al., 2017),\nMetaOpt (Lee et al., 2019), MetaQDA (Zhang et al., 2021), SNAIL (Mishra et al., 2017), and\nGPICL (Kirsch et al., 2022) in a universal meta-learning setting by pre-training them with a ViT-\nbase (Dosovitskiy et al., 2020) feature extractor initialized with weights from CLIP (Radford et al.,\n6\n\n\nPublished as a conference paper at ICLR 2024\nTable 3: meta-iNat & tiered meta-iNat & ChestX mean accuracy and standard error across 10,000\ntest epochs. † indicates the the pre-trained image encoder backbone was frozen during training.\nMethod (Backbone)\nmeta-iNat\ntiered meta-iNat\nChestX\n5w-1s\n5w-5s\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nIn-Domain [Meta-Training]\nP>M>F\n91.2\n96.1\n74.8\n89.9\n27.0\n32.1\nUniversal Meta-Learning\nProtoNet\n78.4±.2\n89.4±.1\n66.3±.2\n82.2±.2\n22.4±.1\n25.3±.1\nProtoNet†\n84.5±.2\n94.8±.1\n73.8±.2\n89.5±.1\n22.7±.1\n25.8±.1\nMetaOpt\n53.0±.2\n77.7±.2\n37.3±.2\n63.0±.2\n20.8±.1\n23.0±.1\nMetaOpt†\n85.5±.2\n95.5±.1\n75.1±.2\n91.9±.1\n23.0±.1\n27.4±.1\nMetaQDA\n86.3±.2\n95.9±.1\n76.0±.2\n92.4±.1\n22.6±.1\n27.0±.1\nGPICL\n90.0±.2\n95.1±.1\n60.8±.5\n87.6±.2\n20.1±.1\n20.9±.1\nSNAIL\n89.1±.2\n94.8±.1\n77.3±.2\n86.5±.2\n20.2±.0\n20.0±.0\nCAML\n91.2±.2\n96.3±.1\n81.9±.2\n91.6±.1\n21.5±.1\n22.2±.1\nTable 4: CUB & tiered-ImageNet & Aircraft mean accuracy and standard error across 10,000 test\nepochs. † indicates the the pre-trained image encoder backbone was frozen during training.\nMethod (Backbone)\nCUB\ntiered-ImageNet\nAircraft\n5w-1s\n5w-5s\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nIn-Domain [Meta-Training]\nP>M>F\n92.3\n97.0\n93.5\n97.3\n79.8\n89.3\nUniversal Meta-Learning\nProtoNet\n59.4±.2\n77.3±.2\n93.5±.1\n97.4±.1\n37.9±.2\n52.5±.2\nProtoNet†\n87.0±.2\n97.1±.1\n87.3±.2\n96.1±.1\n62.4±.3\n82.0±.2\nMetaOpt\n71.5±.2\n41.2±.2\n76.6±.2\n89.6±.1\n41.6±.2\n26.7±.1\nMetaOpt †\n87.9±.2\n97.2±.1\n88.2±.2\n96.5±.1\n64.8±.2\n82.6±.2\nMetaQDA\n88.3±.2\n97.4±.1\n89.4±.2\n97.0±.1\n63.6±.3\n83.0±.2\nGPICL\n75.1±.5\n94.5±.1\n94.6±.1\n97.2±.1\n19.8±.2\n61.8±.3\nSNAIL\n87.5±.2\n92.8±.2\n93.1±.1\n97.3±.1\n48.9 ± .3\n35.8±.3\nCAML\n91.8±.2\n97.1±.1\n95.4±.1\n98.1±.1\n63.3±.3\n79.1±.2\n2021). Pre-training runs over few-shot classification tasks from ImageNet-1k, Fungi, MSCOCO,\nand WikiArt, and during evaluation on the set of 11 meta-learning benchmarks, models are not\nmeta-trained or fine-tuned. We compare with ProtoNet, MetaOpt, and MetaQDA as they achieve\nstate-of-the-art results when paired with a pre-trained feature extractor (Hu et al., 2022). As sequence\nmodeling underpins CAML, we also compare with SNAIL and GPICL to evaluate the performance\nof past formulations of causal sequence-based meta-learning algorithms in the universal setting.\nTo assess the gap between universal and in-domain meta-learning performance, we benchmark the\ncurrent state-of-the-art meta-learning algorithm P>M>F (Hu et al., 2022). Similar to the universal\nsetting, P>M>F uses a ViT-base feature extractor initialized with weights from DINO (Caron et al.,\n2021); however, it meta-trains on the training set of each benchmark before evaluating on that\nbenchmark’s test set.\nWhen pre-training all models in the universal setting, we set the learning rate to a fixed 1 × 10−5 and\ndo not perform any hyperparameter tuning in order to match the practices used by P>M>F. We use\nearly stopping with a window size of 10 epochs during pre-training and the code release of Hu et al.\n(2022) to benchmark P>M>F with the training settings and hyperparameters described in their work.\n5.2\nRESULTS\nOur findings are summarized in Table 1, Table 2, Table 3, and Table 4 and indicate that CAML sets a\nnew state-of-the-art for universal meta-learning by significantly outperforming other baselines on\n14 of 22 evaluation settings. For 5 of the other 8 evaluation settings, CAML matches—or nearly\nmatches—the best performing baseline. Remarkably, CAML also performs competitively with\n7\n\n\nPublished as a conference paper at ICLR 2024\nP>M>F on 8 out of 11 meta-learning benchmarks, even though P>M>F meta-trains on the training\nset of each benchmark.\nThis result suggests that the amount of new visual information learned during inference through\nvisual in-context learning can be comparable to the amount learned when directly meta-training on\nin-domain data. This capacity may unlock new applications in the visual space, just as the emergence\nof in-context learning in LLMs has enabled many new applications in natural language.\nBenchmarks Where CAML Underperforms. The 3 datasets where P>M>F outperforms CAML\nare CIFAR-fs, Aircraft, and ChestX. CIFAR-fs is a generic object recognition benchmark containing\nCIFAR images downsampled to 32x32 resolution. As CAML and CLIP pre-train on 224x224\nresolution images, downsampling by a factor of 49 likely induces a distribution shift that was not\nlearned by CAML during large-scale pre-training. In the cases of Aircraft and ChestX, we postulate\nthat the CLIP embedding space—structured so images with similar captions have similar embeddings–\nstruggles to effectively differentiate between the fine-grained, specialized classes in these tasks. For\nexample, while a Boeing 737 and Airbus A380 have different labels in the Aircraft dataset, the\nscraped CLIP captions for those images may not reach that level of granularity. This corroborates the\nfindings from Radford et al. (2021), which found that in a zero-shot setting, CLIP underperforms in\nspecialized or complex tasks.\nOur ablation study into non-CLIP pre-trained feature extractors in Tables 5 to 8 of Appendix C shows\nCAML’s performance on Aircraft can drastically improve. Specifically, 5w-1s performance increases\nfrom 63.3 to 81.8 and 5w-5s performance increases from 79.1 to 92.1 when a ViT-Huge pre-trained\non Laion-2b (Schuhmann et al., 2022) initializes the weights of the image encoder rather than CLIP.\nFine-tuning CLIP Backbone. Our findings in Tables 1 to 4 indicate that updating the CLIP image\nencoder during pre-training hurts the performance of ProtoNet and MetaOpt. We observe that these\nmethods tend to overfit during pre-training, and our empirical results show a similar pattern: pre-\ntraining with these methods often helps the performance of benchmarks similar to ImageNet (i.e.\nPascal, MiniImageNet, tiered-ImageNet), but it significantly hurts the performance of out-of-domain\ntasks (i.e. Aircraft, CUB, Paintings) as shown in Tables 1 to 4. We believe that further training the\nCLIP backbone distorts the structure of its embedding space, leading to catastrophic forgetting on\nout-of-domain tasks. Conversely, CAML, MetaQDA, SNAIL, and GPICL—all of which freeze the\nparameters of the CLIP feature extractor—benefit greatly from large-scale episodic pre-training on\nImageNet-1k, Fungi, MSCOCO, and WikiArt.\n6\nANALYSIS\nTo better understand how CAML learns during inference, we analyze its ability to dynamically update\nits representations. Due to casting meta-learning as non-causal sequence modeling, CAML considers\nthe full context of query and support set to predict the label of the query. Specifically, the query\ndynamically influences the representation of support set points, and the support set points dynamically\ninfluence the representation of the query as this sequence is passed through the layers of a non-causal\nsequence model. This property enables universal meta-learning by allowing the model to update\nthe support and query representations based on the context of the task, not only the contents of the\nimages, within the parameter space of the sequence model.\nAn example where the query dynamically influences the support set is visualized in Figure 2. Given\nonly the 5 support examples, the prediction task is ambiguous. However, the nature of the query\ndetermines the prediction task. The query image of a tower in Figure 2a reduces the task to generic\nobject recognition: classify the query based on the object portrayed in the image. On the other hand,\nand as visualized in Figure 2b, the query image of embroidery reduces the prediction task to texture\nidentification: classify the query based on artistic medium.\nTo analyze how dynamic representations affect CAML, we examine the representations of the support\nset and query vectors at the input to and output of the non-causal sequence model. For both examples\nvisualized in Figure 2a and Figure 2b, the non-causal sequence model learns to separate support set\nvectors by class identity and group the query representation with the correct support set example.\nWe find the frozen CLIP image embeddings are actually antagonistic for the classification-by-texture\ntask visualized in Figure 2b: the query image embedding is closest to the support set example for\n8\n\n\nPublished as a conference paper at ICLR 2024\nTower\nChicken\nSupport\nCow\nFlower\nHuman\nTower\nQuery\n(a) Left: An example task—classify images by the objects depicted. Center: image embeddings output from the\nImage Encoder (CLIP) in CAML . Right: joint image-label representations output by the non-causal sequence\nmodel in CAML for the same task.\nEmbroidery\n3D Model\nSupport\nPencil Sketch\nOil Painting\nStained Glass\nEmbroidery\nQuery\n(b) Left: An example task—classify images by the artistic medium used. Center: CLIP image embeddings output\nfrom the Image Encoder (CLIP) in CAML . Right: joint image-label representations output by the non-causal\nsequence model in CAML for the same task.\nFigure 2: Two sample tasks over the same support images but utilizing different criteria to define classes. The\nnature of the query image informs the task being presented, e.g. classification by object (top) vs. classification\nby texture (bottom). For both tasks, the output of the non-causal sequence model provides better separation\namong class representations than CLIP embeddings and groups the query representation with the proper task,\neven when projected into 2D space by PCA.\nthe second class, “oil painting”. Unsurprisingly, the baseline methods that rely on frozen CLIP\nembeddings—specificially MetaQDA, ProtoNet†, and MetaOpt†—group the query with “oil painting”\nand therefore misclassify this example. On the other hand, as CAML considers the full context of the\nquery and support set, it develops representations of the query in the context of the support set—and\nthe support set in the context of the query—to group the query with the “embroidery” support set\nimage as they share the same texture, thereby correctly classifying this example.\n7\nCONCLUSION\nIn this work, we develop universal meta-learning to approximate the performance of visual meta-\nlearners deployed to a ChatGPT-like application and present CAML: a meta-learning algorithm that\nemulates in-context learning in LLMs by learning new visual concepts during inference without\nfine-tuning. Our empirical findings show that CAML—without meta-training or fine-tuning—exceeds\nor matches the performance of the current state-of-the-art meta-learning algorithm on 8 out of 11\nbenchmarks. This result indicates visual meta-learning models are ready for deployment in a manner\nsimilar to LLMs, and we hope this work recalibrates our sense of limitations for the universal\nmeta-learning paradigm.\nNevertheless, there are areas where CAML struggles. Specifically, the performance of CAML on\nhighly out-of-distribution images—e.g. chest x-rays—and varying image resolutions—e.g. rescaled\nCIFAR images—lags behind that of the best in-domain approaches. Developing methods for the\nuniversal setting that are robust to these cases is a promising direction for future work.\n9\n\n\nPublished as a conference paper at ICLR 2024\nACKNOWLEDGMENTS\nWe thank Mayee Chen, Dan Fu, Jerry Liu, and Benjamin Spector for their invaluable feedback\nand help during revisions of this work. We also thank Chelsea Finn for helping us improve the\nrelated work, Victor Butoi for constructive dialogue over Twitter, and the Hazy Group at Stanford\nas a whole their support throughout the research process. We gratefully acknowledge the support\nof NIH under No. U54EB020405 (Mobilize), DARPA under Nos. N660011924033 (MCS), NSF\nunder Nos. OAC-1835598 (CINES), CCF-1918940 (Expeditions), DMS-2327709 (IHBEM), Nos.\nCCF2247015 (Hardware-Aware), CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Veloc-\nity), and 1937301 (RTML); US DEVCOM ARL under Nos. W911NF-23-2-0184 (Long-context)\nand W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under Nos. N000142312633 (Deep\nSignal Processing); Stanford HAI under No. 247183; NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft,\nNEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices,\nGoogle Cloud, Salesforce, Total, Wu Tsai Neurosciences Institute, Chan Zuckerberg Initiative, Ama-\nzon, Genentech, GSK, Juniper Networks, KDDI, UCB, the HAI-GCP Cloud Credits for Research\nprogram, the Stanford Data Applications Initiative, and the Stanford Data Science Initiative (SDSI).\nThe U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes\nnotwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recom-\nmendations expressed in this material are those of the authors and do not necessarily reflect the views,\npolicies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government.\nREFERENCES\nIvana Balazevic, David Steiner, Nikhil Parthasarathy, Relja Arandjelovi´c, and Olivier Henaff. Towards\nin-context scene understanding. Advances in Neural Information Processing Systems, 36, 2024.\nAmir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting\nvia image inpainting. Advances in Neural Information Processing Systems, 35:25005–25017, 2022.\nLuca Bertinetto, Joao F Henriques, Philip HS Torr, and Andrea Vedaldi. Meta-learning with differen-\ntiable closed-form solvers. arXiv preprint arXiv:1805.08136, 2018.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\nVictor Ion Butoi, Jose Javier Gonzalez Ortiz, Tianyu Ma, Mert R Sabuncu, John Guttag, and Adrian V\nDalca. Universeg: Universal medical image segmentation. pp. 21438–21451, 2023.\nMathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the\nIEEE/CVF international conference on computer vision, pp. 9650–9660, 2021.\nWei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look\nat few-shot classification. arXiv preprint arXiv:1904.04232, 2019.\nWei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look\nat few-shot classification, 2020.\nElliot J Crowley and Andrew Zisserman. In search of art. In Computer Vision-ECCV 2014 Workshops:\nZurich, Switzerland, September 6-7 and 12, 2014, Proceedings, Part I 13, pp. 54–70. Springer,\n2015.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hier-\narchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,\npp. 248–255, 2009. doi: 10.1109/CVPR.2009.5206848.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020.\n10\n\n\nPublished as a conference paper at ICLR 2024\nM. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman.\nThe\nPASCAL Visual Object Classes Challenge 2012 (VOC2012) Results.\nhttp://www.pascal-\nnetwork.org/challenges/VOC/voc2012/workshop/index.html.\nMatthew Fickus, John Jasper, Emily J King, and Dustin G Mixon. Equiangular tight frames that\ncontain regular simplices. Linear Algebra and its applications, 555:98–138, 2018.\nChristopher Fifty, Jure Leskovec, and Sebastian Thrun. In-context learning for few-shot molecular\nproperty prediction, 2023.\nMarta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray\nShanahan, Yee Whye Teh, Danilo Rezende, and SM Ali Eslami. Conditional neural processes. In\nInternational conference on machine learning, pp. 1704–1713. PMLR, 2018.\nAlex Graves, Greg Wayne, and Ivo Danihelka.\nNeural turing machines.\narXiv preprint\narXiv:1410.5401, 2014.\nYunhui Guo, Noel C Codella, Leonid Karlinsky, James V Codella, John R Smith, Kate Saenko, Tajana\nRosing, and Rogerio Feris. A broader study of cross-domain few-shot learning. In Computer\nVision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,\nPart XXVII 16, pp. 124–141. Springer, 2020.\nSepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n1735–1780, 1997.\nSepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent.\nIn Artificial Neural Networks—ICANN 2001: International Conference Vienna, Austria, August\n21–25, 2001 Proceedings 11, pp. 87–94. Springer, 2001.\nShell Xu Hu, Da Li, Jan Stühmer, Minyoung Kim, and Timothy M. Hospedales. Pushing the limits\nof simple pipelines for few-shot learning: External data and fine-tuning make a difference, 2022.\nAshraful Islam, Chun-Fu Richard Chen, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, and\nRichard J Radke. Dynamic distillation network for cross-domain few-shot recognition with\nunlabeled data. Advances in Neural Information Processing Systems, 34:3584–3595, 2021.\nNorbert Jankowski and Krzysztof Gr ˛abczewski. Universal meta-learning architecture and algorithms.\nMeta-learning in computational intelligence, pp. 1–76, 2011.\nŁukasz Kaiser, Ofir Nachum, Aurko Roy, and Samy Bengio. Learning to remember rare events.\narXiv preprint arXiv:1703.03129, 2017.\nDonggyun Kim, Jinwoo Kim, Seongwoong Cho, Chong Luo, and Seunghoon Hong.\nUniver-\nsal few-shot learning of dense prediction tasks with visual token matching.\narXiv preprint\narXiv:2303.14969, 2023.\nLouis Kirsch, James Harrison, Jascha Sohl-Dickstein, and Luke Metz. General-purpose in-context\nlearning by meta-learning transformers. arXiv preprint arXiv:2212.04458, 2022.\nJannik Kossen, Neil Band, Clare Lyle, Aidan N Gomez, Thomas Rainforth, and Yarin Gal. Self-\nattention between datapoints: Going beyond individual input-output pairs in deep learning. Ad-\nvances in Neural Information Processing Systems, 34:28742–28756, 2021.\nBrenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning\nthrough probabilistic program induction. Science, 350(6266):1332–1338, 2015.\nBrenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building\nmachines that learn and think like people. Behavioral and brain sciences, 40:e253, 2017.\nKwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with\ndifferentiable convex optimization, 2019.\n11\n\n\nPublished as a conference paper at ICLR 2024\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–\nECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pp. 740–755. Springer, 2014.\nSubhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained\nvisual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.\nNikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-\nlearner. arXiv preprint arXiv:1707.03141, 2017.\nSamuel Müller, Noah Hollmann, Sebastian Pineda Arango, Josif Grabocka, and Frank Hutter.\nTransformers can do bayesian inference. arXiv preprint arXiv:2112.10510, 2021.\nJaehoon Oh, Sungnyun Kim, Namgyu Ho, Jin-Hwa Kim, Hwanjun Song, and Se-Young Yun.\nUnderstanding cross-domain few-shot learning based on domain similarity and few-shot difficulty.\nAdvances in Neural Information Processing Systems, 35:2622–2636, 2022.\nVardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the terminal\nphase of deep learning training. Proceedings of the National Academy of Sciences, 117(40):\n24652–24663, 2020.\nCheng Perng Phoo and Bharath Hariharan. Self-training for few-shot transfer across extreme task\ndifferences. arXiv preprint arXiv:2010.07734, 2020.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748–8763. PMLR, 2021.\nMengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum,\nHugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classification.\narXiv preprint arXiv:1803.00676, 2018.\nBabak Saleh and Ahmed Elgammal. Large-scale classification of fine-art paintings: Learning the\nright metric on the right feature. arXiv preprint arXiv:1505.00855, 2015.\nAdam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-\nlearning with memory-augmented neural networks. In International conference on machine\nlearning, pp. 1842–1850. PMLR, 2016.\nBrigit Schroeder and Yin Cui.\nFGVCx fungi classification challenge 2018.\ngithub.com/\nvisipedia/fgvcx_fungi_comp, 2018.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An\nopen large-scale dataset for training next generation image-text models. Advances in Neural\nInformation Processing Systems, 35:25278–25294, 2022.\nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning, 2017.\nAndreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas\nBeyer. How to train your vit? data, augmentation, and regularization in vision transformers. arXiv\npreprint arXiv:2106.10270, 2021.\nLaurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\nlearning research, 9(11), 2008.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing\nsystems, 30, 2017.\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one\nshot learning. Advances in neural information processing systems, 29, 2016.\n12\n\n\nPublished as a conference paper at ICLR 2024\nCatherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd\nbirds-200-2011 dataset. 2011.\nXinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A\ngeneralist painter for in-context visual learning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 6830–6839, 2023.\nManfred K Warmuth and Dima Kuzmin. Randomized online pca algorithms with regret bounds\nthat are logarithmic in the dimension. Journal of Machine Learning Research, 9(Oct):2287–2320,\n2008.\nLloyd Welch. Lower bounds on the maximum cross correlation of signals (corresp.). IEEE Transac-\ntions on Information theory, 20(3):397–399, 1974.\nDavis Wertheimer and Bharath Hariharan. Few-shot learning with localization in realistic settings.\nIn Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.\n6558–6567, 2019.\nDavis Wertheimer, Luming Tang, and Bharath Hariharan. Few-shot classification with feature map\nreconstruction networks, 2020.\nYibo Yang, Liang Xie, Shixiang Chen, Xiangtai Li, Zhouchen Lin, and Dacheng Tao. Do we really\nneed a learnable classifier at the end of deep neural network? arXiv e-prints, pp. arXiv–2203,\n2022.\nXueting Zhang, Debin Meng, Henry Gouk, and Timothy Hospedales. Shallow bayesian meta learning\nfor real-world few-shot recognition, 2021.\nYuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. What makes good examples for visual in-context\nlearning? arXiv preprint arXiv:2301.13670, 2023.\n13\n\n\nPublished as a conference paper at ICLR 2024\nA\nAPPENDIX\nA.1\nSUPPLEMENTARY THEORETICAL ANALYSIS\nWe offer additional insight into the theoretical analysis presented in Section 4 and provide the omitted\nremarks, properties, lemmas, and proofs.\nA.1.1\nEQUIANGULAR TIGHT FRAMES\nPapyan et al. (2020) coin the term Simplex Equianguar Tight Frame to describe a set of vectors\n{ϕj}d\nj=1 such that the minimum angle between any two pairs of vectors is maximized and all vectors\nhave equal norm. Formally,\nDefinition 3. Let Rd be a d−dimensional inner product space over R with the Euclidean inner\nproduct. A Simplex ETF is a set of d vectors {ϕj}d\nj=1, ϕj ∈Rd, specified by the columns of\nq\nd\nd−1(Id −1\nd11T )\nwhere Id ∈Rd×d is the identity matrix and 1 ∈Rd×1 is the ones vector. Somewhat contradictory, a\nSimplex Equiangular Tight Frame is not an Equiangular Tight Frame (Welch, 1974) as this set of\nvectors does not form a tight frame in Rd.\nDefinition 4. Let R be a d−dimensional space over R with the Euclidean inner product. An\nEquiangular Tight Frame (ETF) is a set of non-zero, equal norm vectors {ϕj}n\nj=1, n ≥d, that\nachieves the Welch lower bound:\nmax\nj̸=j′\n|⟨ϕj , ϕj′⟩|\n∥ϕj∥∥ϕj′∥=\ns\nn −d\nd(n −1)\nIt is well-known that a set of non-zero equal-norm vectors satisfies the Welch lower bound if and\nonly if that set of vectors is equiangular and also a tight frame for Rd (Fickus et al., 2018).\nDefinition 5. A set of non-zero, equal norm vectors {ϕj}n\nj=1 is equiangular if ∀j ̸= j′, |⟨ϕj , ϕj′⟩| =\nc for some c ∈R, c > 0.\nDefinition 6. {ϕj}n\nj=1 is a tight frame for Rd if, ∀v ∈Rd, ∃A > 0 such that A∥v∥2 =\nPn\nj=1 |⟨ϕj , v⟩|2.\nRemark 1. A Simplex Equiangular Tight Frame is not a tight frame.\nProof. Observe that for any finite d, for {ϕj}d\nj=1 equal to the columns of\nq\nd\nd−1(Id −1\nd11T ), it\nis the case that\nd−1\nP\nj=1\nϕj = −1 ∗ϕd. So {ϕj}n\nj=1 do not span Rd, and therefore, cannot be a tight\nframe.\nSimilarly, a Simplex ETF is not a d−simplex.\nRemark 2. A Simplex Equiangular Tight Frame is not a simplex.\nProof. A simplex in Rn requires n + 1 points.\nTo align terminology with properties, we generalize a Simplex ETF to an ELMES in Definition 1: a\nset of d vectors in a (d + k)-dimensional ambient space with k ≥0. Observe that a regular simplex\nis a special type of ETF in which the number of vectors in the set is one more than the dimension\nof the space that they span (Fickus et al., 2018). Building on this observation, an intuitive view of\nELMES is a regular d−simplex immersed in Rd+k.\nRemark 3. Consider a centered d-dimensional regular simplex with vertices {ϕj}d+1\nj=1, ϕj ∈Rd+1.\nLet ıcan be the canonical inclusion map: Rd →Rd+1, ıcan(x1, x2, ..., xd) = (x1, x2, ..., xd, 0d+1),\nthen {ıcan(ϕj)}d+1\nj=1 is an ELMES.\n14\n\n\nPublished as a conference paper at ICLR 2024\nx\n1.0\n0.5\n0.0\n0.5\n1.0\ny\n1.0\n0.5\n0.0\n0.5\n1.0\nz\n1.0\n0.5\n0.0\n0.5\n1.0\nFigure 3: A visualization of a d = 4 ELMES in R3. Observe the endpoints of the vectors of an ELMES lie on\nthe vertices of a centered regular tetrahedron.\nProof. The two criteria of an ELMES are maximally equiangular and equal length. As all vertices\nof a centered regular d−simplex are equal length from the origin, {ϕj}d+1\nj=1 are equal length and\ntherefore {ıcan(ϕj)}d+1\nj=1 must also have equal length.\nSimilarly, from Lemma 10 of Papyan et al. (2020), we know the cosine of the angle between any\ntwo vectors in a (d + 1)−dimensional ELMES is −1\nd . It is known that for a d−dimensional regular\nsimplex in Rd centered at the origin, the angle subtended by any two verticies through the origin is\ncos(θ) = −1\nd . Immersing {ϕj}d+1\nj=1, ϕj ∈Rd, into Rd+1 via the canonical inclusion operator ıcan\ndoes not change the pairwise angle between vectors in this set: ⟨ϕj , ϕj′⟩= ⟨ıcan(ϕj) , ıcan(ϕj′)⟩.\nAs {ıcan(ϕj)}d+1\nj=1 are equal length and maximally equiangular, it forms an ELMES.\nWe now show that an ELMES immersed in a higher dimension remains an ELMES. Taken with\nRemark 3, we can view a high-dimensional ELMES in Rd composed of n + 1 vectors {ϕj}n+1\nj=1 ,\nd >> n + 1, as simply a n−simplex immersed in Rd via the canonical inclusion operator.\nLemma 1. Let ıcan : Rd →Rd+k. If {ϕj}n\nj=1 is an ELMES , then {ıcan(ϕj)}d\nj=1 is an ELMES.\nProof. This reduces to proving that the maximum angle between a set of d equiangular points in Rd\nis the maximum angle between a set of d equiangular points in Rd+k. Let {ϕj}d\nj=1 be an ELMES\nsuch that ϕj ∈Rd and {ψj}d\nj=1 be an ELMES such that ψj ∈Rd+k. Then {ψj}d\nj=1 lie in a\nd−dimensional subspace of Rd+k: ∃γ1, ..., γd and basis vectors e1, ..., ed such that ∀ψj ∈{ψj}d\nj=1,\nψj = Pd\ni=1 γiei. Therefore, ∀j ̸= j′, ⟨ψj , ψj′⟩≤⟨ϕj , ϕj′⟩as {ϕj}d\nj=1 are an ELMES for\nRd.\nA.1.2\nELMES ROTATIONAL SYMMETRY\nThere are infinitely many ELMES by rotating one such set of vectors about the origin.\nRemark 4. Let {ϕj}d\nj=1 be an ELMES in Rd+k for some k ≥0. Let o : Rd+k →Rd+k be an\noperator from the special orthogonal group SO(d + k). Then {o(ϕj)}d\nj=1 is also an ELMES .\n15\n\n\nPublished as a conference paper at ICLR 2024\nProof. Length is preserved as operations in SO(d + k) have determinant 1 and angles are similarly\npreserved as operations in SO(d + k) are unitary (i.e. preserving inner product).\nA.1.3\nA SET OF ORTHONORMAL BASIS VECTORS IS NOT AN ELMES\nA final remark relates to the common misconception that a set of orthonormal basis vectors {ψj}d\nj=1\nis an ELMES. While {ψj}d\nj=1 is an ETF in Rd since this set realizes the Welch lower-bound\nin Definition 4, these vectors are not maximally equiangular: ⟨ψj , ψj′⟩= 0 >\n−1\nd−1.\nA.2\nELMES MAXIMIZES pψj(X = j)\nJustification of Assumption 1. This property is implied by symmetry in the assignment of class\nembeddings to support classes. As the assignment is arbitrary, all learnable ψi class detectors should\nhave equal probability of detecting their respective class. For simplicity of notation, we say ψi learns\nto detect class embedding ϕi rather another class embedding ϕk, k ̸= i.\nJustification of Assumption 2. Informally, this property states that, for any m-subset of classes\n{ϕj}m\nj=1, the probability of ψj detecting class j is equal to the probability of ψi detecting class\ni. This is again implied by symmetry in the assignment of class embeddings to support classes as\nmeta-learning algorithms may predict among a subset of m classes in the support set rather than the\nmaximum number of classes d.\nJustification of Assumption 3. Recall in Rd, ⟨ψ , ϕ⟩= ∥ψ∥∥ϕ∥cos(θ) where θ is the angle between\nψi and ϕi. Then this assumption constrains our set {ϕj}d\nj=1 so that relative norm of ϕi with respect\nto ϕj is lower bounded by cos(θi,j): ∥ϕi∥\n∥ϕj∥> cos(θi,j).\nInformally, the {ϕj}d\nj=1 are sufficiently spread out in the ambient space so that the learnable ψi that\nmaximizes pψi(X = i) is ϕi itself: ψi =\nϕi\n∥ϕi∥. This constraint helps us avoid degenerative cases like\n{ϕj}d\nj=1 all equal. For example, ϕj = αϕi, i ̸= j with α > 0 is one such degenerative case where\none class embedding vector is stacked on a different class embedding, but with higher norm.\nProof of Theorem 1. Taken with Assumption 1, Assumption 2, and Assumption 3, it suffices to show\nTheorem 2 and Lemma 4 to prove Theorem 1.\nTheorem 2. pψ1(X = 1) = pψ2(X = 2) = ... = pψd(X = d) ⇐⇒{ϕj}d\nj=1 are equiangular and\nequal norm.\nTo show the forward (⇒) direction, it suffices to first show pψ1(X = 1) = pψ2(X = 2) = ... =\npψd(X = d) ⇒{ϕj}d\nj=1 are equal norm and then show pψ1(X = 1) = pψ2(X = 2) = ... =\npψd(X = d) ⇒{ϕj}d\nj=1 are equiangular.\nLemma 2. pψ1 (X = 1) = pψ2 (X = 2) = ... = pψd(X = d) ⇒{ϕj}d\nj=1 are equal norm.\nProof. This implication holds when d = 2:\npψ1(X = 1) =\ne∥ϕ1∥\ne∥ϕ1∥+ e∥ϕ2∥cos(θ1,2) =\ne∥ϕ2∥\ne∥ϕ2∥+ e∥ϕ1∥cos(θ1,2) = pψ2(X = 2)\ne∥ϕ1∥(e∥ϕ2∥+ e∥ϕ1∥cos(θ1,2)) = e∥ϕ2∥(e∥ϕ1∥+ e∥ϕ2∥cos(θ1,2))\ne∥ϕ1∥+∥ϕ1∥cos(θ1,2) = e∥ϕ2∥+∥ϕ2∥cos(θ1,2)\n∥ϕ1∥(1 + cos(θ1,2)) = ∥ϕ2∥(1 + cos(θ1,2))\n∥ϕ1∥= ∥ϕ2∥\nSuppose d > 2 and pψ1(X = 1) = ... = pψd(X = d). By Assumption 2, all m−combinations\n\u0000 d\nm\n\u0001\nof {pψ1(X = 1), ..., pψd(X = d)} are equal. This implies all 2-combinations are equal:\npψi(X = i) = pψj(X = j) ⇒∥ϕi∥= ∥ϕj∥. Therefore, ∥ϕ1∥= ... = ∥ϕd∥.\n16\n\n\nPublished as a conference paper at ICLR 2024\nLemma 3. pψ1(X = 1) = pψ2(X = 2) = ... = pψd(X = d) ⇒{ϕj}d\nj=1 are equiangular.\nProof. This implication is trivially true when d = 2 (see the proof of Lemma 2), and we show it is\nsimilarly true when d = 3. Following the steps in the proof of Lemma 2, we arrive at the following 3\npairs of equalities:\n(1) e∥ϕ1∥(1+cos(θ1,2)) + e∥ϕ1∥+∥ϕ3∥cos(θ2,3) = e∥ϕ2∥(1+cos(θ1,2)) + e∥ϕ2∥+∥ϕ3∥cos(θ1,3)\n(2) e∥ϕ1∥(1+cos(θ1,3)) + e∥ϕ1∥+∥ϕ2∥cos(θ2,3) = e∥ϕ3∥(1+cos(θ1,3)) + e∥ϕ3∥+∥ϕ2∥cos(θ1,3)\n(3) e∥ϕ2∥(1+cos(θ2,3)) + e∥ϕ2∥+∥ϕ1∥cos(θ1,3) = e∥ϕ3∥(1+cos(θ2,3)) + e∥ϕ3∥+∥ϕ1∥cos(θ1,2)\nFrom Lemma 2, pψ1(X = 1) = pψ2(X = 2) = pψ3(X = 3) ⇒∥ϕ1∥= ∥ϕ2∥= ∥ϕ3∥, so the\nabove pairs of equalities reduce to:\n(1) cos(θ2,3) = cos(θ1,3)\n(2) cos(θ2,3) = cos(θ1,3)\n(3) cos(θ1,3) = cos(θ1,2)\nand when d = 3, {ϕj}3\nj=1 are equiangular.\nSuppose d > 3 and pψ1(X = 1) = ... = pψd(X = d). By Assumption 2, all m−combinations\n\u0000 d\nm\n\u0001\nof {pψ1(X = 1), ..., pψd(X = d)} are equal. This implies all 3-combinations are equal:\npψi(X = i) = pψj(X = j) = pψk(X = k) ⇒θi,j = θi,k = θj,k. Therefore, all angles are equal\nθi,j = θl,m for 1 ≤i, j, l, m ≤d.\nProof of Theorem 2. (⇒) Suppose pψ1(X = 1) = pψ2(X = 2) = ... = pψd(X = d).\nBy Lemma 2 and Lemma 3, pψ1(X = 1) = pψ2(X = 2) = ... = pψd(X = d) ⇒{ϕj}d\nj=1 are\nequiangular and equal norm.\n(⇐) Suppose {ϕj}d\nj=1 are equiangular and equal norm. Let ∥ϕ∥be the norm of any vector in our set\nand cos(θ) be the pairwise angle between any two vectors. Then\npψi(X = i) =\ne∥ϕ∥\ne∥ϕ∥+ (d −1)e∥ϕ∥cos(θ) = pψj(X = j)\nfor any 1 ≤i, j ≤d.\nLemma 4. For a set of equiangular and equal norm vectors, maximum equiangularity maximizes\nP\nj\npψj(X = j).\nProof. The maximum pairwise angle between two vectors in Rd is π, and from Theorem 2\npψi(X = i) = pψj(X = j) =\ne∥ϕ∥\ne∥ϕ∥+ (d −1)e∥ϕ∥cos(θ)\nfor all 1 ≤i, j ≤d. Increasing the angle θ decreases cos(θ). Decreasing cos(θ) only decreases\nthe denominator, which in turn, increases pψi(X = i). Therefore, maximizing the pairwise angle\nbetween all vectors maximizes pψi(X = i) for all 1 ≤i ≤d.\nA.2.1\nAN ELMES MINIMIZES Hψi(X)\nProof of Lemma 1. Equal norm and equiangular {ϕj}d\nj=1 are bounded in norm, and thus, the set of\nprobability distributions we obtain {pψi(1), pψi(2), ..., pψi(d)} belong to a capped simplex (Warmuth\n& Kuzmin, 2008) ∆d\nc = {p ∈∆| maxk pψi(k) ≤c} where c =\ne∥ϕ∥2\ne∥ϕ∥2+(d−1)e∥ϕ∥2 cos(θ) . Clearly,\namong such probability vectors, the minimum entropy is achieved at the boundary where cos(θ) is\nminimized, i.e., when the {ϕj}d\nj=1 are maximally equiangular.\n17\n\n\nPublished as a conference paper at ICLR 2024\nA.2.2\nAN ELMES MAINTAINS PERMUTATION INVARIANCE\nProof of Proposition 2. This follows from row-wise equivariance to permutations in matrix mul-\ntiplication. For any permutation π : [1, . . . , n] →[1, . . . , n] applied to the rows of Sn, we have\nπ(S)W = π(SW).\nB\nEXPERIMENTAL SETTINGS\nIn this section, we describe our experimental settings, and further, we direct readers interested in\nreproducing or using any of the methods we benchmark in this work to our released code. Unless\nstated otherwise, all universal meta-learning baselines use a CLIP feature extractor to encode images.\nLarge-Scale Pre-Training. All methods evaluated in the universal meta-learning setting adhere to\nthe same pre-training paradigm. For each large-scale image classification dataset, we reformulate the\nobjective from typical supervised image classification to both a 5-way-1-shot and a 5-way-5-shot\nepisodic prediction tasks. Within a dataset, examples from different classes are randomly sampled\nto compose a batch of episodes, and after exhausting iterating through every training example, this\nprocess is repeated with the next dataset. Iterating through each dataset in our set of ImageNet-1k,\nFungi, MSCOCO, and WikiArt then constitutes a single epoch of training.\nProtoNet and MetaOpt Implementations. For the ProtoNet and MetaOpt algorithms, we evaluate\ntwo settings. The first freezes the CLIP backbone and then applies the metric-learning objective—\ncosine distance for ProtoNet and SVM for MetaOpt—to classify the query image from the unmodified\nCLIP embeddings. The second emulates P>M>F Hu et al. (2022) by fine-tuning the CLIP backbone\nduring large-scale pre-training with the metric-learning objective function. During inference, the\nmetric-learning objective is applied to the fine-tuned CLIP embeddings to classify query images.\nMetaQDA Implementation. We follow the MetaQDA algorithm presented in Zhang et al. (2021).\nSpecifically, we freeze the CLIP feature extractor backbone and train the MetaQDA classifier during\nlarge-scale episodic pre-training.\nSNAIL Implementation. We use the architecture presented in Mishra et al. (2017) but with the\nhidden dimension of the Attention and Temporal Convolution Blocks adapted to CLIP embeddings\nrather than the ResNet embeddings used in the original implementation. As in this Mishra et al.\n(2017), we freeze the CLIP feature extractor and train the SNAIL model parameters during large-scale\npre-training.\nGPICL Implementation. We adapt the GPICL algorithm presented by Kirsch et al. (2022) for\nepisodic meta-training with an ELMES label encoder. Specifically, we represent image feature vectors\nas CLIP embeddings and the label embeddings with an ELMES. Following Kirsch et al. (2022), we\nform a sequence by concatening the current CLIP image embedding with the previous example’s\nELMES label embedding and add learnable positional embeddings so the model can use positional\ninformation of elements in the sequence to classify the query point in a causal-like fashion. We set the\nGeneral-Purpose In-Context Learning Transformer model to a ViT-Large (Dosovitskiy et al., 2020)\nwith leranable positional embeddings.\nCAML Implementation. The image encoder is set to CLIP and the label encoder is an ELMES.\nFor the non-causal sequence model, we use a ViT-Large as described in Table 1 of Dosovitskiy et al.\n(2020). This size is chosen as it has a hidden dimension of 1,024 and the CLIP output embedding\nvectors have hidden dimension of size 768. Choosing a non-causal sequence model with a large\nhidden dimension allows us to concatenate the label embedding to the CLIP embedding; in this case,\nthe label embedding is a 256 dimensional ELMES. In total, the implementation of CAML used for\nempirical evaluation has 302 million trainable parameters.\nOptimization Settings. Following the recommendation of training Vision Transformers (Steiner\net al., 2021) as well as standard practices, all universal meta-learning approaches use a cosine learning\nrate schedule with 9,600 warmup steps increasing linearly from 0 to 1e-5 followed by cosine decay\nto 1e−6 over the subsequent 360,000 steps. Given the size of our pre-training datasets, we do not\napply dropout, attention dropout, or weight decay regularization. We select a batch size of 525 so\nthe 5-way-1-shot episodes contain 520 query predictions and the 5-way-5-shot episodes contain 500\n18\n\n\nPublished as a conference paper at ICLR 2024\nTable 5: MiniImageNet & CIFAR-fs mean accuracy and standard error across 10,000 test epochs.\nMethod\nCIFAR-fs\nMiniImageNet\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nCAML [ELMES Class Embedding]\n70.8±.2\n85.5±.1\n96.2±.1\n98.6±.0\nCAML [Learnable Class Embedding]\n71.1±.2\n85.9±.1\n96.1±.1\n98.7±.0\nTable 6: CUB & tiered-ImageNet & Aircraft mean accuracy and standard error across 10,000 test\nepochs.\nMethod\nCUB\ntiered-ImageNet\nAircraft\n5w-1s\n5w-5s\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nCAML [ELMES Class Embedding]\n91.8±.2\n97.1±.1\n95.4±.1\n98.1±.1\n63.3±.3\n79.1±.2\nCAML [Learnable Class Embedding]\n91.8±.2\n97.1±.1\n95.3±.1\n98.3±.1\n66.3±.2\n80.6±.2\nTable 7: Pascal & Paintings mean accuracy and standard error across 10,000 test epochs.\nMethod\nPascal + Paintings\nPaintings\nPascal\n5w-1s\n5w-5s\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nCAML [ELMES Class Embedding]\n63.8±.2\n78.3±.1\n51.1±.2\n65.2±.1\n82.6±.2\n89.7±.1\nCAML [Learnable Class Embedding]\n63.1±.2\n78.0±.1\n51.3±.2\n65.0±.1\n82.1±.2\n89.7±.1\nTable 8: meta-iNat & tiered meta-iNat & ChestX mean accuracy and standard error across 10,000\ntest epochs.\nMethod\nmeta-iNat\ntiered meta-iNat\nChestX\n5w-1s\n5w-5s\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nCAML [ELMES Class Embedding]\n91.2±.2\n96.3±.1\n81.9±.2\n91.6±.1\n21.5±.1\n22.2±.1\nCAML [Learnable Class Embedding]\n91.4±.2\n96.4±.1\n82.1±.2\n91.8±.1\n21.5±.1\n22.6±.1\nquery predictions. Given the scale of the pre-training datasets—and the computation to train a single\nmodel—we do not conduct any hyperparameter tuning.\nP>M>F Meta-Training. We follow the settings used by Hu et al. (2022) to evaluate P>M>F.\nSpecifically, P>M>F uses a DINO (Caron et al., 2021) feature extractor rather than a CLIP feature\nextractor as the authors of P>M>F found a DINO feature extractor to be preferrable. We refer\nreaders Hu et al. (2022) for this comparison. For meta-training, we use the code released by Hu et al.\n(2022) and simply switch out the datasets to evaluate the In-Domain setting. Both the in-domain and\nuniversal meta-learning settings use the same test-set data; the difference is that P>M>F meta-trains\non each training dataset before evaluating on the testing dataset of each benchmark.\nC\nSUPPLEMENTARY ANALYSIS\nELMES Ablation. To supplement our theoretical analysis in Section 4, we train a version of CAML\nwith learnable class embedding vectors in place of the fixed ELMES encoder. Given our analysis\nin Section 4, it is perhaps unsurprising we find that—without any constraints or limitations—the\nclass embeddings converge to an ELMES. The average pair-wise angle between embedding vectors\nis 1.77 ± 0.02 radians whereas the expected pairwise angle from an ELMES is 1.82. Similarly, the\naverage norm of the learnable class embeddings converges to 1.34 ± 0.02 whereas the learned norm\nof the ELMES model is 1.32.\nAn evaluation comparing CAML with learnable class embeddings to the approach with a fixed\nELMES encoder is presented in Table 5, Table 6, Table 7, and Table 8 of the Appendix. In summary,\nthe performance is approximately the same on each benchmark with the exception of Aircraft. In this\ncase, the learnable embedding model significantly outperforms the ELMES model, and moreover,\nsurpasses all other universal meta-learning baselines on the 5-way-1-shot split with an accuracy of\n66.3 ± .2. Nevertheless, given the similarity between both approaches on the remaining 10 datasets,\n19\n\n\nPublished as a conference paper at ICLR 2024\nTable 9: MiniImageNet & CIFAR-fs mean accuracy and standard error across 10,000 test epochs. ◦\nindicates mean and standard error across 2,500 test epochs.\nMethod\nCIFAR-fs\nMiniImageNet\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nCAML (ResNet34)\n61.8 ± .2\n79.4 ± .2\n94.7 ± .1\n98.1 ± .0\nCAML (ViT-base)\n70.8±.2\n85.5±.1\n96.2±.1\n98.6±.0\nCAML (ViT-huge)◦\n83.3±.4\n93.5±.2\n98.6±.1\n99.6±.0\nTable 10: CUB & tiered-ImageNet & Aircraft mean accuracy and standard error across 10,000 test\nepochs. ◦indicates mean and standard error across 2,500 test epochs.\nMethod\nCUB\ntiered-ImageNet\nAircraft\n5w-1s\n5w-5s\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nCAML (ResNet34)\n75.4 ± .2\n88.3 ± .1\n96.1 ± .1\n98.5 ± .0\n45.1 ± .2\n58.7 ± .2\nCAML (ViT-base)\n91.8±.2\n97.1±.1\n95.4±.1\n98.1±.1\n63.3±.3\n79.1±.2\nCAML (ViT-huge)◦\n95.8±.2\n98.7±.1\n96.8±.2\n98.8±.1\n81.8±.4\n92.1±.3\nTable 11: Pascal & Paintings mean accuracy and standard error across 10,000 test epochs. ◦\nindicates mean and standard error across 2,500 test epochs.\nMethod\nPascal + Paintings\nPaintings\nPascal\n5w-1s\n5w-5s\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nCAML (ResNet34)\n57.5 ± .2\n71.0 ± .1\n46.1 ± .2\n57.3 ± .1\n77.4 ± .2\n86.8 ± .1\nCAML (ViT-base)\n63.8±.2\n78.3±.1\n51.1±.2\n65.2±.1\n82.6±.2\n89.7±.1\nCAML (ViT-huge)◦\n66.4±.4\n81.0±.2\n54.7±.3\n69.9±.2\n83.4±.4\n90.1±.3\nTable 12: meta-iNat & tiered meta-iNat & ChestX mean accuracy and standard error across 10,000\ntest epochs. ◦indicates mean and standard error across 2,500 test epochs.\nMethod\nmeta-iNat\ntiered meta-iNat\nChestX\n5w-1s\n5w-5s\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nCAML (ResNet34)\n82.4 ± .2\n91.4 ± .1\n72.3 ± .2\n84.6 ± .2\n21.8 ± .1\n23.6 ± .1\nCAML (ViT-base)\n91.2±.2\n96.3±.1\n81.9±.2\n91.6±.1\n21.5±.1\n22.2±.1\nCAML (ViT-huge)◦\n94.6±.3\n97.9±.1\n89.3±.4\n95.6±.2\n21.6±.2\n22.0±.2\nand the learnable class embeddings actually forming an ELMES, we attribute the difference in\nAircraft performance to stochasticity in training the model, suggesting that the fixed ELMES encoder\nis indeed optimal.\nImage Encoder Ablation. To evaluate how the performance of CAML is affected by the pre-trained\nimage encoder, we evaluate CAML with a ResNet-34 image encoder pre-trained on ImageNet-1k, a\nViT-base image encoder pre-trained with CLIP, and a ViT-huge image encoder that is pre-trained on\nLaion-2b (Schuhmann et al., 2022). We use the open source models released by Hugging Face in our\nevaluation.\nAs indicated in Table 9, Table 10, Table 11, and Table 12, the performance of CAML scales with the\nstrength of the feature extractor. Specifically, the performance with a ResNet-34 feature extractor\nis significantly worse than the performance with a CLIP ViT-base feature extractor, and in turn, the\nperformance with a CLIP ViT-base is significantly worse than the performance with a Laion-2b\nViT-huge feature extractor. However, its unclear what facet of the improved feature extractor is\nrelevant for CAML , especially on out-of-distribution tasks like Aircraft where the most benefit is\nseen. Moreover, it is unclear why there is no improvement on another out-of-distribution dataset,\nChestX.\n20\n\n\nPublished as a conference paper at ICLR 2024\nt-SNE Plots of Image Encoder Embeddings \nViT-b CLIP ChestX\nViT-b CLIP Aircraft\nViT-h Laion-2b ChestX\nViT-h Laion-2b Aircraft\nFigure 4: t-SNE projections of different image embeddings of various benchmark datasets with embeddings\ncolored class identity. We see ViT-huge trained with Laion-2b better separates the Aircraft dataset than does\nViT-base trained with CLIP. However, both image encoders are unable to separate ChestX.\nTo investigate this dimension, we visualize the image embeddings of both Aircraft and ChestX\nusing t-sne (Van der Maaten & Hinton, 2008) dimensionality reduction. Figure 4 visualizes these\nembeddings colored by class identity. We find the ViT-huge model pre-trained on Laion-2b better\nseparates the Aircraft dataset than the ViT-base model pre-trained using the CLIP objective; however,\nboth models do not reasonably separate ChestX. We postulate that an image encoder that can capture\nthe axes of variability among image embeddings is crucial for strong CAML performance, and the\nreason we observe significantly improved results on Aircraft but not ChestX when using a Laion-2b\nViT-h image encoder.\nTaken together, these results indicate CAML is modular: as foundational model feature extractors\ncontinue to improve, CAML will be able to capture these advances to improve its own performance.\nAssignment of Labels to Support Set Classes Analysis. Symmetry to the assignment of labels\nto support set classes is a desirable property of few-shot learning algorithms. For instance, the\npredictions for [(bear, 1), (tower, 2), (tree, 3)] should be the same if the labels are permuted to [(bear,\n3), (tower 1), (tree, 2)]. CAML is not invariant to permutations in the assignment of classes to support\nset examples as implied by eq. (1) in Section 4.2; however, we empirically find it is robust to them.\nLabel symmetry is distinct from the permutation invariance property of CAML that is discussed in\nSection 4.3. Tangibly for the sequence [(bear, 1), (tower, 2), (tree, 3)], permutation invariance ensures\nthe predictions are the same as if the order of demonstrations is permuted to [(tower, 2), (tree, 3),\n(bear, 1)].\nIn Figure 5(left), we visualize the histogram of the correct class probability for the example presented\nin Figure 2a after permuting the assignment of labels to support-set images for all 120 permutations\n21\n\n\nPublished as a conference paper at ICLR 2024\nHistogram Plots of Variability to Permutations in Label Assignment\nDistribution of P(Tower) after Permuting Label \nAssignment from Task in Figure 2a\nAverage Std. of Correct Class Probability after \nPermuting Label Assignment in mini-ImageNet\nFigure 5: (Left) histogram of the correct class probability for the example presented in Figure 2a after\npermuting the assignment of labels to support-set images for all 120 permutations of the 5-way-1-shot task.\n(Right) histogram of the average standard deviation of all 120 permutations of the 5-way-1-shot task for 1,000\nsamples from mini-ImageNet.\nof the 5-way-1-shot task. In Figure 5(right), we visualize the average standard deviation of all 120\npermutations of the 5-way-1-shot task for 1,000 samples from mini-ImageNet. The mean of this\nstatistic is 0.004±0.0004. Taken together, this indicates CAML is empirically robust to permutations\nin the assignment of labels to support set classes.\nD\nDISCUSSION\nWeaknesses of CAML. Despite its strong empirical performance, CAML presents several weaknesses.\nFirst, the maximum number of classes present in the support set at any point during inference must be\nknown at pre-training to instantiate a d-way ELMES. Further, at least one dataset during pre-training\nmust use a d-way classification setting so the ψi class detectors referenced in Section 4 are trained\nwithin the Transformer encoder’s attention layers.\nWhy does CAML not fine-tune the image encoder during pre-training? We do not fine-tune the\nimage encoder because it is not advantageous for universal meta-learning.\nOur goal is to develop a meta-learning algorithm that may function in a ChatGPT-like application; it\nshould be able to run in-context learning on any set of images. Foundational image models are trained\nfor exactly this purpose: they are pre-trained on billions of images to form a well-structured image\nembedding space that is robust to augmentations, occlusions, etc. Moreover, valuable characteristics\nsuch as the presence of objects, textures, etc. of an image are encoded into the structure of the\nembedding space so that the axes of variability among the embeddings encode variation in specific\nvisual attributes.\nFine-tuning the image encoder can corrupt this embedding space; especially since the datasets we\nuse for pre-training are orders of magnitude smaller than the ones used to train the Foundational\nmodel. This hypothesis is supported by our experiments with ProtoNet and MetaOpt in Tables 1 to 4.\nSpecifically, we find fine-tuning the backbone during pre-training leads to performance degradation\non many of our benchmarks when evaluated in the universal meta-learning setting.\n22\n"
}