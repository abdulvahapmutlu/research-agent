{
  "filename": "2003.00804v1.pdf",
  "num_pages": 9,
  "pages": [
    "Task Augmentation by Rotating for Meta-Learning\nJialin Liu\nCognitive Science Department\nXiamen University\nFujian, P. R. China 361005\njialin@stu.xmu.edu.cn\nFei Chao\nCognitive Science Department\nXiamen University\nFujian, P. R. China 361005\nfchao@xmu.edu.cn\nChih-Min Lin\nDepartment of Electrical Engineering\nYuan ze University\nChung-Li, Tao-Yuan 320, Taiwan\ncml@saturn.yzu.edu.tw\nAbstract\nData augmentation is one of the most effective ap-\nproaches for improving the accuracy of modern machine\nlearning models, and it is also indispensable to train a\ndeep model for meta-learning. In this paper, we introduce\na task augmentation method by rotating, which increases\nthe number of classes by rotating the original images 90,\n180 and 270 degrees, different from traditional augmenta-\ntion methods which increase the number of images. With\na larger amount of classes, we can sample more diverse\ntask instances during training. Therefore, task augmenta-\ntion by rotating allows us to train a deep network by meta-\nlearning methods with little over-ﬁtting. Experimental re-\nsults show that our approach is better than the rotation for\nincreasing the number of images and achieves state-of-the-\nart performance on miniImageNet, CIFAR-FS, and FC100\nfew-shot learning benchmarks. The code is available on\nwww.github.com/AceChuse/TaskLevelAug.\n1. Introduction\nAlthough the machine learning systems have achieved\na human-level ability in many ﬁelds with a large amount\nof data, learning from a few examples is still a challenge\nfor modern machine learning techniques. Recently, the ma-\nchine learning community has paid signiﬁcant attention to\nthis problem, where few-shot learning is the common task\nfor meta-learning (e.g., [20, 5, 30, 27]). The purpose of\nfew-shot learning is to learn to maximize generalization ac-\ncuracy across different tasks with few training examples. In\na classiﬁcation application of the few-shot learning, tasks\nare generated by sampling from a conventional classiﬁca-\ntion dataset; then, training samples are randomly selected\nfrom several classes in the classiﬁcation dataset. In addi-\ntion, a part of the examples is used as training examples and\ntesting examples. Thus, a tiny learning task is formed by\nthese examples. The meta-learning methods are applied to\ncontrol the learning process of a base learner, so as to cor-\nrectly classify on testing examples.\nData augmentation is widely used to improve the train-\ning of deep learning models. Usually, data augmentation is\nregarded as an explicit form of regularization [9, 26, 13].\nData augmentation aims at artiﬁcially generating the train-\ning data by using various translations on existing data, such\nas: adding noises, cropping, ﬂipping, rotation, translation,\netc. The general idea of data augmentations is increasing\nthe number of images by change data slightly to be differ-\nent from original data, but the data still can be recognized\nby human. The new images involved in the classes are iden-\ntical to the original data, we call this as Image Aug.\nHowever, the minimum units of meta-learning are tasks\nrather than data, so we should use rotation operation to aug-\nment the number of tasks, which is called as task augmen-\ntation (referred to Task Aug). Task Aug means increasing\nthe types of task instances by increasing the data that can\nbe clearly recognized as the different classes as the origi-\nnal data and associating them as the novel classes(we show\nexamples in Figure 1).\nThis is important for the meta-\nlearning, since meta-learning models require to predict un-\nseen classes during the testing phase, increasing the di-\nverseness of tasks would help models to generate to unseen\nclasses.\nIn experiments, we compared two cases, 1) the new im-\nages are converted to the classes of original images and 2)\n1\narXiv:2003.00804v1  [cs.CV]  8 Feb 2020\n",
    "the new images are associated to the novel classes with the\nmethod proposed in [3] on CIFAR-FS, FC100, miniIma-\ngeNet few-shot learning tasks, and showed the second case\ngot better results. Then the proposed method is evaluated\nby experiments with the state of art meta-learning meth-\nods [27, 15, 3] on CIFAR-FS, FC100, miniImageNet few-\nshot learning tasks, and compare with the results without the\ndata augmentation by rotating. In the comparative experi-\nments, Task Aug by rotating achieves the better accuracy\nthan the original meta-learning methods. Besides, the best\nresults of our experiments exceed the current state-of-art re-\nsult over a large margin.\n2. Related Work\nMeta-learning involves two hierarchies learning pro-\ncesses: low-level and high-level. The low-level learning\nprocess learns to deal with general tasks, often termed\nas the “inner loop”; and the high-level learning process\nlearns to improve the performance of a low-level task, of-\nten termed as the “outer loop”.\nSince models are re-\nquired to handle sensory data like images, deep learning\nmethods are often applied for the “outer loop”. However,\nthe machine learning methods applied for the “inner loop”\nare very diverse. Based on different methods in the “in-\nner loop”, meta-learning can be applied in image recogni-\ntion [4, 24, 5, 30, 20], image generation [2, 31, 21], rein-\nforce learning [5, 1], and etc. This work focuses on few-shot\nlearning image recognition based on meta-learning. There-\nfore, in the experiment, the methods applied in the “inner\nloop” are able to classify data, and they are K-nearest neigh-\nbor (KNN), Support Vector Machine (SVM) and ridge re-\ngression, respectively [27, 15, 3].\nPrevious studies have introduced many popular regular-\nization techniques to few-shot learning from deep learning,\nsuch as weight decay, dropout, label smooth [3], and data\naugmentation. Common data augmentation techniques for\nimage recognition are usually designed manually and the\nbest augmentation strategies depend on dataset. In natural\ncolor image datasets, random cropping and random hori-\nzontal ﬂipping are the most common. Since the few-shot\nlearning tasks consist of natural color images, the random\nhorizontal ﬂipping and random cropping are applied in few-\nshot learning. In addition, color (brightness, contrast, and\nsaturation) jitter is often applied in the works of few-shot\nlearning [7, 19].\nOther data augmentation technologies related to few-\nshot learning include generating samples by few-shot learn-\ning and generating samples for few-shot learning. The for-\nmer tried to synthesize additional examples via transfer-\nring, extracting, and encoding to create the data of the new\nclasses, that are intra-class relationships between pairs of\nreference classes’ data instances [8, 25].\nThe later tried\nto apply meta-learning in a few-shot generation to gener-\nate samples from other models [2].In addition to these two\ntypes of studies, the data augmentation technology most\nclosed to the new proposed approach is applied to Omniglot\ndataset, which consists of handwritten words [14]. They\ncreated the novel classes by rotating the original images 90,\n180 and 270 degrees [24]. However, when this approach\nis applied for the natural color image, it would be slightly\nchanged, and we will explain this in Section 3.\n3. Method\n3.1. Problem Deﬁnition\nWe adopt the formulation purposed by [30] to describe\nthe N-way K-shot task. A few-shot task contains many\ntask instances (denoted by Ti), each instance is a classiﬁca-\ntion problem consisting of the data sampled from N classes.\nThe classes are randomly selected from a classes set. The\nclasses set are split into M tr, M val and M test for a train-\ning class set Ctr, a validation classes set Cval, and a test\nclasses set Ctest. In particular, each class does not over-\nlap others (i.e., the classes used during testing are unseen\nclasses during training). Data is randomly sampled from\nCtr, Cval and Ctest, so as to create task instances for train-\ning meta-set Str, validation meta-set Sval, and test meta-set\nStest, respectively. The validation and testing meta-sets are\nused for model selection and ﬁnal evaluation, respectively.\nThe data in each task instance, Ti, are divided into train-\ning examples Dtr and validation examples Dval. Both of\nthem only contains the data from N classes which sam-\npled from the appropriate classes set randomly (for a task\ninstance applied during training, the classes form a subset\nof the training classes set Ctr). In most settings, the training\nset Dtr = {(xk\nn, yk\nn)|n = 1 . . . N; k = 1 . . . K} consists\nof K data instances from each class, this processing usu-\nally called as a “shot”. The validation set, Dval, consists\nof several other data instances from the same classes, this\nprocessing is usually called as a “query”. An evaluation is\nprovided for generalization performance on the N classiﬁ-\ncation task instance Dtr. Note that: the validation set of\na task instance Dval (for optimizing model during “outer\nloop”) is different from the held-out validation classes set\nCval and meta-set Sval (for model selection).\n3.2. Task Augmentation by Rotating\nThis work is to increase the size of the training classes\nset, M tr, by rotating all images within the training classes\nset with 90, 180, 270 degrees. The size, M tr, is increased\nfor three times. In the Omniglot dataset consisting of hand-\nwritten words [24], this approach works well, since it can\nrotate a handwritten word multiple of 90 degrees and treat\nthe new one as another word; in addition, it is really possi-\nble that the novel word is similar to some words, which are\nnot included in the training classes but existed.\n",
    "Original classes\nNovel classes 2\nOriginal classes\nNovel classes 1\nNovel classes 3\nNovel classes 2\nNovel classes 1\nNovel classes 3\nFigure 1: Examples of the novel created classes.\nAlgorithm 1 Task Augmentation by Rotating.\nRequire: Classes set C = {c1, c2, . . . , cM}; Max possibility for\nTask Aug pmax; The delay to Task Aug T; The current count\nt; The number of ways, shots and queries N, K, H\n1: t ←t + 1\n2: p ←pmax ∗min{1, t\nT}\n3: n ∼Binomial(N, p)\n4: Dtr, Dval ←{}, {}\n5: V ←Sample N −n from {1, 2, · · · , M}\n6: for all v ∈V do\n7:\nD ←Sample K + H from cv\n8:\nDtr ←Dtr∪First K of D\n9:\nDval ←Dval∪Last H of D\n10: end for\n11: U ←Sample n from {M, M + 1, · · · , 4M}\n12: for all u ∈U do\n13:\nv ←(u mod M) + 1\n14:\nD ←Sample K + H from cv\n15:\nr ←⌊u\nM ⌋\n16:\nRotate all x ∈{x|(x, y) ∈D} 90r degrees\n17:\nDtr ←Dtr∪First K of D\n18:\nDval ←Dval∪Last H of D\n19: end for\n20: return (Dtr, Dval)\nc1\nc2\ncM\np\n1-p\npmax∗\nmin{1, t\nT }\nN-n from original classes\nn from novel classes\nDtr\nDval\nFigure 2: The process of generating a task instance with\nTask Aug by rotating.\nFor natural images, it is obvious that the images gener-\nated by rotating is real enough. But should the new gener-\nated images be classiﬁed as the novel classes or the original\nclasses? It dependents on the similarity between the new\nimages and the original classes. If the most of they are sim-\nilar enough, the new images should be classiﬁed as the orig-\ninal classes, and vice versa. This logic shows that one of the\ntwo methods must be effective. Since there are almost not\nworks merge the new images into the original classes which\nworked well, we assume that Task Aug by rotating is effec-\ntive for meta-learning, and we will compare two methods in\nexperiments.\nBesides, it is different from in handwritten that we assign\nthe new data smaller weights than the original data, so as to\nmake models prioritize learning the features of the original\nclasses, since the images generated by rotating rarely exist\nin the original data. This way makes the features of the\nnovel classes as a supplement to prevent the augmented data\n",
    "from taking up large capacity in the model, which is same\nas other common data augment methods.\nThe smaller weights are implemented in two ways, 1)\nlower probability and 2) delaying the probability of select-\ning the novel classes. For a class in a task instance, the\nprobability of the class coming from the novel classes is p,\nand the probability coming from the original classes is 1−p.\nBesides, the initial p is set to 0, then linearly rises from 0 to\npmax for the ﬁrst T tasks. The max probability pmax is set\nlower than the proportion of the novel classes in all classes\nto make each novel class have a lower probability than each\noriginal class. The whole process of Task Aug on a classes\nset is summarized in Algorithm 1 and Figure 2.\n3.3. Ensemble\nIn this work, we also compare the methods with the\ntraining protocol with ensemble method [11] in addition to\nthe standard training protocol, which choosing a model by\nthe validation set. The training protocol with an ensemble\nmethod use the models with different training epoch to an\nensemble model, in order to better use the models obtained\nin a single training process, and this approach has been\nproved to be valid for meta-learning by experiments [16].\nWe adopt this ensemble method. However, unlike [11] and\n[16] that we did not use cyclic annealing for learning rate\nand any methods to select models. We directly took the\naverage of the prediction of all models, which are saved ac-\ncording to an interval of 1 epoch. In Section 4, the methods\nwith this ensemble approach are marked by “+ens”.\n4. Experiments\nWe evaluate the proposed method on few-shot learning\ntasks. In order to ensure fair, both the results of baseline and\nTask Aug were run in our own environment. The compar-\native experiment is designed to answer the following ques-\ntions: (1) Image Aug and Task Aug by rotating which is\nable to improve the performance of meta-learning? (2) How\nmuch should the probably for the novel classes be set? (3)\nIs Task Aug by rotating able to improve the performance of\nthe current popular meta-learning methods?\n4.1. Experimental Conﬁguration\n4.1.1\nBackbone\nFollowing [15, 18, 17], we used ResNet-12 network in our\nexperiments.\nThe ResNet-12 network had four residual\nblocks which contains three 3 × 3 convolution, batch nor-\nmalization and Leaky ReLU with 0.1 negative slope. One\n2 × 2 max-pooling layer is used for reducing the size of the\nfeature map. The numbers of the network channels were\n64, 160, 320 and 640, respectively. DropBlock regulariza-\ntion [6] is used in the last two residual blocks, the conven-\ntional dropout [10] is used in the ﬁrst two residual blocks.\nThe block sizes of DropBlock were set to 2 and 5 for CI-\nFAR derivatives and ImageNet derivatives, respectively. In\nall experiments, the dropout possibility was set to 0.1. The\nglobal average pooling was not used for the ﬁnal output of\nthe last residual block.\n4.1.2\nBase Learners\nWe used ProtoNets [27], MetaOptNet-SVM [15] (we write\nit as M-SVM) and Ridge Regression Differentiable Dis-\ncriminator (R2-D2) [3] as basic methods to verify the ef-\nfective of Task Aug.\nFor ProtoNets, we did not use a higher way for training\nthan testing like [27]. Instead, the equal number of shot\nand way were used in both training and evaluation, and its\noutput multiplied by a learnable scale before the softmax\nfollowing [18, 15].\nFor M-SVM, we set training shot to 5 for CIFAR-FS;\n15 for FC100; and 15 for miniImageNet; regularization pa-\nrameter of SVM was set to 0.1; and a learnable scale was\nused following [15]. We did not use label smoothing like\n[15], because we did not ﬁnd that label smoothing can im-\nprove the performance in our environment. This was also\nafﬁrmed from the [15] author’s message on GitHub, that\nProgram language packages and environment might affect\nresults of the meta-learning method.\nFor R2-D2, we set the same training shot as for M-SVM,\nand used a learnable scale and bias following [3]. It was dif-\nferent from [3] we used a ﬁxed regularization parameter of\nridge regression which was set to 50 because [3] has con-\nﬁrmed that making it learnable might not be helpful.\nLast, for all methods, each class in a task instance con-\ntained 6 test (query) examples during training and 15 test\n(query) examples during testing.\n4.1.3\nTraining Conﬁguration\nStochastic gradient descent (SGD) was used.\nFollowing\n[29], we set weight decay and Nesterov momentum to\n0.0005 and 0.9, respectively. Each mini-batch contained 8\ntask instances. The meta-learning model was trained for\n60 epochs, and 1000 mini-batchs for each epoch. We set\nthe initial learning rate to 0.1, then multiplied it by 0.06,\n0.012, and 0.0024 at epochs 20, 40 and 50, respectively,\nas in [7]. The results, which are marked by “+ens” were\nused the 60 models saved after each epoch to become an\nensemble model. For the ﬁnal training, the training classes\nset was augmented by the validation classes set. When we\nonly chose one model, we will chose the model at the epoch\nwhere we got the best model during training on the train-\ning classes set. The results of the ﬁnal run are marked by\n“+val” in this subsection. Since the base idea of “+ens” was\nproposed by other works and “+val” is popular for meta-\nlearning, we do not explain more details about them.\n",
    "0.00\n0.25\n0.50\n0.75\nThe max probability of rotation\n80.0\n82.5\n85.0\n87.5\nAccuracy (%)\nCIFAR-FS 5-way 5-shot\nBaseline 5-shot\nImage Aug 5-shot\nTask Aug 5-shot\n0.00\n0.25\n0.50\n0.75\nThe max probability of rotation\n54\n56\n58\nAccuracy (%)\nFC100 5-way 5-shot\n0.00\n0.25\n0.50\n0.75\nThe max probability of rotation\n76\n78\n80\nAccuracy (%)\nminiImageNet 5-way 5-shot\n0.00\n0.25\n0.50\n0.75\nThe max probability of rotation\n70\n75\nAccuracy (%)\nCIFAR-FS 5-way 1-shot\nBaseline 1-shot\nImage Aug 1-shot\nTask Aug 1-shot\n0.00\n0.25\n0.50\n0.75\nThe max probability of rotation\n37.5\n40.0\n42.5\nAccuracy (%)\nFC100 5-way 1-shot\n0.00\n0.25\n0.50\n0.75\nThe max probability of rotation\n60.0\n62.5\nAccuracy (%)\nminiImageNet 5-way 1-shot\n0.00\n0.25\n0.50\n0.75\nThe max probability of rotation\n85.0\n87.5\nAccuracy (%)\nCIFAR-FS 5-way 5-shot with ensemble\n0.00\n0.25\n0.50\n0.75\nThe max probability of rotation\n56\n58\n60\nAccuracy (%)\nFC100 5-way 5-shot with ensemble\n0.00\n0.25\n0.50\n0.75\nThe max probability of rotation\n78\n80\n82\nAccuracy (%)\nminiImageNet 5-way 5-shot with ensemble\n0.00\n0.25\n0.50\n0.75\nThe max probability of rotation\n72.5\n75.0\n77.5\nAccuracy (%)\nCIFAR-FS 5-way 1-shot with ensemble\n0.00\n0.25\n0.50\n0.75\nThe max probability of rotation\n42\n44\n46\nAccuracy (%)\nFC100 5-way 1-shot with ensemble\n0.00\n0.25\n0.50\n0.75\nThe max probability of rotation\n62\n64\n66\nAccuracy (%)\nminiImageNet 5-way 1-shot with ensemble\nFigure 3: The accuracies (%) on meta-test sets with varying probability pmax for the novel classes.The 95% conﬁdence\ninterval is denoted by the shaded region.\nFor data augmentation, we adopted random crop, hori-\nzontal ﬂip, and color (brightness, saturation, and contrast)\njitter data augmentation following the work of [7, 19]. In\nthe experiments of comparing Task Aug and Image Aug by\nrotating, R2-D2 was applied, and we set T to 80000. In the\nevaluation of Task Aug for ProtoNets and M-SVM, we set\npmax to the value getting the best results for R2-D2.\n4.1.4\nDataset\nThe CIFAR-FS [3] containing all 100 classes from CIFAR-\n100 [12] is proposed as few-shot classiﬁcation benchmark\nrecently. These classes are randomly divided into training\nclasses, validation classes and test classes. The three types\ncontain 64, 16 and 20 classes, respectively. There are 600\nnature color images of size 32 × 32 in each class.\nThe FC100 [18] are also derived from CIFAR-100 [12],\nand the 100 classes are grouped into 20 superclasses. The\ntraining, validation, and testing classes contain 60 classes\nfrom 12 superclasses, 20 classes from 4 superclasses, and\n20 classes from 4 superclasses, respectively.\nThe target\nis to minimize the information overlap between classes to\nmake it more challenging than current few-shot classiﬁca-\ntion tasks. Same as CIFAR-FS, there are 600 nature color\nimages of size 32 × 32 in each class.\nThe miniImageNet [30] is one of the most popular\nbenchmark for few-shot classiﬁcation, which contains 100\nclasses randomly selected from ILSVRC-2012 [22]. The\nclasses are randomly divided into training classes, valida-\ntion classes and test classes, and them contain 64, 16 and\n20 classes, respectively. There are 600 nature color images\nof size 84 × 84 in each class. Since [30] did not release\nthe class splits, we use the more common split proposed by\n[20].\n4.2. Comparison between Task Aug and Image Aug\nTo prove our assumption that rotation multi 90 degrees\nfor Task Aug is better than that for Image Aug, we draw\nthe accuracy curves depending on pmax for both Task Aug\nand Image Aug (curves showed in Figure 3). The linear\nrising of p was also used for Image Aug, and T = 80000\nfor both Task Aug and Image Aug. In all the results showed\nin Figure 3, the training classes set was not augmented by\nthe validation classes set.\nAs shown in Figure 3, the performance of Task Aug on\nmost of the regimes is better than Image Aug and baseline\nin general. Besides, we observed that: with the increase\nof pmax, the accuracy rises at ﬁrst, reaches the peaks be-\ntween 0.25 and 0.5, then declines and reaches baseline when\npmax = 0.75 at the end, which is the proportion of the novel\nclasses in all classes. The accuracy of Task Aug on CIFAR-\nFS, FC100 and miniImagNet reach the peaks at 0.5, 0.25\nand 0.25 respectively. At the same time, the rotation multi\n90 degrees for Image Aug cannot improve or even cause\n",
    "Table 1: Comparison to the average accuracies (%) with\n95% conﬁdence intervals between the methods with and\nwithout Task Aug on CIFAR-FS 5-way 1-shot.\nMethod\nBaseline\nTask Aug\nProtoNets [27]\n71.88±0.52\n74.15±0.50\nProtoNets (+ens)\n73.95±0.51\n75.89±0.48\nProtoNets (+val)\n73.20±0.51\n75.10±0.49\nProtoNets (+ens+val)\n76.05±0.49\n77.28±0.47\nM-SVM [15]\n71.52±0.51\n72.95±0.48\nM-SVM (+ens)\n74.12±0.50\n75.85±0.47\nM-SVM (+val)\n72.42±0.50\n73.13±0.47\nM-SVM (+ens+val)\n75.91±0.48\n76.75±0.46\nR2-D2 [3]\n72.27±0.51\n74.42±0.48\nR2-D2 (+ens)\n75.06±0.50\n76.51±0.47\nR2-D2 (+val)\n73.52±0.50\n76.02±0.47\nR2-D2 (+ens+val)\n76.40±0.49\n77.66±0.46\nworse performance.\n4.3. Evaluation of Task Aug\nIn order to further prove the proposed approach can im-\nprove the performance of meta-learning, we evaluate Task\nAug by rotating on several meta-learning methods in this\nsection.\nWe choose several currently the state of art base learn-\ners for experiments, we detail in Section 4.1.2. Besides, the\ntraining protocol with ensemble method can get better re-\nsults than the standard training protocol, we involve it in the\nexperiments. We think this is important, because the pro-\nposed method can only be a contribution if it can further\nimprove performance based on the best method available at\npresent.\nResults. Table 1-6 show the average accuracies (%) with\n95% conﬁdence intervals of the methods with and without\nTask Aug, and the best results are highlighted.\nThe ta-\nbles show that the proposed method can improve the per-\nformance in most of cases.\nWe can observe that: some results without the ensem-\nble approach [11] of baseline and Task Aug are close, but\nthe advantage of Task Aug is still obvious on the compari-\nson results with the ensemble approach. We suspect that the\nscale of backbone limits the performance of the best model.\nA larger scale backbone is needed for the training process\nwith Task Aug. For the results of ensemble approach, since\nTask Aug reduces the over-ﬁtting, more models during the\ntraining process have good performance, which provide en-\nsemble with models of higher quality.\nLast we compare the results of this work with the results\nproposed by the prior works, in order to show how much\nTable 2: Comparison to the average accuracies (%) with\n95% conﬁdence intervals between the methods with and\nwithout Task Aug on CIFAR-FS 5-way 5-shot.\nMethod\nBaseline\nTask Aug\nProtoNets [27]\n84.14±0.36\n85.37±0.35\nProtoNets (+ens)\n85.72±0.35\n87.33±0.33\nProtoNets (+val)\n85.29±0.35\n86.53±0.34\nProtoNets (+ens+val)\n86.88±0.34\n88.24±0.33\nM-SVM [15]\n84.01±0.36\n85.91±0.36\nM-SVM (+ens)\n85.85±0.34\n87.73±0.33\nM-SVM (+val)\n84.94±0.36\n86.94±0.34\nM-SVM (+ens+val)\n87.15±0.34\n88.38±0.33\nR2-D2 [3]\n84.60±0.36\n86.02±0.35\nR2-D2 (+ens)\n86.11±0.34\n87.63±0.34\nR2-D2 (+val)\n85.39±0.36\n86.73±0.34\nR2-D2 (+ens+val)\n87.04±0.34\n88.33±0.33\nTable 3: Comparison to the average accuracies (%) with\n95% conﬁdence intervals between the methods with and\nwithout Task Aug on FC100 5-way 1-shot.\nMethod\nBaseline\nTask Aug\nProtoNets [27]\n37.53±0.40\n38.89±0.40\nProtoNets (+ens)\n40.04±0.41\n42.00±0.43\nProtoNets (+val)\n43.63±0.43\n44.91±0.46\nProtoNets (+ens+val)\n47.16±0.46\n48.91±0.47\nM-SVM [15]\n40.50±0.39\n41.17±0.40\nM-SVM (+ens)\n43.24±0.42\n44.38±0.42\nM-SVM (+val)\n46.72±0.45\n47.39±0.44\nM-SVM (+ens+val)\n49.50±0.46\n49.77±0.45\nR2-D2 [3]\n40.66±0.41\n41.47±0.40\nR2-D2 (+ens)\n43.27±0.42\n44.75±0.43\nR2-D2 (+val)\n47.12±0.44\n48.21±0.45\nR2-D2 (+ens+val)\n49.92±0.45\n51.35±0.46\nthis work raises the baselines after combining several prior\nmethods and the proposed method, and they are showed in\nTable 7, 8 and 9. The tables show that the highest accu-\nracies of our experiments exceeded the current state-of-art\naccuracies 2% to 5%.\n5. Conclusion\nWe proposed a Task Level Data Augmentation (Task\nAug), a data augmentation technique that increased the\nnumber of training classes to provide more diverse few-\nshow task instances for meta-learning. We proved that Task\nAug was valid for CIFAR-FS, FC100, and miniImageNet,\n",
    "Table 4: Comparison to the average accuracies (%) with\n95% conﬁdence intervals between the methods with and\nwithout Task Aug on FC100 5-way 5-shot.\nMethod\nBaseline\nTask Aug\nProtoNets [27]\n51.43±0.39\n53.92±0.39\nProtoNets (+ens)\n54.24±0.40\n56.55±0.40\nProtoNets (+val)\n61.16±0.42\n60.86±0.41\nProtoNets (+ens+val)\n63.64±0.43\n65.47±0.42\nM-SVM [15]\n54.83±0.40\n56.23±0.40\nM-SVM (+ens)\n58.49±0.41\n60.14±0.41\nM-SVM (+val)\n62.99±0.42\n63.64±0.42\nM-SVM (+ens+val)\n66.37±0.42\n67.17±0.41\nR2-D2 [3]\n55.85±0.39\n56.29±0.40\nR2-D2 (+ens)\n58.01±0.40\n59.94±0.41\nR2-D2 (+val)\n63.32±0.40\n64.53±0.42\nR2-D2 (+ens+val)\n65.58±0.42\n67.66±0.42\nTable 5: Comparison to the average accuracies (%) with\n95% conﬁdence intervals between the methods with and\nwithout Task Aug on miniImageNet 5-way 1-shot.\nMethod\nBaseline\nTask Aug\nProtoNets [27]\n58.67±0.48\n60.52±0.48\nProtoNets (+ens)\n62.12±0.48\n63.69±0.47\nProtoNets (+val)\n60.13±0.48\n62.22±0.49\nProtoNets (+ens+val)\n63.84±0.48\n65.04±0.48\nM-SVM [15]\n60.02±0.45\n62.12±0.44\nM-SVM (+ens)\n63.44±0.45\n64.56±0.44\nM-SVM (+val)\n61.58±0.45\n63.14±0.45\nM-SVM (+ens+val)\n64.74±0.45\n65.38±0.45\nR2-D2 [3]\n60.57±0.44\n62.32±0.45\nR2-D2 (+ens)\n63.72±0.44\n64.79±0.45\nR2-D2 (+val)\n62.82±0.45\n62.64±0.44\nR2-D2 (+ens+val)\n65.50±0.45\n65.95±0.45\nand exceeded the result of the previous works. Task Aug\nachieved the performance by rotating the images 90, 180\nand 270 degrees. This method is simple and cost-effective.\nWith the ensemble method, we exceeded the state-of-the-art\nresult over a large margin.\nFuture work will focus on searching different network\nstructures for meta-learning, since the training with Task\nAug would require larger model. Besides, we will try to\napply Task Aug to other few-shot learning tasks to verify\nits effectiveness. Another interesting topic is to build other\napproaches for Task Aug, such as swapping channel order,\npicture blend or even auto augmentation.\nTable 6: Comparison to the average accuracies (%) with\n95% conﬁdence intervals between the methods with and\nwithout Task Aug on miniImageNet 5-way 5-shot.\nMethod\nBaseline\nTask Aug\nProtoNets [27]\n75.24±0.37\n77.00±0.36\nProtoNets (+ens)\n78.11±0.34\n79.77±0.34\nProtoNets (+val)\n76.98±0.36\n77.59±0.37\nProtoNets (+ens+val)\n79.54±0.35\n80.60±0.34\nM-SVM [15]\n77.85±0.34\n78.90±0.34\nM-SVM (+ens)\n80.18±0.32\n81.35±0.32\nM-SVM (+val)\n78.65±0.34\n79.97±0.33\nM-SVM (+ens+val)\n81.39±0.32\n82.13±0.31\nR2-D2 [3]\n77.44±0.34\n78.81±0.34\nR2-D2 (+ens)\n79.90±0.33\n81.08±0.32\nR2-D2 (+val)\n78.61±0.35\n79.58±0.33\nR2-D2 (+ens+val)\n81.34±0.32\n81.96±0.32\nTable 7:\nThe average accuracies (%) with 95% conﬁ-\ndence intervals on CIFAR-FS. ∗CIFAR-FS results from [3].\n†Result from [15].\nMethod\n1-shot\n5-shot\nMAML∗[5]\n58.9±1.9\n71.5±1.0\nR2-D2 [3]\n65.3±0.2\n79.4±0.1\nProtoNets† [27]\n72.2±0.7\n83.5±0.5\nM-SVM [15]\n72.8±0.7\n85.0±0.5\nM-SVM (best) (our)\n76.75±0.46\n88.38±0.33\nR2-D2 (best) (our)\n77.66±0.46\n88.33±0.33\nTable 8: The average accuracies (%) with 95% conﬁdence\nintervals on FC100. †FC100 result from [15].\nMethod\n1-shot\n5-shot\nTADAM [18]\n40.1±0.4\n56.1±0.4\nProtoNets† [27]\n37.5±0.6\n52.5±0.6\nMTL [28]\n45.1±1.8\n57.6±0.9\nM-SVM [15]\n47.2±0.6\n62.5±0.6\nM-SVM (best) (our)\n49.77±0.45\n67.17±0.41\nR2-D2 (best) (our)\n51.35±0.46\n67.66±0.42\nReferences\n[1] Maruan Al-Shedivat,\nTrapit Bansal,\nYuri Burda,\nIlya\nSutskever, Igor Mordatch, and Pieter Abbeel. Continuous\nadaptation via meta-learning in nonstationary and competi-\ntive environments. arXiv preprint arXiv:1710.03641, 2017.\n",
    "Table 9: The average accuracies (%) with 95% conﬁdence\nintervals on miniImageNet. ∗Result from [15]. Here only\nlist the best results of previous works due to the shortage of\nspace.\nMethod\n1-shot\n5-shot\n[7]\n56.20±0.86\n73.00±0.64\nTADAM [18]\n58.50±0.30\n76.70±0.30\nLEO [23]\n61.76±0.08\n77.59±0.12\nProtoNets∗[27]\n59.25±0.64\n75.60±0.48\nM-SVM [15]\n64.09±0.62\n80.00±0.45\nM-SVM (best) (our)\n65.38±0.45\n82.13±0.31\nR2-D2 (best) (our)\n65.95±0.45\n81.96±0.32\n[2] Antreas Antoniou, Amos Storkey, and Harrison Edwards.\nData augmentation generative adversarial networks. arXiv\npreprint arXiv:1711.04340, 2017.\n[3] Luca Bertinetto, Joao F Henriques, Philip HS Torr, and An-\ndrea Vedaldi. Meta-learning with differentiable closed-form\nsolvers. arXiv preprint arXiv:1805.08136, 2018.\n[4] Li Fei-Fei, Rob Fergus, and Pietro Perona. One-shot learning\nof object categories. IEEE transactions on pattern analysis\nand machine intelligence, 28(4):594–611, 2006.\n[5] Chelsea Finn, Pieter Abbeel, and Sergey Levine.\nModel-\nagnostic meta-learning for fast adaptation of deep networks.\nIn Proceedings of the 34th International Conference on Ma-\nchine Learning-Volume 70, pages 1126–1135. JMLR. org,\n2017.\n[6] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Dropblock:\nA regularization method for convolutional networks.\nIn\nAdvances in Neural Information Processing Systems, pages\n10727–10737, 2018.\n[7] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot\nvisual learning without forgetting.\nIn Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 4367–4375, 2018.\n[8] Bharath Hariharan and Ross Girshick.\nLow-shot visual\nrecognition by shrinking and hallucinating features. In Pro-\nceedings of the IEEE International Conference on Computer\nVision, pages 3018–3027, 2017.\n[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016.\n[10] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya\nSutskever, and Ruslan R Salakhutdinov. Improving neural\nnetworks by preventing co-adaptation of feature detectors.\narXiv preprint arXiv:1207.0580, 2012.\n[11] Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E\nHopcroft, and Kilian Q Weinberger. Snapshot ensembles:\nTrain 1, get m for free. arXiv preprint arXiv:1704.00109,\n2017.\n[12] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10\n(canadian institute for advanced research). URL http://www.\ncs. toronto. edu/kriz/cifar. html, 8, 2010.\n[13] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classiﬁcation with deep convolutional neural net-\nworks. In Advances in neural information processing sys-\ntems, pages 1097–1105, 2012.\n[14] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B\nTenenbaum. Human-level concept learning through proba-\nbilistic program induction. Science, 350(6266):1332–1338,\n2015.\n[15] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and\nStefano Soatto. Meta-learning with differentiable convex op-\ntimization. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 10657–10665,\n2019.\n[16] Jinchao Liu, Stuart J Gibson, and Margarita Osadchy. Learn-\ning to support: Exploiting structure information in support\nsets for one-shot learning. arXiv preprint arXiv:1808.07270,\n2018.\n[17] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter\nAbbeel.\nA simple neural attentive meta-learner.\narXiv\npreprint arXiv:1707.03141, 2017.\n[18] Boris Oreshkin, Pau Rodr´ıguez L´opez, and Alexandre La-\ncoste. Tadam: Task dependent adaptive metric for improved\nfew-shot learning. In Advances in Neural Information Pro-\ncessing Systems, pages 721–731, 2018.\n[19] Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan L Yuille. Few-\nshot image recognition by predicting parameters from activa-\ntions. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 7229–7238, 2018.\n[20] Sachin Ravi and Hugo Larochelle. Optimization as a model\nfor few-shot learning. 2017.\n[21] Danilo Jimenez Rezende, Shakir Mohamed, Ivo Danihelka,\nKarol Gregor, and Daan Wierstra. One-shot generalization in\ndeep generative models. arXiv preprint arXiv:1603.05106,\n2016.\n[22] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al.\nImagenet large\nscale visual recognition challenge. International journal of\ncomputer vision, 115(3):211–252, 2015.\n[23] Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol\nVinyals, Razvan Pascanu, Simon Osindero, and Raia Had-\nsell.\nMeta-learning with latent embedding optimization.\narXiv preprint arXiv:1807.05960, 2018.\n[24] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan\nWierstra, and Timothy Lillicrap.\nMeta-learning with\nmemory-augmented neural networks. In International con-\nference on machine learning, pages 1842–1850, 2016.\n[25] Eli Schwartz, Leonid Karlinsky, Joseph Shtok, Sivan Harary,\nMattias Marder, Abhishek Kumar, Rogerio Feris, Raja\nGiryes, and Alex Bronstein.\nDelta-encoder: an effective\nsample synthesis method for few-shot object recognition. In\nAdvances in Neural Information Processing Systems, pages\n2845–2855, 2018.\n",
    "[26] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014.\n[27] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypi-\ncal networks for few-shot learning. In Advances in Neural\nInformation Processing Systems, pages 4077–4087, 2017.\n[28] Qianru Sun, Yaoyao Liu, Tat-Seng Chua, and Bernt Schiele.\nMeta-transfer learning for few-shot learning.\nIn Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 403–412, 2019.\n[29] Ilya Sutskever, James Martens, George Dahl, and Geoffrey\nHinton. On the importance of initialization and momentum\nin deep learning. In International conference on machine\nlearning, pages 1139–1147, 2013.\n[30] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan\nWierstra, et al. Matching networks for one shot learning. In\nAdvances in neural information processing systems, pages\n3630–3638, 2016.\n[31] Ruixiang Zhang, Tong Che, Zoubin Ghahramani, Yoshua\nBengio, and Yangqiu Song. Metagan: An adversarial ap-\nproach to few-shot learning. In Advances in Neural Informa-\ntion Processing Systems, pages 2365–2374, 2018.\n"
  ],
  "full_text": "Task Augmentation by Rotating for Meta-Learning\nJialin Liu\nCognitive Science Department\nXiamen University\nFujian, P. R. China 361005\njialin@stu.xmu.edu.cn\nFei Chao\nCognitive Science Department\nXiamen University\nFujian, P. R. China 361005\nfchao@xmu.edu.cn\nChih-Min Lin\nDepartment of Electrical Engineering\nYuan ze University\nChung-Li, Tao-Yuan 320, Taiwan\ncml@saturn.yzu.edu.tw\nAbstract\nData augmentation is one of the most effective ap-\nproaches for improving the accuracy of modern machine\nlearning models, and it is also indispensable to train a\ndeep model for meta-learning. In this paper, we introduce\na task augmentation method by rotating, which increases\nthe number of classes by rotating the original images 90,\n180 and 270 degrees, different from traditional augmenta-\ntion methods which increase the number of images. With\na larger amount of classes, we can sample more diverse\ntask instances during training. Therefore, task augmenta-\ntion by rotating allows us to train a deep network by meta-\nlearning methods with little over-ﬁtting. Experimental re-\nsults show that our approach is better than the rotation for\nincreasing the number of images and achieves state-of-the-\nart performance on miniImageNet, CIFAR-FS, and FC100\nfew-shot learning benchmarks. The code is available on\nwww.github.com/AceChuse/TaskLevelAug.\n1. Introduction\nAlthough the machine learning systems have achieved\na human-level ability in many ﬁelds with a large amount\nof data, learning from a few examples is still a challenge\nfor modern machine learning techniques. Recently, the ma-\nchine learning community has paid signiﬁcant attention to\nthis problem, where few-shot learning is the common task\nfor meta-learning (e.g., [20, 5, 30, 27]). The purpose of\nfew-shot learning is to learn to maximize generalization ac-\ncuracy across different tasks with few training examples. In\na classiﬁcation application of the few-shot learning, tasks\nare generated by sampling from a conventional classiﬁca-\ntion dataset; then, training samples are randomly selected\nfrom several classes in the classiﬁcation dataset. In addi-\ntion, a part of the examples is used as training examples and\ntesting examples. Thus, a tiny learning task is formed by\nthese examples. The meta-learning methods are applied to\ncontrol the learning process of a base learner, so as to cor-\nrectly classify on testing examples.\nData augmentation is widely used to improve the train-\ning of deep learning models. Usually, data augmentation is\nregarded as an explicit form of regularization [9, 26, 13].\nData augmentation aims at artiﬁcially generating the train-\ning data by using various translations on existing data, such\nas: adding noises, cropping, ﬂipping, rotation, translation,\netc. The general idea of data augmentations is increasing\nthe number of images by change data slightly to be differ-\nent from original data, but the data still can be recognized\nby human. The new images involved in the classes are iden-\ntical to the original data, we call this as Image Aug.\nHowever, the minimum units of meta-learning are tasks\nrather than data, so we should use rotation operation to aug-\nment the number of tasks, which is called as task augmen-\ntation (referred to Task Aug). Task Aug means increasing\nthe types of task instances by increasing the data that can\nbe clearly recognized as the different classes as the origi-\nnal data and associating them as the novel classes(we show\nexamples in Figure 1).\nThis is important for the meta-\nlearning, since meta-learning models require to predict un-\nseen classes during the testing phase, increasing the di-\nverseness of tasks would help models to generate to unseen\nclasses.\nIn experiments, we compared two cases, 1) the new im-\nages are converted to the classes of original images and 2)\n1\narXiv:2003.00804v1  [cs.CV]  8 Feb 2020\n\n\nthe new images are associated to the novel classes with the\nmethod proposed in [3] on CIFAR-FS, FC100, miniIma-\ngeNet few-shot learning tasks, and showed the second case\ngot better results. Then the proposed method is evaluated\nby experiments with the state of art meta-learning meth-\nods [27, 15, 3] on CIFAR-FS, FC100, miniImageNet few-\nshot learning tasks, and compare with the results without the\ndata augmentation by rotating. In the comparative experi-\nments, Task Aug by rotating achieves the better accuracy\nthan the original meta-learning methods. Besides, the best\nresults of our experiments exceed the current state-of-art re-\nsult over a large margin.\n2. Related Work\nMeta-learning involves two hierarchies learning pro-\ncesses: low-level and high-level. The low-level learning\nprocess learns to deal with general tasks, often termed\nas the “inner loop”; and the high-level learning process\nlearns to improve the performance of a low-level task, of-\nten termed as the “outer loop”.\nSince models are re-\nquired to handle sensory data like images, deep learning\nmethods are often applied for the “outer loop”. However,\nthe machine learning methods applied for the “inner loop”\nare very diverse. Based on different methods in the “in-\nner loop”, meta-learning can be applied in image recogni-\ntion [4, 24, 5, 30, 20], image generation [2, 31, 21], rein-\nforce learning [5, 1], and etc. This work focuses on few-shot\nlearning image recognition based on meta-learning. There-\nfore, in the experiment, the methods applied in the “inner\nloop” are able to classify data, and they are K-nearest neigh-\nbor (KNN), Support Vector Machine (SVM) and ridge re-\ngression, respectively [27, 15, 3].\nPrevious studies have introduced many popular regular-\nization techniques to few-shot learning from deep learning,\nsuch as weight decay, dropout, label smooth [3], and data\naugmentation. Common data augmentation techniques for\nimage recognition are usually designed manually and the\nbest augmentation strategies depend on dataset. In natural\ncolor image datasets, random cropping and random hori-\nzontal ﬂipping are the most common. Since the few-shot\nlearning tasks consist of natural color images, the random\nhorizontal ﬂipping and random cropping are applied in few-\nshot learning. In addition, color (brightness, contrast, and\nsaturation) jitter is often applied in the works of few-shot\nlearning [7, 19].\nOther data augmentation technologies related to few-\nshot learning include generating samples by few-shot learn-\ning and generating samples for few-shot learning. The for-\nmer tried to synthesize additional examples via transfer-\nring, extracting, and encoding to create the data of the new\nclasses, that are intra-class relationships between pairs of\nreference classes’ data instances [8, 25].\nThe later tried\nto apply meta-learning in a few-shot generation to gener-\nate samples from other models [2].In addition to these two\ntypes of studies, the data augmentation technology most\nclosed to the new proposed approach is applied to Omniglot\ndataset, which consists of handwritten words [14]. They\ncreated the novel classes by rotating the original images 90,\n180 and 270 degrees [24]. However, when this approach\nis applied for the natural color image, it would be slightly\nchanged, and we will explain this in Section 3.\n3. Method\n3.1. Problem Deﬁnition\nWe adopt the formulation purposed by [30] to describe\nthe N-way K-shot task. A few-shot task contains many\ntask instances (denoted by Ti), each instance is a classiﬁca-\ntion problem consisting of the data sampled from N classes.\nThe classes are randomly selected from a classes set. The\nclasses set are split into M tr, M val and M test for a train-\ning class set Ctr, a validation classes set Cval, and a test\nclasses set Ctest. In particular, each class does not over-\nlap others (i.e., the classes used during testing are unseen\nclasses during training). Data is randomly sampled from\nCtr, Cval and Ctest, so as to create task instances for train-\ning meta-set Str, validation meta-set Sval, and test meta-set\nStest, respectively. The validation and testing meta-sets are\nused for model selection and ﬁnal evaluation, respectively.\nThe data in each task instance, Ti, are divided into train-\ning examples Dtr and validation examples Dval. Both of\nthem only contains the data from N classes which sam-\npled from the appropriate classes set randomly (for a task\ninstance applied during training, the classes form a subset\nof the training classes set Ctr). In most settings, the training\nset Dtr = {(xk\nn, yk\nn)|n = 1 . . . N; k = 1 . . . K} consists\nof K data instances from each class, this processing usu-\nally called as a “shot”. The validation set, Dval, consists\nof several other data instances from the same classes, this\nprocessing is usually called as a “query”. An evaluation is\nprovided for generalization performance on the N classiﬁ-\ncation task instance Dtr. Note that: the validation set of\na task instance Dval (for optimizing model during “outer\nloop”) is different from the held-out validation classes set\nCval and meta-set Sval (for model selection).\n3.2. Task Augmentation by Rotating\nThis work is to increase the size of the training classes\nset, M tr, by rotating all images within the training classes\nset with 90, 180, 270 degrees. The size, M tr, is increased\nfor three times. In the Omniglot dataset consisting of hand-\nwritten words [24], this approach works well, since it can\nrotate a handwritten word multiple of 90 degrees and treat\nthe new one as another word; in addition, it is really possi-\nble that the novel word is similar to some words, which are\nnot included in the training classes but existed.\n\n\nOriginal classes\nNovel classes 2\nOriginal classes\nNovel classes 1\nNovel classes 3\nNovel classes 2\nNovel classes 1\nNovel classes 3\nFigure 1: Examples of the novel created classes.\nAlgorithm 1 Task Augmentation by Rotating.\nRequire: Classes set C = {c1, c2, . . . , cM}; Max possibility for\nTask Aug pmax; The delay to Task Aug T; The current count\nt; The number of ways, shots and queries N, K, H\n1: t ←t + 1\n2: p ←pmax ∗min{1, t\nT}\n3: n ∼Binomial(N, p)\n4: Dtr, Dval ←{}, {}\n5: V ←Sample N −n from {1, 2, · · · , M}\n6: for all v ∈V do\n7:\nD ←Sample K + H from cv\n8:\nDtr ←Dtr∪First K of D\n9:\nDval ←Dval∪Last H of D\n10: end for\n11: U ←Sample n from {M, M + 1, · · · , 4M}\n12: for all u ∈U do\n13:\nv ←(u mod M) + 1\n14:\nD ←Sample K + H from cv\n15:\nr ←⌊u\nM ⌋\n16:\nRotate all x ∈{x|(x, y) ∈D} 90r degrees\n17:\nDtr ←Dtr∪First K of D\n18:\nDval ←Dval∪Last H of D\n19: end for\n20: return (Dtr, Dval)\nc1\nc2\ncM\np\n1-p\npmax∗\nmin{1, t\nT }\nN-n from original classes\nn from novel classes\nDtr\nDval\nFigure 2: The process of generating a task instance with\nTask Aug by rotating.\nFor natural images, it is obvious that the images gener-\nated by rotating is real enough. But should the new gener-\nated images be classiﬁed as the novel classes or the original\nclasses? It dependents on the similarity between the new\nimages and the original classes. If the most of they are sim-\nilar enough, the new images should be classiﬁed as the orig-\ninal classes, and vice versa. This logic shows that one of the\ntwo methods must be effective. Since there are almost not\nworks merge the new images into the original classes which\nworked well, we assume that Task Aug by rotating is effec-\ntive for meta-learning, and we will compare two methods in\nexperiments.\nBesides, it is different from in handwritten that we assign\nthe new data smaller weights than the original data, so as to\nmake models prioritize learning the features of the original\nclasses, since the images generated by rotating rarely exist\nin the original data. This way makes the features of the\nnovel classes as a supplement to prevent the augmented data\n\n\nfrom taking up large capacity in the model, which is same\nas other common data augment methods.\nThe smaller weights are implemented in two ways, 1)\nlower probability and 2) delaying the probability of select-\ning the novel classes. For a class in a task instance, the\nprobability of the class coming from the novel classes is p,\nand the probability coming from the original classes is 1−p.\nBesides, the initial p is set to 0, then linearly rises from 0 to\npmax for the ﬁrst T tasks. The max probability pmax is set\nlower than the proportion of the novel classes in all classes\nto make each novel class have a lower probability than each\noriginal class. The whole process of Task Aug on a classes\nset is summarized in Algorithm 1 and Figure 2.\n3.3. Ensemble\nIn this work, we also compare the methods with the\ntraining protocol with ensemble method [11] in addition to\nthe standard training protocol, which choosing a model by\nthe validation set. The training protocol with an ensemble\nmethod use the models with different training epoch to an\nensemble model, in order to better use the models obtained\nin a single training process, and this approach has been\nproved to be valid for meta-learning by experiments [16].\nWe adopt this ensemble method. However, unlike [11] and\n[16] that we did not use cyclic annealing for learning rate\nand any methods to select models. We directly took the\naverage of the prediction of all models, which are saved ac-\ncording to an interval of 1 epoch. In Section 4, the methods\nwith this ensemble approach are marked by “+ens”.\n4. Experiments\nWe evaluate the proposed method on few-shot learning\ntasks. In order to ensure fair, both the results of baseline and\nTask Aug were run in our own environment. The compar-\native experiment is designed to answer the following ques-\ntions: (1) Image Aug and Task Aug by rotating which is\nable to improve the performance of meta-learning? (2) How\nmuch should the probably for the novel classes be set? (3)\nIs Task Aug by rotating able to improve the performance of\nthe current popular meta-learning methods?\n4.1. Experimental Conﬁguration\n4.1.1\nBackbone\nFollowing [15, 18, 17], we used ResNet-12 network in our\nexperiments.\nThe ResNet-12 network had four residual\nblocks which contains three 3 × 3 convolution, batch nor-\nmalization and Leaky ReLU with 0.1 negative slope. One\n2 × 2 max-pooling layer is used for reducing the size of the\nfeature map. The numbers of the network channels were\n64, 160, 320 and 640, respectively. DropBlock regulariza-\ntion [6] is used in the last two residual blocks, the conven-\ntional dropout [10] is used in the ﬁrst two residual blocks.\nThe block sizes of DropBlock were set to 2 and 5 for CI-\nFAR derivatives and ImageNet derivatives, respectively. In\nall experiments, the dropout possibility was set to 0.1. The\nglobal average pooling was not used for the ﬁnal output of\nthe last residual block.\n4.1.2\nBase Learners\nWe used ProtoNets [27], MetaOptNet-SVM [15] (we write\nit as M-SVM) and Ridge Regression Differentiable Dis-\ncriminator (R2-D2) [3] as basic methods to verify the ef-\nfective of Task Aug.\nFor ProtoNets, we did not use a higher way for training\nthan testing like [27]. Instead, the equal number of shot\nand way were used in both training and evaluation, and its\noutput multiplied by a learnable scale before the softmax\nfollowing [18, 15].\nFor M-SVM, we set training shot to 5 for CIFAR-FS;\n15 for FC100; and 15 for miniImageNet; regularization pa-\nrameter of SVM was set to 0.1; and a learnable scale was\nused following [15]. We did not use label smoothing like\n[15], because we did not ﬁnd that label smoothing can im-\nprove the performance in our environment. This was also\nafﬁrmed from the [15] author’s message on GitHub, that\nProgram language packages and environment might affect\nresults of the meta-learning method.\nFor R2-D2, we set the same training shot as for M-SVM,\nand used a learnable scale and bias following [3]. It was dif-\nferent from [3] we used a ﬁxed regularization parameter of\nridge regression which was set to 50 because [3] has con-\nﬁrmed that making it learnable might not be helpful.\nLast, for all methods, each class in a task instance con-\ntained 6 test (query) examples during training and 15 test\n(query) examples during testing.\n4.1.3\nTraining Conﬁguration\nStochastic gradient descent (SGD) was used.\nFollowing\n[29], we set weight decay and Nesterov momentum to\n0.0005 and 0.9, respectively. Each mini-batch contained 8\ntask instances. The meta-learning model was trained for\n60 epochs, and 1000 mini-batchs for each epoch. We set\nthe initial learning rate to 0.1, then multiplied it by 0.06,\n0.012, and 0.0024 at epochs 20, 40 and 50, respectively,\nas in [7]. The results, which are marked by “+ens” were\nused the 60 models saved after each epoch to become an\nensemble model. For the ﬁnal training, the training classes\nset was augmented by the validation classes set. When we\nonly chose one model, we will chose the model at the epoch\nwhere we got the best model during training on the train-\ning classes set. The results of the ﬁnal run are marked by\n“+val” in this subsection. Since the base idea of “+ens” was\nproposed by other works and “+val” is popular for meta-\nlearning, we do not explain more details about them.\n\n\n0.00\n0.25\n0.50\n0.75\nThe max probability of rotation\n80.0\n82.5\n85.0\n87.5\nAccuracy (%)\nCIFAR-FS 5-way 5-shot\nBaseline 5-shot\nImage Aug 5-shot\nTask Aug 5-shot\n0.00\n0.25\n0.50\n0.75\nThe max probability of rotation\n54\n56\n58\nAccuracy (%)\nFC100 5-way 5-shot\n0.00\n0.25\n0.50\n0.75\nThe max probability of rotation\n76\n78\n80\nAccuracy (%)\nminiImageNet 5-way 5-shot\n0.00\n0.25\n0.50\n0.75\nThe max probability of rotation\n70\n75\nAccuracy (%)\nCIFAR-FS 5-way 1-shot\nBaseline 1-shot\nImage Aug 1-shot\nTask Aug 1-shot\n0.00\n0.25\n0.50\n0.75\nThe max probability of rotation\n37.5\n40.0\n42.5\nAccuracy (%)\nFC100 5-way 1-shot\n0.00\n0.25\n0.50\n0.75\nThe max probability of rotation\n60.0\n62.5\nAccuracy (%)\nminiImageNet 5-way 1-shot\n0.00\n0.25\n0.50\n0.75\nThe max probability of rotation\n85.0\n87.5\nAccuracy (%)\nCIFAR-FS 5-way 5-shot with ensemble\n0.00\n0.25\n0.50\n0.75\nThe max probability of rotation\n56\n58\n60\nAccuracy (%)\nFC100 5-way 5-shot with ensemble\n0.00\n0.25\n0.50\n0.75\nThe max probability of rotation\n78\n80\n82\nAccuracy (%)\nminiImageNet 5-way 5-shot with ensemble\n0.00\n0.25\n0.50\n0.75\nThe max probability of rotation\n72.5\n75.0\n77.5\nAccuracy (%)\nCIFAR-FS 5-way 1-shot with ensemble\n0.00\n0.25\n0.50\n0.75\nThe max probability of rotation\n42\n44\n46\nAccuracy (%)\nFC100 5-way 1-shot with ensemble\n0.00\n0.25\n0.50\n0.75\nThe max probability of rotation\n62\n64\n66\nAccuracy (%)\nminiImageNet 5-way 1-shot with ensemble\nFigure 3: The accuracies (%) on meta-test sets with varying probability pmax for the novel classes.The 95% conﬁdence\ninterval is denoted by the shaded region.\nFor data augmentation, we adopted random crop, hori-\nzontal ﬂip, and color (brightness, saturation, and contrast)\njitter data augmentation following the work of [7, 19]. In\nthe experiments of comparing Task Aug and Image Aug by\nrotating, R2-D2 was applied, and we set T to 80000. In the\nevaluation of Task Aug for ProtoNets and M-SVM, we set\npmax to the value getting the best results for R2-D2.\n4.1.4\nDataset\nThe CIFAR-FS [3] containing all 100 classes from CIFAR-\n100 [12] is proposed as few-shot classiﬁcation benchmark\nrecently. These classes are randomly divided into training\nclasses, validation classes and test classes. The three types\ncontain 64, 16 and 20 classes, respectively. There are 600\nnature color images of size 32 × 32 in each class.\nThe FC100 [18] are also derived from CIFAR-100 [12],\nand the 100 classes are grouped into 20 superclasses. The\ntraining, validation, and testing classes contain 60 classes\nfrom 12 superclasses, 20 classes from 4 superclasses, and\n20 classes from 4 superclasses, respectively.\nThe target\nis to minimize the information overlap between classes to\nmake it more challenging than current few-shot classiﬁca-\ntion tasks. Same as CIFAR-FS, there are 600 nature color\nimages of size 32 × 32 in each class.\nThe miniImageNet [30] is one of the most popular\nbenchmark for few-shot classiﬁcation, which contains 100\nclasses randomly selected from ILSVRC-2012 [22]. The\nclasses are randomly divided into training classes, valida-\ntion classes and test classes, and them contain 64, 16 and\n20 classes, respectively. There are 600 nature color images\nof size 84 × 84 in each class. Since [30] did not release\nthe class splits, we use the more common split proposed by\n[20].\n4.2. Comparison between Task Aug and Image Aug\nTo prove our assumption that rotation multi 90 degrees\nfor Task Aug is better than that for Image Aug, we draw\nthe accuracy curves depending on pmax for both Task Aug\nand Image Aug (curves showed in Figure 3). The linear\nrising of p was also used for Image Aug, and T = 80000\nfor both Task Aug and Image Aug. In all the results showed\nin Figure 3, the training classes set was not augmented by\nthe validation classes set.\nAs shown in Figure 3, the performance of Task Aug on\nmost of the regimes is better than Image Aug and baseline\nin general. Besides, we observed that: with the increase\nof pmax, the accuracy rises at ﬁrst, reaches the peaks be-\ntween 0.25 and 0.5, then declines and reaches baseline when\npmax = 0.75 at the end, which is the proportion of the novel\nclasses in all classes. The accuracy of Task Aug on CIFAR-\nFS, FC100 and miniImagNet reach the peaks at 0.5, 0.25\nand 0.25 respectively. At the same time, the rotation multi\n90 degrees for Image Aug cannot improve or even cause\n\n\nTable 1: Comparison to the average accuracies (%) with\n95% conﬁdence intervals between the methods with and\nwithout Task Aug on CIFAR-FS 5-way 1-shot.\nMethod\nBaseline\nTask Aug\nProtoNets [27]\n71.88±0.52\n74.15±0.50\nProtoNets (+ens)\n73.95±0.51\n75.89±0.48\nProtoNets (+val)\n73.20±0.51\n75.10±0.49\nProtoNets (+ens+val)\n76.05±0.49\n77.28±0.47\nM-SVM [15]\n71.52±0.51\n72.95±0.48\nM-SVM (+ens)\n74.12±0.50\n75.85±0.47\nM-SVM (+val)\n72.42±0.50\n73.13±0.47\nM-SVM (+ens+val)\n75.91±0.48\n76.75±0.46\nR2-D2 [3]\n72.27±0.51\n74.42±0.48\nR2-D2 (+ens)\n75.06±0.50\n76.51±0.47\nR2-D2 (+val)\n73.52±0.50\n76.02±0.47\nR2-D2 (+ens+val)\n76.40±0.49\n77.66±0.46\nworse performance.\n4.3. Evaluation of Task Aug\nIn order to further prove the proposed approach can im-\nprove the performance of meta-learning, we evaluate Task\nAug by rotating on several meta-learning methods in this\nsection.\nWe choose several currently the state of art base learn-\ners for experiments, we detail in Section 4.1.2. Besides, the\ntraining protocol with ensemble method can get better re-\nsults than the standard training protocol, we involve it in the\nexperiments. We think this is important, because the pro-\nposed method can only be a contribution if it can further\nimprove performance based on the best method available at\npresent.\nResults. Table 1-6 show the average accuracies (%) with\n95% conﬁdence intervals of the methods with and without\nTask Aug, and the best results are highlighted.\nThe ta-\nbles show that the proposed method can improve the per-\nformance in most of cases.\nWe can observe that: some results without the ensem-\nble approach [11] of baseline and Task Aug are close, but\nthe advantage of Task Aug is still obvious on the compari-\nson results with the ensemble approach. We suspect that the\nscale of backbone limits the performance of the best model.\nA larger scale backbone is needed for the training process\nwith Task Aug. For the results of ensemble approach, since\nTask Aug reduces the over-ﬁtting, more models during the\ntraining process have good performance, which provide en-\nsemble with models of higher quality.\nLast we compare the results of this work with the results\nproposed by the prior works, in order to show how much\nTable 2: Comparison to the average accuracies (%) with\n95% conﬁdence intervals between the methods with and\nwithout Task Aug on CIFAR-FS 5-way 5-shot.\nMethod\nBaseline\nTask Aug\nProtoNets [27]\n84.14±0.36\n85.37±0.35\nProtoNets (+ens)\n85.72±0.35\n87.33±0.33\nProtoNets (+val)\n85.29±0.35\n86.53±0.34\nProtoNets (+ens+val)\n86.88±0.34\n88.24±0.33\nM-SVM [15]\n84.01±0.36\n85.91±0.36\nM-SVM (+ens)\n85.85±0.34\n87.73±0.33\nM-SVM (+val)\n84.94±0.36\n86.94±0.34\nM-SVM (+ens+val)\n87.15±0.34\n88.38±0.33\nR2-D2 [3]\n84.60±0.36\n86.02±0.35\nR2-D2 (+ens)\n86.11±0.34\n87.63±0.34\nR2-D2 (+val)\n85.39±0.36\n86.73±0.34\nR2-D2 (+ens+val)\n87.04±0.34\n88.33±0.33\nTable 3: Comparison to the average accuracies (%) with\n95% conﬁdence intervals between the methods with and\nwithout Task Aug on FC100 5-way 1-shot.\nMethod\nBaseline\nTask Aug\nProtoNets [27]\n37.53±0.40\n38.89±0.40\nProtoNets (+ens)\n40.04±0.41\n42.00±0.43\nProtoNets (+val)\n43.63±0.43\n44.91±0.46\nProtoNets (+ens+val)\n47.16±0.46\n48.91±0.47\nM-SVM [15]\n40.50±0.39\n41.17±0.40\nM-SVM (+ens)\n43.24±0.42\n44.38±0.42\nM-SVM (+val)\n46.72±0.45\n47.39±0.44\nM-SVM (+ens+val)\n49.50±0.46\n49.77±0.45\nR2-D2 [3]\n40.66±0.41\n41.47±0.40\nR2-D2 (+ens)\n43.27±0.42\n44.75±0.43\nR2-D2 (+val)\n47.12±0.44\n48.21±0.45\nR2-D2 (+ens+val)\n49.92±0.45\n51.35±0.46\nthis work raises the baselines after combining several prior\nmethods and the proposed method, and they are showed in\nTable 7, 8 and 9. The tables show that the highest accu-\nracies of our experiments exceeded the current state-of-art\naccuracies 2% to 5%.\n5. Conclusion\nWe proposed a Task Level Data Augmentation (Task\nAug), a data augmentation technique that increased the\nnumber of training classes to provide more diverse few-\nshow task instances for meta-learning. We proved that Task\nAug was valid for CIFAR-FS, FC100, and miniImageNet,\n\n\nTable 4: Comparison to the average accuracies (%) with\n95% conﬁdence intervals between the methods with and\nwithout Task Aug on FC100 5-way 5-shot.\nMethod\nBaseline\nTask Aug\nProtoNets [27]\n51.43±0.39\n53.92±0.39\nProtoNets (+ens)\n54.24±0.40\n56.55±0.40\nProtoNets (+val)\n61.16±0.42\n60.86±0.41\nProtoNets (+ens+val)\n63.64±0.43\n65.47±0.42\nM-SVM [15]\n54.83±0.40\n56.23±0.40\nM-SVM (+ens)\n58.49±0.41\n60.14±0.41\nM-SVM (+val)\n62.99±0.42\n63.64±0.42\nM-SVM (+ens+val)\n66.37±0.42\n67.17±0.41\nR2-D2 [3]\n55.85±0.39\n56.29±0.40\nR2-D2 (+ens)\n58.01±0.40\n59.94±0.41\nR2-D2 (+val)\n63.32±0.40\n64.53±0.42\nR2-D2 (+ens+val)\n65.58±0.42\n67.66±0.42\nTable 5: Comparison to the average accuracies (%) with\n95% conﬁdence intervals between the methods with and\nwithout Task Aug on miniImageNet 5-way 1-shot.\nMethod\nBaseline\nTask Aug\nProtoNets [27]\n58.67±0.48\n60.52±0.48\nProtoNets (+ens)\n62.12±0.48\n63.69±0.47\nProtoNets (+val)\n60.13±0.48\n62.22±0.49\nProtoNets (+ens+val)\n63.84±0.48\n65.04±0.48\nM-SVM [15]\n60.02±0.45\n62.12±0.44\nM-SVM (+ens)\n63.44±0.45\n64.56±0.44\nM-SVM (+val)\n61.58±0.45\n63.14±0.45\nM-SVM (+ens+val)\n64.74±0.45\n65.38±0.45\nR2-D2 [3]\n60.57±0.44\n62.32±0.45\nR2-D2 (+ens)\n63.72±0.44\n64.79±0.45\nR2-D2 (+val)\n62.82±0.45\n62.64±0.44\nR2-D2 (+ens+val)\n65.50±0.45\n65.95±0.45\nand exceeded the result of the previous works. Task Aug\nachieved the performance by rotating the images 90, 180\nand 270 degrees. This method is simple and cost-effective.\nWith the ensemble method, we exceeded the state-of-the-art\nresult over a large margin.\nFuture work will focus on searching different network\nstructures for meta-learning, since the training with Task\nAug would require larger model. Besides, we will try to\napply Task Aug to other few-shot learning tasks to verify\nits effectiveness. Another interesting topic is to build other\napproaches for Task Aug, such as swapping channel order,\npicture blend or even auto augmentation.\nTable 6: Comparison to the average accuracies (%) with\n95% conﬁdence intervals between the methods with and\nwithout Task Aug on miniImageNet 5-way 5-shot.\nMethod\nBaseline\nTask Aug\nProtoNets [27]\n75.24±0.37\n77.00±0.36\nProtoNets (+ens)\n78.11±0.34\n79.77±0.34\nProtoNets (+val)\n76.98±0.36\n77.59±0.37\nProtoNets (+ens+val)\n79.54±0.35\n80.60±0.34\nM-SVM [15]\n77.85±0.34\n78.90±0.34\nM-SVM (+ens)\n80.18±0.32\n81.35±0.32\nM-SVM (+val)\n78.65±0.34\n79.97±0.33\nM-SVM (+ens+val)\n81.39±0.32\n82.13±0.31\nR2-D2 [3]\n77.44±0.34\n78.81±0.34\nR2-D2 (+ens)\n79.90±0.33\n81.08±0.32\nR2-D2 (+val)\n78.61±0.35\n79.58±0.33\nR2-D2 (+ens+val)\n81.34±0.32\n81.96±0.32\nTable 7:\nThe average accuracies (%) with 95% conﬁ-\ndence intervals on CIFAR-FS. ∗CIFAR-FS results from [3].\n†Result from [15].\nMethod\n1-shot\n5-shot\nMAML∗[5]\n58.9±1.9\n71.5±1.0\nR2-D2 [3]\n65.3±0.2\n79.4±0.1\nProtoNets† [27]\n72.2±0.7\n83.5±0.5\nM-SVM [15]\n72.8±0.7\n85.0±0.5\nM-SVM (best) (our)\n76.75±0.46\n88.38±0.33\nR2-D2 (best) (our)\n77.66±0.46\n88.33±0.33\nTable 8: The average accuracies (%) with 95% conﬁdence\nintervals on FC100. †FC100 result from [15].\nMethod\n1-shot\n5-shot\nTADAM [18]\n40.1±0.4\n56.1±0.4\nProtoNets† [27]\n37.5±0.6\n52.5±0.6\nMTL [28]\n45.1±1.8\n57.6±0.9\nM-SVM [15]\n47.2±0.6\n62.5±0.6\nM-SVM (best) (our)\n49.77±0.45\n67.17±0.41\nR2-D2 (best) (our)\n51.35±0.46\n67.66±0.42\nReferences\n[1] Maruan Al-Shedivat,\nTrapit Bansal,\nYuri Burda,\nIlya\nSutskever, Igor Mordatch, and Pieter Abbeel. Continuous\nadaptation via meta-learning in nonstationary and competi-\ntive environments. arXiv preprint arXiv:1710.03641, 2017.\n\n\nTable 9: The average accuracies (%) with 95% conﬁdence\nintervals on miniImageNet. ∗Result from [15]. Here only\nlist the best results of previous works due to the shortage of\nspace.\nMethod\n1-shot\n5-shot\n[7]\n56.20±0.86\n73.00±0.64\nTADAM [18]\n58.50±0.30\n76.70±0.30\nLEO [23]\n61.76±0.08\n77.59±0.12\nProtoNets∗[27]\n59.25±0.64\n75.60±0.48\nM-SVM [15]\n64.09±0.62\n80.00±0.45\nM-SVM (best) (our)\n65.38±0.45\n82.13±0.31\nR2-D2 (best) (our)\n65.95±0.45\n81.96±0.32\n[2] Antreas Antoniou, Amos Storkey, and Harrison Edwards.\nData augmentation generative adversarial networks. arXiv\npreprint arXiv:1711.04340, 2017.\n[3] Luca Bertinetto, Joao F Henriques, Philip HS Torr, and An-\ndrea Vedaldi. Meta-learning with differentiable closed-form\nsolvers. arXiv preprint arXiv:1805.08136, 2018.\n[4] Li Fei-Fei, Rob Fergus, and Pietro Perona. One-shot learning\nof object categories. IEEE transactions on pattern analysis\nand machine intelligence, 28(4):594–611, 2006.\n[5] Chelsea Finn, Pieter Abbeel, and Sergey Levine.\nModel-\nagnostic meta-learning for fast adaptation of deep networks.\nIn Proceedings of the 34th International Conference on Ma-\nchine Learning-Volume 70, pages 1126–1135. JMLR. org,\n2017.\n[6] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Dropblock:\nA regularization method for convolutional networks.\nIn\nAdvances in Neural Information Processing Systems, pages\n10727–10737, 2018.\n[7] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot\nvisual learning without forgetting.\nIn Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 4367–4375, 2018.\n[8] Bharath Hariharan and Ross Girshick.\nLow-shot visual\nrecognition by shrinking and hallucinating features. In Pro-\nceedings of the IEEE International Conference on Computer\nVision, pages 3018–3027, 2017.\n[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016.\n[10] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya\nSutskever, and Ruslan R Salakhutdinov. Improving neural\nnetworks by preventing co-adaptation of feature detectors.\narXiv preprint arXiv:1207.0580, 2012.\n[11] Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E\nHopcroft, and Kilian Q Weinberger. Snapshot ensembles:\nTrain 1, get m for free. arXiv preprint arXiv:1704.00109,\n2017.\n[12] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10\n(canadian institute for advanced research). URL http://www.\ncs. toronto. edu/kriz/cifar. html, 8, 2010.\n[13] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classiﬁcation with deep convolutional neural net-\nworks. In Advances in neural information processing sys-\ntems, pages 1097–1105, 2012.\n[14] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B\nTenenbaum. Human-level concept learning through proba-\nbilistic program induction. Science, 350(6266):1332–1338,\n2015.\n[15] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and\nStefano Soatto. Meta-learning with differentiable convex op-\ntimization. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 10657–10665,\n2019.\n[16] Jinchao Liu, Stuart J Gibson, and Margarita Osadchy. Learn-\ning to support: Exploiting structure information in support\nsets for one-shot learning. arXiv preprint arXiv:1808.07270,\n2018.\n[17] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter\nAbbeel.\nA simple neural attentive meta-learner.\narXiv\npreprint arXiv:1707.03141, 2017.\n[18] Boris Oreshkin, Pau Rodr´ıguez L´opez, and Alexandre La-\ncoste. Tadam: Task dependent adaptive metric for improved\nfew-shot learning. In Advances in Neural Information Pro-\ncessing Systems, pages 721–731, 2018.\n[19] Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan L Yuille. Few-\nshot image recognition by predicting parameters from activa-\ntions. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 7229–7238, 2018.\n[20] Sachin Ravi and Hugo Larochelle. Optimization as a model\nfor few-shot learning. 2017.\n[21] Danilo Jimenez Rezende, Shakir Mohamed, Ivo Danihelka,\nKarol Gregor, and Daan Wierstra. One-shot generalization in\ndeep generative models. arXiv preprint arXiv:1603.05106,\n2016.\n[22] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al.\nImagenet large\nscale visual recognition challenge. International journal of\ncomputer vision, 115(3):211–252, 2015.\n[23] Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol\nVinyals, Razvan Pascanu, Simon Osindero, and Raia Had-\nsell.\nMeta-learning with latent embedding optimization.\narXiv preprint arXiv:1807.05960, 2018.\n[24] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan\nWierstra, and Timothy Lillicrap.\nMeta-learning with\nmemory-augmented neural networks. In International con-\nference on machine learning, pages 1842–1850, 2016.\n[25] Eli Schwartz, Leonid Karlinsky, Joseph Shtok, Sivan Harary,\nMattias Marder, Abhishek Kumar, Rogerio Feris, Raja\nGiryes, and Alex Bronstein.\nDelta-encoder: an effective\nsample synthesis method for few-shot object recognition. In\nAdvances in Neural Information Processing Systems, pages\n2845–2855, 2018.\n\n\n[26] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014.\n[27] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypi-\ncal networks for few-shot learning. In Advances in Neural\nInformation Processing Systems, pages 4077–4087, 2017.\n[28] Qianru Sun, Yaoyao Liu, Tat-Seng Chua, and Bernt Schiele.\nMeta-transfer learning for few-shot learning.\nIn Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 403–412, 2019.\n[29] Ilya Sutskever, James Martens, George Dahl, and Geoffrey\nHinton. On the importance of initialization and momentum\nin deep learning. In International conference on machine\nlearning, pages 1139–1147, 2013.\n[30] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan\nWierstra, et al. Matching networks for one shot learning. In\nAdvances in neural information processing systems, pages\n3630–3638, 2016.\n[31] Ruixiang Zhang, Tong Che, Zoubin Ghahramani, Yoshua\nBengio, and Yangqiu Song. Metagan: An adversarial ap-\nproach to few-shot learning. In Advances in Neural Informa-\ntion Processing Systems, pages 2365–2374, 2018.\n"
}