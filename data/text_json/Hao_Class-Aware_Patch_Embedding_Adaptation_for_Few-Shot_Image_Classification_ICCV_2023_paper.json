{
  "filename": "Hao_Class-Aware_Patch_Embedding_Adaptation_for_Few-Shot_Image_Classification_ICCV_2023_paper.pdf",
  "num_pages": 11,
  "pages": [
    "Class-Aware Patch Embedding Adaptation for Few-Shot Image Classification\nFusheng Hao1,2\nFengxiang He3\nLiu Liu4\nFuxiang Wu1,2\nDacheng Tao4\nJun Cheng1,2*\n1Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems,\nShenzhen Institute of Advanced Technology, Chinese Academy of Sciences, China\n2The Chinese University of Hong Kong, Hong Kong, China\n3AIAI, School of Informatics, University of Edinburgh, United Kingdom\n4School of Computer Science, Faculty of Engineering, The University of Sydney, Australia\nAbstract\n“A picture is worth a thousand words”, significantly be-\nyond mere a categorization. Accompanied by that, many\npatches of the image could have completely irrelevant\nmeanings with the categorization if they were indepen-\ndently observed. This could significantly reduce the effi-\nciency of a large family of few-shot learning algorithms,\nwhich have limited data and highly rely on the compari-\nson of image patches. To address this issue, we propose a\nClass-aware Patch Embedding Adaptation (CPEA) method\nto learn “class-aware embeddings” of the image patches.\nThe key idea of CPEA is to integrate patch embeddings with\nclass-aware embeddings to make them class-relevant. Fur-\nthermore, we define a dense score matrix between class-\nrelevant patch embeddings across images, based on which\nthe degree of similarity between paired images is quantified.\nVisualization results show that CPEA concentrates patch\nembeddings by class, thus making them class-relevant.\nExtensive experiments on four benchmark datasets, mini-\nImageNet, tieredImageNet, CIFAR-FS, and FC-100, indi-\ncate that our CPEA significantly outperforms the existing\nstate-of-the-art methods. The source code is available at\nhttps://github.com/FushengHao/CPEA.\n1. Introduction\nReal-world images are usually composed of many dif-\nferent entities, e.g., two oxen grazing surrounded by a barn,\na fence and trees as shown in Figure 1. Assigning a sin-\ngle annotation to each image that corresponds to only one\ntype of entity is a common practice to construct computer\nvision datasets, e.g., CIFAR [33] and ImageNet [54]. Such\nan annotation can only describe part of an image’ contents.\nThis is acceptable in many classification scenarios, because\nthe interference caused by other image contents can be mit-\n*Corresponding author (email: jun.cheng@siat.ac.cn).\nFigure 1. Illustration of multiple entities from different classes si-\nmultaneously existing in a real-world image. Despite being anno-\ntated as “ox”, the image contains entities of other classes, such as\n“fence”, “barn”, “tree”, etc. The core idea of CPEA is to learn\n“class-aware embeddings” of the image patches.\nigated by the use of a large number of labeled images.\nSpecifically, since each class contains a sufficient number\nof labeled images that vary greatly within the class and the\ncorresponding entities always appear in these images, deep\nmodels trained on such data tend to pay attention to the fre-\nquently occurring class-relevant entities (e.g., “ox” in Fig-\nure 1) while ignoring other irrelevant ones, especially those\nthat frequently appear across classes [26].\nBig challenges, however, arise in the context of few-shot\nimage classification, in which approaches are expected to\ncorrectly identify new classes that are disjoint with the train-\ning classes during the test phase, given only a few (e.g., one\nor five) labeled images for each of these new classes. The\nchallenges are as follows: 1) Due to the scarcity of labeled\nimages of new classes and the extremely limited number\nof class-relevant entities, it is very difficult for a model to\nidentify which entity determines the class of an image. 2)\nEntities contained in the training images but not covered by\nthe training classes may happen to be the ones expected to\nbe covered by the new classes at test time, which would in-\ntroduce ambiguity. 3) Specific patterns learned during the\ntraining phase may be overemphasized, but they may not\nThis ICCV paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n18905\n",
    "be relevant to the new classes seen at test time, resulting in\nsupervision collapse [14] and limited generalizability.\nA promising solution is to align semantically-relevant\nregions [24, 27, 70, 14, 26].\nSAML [24] proposes to\nuse the activation-based attention to highlight semantically-\nrelevant regions while suppressing others. CAN [27] per-\nforms cross-attention between class prototypes and query\nfeature maps to highlight class-relevant regions.\nDeep-\nEMD [70] looks for the aligned regions by minimizing the\nearth movers’ distance. CTX [14] uses a Transformer-style\nattention mechanism to perform the spatial and semantic\nalignment and mitigates supervision collapse by incorpo-\nrating self-supervised learning in training. FewTURE [26]\ndetermines the most informative regions via online opti-\nmization and then uses them to reweigh patch correspon-\ndences. While these methods have shown great potential\nin eliminating interference and tackling supervision col-\nlapse, there still exist crucial drawbacks. Firstly, aligned\nsemantically-relevant regions are not always beneficial for\nsimilarity measure, such as those that are irrelevant to the\nclass of interest. Secondly, the scarcity of labeled images\nof new classes and the extremely limited number of class-\nrelevant entities makes it difficult to deal with the inaccurate\nlocalization and alignment induced by large intra-class vari-\nation and background clutter in real-world images.\nIn this paper, we deal with the above challenges from\na new perspective and propose a Class-aware Patch Em-\nbedding Adaptation (CPEA) method that can eliminate the\ninterference of single-label annotations without aligning\nsemantically-relevant regions while avoiding supervision\ncollapse. Specifically, we employ self-supervision pretrain-\ning instead of the supervised one to avoid supervision col-\nlapse, with Masked Image Modelling [76] as a pretext task,\nwhich yields semantically meaningful patch embeddings.\nSince the patch embeddings may be irrelevant to class of\ninterest, this leads to the need for aligning semantically-\nrelevant patches. We avoid the need for localization and\nalignment mechanisms by making patch embeddings class-\nrelevant. To this end, we introduce a class-agnostic embed-\nding and feed it into the transformer to interact with patch\nembeddings to make it class-aware. Then, patch embed-\ndings are adapted with the class-aware embeddings to make\nthem class-relevant, which alleviates the scarcity of labeled\nimages by increasing their amount. Furthermore, we define\na dense score matrix between class-relevant patch embed-\ndings across images, based on which the degree of similar-\nity between paired images is quantified.\nOur main contributions are summarized as follows: 1)\nWe deal with the interference caused by single-label an-\nnotations in few-shot settings from a new perspective and\ndemonstrate that the interference can be successfully miti-\ngated without the need for localization and alignment mech-\nanisms.\n2) We propose the CPEA, a novel method that\nmakes patch embeddings class-relevant and measures the\nsimilarity between class-relevant patch embeddings across\nimages in a dense manner, which improves transferability.\n3) Visualizations show that our CPEA makes patch embed-\ndings class-relevant. Extensive experiments are conducted\non four popular benchmark datasets and the results indi-\ncate that our CPEA achieves superior performance over the\nstate-of-the-art methods.\n2. Related work\nFew-shot image classification. Few-shot image classi-\nfication has recently attracted much attention because of its\ngreat application prospects in real-world scenarios. Exist-\ning methods can be roughly categorized into two groups.\nThe first group is optimization-based methods. They learn\na meta-learner, which can optimize a learner in a few steps\ngiven the few labeled images.\nFor example, the meta-\nlearner of MAML [18] and Reptile [49] output a set of good\nmodel initializations that can be adapted to a specific few-\nshot classification task in a few gradient steps. To avoid\nthe large computational overhead caused by the need to up-\ndate all model parameters during inference, CAVIA [78]\nand LEO [55] propose to perform meta-learning in a low-\ndimensional representation space.\nThen, it is found that\nboth the choice of network architectures and the design of\nmeta-learners have a severe impact on the performance and\nefficiency, which motivates the exploration of various vari-\nants of meta-learning methods [13, 50, 29].\nThe second group is metric-based methods. They fo-\ncus on learning a feature space suitable for all few-shot\nclassification tasks, in which an appropriate distance func-\ntion is used for similarity measure. For example, Match-\ningNet [61] constructs a feature space based on neural net-\nworks, where the cosine distance is used for similarity mea-\nsure. ProtoNet [57] learns a feature space, where the Eu-\nclidean distance is used for similarity measure. Then, it is\nfound that the choice of network architectures, the choice of\ndistance functions, the choice of prototypes, and the choice\nof training strategies all have a severe impact on the perfor-\nmance and efficiency, which motivates the exploration of\nvarious variants of metric-based methods [46, 47, 56, 69,\n36, 62, 4, 5, 19]. Recently, local feature-based methods\nhave achieved great success in addressing the challenging\nfew-shot image classification problem. One line of such\nmethods directly treat local features as image representa-\ntions [58, 41, 12, 11] and the other line is to align the se-\nmantically relevant local features [24, 27, 70, 14, 26, 23].\nOur method makes local features class-relevant, thus avoid-\ning the need for localization and alignment mechanisms and\nincreasing the usability of local features.\nSelf-supervision in few-shot image classification. Al-\nthough self-supervision methods have achieved great suc-\ncess on large-scale image datasets, their potential has not\n18906\n",
    "Figure 2. Overview of the proposed class-aware patch embedding adaptation method. The ViT is pretrained with Masked Image Mod-\nelling [76] as a pretext task. The class-agnostic embedding is a learnable embedding and made class-aware by constantly interacting with\npatch embeddings in the ViT. Then, the patch embeddings at the output are adapted with the class-aware embeddings to make them class-\nrelevant. Finally, the similarity score between paired images is obtained by aggregating the dense score matrix. Note that we distinguish\npatch embeddings by numbers and M denotes the number of patch embeddings.\nbeen fully explored in the few-shot settings. Recent works\nhave shown that they can help improve the generalization\nability of learned models [48, 14, 26, 43, 67]. For example,\nS2M2 [48] integrates two self-supervision methods, i.e., ro-\ntation [22] and exemplars [16], with the standard supervised\nlearning to improve the generalization ability of output fea-\ntures of pre-trained models.\nCTX [14] integrates Sim-\nCLR [8] into the episodic training strategy to improve the\ngeneralization ability of the learned model. FewTURE [26]\nuses Masked Image Modelling [3, 38, 25, 76] as a pretext\ntask to pretrain the Vision Transformer (ViT) [15] on small-\nscale datasets, resulting in features with strong generaliza-\ntion ability.\nViT in few-shot image classification.\nDue to their\nability to build long-range dependencies between image\npatches, ViTs have achieved great success in many appli-\ncation fields of computer vision such as image classifica-\ntion [15, 42] and object detection [42]. However, they rely\nmore heavily on large-scale image datasets than Convolu-\ntional Neural Networks (CNNs) due to the lack of the con-\nvolutional inductive bias [40]. For example, ViTs have to\nlearn from images the locality and the translation invari-\nance embedded in the CNN design. This data-hungry na-\nture makes it difficult for ViTs to be used as a whole in the\nfew-shot settings, or only a very tiny part of ViTs (e.g., s\nsingle Transformer head) can be used in conjunction with\nCNNs [14, 69, 34]. Recently, FewTURE [26] demonstrates\nthat a fully ViT-based architecture can be successfully gen-\neralized on small-scale image datasets.\n3. Method\nWe first formulate the definition of few-shot image\nclassification and then present an overview of the whole\npipeline. Next, we detail the class-aware patch embedding\nadaptation and the dense similarity measure. Finally, we\ndescribe the training and inference strategies.\n3.1. Problem definition\nFew-shot image classification focuses on generalizing\nthe knowledge learned on the training classes Ctrain to the\nunseen test classes Ctest, i.e., Ctrain ∩Ctest = ∅, given\nonly a few labeled images for each of these test classes.\nWe follow the common practice of previous works [61]\nto formulate the N-way K-shot classification task in an\nepisodic manner, where N denotes the number of classes\nand K denotes the number of labeled images contained\nin each class.\nAn episode is composed of a support set\nXs = {(xi, yi)}NK\ni=1 and a query set Xq = {(xi, yi)}NQ\ni=1,\nwhere Q denotes the number of test images contained in\neach class. The query set is used to evaluate the perfor-\nmance of a model on the few-shot classification task defined\nby the support set. Our goal is to learn a model on training\nclasses that generalizes well on episodes randomly sampled\nfrom the unseen test classes within the inductive framework.\n18907\n",
    "(a)\n(b)\nFigure 3. Class-aware embedding visualization.\nTwo different\nsampling results are given in (a) and (b), respectively, with 100\nclass-aware embeddings per class. The class-agnostic embedding\nis denoted by the black “diamond”. After interacting with images\nfrom different classes, the output states of class-agnostic embed-\nding are class-aware.\n3.2. Overview\nFigure 2 shows the pipeline of the proposed method.\nWe decompose an image into patches, embed them using\nlinear projection, add position embeddings to the resulting\npatch embeddings, and prepend a class-agnostic embedding\nto the sequence of patch embeddings, which are then fed\ninto a standard Transformer encoder. Before performing\nclass-aware patch embedding adaptation, we add a projec-\ntion head to further transform the embeddings at the out-\nput of the Transformer encoder, with aim of increasing their\nadaptability to the few-shot classification task. Afterwards,\nthe class-aware embeddings are used to adapt patch embed-\ndings to make them class-relevant. To quantify the degree of\nsimilarity between paired images, we define a dense score\nmatrix between class-relevant patch embeddings, based on\nwhich a MLP is used to aggregate the dense score matrix\ninto a single similarity score.\n3.3. Class-aware patch embedding adaptation\nSelf-supervision pretraining. Single-label annotations\ncause supervision collapse in few-shot settings, which high-\nlights certain patterns that are useful for distinguishing\ntraining classes, rather than those that have good trans-\nferability [26].\nTo address this issue, we employ self-\nsupervision pretraining instead of the supervised one to\npretrain the ViT. Specifically, we decompose an image\ninto patches, randomly mask some patches, encode these\npatches with a ViT and reconstruct the masked patches,\nwhere Masked Image Modeling (MIM) [3, 76] is used as\na pretext task. The reasons for this are two-fold: 1) Labeled\nimages are scarce in few-shot settings and a considerable\namount of patch embeddings can be obtained in a single\nfeedforward. To facilitate making patch embeddings class-\nrelevant, it is necessary to yield semantically meaningful\npatch embeddings. 2) Different from the self-supervision\napproaches [7, 9] that focus on self-similarity of global rep-\nresentations between images from different views, MIM\naims to reconstruct the masked patches and build an under-\nstanding of the structure and content of an image, rather\nthan learning patterns that are mainly useful for training\nclasses, which improves transferability.\nClass-aware embedding. After pretraining, a consid-\nerable number of patch embeddings can be obtained for\nan input image in a single feedforward. However, the se-\nmantics of these patch embeddings may be irrelevant to the\nclass of interest. Therefore, it is necessary to explore ap-\nproaches that make the semantics of the patch embeddings\nrelevant to class of interest, in order to treat them as im-\nage representations. We propose to adapt patch embeddings\nwith class-aware embeddings to make them class-relevant.\nIt is to be noted that class token is a learnable embedding\nand plays a key role in ViTs, whose state at the output is\nserved as image representations [15]. Here, two often over-\nlooked facts need to be highlighted: 1) Before being input\ninto a ViT, class token is class-agnostic. 2) By constantly\ninteracting with patch embeddings in the ViT, class token\nbecomes class-aware and its final output state is treated\nas representations of the corresponding image. Figure 3\nshows the class-aware embedding visualization results of\nimages from different classes, which demonstrates that af-\nter interacting with images from different classes, the output\nstates of class-agnostic embedding are class-aware. These\nfacts suggest that the class-agnostic embedding may have a\nstrong generalization ability, which motivate us to introduce\nthe class-agnostic embedding and make it class-aware in a\nsimilar way1.\nPatch embedding adaptation.\nGiven an image, its\npatch embeddings and class-aware embedding can be ob-\ntained simultaneously. Two facts need to be highlighted:\n1) The semantics of these patch embeddings may be irrele-\nvant to the class to which the image belongs and the spatial\nlocation of patch embeddings relevant to the class of inter-\nest is unknown in advance. This inspires the exploration of\naligning semantically-relevant regions. Due to the inaccu-\nrate localization and alignment induced by large intra-class\nvariation and background clutter, the results of semantic\nalignment are far from satisfactory [24]. 2) The number of\npatch embeddings is usually considerable. Several methods\nhave directly treated patch embeddings as image represen-\ntations to alleviate the scarcity of labeled images [68, 37].\nAlthough promising performance improvements have been\nachieved, they suffer from irrelevant patch embeddings. We\ndeal with the above issues from a new perspective of adapt-\ning patch embeddings with the class-aware embedding to\nmake them class-relevant, which can be formulated as fol-\nlows:\n  \\\nb a r \n{ z }_{\ni}^{o } = z_{i}^{o} + \\lambda z_{class}^{o}\\,, \\label {eq:mix} \n(1)\nwhere ¯zo\ni and zo\ni respectively denote the i-th adapted patch\n1The connection between class-agnostic embedding and class-aware\nembedding is that they are class tokens at different stages of the model\nforward pass.\n18908\n",
    "Figure 4. Illustration of how the class-aware factor λ controls\nthe magnitude of class-awareness. Considering that the similarity\nmeasure used is insensitive to the norm of embeddings, the larger\nthe value of λ, the smaller the angle between the class-aware em-\nbedding and the adapted patch embedding, and the more relevant\nthe adapted patch embedding is to the class of interest.\nembedding and the i-th original patch embedding at the out-\nput of the projection head, zo\nclass denotes the class-aware\nembedding at the output of the projection head, and λ > 0\ndenotes the class-aware factor that controls the magnitude\nof the correlation.\nThe reason behind this design is as\nfollows.\nAlthough the labels of the patch embeddings,\ni.e., y(zo\ni ), are unknown, the class-aware embedding’s, i.e.,\ny(zo\nclass), is known. Therefore, the labels of the adapted\npatch embeddings, i.e., y(¯zo\ni ), can be formulated as follows:\n  y^{\n( \\ b ar {\nz } _ {i}^{\no})} =  y^{(z_{i}^{o})} + \\lambda y^{(z_{class}^{o})}\\,. \n(2)\nThis practice has achieved great success in mixup [72]. The\ndifference is that two known classes are mixed in mixup\nwhile a known class and an unknown class are mixed in our\nmethod. Note that both adapted patch embeddings and la-\nbels are l2 normalized before using, meaning that the larger\nthe value of λ, the more relevant the adapted patch embed-\ndings are to the class of interest, as shown in Figure 4.\n3.4. Dense similarity measure\nWithout loss of generality, we take a support image and a\nquery image as an example to show how to measure the sim-\nilarity between them. When designing the similarity mea-\nsure, two points need to be considered: 1) Since the num-\nber of class-relevant patch embeddings is considerable, it\nwould be better to use them all at the same time, in order\nto impose a strong constraint on the whole pipeline. 2) It\nwould be better to design a similarity measure that requires\nno domain expertise, in order to reduce the difficulty of de-\nployment in practical applications. To this end, we define\na dense score matrix S whose element is a score between\nadapted patch embeddings across images, which can be for-\nmulated as follows:\n  \\ m athcal {\nS\n} _{ij} \n=\n \n{d(\\bar {z}_{i}^{o(S)}, \\bar {z}_{i}^{o(Q)})}^{2}, \\label {eq:score} \n(3)\nwhere ¯zo(S)\ni\nand ¯zo(Q)\ni\nrespectively denote the adapted patch\nembeddings of the support image and the query image, and\nd(·, ·) denotes the cosine similarity. Then, we flatten the\ndense score matrix and directly input it into a MLP to out-\nput a similarity score. The reasons for these choices are as\nfollows: 1) All class-relevant patch embeddings are used,\nwhich helps alleviate the overfitting issue induced by the\nscarcity of labeled images. 2) The difficulty of choosing the\nright function from massive suitable functions is bypassed.\n3) The cosine function is insensitive to the norm of embed-\ndings, which avoids l2 normalization of Eq. (1). 4) The\nuse of the square term can facilitate the independence of the\nadapted embeddings between different classes.\n3.5. Training and inference\nTraining. There are K labeled images in the n-th sup-\nport class. After obtaining the similarity scores between the\ni-th query image and all support images, we can get the sim-\nilarity score of the i-th query image belonging to the n-th\nsupport class as follows:\n  s _\n{\nn\ni} \n= \\s um _{k=1}^{K} s_{nik}\\,, \n(4)\nwhere snik denotes the similarity score between the i-th\nquery image and the k-th support image in the n-th support\nclass. Then, the probability of the i-th query image belong-\ning to the n-th support class can be calculated as follows:\n  p _\n{ni} = \\\nfr\nac {\\exp (s\n_{ni})}{\\sum _{n=1}^{N} \\exp (s_{ni})}\\,. \\label {eq:prob} \n(5)\nFor a given episode, the loss function can be formulated as\nfollows:\n  L  \n= \n-\\\nf\nrac\n \n{\n1}{\nNQ}\\su\nm\n _{ i=1 }^{ NQ} \\sum _{n=1}^{N}I(y_i^{(Q)}=n) \\log p_{ni}\\,, \\label {eq:classification} \n(6)\nwhere y(Q)\ni\ndenotes the label of the i-th query image and\nI(·) is an indicator function that equals one if its arguments\nare true and zero otherwise. All the learnable weights in-\nvolved in our method are finetuned by minimizing Eq. (6)\nusing episodes randomly sampled from training classes.\nInference. Given an episode sampled from the unseen\ntest classes, the probability of a query image belonging to\neach class can be calculated according to Eq. (5). Then, we\nassign the label of the class with the maximum probabil-\nity to the corresponding query image. Note that once fine-\ntuned on the training classes, our method does not need any\nadjustments when generalizing to the unseen test classes,\nin contrast to FewTURE [26] that needs all images of an\nepisode’s support set together with their labels to learn the\nimportance for each individual patch token via online opti-\nmization at inference time, resulting in that our method is\nmuch faster than FewTURE in terms of inference speed.\n18909\n",
    "Model\nBackbone\n≈# Params\nminiImageNet\ntieredImageNet\n1-shot\n5-shot\n1-shot\n5-shot\nSetFeat [2]\nSetFeat-12\n12.3 M\n68.32±0.62\n82.71±0.46\n73.63±0.88\n87.59±0.57\nProtoNet [57]\nResNet-12\n12.4 M\n62.29±0.33\n79.46±0.48\n68.25±0.23\n84.01±0.56\nFEAT [69]\nResNet-12\n12.4 M\n66.78±0.20\n82.05±0.14\n70.80±0.23\n84.79±0.16\nDeepEMD [70]\nResNet-12\n12.4 M\n65.91±0.82\n82.41±0.56\n71.16±0.87\n86.03±0.58\nIEPT [73]\nResNet-12\n12.4 M\n67.05±0.44\n82.90±0.30\n72.24±0.50\n86.73±0.34\nMELR [17]\nResNet-12\n12.4 M\n67.40±0.43\n83.40±0.28\n72.14±0.51\n87.01±0.35\nFRN [63]\nResNet-12\n12.4 M\n66.45±0.19\n82.83±0.13\n72.06±0.22\n86.89±0.14\nCG [75]\nResNet-12\n12.4 M\n67.02±0.20\n82.32±0.14\n71.66±0.23\n85.50±0.15\nDMF [66]\nResNet-12\n12.4 M\n67.76±0.46\n82.71±0.31\n71.89±0.52\n85.96±0.35\nInfoPatch [39]\nResNet-12\n12.4 M\n67.67±0.45\n82.44±0.31\n-\n-\nBML [77]\nResNet-12\n12.4 M\n67.04±0.63\n83.63±0.29\n68.99±0.50\n85.49±0.34\nCNL [75]\nResNet-12\n12.4 M\n67.96±0.98\n83.36±0.51\n73.42±0.95\n87.72±0.75\nMeta-NVG [71]\nResNet-12\n12.4 M\n67.14±0.80\n83.82±0.51\n74.58±0.88\n86.73±0.61\nPAL [45]\nResNet-12\n12.4 M\n69.37±0.64\n84.40±0.44\n72.25±0.72\n86.95±0.47\nCOSOC [44]\nResNet-12\n12.4 M\n69.28±0.49\n85.16±0.42\n73.57±0.43\n87.57±0.10\nMeta DeepBDC [65]\nResNet-12\n12.4 M\n67.34±0.43\n84.46±0.28\n72.34±0.49\n87.31±0.32\nLEO [55]\nWRN-28-10\n36.5 M\n61.76±0.08\n77.59±0.12\n66.33±0.05\n81.44±0.09\nCC+rot [21]\nWRN-28-10\n36.5 M\n62.93±0.45\n79.87±0.33\n70.53±0.51\n84.98±0.36\nFEAT [69]\nWRN-28-10\n36.5 M\n65.10±0.20\n81.11±0.14\n70.41±0.23\n84.38±0.16\nPSST [10]\nWRN-28-10\n36.5 M\n64.16±0.44\n80.64±0.32\n-\n-\nMetaQDA [74]\nWRN-28-10\n36.5 M\n67.83±0.64\n84.28±0.69\n74.33±0.65\n89.56±0.79\nOM [52]\nWRN-28-10\n36.5 M\n66.78±0.30\n85.29±0.41\n71.54±0.29\n87.79±0.46\nFewTURE [26]\nViT-S/16\n22 M\n68.02±0.88\n84.51±0.53\n72.96±0.92\n86.43±0.67\nCPEA (ours)\nViT-S/16\n22 M\n71.97±0.65\n87.06±0.38\n76.93±0.70\n90.12±0.45\nTable 1. Few-shot classification accuracies for the 5-way 1-shot and 5-way 5-shot settings on miniImageNet and tieredImageNet. The\naverage accuracies with 95% confidence interval are reported according to the evaluation protocol.\n4. Experiments\nWe first detail the experimental settings and then com-\npare with the counterparts.\nFinally, we ablate the key\ncomponents. It is to be noted that more details regarding\ndatasets and ablation study are provided in the supplemen-\ntary material.\n4.1. Experimental settings\nDatasets.\nWe evaluate our method on four popular\nfew-shot classification benchmark datasets, i.e., miniIma-\ngeNet [61], tieredImageNet [53], CIFAR-FS [6], and FC-\n100 [51]. We follow the common practice of previous meth-\nods [69, 26] to split each dataset into training/validation/test\ndatasets. Their label spaces are disjoint, meaning that the\nclasses seen in the training set will not appear in the valida-\ntion/test set.\nBackbone. We use the ViT-S/16 [15] as the backbone.\nThe reason for this choice is that the number of parameters\nof ViT-S/16 is comparable to that of the backbones com-\nmonly used in the few-shot image classification task. The\nprojection head is a MLP and it has two layers with GELU\napplied to the first fully-connected layer and LayerNorm ap-\nplied to the second fully-connected layer. The MLP used to\naggregate the dense score matrix has two layers with GELU\napplied to the first fully-connected layer and its output fully-\nconnected layer is 1-d. The ViT-S/16 takes images with a\nresolution of 224×224 as input and the patch embeddings\nare 384-d.\nImplementation details. Our training procedure con-\nsists of two stages. In the first stage, we pretrain the ViT-\nS/16 by using the strategy proposed in [76] and sticking\nto the hyperparameter settings reported. Four A100 40G\nGPUs are used to pretrain the ViT-S/16 and the total num-\nber of training epochs is set to be 1,600. To match our com-\nputing resources, the batch size is set to be 512. It is to be\nnoted that only the training set of the corresponding dataset\nis used for pretraining. In the second stage, we finetune the\nwhole pipeline by minimizing Eq. (6). It is to be noted that\nonly episodes sampled from the training classes are used for\nfinetuning. The optimizer used is Adam [32]. The global\ninitial learning rate is set to be 0.001, which is halved ev-\nery 500 episodes, and the learning rate of the ViT-S/16 is\nalways kept to be one percent of the global learning rate.\nThe weight decay is set to be 0.001 and the total number of\nepisodes is set to be 10,000.\nEvaluation protocol. We report the performance on the\n5-way 1-shot and 5-way 5-shot image classification tasks,\nand the average accuracy of 1,000 episodes randomly sam-\npled from the test classes is taken as the final performance\nwith 15 query images per class.\n18910\n",
    "Model\nBackbone\n≈# Params\nCIFAR-FS\nFC100\n1-shot\n5-shot\n1-shot\n5-shot\nProtoNet [57]\nResNet-12\n12.4 M\n-\n-\n41.54±0.76\n57.08±0.76\nMetaOpt [35]\nResNet-12\n12.4 M\n72.00±0.70\n84.20±0.50\n41.10±0.60\n55.50±0.60\nMABAS [31]\nResNet-12\n12.4 M\n73.51±0.92\n85.65±0.65\n42.31±0.75\n58.16±0.78\nRFS [59]\nResNet-12\n12.4 M\n73.90±0.80\n86.90±0.50\n44.60±0.70\n60.90±0.60\nBML [77]\nResNet-12\n12.4 M\n73.45±0.47\n88.04±0.33\n-\n-\nCG [20]\nResNet-12\n12.4 M\n73.00±0.70\n85.80±0.50\n-\n-\nMeta-NVG [71]\nResNet-12\n12.4 M\n74.63±0.91\n86.45±0.59\n46.40±0.81\n61.33±0.71\nRENet [30]\nResNet-12\n12.4 M\n74.51±0.46\n86.60±0.32\n-\n-\nTPMN [64]\nResNet-12\n12.4 M\n75.50±0.90\n87.20±0.60\n46.93±0.71\n63.26±0.74\nMixFSL [1]\nResNet-12\n12.4 M\n-\n-\n44.89±0.63\n60.70±0.60\nCC+rot [21]\nWRN-28-10\n36.5 M\n73.62±0.31\n86.05±0.22\n-\n-\nPSST [10]\nWRN-28-10\n36.5 M\n77.02±0.38\n88.45±0.35\n-\n-\nMeta-QDA [74]\nWRN-28-10\n36.5 M\n75.83±0.88\n88.79±0.75\n-\n-\nFewTURE [26]\nViT-S/16\n22 M\n76.10±0.88\n86.14±0.64\n46.20±0.79\n63.14±0.73\nCPEA (ours)\nViT-S/16\n22 M\n77.82±0.66\n88.98±0.45\n47.24±0.58\n65.02±0.60\nTable 2. Few-shot classification accuracies for the 5-way 1-shot and 5-way 5-shot settings on CIFAR-FS and FC-100. The average\naccuracies with 95% confidence interval are reported according to the evaluation protocol.\nProjection head\n1-shot\n5-shot\n✓\n71.99±0.28\n87.01±0.17\n×\n70.74±0.29\n86.69±0.18\nTable 3. Impact of the projection head on the few-shot classifica-\ntion performance.\nClass-aware factor\n1-shot\n5-shot\nλ = 0.0\n70.40±0.66\n85.05±0.43\nλ = 0.5\n71.27±0.66\n86.40±0.40\nλ = 1.0\n71.93±0.66\n86.91±0.39\nλ = 2.0\n71.97±0.65\n87.06±0.38\nλ = 4.0\n71.80±0.65\n87.13±0.38\nλ = 8.0\n71.94±0.65\n87.22±0.38\nλ = 16.0\n71.85±0.65\n87.03±0.38\nTable 4. Impact of the class-aware factor on the few-shot classifi-\ncation performance.\n4.2. Comparison results\nTable 1 and Table 2 show the comparison results for the\n5-way 1-shot and 5-way 5-shot settings on four benchmark\ndatasets. It is to be noted that 1) CPEA outperforms the\ncounterparts by a large margin. 2) CPEA exceeds the se-\nmantic alignment-based methods by a noticeable margin,\nincluding FewTURE [26] and DeepEMD [70]. Moreover,\nCPEA beats the state-of-the-art semantic alignment-based\nmethod in terms of inference speed as shown in Table 9.\nThese observations demonstrate the effectiveness of class-\naware patch embedding adaptation.\n4.3. Ablation study\nThe key components of the proposed method are ablated\nand experiments are conducted on miniImageNet [61].\nProjection head. Since the projection head increases\nthe adaptability of embeddings to the few-shot image clas-\nChoices in Eq. (3)\n1-shot\n5-shot\nd(·, ·)2\n71.97±0.65\n87.06±0.38\nd(·, ·)\n71.09±0.65\n86.64±0.38\n|d(·, ·)|\n71.48±0.64\n86.49±0.41\n4 × |d(·, ·)|\n71.62±0.66\n86.34±0.41\nTable 5. Impact of choices in Eq. (3) on the few-shot classification\nperformance.\nPretraining strategy\n1-shot\n5-shot\nDeiT (supervised)\n28.58±0.46\n36.65±0.48\nDINO (self-supervision)\n70.65±0.64\n85.71±0.40\nMIM (self-supervision)\n71.97±0.65\n87.06±0.38\nTable 6. Impact of different pretraining strategies on the few-shot\nclassification performance.\nsification task, the performance is improved by 0.78% on\naverage, as shown in Table 3.\nClass-aware factor. λ = 0.0 means that the patch em-\nbeddings are directly treated as image representations. Ta-\nble 4 shows that adapting patch embeddings to make them\nclass-relevant improves performance by a noticeable mar-\ngin, demonstrating the effectiveness of class-aware patch\nembedding adaptation. Both the 1-shot performance and\nthe 5-shot performance are saturated after λ exceeds a cer-\ntain value, i.e., 1-shot: 2 and 5-shot: 8. Since there are\nmore diverse entities under the 5-shot setting, a larger λ is\nneeded to make all patch embeddings class-relevant. Con-\nsidering that the 5-shot performance improvement is small\nwhen λ > 2.0, the default class-aware factor is set to be 2.\nChoices in Eq. (3).\nTable 5 shows that both d(·, ·)2\nand |d(·, ·)| can boost performance. Also, scaling |d(·, ·)|\ndoesn’t bring further performance gains. Since the cosine\nvalues within the same class are larger than the others,\nsquaring enlarges the intra-class similarity while reducing\nthe inter-class similarity, thus achieving better performance.\n18911\n",
    "(a) Task 1 without CPEA\n(b) Task 2 without CPEA\n(c) Task 3 without CPEA\n(d) Task 4 without CPEA\n(e) Task 1 with CPEA\n(f) Task 2 with CPEA\n(g) Task 3 with CPEA\n(h) Task 4 with CPEA\nFigure 5. Patch embedding visualization of four randomly sampled 5-way 1-shot classification tasks with one query image per class. (a),\n(b), (c), and (d) show the visualization results without CPEA. (e), (f), (g), and (h) show the corresponding visualization results with CPEA.\nCPEA concentrates patch embeddings by class, thus making them class-relevant.\nClassifier\n5-shot\nPrototype (with Euclidean distance)\n82.80±0.59\nPrototype (with Cosine. distance)\n79.90±0.65\nLinear (optimized online)\n82.37±0.57\nFewTURE ( 0 steps)\n82.68±0.55\nFewTURE (20 steps)\n84.51±0.53\nCPEA (in a dense manner)\n87.06±0.38\nTable 7. Impact of different classifiers attached to the pretrained\nbackbone ViT-S/16 on the few-shot classification performance.\nNumber of class-agnostic\nembeddings\n1-shot\n5-shot\n1\n71.97±0.65\n87.06±0.38\n2\n71.43±0.65\n86.62±0.41\n3\n71.34±0.65\n86.72±0.40\n4\n70.57±0.67\n86.39±0.36\nTable 8. Impact of the number of class-agnostic embeddings on\nthe few-shot classification performance.\nPretraining strategies. Table 6 shows the impact of dif-\nferent pretraining strategies on the few-shot classification\nperformance. Supervision collapse exists in the supervised\npretraining, i.e., DeiT [60], thus leading to its poor gen-\neralization ability on the unseen test classes.\nDue to its\npatch-based nature, MIM [76] performs much better than\nDINO [7].\nClassifier. Table 7 shows the impact of different classi-\nfiers attached to the pretrained backbone ViT-S/16 on the\nfew-shot classification performance. The comparison re-\nsults demonstrate the superiority of CPEA.\nNumber of class-agnostic embeddings. Table 8 shows\nthe impact of the number of class-agnostic embeddings\nModel\nInference time [ms]\n5-shot\nFewTURE ( 0 steps)\n156.8±2.16\n82.68±0.59\nFewTURE (10 steps)\n162.1±2.11\n83.89±0.57\nFewTURE (20 steps)\n168.6±2.22\n84.51±0.53\nCPEA\n1.352±0.06\n87.06±0.38\nTable 9. Few-shot classification accuracy and inference time. In-\nference time are averaged over 1800 query images. An NVIDIA-\n2080TI is used for evaluation.\nImage resolution\n1-shot\n5-shot\n224 × 224\n71.97±0.65\n87.06±0.38\n384 × 384\n73.12±0.63\n87.60±0.39\n448 × 448\n73.29±0.63\n88.00±0.37\nTable 10. Impact of the image resolution on the few-shot classifi-\ncation performance.\non the few-shot classification performance. Increasing the\nnumber of class-agnostic embeddings does not improve per-\nformance. Therefore,we use one class-agnostic embedding\nby default.\nInference time. When generalizing to the unseen test\nclasses, our CPEA does not need any adjustments while\nFewTURE [26] needs to learn the token importance weight\nvia online optimization. As a result, our CPEA is much\nfaster than FewTURE in terms of inference speed, as shown\nin Table 9.\nFeature visualization. Figure 5 shows the patch embed-\nding visualization results of four randomly sampled 5-way\n1-shot classification tasks with/without CPEA. It can be ob-\nserved that with CPEA, the patch embeddings are clustered\nby class. This means that the patch embeddings are made\nclass-relevant by CPEA.\n18912\n",
    "Backbone\nModel\nminiImageNet\nCIFAR-FS\n1-shot 5-shot 1-shot 5-shot\nViT-S/16 [15] PMF [28]\n93.1\n98.0\n81.1\n92.5\nCPEA\n94.3\n98.3\n90.4\n96.3\nViT-B/16 [15] PMF [28]\n95.3\n98.4\n84.3\n92.2\nCPEA\n95.7\n98.7\n92.4\n96.6\nTable 11. Impact of the external data on the few-shot classification\nperformance.\nMetric\n1-shot\n5-shot\n∥· ∥2\n68.41±0.68\n84.45±0.44\n∥· ∥2 /dfeature\n71.53±0.67\n85.78±0.41\n∥· ∥2 /std\n62.37±0.62\n83.11±0.43\ncosine\n71.97±0.65\n87.06±0.38\nTable 12. Impact of using the euclidean metrics in Eq. (3) on the\nfew-shot classification performance.\nScalability in image resolution. It seems that our archi-\ntecture is tied to a certain image resolution. In fact, inserting\nadaptive average pooling with a fixed output size of 14×14\nallows us to increase image resolution without introducing\nadditional parameters in MLP. Also, this strategy brings no-\nticeable performance gains, as shown in Table 10.\nExternal data. PMF [28] uses external data to push the\nlimits of simple pipelines for few-shot image classification.\nWith the same experimental settings as PMF, CPEA signif-\nicantly outperforms PMF, as shown in Table 11. This ob-\nservation demonstrates the effectiveness of our method in\nusing external data.\n5. Discussion\nDespite the effectiveness of euclidean metrics and their\ngeneralizations in few-shot image classification [5, 19, 4],\ntwo major challenges still exist when using them. One is\nthe scale issue. Table 12 shows that it is nontrivial to find a\nsuitable scaling factor that outperforms the cosine similar-\nity. The other one is the inaccurate mean and covariance es-\ntimated with a limited number of labeled data. Our method\nmakes patch embeddings class-relevant, thus increasing the\namount of labeled data. The increased labeled data might\nenable us to accurately estimate the mean and covariance in\nfew-shot settings, which could improve the robustness and\nusability of the scale-invariant generalizations of euclidean\nmetrics such as Mahalanobis distance. This is a promising\ndirection worth exploring in the future.\n6. Conclusion\nIn this paper, we propose a Class-aware Patch Embed-\nding Adaptation (CPEA) method for few-shot image clas-\nsification. CPEA can eliminate the interference of single-\nlabel annotations and avoid supervision collapse without the\nneed for aligning semantically-relevant regions. The core\nof CPEA is to integrate patch embeddings with class-aware\nembeddings to make them class-relevant. To measure the\nsimilarity between paired images, we define a dense score\nmatrix between class-relevant patch embeddings, based on\nwhich a similarity score is aggregated. Visualization results\ndemonstrate that our CPEA makes patch embeddings class-\nrelevant. Extensive experiments on four benchmark datasets\nshow that our CPEA performs much better than the coun-\nterparts while achieving new state-of-the-art results.\nAcknowledgments\nThis work was supported in part by National Natural\nScience Foundation of China (62206268, U21A20487),\nShenzhen Technology Project (JCYJ20220818101206014,\nJCYJ20220818101211025,\nJCYJ20200109113416531),\nGuangdong Technology Project (2022B1515120067), and\nCAS Key Technology Talent Program.\nReferences\n[1] Arman Afrasiyabi, Jean-Franc¸ois Lalonde, and Christian\nGagn´e. Mixture-based feature space learning for few-shot\nimage classification. In ICCV, 2021. 7\n[2] Arman Afrasiyabi, Hugo Larochelle, Jean-Franc¸ois Lalonde,\nand Christian Gagn´e. Matching feature sets for few-shot im-\nage classification. In CVPR, 2022. 6\n[3] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEit:\nBERT pre-training of image transformers. In ICLR, 2022. 3,\n4\n[4] Peyman Bateni, Jarred Barber, Jan-Willem van de Meent,\nand Frank Wood. Enhancing few-shot image classification\nwith unlabelled examples. In WACV, 2022. 2, 9\n[5] Peyman Bateni, Raghav Goyal, Vaden Masrani, Frank Wood,\nand Leonid Sigal. Improved few-shot visual classification. In\nCVPR, 2020. 2, 9\n[6] Luca Bertinetto, Joao F Henriques, Philip Torr, and An-\ndrea Vedaldi. Meta-learning with differentiable closed-form\nsolvers. In ICLR, 2019. 6\n[7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers.\nIn\nICCV, 2021. 4, 8\n[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In ICML, 2020. 3\n[9] Xinlei Chen, Saining Xie, and Kaiming He.\nAn empiri-\ncal study of training self-supervised vision transformers. In\nICCV, 2021. 4\n[10] Zhengyu Chen, Jixie Ge, Heshen Zhan, Siteng Huang, and\nDonglin Wang. Pareto self-supervised training for few-shot\nlearning. In CVPR, 2021. 6, 7\n[11] Jun Cheng, Fusheng Hao, Fengxiang He, Liu Liu, and Qieshi\nZhang. Mixer-based semantic spread for few-shot learning.\nIEEE TMM, 2023. 2\n[12] Jun Cheng, Fusheng Hao, Liu Liu, and Dacheng Tao. Im-\nposing semantic consistency of local descriptors for few-shot\nlearning. IEEE TIP, 2022. 2\n18913\n",
    "[13] Tristan Deleu, David Kanaa, Leo Feng, Giancarlo Kerg,\nYoshua Bengio, Guillaume Lajoie, and Pierre-Luc Bacon.\nContinuous-time meta-learning with forward mode differen-\ntiation. In ICLR, 2022. 2\n[14] Carl Doersch, Ankush Gupta, and Andrew Zisserman.\nCrosstransformers: spatially-aware few-shot transfer.\nIn\nNeurIPS, 2020. 2, 3\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv:2010.11929,\n2020. 3, 4, 6, 9\n[16] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Ried-\nmiller, and Thomas Brox.\nDiscriminative unsupervised\nfeature learning with convolutional neural networks.\nIn\nNeurIPS, 2014. 3\n[17] Nanyi Fei, Zhiwu Lu, Tao Xiang, and Songfang Huang.\nMelr: Meta-learning via modeling episode-level relation-\nships for few-shot learning. In ICLR, 2020. 6\n[18] Chelsea Finn, Pieter Abbeel, and Sergey Levine.\nModel-\nagnostic meta-learning for fast adaptation of deep networks.\nIn ICML, 2017. 2\n[19] Stanislav Fort. Gaussian prototypical networks for few-shot\nlearning on omniglot. arXiv:1708.02735, 2017. 2, 9\n[20] Zhi Gao, Yuwei Wu, Yunde Jia, and Mehrtash Harandi. Cur-\nvature generation in curved spaces for few-shot learning. In\nICCV, 2021. 7\n[21] Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick\nP´erez, and Matthieu Cord. Boosting few-shot visual learning\nwith self-supervision. In ICCV, 2019. 6, 7\n[22] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un-\nsupervised representation learning by predicting image rota-\ntions. In ICLR, 2018. 3\n[23] Fusheng Hao, Fengxiang He, Jun Cheng, and Dacheng Tao.\nGlobal-local interplay in semantic alignment for few-shot\nlearning. IEEE TCSVT, 2022. 2\n[24] Fusheng Hao, Fengxiang He, Jun Cheng, Lei Wang,\nJianzhong Cao, and Dacheng Tao. Collect and select: Se-\nmantic alignment metric learning for few-shot learning. In\nICCV, 2019. 2, 4\n[25] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll´ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. arXiv:2111.06377, 2021. 3\n[26] Markus Hiller, Rongkai Ma, Mehrtash Harandi, and Tom\nDrummond. Rethinking generalization in few-shot classi-\nfication. In NeurIPS, 2022. 1, 2, 3, 4, 5, 6, 7, 8\n[27] Ruibing Hou, Hong Chang, Bingpeng Ma, Shiguang Shan,\nand Xilin Chen. Cross attention network for few-shot classi-\nfication. In NeurIPS, 2019. 2\n[28] Shell Xu Hu, Da Li, Jan St¨uhmer, Minyoung Kim, and Tim-\nothy M. Hospedales. Pushing the limits of simple pipelines\nfor few-shot learning: External data and fine-tuning make a\ndifference. In CVPR, 2022. 9\n[29] Adam Jelley, Amos Storkey, Antreas Antoniou, and Sam De-\nvlin. Contrastive meta-learning for partially observable few-\nshot learning. In ICLR, 2023. 2\n[30] Dahyun Kang, Heeseung Kwon, Juhong Min, and Minsu\nCho. Relational embedding for few-shot classification. In\nICCV, 2021. 7\n[31] Jaekyeom Kim, Hyoungseok Kim, and Gunhee Kim. Model-\nagnostic boundary-adversarial sampling for test-time gener-\nalization in few-shot learning. In ECCV, 2020. 7\n[32] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. ICLR, 2015. 6\n[33] Alex Krizhevsky and Geoffrey Hinton. Learning multiple\nlayers of features from tiny images. 2009. 1\n[34] Jinxiang Lai, Siqian Yang, Wenlong Liu, Yi Zeng, Zhongyi\nHuang, Wenlong Wu, Jun Liu, Bin-Bin Gao, and Chengjie\nWang. tsf: Transformer-based semantic filter for few-shot\nlearning. In ECCV, 2022. 3\n[35] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and\nStefano Soatto. Meta-learning with differentiable convex op-\ntimization. In CVPR, 2019. 7\n[36] SuBeen Lee, WonJun Moon, and Jae-Pil Heo. Task discrep-\nancy maximization for fine-grained few-shot classification.\nIn CVPR, 2022. 2\n[37] Wenbin Li, Jinglin Xu, Jing Huo, Lei Wang, Yang Gao, and\nJiebo Luo. Distribution consistency based covariance metric\nnetworks for few-shot learning. In AAAI, 2019. 4\n[38] Zhaowen Li, Zhiyang Chen, Fan Yang, Wei Li, Yousong\nZhu, Chaoyang Zhao, Rui Deng, Liwei Wu, Rui Zhao, Ming\nTang, et al. Mst: Masked self-supervised transformer for\nvisual representation. In NeurIPS, 2021. 3\n[39] Chen Liu, Yanwei Fu, Chengming Xu, Siqian Yang, Jilin Li,\nChengjie Wang, and Li Zhang. Learning a few-shot embed-\nding model with contrastive learning. In AAAI, 2021. 6\n[40] Yahui Liu, Enver Sangineto, Wei Bi, Nicu Sebe, Bruno\nLepri, and Marco Nadai. Efficient training of visual trans-\nformers with small datasets. In NeurIPS, 2021. 3\n[41] Yang Liu, Weifeng Zhang, Chao Xiang, Tu Zheng, Deng Cai,\nand Xiaofei He. Learning to affiliate: Mutual centralized\nlearning for few-shot classification. In CVPR, 2022. 2\n[42] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nICCV, 2021. 3\n[43] Yuning Lu, Liangjian Wen, Jianzhuang Liu, Yajing Liu,\nand Xinmei Tian. Self-supervision can be a good few-shot\nlearner. In ECCV, 2022. 3\n[44] Xu Luo, Longhui Wei, Liangjian Wen, Jinrong Yang, Lingxi\nXie, Zenglin Xu, and Qi Tian. Rectifying the shortcut learn-\ning of background for few-shot learning. In NeurIPS, 2021.\n6\n[45] Jiawei Ma, Hanchen Xie, Guangxing Han, Shih-Fu Chang,\nAram Galstyan, and Wael Abd-Almageed. Partner-assisted\nlearning for few-shot image classification. In ICCV, 2021. 6\n[46] Rongkai Ma, Pengfei Fang, Gil Avraham, Yan Zuo, Tom\nDrummond, and Mehrtash Harandi.\nLearning instance\nand task-aware dynamic kernels for few shot learning.\narXiv:2112.03494, 2021. 2\n[47] Rongkai Ma, Pengfei Fang, Tom Drummond, and Mehrtash\nHarandi. Adaptive poincar´e point to set distance for few-shot\nclassification. In AAAI, 2022. 2\n18914\n",
    "[48] Puneet Mangla, Nupur Kumari, Abhishek Sinha, Mayank\nSingh, Balaji Krishnamurthy, and Vineeth N Balasubrama-\nnian. Charting the right manifold: Manifold mixup for few-\nshot learning. In IEEE/CVF Winter Conference on Applica-\ntions of Computer Vision, 2020. 3\n[49] Alex Nichol, Joshua Achiam, and John Schulman. On first-\norder meta-learning algorithms. arXiv:1803.02999, 2018. 2\n[50] Jaehoon Oh, Hyungjun Yoo, ChangHwan Kim, and Se-\nYoung Yun. Boil: Towards representation change for few-\nshot learning. In ICLR, 2021. 2\n[51] Boris Oreshkin, Pau Rodr´ıguez L´opez, and Alexandre La-\ncoste. Tadam: Task dependent adaptive metric for improved\nfew-shot learning. In NeurIPS, 2018. 6\n[52] Guodong Qi, Huimin Yu, Zhaohui Lu, and Shuzhao Li.\nTransductive few-shot classification on the oblique manifold.\nIn ICCV, 2021. 6\n[53] Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell,\nKevin Swersky, Joshua B Tenenbaum, Hugo Larochelle, and\nRichard S Zemel. Meta-learning for semi-supervised few-\nshot classification. arXiv:1803.00676, 2018. 6\n[54] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al.\nImagenet large\nscale visual recognition challenge. International Journal of\nComputer Vision, 115(3):211–252, 2015. 1\n[55] Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol\nVinyals, Razvan Pascanu, Simon Osindero, and Raia Had-\nsell. Meta-learning with latent embedding optimization. In\nICLR, 2018. 2, 6\n[56] Christian Simon,\nPiotr Koniusz,\nRichard Nock,\nand\nMehrtash Harandi. Adaptive subspaces for few-shot learn-\ning. In CVPR, 2020. 2\n[57] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical\nnetworks for few-shot learning. In NeurIPS, 2017. 2, 6, 7\n[58] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS\nTorr, and Timothy M Hospedales. Learning to compare: Re-\nlation network for few-shot learning. In CVPR, 2018. 2\n[59] Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenen-\nbaum, and Phillip Isola. Rethinking few-shot image classifi-\ncation: a good embedding is all you need? In ECCV, 2020.\n7\n[60] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv´e J´egou. Training\ndata-efficient image transformers & distillation through at-\ntention. In ICML, 2021. 8\n[61] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan\nWierstra, et al. Matching networks for one shot learning.\nIn NeurIPS, 2016. 2, 3, 6, 7\n[62] Heng Wang, Tan Yue, Xiang Ye, Zihang He, Bohan Li, and\nYong Li. Revisit finetuning strategy for few-shot learning to\ntransfer the emdeddings. In ICLR, 2023. 2\n[63] Davis Wertheimer, Luming Tang, and Bharath Hariharan.\nFew-shot classification with feature map reconstruction net-\nworks. In CVPR, 2021. 6\n[64] Jiamin Wu, Tianzhu Zhang, Yongdong Zhang, and Feng Wu.\nTask-aware part mining network for few-shot learning. In\nICCV, 2021. 7\n[65] Jiangtao Xie, Fei Long, Jiaming Lv, Qilong Wang, and Pei-\nhua Li. Joint distribution matters: Deep brownian distance\ncovariance for few-shot classification. In CVPR, 2022. 6\n[66] Chengming Xu, Yanwei Fu, Chen Liu, Chengjie Wang, Jilin\nLi, Feiyue Huang, Li Zhang, and Xiangyang Xue. Learning\ndynamic alignment via meta-filter for few-shot learning. In\nCVPR, 2021. 6\n[67] Zhanyuan Yang, Jinghua Wang, and Yingying Zhu. Few-shot\nclassification with contrastive learning. In ECCV, 2022. 3\n[68] Lifchitz Yann, Avrithis Yannis, and Picard Sylvaine. Local\npropagation for few-shot learning. In ICPR, 2021. 4\n[69] Han-Jia Ye, Hexiang Hu, De-Chuan Zhan, and Fei Sha. Few-\nshot learning via embedding adaptation with set-to-set func-\ntions. In CVPR, 2020. 2, 3, 6\n[70] Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen.\nDeepemd: Few-shot image classification with differentiable\nearth mover’s distance and structured classifiers. In CVPR,\n2020. 2, 6, 7\n[71] Chi Zhang, Henghui Ding, Guosheng Lin, Ruibo Li,\nChanghu Wang, and Chunhua Shen. Meta navigator: Search\nfor a good adaptation policy for few-shot learning. In ICCV,\n2021. 6, 7\n[72] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. ICLR, 2018. 5\n[73] Manli Zhang, Jianhong Zhang, Zhiwu Lu, Tao Xiang,\nMingyu Ding, and Songfang Huang. Iept: Instance-level and\nepisode-level pretext tasks for few-shot learning. In ICLR,\n2020. 6\n[74] Xueting Zhang, Debin Meng, Henry Gouk, and Timothy M\nHospedales. Shallow bayesian meta learning for real-world\nfew-shot recognition. In ICCV, 2021. 6, 7\n[75] Jiabao Zhao, Yifan Yang, Xin Lin, Jing Yang, and Liang He.\nLooking wider for better adaptive representation in few-shot\nlearning. In AAAI, 2021. 6\n[76] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang\nXie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training\nwith online tokenizer. ICLR, 2022. 2, 3, 4, 6, 8\n[77] Ziqi Zhou, Xi Qiu, Jiangtao Xie, Jianan Wu, and Chi Zhang.\nBinocular mutual learning for improving few-shot classifica-\ntion. In ICCV, 2021. 6, 7\n[78] Luisa Zintgraf, Kyriacos Shiarli, Vitaly Kurin, Katja Hof-\nmann, and Shimon Whiteson. Fast context adaptation via\nmeta-learning. In ICML, 2019. 2\n18915\n"
  ],
  "full_text": "Class-Aware Patch Embedding Adaptation for Few-Shot Image Classification\nFusheng Hao1,2\nFengxiang He3\nLiu Liu4\nFuxiang Wu1,2\nDacheng Tao4\nJun Cheng1,2*\n1Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems,\nShenzhen Institute of Advanced Technology, Chinese Academy of Sciences, China\n2The Chinese University of Hong Kong, Hong Kong, China\n3AIAI, School of Informatics, University of Edinburgh, United Kingdom\n4School of Computer Science, Faculty of Engineering, The University of Sydney, Australia\nAbstract\n“A picture is worth a thousand words”, significantly be-\nyond mere a categorization. Accompanied by that, many\npatches of the image could have completely irrelevant\nmeanings with the categorization if they were indepen-\ndently observed. This could significantly reduce the effi-\nciency of a large family of few-shot learning algorithms,\nwhich have limited data and highly rely on the compari-\nson of image patches. To address this issue, we propose a\nClass-aware Patch Embedding Adaptation (CPEA) method\nto learn “class-aware embeddings” of the image patches.\nThe key idea of CPEA is to integrate patch embeddings with\nclass-aware embeddings to make them class-relevant. Fur-\nthermore, we define a dense score matrix between class-\nrelevant patch embeddings across images, based on which\nthe degree of similarity between paired images is quantified.\nVisualization results show that CPEA concentrates patch\nembeddings by class, thus making them class-relevant.\nExtensive experiments on four benchmark datasets, mini-\nImageNet, tieredImageNet, CIFAR-FS, and FC-100, indi-\ncate that our CPEA significantly outperforms the existing\nstate-of-the-art methods. The source code is available at\nhttps://github.com/FushengHao/CPEA.\n1. Introduction\nReal-world images are usually composed of many dif-\nferent entities, e.g., two oxen grazing surrounded by a barn,\na fence and trees as shown in Figure 1. Assigning a sin-\ngle annotation to each image that corresponds to only one\ntype of entity is a common practice to construct computer\nvision datasets, e.g., CIFAR [33] and ImageNet [54]. Such\nan annotation can only describe part of an image’ contents.\nThis is acceptable in many classification scenarios, because\nthe interference caused by other image contents can be mit-\n*Corresponding author (email: jun.cheng@siat.ac.cn).\nFigure 1. Illustration of multiple entities from different classes si-\nmultaneously existing in a real-world image. Despite being anno-\ntated as “ox”, the image contains entities of other classes, such as\n“fence”, “barn”, “tree”, etc. The core idea of CPEA is to learn\n“class-aware embeddings” of the image patches.\nigated by the use of a large number of labeled images.\nSpecifically, since each class contains a sufficient number\nof labeled images that vary greatly within the class and the\ncorresponding entities always appear in these images, deep\nmodels trained on such data tend to pay attention to the fre-\nquently occurring class-relevant entities (e.g., “ox” in Fig-\nure 1) while ignoring other irrelevant ones, especially those\nthat frequently appear across classes [26].\nBig challenges, however, arise in the context of few-shot\nimage classification, in which approaches are expected to\ncorrectly identify new classes that are disjoint with the train-\ning classes during the test phase, given only a few (e.g., one\nor five) labeled images for each of these new classes. The\nchallenges are as follows: 1) Due to the scarcity of labeled\nimages of new classes and the extremely limited number\nof class-relevant entities, it is very difficult for a model to\nidentify which entity determines the class of an image. 2)\nEntities contained in the training images but not covered by\nthe training classes may happen to be the ones expected to\nbe covered by the new classes at test time, which would in-\ntroduce ambiguity. 3) Specific patterns learned during the\ntraining phase may be overemphasized, but they may not\nThis ICCV paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n18905\n\n\nbe relevant to the new classes seen at test time, resulting in\nsupervision collapse [14] and limited generalizability.\nA promising solution is to align semantically-relevant\nregions [24, 27, 70, 14, 26].\nSAML [24] proposes to\nuse the activation-based attention to highlight semantically-\nrelevant regions while suppressing others. CAN [27] per-\nforms cross-attention between class prototypes and query\nfeature maps to highlight class-relevant regions.\nDeep-\nEMD [70] looks for the aligned regions by minimizing the\nearth movers’ distance. CTX [14] uses a Transformer-style\nattention mechanism to perform the spatial and semantic\nalignment and mitigates supervision collapse by incorpo-\nrating self-supervised learning in training. FewTURE [26]\ndetermines the most informative regions via online opti-\nmization and then uses them to reweigh patch correspon-\ndences. While these methods have shown great potential\nin eliminating interference and tackling supervision col-\nlapse, there still exist crucial drawbacks. Firstly, aligned\nsemantically-relevant regions are not always beneficial for\nsimilarity measure, such as those that are irrelevant to the\nclass of interest. Secondly, the scarcity of labeled images\nof new classes and the extremely limited number of class-\nrelevant entities makes it difficult to deal with the inaccurate\nlocalization and alignment induced by large intra-class vari-\nation and background clutter in real-world images.\nIn this paper, we deal with the above challenges from\na new perspective and propose a Class-aware Patch Em-\nbedding Adaptation (CPEA) method that can eliminate the\ninterference of single-label annotations without aligning\nsemantically-relevant regions while avoiding supervision\ncollapse. Specifically, we employ self-supervision pretrain-\ning instead of the supervised one to avoid supervision col-\nlapse, with Masked Image Modelling [76] as a pretext task,\nwhich yields semantically meaningful patch embeddings.\nSince the patch embeddings may be irrelevant to class of\ninterest, this leads to the need for aligning semantically-\nrelevant patches. We avoid the need for localization and\nalignment mechanisms by making patch embeddings class-\nrelevant. To this end, we introduce a class-agnostic embed-\nding and feed it into the transformer to interact with patch\nembeddings to make it class-aware. Then, patch embed-\ndings are adapted with the class-aware embeddings to make\nthem class-relevant, which alleviates the scarcity of labeled\nimages by increasing their amount. Furthermore, we define\na dense score matrix between class-relevant patch embed-\ndings across images, based on which the degree of similar-\nity between paired images is quantified.\nOur main contributions are summarized as follows: 1)\nWe deal with the interference caused by single-label an-\nnotations in few-shot settings from a new perspective and\ndemonstrate that the interference can be successfully miti-\ngated without the need for localization and alignment mech-\nanisms.\n2) We propose the CPEA, a novel method that\nmakes patch embeddings class-relevant and measures the\nsimilarity between class-relevant patch embeddings across\nimages in a dense manner, which improves transferability.\n3) Visualizations show that our CPEA makes patch embed-\ndings class-relevant. Extensive experiments are conducted\non four popular benchmark datasets and the results indi-\ncate that our CPEA achieves superior performance over the\nstate-of-the-art methods.\n2. Related work\nFew-shot image classification. Few-shot image classi-\nfication has recently attracted much attention because of its\ngreat application prospects in real-world scenarios. Exist-\ning methods can be roughly categorized into two groups.\nThe first group is optimization-based methods. They learn\na meta-learner, which can optimize a learner in a few steps\ngiven the few labeled images.\nFor example, the meta-\nlearner of MAML [18] and Reptile [49] output a set of good\nmodel initializations that can be adapted to a specific few-\nshot classification task in a few gradient steps. To avoid\nthe large computational overhead caused by the need to up-\ndate all model parameters during inference, CAVIA [78]\nand LEO [55] propose to perform meta-learning in a low-\ndimensional representation space.\nThen, it is found that\nboth the choice of network architectures and the design of\nmeta-learners have a severe impact on the performance and\nefficiency, which motivates the exploration of various vari-\nants of meta-learning methods [13, 50, 29].\nThe second group is metric-based methods. They fo-\ncus on learning a feature space suitable for all few-shot\nclassification tasks, in which an appropriate distance func-\ntion is used for similarity measure. For example, Match-\ningNet [61] constructs a feature space based on neural net-\nworks, where the cosine distance is used for similarity mea-\nsure. ProtoNet [57] learns a feature space, where the Eu-\nclidean distance is used for similarity measure. Then, it is\nfound that the choice of network architectures, the choice of\ndistance functions, the choice of prototypes, and the choice\nof training strategies all have a severe impact on the perfor-\nmance and efficiency, which motivates the exploration of\nvarious variants of metric-based methods [46, 47, 56, 69,\n36, 62, 4, 5, 19]. Recently, local feature-based methods\nhave achieved great success in addressing the challenging\nfew-shot image classification problem. One line of such\nmethods directly treat local features as image representa-\ntions [58, 41, 12, 11] and the other line is to align the se-\nmantically relevant local features [24, 27, 70, 14, 26, 23].\nOur method makes local features class-relevant, thus avoid-\ning the need for localization and alignment mechanisms and\nincreasing the usability of local features.\nSelf-supervision in few-shot image classification. Al-\nthough self-supervision methods have achieved great suc-\ncess on large-scale image datasets, their potential has not\n18906\n\n\nFigure 2. Overview of the proposed class-aware patch embedding adaptation method. The ViT is pretrained with Masked Image Mod-\nelling [76] as a pretext task. The class-agnostic embedding is a learnable embedding and made class-aware by constantly interacting with\npatch embeddings in the ViT. Then, the patch embeddings at the output are adapted with the class-aware embeddings to make them class-\nrelevant. Finally, the similarity score between paired images is obtained by aggregating the dense score matrix. Note that we distinguish\npatch embeddings by numbers and M denotes the number of patch embeddings.\nbeen fully explored in the few-shot settings. Recent works\nhave shown that they can help improve the generalization\nability of learned models [48, 14, 26, 43, 67]. For example,\nS2M2 [48] integrates two self-supervision methods, i.e., ro-\ntation [22] and exemplars [16], with the standard supervised\nlearning to improve the generalization ability of output fea-\ntures of pre-trained models.\nCTX [14] integrates Sim-\nCLR [8] into the episodic training strategy to improve the\ngeneralization ability of the learned model. FewTURE [26]\nuses Masked Image Modelling [3, 38, 25, 76] as a pretext\ntask to pretrain the Vision Transformer (ViT) [15] on small-\nscale datasets, resulting in features with strong generaliza-\ntion ability.\nViT in few-shot image classification.\nDue to their\nability to build long-range dependencies between image\npatches, ViTs have achieved great success in many appli-\ncation fields of computer vision such as image classifica-\ntion [15, 42] and object detection [42]. However, they rely\nmore heavily on large-scale image datasets than Convolu-\ntional Neural Networks (CNNs) due to the lack of the con-\nvolutional inductive bias [40]. For example, ViTs have to\nlearn from images the locality and the translation invari-\nance embedded in the CNN design. This data-hungry na-\nture makes it difficult for ViTs to be used as a whole in the\nfew-shot settings, or only a very tiny part of ViTs (e.g., s\nsingle Transformer head) can be used in conjunction with\nCNNs [14, 69, 34]. Recently, FewTURE [26] demonstrates\nthat a fully ViT-based architecture can be successfully gen-\neralized on small-scale image datasets.\n3. Method\nWe first formulate the definition of few-shot image\nclassification and then present an overview of the whole\npipeline. Next, we detail the class-aware patch embedding\nadaptation and the dense similarity measure. Finally, we\ndescribe the training and inference strategies.\n3.1. Problem definition\nFew-shot image classification focuses on generalizing\nthe knowledge learned on the training classes Ctrain to the\nunseen test classes Ctest, i.e., Ctrain ∩Ctest = ∅, given\nonly a few labeled images for each of these test classes.\nWe follow the common practice of previous works [61]\nto formulate the N-way K-shot classification task in an\nepisodic manner, where N denotes the number of classes\nand K denotes the number of labeled images contained\nin each class.\nAn episode is composed of a support set\nXs = {(xi, yi)}NK\ni=1 and a query set Xq = {(xi, yi)}NQ\ni=1,\nwhere Q denotes the number of test images contained in\neach class. The query set is used to evaluate the perfor-\nmance of a model on the few-shot classification task defined\nby the support set. Our goal is to learn a model on training\nclasses that generalizes well on episodes randomly sampled\nfrom the unseen test classes within the inductive framework.\n18907\n\n\n(a)\n(b)\nFigure 3. Class-aware embedding visualization.\nTwo different\nsampling results are given in (a) and (b), respectively, with 100\nclass-aware embeddings per class. The class-agnostic embedding\nis denoted by the black “diamond”. After interacting with images\nfrom different classes, the output states of class-agnostic embed-\nding are class-aware.\n3.2. Overview\nFigure 2 shows the pipeline of the proposed method.\nWe decompose an image into patches, embed them using\nlinear projection, add position embeddings to the resulting\npatch embeddings, and prepend a class-agnostic embedding\nto the sequence of patch embeddings, which are then fed\ninto a standard Transformer encoder. Before performing\nclass-aware patch embedding adaptation, we add a projec-\ntion head to further transform the embeddings at the out-\nput of the Transformer encoder, with aim of increasing their\nadaptability to the few-shot classification task. Afterwards,\nthe class-aware embeddings are used to adapt patch embed-\ndings to make them class-relevant. To quantify the degree of\nsimilarity between paired images, we define a dense score\nmatrix between class-relevant patch embeddings, based on\nwhich a MLP is used to aggregate the dense score matrix\ninto a single similarity score.\n3.3. Class-aware patch embedding adaptation\nSelf-supervision pretraining. Single-label annotations\ncause supervision collapse in few-shot settings, which high-\nlights certain patterns that are useful for distinguishing\ntraining classes, rather than those that have good trans-\nferability [26].\nTo address this issue, we employ self-\nsupervision pretraining instead of the supervised one to\npretrain the ViT. Specifically, we decompose an image\ninto patches, randomly mask some patches, encode these\npatches with a ViT and reconstruct the masked patches,\nwhere Masked Image Modeling (MIM) [3, 76] is used as\na pretext task. The reasons for this are two-fold: 1) Labeled\nimages are scarce in few-shot settings and a considerable\namount of patch embeddings can be obtained in a single\nfeedforward. To facilitate making patch embeddings class-\nrelevant, it is necessary to yield semantically meaningful\npatch embeddings. 2) Different from the self-supervision\napproaches [7, 9] that focus on self-similarity of global rep-\nresentations between images from different views, MIM\naims to reconstruct the masked patches and build an under-\nstanding of the structure and content of an image, rather\nthan learning patterns that are mainly useful for training\nclasses, which improves transferability.\nClass-aware embedding. After pretraining, a consid-\nerable number of patch embeddings can be obtained for\nan input image in a single feedforward. However, the se-\nmantics of these patch embeddings may be irrelevant to the\nclass of interest. Therefore, it is necessary to explore ap-\nproaches that make the semantics of the patch embeddings\nrelevant to class of interest, in order to treat them as im-\nage representations. We propose to adapt patch embeddings\nwith class-aware embeddings to make them class-relevant.\nIt is to be noted that class token is a learnable embedding\nand plays a key role in ViTs, whose state at the output is\nserved as image representations [15]. Here, two often over-\nlooked facts need to be highlighted: 1) Before being input\ninto a ViT, class token is class-agnostic. 2) By constantly\ninteracting with patch embeddings in the ViT, class token\nbecomes class-aware and its final output state is treated\nas representations of the corresponding image. Figure 3\nshows the class-aware embedding visualization results of\nimages from different classes, which demonstrates that af-\nter interacting with images from different classes, the output\nstates of class-agnostic embedding are class-aware. These\nfacts suggest that the class-agnostic embedding may have a\nstrong generalization ability, which motivate us to introduce\nthe class-agnostic embedding and make it class-aware in a\nsimilar way1.\nPatch embedding adaptation.\nGiven an image, its\npatch embeddings and class-aware embedding can be ob-\ntained simultaneously. Two facts need to be highlighted:\n1) The semantics of these patch embeddings may be irrele-\nvant to the class to which the image belongs and the spatial\nlocation of patch embeddings relevant to the class of inter-\nest is unknown in advance. This inspires the exploration of\naligning semantically-relevant regions. Due to the inaccu-\nrate localization and alignment induced by large intra-class\nvariation and background clutter, the results of semantic\nalignment are far from satisfactory [24]. 2) The number of\npatch embeddings is usually considerable. Several methods\nhave directly treated patch embeddings as image represen-\ntations to alleviate the scarcity of labeled images [68, 37].\nAlthough promising performance improvements have been\nachieved, they suffer from irrelevant patch embeddings. We\ndeal with the above issues from a new perspective of adapt-\ning patch embeddings with the class-aware embedding to\nmake them class-relevant, which can be formulated as fol-\nlows:\n  \\\nb a r \n{ z }_{\ni}^{o } = z_{i}^{o} + \\lambda z_{class}^{o}\\,, \\label {eq:mix} \n(1)\nwhere ¯zo\ni and zo\ni respectively denote the i-th adapted patch\n1The connection between class-agnostic embedding and class-aware\nembedding is that they are class tokens at different stages of the model\nforward pass.\n18908\n\n\nFigure 4. Illustration of how the class-aware factor λ controls\nthe magnitude of class-awareness. Considering that the similarity\nmeasure used is insensitive to the norm of embeddings, the larger\nthe value of λ, the smaller the angle between the class-aware em-\nbedding and the adapted patch embedding, and the more relevant\nthe adapted patch embedding is to the class of interest.\nembedding and the i-th original patch embedding at the out-\nput of the projection head, zo\nclass denotes the class-aware\nembedding at the output of the projection head, and λ > 0\ndenotes the class-aware factor that controls the magnitude\nof the correlation.\nThe reason behind this design is as\nfollows.\nAlthough the labels of the patch embeddings,\ni.e., y(zo\ni ), are unknown, the class-aware embedding’s, i.e.,\ny(zo\nclass), is known. Therefore, the labels of the adapted\npatch embeddings, i.e., y(¯zo\ni ), can be formulated as follows:\n  y^{\n( \\ b ar {\nz } _ {i}^{\no})} =  y^{(z_{i}^{o})} + \\lambda y^{(z_{class}^{o})}\\,. \n(2)\nThis practice has achieved great success in mixup [72]. The\ndifference is that two known classes are mixed in mixup\nwhile a known class and an unknown class are mixed in our\nmethod. Note that both adapted patch embeddings and la-\nbels are l2 normalized before using, meaning that the larger\nthe value of λ, the more relevant the adapted patch embed-\ndings are to the class of interest, as shown in Figure 4.\n3.4. Dense similarity measure\nWithout loss of generality, we take a support image and a\nquery image as an example to show how to measure the sim-\nilarity between them. When designing the similarity mea-\nsure, two points need to be considered: 1) Since the num-\nber of class-relevant patch embeddings is considerable, it\nwould be better to use them all at the same time, in order\nto impose a strong constraint on the whole pipeline. 2) It\nwould be better to design a similarity measure that requires\nno domain expertise, in order to reduce the difficulty of de-\nployment in practical applications. To this end, we define\na dense score matrix S whose element is a score between\nadapted patch embeddings across images, which can be for-\nmulated as follows:\n  \\ m athcal {\nS\n} _{ij} \n=\n \n{d(\\bar {z}_{i}^{o(S)}, \\bar {z}_{i}^{o(Q)})}^{2}, \\label {eq:score} \n(3)\nwhere ¯zo(S)\ni\nand ¯zo(Q)\ni\nrespectively denote the adapted patch\nembeddings of the support image and the query image, and\nd(·, ·) denotes the cosine similarity. Then, we flatten the\ndense score matrix and directly input it into a MLP to out-\nput a similarity score. The reasons for these choices are as\nfollows: 1) All class-relevant patch embeddings are used,\nwhich helps alleviate the overfitting issue induced by the\nscarcity of labeled images. 2) The difficulty of choosing the\nright function from massive suitable functions is bypassed.\n3) The cosine function is insensitive to the norm of embed-\ndings, which avoids l2 normalization of Eq. (1). 4) The\nuse of the square term can facilitate the independence of the\nadapted embeddings between different classes.\n3.5. Training and inference\nTraining. There are K labeled images in the n-th sup-\nport class. After obtaining the similarity scores between the\ni-th query image and all support images, we can get the sim-\nilarity score of the i-th query image belonging to the n-th\nsupport class as follows:\n  s _\n{\nn\ni} \n= \\s um _{k=1}^{K} s_{nik}\\,, \n(4)\nwhere snik denotes the similarity score between the i-th\nquery image and the k-th support image in the n-th support\nclass. Then, the probability of the i-th query image belong-\ning to the n-th support class can be calculated as follows:\n  p _\n{ni} = \\\nfr\nac {\\exp (s\n_{ni})}{\\sum _{n=1}^{N} \\exp (s_{ni})}\\,. \\label {eq:prob} \n(5)\nFor a given episode, the loss function can be formulated as\nfollows:\n  L  \n= \n-\\\nf\nrac\n \n{\n1}{\nNQ}\\su\nm\n _{ i=1 }^{ NQ} \\sum _{n=1}^{N}I(y_i^{(Q)}=n) \\log p_{ni}\\,, \\label {eq:classification} \n(6)\nwhere y(Q)\ni\ndenotes the label of the i-th query image and\nI(·) is an indicator function that equals one if its arguments\nare true and zero otherwise. All the learnable weights in-\nvolved in our method are finetuned by minimizing Eq. (6)\nusing episodes randomly sampled from training classes.\nInference. Given an episode sampled from the unseen\ntest classes, the probability of a query image belonging to\neach class can be calculated according to Eq. (5). Then, we\nassign the label of the class with the maximum probabil-\nity to the corresponding query image. Note that once fine-\ntuned on the training classes, our method does not need any\nadjustments when generalizing to the unseen test classes,\nin contrast to FewTURE [26] that needs all images of an\nepisode’s support set together with their labels to learn the\nimportance for each individual patch token via online opti-\nmization at inference time, resulting in that our method is\nmuch faster than FewTURE in terms of inference speed.\n18909\n\n\nModel\nBackbone\n≈# Params\nminiImageNet\ntieredImageNet\n1-shot\n5-shot\n1-shot\n5-shot\nSetFeat [2]\nSetFeat-12\n12.3 M\n68.32±0.62\n82.71±0.46\n73.63±0.88\n87.59±0.57\nProtoNet [57]\nResNet-12\n12.4 M\n62.29±0.33\n79.46±0.48\n68.25±0.23\n84.01±0.56\nFEAT [69]\nResNet-12\n12.4 M\n66.78±0.20\n82.05±0.14\n70.80±0.23\n84.79±0.16\nDeepEMD [70]\nResNet-12\n12.4 M\n65.91±0.82\n82.41±0.56\n71.16±0.87\n86.03±0.58\nIEPT [73]\nResNet-12\n12.4 M\n67.05±0.44\n82.90±0.30\n72.24±0.50\n86.73±0.34\nMELR [17]\nResNet-12\n12.4 M\n67.40±0.43\n83.40±0.28\n72.14±0.51\n87.01±0.35\nFRN [63]\nResNet-12\n12.4 M\n66.45±0.19\n82.83±0.13\n72.06±0.22\n86.89±0.14\nCG [75]\nResNet-12\n12.4 M\n67.02±0.20\n82.32±0.14\n71.66±0.23\n85.50±0.15\nDMF [66]\nResNet-12\n12.4 M\n67.76±0.46\n82.71±0.31\n71.89±0.52\n85.96±0.35\nInfoPatch [39]\nResNet-12\n12.4 M\n67.67±0.45\n82.44±0.31\n-\n-\nBML [77]\nResNet-12\n12.4 M\n67.04±0.63\n83.63±0.29\n68.99±0.50\n85.49±0.34\nCNL [75]\nResNet-12\n12.4 M\n67.96±0.98\n83.36±0.51\n73.42±0.95\n87.72±0.75\nMeta-NVG [71]\nResNet-12\n12.4 M\n67.14±0.80\n83.82±0.51\n74.58±0.88\n86.73±0.61\nPAL [45]\nResNet-12\n12.4 M\n69.37±0.64\n84.40±0.44\n72.25±0.72\n86.95±0.47\nCOSOC [44]\nResNet-12\n12.4 M\n69.28±0.49\n85.16±0.42\n73.57±0.43\n87.57±0.10\nMeta DeepBDC [65]\nResNet-12\n12.4 M\n67.34±0.43\n84.46±0.28\n72.34±0.49\n87.31±0.32\nLEO [55]\nWRN-28-10\n36.5 M\n61.76±0.08\n77.59±0.12\n66.33±0.05\n81.44±0.09\nCC+rot [21]\nWRN-28-10\n36.5 M\n62.93±0.45\n79.87±0.33\n70.53±0.51\n84.98±0.36\nFEAT [69]\nWRN-28-10\n36.5 M\n65.10±0.20\n81.11±0.14\n70.41±0.23\n84.38±0.16\nPSST [10]\nWRN-28-10\n36.5 M\n64.16±0.44\n80.64±0.32\n-\n-\nMetaQDA [74]\nWRN-28-10\n36.5 M\n67.83±0.64\n84.28±0.69\n74.33±0.65\n89.56±0.79\nOM [52]\nWRN-28-10\n36.5 M\n66.78±0.30\n85.29±0.41\n71.54±0.29\n87.79±0.46\nFewTURE [26]\nViT-S/16\n22 M\n68.02±0.88\n84.51±0.53\n72.96±0.92\n86.43±0.67\nCPEA (ours)\nViT-S/16\n22 M\n71.97±0.65\n87.06±0.38\n76.93±0.70\n90.12±0.45\nTable 1. Few-shot classification accuracies for the 5-way 1-shot and 5-way 5-shot settings on miniImageNet and tieredImageNet. The\naverage accuracies with 95% confidence interval are reported according to the evaluation protocol.\n4. Experiments\nWe first detail the experimental settings and then com-\npare with the counterparts.\nFinally, we ablate the key\ncomponents. It is to be noted that more details regarding\ndatasets and ablation study are provided in the supplemen-\ntary material.\n4.1. Experimental settings\nDatasets.\nWe evaluate our method on four popular\nfew-shot classification benchmark datasets, i.e., miniIma-\ngeNet [61], tieredImageNet [53], CIFAR-FS [6], and FC-\n100 [51]. We follow the common practice of previous meth-\nods [69, 26] to split each dataset into training/validation/test\ndatasets. Their label spaces are disjoint, meaning that the\nclasses seen in the training set will not appear in the valida-\ntion/test set.\nBackbone. We use the ViT-S/16 [15] as the backbone.\nThe reason for this choice is that the number of parameters\nof ViT-S/16 is comparable to that of the backbones com-\nmonly used in the few-shot image classification task. The\nprojection head is a MLP and it has two layers with GELU\napplied to the first fully-connected layer and LayerNorm ap-\nplied to the second fully-connected layer. The MLP used to\naggregate the dense score matrix has two layers with GELU\napplied to the first fully-connected layer and its output fully-\nconnected layer is 1-d. The ViT-S/16 takes images with a\nresolution of 224×224 as input and the patch embeddings\nare 384-d.\nImplementation details. Our training procedure con-\nsists of two stages. In the first stage, we pretrain the ViT-\nS/16 by using the strategy proposed in [76] and sticking\nto the hyperparameter settings reported. Four A100 40G\nGPUs are used to pretrain the ViT-S/16 and the total num-\nber of training epochs is set to be 1,600. To match our com-\nputing resources, the batch size is set to be 512. It is to be\nnoted that only the training set of the corresponding dataset\nis used for pretraining. In the second stage, we finetune the\nwhole pipeline by minimizing Eq. (6). It is to be noted that\nonly episodes sampled from the training classes are used for\nfinetuning. The optimizer used is Adam [32]. The global\ninitial learning rate is set to be 0.001, which is halved ev-\nery 500 episodes, and the learning rate of the ViT-S/16 is\nalways kept to be one percent of the global learning rate.\nThe weight decay is set to be 0.001 and the total number of\nepisodes is set to be 10,000.\nEvaluation protocol. We report the performance on the\n5-way 1-shot and 5-way 5-shot image classification tasks,\nand the average accuracy of 1,000 episodes randomly sam-\npled from the test classes is taken as the final performance\nwith 15 query images per class.\n18910\n\n\nModel\nBackbone\n≈# Params\nCIFAR-FS\nFC100\n1-shot\n5-shot\n1-shot\n5-shot\nProtoNet [57]\nResNet-12\n12.4 M\n-\n-\n41.54±0.76\n57.08±0.76\nMetaOpt [35]\nResNet-12\n12.4 M\n72.00±0.70\n84.20±0.50\n41.10±0.60\n55.50±0.60\nMABAS [31]\nResNet-12\n12.4 M\n73.51±0.92\n85.65±0.65\n42.31±0.75\n58.16±0.78\nRFS [59]\nResNet-12\n12.4 M\n73.90±0.80\n86.90±0.50\n44.60±0.70\n60.90±0.60\nBML [77]\nResNet-12\n12.4 M\n73.45±0.47\n88.04±0.33\n-\n-\nCG [20]\nResNet-12\n12.4 M\n73.00±0.70\n85.80±0.50\n-\n-\nMeta-NVG [71]\nResNet-12\n12.4 M\n74.63±0.91\n86.45±0.59\n46.40±0.81\n61.33±0.71\nRENet [30]\nResNet-12\n12.4 M\n74.51±0.46\n86.60±0.32\n-\n-\nTPMN [64]\nResNet-12\n12.4 M\n75.50±0.90\n87.20±0.60\n46.93±0.71\n63.26±0.74\nMixFSL [1]\nResNet-12\n12.4 M\n-\n-\n44.89±0.63\n60.70±0.60\nCC+rot [21]\nWRN-28-10\n36.5 M\n73.62±0.31\n86.05±0.22\n-\n-\nPSST [10]\nWRN-28-10\n36.5 M\n77.02±0.38\n88.45±0.35\n-\n-\nMeta-QDA [74]\nWRN-28-10\n36.5 M\n75.83±0.88\n88.79±0.75\n-\n-\nFewTURE [26]\nViT-S/16\n22 M\n76.10±0.88\n86.14±0.64\n46.20±0.79\n63.14±0.73\nCPEA (ours)\nViT-S/16\n22 M\n77.82±0.66\n88.98±0.45\n47.24±0.58\n65.02±0.60\nTable 2. Few-shot classification accuracies for the 5-way 1-shot and 5-way 5-shot settings on CIFAR-FS and FC-100. The average\naccuracies with 95% confidence interval are reported according to the evaluation protocol.\nProjection head\n1-shot\n5-shot\n✓\n71.99±0.28\n87.01±0.17\n×\n70.74±0.29\n86.69±0.18\nTable 3. Impact of the projection head on the few-shot classifica-\ntion performance.\nClass-aware factor\n1-shot\n5-shot\nλ = 0.0\n70.40±0.66\n85.05±0.43\nλ = 0.5\n71.27±0.66\n86.40±0.40\nλ = 1.0\n71.93±0.66\n86.91±0.39\nλ = 2.0\n71.97±0.65\n87.06±0.38\nλ = 4.0\n71.80±0.65\n87.13±0.38\nλ = 8.0\n71.94±0.65\n87.22±0.38\nλ = 16.0\n71.85±0.65\n87.03±0.38\nTable 4. Impact of the class-aware factor on the few-shot classifi-\ncation performance.\n4.2. Comparison results\nTable 1 and Table 2 show the comparison results for the\n5-way 1-shot and 5-way 5-shot settings on four benchmark\ndatasets. It is to be noted that 1) CPEA outperforms the\ncounterparts by a large margin. 2) CPEA exceeds the se-\nmantic alignment-based methods by a noticeable margin,\nincluding FewTURE [26] and DeepEMD [70]. Moreover,\nCPEA beats the state-of-the-art semantic alignment-based\nmethod in terms of inference speed as shown in Table 9.\nThese observations demonstrate the effectiveness of class-\naware patch embedding adaptation.\n4.3. Ablation study\nThe key components of the proposed method are ablated\nand experiments are conducted on miniImageNet [61].\nProjection head. Since the projection head increases\nthe adaptability of embeddings to the few-shot image clas-\nChoices in Eq. (3)\n1-shot\n5-shot\nd(·, ·)2\n71.97±0.65\n87.06±0.38\nd(·, ·)\n71.09±0.65\n86.64±0.38\n|d(·, ·)|\n71.48±0.64\n86.49±0.41\n4 × |d(·, ·)|\n71.62±0.66\n86.34±0.41\nTable 5. Impact of choices in Eq. (3) on the few-shot classification\nperformance.\nPretraining strategy\n1-shot\n5-shot\nDeiT (supervised)\n28.58±0.46\n36.65±0.48\nDINO (self-supervision)\n70.65±0.64\n85.71±0.40\nMIM (self-supervision)\n71.97±0.65\n87.06±0.38\nTable 6. Impact of different pretraining strategies on the few-shot\nclassification performance.\nsification task, the performance is improved by 0.78% on\naverage, as shown in Table 3.\nClass-aware factor. λ = 0.0 means that the patch em-\nbeddings are directly treated as image representations. Ta-\nble 4 shows that adapting patch embeddings to make them\nclass-relevant improves performance by a noticeable mar-\ngin, demonstrating the effectiveness of class-aware patch\nembedding adaptation. Both the 1-shot performance and\nthe 5-shot performance are saturated after λ exceeds a cer-\ntain value, i.e., 1-shot: 2 and 5-shot: 8. Since there are\nmore diverse entities under the 5-shot setting, a larger λ is\nneeded to make all patch embeddings class-relevant. Con-\nsidering that the 5-shot performance improvement is small\nwhen λ > 2.0, the default class-aware factor is set to be 2.\nChoices in Eq. (3).\nTable 5 shows that both d(·, ·)2\nand |d(·, ·)| can boost performance. Also, scaling |d(·, ·)|\ndoesn’t bring further performance gains. Since the cosine\nvalues within the same class are larger than the others,\nsquaring enlarges the intra-class similarity while reducing\nthe inter-class similarity, thus achieving better performance.\n18911\n\n\n(a) Task 1 without CPEA\n(b) Task 2 without CPEA\n(c) Task 3 without CPEA\n(d) Task 4 without CPEA\n(e) Task 1 with CPEA\n(f) Task 2 with CPEA\n(g) Task 3 with CPEA\n(h) Task 4 with CPEA\nFigure 5. Patch embedding visualization of four randomly sampled 5-way 1-shot classification tasks with one query image per class. (a),\n(b), (c), and (d) show the visualization results without CPEA. (e), (f), (g), and (h) show the corresponding visualization results with CPEA.\nCPEA concentrates patch embeddings by class, thus making them class-relevant.\nClassifier\n5-shot\nPrototype (with Euclidean distance)\n82.80±0.59\nPrototype (with Cosine. distance)\n79.90±0.65\nLinear (optimized online)\n82.37±0.57\nFewTURE ( 0 steps)\n82.68±0.55\nFewTURE (20 steps)\n84.51±0.53\nCPEA (in a dense manner)\n87.06±0.38\nTable 7. Impact of different classifiers attached to the pretrained\nbackbone ViT-S/16 on the few-shot classification performance.\nNumber of class-agnostic\nembeddings\n1-shot\n5-shot\n1\n71.97±0.65\n87.06±0.38\n2\n71.43±0.65\n86.62±0.41\n3\n71.34±0.65\n86.72±0.40\n4\n70.57±0.67\n86.39±0.36\nTable 8. Impact of the number of class-agnostic embeddings on\nthe few-shot classification performance.\nPretraining strategies. Table 6 shows the impact of dif-\nferent pretraining strategies on the few-shot classification\nperformance. Supervision collapse exists in the supervised\npretraining, i.e., DeiT [60], thus leading to its poor gen-\neralization ability on the unseen test classes.\nDue to its\npatch-based nature, MIM [76] performs much better than\nDINO [7].\nClassifier. Table 7 shows the impact of different classi-\nfiers attached to the pretrained backbone ViT-S/16 on the\nfew-shot classification performance. The comparison re-\nsults demonstrate the superiority of CPEA.\nNumber of class-agnostic embeddings. Table 8 shows\nthe impact of the number of class-agnostic embeddings\nModel\nInference time [ms]\n5-shot\nFewTURE ( 0 steps)\n156.8±2.16\n82.68±0.59\nFewTURE (10 steps)\n162.1±2.11\n83.89±0.57\nFewTURE (20 steps)\n168.6±2.22\n84.51±0.53\nCPEA\n1.352±0.06\n87.06±0.38\nTable 9. Few-shot classification accuracy and inference time. In-\nference time are averaged over 1800 query images. An NVIDIA-\n2080TI is used for evaluation.\nImage resolution\n1-shot\n5-shot\n224 × 224\n71.97±0.65\n87.06±0.38\n384 × 384\n73.12±0.63\n87.60±0.39\n448 × 448\n73.29±0.63\n88.00±0.37\nTable 10. Impact of the image resolution on the few-shot classifi-\ncation performance.\non the few-shot classification performance. Increasing the\nnumber of class-agnostic embeddings does not improve per-\nformance. Therefore,we use one class-agnostic embedding\nby default.\nInference time. When generalizing to the unseen test\nclasses, our CPEA does not need any adjustments while\nFewTURE [26] needs to learn the token importance weight\nvia online optimization. As a result, our CPEA is much\nfaster than FewTURE in terms of inference speed, as shown\nin Table 9.\nFeature visualization. Figure 5 shows the patch embed-\nding visualization results of four randomly sampled 5-way\n1-shot classification tasks with/without CPEA. It can be ob-\nserved that with CPEA, the patch embeddings are clustered\nby class. This means that the patch embeddings are made\nclass-relevant by CPEA.\n18912\n\n\nBackbone\nModel\nminiImageNet\nCIFAR-FS\n1-shot 5-shot 1-shot 5-shot\nViT-S/16 [15] PMF [28]\n93.1\n98.0\n81.1\n92.5\nCPEA\n94.3\n98.3\n90.4\n96.3\nViT-B/16 [15] PMF [28]\n95.3\n98.4\n84.3\n92.2\nCPEA\n95.7\n98.7\n92.4\n96.6\nTable 11. Impact of the external data on the few-shot classification\nperformance.\nMetric\n1-shot\n5-shot\n∥· ∥2\n68.41±0.68\n84.45±0.44\n∥· ∥2 /dfeature\n71.53±0.67\n85.78±0.41\n∥· ∥2 /std\n62.37±0.62\n83.11±0.43\ncosine\n71.97±0.65\n87.06±0.38\nTable 12. Impact of using the euclidean metrics in Eq. (3) on the\nfew-shot classification performance.\nScalability in image resolution. It seems that our archi-\ntecture is tied to a certain image resolution. In fact, inserting\nadaptive average pooling with a fixed output size of 14×14\nallows us to increase image resolution without introducing\nadditional parameters in MLP. Also, this strategy brings no-\nticeable performance gains, as shown in Table 10.\nExternal data. PMF [28] uses external data to push the\nlimits of simple pipelines for few-shot image classification.\nWith the same experimental settings as PMF, CPEA signif-\nicantly outperforms PMF, as shown in Table 11. This ob-\nservation demonstrates the effectiveness of our method in\nusing external data.\n5. Discussion\nDespite the effectiveness of euclidean metrics and their\ngeneralizations in few-shot image classification [5, 19, 4],\ntwo major challenges still exist when using them. One is\nthe scale issue. Table 12 shows that it is nontrivial to find a\nsuitable scaling factor that outperforms the cosine similar-\nity. The other one is the inaccurate mean and covariance es-\ntimated with a limited number of labeled data. Our method\nmakes patch embeddings class-relevant, thus increasing the\namount of labeled data. The increased labeled data might\nenable us to accurately estimate the mean and covariance in\nfew-shot settings, which could improve the robustness and\nusability of the scale-invariant generalizations of euclidean\nmetrics such as Mahalanobis distance. This is a promising\ndirection worth exploring in the future.\n6. Conclusion\nIn this paper, we propose a Class-aware Patch Embed-\nding Adaptation (CPEA) method for few-shot image clas-\nsification. CPEA can eliminate the interference of single-\nlabel annotations and avoid supervision collapse without the\nneed for aligning semantically-relevant regions. The core\nof CPEA is to integrate patch embeddings with class-aware\nembeddings to make them class-relevant. To measure the\nsimilarity between paired images, we define a dense score\nmatrix between class-relevant patch embeddings, based on\nwhich a similarity score is aggregated. Visualization results\ndemonstrate that our CPEA makes patch embeddings class-\nrelevant. Extensive experiments on four benchmark datasets\nshow that our CPEA performs much better than the coun-\nterparts while achieving new state-of-the-art results.\nAcknowledgments\nThis work was supported in part by National Natural\nScience Foundation of China (62206268, U21A20487),\nShenzhen Technology Project (JCYJ20220818101206014,\nJCYJ20220818101211025,\nJCYJ20200109113416531),\nGuangdong Technology Project (2022B1515120067), and\nCAS Key Technology Talent Program.\nReferences\n[1] Arman Afrasiyabi, Jean-Franc¸ois Lalonde, and Christian\nGagn´e. Mixture-based feature space learning for few-shot\nimage classification. In ICCV, 2021. 7\n[2] Arman Afrasiyabi, Hugo Larochelle, Jean-Franc¸ois Lalonde,\nand Christian Gagn´e. Matching feature sets for few-shot im-\nage classification. In CVPR, 2022. 6\n[3] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEit:\nBERT pre-training of image transformers. In ICLR, 2022. 3,\n4\n[4] Peyman Bateni, Jarred Barber, Jan-Willem van de Meent,\nand Frank Wood. Enhancing few-shot image classification\nwith unlabelled examples. In WACV, 2022. 2, 9\n[5] Peyman Bateni, Raghav Goyal, Vaden Masrani, Frank Wood,\nand Leonid Sigal. Improved few-shot visual classification. In\nCVPR, 2020. 2, 9\n[6] Luca Bertinetto, Joao F Henriques, Philip Torr, and An-\ndrea Vedaldi. Meta-learning with differentiable closed-form\nsolvers. In ICLR, 2019. 6\n[7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers.\nIn\nICCV, 2021. 4, 8\n[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In ICML, 2020. 3\n[9] Xinlei Chen, Saining Xie, and Kaiming He.\nAn empiri-\ncal study of training self-supervised vision transformers. In\nICCV, 2021. 4\n[10] Zhengyu Chen, Jixie Ge, Heshen Zhan, Siteng Huang, and\nDonglin Wang. Pareto self-supervised training for few-shot\nlearning. In CVPR, 2021. 6, 7\n[11] Jun Cheng, Fusheng Hao, Fengxiang He, Liu Liu, and Qieshi\nZhang. Mixer-based semantic spread for few-shot learning.\nIEEE TMM, 2023. 2\n[12] Jun Cheng, Fusheng Hao, Liu Liu, and Dacheng Tao. Im-\nposing semantic consistency of local descriptors for few-shot\nlearning. IEEE TIP, 2022. 2\n18913\n\n\n[13] Tristan Deleu, David Kanaa, Leo Feng, Giancarlo Kerg,\nYoshua Bengio, Guillaume Lajoie, and Pierre-Luc Bacon.\nContinuous-time meta-learning with forward mode differen-\ntiation. In ICLR, 2022. 2\n[14] Carl Doersch, Ankush Gupta, and Andrew Zisserman.\nCrosstransformers: spatially-aware few-shot transfer.\nIn\nNeurIPS, 2020. 2, 3\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv:2010.11929,\n2020. 3, 4, 6, 9\n[16] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Ried-\nmiller, and Thomas Brox.\nDiscriminative unsupervised\nfeature learning with convolutional neural networks.\nIn\nNeurIPS, 2014. 3\n[17] Nanyi Fei, Zhiwu Lu, Tao Xiang, and Songfang Huang.\nMelr: Meta-learning via modeling episode-level relation-\nships for few-shot learning. In ICLR, 2020. 6\n[18] Chelsea Finn, Pieter Abbeel, and Sergey Levine.\nModel-\nagnostic meta-learning for fast adaptation of deep networks.\nIn ICML, 2017. 2\n[19] Stanislav Fort. Gaussian prototypical networks for few-shot\nlearning on omniglot. arXiv:1708.02735, 2017. 2, 9\n[20] Zhi Gao, Yuwei Wu, Yunde Jia, and Mehrtash Harandi. Cur-\nvature generation in curved spaces for few-shot learning. In\nICCV, 2021. 7\n[21] Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick\nP´erez, and Matthieu Cord. Boosting few-shot visual learning\nwith self-supervision. In ICCV, 2019. 6, 7\n[22] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un-\nsupervised representation learning by predicting image rota-\ntions. In ICLR, 2018. 3\n[23] Fusheng Hao, Fengxiang He, Jun Cheng, and Dacheng Tao.\nGlobal-local interplay in semantic alignment for few-shot\nlearning. IEEE TCSVT, 2022. 2\n[24] Fusheng Hao, Fengxiang He, Jun Cheng, Lei Wang,\nJianzhong Cao, and Dacheng Tao. Collect and select: Se-\nmantic alignment metric learning for few-shot learning. In\nICCV, 2019. 2, 4\n[25] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll´ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. arXiv:2111.06377, 2021. 3\n[26] Markus Hiller, Rongkai Ma, Mehrtash Harandi, and Tom\nDrummond. Rethinking generalization in few-shot classi-\nfication. In NeurIPS, 2022. 1, 2, 3, 4, 5, 6, 7, 8\n[27] Ruibing Hou, Hong Chang, Bingpeng Ma, Shiguang Shan,\nand Xilin Chen. Cross attention network for few-shot classi-\nfication. In NeurIPS, 2019. 2\n[28] Shell Xu Hu, Da Li, Jan St¨uhmer, Minyoung Kim, and Tim-\nothy M. Hospedales. Pushing the limits of simple pipelines\nfor few-shot learning: External data and fine-tuning make a\ndifference. In CVPR, 2022. 9\n[29] Adam Jelley, Amos Storkey, Antreas Antoniou, and Sam De-\nvlin. Contrastive meta-learning for partially observable few-\nshot learning. In ICLR, 2023. 2\n[30] Dahyun Kang, Heeseung Kwon, Juhong Min, and Minsu\nCho. Relational embedding for few-shot classification. In\nICCV, 2021. 7\n[31] Jaekyeom Kim, Hyoungseok Kim, and Gunhee Kim. Model-\nagnostic boundary-adversarial sampling for test-time gener-\nalization in few-shot learning. In ECCV, 2020. 7\n[32] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. ICLR, 2015. 6\n[33] Alex Krizhevsky and Geoffrey Hinton. Learning multiple\nlayers of features from tiny images. 2009. 1\n[34] Jinxiang Lai, Siqian Yang, Wenlong Liu, Yi Zeng, Zhongyi\nHuang, Wenlong Wu, Jun Liu, Bin-Bin Gao, and Chengjie\nWang. tsf: Transformer-based semantic filter for few-shot\nlearning. In ECCV, 2022. 3\n[35] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and\nStefano Soatto. Meta-learning with differentiable convex op-\ntimization. In CVPR, 2019. 7\n[36] SuBeen Lee, WonJun Moon, and Jae-Pil Heo. Task discrep-\nancy maximization for fine-grained few-shot classification.\nIn CVPR, 2022. 2\n[37] Wenbin Li, Jinglin Xu, Jing Huo, Lei Wang, Yang Gao, and\nJiebo Luo. Distribution consistency based covariance metric\nnetworks for few-shot learning. In AAAI, 2019. 4\n[38] Zhaowen Li, Zhiyang Chen, Fan Yang, Wei Li, Yousong\nZhu, Chaoyang Zhao, Rui Deng, Liwei Wu, Rui Zhao, Ming\nTang, et al. Mst: Masked self-supervised transformer for\nvisual representation. In NeurIPS, 2021. 3\n[39] Chen Liu, Yanwei Fu, Chengming Xu, Siqian Yang, Jilin Li,\nChengjie Wang, and Li Zhang. Learning a few-shot embed-\nding model with contrastive learning. In AAAI, 2021. 6\n[40] Yahui Liu, Enver Sangineto, Wei Bi, Nicu Sebe, Bruno\nLepri, and Marco Nadai. Efficient training of visual trans-\nformers with small datasets. In NeurIPS, 2021. 3\n[41] Yang Liu, Weifeng Zhang, Chao Xiang, Tu Zheng, Deng Cai,\nand Xiaofei He. Learning to affiliate: Mutual centralized\nlearning for few-shot classification. In CVPR, 2022. 2\n[42] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nICCV, 2021. 3\n[43] Yuning Lu, Liangjian Wen, Jianzhuang Liu, Yajing Liu,\nand Xinmei Tian. Self-supervision can be a good few-shot\nlearner. In ECCV, 2022. 3\n[44] Xu Luo, Longhui Wei, Liangjian Wen, Jinrong Yang, Lingxi\nXie, Zenglin Xu, and Qi Tian. Rectifying the shortcut learn-\ning of background for few-shot learning. In NeurIPS, 2021.\n6\n[45] Jiawei Ma, Hanchen Xie, Guangxing Han, Shih-Fu Chang,\nAram Galstyan, and Wael Abd-Almageed. Partner-assisted\nlearning for few-shot image classification. In ICCV, 2021. 6\n[46] Rongkai Ma, Pengfei Fang, Gil Avraham, Yan Zuo, Tom\nDrummond, and Mehrtash Harandi.\nLearning instance\nand task-aware dynamic kernels for few shot learning.\narXiv:2112.03494, 2021. 2\n[47] Rongkai Ma, Pengfei Fang, Tom Drummond, and Mehrtash\nHarandi. Adaptive poincar´e point to set distance for few-shot\nclassification. In AAAI, 2022. 2\n18914\n\n\n[48] Puneet Mangla, Nupur Kumari, Abhishek Sinha, Mayank\nSingh, Balaji Krishnamurthy, and Vineeth N Balasubrama-\nnian. Charting the right manifold: Manifold mixup for few-\nshot learning. In IEEE/CVF Winter Conference on Applica-\ntions of Computer Vision, 2020. 3\n[49] Alex Nichol, Joshua Achiam, and John Schulman. On first-\norder meta-learning algorithms. arXiv:1803.02999, 2018. 2\n[50] Jaehoon Oh, Hyungjun Yoo, ChangHwan Kim, and Se-\nYoung Yun. Boil: Towards representation change for few-\nshot learning. In ICLR, 2021. 2\n[51] Boris Oreshkin, Pau Rodr´ıguez L´opez, and Alexandre La-\ncoste. Tadam: Task dependent adaptive metric for improved\nfew-shot learning. In NeurIPS, 2018. 6\n[52] Guodong Qi, Huimin Yu, Zhaohui Lu, and Shuzhao Li.\nTransductive few-shot classification on the oblique manifold.\nIn ICCV, 2021. 6\n[53] Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell,\nKevin Swersky, Joshua B Tenenbaum, Hugo Larochelle, and\nRichard S Zemel. Meta-learning for semi-supervised few-\nshot classification. arXiv:1803.00676, 2018. 6\n[54] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al.\nImagenet large\nscale visual recognition challenge. International Journal of\nComputer Vision, 115(3):211–252, 2015. 1\n[55] Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol\nVinyals, Razvan Pascanu, Simon Osindero, and Raia Had-\nsell. Meta-learning with latent embedding optimization. In\nICLR, 2018. 2, 6\n[56] Christian Simon,\nPiotr Koniusz,\nRichard Nock,\nand\nMehrtash Harandi. Adaptive subspaces for few-shot learn-\ning. In CVPR, 2020. 2\n[57] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical\nnetworks for few-shot learning. In NeurIPS, 2017. 2, 6, 7\n[58] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS\nTorr, and Timothy M Hospedales. Learning to compare: Re-\nlation network for few-shot learning. In CVPR, 2018. 2\n[59] Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenen-\nbaum, and Phillip Isola. Rethinking few-shot image classifi-\ncation: a good embedding is all you need? In ECCV, 2020.\n7\n[60] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv´e J´egou. Training\ndata-efficient image transformers & distillation through at-\ntention. In ICML, 2021. 8\n[61] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan\nWierstra, et al. Matching networks for one shot learning.\nIn NeurIPS, 2016. 2, 3, 6, 7\n[62] Heng Wang, Tan Yue, Xiang Ye, Zihang He, Bohan Li, and\nYong Li. Revisit finetuning strategy for few-shot learning to\ntransfer the emdeddings. In ICLR, 2023. 2\n[63] Davis Wertheimer, Luming Tang, and Bharath Hariharan.\nFew-shot classification with feature map reconstruction net-\nworks. In CVPR, 2021. 6\n[64] Jiamin Wu, Tianzhu Zhang, Yongdong Zhang, and Feng Wu.\nTask-aware part mining network for few-shot learning. In\nICCV, 2021. 7\n[65] Jiangtao Xie, Fei Long, Jiaming Lv, Qilong Wang, and Pei-\nhua Li. Joint distribution matters: Deep brownian distance\ncovariance for few-shot classification. In CVPR, 2022. 6\n[66] Chengming Xu, Yanwei Fu, Chen Liu, Chengjie Wang, Jilin\nLi, Feiyue Huang, Li Zhang, and Xiangyang Xue. Learning\ndynamic alignment via meta-filter for few-shot learning. In\nCVPR, 2021. 6\n[67] Zhanyuan Yang, Jinghua Wang, and Yingying Zhu. Few-shot\nclassification with contrastive learning. In ECCV, 2022. 3\n[68] Lifchitz Yann, Avrithis Yannis, and Picard Sylvaine. Local\npropagation for few-shot learning. In ICPR, 2021. 4\n[69] Han-Jia Ye, Hexiang Hu, De-Chuan Zhan, and Fei Sha. Few-\nshot learning via embedding adaptation with set-to-set func-\ntions. In CVPR, 2020. 2, 3, 6\n[70] Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen.\nDeepemd: Few-shot image classification with differentiable\nearth mover’s distance and structured classifiers. In CVPR,\n2020. 2, 6, 7\n[71] Chi Zhang, Henghui Ding, Guosheng Lin, Ruibo Li,\nChanghu Wang, and Chunhua Shen. Meta navigator: Search\nfor a good adaptation policy for few-shot learning. In ICCV,\n2021. 6, 7\n[72] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. ICLR, 2018. 5\n[73] Manli Zhang, Jianhong Zhang, Zhiwu Lu, Tao Xiang,\nMingyu Ding, and Songfang Huang. Iept: Instance-level and\nepisode-level pretext tasks for few-shot learning. In ICLR,\n2020. 6\n[74] Xueting Zhang, Debin Meng, Henry Gouk, and Timothy M\nHospedales. Shallow bayesian meta learning for real-world\nfew-shot recognition. In ICCV, 2021. 6, 7\n[75] Jiabao Zhao, Yifan Yang, Xin Lin, Jing Yang, and Liang He.\nLooking wider for better adaptive representation in few-shot\nlearning. In AAAI, 2021. 6\n[76] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang\nXie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training\nwith online tokenizer. ICLR, 2022. 2, 3, 4, 6, 8\n[77] Ziqi Zhou, Xi Qiu, Jiangtao Xie, Jianan Wu, and Chi Zhang.\nBinocular mutual learning for improving few-shot classifica-\ntion. In ICCV, 2021. 6, 7\n[78] Luisa Zintgraf, Kyriacos Shiarli, Vitaly Kurin, Katja Hof-\nmann, and Shimon Whiteson. Fast context adaptation via\nmeta-learning. In ICML, 2019. 2\n18915\n"
}