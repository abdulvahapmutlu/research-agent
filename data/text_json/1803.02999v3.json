{
  "filename": "1803.02999v3.pdf",
  "num_pages": 15,
  "pages": [
    "On First-Order Meta-Learning Algorithms\nAlex Nichol and Joshua Achiam and John Schulman\nOpenAI\n{alex, jachiam, joschu}@openai.com\nAbstract\nThis paper considers meta-learning problems, where there is a distribution of tasks, and we\nwould like to obtain an agent that performs well (i.e., learns quickly) when presented with a\npreviously unseen task sampled from this distribution. We analyze a family of algorithms for\nlearning a parameter initialization that can be ﬁne-tuned quickly on a new task, using only ﬁrst-\norder derivatives for the meta-learning updates. This family includes and generalizes ﬁrst-order\nMAML, an approximation to MAML obtained by ignoring second-order derivatives. It also\nincludes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling\na task, training on it, and moving the initialization towards the trained weights on that task.\nWe expand on the results from Finn et al. showing that ﬁrst-order meta-learning algorithms\nperform well on some well-established benchmarks for few-shot classiﬁcation, and we provide\ntheoretical analysis aimed at understanding why these algorithms work.\n1\nIntroduction\nWhile machine learning systems have surpassed humans at many tasks, they generally need far\nmore data to reach the same level of performance. For example, Schmidt et al. [17, 15] showed\nthat human subjects can recognize new object categories based on a few example images. Lake et\nal. [12] noted that on the Atari game of Frostbite, human novices were able to make signiﬁcant\nprogress on the game after 15 minutes, but double-dueling-DQN [19] required more than 1000 times\nmore experience to attain the same score.\nIt is not completely fair to compare humans to algorithms learning from scratch, since humans\nenter the task with a large amount of prior knowledge, encoded in their brains and DNA. Rather\nthan learning from scratch, they are ﬁne-tuning and recombining a set of pre-existing skills. The\nwork cited above, by Tenenbaum and collaborators, argues that humans’ fast-learning abilities can\nbe explained as Bayesian inference, and that the key to developing algorithms with human-level\nlearning speed is to make our algorithms more Bayesian. However, in practice, it is challenging to\ndevelop (from ﬁrst principles) Bayesian machine learning algorithms that make use of deep neural\nnetworks and are computationally feasible.\nMeta-learning has emerged recently as an approach for learning from small amounts of data.\nRather than trying to emulate Bayesian inference (which may be computationally intractable),\nmeta-learning seeks to directly optimize a fast-learning algorithm, using a dataset of tasks. Speciﬁ-\ncally, we assume access to a distribution over tasks, where each task is, for example, a classiﬁcation\nproblem. From this distribution, we sample a training set and a test set of tasks. Our algorithm is\nfed the training set, and it must produce an agent that has good average performance on the test\nset. Since each task corresponds to a learning problem, performing well on a task corresponds to\nlearning quickly.\n1\narXiv:1803.02999v3  [cs.LG]  22 Oct 2018\n",
    "A variety of diﬀerent approaches to meta-learning have been proposed, each with its own pros\nand cons. In one approach, the learning algorithm is encoded in the weights of a recurrent network,\nbut gradient descent is not performed at test time. This approach was proposed by Hochreiter et\nal. [8] who used LSTMs for next-step prediction and has been followed up by a burst of recent\nwork, for example, Santoro et al. [16] on few-shot classiﬁcation, and Duan et al. [3] for the POMDP\nsetting.\nA second approach is to learn the initialization of a network, which is then ﬁne-tuned at test\ntime on the new task. A classic example of this approach is pretraining using a large dataset (such\nas ImageNet [2]) and ﬁne-tuning on a smaller dataset (such as a dataset of diﬀerent species of bird\n[20]). However, this classic pre-training approach has no guarantee of learning an initialization that\nis good for ﬁne-tuning, and ad-hoc tricks are required for good performance. More recently, Finn\net al. [4] proposed an algorithm called MAML, which directly optimizes performance with respect\nto this initialization—diﬀerentiating through the ﬁne-tuning process. In this approach, the learner\nfalls back on a sensible gradient-based learning algorithm even when it receives out-of-sample data,\nthus allowing it to generalize better than the RNN-based approaches [5].\nOn the other hand,\nsince MAML needs to diﬀerentiate through the optimization process, it’s not a good match for\nproblems where we need to perform a large number of gradient steps at test time. The authors also\nproposed a variant called ﬁrst-order MAML (FOMAML), which is deﬁned by ignoring the second\nderivative terms, avoiding this problem but at the expense of losing some gradient information.\nSurprisingly, though, they found that FOMAML worked nearly as well as MAML on the Mini-\nImageNet dataset [18]. (This result was foreshadowed by prior work in meta-learning [1, 13] that\nignored second derivatives when diﬀerentiating through gradient descent, without ill eﬀect.) In this\nwork, we expand on that insight and explore the potential of meta-learning algorithms based on\nﬁrst-order gradient information, motivated by the potential applicability to problems where it’s too\ncumbersome to apply techniques that rely on higher-order gradients (like full MAML).\nWe make the following contributions:\n• We point out that ﬁrst-order MAML [4] is simpler to implement than was widely recognized\nprior to this article.\n• We introduce Reptile, an algorithm closely related to FOMAML, which is equally simple\nto implement. Reptile is so similar to joint training (i.e., training to minimize loss on the\nexpecation over training tasks) that it is especially surprising that it works as a meta-learning\nalgorithm. Unlike FOMAML, Reptile doesn’t need a training-test split for each task, which\nmay make it a more natural choice in certain settings. It is also related to the older idea of\nfast weights / slow weights [7].\n• We provide a theoretical analysis that applies to both ﬁrst-order MAML and Reptile, showing\nthat they both optimize for within-task generalization.\n• On the basis of empirical evaluation on the Mini-ImageNet [18] and Omniglot [11] datasets,\nwe provide some insights for best practices in implementation.\n2\nMeta-Learning an Initialization\nWe consider the optimization problem of MAML [4]: ﬁnd an initial set of parameters, φ, such that\nfor a randomly sampled task τ with corresponding loss Lτ, the learner will have low loss after k\n2\n",
    "updates. That is:\nminimize\nφ\nEτ\nh\nLτ\n\u0010\nUk\nτ (φ)\n\u0011i\n,\n(1)\nwhere Uk\nτ is the operator that updates φ k times using data sampled from τ. In few-shot learning,\nU corresponds to performing gradient descent or Adam [10] on batches of data sampled from τ.\nMAML solves a version of Equation (1) that makes on additional assumption: for a given task\nτ, the inner-loop optimization uses training samples A, whereas the loss is computed using test\nsamples B. This way, MAML optimizes for generalization, akin to cross-validation. Omitting the\nsuperscript k, we notate this as\nminimize\nφ\nEτ [Lτ,B (Uτ,A(φ))] ,\n(2)\nMAML works by optimizing this loss through stochastic gradient descent, i.e., computing\ngMAML = ∂\n∂φLτ,B(Uτ,A(φ))\n(3)\n= U′\nτ,A(φ)L′\nτ,B(eφ),\nwhere\neφ = Uτ,A(φ)\n(4)\nIn Equation (4), U′\nτ,A(φ) is the Jacobian matrix of the update operation Uτ,A. Uτ,A corresponds to\nadding a sequence of gradient vectors to the initial vector, i.e., Uτ,A(φ) = φ + g1 + g2 + · · · + gk. (In\nAdam, the gradients are also rescaled elementwise, but that does not change the conclusions.) First-\norder MAML (FOMAML) treats these gradients as constants, thus, it replaces Jacobian U′\nτ,A(φ)\nby the identity operation. Hence, the gradient used by FOMAML in the outer-loop optimization is\ngFOMAML = L′\nτ,B(eφ). Therefore, FOMAML can be implemented in a particularly simple way: (1)\nsample task τ; (2) apply the update operator, yielding eφ = Uτ,A(φ); (3) compute the gradient at\neφ, gFOMAML = L′\nτ,B(eφ); and ﬁnally (4) plug gFOMAML into the outer-loop optimizer.\n3\nReptile\nIn this section, we describe a new ﬁrst-order gradient-based meta-learning algorithm called Reptile.\nLike MAML, Reptile learns an initialization for the parameters of a neural network model, such\nthat when we optimize these parameters at test time, learning is fast—i.e., the model generalizes\nfrom a small number of examples from the test task. The Reptile algorithm is as follows:\nAlgorithm 1 Reptile (serial version)\nInitialize φ, the vector of initial parameters\nfor iteration = 1, 2, . . . do\nSample task τ, corresponding to loss Lτ on weight vectors eφ\nCompute eφ = Uk\nτ (φ), denoting k steps of SGD or Adam\nUpdate φ ←φ + ϵ(eφ −φ)\nend for\nIn the last step, instead of simply updating φ in the direction eφ −φ, we can treat (φ −eφ) as a\ngradient and plug it into an adaptive algorithm such as Adam [10]. (Actually, as we will discuss in\nSection 5.1, it is most natural to deﬁne the Reptile gradient as (φ −eφ)/α, where α is the stepsize\n3\n",
    "used by the SGD operation.) We can also deﬁne a parallel or batch version of the algorithm that\nevaluates on n tasks each iteration and updates the initialization to\nφ ←φ + ϵ 1\nn\nn\nX\ni=1\n(eφi −φ)\n(5)\nwhere eφi = Uk\nτi(φ); the updated parameters on the ith task.\nThis algorithm looks remarkably similar to joint training on the expected loss Eτ [Lτ]. Indeed,\nif we deﬁne U to be a single step of gradient descent (k = 1), then this algorithm corresponds to\nstochastic gradient descent on the expected loss:\ngReptile,k=1 = Eτ [φ −Uτ(φ)] /α\n(6)\n= Eτ [∇φLτ(φ)]\n(7)\nHowever, if we perform multiple gradient updates in the partial minimization (k > 1), then the\nexpected update Eτ\n\u0002\nUk\nτ (φ)\n\u0003\ndoes not correspond to taking a gradient step on the expected loss\nEτ [Lτ]. Instead, the update includes important terms coming from second-and-higher derivatives\nof Lτ, as we will analyze in Section 5.1. Hence, Reptile converges to a solution that’s very diﬀerent\nfrom the minimizer of the expected loss Eτ [Lτ].\nOther than the stepsize parameter ϵ and task sampling, the batched version of Reptile is the\nsame as the SimuParallelSGD algorithm [21]. SimuParallelSGD is a method for communication-\neﬃcient distributed optimization, where workers perform gradient updates locally and infrequently\naverage their parameters, rather than the standard approach of averaging gradients.\n4\nCase Study: One-Dimensional Sine Wave Regression\nAs a simple case study, let’s consider the 1D sine wave regression problem, which is slightly modiﬁed\nfrom Finn et al. [4]. This problem is instructive since by design, joint training can’t learn a very\nuseful initialization; however, meta-learning methods can.\n• The task τ = (a, b) is deﬁned by the amplitude a and phase φ of a sine wave function\nfτ(x) = a sin(x + b). The task distribution by sampling a ∼U([0.1, 5.0]) and b ∼U([0, 2π]).\n• Sample p points x1, x2, . . . , xp ∼U([−5, 5])\n• Learner sees (x1, y1), (x2, y2), . . . , (xp, yp) and predicts the whole function f(x)\n• Loss is ℓ2 error on the whole interval [−5, 5]\nLτ(f) =\nZ 5\n−5\ndx∥f(x) −fτ(x)∥2\n(8)\nWe calculate this integral using 50 equally-spaced points x.\nFirst note that the average function is zero everywhere, i.e., Eτ [fτ(x)] = 0, due to the random\nphase b. Therefore, it is useless to train on the expected loss Eτ [Lτ], as this loss is minimized by\nthe zero function f(x) = 0.\nOn the other hand, MAML and Reptile give us an initialization that outputs approximately\nf(x) = 0 before training on a task τ, but the internal feature representations of the network are such\nthat after training on the sampled datapoints (x1, y1), (x2, y2), . . . , (xp, yp), it closely approximates\n4\n",
    "the target function fτ. This learning progress is shown in the ﬁgures below. Figure 1 shows that\nafter Reptile training, the network can quickly converge to a sampled sine wave and infer the values\naway from the sampled points. As points of comparison, we also show the behaviors of MAML and\na randomly-initialized network on the same task.\n4\n2\n0\n2\n4\n3\n2\n1\n0\n1\n2\n3\nBefore\nAfter 32\nTrue\nSampled\n(a) Before training\n4\n2\n0\n2\n4\n4\n3\n2\n1\n0\n1\n2\n3\n4\nBefore\nAfter 32\nTrue\nSampled\n(b) After MAML training\n4\n2\n0\n2\n4\n4\n3\n2\n1\n0\n1\n2\n3\n4\nBefore\nAfter 32\nTrue\nSampled\n(c) After Reptile training\nFigure 1: Demonstration of MAML and Reptile on a toy few-shot regression problem, where we train on 10\nsampled points of a sine wave, performing 32 gradient steps on an MLP with layers 1 →64 →64 →1.\n5\nAnalysis\nIn this section, we provide two alternative explanations of why Reptile works.\n5.1\nLeading Order Expansion of the Update\nHere, we will use a Taylor series expansion to approximate the update performed by Reptile and\nMAML. We will show that both algorithms contain the same leading-order terms: the ﬁrst term\nminimizes the expected loss (joint training), the second and more interesting term maximizes\nwithin-task generalization. Speciﬁcally, it maximizes the inner product between the gradients on\ndiﬀerent minibatches from the same task. If gradients from diﬀerent batches have positive inner\nproduct, then taking a gradient step on one batch improves performance on the other batch.\nUnlike in the discussion and analysis of MAML, we won’t consider a training set and test set\nfrom each task; instead, we’ll just assume that each task gives us a sequence of k loss functions\nL1, L2, . . . , Lk; for example, classiﬁcation loss on diﬀerent minibatches. We will use the following\ndeﬁnitions:\ngi = L′\ni(φi)\n(gradient obtained during SGD)\n(9)\nφi+1 = φi −αgi\n(sequence of parameter vectors)\n(10)\ngi = L′\ni(φ1)\n(gradient at initial point)\n(11)\nHi = L′′\ni (φ1)\n(Hessian at initial point)\n(12)\nFor each of these deﬁnitions, i ∈[1, k].\n5\n",
    "First, let’s calculate the SGD gradients to O(α2) as follows.\ngi = L′\ni(φi) = L′\ni(φ1) + L′′\ni (φ1)(φi −φ1) + O(∥φi −φ1∥2)\n|\n{z\n}\n=O(α2)\n(Taylor’s theorem)\n(13)\n= gi + Hi(φi −φ1) + O(α2)\n(using deﬁnition of gi, Hi)\n(14)\n= gi −αHi\ni−1\nX\nj=1\ngj + O(α2)\n(using φi −φ1 = −α\ni−1\nX\nj=1\ngj)\n(15)\n= gi −αHi\ni−1\nX\nj=1\ngj + O(α2)\n(using gj = gj + O(α))\n(16)\nNext, we will approximate the MAML gradient. Deﬁne Ui as the operator that updates the\nparameter vector on minibatch i: Ui(φ) = φ −αL′\ni(φ).\ngMAML =\n∂\n∂φ1\nLk(φk)\n(17)\n=\n∂\n∂φ1\nLk(Uk−1(Uk−2(. . . (U1(φ1)))))\n(18)\n= U′\n1(φ1) · · · U′\nk−1(φk−1)L′\nk(φk)\n(repeatedly applying the chain rule)\n(19)\n=\n\u0000I −αL′′\n1(φ1)\n\u0001\n· · ·\n\u0000I −αL′′\nk−1(φk−1)\n\u0001\nL′\nk(φk)\n(using U′\ni(φ) = I −αL′′\ni (φ))\n(20)\n=\n\n\nk−1\nY\nj=1\n(I −αL′′\nj (φj))\n\ngk\n(product notation, deﬁnition of gk)\n(21)\nNext, let’s expand to leading order\ngMAML =\n\n\nk−1\nY\nj=1\n(I −αHj)\n\n\n\ngk −αHk\nk−1\nX\nj=1\ngj\n\n+ O(α2)\n(22)\n(replacing L′′\nj (φj) with Hj, and replacing gk using Equation (16))\n=\n\nI −α\nk−1\nX\nj=1\nHj\n\n\n\ngk −αHk\nk−1\nX\nj=1\ngj\n\n+ O(α2)\n(23)\n= gk −α\nk−1\nX\nj=1\nHjgk −αHk\nk−1\nX\nj=1\ngj + O(α2)\n(24)\nFor simplicity of exposition, let’s consider the k = 2 case, and later we’ll provide the general\nformulas.\ngMAML\n= g2 −αH2g1 −αH1g2 + O(α2)\n(25)\ngFOMAML = g2\n= g2 −αH2g1 + O(α2)\n(26)\ngReptile = g1 + g2 = g1 + g2 −αH2g1 + O(α2)\n(27)\nAs we will show in the next paragraph, the terms like H2g1 serve to maximize the inner products\nbetween the gradients computed on diﬀerent minibatches, while lone gradient terms like g1 take us\nto the minimum of the joint training problem.\n6\n",
    "When we take the expectation of gFOMAML, gReptile, and gMAML under minibatch sampling,\nwe are left with only two kinds of terms which we will call AvgGrad and AvgGradInner. In the\nequations below Eτ,1,2 [. . . ] means that we are taking the expectation over the task τ and the two\nminibatches deﬁning L1 and L2, respectively.\n• AvgGrad is deﬁned as gradient of expected loss.\nAvgGrad = Eτ,1 [g1]\n(28)\n(−AvgGrad) is the direction that brings φ towards the minimum of the “joint training”\nproblem; the expected loss over tasks.\n• The more interesting term is AvgGradInner, deﬁned as follows:\nAvgGradInner = Eτ,1,2\n\u0002\nH2g1\n\u0003\n(29)\n= Eτ,1,2\n\u0002\nH1g2\n\u0003\n(interchanging indices 1, 2)\n(30)\n= 1\n2Eτ,1,2\n\u0002\nH2g1 + H1g2\n\u0003\n(averaging last two equations)\n(31)\n= 1\n2Eτ,1,2\n\u0014 ∂\n∂φ1\n(g1 · g2)\n\u0015\n(32)\nThus, (−AvgGradInner) is the direction that increases the inner product between gradients\nof diﬀerent minibatches for a given task, improving generalization.\nRecalling our gradient expressions, we get the following expressions for the meta-gradients, for\nSGD with k = 2:\nE [gMAML] = (1)AvgGrad −(2α)AvgGradInner + O(α2)\n(33)\nE [gFOMAML] = (1)AvgGrad −(α)AvgGradInner + O(α2)\n(34)\nE [gReptile] = (2)AvgGrad −(α)AvgGradInner + O(α2)\n(35)\nIn practice, all three gradient expressions ﬁrst bring us towards the minimum of the expected loss\nover tasks, then the higher-order AvgGradInner term enables fast learning by maximizing the inner\nproduct between gradients within a given task.\nFinally, we can extend these calculations to the general k ≥2 case:\ngMAML = gk −αHk\nk−1\nX\nj=1\ngj −α\nk−1\nX\nj=1\nHjgk + O(α2)\n(36)\nE [gMAML] = (1)AvgGrad −(2(k −1)α)AvgGradInner\n(37)\ngFOMAML = gk = gk −αHk\nk−1\nX\nj=1\ngj + O(α2)\n(38)\nE [gFOMAML] = (1)AvgGrad −((k −1)α)AvgGradInner\n(39)\ngReptile = −(φk+1 −φ1)/α =\nk\nX\ni=1\ngi =\nk\nX\ni=1\ngi −α\nk\nX\ni=1\ni−1\nX\nj=1\nHigj + O(α2)\n(40)\nE [gReptile] = (k)AvgGrad −\n\u0000 1\n2k(k −1)α\n\u0001\nAvgGradInner\n(41)\nAs in the k = 2, the ratio of coeﬃcients of the AvgGradInner term and the AvgGrad term goes\nMAML > FOMAML > Reptile. However, in all cases, this ratio increases linearly with both the\nstepsize α and the number of iterations k. Note that the Taylor series approximation only holds\nfor small αk.\n7\n",
    "풲*1\n풲*2\nϕ\nFigure 2: The above illustration shows the sequence of iterates obtained by moving alternately towards two\noptimal solution manifolds W1 and W2 and converging to the point that minimizes the average squared\ndistance. One might object to this picture on the grounds that we converge to the same point regardless of\nwhether we perform one step or multiple steps of gradient descent. That statement is true, however, note\nthat minimizing the expected distance objective Eτ [D(φ, Wτ)] is diﬀerent than minimizing the expected loss\nobjective Eτ [Lτ(fφ)]. In particular, there is a high-dimensional manifold of minimizers of the expected loss\nLτ (e.g., in the sine wave case, many neural network parameters give the zero function f(φ) = 0), but the\nminimizer of the expected distance objective is typically a single point.\n5.2\nFinding a Point Near All Solution Manifolds\nHere, we argue that Reptile converges towards a solution φ that is close (in Euclidean distance) to\neach task τ’s manifold of optimal solutions. This is a informal argument and should be taken much\nless seriously than the preceding Taylor series analysis.\nLet φ denote the network initialization, and let Wτ denote the set of optimal parameters for\ntask τ. We want to ﬁnd φ such that the distance D(φ, Wτ) is small for all tasks.\nminimize\nφ\nEτ\n\u00021\n2D(φ, Wτ)2\u0003\n(42)\nWe will show that Reptile corresponds to performing SGD on that objective.\nGiven a non-pathological set S ⊂Rd, then for almost all points φ ∈Rd the gradient of the\nsquared distance D(φ, S)2 is 2(φ −PS(φ)), where PS(φ) is the projection (closest point) of φ onto\nS. Thus,\n∇φEτ\n\u00021\n2D(φ, Wτ)2\u0003\n= Eτ\n\u00021\n2∇φD(φ, Wτ)2\u0003\n(43)\n= Eτ [φ −PWτ (φ)] , where PWτ (φ) = arg min\np∈Wτ\nD(p, φ)\n(44)\nEach iteration of Reptile corresponds to sampling a task τ and performing a stochastic gradient\nupdate\nφ ←φ −ϵ∇φ 1\n2D(φ, Wτ)2\n(45)\n= φ −ϵ(φ −PWτ (φ))\n(46)\n= (1 −ϵ)φ + ϵPWτ (φ).\n(47)\nIn practice, we can’t exactly compute PWτ (φ), which is deﬁned as a minimizer of Lτ. However, we\ncan partially minimize this loss using gradient descent. Hence, in Reptile we replace W ∗\nτ (φ) by the\nresult of running k steps of gradient descent on Lτ starting with initialization φ.\n6\nExperiments\n6.1\nFew-Shot Classiﬁcation\nWe evaluate our method on two popular few-shot classiﬁcation tasks: Omniglot [11] and Mini-\nImageNet [18]. These datasets make it easy to compare our method to other few-shot learning\n8\n",
    "approaches like MAML.\nIn few-shot classiﬁcation tasks, we have a meta-dataset D containing many classes C, where each\nclass is itself a set of example instances {c1, c2, ..., cn}. If we are doing K-shot, N-way classiﬁcation,\nthen we sample tasks by selecting N classes from C and then selecting K + 1 examples for each\nclass. We split these examples into a training set and a test set, where the test set contains a single\nexample for each class. The model gets to see the entire training set, and then it must classify a\nrandomly chosen sample from the test set. For example, if you trained a model for 5-shot, 5-way\nclassiﬁcation, then you would show it 25 examples (5 per class) and ask it to classify a 26th example.\nIn addition to the above setup, we also experimented with the transductive setting, where the\nmodel classiﬁes the entire test set at once. In our transductive experiments, information was shared\nbetween the test samples via batch normalization [9]. In our non-transductive experiments, batch\nnormalization statistics were computed using all of the training samples and a single test sample.\nWe note that Finn et al. [4] use transduction for evaluating MAML.\nFor our experiments, we used the same CNN architectures and data preprocessing as Finn et\nal. [4]. We used the Adam optimizer [10] in the inner loop, and vanilla SGD in the outer loop,\nthroughout our experiments. For Adam we set β1 = 0 because we found that momentum reduced\nperformance across the board.1\nDuring training, we never reset or interpolated Adam’s rolling\nmoment data; instead, we let it update automatically at every inner-loop training step. However,\nwe did backup and reset the Adam statistics when evaluating on the test set to avoid information\nleakage.\nThe results on Omniglot and Mini-ImageNet are shown in Tables 1 and 2.\nWhile MAML,\nFOMAML, and Reptile have very similar performance on all of these tasks, Reptile does slightly\nbetter than the alternatives on Mini-ImageNet and slightly worse on Omniglot. It also seems that\ntransduction gives a performance boost in all cases, suggesting that further research should pay\nclose attention to its use of batch normalization during testing.\nAlgorithm\n1-shot 5-way\n5-shot 5-way\nMAML + Transduction\n48.70 ± 1.84%\n63.11 ± 0.92%\n1st-order MAML + Transduction\n48.07 ± 1.75%\n63.15 ± 0.91%\nReptile\n47.07 ± 0.26%\n62.74 ± 0.37%\nReptile + Transduction\n49.97 ± 0.32%\n65.99 ± 0.58%\nTable 1: Results on Mini-ImageNet. Both MAML and 1st-order MAML results are from [4].\nAlgorithm\n1-shot 5-way\n5-shot 5-way\n1-shot 20-way\n5-shot 20-way\nMAML + Transduction\n98.7 ± 0.4%\n99.9 ± 0.1%\n95.8 ± 0.3%\n98.9 ± 0.2%\n1st-order MAML + Transduction\n98.3 ± 0.5%\n99.2 ± 0.2%\n89.4 ± 0.5%\n97.9 ± 0.1%\nReptile\n95.39 ± 0.09%\n98.90 ± 0.10%\n88.14 ± 0.15%\n96.65 ± 0.33%\nReptile + Transduction\n97.68 ± 0.04%\n99.48 ± 0.06%\n89.43 ± 0.14%\n97.12 ± 0.32%\nTable 2: Results on Omniglot. MAML results are from [4]. 1st-order MAML results were generated by the\ncode for [4] with the same hyper-parameters as MAML.\n1This ﬁnding also matches our analysis from Section 5.1, which suggests that Reptile works because sequential\nsteps come from diﬀerent mini-batches. With momentum, a mini-batch has inﬂuence over the next few steps, reducing\nthis eﬀect.\n9\n",
    "6.2\nComparing Diﬀerent Inner-Loop Gradient Combinations\nFor this experiment, we used four non-overlapping mini-batches in each inner-loop, yielding gra-\ndients g1, g2, g3, and g4.\nWe then compared learning performance when using diﬀerent linear\ncombinations of the gi’s for the outer loop update.\nNote that two-step Reptile corresponds to\ng1 + g2, and two-step FOMAML corresponds to g2.\nTo make it easier to get an apples-to-apples comparison between diﬀerent linear combinations,\nwe simpliﬁed our experimental setup in several ways. First, we used vanilla SGD in the inner- and\nouter-loops. Second, we did not use meta-batches. Third, we restricted our experiments to 5-shot,\n5-way Omniglot. With these simpliﬁcations, we did not have to worry as much about the eﬀects\nof hyper-parameters or optimizers.\nFigure 3 shows the learning curves for various inner-loop gradient combinations. For gradient\ncombinations with more than one term, we ran both a sum and an average of the inner gradients\nto correct for the eﬀective step size increase.\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n40000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\ng1\n1\n2 * (g1 + g2)\ng1 + g2\ng2\n1\n3 * (g1 + g2 + g3)\ng1 + g2 + g3\ng3\n1\n4 * (g1 + g2 + g3 + g4)\ng1 + g2 + g3 + g4\ng4\nFigure 3: Diﬀerent inner-loop gradient combinations on 5-shot 5-way Omniglot.\nAs expected, using only the ﬁrst gradient g1 is quite ineﬀective, since it amounts to opti-\nmizing the expected loss over all tasks. Surprisingly, two-step Reptile is noticeably worse than\ntwo-step FOMAML, which might be explained by the fact that two-step Reptile puts less weight\non AvgGradInner relative to AvgGrad (Equations (34) and (35)). Most importantly, though, all\nthe methods improve as the number of mini-batches increases. This improvement is more signiﬁcant\nwhen using a sum of all gradients (Reptile) rather than using just the ﬁnal gradient (FOMAML).\nThis also suggests that Reptile can beneﬁt from taking many inner loop steps, which is consistent\nwith the optimal hyper-parameters found for Section 6.1.\n10\n",
    "1\n3\n5\n7\n9\n11\n13\n15\nInner Iterations\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest Accuracy\nReptile (cycling)\nFOMAML (separate-tail, cycling)\nFOMAML (shared-tail, replacement)\nFOMAML (shared-tail, cycling)\n(a) Final test performance vs.\nnumber of inner-loop iterations.\n20\n40\n60\n80\n100\nInner Batch Size\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest Accuracy\nReptile (cycling)\nFOMAML (separate-tail, cycling)\nFOMAML (shared-tail, replacement)\nFOMAML (shared-tail, cycling)\n(b) Final test performance vs.\ninner-loop batch size.\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\nlog2(Initial Outer Step)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest Accuracy\n(c) Final test performance vs.\nouter-loop step size for shared-\ntail FOMAML with batch size\n100 (full batches).\nFigure 4: The results of hyper-parameter sweeps on 5-shot 5-way Omniglot.\n6.3\nOverlap Between Inner-Loop Mini-Batches\nBoth Reptile and FOMAML use stochastic optimization in their inner-loops. Small changes to\nthis optimization procedure can lead to large changes in ﬁnal performance. This section explores\nthe sensitivity of Reptile and FOMAML to the inner loop hyperparameters, and also shows that\nFOMAML’s performance signiﬁcantly drops if mini-batches are selected the wrong way.\nThe experiments in this section look at the diﬀerence between shared-tail FOMAML, where\nthe ﬁnal inner-loop mini-batch comes from the same set of data as the earlier inner-loop batches,\nto separate-tail FOMAML, where the ﬁnal mini-batch comes from a disjoint set of data. Viewing\nFOMAML as an approximation to MAML, separate-tail FOMAML can be seen as the more correct\napproach (and was used by Finn et al. [4]), since the training-time optimization resembles the\ntest-time optimization (where the test set doesn’t overlap with the training set). Indeed, we ﬁnd\nthat separate-tail FOMAML is signiﬁcantly better than shared-tail FOMAML. As we will show,\nshared-tail FOMAML degrades in performance when the data used to compute the meta-gradient\n(gFOMAML = gk) overlaps signiﬁcantly with the earlier batches; however, Reptile and separate-tail\nMAML maintain performance and are not very sensitive to the inner-loop hyperparameters.\nFigure 4a shows that when minibatches are selected by cycling through the training data\n(shared-tail, cycle), shared-tail FOMAML performs well up to four inner-loop iterations, but\ndrops in performance starting at ﬁve iterations, where the ﬁnal minibatch (used to compute\ngFOMAML = gk) overlaps with the earlier ones. When we use random sampling instead (shared-tail,\nreplacement), shared-tail FOMAML degrades more gradually. We hypothesize that this is because\nsome samples still appear in the ﬁnal batch that were not in the previous batches. The eﬀect is\nstochastic, so it makes sense that the curve is smoother.\nFigure 4b shows a similar phenomenon, but here we ﬁxed the inner-loop to four iterations\nand instead varied the batch size. For batch sizes greater than 25, the ﬁnal inner-loop batch for\nshared-tail FOMAML necessarily contains samples from the previous batches. Similar to Figure 4a,\nhere we observe that shared-tail FOMAML with random sampling degrades more gradually than\nshared-tail FOMAML with cycling.\nIn both of these parameter sweeps, separate-tail FOMAML and Reptile do not degrade in\nperformance as the number of inner-loop iterations or batch size changes.\nThere are several possible explanations for above ﬁndings. For example, one might hypothesize\nthat shared-tail FOMAML is only worse in these experiments because its eﬀective step size is\nmuch lower than that of separate-tail FOMAML. However, Figure 4c suggests that this is not the\n11\n",
    "case: performance was equally poor for every choice of step size in a thorough sweep. A diﬀerent\nhypothesis is that shared-tail FOMAML performs poorly because, after a few inner-loop steps on\na sample, the gradient of the loss for that sample does not contain very much useful information\nabout the sample. In other words, the ﬁrst few SGD steps might bring the model close to a local\noptimum, and then further SGD steps might simply bounce around this local optimum.\n7\nDiscussion\nMeta-learning algorithms that perform gradient descent at test time are appealing because of their\nsimplicity and generalization properties [5].\nThe eﬀectiveness of ﬁne-tuning (e.g.\nfrom models\ntrained on ImageNet [2]) gives us additional faith in these approaches. This paper proposed a new\nalgorithm called Reptile, whose training process is only subtlely diﬀerent from joint training and\nonly uses ﬁrst-order gradient information (like ﬁrst-order MAML).\nWe gave two theoretical explanations for why Reptile works. First, by approximating the update\nwith a Taylor series, we showed that SGD automatically gives us the same kind of second-order\nterm that MAML computes. This term adjusts the initial weights to maximize the dot product\nbetween the gradients of diﬀerent minibatches on the same task—i.e., it encourages the gradients\nto generalize between minibatches of the same task. We also provided a second informal argument,\nwhich is that Reptile ﬁnds a point that is close (in Euclidean distance) to all of the optimal solution\nmanifolds of the training tasks.\nWhile this paper studies the meta-learning setting, the Taylor series analysis in Section 5.1\nmay have some bearing on stochastic gradient descent in general. It suggests that when doing\nstochastic gradient descent, we are automatically performing a MAML-like update that maximizes\nthe generalization between diﬀerent minibatches. This observation partly explains why ﬁne tuning\n(e.g., from ImageNet to a smaller dataset [20]) works well. This hypothesis would suggest that joint\ntraining plus ﬁne tuning will continue to be a strong baseline for meta-learning in various machine\nlearning problems.\n8\nFuture Work\nWe see several promising directions for future work:\n• Understanding to what extent SGD automatically optimizes for generalization, and whether\nthis eﬀect can be ampliﬁed in the non-meta-learning setting.\n• Applying Reptile in the reinforcement learning setting. So far, we have obtained negative\nresults, since joint training is a strong baseline, so some modiﬁcations to Reptile might be\nnecessary.\n• Exploring whether Reptile’s few-shot learning performance can be improved by deeper archi-\ntectures for the classiﬁer.\n• Exploring whether regularization can improve few-shot learning performance, as currently\nthere is a large gap between training and testing error.\n• Evaluating Reptile on the task of few-shot density modeling [14].\n12\n",
    "References\n[1] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoﬀman, David Pfau, Tom Schaul, and\nNando de Freitas. Learning to learn by gradient descent by gradient descent. In Advances in Neural\nInformation Processing Systems, pages 3981–3989, 2016.\n[2] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-\nerarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE\nConference on, pages 248–255. IEEE, 2009.\n[3] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2: Fast\nreinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.\n[4] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of\ndeep networks. arXiv preprint arXiv:1703.03400, 2017.\n[5] Chelsea Finn and Sergey Levine. Meta-learning and universality: Deep representations and gradient\ndescent can approximate any learning algorithm. arXiv preprint arXiv:1710.11622, 2017.\n[6] Nikolaus Hansen. The CMA evolution strategy: a comparing review. In Towards a new evolutionary\ncomputation, pages 75–102. Springer, 2006.\n[7] Geoﬀrey E Hinton and David C Plaut. Using fast weights to deblur old memories. In Proceedings of\nthe ninth annual conference of the Cognitive Science Society, pages 177–186, 1987.\n[8] Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent. In\nInternational Conference on Artiﬁcial Neural Networks, pages 87–94. Springer, 2001.\n[9] Sergey Ioﬀe and Christian Szegedy. Batch normalization: Accelerating deep network training by reduc-\ning internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\n[10] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International\nConference on Learning Representations (ICLR), 2015.\n[11] Brenden M. Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua B. Tenenbaum. One shot learning\nof simple visual concepts. In Conference of the Cognitive Science Society (CogSci), 2011.\n[12] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum.\nHuman-level concept learning\nthrough probabilistic program induction. Science, 350(6266):1332–1338, 2015.\n[13] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International\nConference on Learning Representations (ICLR), 2017.\n[14] Scott Reed, Yutian Chen, Thomas Paine, A¨aron van den Oord, SM Eslami, Danilo Rezende, Oriol\nVinyals, and Nando de Freitas. Few-shot autoregressive density estimation: Towards learning to learn\ndistributions. arXiv preprint arXiv:1710.10304, 2017.\n[15] Ruslan Salakhutdinov, Joshua Tenenbaum, and Antonio Torralba. One-shot learning with a hierarchi-\ncal nonparametric bayesian model. In Proceedings of ICML Workshop on Unsupervised and Transfer\nLearning, pages 195–206, 2012.\n[16] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-\nlearning with memory-augmented neural networks. In International conference on machine learning,\npages 1842–1850, 2016.\n[17] Lauren A Schmidt. Meaning and compositionality as statistical induction of categories and constraints.\nPhD thesis, Massachusetts Institute of Technology, 2009.\n[18] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one shot\nlearning. In Advances in Neural Information Processing Systems, pages 3630–3638, 2016.\n[19] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot, and Nando De Freitas.\nDueling network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581, 2015.\n13\n",
    "[20] Ning Zhang, JeﬀDonahue, Ross Girshick, and Trevor Darrell. Part-based R-CNNs for ﬁne-grained\ncategory detection. In European conference on computer vision, pages 834–849. Springer, 2014.\n[21] Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J Smola.\nParallelized stochastic gradient\ndescent. In Advances in neural information processing systems, pages 2595–2603, 2010.\nA\nHyper-parameters\nFor all experiments, we linearly annealed the outer step size to 0. We ran each experiment with\nthree diﬀerent random seeds, and computed the conﬁdence intervals using the standard deviation\nacross the runs.\nInitially, we tried optimizing the Reptile hyper-parameters using CMA-ES [6]. However, we\nfound that most hyper-parameters had little eﬀect on the resulting performance. After seeing this\nresult, we simpliﬁed all of the hyper-parameters and shared hyper-parameters between experiments\nwhen it made sense.\nTable 3: Reptile hyper-parameters for the Omniglot comparison between all algorithms.\nParameter\n5-way\n20-way\nAdam learning rate\n0.001\n0.0005\nInner batch size\n10\n20\nInner iterations\n5\n10\nTraining shots\n10\n10\nOuter step size\n1.0\n1.0\nOuter iterations\n100K\n200K\nMeta-batch size\n5\n5\nEval. inner iterations\n50\n50\nEval. inner batch\n5\n10\nTable 4: Reptile hyper-parameters for the Mini-ImageNet comparison between all algorithms.\nParameter\n1-shot\n5-shot\nAdam learning rate\n0.001\n0.001\nInner batch size\n10\n10\nInner iterations\n8\n8\nTraining shots\n15\n15\nOuter step size\n1.0\n1.0\nOuter iterations\n100K\n100K\nMeta-batch size\n5\n5\nEval. inner batch size\n5\n15\nEval. inner iterations\n50\n50\n14\n",
    "Table 5: Hyper-parameters for Section 6.2. All outer step sizes were linearly annealed to zero during training.\nParameter\nValue\nInner learning rate\n3 × 10−3\nInner batch size\n25\nOuter step size\n0.25\nOuter iterations\n40K\nEval. inner batch size\n25\nEval. inner iterations\n5\nTable 6: Hyper-parameters Section 6.3. All outer step sizes were linearly annealed to zero during training.\nParameter\nFigure 4b\nFigure 4a\nFigure 4c\nInner learning rate\n3 × 10−3\n3 × 10−3\n3 × 10−3\nInner batch size\n-\n25\n100\nInner iterations\n4\n-\n4\nOuter step size\n1.0\n1.0\n-\nOuter iterations\n40K\n40K\n40K\nEval. inner batch size\n25\n25\n25\nEval. inner iterations\n5\n5\n5\n15\n"
  ],
  "full_text": "On First-Order Meta-Learning Algorithms\nAlex Nichol and Joshua Achiam and John Schulman\nOpenAI\n{alex, jachiam, joschu}@openai.com\nAbstract\nThis paper considers meta-learning problems, where there is a distribution of tasks, and we\nwould like to obtain an agent that performs well (i.e., learns quickly) when presented with a\npreviously unseen task sampled from this distribution. We analyze a family of algorithms for\nlearning a parameter initialization that can be ﬁne-tuned quickly on a new task, using only ﬁrst-\norder derivatives for the meta-learning updates. This family includes and generalizes ﬁrst-order\nMAML, an approximation to MAML obtained by ignoring second-order derivatives. It also\nincludes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling\na task, training on it, and moving the initialization towards the trained weights on that task.\nWe expand on the results from Finn et al. showing that ﬁrst-order meta-learning algorithms\nperform well on some well-established benchmarks for few-shot classiﬁcation, and we provide\ntheoretical analysis aimed at understanding why these algorithms work.\n1\nIntroduction\nWhile machine learning systems have surpassed humans at many tasks, they generally need far\nmore data to reach the same level of performance. For example, Schmidt et al. [17, 15] showed\nthat human subjects can recognize new object categories based on a few example images. Lake et\nal. [12] noted that on the Atari game of Frostbite, human novices were able to make signiﬁcant\nprogress on the game after 15 minutes, but double-dueling-DQN [19] required more than 1000 times\nmore experience to attain the same score.\nIt is not completely fair to compare humans to algorithms learning from scratch, since humans\nenter the task with a large amount of prior knowledge, encoded in their brains and DNA. Rather\nthan learning from scratch, they are ﬁne-tuning and recombining a set of pre-existing skills. The\nwork cited above, by Tenenbaum and collaborators, argues that humans’ fast-learning abilities can\nbe explained as Bayesian inference, and that the key to developing algorithms with human-level\nlearning speed is to make our algorithms more Bayesian. However, in practice, it is challenging to\ndevelop (from ﬁrst principles) Bayesian machine learning algorithms that make use of deep neural\nnetworks and are computationally feasible.\nMeta-learning has emerged recently as an approach for learning from small amounts of data.\nRather than trying to emulate Bayesian inference (which may be computationally intractable),\nmeta-learning seeks to directly optimize a fast-learning algorithm, using a dataset of tasks. Speciﬁ-\ncally, we assume access to a distribution over tasks, where each task is, for example, a classiﬁcation\nproblem. From this distribution, we sample a training set and a test set of tasks. Our algorithm is\nfed the training set, and it must produce an agent that has good average performance on the test\nset. Since each task corresponds to a learning problem, performing well on a task corresponds to\nlearning quickly.\n1\narXiv:1803.02999v3  [cs.LG]  22 Oct 2018\n\n\nA variety of diﬀerent approaches to meta-learning have been proposed, each with its own pros\nand cons. In one approach, the learning algorithm is encoded in the weights of a recurrent network,\nbut gradient descent is not performed at test time. This approach was proposed by Hochreiter et\nal. [8] who used LSTMs for next-step prediction and has been followed up by a burst of recent\nwork, for example, Santoro et al. [16] on few-shot classiﬁcation, and Duan et al. [3] for the POMDP\nsetting.\nA second approach is to learn the initialization of a network, which is then ﬁne-tuned at test\ntime on the new task. A classic example of this approach is pretraining using a large dataset (such\nas ImageNet [2]) and ﬁne-tuning on a smaller dataset (such as a dataset of diﬀerent species of bird\n[20]). However, this classic pre-training approach has no guarantee of learning an initialization that\nis good for ﬁne-tuning, and ad-hoc tricks are required for good performance. More recently, Finn\net al. [4] proposed an algorithm called MAML, which directly optimizes performance with respect\nto this initialization—diﬀerentiating through the ﬁne-tuning process. In this approach, the learner\nfalls back on a sensible gradient-based learning algorithm even when it receives out-of-sample data,\nthus allowing it to generalize better than the RNN-based approaches [5].\nOn the other hand,\nsince MAML needs to diﬀerentiate through the optimization process, it’s not a good match for\nproblems where we need to perform a large number of gradient steps at test time. The authors also\nproposed a variant called ﬁrst-order MAML (FOMAML), which is deﬁned by ignoring the second\nderivative terms, avoiding this problem but at the expense of losing some gradient information.\nSurprisingly, though, they found that FOMAML worked nearly as well as MAML on the Mini-\nImageNet dataset [18]. (This result was foreshadowed by prior work in meta-learning [1, 13] that\nignored second derivatives when diﬀerentiating through gradient descent, without ill eﬀect.) In this\nwork, we expand on that insight and explore the potential of meta-learning algorithms based on\nﬁrst-order gradient information, motivated by the potential applicability to problems where it’s too\ncumbersome to apply techniques that rely on higher-order gradients (like full MAML).\nWe make the following contributions:\n• We point out that ﬁrst-order MAML [4] is simpler to implement than was widely recognized\nprior to this article.\n• We introduce Reptile, an algorithm closely related to FOMAML, which is equally simple\nto implement. Reptile is so similar to joint training (i.e., training to minimize loss on the\nexpecation over training tasks) that it is especially surprising that it works as a meta-learning\nalgorithm. Unlike FOMAML, Reptile doesn’t need a training-test split for each task, which\nmay make it a more natural choice in certain settings. It is also related to the older idea of\nfast weights / slow weights [7].\n• We provide a theoretical analysis that applies to both ﬁrst-order MAML and Reptile, showing\nthat they both optimize for within-task generalization.\n• On the basis of empirical evaluation on the Mini-ImageNet [18] and Omniglot [11] datasets,\nwe provide some insights for best practices in implementation.\n2\nMeta-Learning an Initialization\nWe consider the optimization problem of MAML [4]: ﬁnd an initial set of parameters, φ, such that\nfor a randomly sampled task τ with corresponding loss Lτ, the learner will have low loss after k\n2\n\n\nupdates. That is:\nminimize\nφ\nEτ\nh\nLτ\n\u0010\nUk\nτ (φ)\n\u0011i\n,\n(1)\nwhere Uk\nτ is the operator that updates φ k times using data sampled from τ. In few-shot learning,\nU corresponds to performing gradient descent or Adam [10] on batches of data sampled from τ.\nMAML solves a version of Equation (1) that makes on additional assumption: for a given task\nτ, the inner-loop optimization uses training samples A, whereas the loss is computed using test\nsamples B. This way, MAML optimizes for generalization, akin to cross-validation. Omitting the\nsuperscript k, we notate this as\nminimize\nφ\nEτ [Lτ,B (Uτ,A(φ))] ,\n(2)\nMAML works by optimizing this loss through stochastic gradient descent, i.e., computing\ngMAML = ∂\n∂φLτ,B(Uτ,A(φ))\n(3)\n= U′\nτ,A(φ)L′\nτ,B(eφ),\nwhere\neφ = Uτ,A(φ)\n(4)\nIn Equation (4), U′\nτ,A(φ) is the Jacobian matrix of the update operation Uτ,A. Uτ,A corresponds to\nadding a sequence of gradient vectors to the initial vector, i.e., Uτ,A(φ) = φ + g1 + g2 + · · · + gk. (In\nAdam, the gradients are also rescaled elementwise, but that does not change the conclusions.) First-\norder MAML (FOMAML) treats these gradients as constants, thus, it replaces Jacobian U′\nτ,A(φ)\nby the identity operation. Hence, the gradient used by FOMAML in the outer-loop optimization is\ngFOMAML = L′\nτ,B(eφ). Therefore, FOMAML can be implemented in a particularly simple way: (1)\nsample task τ; (2) apply the update operator, yielding eφ = Uτ,A(φ); (3) compute the gradient at\neφ, gFOMAML = L′\nτ,B(eφ); and ﬁnally (4) plug gFOMAML into the outer-loop optimizer.\n3\nReptile\nIn this section, we describe a new ﬁrst-order gradient-based meta-learning algorithm called Reptile.\nLike MAML, Reptile learns an initialization for the parameters of a neural network model, such\nthat when we optimize these parameters at test time, learning is fast—i.e., the model generalizes\nfrom a small number of examples from the test task. The Reptile algorithm is as follows:\nAlgorithm 1 Reptile (serial version)\nInitialize φ, the vector of initial parameters\nfor iteration = 1, 2, . . . do\nSample task τ, corresponding to loss Lτ on weight vectors eφ\nCompute eφ = Uk\nτ (φ), denoting k steps of SGD or Adam\nUpdate φ ←φ + ϵ(eφ −φ)\nend for\nIn the last step, instead of simply updating φ in the direction eφ −φ, we can treat (φ −eφ) as a\ngradient and plug it into an adaptive algorithm such as Adam [10]. (Actually, as we will discuss in\nSection 5.1, it is most natural to deﬁne the Reptile gradient as (φ −eφ)/α, where α is the stepsize\n3\n\n\nused by the SGD operation.) We can also deﬁne a parallel or batch version of the algorithm that\nevaluates on n tasks each iteration and updates the initialization to\nφ ←φ + ϵ 1\nn\nn\nX\ni=1\n(eφi −φ)\n(5)\nwhere eφi = Uk\nτi(φ); the updated parameters on the ith task.\nThis algorithm looks remarkably similar to joint training on the expected loss Eτ [Lτ]. Indeed,\nif we deﬁne U to be a single step of gradient descent (k = 1), then this algorithm corresponds to\nstochastic gradient descent on the expected loss:\ngReptile,k=1 = Eτ [φ −Uτ(φ)] /α\n(6)\n= Eτ [∇φLτ(φ)]\n(7)\nHowever, if we perform multiple gradient updates in the partial minimization (k > 1), then the\nexpected update Eτ\n\u0002\nUk\nτ (φ)\n\u0003\ndoes not correspond to taking a gradient step on the expected loss\nEτ [Lτ]. Instead, the update includes important terms coming from second-and-higher derivatives\nof Lτ, as we will analyze in Section 5.1. Hence, Reptile converges to a solution that’s very diﬀerent\nfrom the minimizer of the expected loss Eτ [Lτ].\nOther than the stepsize parameter ϵ and task sampling, the batched version of Reptile is the\nsame as the SimuParallelSGD algorithm [21]. SimuParallelSGD is a method for communication-\neﬃcient distributed optimization, where workers perform gradient updates locally and infrequently\naverage their parameters, rather than the standard approach of averaging gradients.\n4\nCase Study: One-Dimensional Sine Wave Regression\nAs a simple case study, let’s consider the 1D sine wave regression problem, which is slightly modiﬁed\nfrom Finn et al. [4]. This problem is instructive since by design, joint training can’t learn a very\nuseful initialization; however, meta-learning methods can.\n• The task τ = (a, b) is deﬁned by the amplitude a and phase φ of a sine wave function\nfτ(x) = a sin(x + b). The task distribution by sampling a ∼U([0.1, 5.0]) and b ∼U([0, 2π]).\n• Sample p points x1, x2, . . . , xp ∼U([−5, 5])\n• Learner sees (x1, y1), (x2, y2), . . . , (xp, yp) and predicts the whole function f(x)\n• Loss is ℓ2 error on the whole interval [−5, 5]\nLτ(f) =\nZ 5\n−5\ndx∥f(x) −fτ(x)∥2\n(8)\nWe calculate this integral using 50 equally-spaced points x.\nFirst note that the average function is zero everywhere, i.e., Eτ [fτ(x)] = 0, due to the random\nphase b. Therefore, it is useless to train on the expected loss Eτ [Lτ], as this loss is minimized by\nthe zero function f(x) = 0.\nOn the other hand, MAML and Reptile give us an initialization that outputs approximately\nf(x) = 0 before training on a task τ, but the internal feature representations of the network are such\nthat after training on the sampled datapoints (x1, y1), (x2, y2), . . . , (xp, yp), it closely approximates\n4\n\n\nthe target function fτ. This learning progress is shown in the ﬁgures below. Figure 1 shows that\nafter Reptile training, the network can quickly converge to a sampled sine wave and infer the values\naway from the sampled points. As points of comparison, we also show the behaviors of MAML and\na randomly-initialized network on the same task.\n4\n2\n0\n2\n4\n3\n2\n1\n0\n1\n2\n3\nBefore\nAfter 32\nTrue\nSampled\n(a) Before training\n4\n2\n0\n2\n4\n4\n3\n2\n1\n0\n1\n2\n3\n4\nBefore\nAfter 32\nTrue\nSampled\n(b) After MAML training\n4\n2\n0\n2\n4\n4\n3\n2\n1\n0\n1\n2\n3\n4\nBefore\nAfter 32\nTrue\nSampled\n(c) After Reptile training\nFigure 1: Demonstration of MAML and Reptile on a toy few-shot regression problem, where we train on 10\nsampled points of a sine wave, performing 32 gradient steps on an MLP with layers 1 →64 →64 →1.\n5\nAnalysis\nIn this section, we provide two alternative explanations of why Reptile works.\n5.1\nLeading Order Expansion of the Update\nHere, we will use a Taylor series expansion to approximate the update performed by Reptile and\nMAML. We will show that both algorithms contain the same leading-order terms: the ﬁrst term\nminimizes the expected loss (joint training), the second and more interesting term maximizes\nwithin-task generalization. Speciﬁcally, it maximizes the inner product between the gradients on\ndiﬀerent minibatches from the same task. If gradients from diﬀerent batches have positive inner\nproduct, then taking a gradient step on one batch improves performance on the other batch.\nUnlike in the discussion and analysis of MAML, we won’t consider a training set and test set\nfrom each task; instead, we’ll just assume that each task gives us a sequence of k loss functions\nL1, L2, . . . , Lk; for example, classiﬁcation loss on diﬀerent minibatches. We will use the following\ndeﬁnitions:\ngi = L′\ni(φi)\n(gradient obtained during SGD)\n(9)\nφi+1 = φi −αgi\n(sequence of parameter vectors)\n(10)\ngi = L′\ni(φ1)\n(gradient at initial point)\n(11)\nHi = L′′\ni (φ1)\n(Hessian at initial point)\n(12)\nFor each of these deﬁnitions, i ∈[1, k].\n5\n\n\nFirst, let’s calculate the SGD gradients to O(α2) as follows.\ngi = L′\ni(φi) = L′\ni(φ1) + L′′\ni (φ1)(φi −φ1) + O(∥φi −φ1∥2)\n|\n{z\n}\n=O(α2)\n(Taylor’s theorem)\n(13)\n= gi + Hi(φi −φ1) + O(α2)\n(using deﬁnition of gi, Hi)\n(14)\n= gi −αHi\ni−1\nX\nj=1\ngj + O(α2)\n(using φi −φ1 = −α\ni−1\nX\nj=1\ngj)\n(15)\n= gi −αHi\ni−1\nX\nj=1\ngj + O(α2)\n(using gj = gj + O(α))\n(16)\nNext, we will approximate the MAML gradient. Deﬁne Ui as the operator that updates the\nparameter vector on minibatch i: Ui(φ) = φ −αL′\ni(φ).\ngMAML =\n∂\n∂φ1\nLk(φk)\n(17)\n=\n∂\n∂φ1\nLk(Uk−1(Uk−2(. . . (U1(φ1)))))\n(18)\n= U′\n1(φ1) · · · U′\nk−1(φk−1)L′\nk(φk)\n(repeatedly applying the chain rule)\n(19)\n=\n\u0000I −αL′′\n1(φ1)\n\u0001\n· · ·\n\u0000I −αL′′\nk−1(φk−1)\n\u0001\nL′\nk(φk)\n(using U′\ni(φ) = I −αL′′\ni (φ))\n(20)\n=\n\n\nk−1\nY\nj=1\n(I −αL′′\nj (φj))\n\ngk\n(product notation, deﬁnition of gk)\n(21)\nNext, let’s expand to leading order\ngMAML =\n\n\nk−1\nY\nj=1\n(I −αHj)\n\n\n\ngk −αHk\nk−1\nX\nj=1\ngj\n\n+ O(α2)\n(22)\n(replacing L′′\nj (φj) with Hj, and replacing gk using Equation (16))\n=\n\nI −α\nk−1\nX\nj=1\nHj\n\n\n\ngk −αHk\nk−1\nX\nj=1\ngj\n\n+ O(α2)\n(23)\n= gk −α\nk−1\nX\nj=1\nHjgk −αHk\nk−1\nX\nj=1\ngj + O(α2)\n(24)\nFor simplicity of exposition, let’s consider the k = 2 case, and later we’ll provide the general\nformulas.\ngMAML\n= g2 −αH2g1 −αH1g2 + O(α2)\n(25)\ngFOMAML = g2\n= g2 −αH2g1 + O(α2)\n(26)\ngReptile = g1 + g2 = g1 + g2 −αH2g1 + O(α2)\n(27)\nAs we will show in the next paragraph, the terms like H2g1 serve to maximize the inner products\nbetween the gradients computed on diﬀerent minibatches, while lone gradient terms like g1 take us\nto the minimum of the joint training problem.\n6\n\n\nWhen we take the expectation of gFOMAML, gReptile, and gMAML under minibatch sampling,\nwe are left with only two kinds of terms which we will call AvgGrad and AvgGradInner. In the\nequations below Eτ,1,2 [. . . ] means that we are taking the expectation over the task τ and the two\nminibatches deﬁning L1 and L2, respectively.\n• AvgGrad is deﬁned as gradient of expected loss.\nAvgGrad = Eτ,1 [g1]\n(28)\n(−AvgGrad) is the direction that brings φ towards the minimum of the “joint training”\nproblem; the expected loss over tasks.\n• The more interesting term is AvgGradInner, deﬁned as follows:\nAvgGradInner = Eτ,1,2\n\u0002\nH2g1\n\u0003\n(29)\n= Eτ,1,2\n\u0002\nH1g2\n\u0003\n(interchanging indices 1, 2)\n(30)\n= 1\n2Eτ,1,2\n\u0002\nH2g1 + H1g2\n\u0003\n(averaging last two equations)\n(31)\n= 1\n2Eτ,1,2\n\u0014 ∂\n∂φ1\n(g1 · g2)\n\u0015\n(32)\nThus, (−AvgGradInner) is the direction that increases the inner product between gradients\nof diﬀerent minibatches for a given task, improving generalization.\nRecalling our gradient expressions, we get the following expressions for the meta-gradients, for\nSGD with k = 2:\nE [gMAML] = (1)AvgGrad −(2α)AvgGradInner + O(α2)\n(33)\nE [gFOMAML] = (1)AvgGrad −(α)AvgGradInner + O(α2)\n(34)\nE [gReptile] = (2)AvgGrad −(α)AvgGradInner + O(α2)\n(35)\nIn practice, all three gradient expressions ﬁrst bring us towards the minimum of the expected loss\nover tasks, then the higher-order AvgGradInner term enables fast learning by maximizing the inner\nproduct between gradients within a given task.\nFinally, we can extend these calculations to the general k ≥2 case:\ngMAML = gk −αHk\nk−1\nX\nj=1\ngj −α\nk−1\nX\nj=1\nHjgk + O(α2)\n(36)\nE [gMAML] = (1)AvgGrad −(2(k −1)α)AvgGradInner\n(37)\ngFOMAML = gk = gk −αHk\nk−1\nX\nj=1\ngj + O(α2)\n(38)\nE [gFOMAML] = (1)AvgGrad −((k −1)α)AvgGradInner\n(39)\ngReptile = −(φk+1 −φ1)/α =\nk\nX\ni=1\ngi =\nk\nX\ni=1\ngi −α\nk\nX\ni=1\ni−1\nX\nj=1\nHigj + O(α2)\n(40)\nE [gReptile] = (k)AvgGrad −\n\u0000 1\n2k(k −1)α\n\u0001\nAvgGradInner\n(41)\nAs in the k = 2, the ratio of coeﬃcients of the AvgGradInner term and the AvgGrad term goes\nMAML > FOMAML > Reptile. However, in all cases, this ratio increases linearly with both the\nstepsize α and the number of iterations k. Note that the Taylor series approximation only holds\nfor small αk.\n7\n\n\n풲*1\n풲*2\nϕ\nFigure 2: The above illustration shows the sequence of iterates obtained by moving alternately towards two\noptimal solution manifolds W1 and W2 and converging to the point that minimizes the average squared\ndistance. One might object to this picture on the grounds that we converge to the same point regardless of\nwhether we perform one step or multiple steps of gradient descent. That statement is true, however, note\nthat minimizing the expected distance objective Eτ [D(φ, Wτ)] is diﬀerent than minimizing the expected loss\nobjective Eτ [Lτ(fφ)]. In particular, there is a high-dimensional manifold of minimizers of the expected loss\nLτ (e.g., in the sine wave case, many neural network parameters give the zero function f(φ) = 0), but the\nminimizer of the expected distance objective is typically a single point.\n5.2\nFinding a Point Near All Solution Manifolds\nHere, we argue that Reptile converges towards a solution φ that is close (in Euclidean distance) to\neach task τ’s manifold of optimal solutions. This is a informal argument and should be taken much\nless seriously than the preceding Taylor series analysis.\nLet φ denote the network initialization, and let Wτ denote the set of optimal parameters for\ntask τ. We want to ﬁnd φ such that the distance D(φ, Wτ) is small for all tasks.\nminimize\nφ\nEτ\n\u00021\n2D(φ, Wτ)2\u0003\n(42)\nWe will show that Reptile corresponds to performing SGD on that objective.\nGiven a non-pathological set S ⊂Rd, then for almost all points φ ∈Rd the gradient of the\nsquared distance D(φ, S)2 is 2(φ −PS(φ)), where PS(φ) is the projection (closest point) of φ onto\nS. Thus,\n∇φEτ\n\u00021\n2D(φ, Wτ)2\u0003\n= Eτ\n\u00021\n2∇φD(φ, Wτ)2\u0003\n(43)\n= Eτ [φ −PWτ (φ)] , where PWτ (φ) = arg min\np∈Wτ\nD(p, φ)\n(44)\nEach iteration of Reptile corresponds to sampling a task τ and performing a stochastic gradient\nupdate\nφ ←φ −ϵ∇φ 1\n2D(φ, Wτ)2\n(45)\n= φ −ϵ(φ −PWτ (φ))\n(46)\n= (1 −ϵ)φ + ϵPWτ (φ).\n(47)\nIn practice, we can’t exactly compute PWτ (φ), which is deﬁned as a minimizer of Lτ. However, we\ncan partially minimize this loss using gradient descent. Hence, in Reptile we replace W ∗\nτ (φ) by the\nresult of running k steps of gradient descent on Lτ starting with initialization φ.\n6\nExperiments\n6.1\nFew-Shot Classiﬁcation\nWe evaluate our method on two popular few-shot classiﬁcation tasks: Omniglot [11] and Mini-\nImageNet [18]. These datasets make it easy to compare our method to other few-shot learning\n8\n\n\napproaches like MAML.\nIn few-shot classiﬁcation tasks, we have a meta-dataset D containing many classes C, where each\nclass is itself a set of example instances {c1, c2, ..., cn}. If we are doing K-shot, N-way classiﬁcation,\nthen we sample tasks by selecting N classes from C and then selecting K + 1 examples for each\nclass. We split these examples into a training set and a test set, where the test set contains a single\nexample for each class. The model gets to see the entire training set, and then it must classify a\nrandomly chosen sample from the test set. For example, if you trained a model for 5-shot, 5-way\nclassiﬁcation, then you would show it 25 examples (5 per class) and ask it to classify a 26th example.\nIn addition to the above setup, we also experimented with the transductive setting, where the\nmodel classiﬁes the entire test set at once. In our transductive experiments, information was shared\nbetween the test samples via batch normalization [9]. In our non-transductive experiments, batch\nnormalization statistics were computed using all of the training samples and a single test sample.\nWe note that Finn et al. [4] use transduction for evaluating MAML.\nFor our experiments, we used the same CNN architectures and data preprocessing as Finn et\nal. [4]. We used the Adam optimizer [10] in the inner loop, and vanilla SGD in the outer loop,\nthroughout our experiments. For Adam we set β1 = 0 because we found that momentum reduced\nperformance across the board.1\nDuring training, we never reset or interpolated Adam’s rolling\nmoment data; instead, we let it update automatically at every inner-loop training step. However,\nwe did backup and reset the Adam statistics when evaluating on the test set to avoid information\nleakage.\nThe results on Omniglot and Mini-ImageNet are shown in Tables 1 and 2.\nWhile MAML,\nFOMAML, and Reptile have very similar performance on all of these tasks, Reptile does slightly\nbetter than the alternatives on Mini-ImageNet and slightly worse on Omniglot. It also seems that\ntransduction gives a performance boost in all cases, suggesting that further research should pay\nclose attention to its use of batch normalization during testing.\nAlgorithm\n1-shot 5-way\n5-shot 5-way\nMAML + Transduction\n48.70 ± 1.84%\n63.11 ± 0.92%\n1st-order MAML + Transduction\n48.07 ± 1.75%\n63.15 ± 0.91%\nReptile\n47.07 ± 0.26%\n62.74 ± 0.37%\nReptile + Transduction\n49.97 ± 0.32%\n65.99 ± 0.58%\nTable 1: Results on Mini-ImageNet. Both MAML and 1st-order MAML results are from [4].\nAlgorithm\n1-shot 5-way\n5-shot 5-way\n1-shot 20-way\n5-shot 20-way\nMAML + Transduction\n98.7 ± 0.4%\n99.9 ± 0.1%\n95.8 ± 0.3%\n98.9 ± 0.2%\n1st-order MAML + Transduction\n98.3 ± 0.5%\n99.2 ± 0.2%\n89.4 ± 0.5%\n97.9 ± 0.1%\nReptile\n95.39 ± 0.09%\n98.90 ± 0.10%\n88.14 ± 0.15%\n96.65 ± 0.33%\nReptile + Transduction\n97.68 ± 0.04%\n99.48 ± 0.06%\n89.43 ± 0.14%\n97.12 ± 0.32%\nTable 2: Results on Omniglot. MAML results are from [4]. 1st-order MAML results were generated by the\ncode for [4] with the same hyper-parameters as MAML.\n1This ﬁnding also matches our analysis from Section 5.1, which suggests that Reptile works because sequential\nsteps come from diﬀerent mini-batches. With momentum, a mini-batch has inﬂuence over the next few steps, reducing\nthis eﬀect.\n9\n\n\n6.2\nComparing Diﬀerent Inner-Loop Gradient Combinations\nFor this experiment, we used four non-overlapping mini-batches in each inner-loop, yielding gra-\ndients g1, g2, g3, and g4.\nWe then compared learning performance when using diﬀerent linear\ncombinations of the gi’s for the outer loop update.\nNote that two-step Reptile corresponds to\ng1 + g2, and two-step FOMAML corresponds to g2.\nTo make it easier to get an apples-to-apples comparison between diﬀerent linear combinations,\nwe simpliﬁed our experimental setup in several ways. First, we used vanilla SGD in the inner- and\nouter-loops. Second, we did not use meta-batches. Third, we restricted our experiments to 5-shot,\n5-way Omniglot. With these simpliﬁcations, we did not have to worry as much about the eﬀects\nof hyper-parameters or optimizers.\nFigure 3 shows the learning curves for various inner-loop gradient combinations. For gradient\ncombinations with more than one term, we ran both a sum and an average of the inner gradients\nto correct for the eﬀective step size increase.\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n40000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\ng1\n1\n2 * (g1 + g2)\ng1 + g2\ng2\n1\n3 * (g1 + g2 + g3)\ng1 + g2 + g3\ng3\n1\n4 * (g1 + g2 + g3 + g4)\ng1 + g2 + g3 + g4\ng4\nFigure 3: Diﬀerent inner-loop gradient combinations on 5-shot 5-way Omniglot.\nAs expected, using only the ﬁrst gradient g1 is quite ineﬀective, since it amounts to opti-\nmizing the expected loss over all tasks. Surprisingly, two-step Reptile is noticeably worse than\ntwo-step FOMAML, which might be explained by the fact that two-step Reptile puts less weight\non AvgGradInner relative to AvgGrad (Equations (34) and (35)). Most importantly, though, all\nthe methods improve as the number of mini-batches increases. This improvement is more signiﬁcant\nwhen using a sum of all gradients (Reptile) rather than using just the ﬁnal gradient (FOMAML).\nThis also suggests that Reptile can beneﬁt from taking many inner loop steps, which is consistent\nwith the optimal hyper-parameters found for Section 6.1.\n10\n\n\n1\n3\n5\n7\n9\n11\n13\n15\nInner Iterations\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest Accuracy\nReptile (cycling)\nFOMAML (separate-tail, cycling)\nFOMAML (shared-tail, replacement)\nFOMAML (shared-tail, cycling)\n(a) Final test performance vs.\nnumber of inner-loop iterations.\n20\n40\n60\n80\n100\nInner Batch Size\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest Accuracy\nReptile (cycling)\nFOMAML (separate-tail, cycling)\nFOMAML (shared-tail, replacement)\nFOMAML (shared-tail, cycling)\n(b) Final test performance vs.\ninner-loop batch size.\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\nlog2(Initial Outer Step)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest Accuracy\n(c) Final test performance vs.\nouter-loop step size for shared-\ntail FOMAML with batch size\n100 (full batches).\nFigure 4: The results of hyper-parameter sweeps on 5-shot 5-way Omniglot.\n6.3\nOverlap Between Inner-Loop Mini-Batches\nBoth Reptile and FOMAML use stochastic optimization in their inner-loops. Small changes to\nthis optimization procedure can lead to large changes in ﬁnal performance. This section explores\nthe sensitivity of Reptile and FOMAML to the inner loop hyperparameters, and also shows that\nFOMAML’s performance signiﬁcantly drops if mini-batches are selected the wrong way.\nThe experiments in this section look at the diﬀerence between shared-tail FOMAML, where\nthe ﬁnal inner-loop mini-batch comes from the same set of data as the earlier inner-loop batches,\nto separate-tail FOMAML, where the ﬁnal mini-batch comes from a disjoint set of data. Viewing\nFOMAML as an approximation to MAML, separate-tail FOMAML can be seen as the more correct\napproach (and was used by Finn et al. [4]), since the training-time optimization resembles the\ntest-time optimization (where the test set doesn’t overlap with the training set). Indeed, we ﬁnd\nthat separate-tail FOMAML is signiﬁcantly better than shared-tail FOMAML. As we will show,\nshared-tail FOMAML degrades in performance when the data used to compute the meta-gradient\n(gFOMAML = gk) overlaps signiﬁcantly with the earlier batches; however, Reptile and separate-tail\nMAML maintain performance and are not very sensitive to the inner-loop hyperparameters.\nFigure 4a shows that when minibatches are selected by cycling through the training data\n(shared-tail, cycle), shared-tail FOMAML performs well up to four inner-loop iterations, but\ndrops in performance starting at ﬁve iterations, where the ﬁnal minibatch (used to compute\ngFOMAML = gk) overlaps with the earlier ones. When we use random sampling instead (shared-tail,\nreplacement), shared-tail FOMAML degrades more gradually. We hypothesize that this is because\nsome samples still appear in the ﬁnal batch that were not in the previous batches. The eﬀect is\nstochastic, so it makes sense that the curve is smoother.\nFigure 4b shows a similar phenomenon, but here we ﬁxed the inner-loop to four iterations\nand instead varied the batch size. For batch sizes greater than 25, the ﬁnal inner-loop batch for\nshared-tail FOMAML necessarily contains samples from the previous batches. Similar to Figure 4a,\nhere we observe that shared-tail FOMAML with random sampling degrades more gradually than\nshared-tail FOMAML with cycling.\nIn both of these parameter sweeps, separate-tail FOMAML and Reptile do not degrade in\nperformance as the number of inner-loop iterations or batch size changes.\nThere are several possible explanations for above ﬁndings. For example, one might hypothesize\nthat shared-tail FOMAML is only worse in these experiments because its eﬀective step size is\nmuch lower than that of separate-tail FOMAML. However, Figure 4c suggests that this is not the\n11\n\n\ncase: performance was equally poor for every choice of step size in a thorough sweep. A diﬀerent\nhypothesis is that shared-tail FOMAML performs poorly because, after a few inner-loop steps on\na sample, the gradient of the loss for that sample does not contain very much useful information\nabout the sample. In other words, the ﬁrst few SGD steps might bring the model close to a local\noptimum, and then further SGD steps might simply bounce around this local optimum.\n7\nDiscussion\nMeta-learning algorithms that perform gradient descent at test time are appealing because of their\nsimplicity and generalization properties [5].\nThe eﬀectiveness of ﬁne-tuning (e.g.\nfrom models\ntrained on ImageNet [2]) gives us additional faith in these approaches. This paper proposed a new\nalgorithm called Reptile, whose training process is only subtlely diﬀerent from joint training and\nonly uses ﬁrst-order gradient information (like ﬁrst-order MAML).\nWe gave two theoretical explanations for why Reptile works. First, by approximating the update\nwith a Taylor series, we showed that SGD automatically gives us the same kind of second-order\nterm that MAML computes. This term adjusts the initial weights to maximize the dot product\nbetween the gradients of diﬀerent minibatches on the same task—i.e., it encourages the gradients\nto generalize between minibatches of the same task. We also provided a second informal argument,\nwhich is that Reptile ﬁnds a point that is close (in Euclidean distance) to all of the optimal solution\nmanifolds of the training tasks.\nWhile this paper studies the meta-learning setting, the Taylor series analysis in Section 5.1\nmay have some bearing on stochastic gradient descent in general. It suggests that when doing\nstochastic gradient descent, we are automatically performing a MAML-like update that maximizes\nthe generalization between diﬀerent minibatches. This observation partly explains why ﬁne tuning\n(e.g., from ImageNet to a smaller dataset [20]) works well. This hypothesis would suggest that joint\ntraining plus ﬁne tuning will continue to be a strong baseline for meta-learning in various machine\nlearning problems.\n8\nFuture Work\nWe see several promising directions for future work:\n• Understanding to what extent SGD automatically optimizes for generalization, and whether\nthis eﬀect can be ampliﬁed in the non-meta-learning setting.\n• Applying Reptile in the reinforcement learning setting. So far, we have obtained negative\nresults, since joint training is a strong baseline, so some modiﬁcations to Reptile might be\nnecessary.\n• Exploring whether Reptile’s few-shot learning performance can be improved by deeper archi-\ntectures for the classiﬁer.\n• Exploring whether regularization can improve few-shot learning performance, as currently\nthere is a large gap between training and testing error.\n• Evaluating Reptile on the task of few-shot density modeling [14].\n12\n\n\nReferences\n[1] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoﬀman, David Pfau, Tom Schaul, and\nNando de Freitas. Learning to learn by gradient descent by gradient descent. In Advances in Neural\nInformation Processing Systems, pages 3981–3989, 2016.\n[2] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-\nerarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE\nConference on, pages 248–255. IEEE, 2009.\n[3] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2: Fast\nreinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.\n[4] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of\ndeep networks. arXiv preprint arXiv:1703.03400, 2017.\n[5] Chelsea Finn and Sergey Levine. Meta-learning and universality: Deep representations and gradient\ndescent can approximate any learning algorithm. arXiv preprint arXiv:1710.11622, 2017.\n[6] Nikolaus Hansen. The CMA evolution strategy: a comparing review. In Towards a new evolutionary\ncomputation, pages 75–102. Springer, 2006.\n[7] Geoﬀrey E Hinton and David C Plaut. Using fast weights to deblur old memories. In Proceedings of\nthe ninth annual conference of the Cognitive Science Society, pages 177–186, 1987.\n[8] Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent. In\nInternational Conference on Artiﬁcial Neural Networks, pages 87–94. Springer, 2001.\n[9] Sergey Ioﬀe and Christian Szegedy. Batch normalization: Accelerating deep network training by reduc-\ning internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\n[10] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International\nConference on Learning Representations (ICLR), 2015.\n[11] Brenden M. Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua B. Tenenbaum. One shot learning\nof simple visual concepts. In Conference of the Cognitive Science Society (CogSci), 2011.\n[12] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum.\nHuman-level concept learning\nthrough probabilistic program induction. Science, 350(6266):1332–1338, 2015.\n[13] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International\nConference on Learning Representations (ICLR), 2017.\n[14] Scott Reed, Yutian Chen, Thomas Paine, A¨aron van den Oord, SM Eslami, Danilo Rezende, Oriol\nVinyals, and Nando de Freitas. Few-shot autoregressive density estimation: Towards learning to learn\ndistributions. arXiv preprint arXiv:1710.10304, 2017.\n[15] Ruslan Salakhutdinov, Joshua Tenenbaum, and Antonio Torralba. One-shot learning with a hierarchi-\ncal nonparametric bayesian model. In Proceedings of ICML Workshop on Unsupervised and Transfer\nLearning, pages 195–206, 2012.\n[16] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-\nlearning with memory-augmented neural networks. In International conference on machine learning,\npages 1842–1850, 2016.\n[17] Lauren A Schmidt. Meaning and compositionality as statistical induction of categories and constraints.\nPhD thesis, Massachusetts Institute of Technology, 2009.\n[18] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one shot\nlearning. In Advances in Neural Information Processing Systems, pages 3630–3638, 2016.\n[19] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot, and Nando De Freitas.\nDueling network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581, 2015.\n13\n\n\n[20] Ning Zhang, JeﬀDonahue, Ross Girshick, and Trevor Darrell. Part-based R-CNNs for ﬁne-grained\ncategory detection. In European conference on computer vision, pages 834–849. Springer, 2014.\n[21] Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J Smola.\nParallelized stochastic gradient\ndescent. In Advances in neural information processing systems, pages 2595–2603, 2010.\nA\nHyper-parameters\nFor all experiments, we linearly annealed the outer step size to 0. We ran each experiment with\nthree diﬀerent random seeds, and computed the conﬁdence intervals using the standard deviation\nacross the runs.\nInitially, we tried optimizing the Reptile hyper-parameters using CMA-ES [6]. However, we\nfound that most hyper-parameters had little eﬀect on the resulting performance. After seeing this\nresult, we simpliﬁed all of the hyper-parameters and shared hyper-parameters between experiments\nwhen it made sense.\nTable 3: Reptile hyper-parameters for the Omniglot comparison between all algorithms.\nParameter\n5-way\n20-way\nAdam learning rate\n0.001\n0.0005\nInner batch size\n10\n20\nInner iterations\n5\n10\nTraining shots\n10\n10\nOuter step size\n1.0\n1.0\nOuter iterations\n100K\n200K\nMeta-batch size\n5\n5\nEval. inner iterations\n50\n50\nEval. inner batch\n5\n10\nTable 4: Reptile hyper-parameters for the Mini-ImageNet comparison between all algorithms.\nParameter\n1-shot\n5-shot\nAdam learning rate\n0.001\n0.001\nInner batch size\n10\n10\nInner iterations\n8\n8\nTraining shots\n15\n15\nOuter step size\n1.0\n1.0\nOuter iterations\n100K\n100K\nMeta-batch size\n5\n5\nEval. inner batch size\n5\n15\nEval. inner iterations\n50\n50\n14\n\n\nTable 5: Hyper-parameters for Section 6.2. All outer step sizes were linearly annealed to zero during training.\nParameter\nValue\nInner learning rate\n3 × 10−3\nInner batch size\n25\nOuter step size\n0.25\nOuter iterations\n40K\nEval. inner batch size\n25\nEval. inner iterations\n5\nTable 6: Hyper-parameters Section 6.3. All outer step sizes were linearly annealed to zero during training.\nParameter\nFigure 4b\nFigure 4a\nFigure 4c\nInner learning rate\n3 × 10−3\n3 × 10−3\n3 × 10−3\nInner batch size\n-\n25\n100\nInner iterations\n4\n-\n4\nOuter step size\n1.0\n1.0\n-\nOuter iterations\n40K\n40K\n40K\nEval. inner batch size\n25\n25\n25\nEval. inner iterations\n5\n5\n5\n15\n"
}