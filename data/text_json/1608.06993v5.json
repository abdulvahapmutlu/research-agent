{
  "filename": "1608.06993v5.pdf",
  "num_pages": 9,
  "pages": [
    "Densely Connected Convolutional Networks\nGao Huang∗\nCornell University\ngh349@cornell.edu\nZhuang Liu∗\nTsinghua University\nliuzhuang13@mails.tsinghua.edu.cn\nLaurens van der Maaten\nFacebook AI Research\nlvdmaaten@fb.com\nKilian Q. Weinberger\nCornell University\nkqw4@cornell.edu\nAbstract\nRecent work has shown that convolutional networks can\nbe substantially deeper, more accurate, and efﬁcient to train\nif they contain shorter connections between layers close to\nthe input and those close to the output. In this paper, we\nembrace this observation and introduce the Dense Convo-\nlutional Network (DenseNet), which connects each layer\nto every other layer in a feed-forward fashion. Whereas\ntraditional convolutional networks with L layers have L\nconnections—one between each layer and its subsequent\nlayer—our network has\nL(L+1)\n2\ndirect connections.\nFor\neach layer, the feature-maps of all preceding layers are\nused as inputs, and its own feature-maps are used as inputs\ninto all subsequent layers. DenseNets have several com-\npelling advantages: they alleviate the vanishing-gradient\nproblem, strengthen feature propagation, encourage fea-\nture reuse, and substantially reduce the number of parame-\nters. We evaluate our proposed architecture on four highly\ncompetitive object recognition benchmark tasks (CIFAR-10,\nCIFAR-100, SVHN, and ImageNet). DenseNets obtain sig-\nniﬁcant improvements over the state-of-the-art on most of\nthem, whilst requiring less computation to achieve high per-\nformance. Code and pre-trained models are available at\nhttps://github.com/liuzhuang13/DenseNet.\n1. Introduction\nConvolutional neural networks (CNNs) have become\nthe dominant machine learning approach for visual object\nrecognition. Although they were originally introduced over\n20 years ago [18], improvements in computer hardware and\nnetwork structure have enabled the training of truly deep\nCNNs only recently. The original LeNet5 [19] consisted of\n5 layers, VGG featured 19 [29], and only last year Highway\n∗Authors contributed equally\nx0\nx1\nH1\nx2\nH2\nH3\nH4\nx3\nx4\nFigure 1: A 5-layer dense block with a growth rate of k = 4.\nEach layer takes all preceding feature-maps as input.\nNetworks [34] and Residual Networks (ResNets) [11] have\nsurpassed the 100-layer barrier.\nAs CNNs become increasingly deep, a new research\nproblem emerges: as information about the input or gra-\ndient passes through many layers, it can vanish and “wash\nout” by the time it reaches the end (or beginning) of the\nnetwork. Many recent publications address this or related\nproblems. ResNets [11] and Highway Networks [34] by-\npass signal from one layer to the next via identity connec-\ntions. Stochastic depth [13] shortens ResNets by randomly\ndropping layers during training to allow better information\nand gradient ﬂow. FractalNets [17] repeatedly combine sev-\neral parallel layer sequences with different number of con-\nvolutional blocks to obtain a large nominal depth, while\nmaintaining many short paths in the network.\nAlthough\nthese different approaches vary in network topology and\ntraining procedure, they all share a key characteristic: they\ncreate short paths from early layers to later layers.\n1\narXiv:1608.06993v5  [cs.CV]  28 Jan 2018\n",
    "In this paper, we propose an architecture that distills this\ninsight into a simple connectivity pattern: to ensure maxi-\nmum information ﬂow between layers in the network, we\nconnect all layers (with matching feature-map sizes) di-\nrectly with each other. To preserve the feed-forward nature,\neach layer obtains additional inputs from all preceding lay-\ners and passes on its own feature-maps to all subsequent\nlayers. Figure 1 illustrates this layout schematically. Cru-\ncially, in contrast to ResNets, we never combine features\nthrough summation before they are passed into a layer; in-\nstead, we combine features by concatenating them. Hence,\nthe ℓth layer has ℓinputs, consisting of the feature-maps\nof all preceding convolutional blocks. Its own feature-maps\nare passed on to all L−ℓsubsequent layers. This introduces\nL(L+1)\n2\nconnections in an L-layer network, instead of just\nL, as in traditional architectures. Because of its dense con-\nnectivity pattern, we refer to our approach as Dense Convo-\nlutional Network (DenseNet).\nA possibly counter-intuitive effect of this dense connec-\ntivity pattern is that it requires fewer parameters than tra-\nditional convolutional networks, as there is no need to re-\nlearn redundant feature-maps. Traditional feed-forward ar-\nchitectures can be viewed as algorithms with a state, which\nis passed on from layer to layer. Each layer reads the state\nfrom its preceding layer and writes to the subsequent layer.\nIt changes the state but also passes on information that needs\nto be preserved. ResNets [11] make this information preser-\nvation explicit through additive identity transformations.\nRecent variations of ResNets [13] show that many layers\ncontribute very little and can in fact be randomly dropped\nduring training. This makes the state of ResNets similar\nto (unrolled) recurrent neural networks [21], but the num-\nber of parameters of ResNets is substantially larger because\neach layer has its own weights. Our proposed DenseNet ar-\nchitecture explicitly differentiates between information that\nis added to the network and information that is preserved.\nDenseNet layers are very narrow (e.g., 12 ﬁlters per layer),\nadding only a small set of feature-maps to the “collective\nknowledge” of the network and keep the remaining feature-\nmaps unchanged—and the ﬁnal classiﬁer makes a decision\nbased on all feature-maps in the network.\nBesides better parameter efﬁciency, one big advantage of\nDenseNets is their improved ﬂow of information and gra-\ndients throughout the network, which makes them easy to\ntrain. Each layer has direct access to the gradients from the\nloss function and the original input signal, leading to an im-\nplicit deep supervision [20]. This helps training of deeper\nnetwork architectures. Further, we also observe that dense\nconnections have a regularizing effect, which reduces over-\nﬁtting on tasks with smaller training set sizes.\nWe evaluate DenseNets on four highly competitive\nbenchmark datasets (CIFAR-10, CIFAR-100, SVHN, and\nImageNet). Our models tend to require much fewer param-\neters than existing algorithms with comparable accuracy.\nFurther, we signiﬁcantly outperform the current state-of-\nthe-art results on most of the benchmark tasks.\n2. Related Work\nThe exploration of network architectures has been a part\nof neural network research since their initial discovery. The\nrecent resurgence in popularity of neural networks has also\nrevived this research domain. The increasing number of lay-\ners in modern networks ampliﬁes the differences between\narchitectures and motivates the exploration of different con-\nnectivity patterns and the revisiting of old research ideas.\nA cascade structure similar to our proposed dense net-\nwork layout has already been studied in the neural networks\nliterature in the 1980s [3]. Their pioneering work focuses on\nfully connected multi-layer perceptrons trained in a layer-\nby-layer fashion. More recently, fully connected cascade\nnetworks to be trained with batch gradient descent were\nproposed [40]. Although effective on small datasets, this\napproach only scales to networks with a few hundred pa-\nrameters. In [9, 23, 31, 41], utilizing multi-level features\nin CNNs through skip-connnections has been found to be\neffective for various vision tasks. Parallel to our work, [1]\nderived a purely theoretical framework for networks with\ncross-layer connections similar to ours.\nHighway Networks [34] were amongst the ﬁrst architec-\ntures that provided a means to effectively train end-to-end\nnetworks with more than 100 layers. Using bypassing paths\nalong with gating units, Highway Networks with hundreds\nof layers can be optimized without difﬁculty. The bypass-\ning paths are presumed to be the key factor that eases the\ntraining of these very deep networks. This point is further\nsupported by ResNets [11], in which pure identity mappings\nare used as bypassing paths. ResNets have achieved im-\npressive, record-breaking performance on many challeng-\ning image recognition, localization, and detection tasks,\nsuch as ImageNet and COCO object detection [11]. Re-\ncently, stochastic depth was proposed as a way to success-\nfully train a 1202-layer ResNet [13]. Stochastic depth im-\nproves the training of deep residual networks by dropping\nlayers randomly during training. This shows that not all\nlayers may be needed and highlights that there is a great\namount of redundancy in deep (residual) networks. Our pa-\nper was partly inspired by that observation. ResNets with\npre-activation also facilitate the training of state-of-the-art\nnetworks with > 1000 layers [12].\nAn orthogonal approach to making networks deeper\n(e.g., with the help of skip connections) is to increase the\nnetwork width. The GoogLeNet [36, 37] uses an “Incep-\ntion module” which concatenates feature-maps produced\nby ﬁlters of different sizes. In [38], a variant of ResNets\nwith wide generalized residual blocks was proposed.\nIn\nfact, simply increasing the number of ﬁlters in each layer of\n",
    "C\no\nn\nv\no\nl\nu\nt\ni\no\nn\nP\no\no\nl\ni\nn\ng\nDense Block 1\nC\no\nn\nv\no\nl\nu\nt\ni\no\nn\nP\no\no\nl\ni\nn\ng\nP\no\no\nl\ni\nn\ng\nL\ni\nn\ne\na\nr\nC\no\nn\nv\no\nl\nu\nt\ni\no\nn\nInput\nPrediction\n“horse”\nDense Block 2\nDense Block 3\nFigure 2: A deep DenseNet with three dense blocks. The layers between two adjacent blocks are referred to as transition layers and change\nfeature-map sizes via convolution and pooling.\nResNets can improve its performance provided the depth is\nsufﬁcient [42]. FractalNets also achieve competitive results\non several datasets using a wide network structure [17].\nInstead of drawing representational power from ex-\ntremely deep or wide architectures, DenseNets exploit the\npotential of the network through feature reuse, yielding con-\ndensed models that are easy to train and highly parameter-\nefﬁcient. Concatenating feature-maps learned by different\nlayers increases variation in the input of subsequent layers\nand improves efﬁciency. This constitutes a major difference\nbetween DenseNets and ResNets. Compared to Inception\nnetworks [36, 37], which also concatenate features from dif-\nferent layers, DenseNets are simpler and more efﬁcient.\nThere are other notable network architecture innovations\nwhich have yielded competitive results. The Network in\nNetwork (NIN) [22] structure includes micro multi-layer\nperceptrons into the ﬁlters of convolutional layers to ex-\ntract more complicated features. In Deeply Supervised Net-\nwork (DSN) [20], internal layers are directly supervised\nby auxiliary classiﬁers, which can strengthen the gradients\nreceived by earlier layers. Ladder Networks [27, 25] in-\ntroduce lateral connections into autoencoders, producing\nimpressive accuracies on semi-supervised learning tasks.\nIn [39], Deeply-Fused Nets (DFNs) were proposed to im-\nprove information ﬂow by combining intermediate layers\nof different base networks. The augmentation of networks\nwith pathways that minimize reconstruction losses was also\nshown to improve image classiﬁcation models [43].\n3. DenseNets\nConsider a single image x0 that is passed through a con-\nvolutional network. The network comprises L layers, each\nof which implements a non-linear transformation Hℓ(·),\nwhere ℓindexes the layer. Hℓ(·) can be a composite func-\ntion of operations such as Batch Normalization (BN) [14],\nrectiﬁed linear units (ReLU) [6], Pooling [19], or Convolu-\ntion (Conv). We denote the output of the ℓth layer as xℓ.\nResNets.\nTraditional\nconvolutional\nfeed-forward\nnet-\nworks connect the output of the ℓth layer as input to the\n(ℓ+ 1)th layer [16], which gives rise to the following\nlayer transition: xℓ= Hℓ(xℓ−1).\nResNets [11] add a\nskip-connection that bypasses the non-linear transforma-\ntions with an identity function:\nxℓ= Hℓ(xℓ−1) + xℓ−1.\n(1)\nAn advantage of ResNets is that the gradient can ﬂow di-\nrectly through the identity function from later layers to the\nearlier layers. However, the identity function and the output\nof Hℓare combined by summation, which may impede the\ninformation ﬂow in the network.\nDense connectivity.\nTo further improve the information\nﬂow between layers we propose a different connectivity\npattern: we introduce direct connections from any layer\nto all subsequent layers. Figure 1 illustrates the layout of\nthe resulting DenseNet schematically. Consequently, the\nℓth layer receives the feature-maps of all preceding layers,\nx0, . . . , xℓ−1, as input:\nxℓ= Hℓ([x0, x1, . . . , xℓ−1]),\n(2)\nwhere [x0, x1, . . . , xℓ−1] refers to the concatenation of the\nfeature-maps produced in layers 0, . . . , ℓ−1. Because of its\ndense connectivity we refer to this network architecture as\nDense Convolutional Network (DenseNet). For ease of im-\nplementation, we concatenate the multiple inputs of Hℓ(·)\nin eq. (2) into a single tensor.\nComposite function.\nMotivated by [12], we deﬁne Hℓ(·)\nas a composite function of three consecutive operations:\nbatch normalization (BN) [14], followed by a rectiﬁed lin-\near unit (ReLU) [6] and a 3 × 3 convolution (Conv).\nPooling layers.\nThe concatenation operation used in\nEq. (2) is not viable when the size of feature-maps changes.\nHowever, an essential part of convolutional networks is\ndown-sampling layers that change the size of feature-maps.\nTo facilitate down-sampling in our architecture we divide\nthe network into multiple densely connected dense blocks;\nsee Figure 2. We refer to layers between blocks as transition\nlayers, which do convolution and pooling. The transition\nlayers used in our experiments consist of a batch normal-\nization layer and an 1×1 convolutional layer followed by a\n2×2 average pooling layer.\nGrowth rate.\nIf each function Hℓproduces k feature-\nmaps, it follows that the ℓth layer has k0 +k ×(ℓ−1) input\nfeature-maps, where k0 is the number of channels in the in-\nput layer. An important difference between DenseNet and\nexisting network architectures is that DenseNet can have\nvery narrow layers, e.g., k = 12. We refer to the hyper-\nparameter k as the growth rate of the network. We show in\nSection 4 that a relatively small growth rate is sufﬁcient to\n",
    "Layers\nOutput Size\nDenseNet-121\nDenseNet-169\nDenseNet-201\nDenseNet-264\nConvolution\n112 × 112\n7 × 7 conv, stride 2\nPooling\n56 × 56\n3 × 3 max pool, stride 2\nDense Block\n(1)\n56 × 56\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 6\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 6\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 6\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 6\nTransition Layer\n(1)\n56 × 56\n1 × 1 conv\n28 × 28\n2 × 2 average pool, stride 2\nDense Block\n(2)\n28 × 28\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 12\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 12\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 12\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 12\nTransition Layer\n(2)\n28 × 28\n1 × 1 conv\n14 × 14\n2 × 2 average pool, stride 2\nDense Block\n(3)\n14 × 14\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 24\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 32\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 48\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 64\nTransition Layer\n(3)\n14 × 14\n1 × 1 conv\n7 × 7\n2 × 2 average pool, stride 2\nDense Block\n(4)\n7 × 7\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 16\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 32\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 32\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 48\nClassiﬁcation\nLayer\n1 × 1\n7 × 7 global average pool\n1000D fully-connected, softmax\nTable 1: DenseNet architectures for ImageNet. The growth rate for all the networks is k = 32. Note that each “conv” layer shown in the\ntable corresponds the sequence BN-ReLU-Conv.\nobtain state-of-the-art results on the datasets that we tested\non. One explanation for this is that each layer has access\nto all the preceding feature-maps in its block and, therefore,\nto the network’s “collective knowledge”. One can view the\nfeature-maps as the global state of the network. Each layer\nadds k feature-maps of its own to this state. The growth\nrate regulates how much new information each layer con-\ntributes to the global state. The global state, once written,\ncan be accessed from everywhere within the network and,\nunlike in traditional network architectures, there is no need\nto replicate it from layer to layer.\nBottleneck layers.\nAlthough each layer only produces k\noutput feature-maps, it typically has many more inputs. It\nhas been noted in [37, 11] that a 1×1 convolution can be in-\ntroduced as bottleneck layer before each 3×3 convolution\nto reduce the number of input feature-maps, and thus to\nimprove computational efﬁciency. We ﬁnd this design es-\npecially effective for DenseNet and we refer to our network\nwith such a bottleneck layer, i.e., to the BN-ReLU-Conv(1×\n1)-BN-ReLU-Conv(3×3) version of Hℓ, as DenseNet-B. In\nour experiments, we let each 1×1 convolution produce 4k\nfeature-maps.\nCompression.\nTo further improve model compactness,\nwe can reduce the number of feature-maps at transition\nlayers. If a dense block contains m feature-maps, we let\nthe following transition layer generate ⌊θm⌋output feature-\nmaps, where 0 <θ ≤1 is referred to as the compression fac-\ntor. When θ = 1, the number of feature-maps across transi-\ntion layers remains unchanged. We refer the DenseNet with\nθ<1 as DenseNet-C, and we set θ = 0.5 in our experiment.\nWhen both the bottleneck and transition layers with θ < 1\nare used, we refer to our model as DenseNet-BC.\nImplementation Details.\nOn all datasets except Ima-\ngeNet, the DenseNet used in our experiments has three\ndense blocks that each has an equal number of layers. Be-\nfore entering the ﬁrst dense block, a convolution with 16 (or\ntwice the growth rate for DenseNet-BC) output channels is\nperformed on the input images. For convolutional layers\nwith kernel size 3×3, each side of the inputs is zero-padded\nby one pixel to keep the feature-map size ﬁxed. We use 1×1\nconvolution followed by 2×2 average pooling as transition\nlayers between two contiguous dense blocks. At the end of\nthe last dense block, a global average pooling is performed\nand then a softmax classiﬁer is attached. The feature-map\nsizes in the three dense blocks are 32× 32, 16×16, and\n8×8, respectively. We experiment with the basic DenseNet\nstructure with conﬁgurations {L = 40, k = 12}, {L =\n100, k = 12} and {L = 100, k = 24}.\nFor DenseNet-\nBC, the networks with conﬁgurations {L = 100, k = 12},\n{L=250, k=24} and {L=190, k=40} are evaluated.\nIn our experiments on ImageNet, we use a DenseNet-BC\nstructure with 4 dense blocks on 224×224 input images.\nThe initial convolution layer comprises 2k convolutions of\nsize 7×7 with stride 2; the number of feature-maps in all\nother layers also follow from setting k. The exact network\nconﬁgurations we used on ImageNet are shown in Table 1.\n4. Experiments\nWe empirically demonstrate DenseNet’s effectiveness on\nseveral benchmark datasets and compare with state-of-the-\nart architectures, especially with ResNet and its variants.\n",
    "Method\nDepth\nParams\nC10\nC10+\nC100\nC100+\nSVHN\nNetwork in Network [22]\n-\n-\n10.41\n8.81\n35.68\n-\n2.35\nAll-CNN [32]\n-\n-\n9.08\n7.25\n-\n33.71\n-\nDeeply Supervised Net [20]\n-\n-\n9.69\n7.97\n-\n34.57\n1.92\nHighway Network [34]\n-\n-\n-\n7.72\n-\n32.39\n-\nFractalNet [17]\n21\n38.6M\n10.18\n5.22\n35.34\n23.30\n2.01\nwith Dropout/Drop-path\n21\n38.6M\n7.33\n4.60\n28.20\n23.73\n1.87\nResNet [11]\n110\n1.7M\n-\n6.61\n-\n-\n-\nResNet (reported by [13])\n110\n1.7M\n13.63\n6.41\n44.74\n27.22\n2.01\nResNet with Stochastic Depth [13]\n110\n1.7M\n11.66\n5.23\n37.80\n24.58\n1.75\n1202\n10.2M\n-\n4.91\n-\n-\n-\nWide ResNet [42]\n16\n11.0M\n-\n4.81\n-\n22.07\n-\n28\n36.5M\n-\n4.17\n-\n20.50\n-\nwith Dropout\n16\n2.7M\n-\n-\n-\n-\n1.64\nResNet (pre-activation) [12]\n164\n1.7M\n11.26∗\n5.46\n35.58∗\n24.33\n-\n1001\n10.2M\n10.56∗\n4.62\n33.47∗\n22.71\n-\nDenseNet (k = 12)\n40\n1.0M\n7.00\n5.24\n27.55\n24.42\n1.79\nDenseNet (k = 12)\n100\n7.0M\n5.77\n4.10\n23.79\n20.20\n1.67\nDenseNet (k = 24)\n100\n27.2M\n5.83\n3.74\n23.42\n19.25\n1.59\nDenseNet-BC (k = 12)\n100\n0.8M\n5.92\n4.51\n24.15\n22.27\n1.76\nDenseNet-BC (k = 24)\n250\n15.3M\n5.19\n3.62\n19.64\n17.60\n1.74\nDenseNet-BC (k = 40)\n190\n25.6M\n-\n3.46\n-\n17.18\n-\nTable 2: Error rates (%) on CIFAR and SVHN datasets. k denotes network’s growth rate. Results that surpass all competing methods are\nbold and the overall best results are blue. “+” indicates standard data augmentation (translation and/or mirroring). ∗indicates results run\nby ourselves. All the results of DenseNets without data augmentation (C10, C100, SVHN) are obtained using Dropout. DenseNets achieve\nlower error rates while using fewer parameters than ResNet. Without data augmentation, DenseNet performs better by a large margin.\n4.1. Datasets\nCIFAR.\nThe two CIFAR datasets [15] consist of colored\nnatural images with 32×32 pixels. CIFAR-10 (C10) con-\nsists of images drawn from 10 and CIFAR-100 (C100) from\n100 classes. The training and test sets contain 50,000 and\n10,000 images respectively, and we hold out 5,000 training\nimages as a validation set. We adopt a standard data aug-\nmentation scheme (mirroring/shifting) that is widely used\nfor these two datasets [11, 13, 17, 22, 28, 20, 32, 34]. We\ndenote this data augmentation scheme by a “+” mark at the\nend of the dataset name (e.g., C10+). For preprocessing,\nwe normalize the data using the channel means and stan-\ndard deviations. For the ﬁnal run we use all 50,000 training\nimages and report the ﬁnal test error at the end of training.\nSVHN.\nThe Street View House Numbers (SVHN) dataset\n[24] contains 32×32 colored digit images. There are 73,257\nimages in the training set, 26,032 images in the test set, and\n531,131 images for additional training. Following common\npractice [7, 13, 20, 22, 30] we use all the training data with-\nout any data augmentation, and a validation set with 6,000\nimages is split from the training set. We select the model\nwith the lowest validation error during training and report\nthe test error. We follow [42] and divide the pixel values by\n255 so they are in the [0, 1] range.\nImageNet.\nThe ILSVRC 2012 classiﬁcation dataset [2]\nconsists 1.2 million images for training, and 50,000 for val-\nidation, from 1, 000 classes. We adopt the same data aug-\nmentation scheme for training images as in [8, 11, 12], and\napply a single-crop or 10-crop with size 224×224 at test\ntime. Following [11, 12, 13], we report classiﬁcation errors\non the validation set.\n4.2. Training\nAll the networks are trained using stochastic gradient de-\nscent (SGD). On CIFAR and SVHN we train using batch\nsize 64 for 300 and 40 epochs, respectively.\nThe initial\nlearning rate is set to 0.1, and is divided by 10 at 50% and\n75% of the total number of training epochs. On ImageNet,\nwe train models for 90 epochs with a batch size of 256.\nThe learning rate is set to 0.1 initially, and is lowered by\n10 times at epoch 30 and 60. Note that a naive implemen-\ntation of DenseNet may contain memory inefﬁciencies. To\nreduce the memory consumption on GPUs, please refer to\nour technical report on the memory-efﬁcient implementa-\ntion of DenseNets [26].\nFollowing [8], we use a weight decay of 10−4 and a\nNesterov momentum [35] of 0.9 without dampening. We\nadopt the weight initialization introduced by [10]. For the\nthree datasets without data augmentation, i.e., C10, C100\n",
    "Model\ntop-1\ntop-5\nDenseNet-121 25.02 / 23.61 7.71 / 6.66\nDenseNet-169 23.80 / 22.08 6.85 / 5.92\nDenseNet-201 22.58 / 21.46 6.34 / 5.54\nDenseNet-264 22.15 / 20.80 6.12 / 5.29\nTable 3: The top-1 and top-5 error rates on the\nImageNet validation set, with single-crop / 10-\ncrop testing.\n0\n1\n2\n3\n4\n5\n6\n7\n8\nx 10\n7\n21.5\n22.5\n23.5\n24.5\n25.5\n26.5\n27.5\n#parameters\nvalidation error (%)\nResNet−34\nResNet−101\nResNet−152\nDenseNet−121\n \nDenseNet−169\nDenseNet−201\nDenseNet−264\nResNets\nDenseNets−BC\n0.5\n0.75\n1\n1.25\n1.5\n1.75\n2\n2.25\n2.5\nx 10\n10\n21.5\n22.5\n23.5\n24.5\n25.5\n26.5\n27.5\n#flops\nvalidation error (%)\nResNet−34\nResNet−101\nResNet−152\nDenseNet−121\n DenseNet−169\nDenseNet−201\nDenseNet−264\nResNets\nDenseNets−BC\nResNet−50\nResNet−50\nFigure 3: Comparison of the DenseNets and ResNets top-1 error rates (single-crop\ntesting) on the ImageNet validation dataset as a function of learned parameters (left)\nand FLOPs during test-time (right).\nand SVHN, we add a dropout layer [33] after each convolu-\ntional layer (except the ﬁrst one) and set the dropout rate to\n0.2. The test errors were only evaluated once for each task\nand model setting.\n4.3. Classiﬁcation Results on CIFAR and SVHN\nWe train DenseNets with different depths, L, and growth\nrates, k. The main results on CIFAR and SVHN are shown\nin Table 2. To highlight general trends, we mark all results\nthat outperform the existing state-of-the-art in boldface and\nthe overall best result in blue.\nAccuracy.\nPossibly the most noticeable trend may orig-\ninate from the bottom row of Table 2, which shows that\nDenseNet-BC with L = 190 and k = 40 outperforms\nthe existing state-of-the-art consistently on all the CIFAR\ndatasets. Its error rates of 3.46% on C10+ and 17.18% on\nC100+ are signiﬁcantly lower than the error rates achieved\nby wide ResNet architecture [42].\nOur best results on\nC10 and C100 (without data augmentation) are even more\nencouraging: both are close to 30% lower than Fractal-\nNet with drop-path regularization [17]. On SVHN, with\ndropout, the DenseNet with L = 100 and k = 24 also\nsurpasses the current best result achieved by wide ResNet.\nHowever, the 250-layer DenseNet-BC doesn’t further im-\nprove the performance over its shorter counterpart. This\nmay be explained by that SVHN is a relatively easy task,\nand extremely deep models may overﬁt to the training set.\nCapacity.\nWithout compression or bottleneck layers,\nthere is a general trend that DenseNets perform better as\nL and k increase. We attribute this primarily to the corre-\nsponding growth in model capacity. This is best demon-\nstrated by the column of C10+ and C100+. On C10+, the\nerror drops from 5.24% to 4.10% and ﬁnally to 3.74% as\nthe number of parameters increases from 1.0M, over 7.0M\nto 27.2M. On C100+, we observe a similar trend. This sug-\ngests that DenseNets can utilize the increased representa-\ntional power of bigger and deeper models. It also indicates\nthat they do not suffer from overﬁtting or the optimization\ndifﬁculties of residual networks [11].\nParameter Efﬁciency.\nThe results in Table 2 indicate that\nDenseNets utilize parameters more efﬁciently than alterna-\ntive architectures (in particular, ResNets). The DenseNet-\nBC with bottleneck structure and dimension reduction at\ntransition layers is particularly parameter-efﬁcient. For ex-\nample, our 250-layer model only has 15.3M parameters, but\nit consistently outperforms other models such as FractalNet\nand Wide ResNets that have more than 30M parameters. We\nalso highlight that DenseNet-BC with L = 100 and k = 12\nachieves comparable performance (e.g., 4.51% vs 4.62% er-\nror on C10+, 22.27% vs 22.71% error on C100+) as the\n1001-layer pre-activation ResNet using 90% fewer parame-\nters. Figure 4 (right panel) shows the training loss and test\nerrors of these two networks on C10+. The 1001-layer deep\nResNet converges to a lower training loss value but a similar\ntest error. We analyze this effect in more detail below.\nOverﬁtting.\nOne positive side-effect of the more efﬁcient\nuse of parameters is a tendency of DenseNets to be less\nprone to overﬁtting. We observe that on the datasets without\ndata augmentation, the improvements of DenseNet architec-\ntures over prior work are particularly pronounced. On C10,\nthe improvement denotes a 29% relative reduction in error\nfrom 7.33% to 5.19%. On C100, the reduction is about 30%\nfrom 28.20% to 19.64%. In our experiments, we observed\npotential overﬁtting in a single setting: on C10, a 4× growth\nof parameters produced by increasing k =12 to k =24 lead\nto a modest increase in error from 5.77% to 5.83%. The\nDenseNet-BC bottleneck and compression layers appear to\nbe an effective way to counter this trend.\n4.4. Classiﬁcation Results on ImageNet\nWe evaluate DenseNet-BC with different depths and\ngrowth rates on the ImageNet classiﬁcation task, and com-\npare it with state-of-the-art ResNet architectures. To en-\nsure a fair comparison between the two architectures, we\neliminate all other factors such as differences in data pre-\nprocessing and optimization settings by adopting the pub-\nlicly available Torch implementation for ResNet by [8]1.\n1https://github.com/facebook/fb.resnet.torch\n",
    "0\n1\n2\n3\n4\n5\n6\n7\n8\n#parameters\n×105\n4\n6\n8\n10\n12\n14\n16\ntest error (%)\nDenseNet \nDenseNet-C\nDenseNet-B \nDenseNet-BC\n0\n1\n2\n3\n4\n5\n6\n7\n8\n#parameters\n⇥105\n4\n6\n8\n10\n12\n14\n16\ntest error (%)\nResNet\nDenseNet-BC\n3x fewer parameters\n0\n50\n100\n150\n200\n250\n300\nepoch\n4\n6\n8\n10\n12\n14\n16\ntest error (%)\nTest error: ResNet-1001 (10.2M)\nTest error: DenseNet-BC-100 (0.8M)\nTraining loss: ResNet-1001 (10.2M)\nTraining loss: DenseNet-BC-100 (0.8M)\n10−3\n10−2\n10−1\n100\ntraining loss\nFigure 4: Left: Comparison of the parameter efﬁciency on C10+ between DenseNet variations. Middle: Comparison of the parameter\nefﬁciency between DenseNet-BC and (pre-activation) ResNets. DenseNet-BC requires about 1/3 of the parameters as ResNet to achieve\ncomparable accuracy. Right: Training and testing curves of the 1001-layer pre-activation ResNet [12] with more than 10M parameters and\na 100-layer DenseNet with only 0.8M parameters.\nWe simply replace the ResNet model with the DenseNet-\nBC network, and keep all the experiment settings exactly\nthe same as those used for ResNet.\nWe report the single-crop and 10-crop validation errors\nof DenseNets on ImageNet in Table 3.\nFigure 3 shows\nthe single-crop top-1 validation errors of DenseNets and\nResNets as a function of the number of parameters (left) and\nFLOPs (right). The results presented in the ﬁgure reveal that\nDenseNets perform on par with the state-of-the-art ResNets,\nwhilst requiring signiﬁcantly fewer parameters and compu-\ntation to achieve comparable performance. For example, a\nDenseNet-201 with 20M parameters model yields similar\nvalidation error as a 101-layer ResNet with more than 40M\nparameters. Similar trends can be observed from the right\npanel, which plots the validation error as a function of the\nnumber of FLOPs: a DenseNet that requires as much com-\nputation as a ResNet-50 performs on par with a ResNet-101,\nwhich requires twice as much computation.\nIt is worth noting that our experimental setup implies\nthat we use hyperparameter settings that are optimized for\nResNets but not for DenseNets. It is conceivable that more\nextensive hyper-parameter searches may further improve\nthe performance of DenseNet on ImageNet.\n5. Discussion\nSuperﬁcially, DenseNets are quite similar to ResNets:\nEq. (2) differs from Eq. (1) only in that the inputs to Hℓ(·)\nare concatenated instead of summed. However, the implica-\ntions of this seemingly small modiﬁcation lead to substan-\ntially different behaviors of the two network architectures.\nModel compactness.\nAs a direct consequence of the in-\nput concatenation, the feature-maps learned by any of the\nDenseNet layers can be accessed by all subsequent layers.\nThis encourages feature reuse throughout the network, and\nleads to more compact models.\nThe left two plots in Figure 4 show the result of an\nexperiment that aims to compare the parameter efﬁciency\nof all variants of DenseNets (left) and also a comparable\nResNet architecture (middle). We train multiple small net-\nworks with varying depths on C10+ and plot their test ac-\ncuracies as a function of network parameters.\nIn com-\nparison with other popular network architectures, such as\nAlexNet [16] or VGG-net [29], ResNets with pre-activation\nuse fewer parameters while typically achieving better re-\nsults [12]. Hence, we compare DenseNet (k = 12) against\nthis architecture. The training setting for DenseNet is kept\nthe same as in the previous section.\nThe graph shows that DenseNet-BC is consistently the\nmost parameter efﬁcient variant of DenseNet. Further, to\nachieve the same level of accuracy, DenseNet-BC only re-\nquires around 1/3 of the parameters of ResNets (middle\nplot). This result is in line with the results on ImageNet\nwe presented in Figure 3. The right plot in Figure 4 shows\nthat a DenseNet-BC with only 0.8M trainable parameters\nis able to achieve comparable accuracy as the 1001-layer\n(pre-activation) ResNet [12] with 10.2M parameters.\nImplicit Deep Supervision.\nOne explanation for the im-\nproved accuracy of dense convolutional networks may be\nthat individual layers receive additional supervision from\nthe loss function through the shorter connections. One can\ninterpret DenseNets to perform a kind of “deep supervi-\nsion”.\nThe beneﬁts of deep supervision have previously\nbeen shown in deeply-supervised nets (DSN; [20]), which\nhave classiﬁers attached to every hidden layer, enforcing the\nintermediate layers to learn discriminative features.\nDenseNets perform a similar deep supervision in an im-\nplicit fashion: a single classiﬁer on top of the network pro-\nvides direct supervision to all layers through at most two or\nthree transition layers. However, the loss function and gra-\ndient of DenseNets are substantially less complicated, as the\nsame loss function is shared between all layers.\nStochastic vs.\ndeterministic connection.\nThere is an\ninteresting connection between dense convolutional net-\nworks and stochastic depth regularization of residual net-\nworks [13]. In stochastic depth, layers in residual networks\nare randomly dropped, which creates direct connections be-\n",
    "tween the surrounding layers. As the pooling layers are\nnever dropped, the network results in a similar connectiv-\nity pattern as DenseNet: there is a small probability for\nany two layers, between the same pooling layers, to be di-\nrectly connected—if all intermediate layers are randomly\ndropped. Although the methods are ultimately quite dif-\nferent, the DenseNet interpretation of stochastic depth may\nprovide insights into the success of this regularizer.\nFeature Reuse.\nBy design, DenseNets allow layers ac-\ncess to feature-maps from all of its preceding layers (al-\nthough sometimes through transition layers). We conduct\nan experiment to investigate if a trained network takes ad-\nvantage of this opportunity. We ﬁrst train a DenseNet on\nC10+ with L = 40 and k = 12. For each convolutional\nlayer ℓwithin a block, we compute the average (absolute)\nweight assigned to connections with layer s. Figure 5 shows\na heat-map for all three dense blocks. The average absolute\nweight serves as a surrogate for the dependency of a convo-\nlutional layer on its preceding layers. A red dot in position\n(ℓ, s) indicates that the layer ℓmakes, on average, strong use\nof feature-maps produced s-layers before. Several observa-\ntions can be made from the plot:\n1. All layers spread their weights over many inputs within\nthe same block. This indicates that features extracted\nby very early layers are, indeed, directly used by deep\nlayers throughout the same dense block.\n2. The weights of the transition layers also spread their\nweight across all layers within the preceding dense\nblock, indicating information ﬂow from the ﬁrst to the\nlast layers of the DenseNet through few indirections.\n3. The layers within the second and third dense block\nconsistently assign the least weight to the outputs of\nthe transition layer (the top row of the triangles), in-\ndicating that the transition layer outputs many redun-\ndant features (with low weight on average). This is in\nkeeping with the strong results of DenseNet-BC where\nexactly these outputs are compressed.\n4. Although the ﬁnal classiﬁcation layer, shown on the\nvery right, also uses weights across the entire dense\nblock, there seems to be a concentration towards ﬁnal\nfeature-maps, suggesting that there may be some more\nhigh-level features produced late in the network.\n6. Conclusion\nWe proposed a new convolutional network architec-\nture, which we refer to as Dense Convolutional Network\n(DenseNet). It introduces direct connections between any\ntwo layers with the same feature-map size. We showed that\nDenseNets scale naturally to hundreds of layers, while ex-\nhibiting no optimization difﬁculties. In our experiments,\nDense Block 1\nSource layer (s)\nDense Block 2\n9\n1\nDense Block 3\nTarget layer ()\n  0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n  1\nTransition layer 1 \nTransition layer 2 \nClassification layer\n1\n3\n5\n7\n2\n4\n6\n8\n10\n12\nTarget layer ()\nTarget layer ()\n1\n3\n5\n7\n9\n11\n1\n3\n5\n7\n9\n11\n2\n4\n6\n8\n10\n12\n2\n4\n6\n8\n10\n12\nFigure 5: The average absolute ﬁlter weights of convolutional lay-\ners in a trained DenseNet. The color of pixel (s, ℓ) encodes the av-\nerage L1 norm (normalized by number of input feature-maps) of\nthe weights connecting convolutional layer s to ℓwithin a dense\nblock. Three columns highlighted by black rectangles correspond\nto two transition layers and the classiﬁcation layer. The ﬁrst row\nencodes weights connected to the input layer of the dense block.\nDenseNets tend to yield consistent improvement in accu-\nracy with growing number of parameters, without any signs\nof performance degradation or overﬁtting.\nUnder multi-\nple settings, it achieved state-of-the-art results across sev-\neral highly competitive datasets.\nMoreover, DenseNets\nrequire substantially fewer parameters and less computa-\ntion to achieve state-of-the-art performances. Because we\nadopted hyperparameter settings optimized for residual net-\nworks in our study, we believe that further gains in accuracy\nof DenseNets may be obtained by more detailed tuning of\nhyperparameters and learning rate schedules.\nWhilst following a simple connectivity rule, DenseNets\nnaturally integrate the properties of identity mappings, deep\nsupervision, and diversiﬁed depth. They allow feature reuse\nthroughout the networks and can consequently learn more\ncompact and, according to our experiments, more accurate\nmodels. Because of their compact internal representations\nand reduced feature redundancy, DenseNets may be good\nfeature extractors for various computer vision tasks that\nbuild on convolutional features, e.g., [4, 5]. We plan to\nstudy such feature transfer with DenseNets in future work.\nAcknowledgements.\nThe authors are supported in part by\nthe NSF III-1618134, III-1526012, IIS-1149882, the Of-\nﬁce of Naval Research Grant N00014-17-1-2175 and the\nBill and Melinda Gates foundation. GH is supported by\nthe International Postdoctoral Exchange Fellowship Pro-\ngram of China Postdoctoral Council (No.20150015). ZL\nis supported by the National Basic Research Program of\nChina Grants 2011CBA00300, 2011CBA00301, the NSFC\n61361136003. We also thank Daniel Sedra, Geoff Pleiss\nand Yu Sun for many insightful discussions.\nReferences\n[1] C. Cortes, X. Gonzalvo, V. Kuznetsov, M. Mohri, and\nS. Yang. Adanet: Adaptive structural learning of artiﬁcial\nneural networks. arXiv preprint arXiv:1607.01097, 2016. 2\n",
    "[2] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei. Imagenet: A large-scale hierarchical image database. In\nCVPR, 2009. 5\n[3] S. E. Fahlman and C. Lebiere. The cascade-correlation learn-\ning architecture. In NIPS, 1989. 2\n[4] J. R. Gardner, M. J. Kusner, Y. Li, P. Upchurch, K. Q.\nWeinberger, and J. E. Hopcroft. Deep manifold traversal:\nChanging labels with convolutional features. arXiv preprint\narXiv:1511.06421, 2015. 8\n[5] L. Gatys, A. Ecker, and M. Bethge. A neural algorithm of\nartistic style. Nature Communications, 2015. 8\n[6] X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectiﬁer\nneural networks. In AISTATS, 2011. 3\n[7] I. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and\nY. Bengio. Maxout networks. In ICML, 2013. 5\n[8] S. Gross and M. Wilber. Training and investigating residual\nnets, 2016. 5, 7\n[9] B. Hariharan, P. Arbeláez, R. Girshick, and J. Malik. Hyper-\ncolumns for object segmentation and ﬁne-grained localiza-\ntion. In CVPR, 2015. 2\n[10] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into\nrectiﬁers: Surpassing human-level performance on imagenet\nclassiﬁcation. In ICCV, 2015. 5\n[11] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In CVPR, 2016. 1, 2, 3, 4, 5, 6\n[12] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in\ndeep residual networks. In ECCV, 2016. 2, 3, 5, 7\n[13] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger.\nDeep networks with stochastic depth. In ECCV, 2016. 1, 2,\n5, 8\n[14] S. Ioffe and C. Szegedy. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift. In\nICML, 2015. 3\n[15] A. Krizhevsky and G. Hinton. Learning multiple layers of\nfeatures from tiny images. Tech Report, 2009. 5\n[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImagenet\nclassiﬁcation with deep convolutional neural networks. In\nNIPS, 2012. 3, 7\n[17] G. Larsson, M. Maire, and G. Shakhnarovich. Fractalnet:\nUltra-deep neural networks without residuals. arXiv preprint\narXiv:1605.07648, 2016. 1, 3, 5, 6\n[18] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\nHoward, W. Hubbard, and L. D. Jackel. Backpropagation\napplied to handwritten zip code recognition. Neural compu-\ntation, 1(4):541–551, 1989. 1\n[19] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-\nbased learning applied to document recognition. Proceed-\nings of the IEEE, 86(11):2278–2324, 1998. 1, 3\n[20] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-\nsupervised nets. In AISTATS, 2015. 2, 3, 5, 7\n[21] Q. Liao and T. Poggio. Bridging the gaps between residual\nlearning, recurrent neural networks and visual cortex. arXiv\npreprint arXiv:1604.03640, 2016. 2\n[22] M. Lin, Q. Chen, and S. Yan. Network in network. In ICLR,\n2014. 3, 5\n[23] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional\nnetworks for semantic segmentation. In CVPR, 2015. 2\n[24] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.\nNg. Reading digits in natural images with unsupervised fea-\nture learning, 2011. In NIPS Workshop, 2011. 5\n[25] M. Pezeshki, L. Fan, P. Brakel, A. Courville, and Y. Bengio.\nDeconstructing the ladder network architecture. In ICML,\n2016. 3\n[26] G. Pleiss, D. Chen, G. Huang, T. Li, L. van der Maaten,\nand K. Q. Weinberger. Memory-efﬁcient implementation of\ndensenets. arXiv preprint arXiv:1707.06990, 2017. 5\n[27] A. Rasmus, M. Berglund, M. Honkala, H. Valpola, and\nT. Raiko. Semi-supervised learning with ladder networks.\nIn NIPS, 2015. 3\n[28] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta,\nand Y. Bengio. Fitnets: Hints for thin deep nets. In ICLR,\n2015. 5\n[29] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\net al.\nImagenet large scale visual recognition challenge.\nIJCV. 1, 7\n[30] P. Sermanet, S. Chintala, and Y. LeCun. Convolutional neu-\nral networks applied to house numbers digit classiﬁcation. In\nICPR, pages 3288–3291. IEEE, 2012. 5\n[31] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun.\nPedestrian detection with unsupervised multi-stage feature\nlearning. In CVPR, 2013. 2\n[32] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Ried-\nmiller.\nStriving for simplicity: The all convolutional net.\narXiv preprint arXiv:1412.6806, 2014. 5\n[33] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov. Dropout: a simple way to prevent neural\nnetworks from overﬁtting. JMLR, 2014. 6\n[34] R. K. Srivastava, K. Greff, and J. Schmidhuber. Training\nvery deep networks. In NIPS, 2015. 1, 2, 5\n[35] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the\nimportance of initialization and momentum in deep learning.\nIn ICML, 2013. 5\n[36] C. Szegedy,\nW. Liu,\nY. Jia,\nP. Sermanet,\nS. Reed,\nD. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.\nGoing deeper with convolutions. In CVPR, 2015. 2, 3\n[37] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.\nRethinking the inception architecture for computer vision. In\nCVPR, 2016. 2, 3, 4\n[38] S. Targ,\nD. Almeida,\nand K. Lyman.\nResnet in\nresnet: Generalizing residual architectures. arXiv preprint\narXiv:1603.08029, 2016. 2\n[39] J. Wang, Z. Wei, T. Zhang, and W. Zeng. Deeply-fused nets.\narXiv preprint arXiv:1605.07716, 2016. 3\n[40] B. M. Wilamowski and H. Yu.\nNeural network learning\nwithout backpropagation. IEEE Transactions on Neural Net-\nworks, 21(11):1793–1803, 2010. 2\n[41] S. Yang and D. Ramanan. Multi-scale recognition with dag-\ncnns. In ICCV, 2015. 2\n[42] S. Zagoruyko and N. Komodakis. Wide residual networks.\narXiv preprint arXiv:1605.07146, 2016. 3, 5, 6\n[43] Y. Zhang, K. Lee, and H. Lee. Augmenting supervised neural\nnetworks with unsupervised objectives for large-scale image\nclassiﬁcation. In ICML, 2016. 3\n"
  ],
  "full_text": "Densely Connected Convolutional Networks\nGao Huang∗\nCornell University\ngh349@cornell.edu\nZhuang Liu∗\nTsinghua University\nliuzhuang13@mails.tsinghua.edu.cn\nLaurens van der Maaten\nFacebook AI Research\nlvdmaaten@fb.com\nKilian Q. Weinberger\nCornell University\nkqw4@cornell.edu\nAbstract\nRecent work has shown that convolutional networks can\nbe substantially deeper, more accurate, and efﬁcient to train\nif they contain shorter connections between layers close to\nthe input and those close to the output. In this paper, we\nembrace this observation and introduce the Dense Convo-\nlutional Network (DenseNet), which connects each layer\nto every other layer in a feed-forward fashion. Whereas\ntraditional convolutional networks with L layers have L\nconnections—one between each layer and its subsequent\nlayer—our network has\nL(L+1)\n2\ndirect connections.\nFor\neach layer, the feature-maps of all preceding layers are\nused as inputs, and its own feature-maps are used as inputs\ninto all subsequent layers. DenseNets have several com-\npelling advantages: they alleviate the vanishing-gradient\nproblem, strengthen feature propagation, encourage fea-\nture reuse, and substantially reduce the number of parame-\nters. We evaluate our proposed architecture on four highly\ncompetitive object recognition benchmark tasks (CIFAR-10,\nCIFAR-100, SVHN, and ImageNet). DenseNets obtain sig-\nniﬁcant improvements over the state-of-the-art on most of\nthem, whilst requiring less computation to achieve high per-\nformance. Code and pre-trained models are available at\nhttps://github.com/liuzhuang13/DenseNet.\n1. Introduction\nConvolutional neural networks (CNNs) have become\nthe dominant machine learning approach for visual object\nrecognition. Although they were originally introduced over\n20 years ago [18], improvements in computer hardware and\nnetwork structure have enabled the training of truly deep\nCNNs only recently. The original LeNet5 [19] consisted of\n5 layers, VGG featured 19 [29], and only last year Highway\n∗Authors contributed equally\nx0\nx1\nH1\nx2\nH2\nH3\nH4\nx3\nx4\nFigure 1: A 5-layer dense block with a growth rate of k = 4.\nEach layer takes all preceding feature-maps as input.\nNetworks [34] and Residual Networks (ResNets) [11] have\nsurpassed the 100-layer barrier.\nAs CNNs become increasingly deep, a new research\nproblem emerges: as information about the input or gra-\ndient passes through many layers, it can vanish and “wash\nout” by the time it reaches the end (or beginning) of the\nnetwork. Many recent publications address this or related\nproblems. ResNets [11] and Highway Networks [34] by-\npass signal from one layer to the next via identity connec-\ntions. Stochastic depth [13] shortens ResNets by randomly\ndropping layers during training to allow better information\nand gradient ﬂow. FractalNets [17] repeatedly combine sev-\neral parallel layer sequences with different number of con-\nvolutional blocks to obtain a large nominal depth, while\nmaintaining many short paths in the network.\nAlthough\nthese different approaches vary in network topology and\ntraining procedure, they all share a key characteristic: they\ncreate short paths from early layers to later layers.\n1\narXiv:1608.06993v5  [cs.CV]  28 Jan 2018\n\n\nIn this paper, we propose an architecture that distills this\ninsight into a simple connectivity pattern: to ensure maxi-\nmum information ﬂow between layers in the network, we\nconnect all layers (with matching feature-map sizes) di-\nrectly with each other. To preserve the feed-forward nature,\neach layer obtains additional inputs from all preceding lay-\ners and passes on its own feature-maps to all subsequent\nlayers. Figure 1 illustrates this layout schematically. Cru-\ncially, in contrast to ResNets, we never combine features\nthrough summation before they are passed into a layer; in-\nstead, we combine features by concatenating them. Hence,\nthe ℓth layer has ℓinputs, consisting of the feature-maps\nof all preceding convolutional blocks. Its own feature-maps\nare passed on to all L−ℓsubsequent layers. This introduces\nL(L+1)\n2\nconnections in an L-layer network, instead of just\nL, as in traditional architectures. Because of its dense con-\nnectivity pattern, we refer to our approach as Dense Convo-\nlutional Network (DenseNet).\nA possibly counter-intuitive effect of this dense connec-\ntivity pattern is that it requires fewer parameters than tra-\nditional convolutional networks, as there is no need to re-\nlearn redundant feature-maps. Traditional feed-forward ar-\nchitectures can be viewed as algorithms with a state, which\nis passed on from layer to layer. Each layer reads the state\nfrom its preceding layer and writes to the subsequent layer.\nIt changes the state but also passes on information that needs\nto be preserved. ResNets [11] make this information preser-\nvation explicit through additive identity transformations.\nRecent variations of ResNets [13] show that many layers\ncontribute very little and can in fact be randomly dropped\nduring training. This makes the state of ResNets similar\nto (unrolled) recurrent neural networks [21], but the num-\nber of parameters of ResNets is substantially larger because\neach layer has its own weights. Our proposed DenseNet ar-\nchitecture explicitly differentiates between information that\nis added to the network and information that is preserved.\nDenseNet layers are very narrow (e.g., 12 ﬁlters per layer),\nadding only a small set of feature-maps to the “collective\nknowledge” of the network and keep the remaining feature-\nmaps unchanged—and the ﬁnal classiﬁer makes a decision\nbased on all feature-maps in the network.\nBesides better parameter efﬁciency, one big advantage of\nDenseNets is their improved ﬂow of information and gra-\ndients throughout the network, which makes them easy to\ntrain. Each layer has direct access to the gradients from the\nloss function and the original input signal, leading to an im-\nplicit deep supervision [20]. This helps training of deeper\nnetwork architectures. Further, we also observe that dense\nconnections have a regularizing effect, which reduces over-\nﬁtting on tasks with smaller training set sizes.\nWe evaluate DenseNets on four highly competitive\nbenchmark datasets (CIFAR-10, CIFAR-100, SVHN, and\nImageNet). Our models tend to require much fewer param-\neters than existing algorithms with comparable accuracy.\nFurther, we signiﬁcantly outperform the current state-of-\nthe-art results on most of the benchmark tasks.\n2. Related Work\nThe exploration of network architectures has been a part\nof neural network research since their initial discovery. The\nrecent resurgence in popularity of neural networks has also\nrevived this research domain. The increasing number of lay-\ners in modern networks ampliﬁes the differences between\narchitectures and motivates the exploration of different con-\nnectivity patterns and the revisiting of old research ideas.\nA cascade structure similar to our proposed dense net-\nwork layout has already been studied in the neural networks\nliterature in the 1980s [3]. Their pioneering work focuses on\nfully connected multi-layer perceptrons trained in a layer-\nby-layer fashion. More recently, fully connected cascade\nnetworks to be trained with batch gradient descent were\nproposed [40]. Although effective on small datasets, this\napproach only scales to networks with a few hundred pa-\nrameters. In [9, 23, 31, 41], utilizing multi-level features\nin CNNs through skip-connnections has been found to be\neffective for various vision tasks. Parallel to our work, [1]\nderived a purely theoretical framework for networks with\ncross-layer connections similar to ours.\nHighway Networks [34] were amongst the ﬁrst architec-\ntures that provided a means to effectively train end-to-end\nnetworks with more than 100 layers. Using bypassing paths\nalong with gating units, Highway Networks with hundreds\nof layers can be optimized without difﬁculty. The bypass-\ning paths are presumed to be the key factor that eases the\ntraining of these very deep networks. This point is further\nsupported by ResNets [11], in which pure identity mappings\nare used as bypassing paths. ResNets have achieved im-\npressive, record-breaking performance on many challeng-\ning image recognition, localization, and detection tasks,\nsuch as ImageNet and COCO object detection [11]. Re-\ncently, stochastic depth was proposed as a way to success-\nfully train a 1202-layer ResNet [13]. Stochastic depth im-\nproves the training of deep residual networks by dropping\nlayers randomly during training. This shows that not all\nlayers may be needed and highlights that there is a great\namount of redundancy in deep (residual) networks. Our pa-\nper was partly inspired by that observation. ResNets with\npre-activation also facilitate the training of state-of-the-art\nnetworks with > 1000 layers [12].\nAn orthogonal approach to making networks deeper\n(e.g., with the help of skip connections) is to increase the\nnetwork width. The GoogLeNet [36, 37] uses an “Incep-\ntion module” which concatenates feature-maps produced\nby ﬁlters of different sizes. In [38], a variant of ResNets\nwith wide generalized residual blocks was proposed.\nIn\nfact, simply increasing the number of ﬁlters in each layer of\n\n\nC\no\nn\nv\no\nl\nu\nt\ni\no\nn\nP\no\no\nl\ni\nn\ng\nDense Block 1\nC\no\nn\nv\no\nl\nu\nt\ni\no\nn\nP\no\no\nl\ni\nn\ng\nP\no\no\nl\ni\nn\ng\nL\ni\nn\ne\na\nr\nC\no\nn\nv\no\nl\nu\nt\ni\no\nn\nInput\nPrediction\n“horse”\nDense Block 2\nDense Block 3\nFigure 2: A deep DenseNet with three dense blocks. The layers between two adjacent blocks are referred to as transition layers and change\nfeature-map sizes via convolution and pooling.\nResNets can improve its performance provided the depth is\nsufﬁcient [42]. FractalNets also achieve competitive results\non several datasets using a wide network structure [17].\nInstead of drawing representational power from ex-\ntremely deep or wide architectures, DenseNets exploit the\npotential of the network through feature reuse, yielding con-\ndensed models that are easy to train and highly parameter-\nefﬁcient. Concatenating feature-maps learned by different\nlayers increases variation in the input of subsequent layers\nand improves efﬁciency. This constitutes a major difference\nbetween DenseNets and ResNets. Compared to Inception\nnetworks [36, 37], which also concatenate features from dif-\nferent layers, DenseNets are simpler and more efﬁcient.\nThere are other notable network architecture innovations\nwhich have yielded competitive results. The Network in\nNetwork (NIN) [22] structure includes micro multi-layer\nperceptrons into the ﬁlters of convolutional layers to ex-\ntract more complicated features. In Deeply Supervised Net-\nwork (DSN) [20], internal layers are directly supervised\nby auxiliary classiﬁers, which can strengthen the gradients\nreceived by earlier layers. Ladder Networks [27, 25] in-\ntroduce lateral connections into autoencoders, producing\nimpressive accuracies on semi-supervised learning tasks.\nIn [39], Deeply-Fused Nets (DFNs) were proposed to im-\nprove information ﬂow by combining intermediate layers\nof different base networks. The augmentation of networks\nwith pathways that minimize reconstruction losses was also\nshown to improve image classiﬁcation models [43].\n3. DenseNets\nConsider a single image x0 that is passed through a con-\nvolutional network. The network comprises L layers, each\nof which implements a non-linear transformation Hℓ(·),\nwhere ℓindexes the layer. Hℓ(·) can be a composite func-\ntion of operations such as Batch Normalization (BN) [14],\nrectiﬁed linear units (ReLU) [6], Pooling [19], or Convolu-\ntion (Conv). We denote the output of the ℓth layer as xℓ.\nResNets.\nTraditional\nconvolutional\nfeed-forward\nnet-\nworks connect the output of the ℓth layer as input to the\n(ℓ+ 1)th layer [16], which gives rise to the following\nlayer transition: xℓ= Hℓ(xℓ−1).\nResNets [11] add a\nskip-connection that bypasses the non-linear transforma-\ntions with an identity function:\nxℓ= Hℓ(xℓ−1) + xℓ−1.\n(1)\nAn advantage of ResNets is that the gradient can ﬂow di-\nrectly through the identity function from later layers to the\nearlier layers. However, the identity function and the output\nof Hℓare combined by summation, which may impede the\ninformation ﬂow in the network.\nDense connectivity.\nTo further improve the information\nﬂow between layers we propose a different connectivity\npattern: we introduce direct connections from any layer\nto all subsequent layers. Figure 1 illustrates the layout of\nthe resulting DenseNet schematically. Consequently, the\nℓth layer receives the feature-maps of all preceding layers,\nx0, . . . , xℓ−1, as input:\nxℓ= Hℓ([x0, x1, . . . , xℓ−1]),\n(2)\nwhere [x0, x1, . . . , xℓ−1] refers to the concatenation of the\nfeature-maps produced in layers 0, . . . , ℓ−1. Because of its\ndense connectivity we refer to this network architecture as\nDense Convolutional Network (DenseNet). For ease of im-\nplementation, we concatenate the multiple inputs of Hℓ(·)\nin eq. (2) into a single tensor.\nComposite function.\nMotivated by [12], we deﬁne Hℓ(·)\nas a composite function of three consecutive operations:\nbatch normalization (BN) [14], followed by a rectiﬁed lin-\near unit (ReLU) [6] and a 3 × 3 convolution (Conv).\nPooling layers.\nThe concatenation operation used in\nEq. (2) is not viable when the size of feature-maps changes.\nHowever, an essential part of convolutional networks is\ndown-sampling layers that change the size of feature-maps.\nTo facilitate down-sampling in our architecture we divide\nthe network into multiple densely connected dense blocks;\nsee Figure 2. We refer to layers between blocks as transition\nlayers, which do convolution and pooling. The transition\nlayers used in our experiments consist of a batch normal-\nization layer and an 1×1 convolutional layer followed by a\n2×2 average pooling layer.\nGrowth rate.\nIf each function Hℓproduces k feature-\nmaps, it follows that the ℓth layer has k0 +k ×(ℓ−1) input\nfeature-maps, where k0 is the number of channels in the in-\nput layer. An important difference between DenseNet and\nexisting network architectures is that DenseNet can have\nvery narrow layers, e.g., k = 12. We refer to the hyper-\nparameter k as the growth rate of the network. We show in\nSection 4 that a relatively small growth rate is sufﬁcient to\n\n\nLayers\nOutput Size\nDenseNet-121\nDenseNet-169\nDenseNet-201\nDenseNet-264\nConvolution\n112 × 112\n7 × 7 conv, stride 2\nPooling\n56 × 56\n3 × 3 max pool, stride 2\nDense Block\n(1)\n56 × 56\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 6\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 6\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 6\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 6\nTransition Layer\n(1)\n56 × 56\n1 × 1 conv\n28 × 28\n2 × 2 average pool, stride 2\nDense Block\n(2)\n28 × 28\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 12\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 12\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 12\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 12\nTransition Layer\n(2)\n28 × 28\n1 × 1 conv\n14 × 14\n2 × 2 average pool, stride 2\nDense Block\n(3)\n14 × 14\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 24\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 32\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 48\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 64\nTransition Layer\n(3)\n14 × 14\n1 × 1 conv\n7 × 7\n2 × 2 average pool, stride 2\nDense Block\n(4)\n7 × 7\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 16\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 32\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 32\n\u0014 1 × 1 conv\n3 × 3 conv\n\u0015\n× 48\nClassiﬁcation\nLayer\n1 × 1\n7 × 7 global average pool\n1000D fully-connected, softmax\nTable 1: DenseNet architectures for ImageNet. The growth rate for all the networks is k = 32. Note that each “conv” layer shown in the\ntable corresponds the sequence BN-ReLU-Conv.\nobtain state-of-the-art results on the datasets that we tested\non. One explanation for this is that each layer has access\nto all the preceding feature-maps in its block and, therefore,\nto the network’s “collective knowledge”. One can view the\nfeature-maps as the global state of the network. Each layer\nadds k feature-maps of its own to this state. The growth\nrate regulates how much new information each layer con-\ntributes to the global state. The global state, once written,\ncan be accessed from everywhere within the network and,\nunlike in traditional network architectures, there is no need\nto replicate it from layer to layer.\nBottleneck layers.\nAlthough each layer only produces k\noutput feature-maps, it typically has many more inputs. It\nhas been noted in [37, 11] that a 1×1 convolution can be in-\ntroduced as bottleneck layer before each 3×3 convolution\nto reduce the number of input feature-maps, and thus to\nimprove computational efﬁciency. We ﬁnd this design es-\npecially effective for DenseNet and we refer to our network\nwith such a bottleneck layer, i.e., to the BN-ReLU-Conv(1×\n1)-BN-ReLU-Conv(3×3) version of Hℓ, as DenseNet-B. In\nour experiments, we let each 1×1 convolution produce 4k\nfeature-maps.\nCompression.\nTo further improve model compactness,\nwe can reduce the number of feature-maps at transition\nlayers. If a dense block contains m feature-maps, we let\nthe following transition layer generate ⌊θm⌋output feature-\nmaps, where 0 <θ ≤1 is referred to as the compression fac-\ntor. When θ = 1, the number of feature-maps across transi-\ntion layers remains unchanged. We refer the DenseNet with\nθ<1 as DenseNet-C, and we set θ = 0.5 in our experiment.\nWhen both the bottleneck and transition layers with θ < 1\nare used, we refer to our model as DenseNet-BC.\nImplementation Details.\nOn all datasets except Ima-\ngeNet, the DenseNet used in our experiments has three\ndense blocks that each has an equal number of layers. Be-\nfore entering the ﬁrst dense block, a convolution with 16 (or\ntwice the growth rate for DenseNet-BC) output channels is\nperformed on the input images. For convolutional layers\nwith kernel size 3×3, each side of the inputs is zero-padded\nby one pixel to keep the feature-map size ﬁxed. We use 1×1\nconvolution followed by 2×2 average pooling as transition\nlayers between two contiguous dense blocks. At the end of\nthe last dense block, a global average pooling is performed\nand then a softmax classiﬁer is attached. The feature-map\nsizes in the three dense blocks are 32× 32, 16×16, and\n8×8, respectively. We experiment with the basic DenseNet\nstructure with conﬁgurations {L = 40, k = 12}, {L =\n100, k = 12} and {L = 100, k = 24}.\nFor DenseNet-\nBC, the networks with conﬁgurations {L = 100, k = 12},\n{L=250, k=24} and {L=190, k=40} are evaluated.\nIn our experiments on ImageNet, we use a DenseNet-BC\nstructure with 4 dense blocks on 224×224 input images.\nThe initial convolution layer comprises 2k convolutions of\nsize 7×7 with stride 2; the number of feature-maps in all\nother layers also follow from setting k. The exact network\nconﬁgurations we used on ImageNet are shown in Table 1.\n4. Experiments\nWe empirically demonstrate DenseNet’s effectiveness on\nseveral benchmark datasets and compare with state-of-the-\nart architectures, especially with ResNet and its variants.\n\n\nMethod\nDepth\nParams\nC10\nC10+\nC100\nC100+\nSVHN\nNetwork in Network [22]\n-\n-\n10.41\n8.81\n35.68\n-\n2.35\nAll-CNN [32]\n-\n-\n9.08\n7.25\n-\n33.71\n-\nDeeply Supervised Net [20]\n-\n-\n9.69\n7.97\n-\n34.57\n1.92\nHighway Network [34]\n-\n-\n-\n7.72\n-\n32.39\n-\nFractalNet [17]\n21\n38.6M\n10.18\n5.22\n35.34\n23.30\n2.01\nwith Dropout/Drop-path\n21\n38.6M\n7.33\n4.60\n28.20\n23.73\n1.87\nResNet [11]\n110\n1.7M\n-\n6.61\n-\n-\n-\nResNet (reported by [13])\n110\n1.7M\n13.63\n6.41\n44.74\n27.22\n2.01\nResNet with Stochastic Depth [13]\n110\n1.7M\n11.66\n5.23\n37.80\n24.58\n1.75\n1202\n10.2M\n-\n4.91\n-\n-\n-\nWide ResNet [42]\n16\n11.0M\n-\n4.81\n-\n22.07\n-\n28\n36.5M\n-\n4.17\n-\n20.50\n-\nwith Dropout\n16\n2.7M\n-\n-\n-\n-\n1.64\nResNet (pre-activation) [12]\n164\n1.7M\n11.26∗\n5.46\n35.58∗\n24.33\n-\n1001\n10.2M\n10.56∗\n4.62\n33.47∗\n22.71\n-\nDenseNet (k = 12)\n40\n1.0M\n7.00\n5.24\n27.55\n24.42\n1.79\nDenseNet (k = 12)\n100\n7.0M\n5.77\n4.10\n23.79\n20.20\n1.67\nDenseNet (k = 24)\n100\n27.2M\n5.83\n3.74\n23.42\n19.25\n1.59\nDenseNet-BC (k = 12)\n100\n0.8M\n5.92\n4.51\n24.15\n22.27\n1.76\nDenseNet-BC (k = 24)\n250\n15.3M\n5.19\n3.62\n19.64\n17.60\n1.74\nDenseNet-BC (k = 40)\n190\n25.6M\n-\n3.46\n-\n17.18\n-\nTable 2: Error rates (%) on CIFAR and SVHN datasets. k denotes network’s growth rate. Results that surpass all competing methods are\nbold and the overall best results are blue. “+” indicates standard data augmentation (translation and/or mirroring). ∗indicates results run\nby ourselves. All the results of DenseNets without data augmentation (C10, C100, SVHN) are obtained using Dropout. DenseNets achieve\nlower error rates while using fewer parameters than ResNet. Without data augmentation, DenseNet performs better by a large margin.\n4.1. Datasets\nCIFAR.\nThe two CIFAR datasets [15] consist of colored\nnatural images with 32×32 pixels. CIFAR-10 (C10) con-\nsists of images drawn from 10 and CIFAR-100 (C100) from\n100 classes. The training and test sets contain 50,000 and\n10,000 images respectively, and we hold out 5,000 training\nimages as a validation set. We adopt a standard data aug-\nmentation scheme (mirroring/shifting) that is widely used\nfor these two datasets [11, 13, 17, 22, 28, 20, 32, 34]. We\ndenote this data augmentation scheme by a “+” mark at the\nend of the dataset name (e.g., C10+). For preprocessing,\nwe normalize the data using the channel means and stan-\ndard deviations. For the ﬁnal run we use all 50,000 training\nimages and report the ﬁnal test error at the end of training.\nSVHN.\nThe Street View House Numbers (SVHN) dataset\n[24] contains 32×32 colored digit images. There are 73,257\nimages in the training set, 26,032 images in the test set, and\n531,131 images for additional training. Following common\npractice [7, 13, 20, 22, 30] we use all the training data with-\nout any data augmentation, and a validation set with 6,000\nimages is split from the training set. We select the model\nwith the lowest validation error during training and report\nthe test error. We follow [42] and divide the pixel values by\n255 so they are in the [0, 1] range.\nImageNet.\nThe ILSVRC 2012 classiﬁcation dataset [2]\nconsists 1.2 million images for training, and 50,000 for val-\nidation, from 1, 000 classes. We adopt the same data aug-\nmentation scheme for training images as in [8, 11, 12], and\napply a single-crop or 10-crop with size 224×224 at test\ntime. Following [11, 12, 13], we report classiﬁcation errors\non the validation set.\n4.2. Training\nAll the networks are trained using stochastic gradient de-\nscent (SGD). On CIFAR and SVHN we train using batch\nsize 64 for 300 and 40 epochs, respectively.\nThe initial\nlearning rate is set to 0.1, and is divided by 10 at 50% and\n75% of the total number of training epochs. On ImageNet,\nwe train models for 90 epochs with a batch size of 256.\nThe learning rate is set to 0.1 initially, and is lowered by\n10 times at epoch 30 and 60. Note that a naive implemen-\ntation of DenseNet may contain memory inefﬁciencies. To\nreduce the memory consumption on GPUs, please refer to\nour technical report on the memory-efﬁcient implementa-\ntion of DenseNets [26].\nFollowing [8], we use a weight decay of 10−4 and a\nNesterov momentum [35] of 0.9 without dampening. We\nadopt the weight initialization introduced by [10]. For the\nthree datasets without data augmentation, i.e., C10, C100\n\n\nModel\ntop-1\ntop-5\nDenseNet-121 25.02 / 23.61 7.71 / 6.66\nDenseNet-169 23.80 / 22.08 6.85 / 5.92\nDenseNet-201 22.58 / 21.46 6.34 / 5.54\nDenseNet-264 22.15 / 20.80 6.12 / 5.29\nTable 3: The top-1 and top-5 error rates on the\nImageNet validation set, with single-crop / 10-\ncrop testing.\n0\n1\n2\n3\n4\n5\n6\n7\n8\nx 10\n7\n21.5\n22.5\n23.5\n24.5\n25.5\n26.5\n27.5\n#parameters\nvalidation error (%)\nResNet−34\nResNet−101\nResNet−152\nDenseNet−121\n \nDenseNet−169\nDenseNet−201\nDenseNet−264\nResNets\nDenseNets−BC\n0.5\n0.75\n1\n1.25\n1.5\n1.75\n2\n2.25\n2.5\nx 10\n10\n21.5\n22.5\n23.5\n24.5\n25.5\n26.5\n27.5\n#flops\nvalidation error (%)\nResNet−34\nResNet−101\nResNet−152\nDenseNet−121\n DenseNet−169\nDenseNet−201\nDenseNet−264\nResNets\nDenseNets−BC\nResNet−50\nResNet−50\nFigure 3: Comparison of the DenseNets and ResNets top-1 error rates (single-crop\ntesting) on the ImageNet validation dataset as a function of learned parameters (left)\nand FLOPs during test-time (right).\nand SVHN, we add a dropout layer [33] after each convolu-\ntional layer (except the ﬁrst one) and set the dropout rate to\n0.2. The test errors were only evaluated once for each task\nand model setting.\n4.3. Classiﬁcation Results on CIFAR and SVHN\nWe train DenseNets with different depths, L, and growth\nrates, k. The main results on CIFAR and SVHN are shown\nin Table 2. To highlight general trends, we mark all results\nthat outperform the existing state-of-the-art in boldface and\nthe overall best result in blue.\nAccuracy.\nPossibly the most noticeable trend may orig-\ninate from the bottom row of Table 2, which shows that\nDenseNet-BC with L = 190 and k = 40 outperforms\nthe existing state-of-the-art consistently on all the CIFAR\ndatasets. Its error rates of 3.46% on C10+ and 17.18% on\nC100+ are signiﬁcantly lower than the error rates achieved\nby wide ResNet architecture [42].\nOur best results on\nC10 and C100 (without data augmentation) are even more\nencouraging: both are close to 30% lower than Fractal-\nNet with drop-path regularization [17]. On SVHN, with\ndropout, the DenseNet with L = 100 and k = 24 also\nsurpasses the current best result achieved by wide ResNet.\nHowever, the 250-layer DenseNet-BC doesn’t further im-\nprove the performance over its shorter counterpart. This\nmay be explained by that SVHN is a relatively easy task,\nand extremely deep models may overﬁt to the training set.\nCapacity.\nWithout compression or bottleneck layers,\nthere is a general trend that DenseNets perform better as\nL and k increase. We attribute this primarily to the corre-\nsponding growth in model capacity. This is best demon-\nstrated by the column of C10+ and C100+. On C10+, the\nerror drops from 5.24% to 4.10% and ﬁnally to 3.74% as\nthe number of parameters increases from 1.0M, over 7.0M\nto 27.2M. On C100+, we observe a similar trend. This sug-\ngests that DenseNets can utilize the increased representa-\ntional power of bigger and deeper models. It also indicates\nthat they do not suffer from overﬁtting or the optimization\ndifﬁculties of residual networks [11].\nParameter Efﬁciency.\nThe results in Table 2 indicate that\nDenseNets utilize parameters more efﬁciently than alterna-\ntive architectures (in particular, ResNets). The DenseNet-\nBC with bottleneck structure and dimension reduction at\ntransition layers is particularly parameter-efﬁcient. For ex-\nample, our 250-layer model only has 15.3M parameters, but\nit consistently outperforms other models such as FractalNet\nand Wide ResNets that have more than 30M parameters. We\nalso highlight that DenseNet-BC with L = 100 and k = 12\nachieves comparable performance (e.g., 4.51% vs 4.62% er-\nror on C10+, 22.27% vs 22.71% error on C100+) as the\n1001-layer pre-activation ResNet using 90% fewer parame-\nters. Figure 4 (right panel) shows the training loss and test\nerrors of these two networks on C10+. The 1001-layer deep\nResNet converges to a lower training loss value but a similar\ntest error. We analyze this effect in more detail below.\nOverﬁtting.\nOne positive side-effect of the more efﬁcient\nuse of parameters is a tendency of DenseNets to be less\nprone to overﬁtting. We observe that on the datasets without\ndata augmentation, the improvements of DenseNet architec-\ntures over prior work are particularly pronounced. On C10,\nthe improvement denotes a 29% relative reduction in error\nfrom 7.33% to 5.19%. On C100, the reduction is about 30%\nfrom 28.20% to 19.64%. In our experiments, we observed\npotential overﬁtting in a single setting: on C10, a 4× growth\nof parameters produced by increasing k =12 to k =24 lead\nto a modest increase in error from 5.77% to 5.83%. The\nDenseNet-BC bottleneck and compression layers appear to\nbe an effective way to counter this trend.\n4.4. Classiﬁcation Results on ImageNet\nWe evaluate DenseNet-BC with different depths and\ngrowth rates on the ImageNet classiﬁcation task, and com-\npare it with state-of-the-art ResNet architectures. To en-\nsure a fair comparison between the two architectures, we\neliminate all other factors such as differences in data pre-\nprocessing and optimization settings by adopting the pub-\nlicly available Torch implementation for ResNet by [8]1.\n1https://github.com/facebook/fb.resnet.torch\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n#parameters\n×105\n4\n6\n8\n10\n12\n14\n16\ntest error (%)\nDenseNet \nDenseNet-C\nDenseNet-B \nDenseNet-BC\n0\n1\n2\n3\n4\n5\n6\n7\n8\n#parameters\n⇥105\n4\n6\n8\n10\n12\n14\n16\ntest error (%)\nResNet\nDenseNet-BC\n3x fewer parameters\n0\n50\n100\n150\n200\n250\n300\nepoch\n4\n6\n8\n10\n12\n14\n16\ntest error (%)\nTest error: ResNet-1001 (10.2M)\nTest error: DenseNet-BC-100 (0.8M)\nTraining loss: ResNet-1001 (10.2M)\nTraining loss: DenseNet-BC-100 (0.8M)\n10−3\n10−2\n10−1\n100\ntraining loss\nFigure 4: Left: Comparison of the parameter efﬁciency on C10+ between DenseNet variations. Middle: Comparison of the parameter\nefﬁciency between DenseNet-BC and (pre-activation) ResNets. DenseNet-BC requires about 1/3 of the parameters as ResNet to achieve\ncomparable accuracy. Right: Training and testing curves of the 1001-layer pre-activation ResNet [12] with more than 10M parameters and\na 100-layer DenseNet with only 0.8M parameters.\nWe simply replace the ResNet model with the DenseNet-\nBC network, and keep all the experiment settings exactly\nthe same as those used for ResNet.\nWe report the single-crop and 10-crop validation errors\nof DenseNets on ImageNet in Table 3.\nFigure 3 shows\nthe single-crop top-1 validation errors of DenseNets and\nResNets as a function of the number of parameters (left) and\nFLOPs (right). The results presented in the ﬁgure reveal that\nDenseNets perform on par with the state-of-the-art ResNets,\nwhilst requiring signiﬁcantly fewer parameters and compu-\ntation to achieve comparable performance. For example, a\nDenseNet-201 with 20M parameters model yields similar\nvalidation error as a 101-layer ResNet with more than 40M\nparameters. Similar trends can be observed from the right\npanel, which plots the validation error as a function of the\nnumber of FLOPs: a DenseNet that requires as much com-\nputation as a ResNet-50 performs on par with a ResNet-101,\nwhich requires twice as much computation.\nIt is worth noting that our experimental setup implies\nthat we use hyperparameter settings that are optimized for\nResNets but not for DenseNets. It is conceivable that more\nextensive hyper-parameter searches may further improve\nthe performance of DenseNet on ImageNet.\n5. Discussion\nSuperﬁcially, DenseNets are quite similar to ResNets:\nEq. (2) differs from Eq. (1) only in that the inputs to Hℓ(·)\nare concatenated instead of summed. However, the implica-\ntions of this seemingly small modiﬁcation lead to substan-\ntially different behaviors of the two network architectures.\nModel compactness.\nAs a direct consequence of the in-\nput concatenation, the feature-maps learned by any of the\nDenseNet layers can be accessed by all subsequent layers.\nThis encourages feature reuse throughout the network, and\nleads to more compact models.\nThe left two plots in Figure 4 show the result of an\nexperiment that aims to compare the parameter efﬁciency\nof all variants of DenseNets (left) and also a comparable\nResNet architecture (middle). We train multiple small net-\nworks with varying depths on C10+ and plot their test ac-\ncuracies as a function of network parameters.\nIn com-\nparison with other popular network architectures, such as\nAlexNet [16] or VGG-net [29], ResNets with pre-activation\nuse fewer parameters while typically achieving better re-\nsults [12]. Hence, we compare DenseNet (k = 12) against\nthis architecture. The training setting for DenseNet is kept\nthe same as in the previous section.\nThe graph shows that DenseNet-BC is consistently the\nmost parameter efﬁcient variant of DenseNet. Further, to\nachieve the same level of accuracy, DenseNet-BC only re-\nquires around 1/3 of the parameters of ResNets (middle\nplot). This result is in line with the results on ImageNet\nwe presented in Figure 3. The right plot in Figure 4 shows\nthat a DenseNet-BC with only 0.8M trainable parameters\nis able to achieve comparable accuracy as the 1001-layer\n(pre-activation) ResNet [12] with 10.2M parameters.\nImplicit Deep Supervision.\nOne explanation for the im-\nproved accuracy of dense convolutional networks may be\nthat individual layers receive additional supervision from\nthe loss function through the shorter connections. One can\ninterpret DenseNets to perform a kind of “deep supervi-\nsion”.\nThe beneﬁts of deep supervision have previously\nbeen shown in deeply-supervised nets (DSN; [20]), which\nhave classiﬁers attached to every hidden layer, enforcing the\nintermediate layers to learn discriminative features.\nDenseNets perform a similar deep supervision in an im-\nplicit fashion: a single classiﬁer on top of the network pro-\nvides direct supervision to all layers through at most two or\nthree transition layers. However, the loss function and gra-\ndient of DenseNets are substantially less complicated, as the\nsame loss function is shared between all layers.\nStochastic vs.\ndeterministic connection.\nThere is an\ninteresting connection between dense convolutional net-\nworks and stochastic depth regularization of residual net-\nworks [13]. In stochastic depth, layers in residual networks\nare randomly dropped, which creates direct connections be-\n\n\ntween the surrounding layers. As the pooling layers are\nnever dropped, the network results in a similar connectiv-\nity pattern as DenseNet: there is a small probability for\nany two layers, between the same pooling layers, to be di-\nrectly connected—if all intermediate layers are randomly\ndropped. Although the methods are ultimately quite dif-\nferent, the DenseNet interpretation of stochastic depth may\nprovide insights into the success of this regularizer.\nFeature Reuse.\nBy design, DenseNets allow layers ac-\ncess to feature-maps from all of its preceding layers (al-\nthough sometimes through transition layers). We conduct\nan experiment to investigate if a trained network takes ad-\nvantage of this opportunity. We ﬁrst train a DenseNet on\nC10+ with L = 40 and k = 12. For each convolutional\nlayer ℓwithin a block, we compute the average (absolute)\nweight assigned to connections with layer s. Figure 5 shows\na heat-map for all three dense blocks. The average absolute\nweight serves as a surrogate for the dependency of a convo-\nlutional layer on its preceding layers. A red dot in position\n(ℓ, s) indicates that the layer ℓmakes, on average, strong use\nof feature-maps produced s-layers before. Several observa-\ntions can be made from the plot:\n1. All layers spread their weights over many inputs within\nthe same block. This indicates that features extracted\nby very early layers are, indeed, directly used by deep\nlayers throughout the same dense block.\n2. The weights of the transition layers also spread their\nweight across all layers within the preceding dense\nblock, indicating information ﬂow from the ﬁrst to the\nlast layers of the DenseNet through few indirections.\n3. The layers within the second and third dense block\nconsistently assign the least weight to the outputs of\nthe transition layer (the top row of the triangles), in-\ndicating that the transition layer outputs many redun-\ndant features (with low weight on average). This is in\nkeeping with the strong results of DenseNet-BC where\nexactly these outputs are compressed.\n4. Although the ﬁnal classiﬁcation layer, shown on the\nvery right, also uses weights across the entire dense\nblock, there seems to be a concentration towards ﬁnal\nfeature-maps, suggesting that there may be some more\nhigh-level features produced late in the network.\n6. Conclusion\nWe proposed a new convolutional network architec-\nture, which we refer to as Dense Convolutional Network\n(DenseNet). It introduces direct connections between any\ntwo layers with the same feature-map size. We showed that\nDenseNets scale naturally to hundreds of layers, while ex-\nhibiting no optimization difﬁculties. In our experiments,\nDense Block 1\nSource layer (s)\nDense Block 2\n9\n1\nDense Block 3\nTarget layer ()\n  0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n  1\nTransition layer 1 \nTransition layer 2 \nClassification layer\n1\n3\n5\n7\n2\n4\n6\n8\n10\n12\nTarget layer ()\nTarget layer ()\n1\n3\n5\n7\n9\n11\n1\n3\n5\n7\n9\n11\n2\n4\n6\n8\n10\n12\n2\n4\n6\n8\n10\n12\nFigure 5: The average absolute ﬁlter weights of convolutional lay-\ners in a trained DenseNet. The color of pixel (s, ℓ) encodes the av-\nerage L1 norm (normalized by number of input feature-maps) of\nthe weights connecting convolutional layer s to ℓwithin a dense\nblock. Three columns highlighted by black rectangles correspond\nto two transition layers and the classiﬁcation layer. The ﬁrst row\nencodes weights connected to the input layer of the dense block.\nDenseNets tend to yield consistent improvement in accu-\nracy with growing number of parameters, without any signs\nof performance degradation or overﬁtting.\nUnder multi-\nple settings, it achieved state-of-the-art results across sev-\neral highly competitive datasets.\nMoreover, DenseNets\nrequire substantially fewer parameters and less computa-\ntion to achieve state-of-the-art performances. Because we\nadopted hyperparameter settings optimized for residual net-\nworks in our study, we believe that further gains in accuracy\nof DenseNets may be obtained by more detailed tuning of\nhyperparameters and learning rate schedules.\nWhilst following a simple connectivity rule, DenseNets\nnaturally integrate the properties of identity mappings, deep\nsupervision, and diversiﬁed depth. They allow feature reuse\nthroughout the networks and can consequently learn more\ncompact and, according to our experiments, more accurate\nmodels. Because of their compact internal representations\nand reduced feature redundancy, DenseNets may be good\nfeature extractors for various computer vision tasks that\nbuild on convolutional features, e.g., [4, 5]. We plan to\nstudy such feature transfer with DenseNets in future work.\nAcknowledgements.\nThe authors are supported in part by\nthe NSF III-1618134, III-1526012, IIS-1149882, the Of-\nﬁce of Naval Research Grant N00014-17-1-2175 and the\nBill and Melinda Gates foundation. GH is supported by\nthe International Postdoctoral Exchange Fellowship Pro-\ngram of China Postdoctoral Council (No.20150015). ZL\nis supported by the National Basic Research Program of\nChina Grants 2011CBA00300, 2011CBA00301, the NSFC\n61361136003. We also thank Daniel Sedra, Geoff Pleiss\nand Yu Sun for many insightful discussions.\nReferences\n[1] C. Cortes, X. Gonzalvo, V. Kuznetsov, M. Mohri, and\nS. Yang. Adanet: Adaptive structural learning of artiﬁcial\nneural networks. arXiv preprint arXiv:1607.01097, 2016. 2\n\n\n[2] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei. Imagenet: A large-scale hierarchical image database. In\nCVPR, 2009. 5\n[3] S. E. Fahlman and C. Lebiere. The cascade-correlation learn-\ning architecture. In NIPS, 1989. 2\n[4] J. R. Gardner, M. J. Kusner, Y. Li, P. Upchurch, K. Q.\nWeinberger, and J. E. Hopcroft. Deep manifold traversal:\nChanging labels with convolutional features. arXiv preprint\narXiv:1511.06421, 2015. 8\n[5] L. Gatys, A. Ecker, and M. Bethge. A neural algorithm of\nartistic style. Nature Communications, 2015. 8\n[6] X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectiﬁer\nneural networks. In AISTATS, 2011. 3\n[7] I. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and\nY. Bengio. Maxout networks. In ICML, 2013. 5\n[8] S. Gross and M. Wilber. Training and investigating residual\nnets, 2016. 5, 7\n[9] B. Hariharan, P. Arbeláez, R. Girshick, and J. Malik. Hyper-\ncolumns for object segmentation and ﬁne-grained localiza-\ntion. In CVPR, 2015. 2\n[10] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into\nrectiﬁers: Surpassing human-level performance on imagenet\nclassiﬁcation. In ICCV, 2015. 5\n[11] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In CVPR, 2016. 1, 2, 3, 4, 5, 6\n[12] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in\ndeep residual networks. In ECCV, 2016. 2, 3, 5, 7\n[13] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger.\nDeep networks with stochastic depth. In ECCV, 2016. 1, 2,\n5, 8\n[14] S. Ioffe and C. Szegedy. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift. In\nICML, 2015. 3\n[15] A. Krizhevsky and G. Hinton. Learning multiple layers of\nfeatures from tiny images. Tech Report, 2009. 5\n[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImagenet\nclassiﬁcation with deep convolutional neural networks. In\nNIPS, 2012. 3, 7\n[17] G. Larsson, M. Maire, and G. Shakhnarovich. Fractalnet:\nUltra-deep neural networks without residuals. arXiv preprint\narXiv:1605.07648, 2016. 1, 3, 5, 6\n[18] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\nHoward, W. Hubbard, and L. D. Jackel. Backpropagation\napplied to handwritten zip code recognition. Neural compu-\ntation, 1(4):541–551, 1989. 1\n[19] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-\nbased learning applied to document recognition. Proceed-\nings of the IEEE, 86(11):2278–2324, 1998. 1, 3\n[20] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-\nsupervised nets. In AISTATS, 2015. 2, 3, 5, 7\n[21] Q. Liao and T. Poggio. Bridging the gaps between residual\nlearning, recurrent neural networks and visual cortex. arXiv\npreprint arXiv:1604.03640, 2016. 2\n[22] M. Lin, Q. Chen, and S. Yan. Network in network. In ICLR,\n2014. 3, 5\n[23] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional\nnetworks for semantic segmentation. In CVPR, 2015. 2\n[24] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.\nNg. Reading digits in natural images with unsupervised fea-\nture learning, 2011. In NIPS Workshop, 2011. 5\n[25] M. Pezeshki, L. Fan, P. Brakel, A. Courville, and Y. Bengio.\nDeconstructing the ladder network architecture. In ICML,\n2016. 3\n[26] G. Pleiss, D. Chen, G. Huang, T. Li, L. van der Maaten,\nand K. Q. Weinberger. Memory-efﬁcient implementation of\ndensenets. arXiv preprint arXiv:1707.06990, 2017. 5\n[27] A. Rasmus, M. Berglund, M. Honkala, H. Valpola, and\nT. Raiko. Semi-supervised learning with ladder networks.\nIn NIPS, 2015. 3\n[28] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta,\nand Y. Bengio. Fitnets: Hints for thin deep nets. In ICLR,\n2015. 5\n[29] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\net al.\nImagenet large scale visual recognition challenge.\nIJCV. 1, 7\n[30] P. Sermanet, S. Chintala, and Y. LeCun. Convolutional neu-\nral networks applied to house numbers digit classiﬁcation. In\nICPR, pages 3288–3291. IEEE, 2012. 5\n[31] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun.\nPedestrian detection with unsupervised multi-stage feature\nlearning. In CVPR, 2013. 2\n[32] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Ried-\nmiller.\nStriving for simplicity: The all convolutional net.\narXiv preprint arXiv:1412.6806, 2014. 5\n[33] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov. Dropout: a simple way to prevent neural\nnetworks from overﬁtting. JMLR, 2014. 6\n[34] R. K. Srivastava, K. Greff, and J. Schmidhuber. Training\nvery deep networks. In NIPS, 2015. 1, 2, 5\n[35] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the\nimportance of initialization and momentum in deep learning.\nIn ICML, 2013. 5\n[36] C. Szegedy,\nW. Liu,\nY. Jia,\nP. Sermanet,\nS. Reed,\nD. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.\nGoing deeper with convolutions. In CVPR, 2015. 2, 3\n[37] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.\nRethinking the inception architecture for computer vision. In\nCVPR, 2016. 2, 3, 4\n[38] S. Targ,\nD. Almeida,\nand K. Lyman.\nResnet in\nresnet: Generalizing residual architectures. arXiv preprint\narXiv:1603.08029, 2016. 2\n[39] J. Wang, Z. Wei, T. Zhang, and W. Zeng. Deeply-fused nets.\narXiv preprint arXiv:1605.07716, 2016. 3\n[40] B. M. Wilamowski and H. Yu.\nNeural network learning\nwithout backpropagation. IEEE Transactions on Neural Net-\nworks, 21(11):1793–1803, 2010. 2\n[41] S. Yang and D. Ramanan. Multi-scale recognition with dag-\ncnns. In ICCV, 2015. 2\n[42] S. Zagoruyko and N. Komodakis. Wide residual networks.\narXiv preprint arXiv:1605.07146, 2016. 3, 5, 6\n[43] Y. Zhang, K. Lee, and H. Lee. Augmenting supervised neural\nnetworks with unsupervised objectives for large-scale image\nclassiﬁcation. In ICML, 2016. 3\n"
}