{
  "filename": "2007.11498v5.pdf",
  "num_pages": 24,
  "pages": [
    "CrossTransformers: spatially-aware few-shot transfer\nCarl Doersch∗\nAnkush Gupta∗\nAndrew Zisserman∗†\n∗DeepMind, London\n† VGG, Department of Engineering Science, University of Oxford\nAbstract\nGiven new tasks with very little data—such as new classes in a classiﬁcation\nproblem or a domain shift in the input—performance of modern vision systems\ndegrades remarkably quickly. In this work, we illustrate how the neural network\nrepresentations which underpin modern vision systems are subject to supervision\ncollapse, whereby they lose any information that is not necessary for performing\nthe training task, including information that may be necessary for transfer to new\ntasks or domains. We then propose two methods to mitigate this problem. First,\nwe employ self-supervised learning to encourage general-purpose features that\ntransfer better. Second, we propose a novel Transformer based neural network\narchitecture called CrossTransformers, which can take a small number of labeled\nimages and an unlabeled query, ﬁnd coarse spatial correspondence between the\nquery and the labeled images, and then infer class membership by computing\ndistances between spatially-corresponding features. The result is a classiﬁer that\nis more robust to task and domain shift, which we demonstrate via state-of-the-\nart performance on Meta-Dataset, a recent dataset for evaluating transfer from\nImageNet to many other vision datasets. Code and pretrained checkpoints available\nat: https://github.com/google-research/meta-dataset.\n1\nIntroduction\nGeneral-purpose vision systems must be adaptable. Home robots must be able to operate in new,\nunseen homes; photo-organizing software must recognize unseen objects (e.g., to ﬁnd examples of\n“my sixth-grade son’s abstract art project”); industrial quality-assurance systems must spot defects in\nnew products. Deep neural network representations can bring some visual knowledge from datasets\nlike ImageNet [67] to bear on different tasks beyond ImageNet [14, 31, 61], but empirically, this\nrequires a non-trivial amount of labeled data in the new task. With too little labeled data, or for a\nlarge change in distribution, such systems empirically perform poorly.\nResearch on meta-learning directly benchmarks adaptability. At training time, the algorithm receives\na large amount of data and accompanying supervision (e.g., labels). At test time, however, the\nalgorithm receives a series of episodes, each of which consists of a small number of datapoints from a\ndifferent distribution than the training set (e.g., a different domain or different classes). Only a subset\nof this data has the accompanying supervision (called the support set); the algorithm must make\npredictions about the rest (the query set). Meta-Dataset [85] is particularly relevant for vision, as the\nchallenge is few-shot ﬁne-grained image classiﬁcation. The training data is a subset of ImageNet\nclasses. At test time, each episode either contains images from the other ImageNet classes, or from\none of nine other visually distinct ﬁne-grained recognition datasets. The algorithm must rapidly adapt\nits representations to the new classes and domains.\nSimple centroid-based algorithms like Prototypical Nets [16, 75] are near state-of-the-art on Meta-\nDataset, achieving around 50% accuracy on the held-out ImageNet classes in Meta-Dataset’s valida-\ntion set (chance is roughly 1 in 20). An equivalent classiﬁer trained on those validation classes can\nachieve roughly 84% accuracy on the same challenge. What accounts for the enormous discrepancy\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2007.11498v5  [cs.CV]  17 Feb 2021\n",
    "Query\nNearest Neighbors\ngila monster\ngila monster hognose snake gila monster\ngila monster\ngila monster\nnight snake\nhognose snake gila monster\nknot\nletter opener\nletter opener\nscrewdriver\nletter opener\nballpoint\nfountain pen\nbuckle\nletter opener\nletter opener\nbassoon\nhammer\nhammer\nmicrophone\nhammer\nscrew\nhammer\nsyringe\nshovel\nhook\nscrew\nbuckeye\nbuckeye\nbuckeye\nhip\npomegranate\nsunglasses\nbuckeye\nbuckeye\nbuckeye\nscrew\n  : in test split\nFigure 1: Illustration of supervision collapse with nearest neighbors. In each row, the leftmost\nimage is a query taken from the Meta-Dataset ImageNet test classes, and the rest are the top 9\nnearest neighbors from both training and test support set classes, using the embedding learned by a\nPrototypical Net (training details in Appendix B). Images belonging to the test split are indicated by a\nnear the bottom left corner; rest are from the training split. For a simple classiﬁer to work well on\nthese test classes, semantically similar images should have similar representations, and so we hope the\nnearest neighbors would come from the same—or semantically similar—classes. Instead, we observe\nthat only 5% of matches for test-set queries are from the same class as the query. Furthermore,\nmany matches are all from the same incorrect training class (highlighted in red). We see a knot is\nmatched with several gila monsters (and other reptiles); a bassoon with letter openers (and pens); a\nscrew with hammers; another screw with buckeyes. The errors within that wrong class often have\nwidely different appearance: for example, the bottom-most screw is matched with single buckeyes\nand also a pile of buckeyes. One interpretation is that the network picks up on image patterns during\ntraining that allow images of each class to be tightly grouped in the feature space, minimizing other\nways that the image might be similar to other classes in preparation for a conﬁdent classiﬁcation. For\nout-of-domain samples, the network can then overemphasize a spurious image pattern that suggests\nmembership in one training-set class. This is the consequence of supervision collapse, where image\npatterns that might help make the correct associations are lost.\nbetween performance on within-distribution samples and out-of-distribution samples? We hypothe-\nsize that because the neural network backbone of Prototypical Nets is designed for classiﬁcation, they\ndo just this: represent only an image’s (training-set) class, and discard information that might help\nwith out-of-distribution classes. Doing so minimizes the losses for many meta-learning algorithms,\nincluding Prototypical Nets. We call this problem supervision collapse, and illustrate it in Figure 1.\nOur ﬁrst contribution is to explore using self-supervision to overcome supervision collapse. We\nemploy SimCLR [15], which learns embeddings that discriminate between every image in the dataset\nwhile maintaining invariance to transformations (e.g., cropping and color shifts), thus capturing more\nthan just classes. However, rather than treat SimCLR as an auxiliary loss, we reformulate SimCLR as\n“episodes” that can be classiﬁed in the same manner as a training episode.\nOur second contribution is a novel architecture called CrossTransformers, which extends Trans-\nformers [87] to few-shot ﬁne-grained classiﬁcation. Our key insight is that objects and scenes are\ngenerally composed of smaller parts, with local appearance that may be similar to what has been seen\nat training time. The classical example of this is the centaur that appeared in several early papers on\nvisual representation [11, 40, 88], where the parts from the human and horse composed the centaur.\nCrossTransformers operationalize this insight of (i) local part-based comparisons, and (ii) accounting\nfor spatial alignment, resulting in a procedure for comparing images which is more agnostic to the\nunderlying classes. In more detail, ﬁrst a coarse alignment between geometric or functional parts in\nthe query- and support-set images is established using attention as in Transformers. Then, given this\nalignment, distances between corresponding local features are computed to inform classiﬁcation. We\ndemonstrate this improves generalization to unseen classes and domains.\n2\n",
    "In summary, our contributions in this paper are: (i) We improve the robustness of our local features\nwith a self-supervised technique, modifying the state-of-the-art SimCLR [15] algorithm. (ii) We\npropose the CrossTransformer, a network architecture that is spatially aware and performs few-shot\nclassiﬁcation using more local features, which improves transfer. Finally, (iii) we evaluate and\nablate how the choices in these algorithms impact Meta-Dataset [85] performance, and demonstrate\nstate-of-the-art results on nearly every dataset within it, often by large margins.\n2\nRelated Work\nFew-shot image classiﬁcation.\nFew-shot learning [25, 35, 46, 53] has recently been primarily\naddressed in the meta-learning framework [57, 71, 80], where a model learns an update rule for\nthe parameters of a base-learner model [5, 6, 72] through a sequence of training episodes [79, 89].\nThe meta-learner either learns to produce new parameters directly from the new data [8, 33, 56, 62,\n64, 72, 73], or learns to produce an update rule to iteratively optimize the base learner to ﬁt the\nnew data [1, 5, 7, 38, 63, 98]. [27, 51, 59] do not use any explicit meta-learner model, but instead\nunroll the base-learner gradient updates and optimize for model initializations which generalize\nwell on novel tasks. Matching-based methods [28, 75, 78, 90] instead learn representations for\nsimilarity functions [10, 16, 17, 44, 82], in the hope that the similarities will generalize to new\ndata. CrossTransformers fall in this category, and share much of their architecture with Prototypical\nNets [75].\nAttention for few-shot learning.\nCrossTransformers attend [3] individually over each class’s\nsupport set to establish local correspondences, whereas Matching Networks [90] attend over the\nwhole support set to “point to” matching instances. [54] extend this idea to larger contexts using\ntemporally dilated convolutions [86]. In the limit, attention over long-term experiences accumulated\nin memories [43, 56, 69, 76] can augment more traditional learning.\nCorrespondences for visual recognition.\nCrossTransformers perform classiﬁcation by matching\nmore local parts. Discriminative parts [4, 9, 22, 34, 42, 83] and visual words [74, 101] have a\nrich history, and have found applications in deformable-parts models [26], classiﬁcation [70, 101],\nand retrieval [12, 84]. Part-based correspondences for recognition [104] have been particularly\nsuccessful in ﬁne-grained face retrieval and recognition [48, 96]. CrossTransformers establish\nsoft correspondences between pixels in the query- and support-set images; such dense pairwise\ninteractions [93] have recently proved useful for generative networks [100], semantic matching [66]\nand tracking [91]. [49] learns spatially dense classiﬁers for few-shot classiﬁcation, but pools the\nspatial dimensions of the prototypes, and hence does not have a notion of correspondence.\nSelf-supervised learning for few-shot.\nOur work on SimCLR episodes inherits from a line of\nself-supervised learning research, which typically deal with transfer from pretext tasks to semantic\nones and must therefore represent more than their training data [2, 13, 15, 21, 24, 30, 36, 47,\n60, 102, 103]. Some recent works [29, 77] demonstrate that this can improve few-shot learning,\nalthough these use self-supervised auxiliary losses rather than integrating self-supervised instance\ndiscrimination [15, 24, 55, 81, 95] into episodic training. Also particularly relevant are methods\nthat use self-supervision for correspondence [41, 50, 91], which may in future work improve the\ncorrespondences that CrossTransformers use.\n3\nStopping Collapse: SimCLR Episodes and CrossTransformers\nWe take a two-pronged approach to dealing with the supervision collapse problem. Modern ap-\nproaches to few-shot learning typically involve learning an embedding for each image, followed by a\nclassiﬁer that aggregates information across an episode’s support set in order to classify the episode’s\nqueries. Our ﬁrst step aims to use self-supervised learning to improve the embedding so it expresses\ninformation beyond the classes, in a way that is as algorithm-agnostic as possible. Once we have these\nembeddings, we build a classiﬁer called a CrossTransformer. CrossTransformers use Prototypical\nNets [75] as a blueprint, chosen due to their simplicity and strong performance; the main modiﬁcation\nis to aggregate information in a spatially-aware way. We begin by reviewing Prototypical Nets, and\nthen describe the two approaches.\n3\n",
    "Prototypical Nets are episodic learners, which means training is performed on the same kind of\nepisodes that will be presented at test time: a query set Q of images, and a support set S which can\nbe partitioned into classes c ∈{1, 2, . . . , C}: each Sc = {xc\ni}N\ni=1 is composed of N example images\nxc\ni. Prototypical Nets learn a distance function between the query and each subset Sc. Both the\nquery- and support-set images are ﬁrst encoded into a D-dimensional representation Φ(x), using a\nshared ConvNet Φ : RH×W ×3 7→RD, where H, W are the height and width respectively. Then a\n“prototype” tc ∈RD for the class c is obtained by averaging the representations of the support set Sc,\ntc =\n1\n|Sc|\nP\nx∈Sc Φ(x). Finally, a distribution of classes is obtained using softmax over the distances\nbetween the query image and class prototypes: p(y = c|xq) =\nexp(−d(Φ(xq),tc))\nPC\nc′=1 exp(−d(Φ(xq),tc′)). In practice,\nthe distance function d is ﬁxed to be the squared Euclidean distance d(xq, Sc) = ||Φ(xq) −tc||2\n2.\nThe learning objective is to train the embedding network Φ to maximize the probability of the correct\nclass for each query.\n3.1\nSelf-supervised training with SimCLR\nOur ﬁrst challenge is to improve the neural network embedding Φ: after all, if these features have\ncollapsed to represent little information beyond the classes, then a subsequent classiﬁer cannot can\nrecover this information. But how can we train features to represent things beyond labels when our\nonly supervision is the labels? Our solution is self-supervised learning, which invents “pretext tasks”\nthat train representations without labels [21, 24], and better yet, has a reputation for representations\nthat transfer beyond this pretext task. Speciﬁcally we use SimCLR [15], which uses “instance\ndiscrimination” as a pretext task. It works by applying random image transformations (e.g., cropping\nor color shifts) twice to the same image, thus generating two “views” of that image. Then it trains the\nnetwork so that representations of the two views of the same image are more similar to each other\nthan they are to those of different images. Empirically, networks trained in this way become sensitive\nto semantic information, but also learn to discriminate between different images within a single class,\nwhich is useful for combating supervision collapse.\nWhile we could treat SimCLR as an auxiliary loss on the embedding, we instead reformulate SimCLR\nas episodic learning, so that the technique can be applied to all episodic learners with minimal\nhyper-parameters. To do this, we randomly convert 50% of the training episodes into what we call\nSimCLR episodes, by treating every image as its own class. For clarity, we will call the original\nepisodes that have not been converted SimCLR episodes MD-categorization episodes, to emphasize\nthat they use the original categories from Meta-Dataset. Speciﬁcally, let ρ(·) be SimCLR’s (random)\nimage transformation function, and let S = {xi}|S|\ni=1 be a training support set. We generate a\nSimCLR episode by sampling a new support set, transforming each image in the original support\nset S′ = {ρ(xi)}|S|\ni=1, and then generating query images by sampling other transformations from the\nsame support set: Q′ = {ρ(random_sample(S))}|Q|\ni=1, where random_sample just takes a random\nimage from the set.1 The original query set Q is discarded. The label for an image in the SimCLR\nepisode is its index in the original support set, resulting in an |S|-way classiﬁcation for each query.\nNote that for a SimCLR episode, the ‘prototypes’ in Prototypical Nets average over just a single\nimage, and therefore the Prototypical Net loss can be written as\nexp(−d(Φ(ρ(xq)),Φ(ρ(xq))))\nPn\ni=1 exp(−d(Φ(ρ(xq)),Φ(ρ(xi))). If we\ndeﬁne d as the cosine distance rather than Euclidean, this loss is identical to the one used in SimCLR.\n3.2\nCrossTransformers\nGiven a query image xq and a support set Sc = {xc\ni}N\ni=1 for the class c, CrossTransformers aim to\nbuild a representation which enables local part-based comparisons between them.\nCrossTransformers start by making the image representation a spatial tensor, and then assemble\nquery-aligned class prototypes by putting the support-set images Sc in correspondence with the\nquery image. The distance between the query image and the query-aligned prototype for each\nis then computed and used in a similar way to Prototypical Nets. In practice, we establish soft\ncorrespondences using attention [3] based Transformers [87]. In contrast, Prototypical Nets use ﬂat\nvector representations which lose the location of image features, and have a ﬁxed class prototype\nwhich is independent of the query image.\n1We enforce that the sampled queries have the same class distribution as Q, and have no repeats.\n4\n",
    "p\nΓ\nΓ\nΩ\nm\nn\nDot-product query features at location \np against all support set (Sc) spatial \nfeatures in class\nΛ\nΛ\nSoftmax across all\nspatial features in class\np\nKey Heads\nValue Heads\nQuery-aligned\nprototype (tc)\nweighted sum\nSoftmax normalized\nattention weights\nSupport-set (Sc) image \nfeatures for the category c\nQuery Head\ndv\ndv\ndk\ndk\nQuery image \nfeatures\nm\nn\na2\n    c\na1\n    c\n~a2\n    c\n~a1\n    c\nKeys (kc)\nValues (vc)\nQueries (q)\nFigure 2: CrossTransformers. Construction of query-aligned class prototype vector tc\np for the class\nc and the query image xq, focusing on the spatial location p in xq. The query vector qp is compared\nagainst keys kc from all spatial locations in the support set Sc to obtain attention scores ac, which are\nsoftmax normalized before being used to aggregate the values vc for the aligned prototype vector tc\np.\nConcretely, CrossTransformers remove the ﬁnal spatial pooling in Prototypical Nets’ embedding\nnetwork Φ(·), such that the spatial dimensions H′, W ′ are preserved: Φ(x) ∈RH′×W ′×D. Following\nTransformers, key-value pairs are then generated for each image in the support set using two\nindependent linear maps: the key-head Γ : RD 7→Rdk, and the value-head Λ : RD 7→Rdv\nrespectively. Similarly, the query image features Φ(xq) are embedded using the query-head Ω:\nRD 7→Rdk. Dot-product attention scores are then obtained between keys and queries, followed\nby softmax normalization across all the images and locations in Sc. This attention serves as our\ncoarse correspondence (see example attention visualizations in Figure 3 and Appendix D), and is\nused to aggregate the support-set features into alignment with the query. This process is visualized in\nFigure 2.\nMathematically, let kc\njm = Γ · Φ(xc\nj)m be the key for the jth image in the support set for class c at\nspatial position m (index over the two dimensions H′, W ′), and similarly let qp = Ω· Φ(xq)p be the\nquery vector at spatial position p in the query image xq. The attention ˜ac\njmp ∈R between the two is\nthen obtained as:\n˜ac\njmp =\nexp(ac\njmp/τ)\nP\ni,n exp(ac\ninp/τ),\nwhere\nac\njmp = kc\njm · qp,\nand\nτ =\np\ndk.\n(1)\nNext, the aligned prototype vector tc\np corresponding to spatial location p in the query is obtained by\naggregating the support-set values vc\njm = Λ · Φ(xc\nj)m using the attention weights above:\ntc\np =\nX\njm\n˜ac\njmpvc\njm\n(2)\nFinally, squared Euclidean distances between aligned local features from the above prototype and\ncorresponding query image values wp = Λ · Φ(xq)p are aggregated as below. This scalar distance\nacts as a negative logit for a distribution over classes as in Prototypical Nets.\nd(xq, Sc) =\n1\nH′W ′\nX\np\n||tc\np −wp||2\n2\n(3)\nNote we apply the same value-head Λ to both the query and support-set images. This ensures that\nthe CrossTransformer behaves somewhat like a distance. That is, imagine a trivial case where, for\none class, all images in Sc are identical to xq. We would want d(xq, Sc) to approach 0 even if the\nnetwork is untrained, or if these images are highly dissimilar from those used for training. Sharing Λ\nbetween the support and query sets helps accomplish this: in fact, if ˜ac\njmp is 1 where p = m and 0\nelsewhere for all j, then d(xq, Sc) will be identically 0 under this architecture, no matter the network\n5\n",
    "Query\nCorrespondence in support set\nQuery\nCorrespondence in support set\nFigure 3: Visualization of the attention ˜a. We show four query images, along with three support-set\nimages for each. Within each query image, we choose three spatial locations (red, green, and blue\nsquares), and plot the CrossTransformer attention weights for each one in the corresponding color\n(brighter colors mean higher weight). The four examples are from Aircraft, CU-Birds, VGG Flowers,\nand ImNet test sets respectively (clockwise, starting from top-left). No matter which dataset, the\nattention masks are semantically relevant, even when the correspondence is not one-to-one. More\nvisualizations are given in Appendix D.\nweights. To encourage this behavior for the attention ˜a, we also set Γ = Ω, i.e., the key and query\nheads are the same. This way, in our trivial case, the attention is likely to be maximal for spatial\nlocations that correspond, because kc\njm and qp will be the same for p = m.\nFor one experiment, we also augment the CrossTransformer with a global feature, which can help for\nsome datasets like DTD (Describable Textures Dataset) with less spatial structure.\n4\nExperiments\nWe evaluate on Meta-Dataset [85], speciﬁcally the setting where the training is performed on the\nImageNet train split only, which is 712 classes (plus 158 classes for validation, which are not used\nfor training but only to perform early stopping). We then test on the remaining 130 held-out classes\nfrom ImageNet, as well as 9 other image datasets. Note that this is in contrast to another popular (and\neasier) setting, where the training also uses a subset of categories from more of these datasets: usually\nall datasets except Trafﬁc Signs and COCO. For clarity, we’ll use “Meta-Dataset Train-on-ILSVRC”\nto denote training on ImageNet only, and “Meta-Dataset Train-on-all” to denote when training occurs\non more datasets. Test time consists of a series of episodes, each of which contains: (1) a support set\nbetween 50 and 500 labeled images which come from between 5 and 50 classes; and (2) an unlabeled\nquery set with 10 images per class. Meta-Dataset aims for ﬁne-grained recognition, so the classes in\neach episode are mutually similar: one episode may contain only musical instruments, another may\ncontain only birds, etc.\nMeta-Dataset is useful for studying transfer because different test datasets encapsulate different\nkinds of transfer challenges. For test datasets like CU-Birds [92], there are numerous similar classes\nin ImageNet train (20 bird classes in ImageNet train, versus 100 in the CU-birds test dataset). In\ncontrast, for test datasets like Aircraft [52], there is just a single corresponding class in ImageNet\ntrain; therefore, algorithms which don’t represent the intra-class variability for this class will be\npenalized. The ImageNet test set has images in a similar domain to the ImageNet train set but with\ndifferent classes, while test datasets like COCO contain many similar classes to ImageNet, but with\ndomain shift (in COCO, instances are generally not the subject of their photographs, and may be\nlow-resolution or occluded). Finally, test datasets like OmniGlot combine these challenges, i.e.,\ndifferent classes in a substantially different domain.\n4.1\nImplementation details\nTo ensure comparability, we followed the public implementation of Prototypical Nets for Meta-\nDataset [85] wherever possible. This includes using the same hyperparameters, unless otherwise\nnoted. For the hyperparameters that were chosen with a sweep on the validation set (learning rate\nschedule and weight decay), we simply used the best values discovered for Prototypical Nets for all\nthe experiments in this paper. See Appendix C.2 for details of the CrossTransformer architecture. We\n6\n",
    "use no pretraining for CrossTransformers, although to be consistent with prior work [85] we use it\nfor the experiments involving Prototypical Nets.\nWe incorporate two improvements from Meta-Baseline [16], which at test time is similar to Proto-\ntypical Nets (though it isn’t trained as an episodic learner). The ﬁrst is to keep exponential moving\naverages for Batch Norm statistics during training, and use those for Batch Norm at test time. Second,\nwe note that Meta-Baseline does not train on ﬁne-grained episodes sampled from the ImageNet hier-\narchy, as Prototypical Nets does, but rather on batches with uniformly-sampled classes. Empirically,\nPrototypical Nets trained only on ﬁne-grained episodes struggle to do coarse-grained recognition, as\nrequired for datasets like COCO. Therefore, we only use the ImageNet hierarchy to make 50% of\nepisodes ﬁne-grained; the rest have categories sampled uniformly.\nChoice of network.\nPrior implementations of networks like Prototypical Nets use relatively small\nnetworks (e.g., ResNet-18) with small input images (e.g. 126×126 pixels), and report that measures\nto increase capacity (e.g., Wide ResNets [99]) provide minimal beneﬁts. This is surprising given that\nstandard networks show improvements for increasing capacity (e.g., ResNet-34 outperforms ResNet-\n18 by 3% on ImageNet [37]). Making our networks spatially-aware requires higher-resolution, and\nalso higher-capacity networks are especially important in self-supervised learning [23, 45]. Therefore,\nour experiments increase resolution to the standard 224×224 and use ResNet-34, and we also use\nnormalized stochastic gradient descent [18, 58], which we found improved stability when ﬁne-tuning\nmore complex networks. Table 1 compares the Prototypical Nets performance of this network to that\nof using a ResNet-18. Increased capacity leads to only slight performance improvements, which are\nmore pronounced for datasets that are similar to ImageNet; it harms, e.g., OmniGlot. Further details\nin Appendix C.1.\nFor experiments with CrossTransformers, we also increased the resolution of the convolutional feature\nmap by setting the stride of ﬁnal block of the ResNet to 1, and using dilated convolutions to preserve\nthe feature computation [32, 39]. This turns the usual 7×7 feature map for a 224×224 image into a\n14×14 feature map. We ablate this choice in Appendix C.2.\nAugmenting CTX with a global feature.\nRecent works have also shown beneﬁts for applying\nlogistic regression (LR) at test time [82]. In practice, it is too expensive to apply LR to our query-\naligned prototypes (as this would involve a separate classiﬁer for every query). Therefore, we instead\napply logistic regression to a globally-pooled feature and average the logits with those produced by\nthe CrossTransformer. See Appendix C.3 for details.\nAugmentation.\nWhile most experiments use no augmentation (apart from SimCLR episodes) to be\nconsistent with prior work [85], more recent work [16, 68, 82] showed that stronger data augmentation\nis effective. Therefore, for two experiments, we employ augmentation using the settings discovered\nin BOHB [68] (via Auto-Augment [19] on the validation set), with an extra stage that randomly\ndownsamples and then upsamples images, which we ﬁnd helpful as our network operates at higher\nresolution than many of the test datasets. This BOHB augmentation is only applied to the “MD-\ncategorization” episodes, and not to the SimCLR episodes. Note this BOHB augmentation is different\nfrom SimCLR-style augmentation, which is used in SimCLR Episodes as well as in the ablation\n(SC-Aug) in Table 1. See Appendix C.4 for details.\n4.2\nResults for self-supervised learning with SimCLR on Prototypical Nets\nWe ﬁrst analyze the impact of SimCLR Episodes and other architectural choices in Table 1. For\nbaseline Prototypical Nets, SimCLR Episodes generally improve performance, but this depends on\narchitectural choices. Improvements are largest for datasets that are more distant from ImageNet,\ne.g., OmniGlot and Quickdraw, and datasets which require distinguishing between sub-categories\nImageNet categories, e.g., Aircraft and Trafﬁc Signs. In ImageNet, all commercial airplanes fall\nin a single ImageNet class; therefore, the success of SimCLR Episodes here suggests they recover\nfeatures which are lost due to supervision collapse. Strangely, however, SimCLR Episodes interact\nwith Batch Norm: we ﬁnd more robust improvements when computing Batch Norm statistics from\nthe test-time support set, but not when using exponential moving averages (EMA) as suggested by\n[16]. One possible interpretation is that the network has learned to use Batch Norm to communicate\ninformation across the batch: e.g., to distinguish between SimCLR Episodes and MD-categorization\nepisodes. Using EMA at test time may prevent this, which may confuse the network. Interestingly,\n7\n",
    "Table 1: Effects of architecture and SimCLR Episodes on Prototypical Nets, for Meta-Dataset\nTrain-on-ILSVRC. We ablate architectural choices: use of Exponential Moving Averages (EMA) at\ntest time for Batch Norm (versus computing Batch Norm statistics on the support set at test time),\nimage resolution (224, versus the baseline’s 126), ResNet-34 (R34) replacing ResNet-18, SimCLR-\nstyle augmentation (SC-Aug), and the addition of 50% SimCLR Episodes (SC-Eps). The test datasets\nfrom Meta-Dataset are ImNet: Meta-Dataset’s ImageNet Test classes; Omni: OmniGlot drawn\ncharacters; Acraft: Aircraft; Bird: CU-Birds; DTD: Textures; QDraw: Quick Draw drawings; Fungi:\nFGVCx fungi challenge; Flower: VGG Flowers; COCO: Microsoft COCO cropped objects. The best\nnumber in each column is bolded, along with others that are within a conﬁdence interval [85]. Rank\nis the average rank for each method. Using SimCLR Episodes provides improvements on almost\nall datasets, and provides especially large boosts for datasets which are dissimilar from ImageNet,\nsuch as OmniGlot. However, simply using SimCLR transformations without instance discrimination\n(SC-Aug) harms results on almost all datasets. Increased capacity provides small beneﬁts on some\ndatasets, especially the more realistic and ImageNet-like datasets (e.g., birds), but actually harm\nothers like OmniGlot. Note that in this table, QuickDraw uses the split from the original paper [85]\nrather than the (somewhat easier) split published for that paper’s public benchmark. For all other\ntables, we use the split from the published benchmark.\n224 R34 SC-Aug SC-Eps EMA ImNet Omni Acraft\nBird DTD QDraw Fungi Flower\nSign COCO Rank\n\u0013\n49.10 59.27\n49.31 68.43 66.70\n45.83 38.48\n85.34 49.49\n42.88\n5.55\n49.77 55.70\n52.06 68.58 67.27\n49.86 37.68\n84.32 50.27\n41.92\n5.20\n\u0013\n\u0013\n\u0013\n51.66 57.22\n51.63 71.73 69.72\n47.31 42.07\n87.29 47.45\n44.38\n4.35\n\u0013\n\u0013\n52.51 49.87\n56.47 72.81 68.45\n51.41 42.16\n87.92 54.40\n40.60\n3.30\n\u0013\n\u0013\n\u0013\n\u0013\n47.58 55.73\n46.93 57.75 54.88\n42.91 37.42\n83.82 46.88\n43.36\n7.55\n\u0013\n\u0013\n\u0013\n47.94 51.79\n54.58 62.84 58.64\n46.36 36.06\n76.88 48.35\n38.77\n7.45\n\u0013\n\u0013\n\u0013\n\u0013\n49.67 65.21\n54.46 60.94 63.96\n50.64 37.84\n88.70 51.61\n42.97\n4.35\n\u0013\n\u0013\n\u0013\n53.69 68.50\n58.04 74.07 68.76\n53.30 40.73\n86.96 58.11\n41.70\n1.90\nProtoNets [85]\n50.50 59.98\n53.10 68.79 66.56\n48.96 39.71\n85.27 47.12\n41.00\n5.35\nwe will show later that SimCLR Episodes don’t harm CrossTransformers as they harm Prototypical\nNets when using EMA at test time, suggesting the two architectures solve the problem differently.\nRecall that converting an MD-categorization episode into a SimCLR episode makes two changes to\nthe episode: it 1) applies data augmentation, and 2) converts the classiﬁcation problem to “instance\ndiscrimination,” by selecting images from the support set as a new query set, and requiring the\nnetwork to predict the selected indices. To ensure that we are not simply seeing the effect of data\naugmentation, we also implemented a baseline (SC-Aug) that does 1 but not 2 to the input MD-\ncategorization episodes, and does this augmentation for all episodes (rather than 50%, which is\nthe fraction of MD-categorization episodes that are converted to SimCLR episodes for SC-Eps\nexperiments). Indeed, we see no improvements for this change, and in fact non-trivial performance\nloss from this, mirroring the result for supervised learning in the original paper [15]. This reinforces\nthat SimCLR was designed for self-supervised learning, and so the transformations are more severe\nthan is usually optimal for supervised learning.\nFinally, we see small improvements from using larger networks and higher resolution for the baseline\nmodel. While our baseline is overall better than the baseline Prototypical Nets implementation [85],\nit is still below the state-of-the-art for centroid-based methods which rely more heavily on pretraining,\nand use no episodic training [16].\n4.3\nCrossTransformers results\nGiven these performant features, we next turn to CrossTransformers. Table 2 compares CrossTrans-\nformers (CTX) with and without SimCLR episodes to several state-of-the-art methods, including the\nPrototypical Nets on which they are based. We see that CrossTransformers provide strong perfor-\nmance on their own, including having a better average rank than all baselines. With SimCLR episodes\nproviding more versatile features, we see further improvements, with performance on-par or better\nthan the best methods on almost every dataset. We note particularly large improvements on OmniGlot,\nwhich has a large domain gap relative to the training data. We also see strong improvements on Street\nSigns, Aircraft, and Flowers, where multiple test-time categories map to few training-time categories,\nand often exhibit well-deﬁned spatial correspondence.\n8\n",
    "Table 2: CrossTransformers (CTX) comparison to state-of-the-art. We compare four versions\nof CrossTransformers to several state-of-the-art methods, which are the best performers among\nthose evaluated for Meta-Dataset Train-on-ILSVRC. We see that CTX alone has better average rank\nthan any baseline. Adding SimCLR episodes (+SimCLR Eps) and data augmentation inspired by\nBOHB [68] (+Aug) further improves results. Our full model is on-par or above prior methods on all\nbut one dataset, sometimes with large gaps over the best baseline (e.g., +5% on OmniGlot, +13% on\nAircraft, +5% on Signs), and furthermore, each prior method has some datasets where we outperform\nby a larger margin (the next best average rank [82], performs 19% worse on Aircraft and 17% worse\non OmniGlot). Finally, adding a test-time Logistic Regression classiﬁer inspired by Tian et al. [82]\nimproves performance on the one dataset—DTD textures—that was otherwise lacking. Note that\nmost of these methods [16, 68, 82] are unpublished concurrent work.\nImNet Omni Acraft\nBird\nDTD QDraw Fungi Flower\nSign COCO\nRank\nFinetuning [85]\n45.78 60.85\n68.69 57.31 69.05\n42.60 38.20\n85.51 66.79\n34.86 12.20\nProtoNets [85]\n50.50 59.98\n53.10 68.79 66.56\n48.96 39.71\n85.27 47.12\n41.00 12.65\nProtoNets+MAML [85]\n49.53 63.37\n55.95 68.66 66.49\n51.52 39.96\n87.15 48.83\n43.74 11.55\nCNAPS [65]\n50.60 45.20\n36.00 60.70 67.50\n42.30 30.10\n70.70 53.30\n45.20 13.55\nBOHB-L [68]\n50.60 64.09\n57.36 67.68 70.38\n46.26 33.82\n85.51 55.17\n41.58 11.50\nBOHB-NC [68]\n51.92 67.57\n54.12 70.69 68.34\n50.33 41.38\n87.34 51.80\n48.03 10.15\nBOHB-NC Ensemble [68]\n55.39 77.45\n60.85 73.56 72.86\n61.16 44.54\n90.62 57.53\n51.86\n7.45\nDhillon et al. [20]\n-\n-\n68.69 74.26 77.35\n-\n-\n88.14 55.98\n40.62\n-\nMeta-Baseline [16]\n59.20 69.10\n54.10 77.30 76.00\n57.30 45.40\n89.60 66.20\n55.70\n7.20\nTian et al. LR [82]\n60.14 64.92\n63.12 77.69 78.59\n62.48 47.12\n91.60 77.51\n57.00\n5.50\nTian et al. LR-distill [82]\n61.58 64.31\n62.32 79.47 79.28\n60.83 48.53\n91.00 76.33\n59.28\n4.60\nProtoNets (Our implementation)\n51.66 57.22\n51.63 71.73 69.72\n53.81 42.07\n87.29 47.45\n44.38 11.10\nCTX\n61.94 76.52\n79.65 84.06 76.26\n65.67 52.53\n94.11 70.47\n53.51\n3.85\nCTX+SimCLR Eps\n63.79 80.83\n82.05 82.01 75.76\n68.84 52.01\n94.62 75.01\n52.76\n3.05\nCTX+SimCLR Eps+Aug\n62.76 82.21\n79.49 80.63 75.57\n72.68 51.58\n95.34 82.65\n59.90\n2.25\nCTX+SimCLR Eps+Aug+LR\n62.25 82.03\n77.41 76.66 80.29\n72.24 49.39\n93.05 75.25\n60.35\n3.40\nDTD, however, is more challenging for basic CTX, which is unsurprising since textures have little of\nthe kind of spatial correspondence that CTX attempts to ﬁnd. COCO is also challenging, likely due\nto its extremely large intra-class variation (e.g., occlusion) and the fact that many categories overlap\nwith ImageNet-train categories, meaning that simply memorizing categories from the training set\nmay be more useful than using test-time appearance. To explore this trade-off, we applied logistic\nregression at test time to a globally pooled feature (see Appendix C.3), which provides additional\nlogits that are averaged with the CTX logits. We see non-trivial improvements on DTD by using\nthis, although we sacriﬁce some performance on other datasets, such as Signs and Aircraft. This\nimplies that there’s a fundamental tension between learning categories based on global features, and\ndecomposing the task into local features. More work is needed to better combine these two ideas.\nFinally, Figure 3 depicts the correspondence inferred by the CrossTransformer. The attention is often\nsemantically meaningful: object parts are well matched, including heads, bodies, feet, engines, and\nstrings. The attention is often not one-to-one either: for the ﬂower, the single query ﬂower is matched\nto multiple ﬂowers in some of the support images. Furthermore, the matching works even when\nthe ﬁne-grained classes are not the same, such as the different species of birds, suggesting that the\nattention is indeed a coarse-grained matching that has not overﬁt to the training-set classes.\n5\nConclusion\nWithin a single domain, deep networks have a remarkable ability to compose and reuse features in or-\nder to achieve statistical efﬁciency. However, this work shows the hidden problem with such systems:\nthe networks compose features in a way that conﬂates images which have different appearance but the\nsame label, i.e., it loses information about intra-class variation that may be necessary to understand\nnovel classes. We propose two techniques that help resolve this problem: self-supervised learning,\nwhich prevents features from losing that intra-class variation, and CrossTransformers, which help\nneural networks classify images using local features that are more likely to generalize. However,\nthis problem is far from resolved. In particular, our algorithm provides less beneﬁt when less spatial\nstructure is available, when knowledge of train-time categories can be useful (as in, e.g., COCO), or\nwhen higher-level reasoning is required (e.g., ﬁnding conjunctions of multiple objects). Allowing this\nalgorithm to use spatial structure only where relevant remains an open problem.\n9\n",
    "Broader Impact\nThe algorithm presented in this paper most directly applies to few-shot recognition, which has\nnumerous uses in industry, including vision systems for robotics that must adapt to new objects,\nand photo-organizing software which must infer the presence of new classes of objects on-the-ﬂy.\nUnfamiliar objects are ubiquitous in many real-world vision applications due to the so-called ‘long\ntail’ [94] of objects that occur in real scenes, and therefore we expect our algorithm to improve\nthe robustness of visual recognition systems. While our current work only addresses classiﬁcation,\nmany other tasks in computer vision, such as object detection and segmentation, use neural network\nrepresentations that can likewise be made more robust using the kind of architectures presented here.\nOur algorithm attempts to build representations which factorize the object recognition problem\ninto sub-problems (feature correspondence and feature comparison) that will each transfer correctly\nto new datasets. We hope that further research in this direction may help address dataset biases,\nincluding biases regarding race, gender, or other attributes [97], by helping to disentangle the truly\nmeaningful traits from the spurious correlations. Finally, while this algorithm presents an advance\nto state-of-the-art in understanding rare objects, the general performance of such systems is still far\nbelow human performance. For safety-critical applications (e.g., surgery or self-driving cars), relying\non the ability of vision systems to correctly interpret unusual situations is risky with current systems,\neven with the advances presented here.\nFunding Disclosure\nThis work was funded by DeepMind.\nAcknowledgments\nThe authors would like to thank Pascal Lamblin for help with Meta-Dataset, Olivier Hénaff for\nhelp with SimCLR, Yonglong Tian for help in reproducing baselines, and Relja Arandjelovi´c for\ninvaluable advice on the paper. They are also grateful to Jean-Baptiste Alayrac, Joao Carreira,\nMateusz Malinowski, Viorica P˘atr˘aucean, Adria Recasens, and Lucas Smaira for helpful discussions,\nsupport, and feedback on the project.\nReferences\n[1] M. Andrychowicz, M. Denil, S. Gomez, M. W. Hoffman, D. Pfau, T. Schaul, B. Shillingford, and\nN. De Freitas. Learning to learn by gradient descent by gradient descent. In NeurIPS, 2016.\n[2] P. Bachman, R. D. Hjelm, and W. Buchwalter. Learning representations by maximizing mutual information\nacross views. In NeurIPS, 2019.\n[3] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate.\nIn Proc. ICLR, 2015.\n[4] E. Bart and S. Ullman. Cross-generalization: Learning novel classes from a single example by feature\nreplacement. In Proc. CVPR, 2005.\n[5] S. Bengio, Y. Bengio, J. Cloutier, and J. Gecsei. On the optimization of a synaptic learning rule. In\nPreprints Conf. Optimality in Artiﬁcial and Biological Neural Networks, volume 2. Univ. of Texas, 1992.\n[6] Y. Bengio, S. Bengio, and J. Cloutier. Learning a synaptic learning rule. Citeseer, 1990.\n[7] L. Bertinetto, J. F. Henriques, P. H. Torr, and A. Vedaldi. Meta-learning with differentiable closed-form\nsolvers. In Proc. ICLR, 2019.\n[8] L. Bertinetto, J. F. Henriques, J. Valmadre, P. Torr, and A. Vedaldi. Learning feed-forward one-shot\nlearners. In NeurIPS, 2016.\n[9] L. Bourdev and J. Malik. Poselets: Body part detectors trained using 3d human pose annotations. In Proc.\nICCV, 2009.\n[10] J. Bromley, I. Guyon, Y. LeCun, E. Säckinger, and R. Shah. Signature veriﬁcation using a\" siamese\" time\ndelay neural network. In NeurIPS, 1994.\n[11] A. M. Bronstein, M. M. Bronstein, A. M. Bruckstein, and R. Kimmel. Partial similarity of objects, or\nhow to compare a centaur to a horse. Proc. ICCV, 2009.\n[12] B. Cao, A. Araujo, and J. Sim. Unifying deep local and global features for image search. In Proc. ECCV,\n2020.\n[13] M. Caron, P. Bojanowski, A. Joulin, and M. Douze. Deep clustering for unsupervised learning of visual\nfeatures. In Proc. ECCV, 2018.\n[14] K. Chatﬁeld, K. Simonyan, and A. Zisserman. Efﬁcient on-the-ﬂy category retrieval using convnets and\ngpus. In Asian Conference on Computer Vision, 2014.\n10\n",
    "[15] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning of visual\nrepresentations. In Proc. ICML, 2020.\n[16] Y. Chen, X. Wang, Z. Liu, H. Xu, and T. Darrell. A new meta-baseline for few-shot learning. arXiv\npreprint arXiv:2003.04390, 2020.\n[17] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity metric discriminatively, with application to\nface veriﬁcation. In Proc. CVPR, 2005.\n[18] J. Cortés. Finite-time convergent gradient ﬂows with applications to network consensus. Automatica,\n42(11), 2006.\n[19] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le. Autoaugment: Learning augmentation\nstrategies from data. In Proc. CVPR, 2019.\n[20] G. S. Dhillon et al. A baseline for few-shot image classiﬁcation. Proc. ICLR, 2020.\n[21] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised visual representation learning by context prediction.\nIn Proc. ICCV, 2015.\n[22] C. Doersch, S. Singh, A. Gupta, J. Sivic, and A. A. Efros. What makes paris look like paris? Proc. ACM\nSIGGRAPH, 31(4), 2012.\n[23] C. Doersch and A. Zisserman. Multi-task self-supervised visual learning. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pages 2051–2060, 2017.\n[24] A. Dosovitskiy, J. T. Springenberg, M. Riedmiller, and T. Brox. Discriminative unsupervised feature\nlearning with convolutional neural networks. In NeurIPS. 2014.\n[25] L. Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object categories. IEEE PAMI, 2006.\n[26] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively\ntrained part-based models. IEEE PAMI, 32(9), 2009.\n[27] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In\nProc. ICML, 2017.\n[28] V. Garcia and J. Bruna. Few-shot learning with graph neural networks. In Proc. ICLR, 2018.\n[29] S. Gidaris, A. Bursuc, N. Komodakis, P. Perez, and M. Cord. Boosting few-shot visual learning with\nself-supervision. In The IEEE International Conference on Computer Vision (ICCV), October 2019.\n[30] S. Gidaris, P. Singh, and N. Komodakis. Unsupervised representation learning by predicting image\nrotations. In Proc. ICLR, 2018.\n[31] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection\nand semantic segmentation. In Proc. CVPR, 2014.\n[32] A. Giusti, D. C. Cire¸san, J. Masci, L. M. Gambardella, and J. Schmidhuber. Fast image scanning with\ndeep max-pooling convolutional neural networks. In Intl. Conf. Image Proc., 2013.\n[33] D. Ha, A. Dai, and Q. V. Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016.\n[34] X. Han, T. Leung, Y. Jia, R. Sukthankar, and A. C. Berg. Matchnet: Unifying feature and metric learning\nfor patch-based matching. In Proc. CVPR, 2015.\n[35] B. Hariharan and R. Girshick. Low-shot visual recognition by shrinking and hallucinating features. In\nProc. CVPR, 2017.\n[36] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation\nlearning. In Proc. CVPR, 2020.\n[37] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proc. CVPR, 2016.\n[38] S. Hochreiter, A. S. Younger, and P. R. Conwell. Learning to learn using gradient descent. In International\nConference on Artiﬁcial Neural Networks. Springer, 2001.\n[39] M. Holschneider, R. Kronland-Martinet, J. Morlet, and P. Tchamitchian. A real-time algorithm for signal\nanalysis with the help of the wavelet transform. In Wavelets, pages 286–297. 1990.\n[40] D. Jacobs, D. Weinshall, and Y. Gdalyahu. Class representation and image retrieval with non-metric\ndistances. IEEE PAMI, 22(6):583–600, 2000.\n[41] J. Y. Jason, A. W. Harley, and K. G. Derpanis. Back to basics: Unsupervised learning of optical ﬂow via\nbrightness constancy and motion smoothness. In Proc. CVPR, 2016.\n[42] M. Juneja, A. Vedaldi, C. Jawahar, and A. Zisserman. Blocks that shout: Distinctive parts for scene\nclassiﬁcation. In Proc. CVPR, 2013.\n[43] Ł. Kaiser, O. Nachum, A. Roy, and S. Bengio. Learning to remember rare events. In Proc. ICLR, 2017.\n[44] G. Koch, R. Zemel, and R. Salakhutdinov. Siamese neural networks for one-shot image recognition. In\nICML deep learning workshop, volume 2. Lille, 2015.\n[45] A. Kolesnikov, X. Zhai, and L. Beyer. Revisiting self-supervised visual representation learning. In Proc.\nCVPR, 2019.\n[46] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level concept learning through probabilistic\nprogram induction. Science, 350, 2015.\n[47] G. Larsson, M. Maire, and G. Shakhnarovich. Learning representations for automatic colorization. In\nProc. ECCV, 2016.\n[48] H. Li, G. Hua, Z. Lin, J. Brandt, and J. Yang. Probabilistic elastic matching for pose variant face\nveriﬁcation. In Proc. CVPR, 2013.\n[49] Y. Lifchitz, Y. Avrithis, S. Picard, and A. Bursuc. Dense classiﬁcation and implanting for few-shot\nlearning. In Proc. CVPR, 2019.\n[50] P. Liu, M. Lyu, I. King, and J. Xu. Selﬂow: Self-supervised learning of optical ﬂow. In Proc. CVPR,\n2019.\n[51] D. Maclaurin, D. Duvenaud, and R. Adams. Gradient-based hyperparameter optimization through\nreversible learning. In International Conference on Machine Learning, 2015.\n11\n",
    "[52] S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi. Fine-grained visual classiﬁcation of aircraft.\narXiv preprint arXiv:1306.5151, 2013.\n[53] E. G. Miller, N. E. Matsakis, and P. A. Viola. Learning from one example through shared densities on\ntransforms. In Proc. CVPR, 2000.\n[54] N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel. A simple neural attentive meta-learner. In Proc.\nICLR, 2017.\n[55] I. Misra and L. v. d. Maaten. Self-supervised learning of pretext-invariant representations. In Proc. CVPR,\n2020.\n[56] T. Munkhdalai and H. Yu. Meta networks. In Proc. ICML, 2017.\n[57] D. K. Naik and R. J. Mammone. Meta-neural networks that learn by learning. In [Proceedings 1992]\nIJCNN International Joint Conference on Neural Networks, volume 1. IEEE, 1992.\n[58] Y. E. Nesterov. Minimization methods for nonsmooth convex and quasiconvex functions. Matekon, 29,\n1984.\n[59] J. Nichol, Alex any Andrychowicz ed Achiam and J. Schulman. On ﬁrst-order meta-learning algorithms.\narXiv preprint arXiv:1803.02999, 2018.\n[60] M. Noroozi and P. Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In\nProc. ECCV, 2016.\n[61] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning and transferring mid-level image representations\nusing convolutional neural networks. In Proc. CVPR, 2014.\n[62] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville. Film: Visual reasoning with a general\nconditioning layer. In Proc. AAAI, 2018.\n[63] S. Ravi and H. Larochelle. Optimization as a model for few-shot learning. In Proc. ICLR, 2017.\n[64] S.-A. Rebufﬁ, H. Bilen, and A. Vedaldi. Learning multiple visual domains with residual adapters. In\nNeurIPS, 2017.\n[65] J. Requeima, J. Gordon, J. Bronskill, S. Nowozin, and R. E. Turner. Fast and ﬂexible multi-task\nclassiﬁcation using conditional neural adaptive processes. In NeurIPS, 2019.\n[66] I. Rocco, M. Cimpoi, R. Arandjelovi´c, A. Torii, T. Pajdla, and J. Sivic. Neighbourhood consensus\nnetworks. In NeurIPS, pages 1651–1662, 2018.\n[67] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla,\nM. Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 115(3), 2015.\n[68] T. Saikia, T. Brox, and C. Schmid. Optimized generic feature learning for few-shot classiﬁcation across\ndomains. arXiv preprint arXiv:2001.07926, 2020.\n[69] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and T. Lillicrap. Meta-learning with memory-\naugmented neural networks. In Proc. ICML, 2016.\n[70] S. Savarese, J. Winn, and A. Criminisi. Discriminative object class models of appearance and shape by\ncorrelatons. In Proc. CVPR, 2006.\n[71] J. Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: the\nmeta-meta-... hook. PhD thesis, Technische Universität München, 1987.\n[72] J. Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks.\nNeural Computation, 4(1):131–139, 1992.\n[73] J. Schmidhuber. A neural network that embeds its own meta-levels. In IEEE International Conference on\nNeural Networks, pages 407–412. IEEE, 1993.\n[74] J. Sivic and A. Zisserman. Video Google: A text retrieval approach to object matching in videos. In Proc.\nICCV, 2003.\n[75] J. Snell, K. Swersky, and R. Zemel. Prototypical networks for few-shot learning. In NeurIPS, 2017.\n[76] P. Sprechmann, S. M. Jayakumar, J. W. Rae, A. Pritzel, A. P. Badia, B. Uria, O. Vinyals, D. Hassabis,\nR. Pascanu, and C. Blundell. Memory-based parameter adaptation. In Proc. ICLR, 2018.\n[77] J.-C. Su, S. Maji, and B. Hariharan. When does self-supervision improve few-shot learning? In Proc.\nECCV, 2020.\n[78] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and T. M. Hospedales. Learning to compare: Relation\nnetwork for few-shot learning. In Proc. CVPR, 2018.\n[79] S. Thrun. Lifelong learning algorithms. In Learning to learn. Springer, 1998.\n[80] S. Thrun and L. Pratt. Learning to learn. Springer Science & Business Media, 1998.\n[81] Y. Tian, D. Krishnan, and P. Isola. Contrastive multiview coding. arXiv preprint arXiv:1906.05849, 2019.\n[82] Y. Tian, Y. Wang, D. Krishnan, J. B. Tenenbaum, and P. Isola. Rethinking few-shot image classiﬁcation:\na good embedding is all you need? In Proc. ECCV, 2020.\n[83] P. Tokmakov, Y.-X. Wang, and M. Hebert. Learning compositional representations for few-shot recogni-\ntion. In Proc. ICCV, 2019.\n[84] G. Tolias, T. Jenicek, and O. Chum. Learning and aggregating deep local descriptors for instance-level\nrecognition. In Proc. ECCV, 2020.\n[85] E. Triantaﬁllou, T. Zhu, V. Dumoulin, P. Lamblin, U. Evci, K. Xu, R. Goroshin, C. Gelada, K. J. Swersky,\nP.-A. Manzagol, and H. Larochelle. Meta-dataset: A dataset of datasets for learning to learn from few\nexamples. In Proc. ICLR, 2020.\n[86] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior,\nand K. Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499,\n2016.\n[87] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.\nAttention is all you need. In NeurIPS, 2017.\n12\n",
    "[88] R. C. Veltkamp. Shape matching: Similarity measures and algorithms. In International Conference on\nShape Modeling and Applications, 2001.\n[89] R. Vilalta and Y. Drissi. A perspective view and survey of meta-learning. Artiﬁcial intelligence review,\n18, 2002.\n[90] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and D. Wierstra. Matching networks for one shot\nlearning. In NeurIPS, 2016.\n[91] C. Vondrick, A. Shrivastava, A. Fathi, S. Guadarrama, and K. Murphy. Tracking emerges by colorizing\nvideos. In Proc. ECCV, 2018.\n[92] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 dataset.\n2011.\n[93] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural networks. In Proc. CVPR, 2018.\n[94] Y.-X. Wang, D. Ramanan, and M. Hebert. Learning to model the tail. In NeurIPS, 2017.\n[95] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin. Unsupervised feature learning via non-parametric instance\ndiscrimination. In Proc. CVPR, 2018.\n[96] W. Xie, L. Shen, and A. Zisserman. Comparator networks. In Proc. ECCV, 2018.\n[97] K. Yang, K. Qinami, L. Fei-Fei, J. Deng, and O. Russakovsky. Towards fairer datasets: Filtering and\nbalancing the distribution of the people subtree in the imagenet hierarchy. In Conference on Fairness,\nAccountability, and Transparency, 2020.\n[98] A. S. Younger, S. Hochreiter, and P. R. Conwell. Meta-learning with backpropagation. In IJCNN’01.\nInternational Joint Conference on Neural Networks. Proceedings (Cat. No. 01CH37222), volume 3. IEEE,\n2001.\n[99] S. Zagoruyko and N. Komodakis. Wide residual networks. In Proc. BMVC., 2016.\n[100] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena. Self-attention generative adversarial networks. In\nProc. ICML, 2019.\n[101] J. Zhang, M. Marszałek, S. Lazebnik, and C. Schmid. Local features and kernels for classiﬁcation of\ntexture and object categories: A comprehensive study. IJCV, 73(2), 2007.\n[102] R. Zhang, P. Isola, and A. A. Efros. Colorful image colorization. In Proc. ECCV. Springer, 2016.\n[103] R. Zhang, P. Isola, and A. A. Efros. Split-brain autoencoders: Unsupervised learning by cross-channel\nprediction. In Proc. CVPR, 2017.\n[104] H. Zheng, J. Fu, T. Mei, and J. Luo. Learning multi-attention convolutional neural network for ﬁne-grained\nimage recognition. In Proc. ICCV, 2017.\n13\n",
    "A\nOut-of-domain vs. within-domain classiﬁcation implementation details\nIn the introduction, we state that algorithms like Prototypical Nets [75] achieve 50% accuracy on\nImageNet held-out categories (out-of-domain), versus 84% accuracy for a similar challenge given\nsupervised training data (within-domain).\nTo arrive at these numbers, we used the Meta-Dataset [85] validation classes. For Prototypical\nNets out-of-domain classiﬁcation, we ﬁnd that 50% is an upper bound: the performance from our\nreimplementation of Prototypical Nets (using 224×224 inputs, ResNet 34, and Normalized SGD)\non this set is actually 46.4%. For the supervised within-domain classiﬁcation baseline we trained\na ResNet-34 on the full ImageNet train set (all classes) with 224×224 images, using modern best\npractices for training, and found a top-1 performance of 73.0% on ImageNet’s standard val set\n(i.e., held-out images, but not held-out categories). We then applied this network to ﬁne-grained\nclassiﬁcation by constructing query sets, using images from the standard ImageNet val set, but\nfollowed the class distribution in Meta-Dataset’s query sets for the Meta-Dataset ImageNet val set.\nWe then classiﬁed all these images with the network as following: we discarded all logits except\nthose categories that are present in the query set, in order to ensure that chance performance is the\nsame for the Prototypical Nets and this baseline classiﬁer. The result was 84.2%.\nB\nSupervision Collapse: nearest neighbor experiments\nComputing nearest neighbors for Prototypical Net representations proved challenging due to another,\nentirely different source of supervision collapse: the default implementation of Prototypical Nets\nactually only produces representations that are comparable within a single episode. This is likely\nbecause baseline Prototypical Nets are trained only on episodes: that is, the network only sees\nﬁne-grained classiﬁcation problems (e.g., classifying insects versus other insects), rather than coarse-\ngrained episodes (e.g., insects versus cars). Therefore, nothing encourages the network to have having\ndistinct, non-overlapping representations of widely different categories (e.g., beetle the insect may\nhave the same representation as beetle the car, without affecting the training loss). Worse, Batch\nNorm allows communication within a support set, and so the ﬁnal representation of each image\ncontains not only information about the image, but also about how it contrasts with other images in\nthe support set.\nAs a result, even ImageNet images from the Meta-Dataset training set, grouped randomly into\nepisodes and fed through prototypical nets, have virtually meaningless representations. In the ﬁle\nbatch_norm_train_nearest_neigbors. html in the paper supplement,2 we show nearest neighbors\nretrieved in this way. The nearest-neighbor retrieval set includes 10% of images from both the\nImageNet train and test sets (Meta-Dataset’s split; speciﬁcally, 130 images per class). These are\npassed in batches of size 256 (Batch Norm is set to train mode) to obtain a feature vector for each\nimage. We use a ResNet-34 Prototypical Net with 224×224 images, trained with normalized SGD.\nFor each row in the HTML ﬁle, we show the query image (left), along with the top 9 nearest neighbors,\nusing Euclidean distance. For batch_norm_train_nearest_neigbors. html, the results are close to\nrandom, even though all query images are taken from the training set. This is not particularly useful\nfor analysis.\nTo ﬁx this problem, we make two modiﬁcations to Prototypical Net training. First, rather than\ntrain only on ﬁne-grained episodes, we train on episodes that contain classes sampled uniformly\nat random from the full ImageNet training set. This means that a single episode can now contain\nboth cars and insects. We also replace Batch Norm with Layer Norm, ensuring that there can no\nlonger be communication within the batch. Results for queries from the training classes are shown\nin layer_norm_train_nearest_neigbors. html. We can see a substantial improvement in the quality\nof the matches, as would be expected for the retrievals using a representation trained with standard\nImageNet classiﬁcation.\nFinally, in layer_norm_test_nearest_neigbors. html, we show the results of using the same retrieval\nprocedure, but using images from test-set classes as queries. We can see that, due to supervision\ncollapse, the nearest neighbors have returned to being quite poor.\n2Supplementary material is available on the NeurIPS 2020 webpage for this paper.\n14\n",
    "Same class as query\nAny train class\nMost frequent train class\nFigure 4: Nearest neighbors statistics. We sample 1000 random queries from the Meta-Dataset test\nset, and ﬁnd the top 9 nearest neighbors in Prototypical Net embedding space, in both training and\ntest sets. We show histograms of nearest neighbors: the x-axis is a count of the number of retrievals\nfor a single query that were of some type, and the bar height is the number of queries for which the\ncount was equal to that x-axis value. Left: the number of nearest neighbors that came from the same\n(test) class as the query. Center: the number of nearest neighbors that came from the train set. Right:\nthe number of retrievals that come from the same train class, for the most frequently-retrieved such\nclass. Note that the 0’th bin of this plot indicates that all retrievals were from the test set.\nB.1\nSupervision collapse: quantitative analysis\nIn Figure 4, we show statistics for a larger set of query images from the test set (1000 queries), which\nunderscore how poor the results are. In Figure 4 left, we see that over 60% of queries had 0 nearest\nneighbors of the correct category, even though 130 images of the correct category are guaranteed\nto exist in the retrieval set. Furthermore, Figure 4 center shows a large proportion of matches for\ntest set images are from the training set. There are 712 training categories and 130 test categories;\ntherefore, at random chance, we would expect 712/(130 + 712) = 84.6% to come from the training\nset (and note that all test-set images are devices, while there are no devices in the train set). Retrievals\nfrom the test set happen more often than chance, but a large fraction do not, and roughly 6% have all\nretrievals from the training set. Finally, in Figure 4 right, we see that often, the nearest neighbors\nfrom the train set are far from random. Having two or more matches from the the same training\nset class is quite common (more than 55.3% of the queries), even more frequent than having even\none match that’s from the correct val set class (34.1% of the queries). Many examples have far\nmore than two matches from the same training set class. In one case, all 9 retrievals were from the\nsame (incorrect) training category. The statistics reafﬁrm our intuition that individual Prototypical\nNet embeddings for held-out images are not likely to capture the correct semantics; instead, the\nembedding overemphasizes features it has in common with one particular category, which skews its\nnotion of similarity.\nWe repeated this experiment after training Prototypical Nets with 50% SimCLR episodes and found\nimprovements: only 43.3% of queries now have 2 or more neighbors from the same train set category,\nand 48.8% have at least one neighbor from the correct class. This suggests that SimCLR episodes\nare effective at reducing supervision collapse, but the problem is far from solved with this technique\nalone.\nC\nCrossTransformer implementation details\nC.1\nTraining\nOur experiments with CrossTransformers use no pretraining, although we use it for the experiments\ninvolving Prototypical Nets to be consistent with prior work [85], which has shown that pretraining\ngives a boost for Prototypical Net models. Speciﬁcally, for Prototypical Nets, the representation\nis pretrained for direct classiﬁcation on the training set, i.e., the network predicts a ﬁxed number\nof logits from batches of images sampled uniformly from the training categories. This is trained\nwith early stopping, where the stopping criterion involves training a linear classiﬁer on validation\ncategories and stopping when this ﬁne-tuning performance begins to decline. Only then is the network\nre-architectured into a Prototypical Net, where it is trained on episodes with support sets and query\nsets with relatively few categories.\n15\n",
    "Table 3: CrossTransformer comparison of feature map spatial resolution. We see that increasing\nthe spatial resolution from 7 (CTX7) to 14 (CTX14) via dilated convolution typically gives a small\nperformance boost, and almost never harms performance.\nImNet Omni Plane\nBird DTD QDraw Fungi Flower\nSign COCO Rank\nCTX7\n59.73 74.11 70.90 80.29 73.91\n65.61 48.53\n91.98 68.81\n50.62\n5.35\nCTX14\n61.94 76.52 79.65 84.06 76.26\n65.67 52.53\n94.11 70.47\n53.51\n3.25\nCTX7+SimCLR Eps\n60.69 79.22 76.64 77.86 77.31\n67.43 43.68\n93.30 69.56\n52.35\n4.10\nCTX14+SimCLR Eps\n63.79 80.83 82.05 82.01 75.76\n68.84 52.01\n94.62 75.01\n52.76\n2.50\nCTX7+SimCLR Eps+Aug\n60.76 87.26 77.56 68.31 71.44\n72.62 44.12\n92.45 81.20\n54.66\n3.85\nCTX+SimCLR Eps+Aug\n62.76 82.21 79.49 80.63 75.57\n72.68 51.58\n95.34 82.65\n59.90\n1.95\nFor other aspects of training, we follow prior work [85] where possible, including sampling episodes\nin the same way, and training the full network using ADAM (applied after normalizing the gradients\nin the case of Normalized SGD). We train until convergence, and select the best checkpoint using\nthe error on the validation set. For the hyperparameters chosen via hyperparameter sweep in the\noriginal paper, we use the best values for Prototypical Nets, which are a weight decay of 8.86e −5,\nand decaying the learning rate by a factor of 0.915 after a ﬁxed episode interval. The exception is\nthe learning rate, where we ﬁnd that 1.21e −3 is too high initially, and learning for CTX doesn’t\ntake off until it has decayed to half its original value (though this doesn’t affect ﬁnal performance).\nTherefore, we use an initial learning rate of 6e −4 for all CTX experiments. For Prototypical Nets,\nthis interval is 500 episodes, but we use a longer interval for CrossTransformers, as they train from\nscratch. For CrossTransformers alone this interval is 2000; we increase this interval by a factor of two\nwhen adding SimCLR Episodes, and another factor of two when adding BOHB-style augmentation,\nas both of these additions make learning more difﬁcult.\nTherefore, the main departures from prior work [85] are that 1) we use ResNet-34, 2) we feed\nimages at a higher resolution (224×224), 3) we use normalized gradient descent, 4) we use 50%\nepisodes where the categories are selected uniformly at random from ImageNet, 5) we use Batch\nNorm statistics in test mode at test time (i.e. exponential moving averages computed during training,\ndecaying at a rate of .9 per episode), and 6) we use no pretraining.\nC.2\nCrossTransformers architecture\nThe output of our ResNet-34 with dilated ﬁnal block has 512 channels and a 14×14 grid. We compute\nkey and value heads with 128 dimensions each, with no non-linearities and no bias. We ﬁnd that\nthe attention maps are rather memory-intensive (they contain all pairs of spatial positions between\nquery and support set). Therefore, we distributed the model across 8 NVIDIA V-100 GPUs, and use\ngradient rematerialization for the CrossTransformer attention maps. Training to convergence requires\nroughly 7 days for our most complex model with SimCLR episodes and BOHB-style augmentations\nenabled.\nTo demonstrate the importance of high-resolution feature maps, Table 3 shows the performance\nof a few versions of CrossTransformer without dilation, which results in stride-32 network with a\n7×7 output grid, like the standard ResNet implementation. We see that higher resolution almost\nalways gives a small boost. The boosts are largest on datasets with non-trivial spatial structure where\nthe distinguishing features may be small, e.g., Aircraft, Birds, and Fungi. On the other hand, the\nincreased resolution makes little difference for DTD textures and QuickDraw, and the the lower\nresolution actually performs best on OmniGlot. Textures lack spatial structure, and OmniGlot and\nQuickdraw contain low-resolution images with few identifying features: therefore, it’s unsurprising\nthat the extra resolution isn’t useful. One possible interpretation is that the network may subdivide\nthe scene into more parts than are justiﬁed, which can degrade performance when correspondences\nare wrong, suggesting that an adaptive mechanism for choosing the resolution might be useful.\nC.3\nAugmenting CTX with a global feature\nConcurrent work [82] showed that applying logistic regression to a globally-pooled feature at test\ntime can improve results. Here, we modify CTX to use the same ideas. We ﬁrst globally pool the\nfeature Φ(x) spatially, which results in a ﬂat 512-dimension vector for each image in both the query\n16\n",
    "and test set. We then train a simple Logistic Regression classiﬁer using the same parameters from [82]\n(sklearn’s implementation with a multinomial loss C=10, applied to ℓ2-normalized features). Running\nthe classiﬁer on the query images produces another set of logits, which we ﬁnd are scaled smaller than\nCTX logits. Therefore, we produce ﬁnal classiﬁer logits via argmax(CTX(S, xq) + λ LR(S, xq)),\nwhere CTX(S, xq) is the logits produced by CTX for query xq and support set S, LR(S, xq) are the\nlogits from the logistic regression classiﬁer, and λ is a scalar constant that we set to 5.\nWe ﬁnd that this provides no beneﬁt if the embedding network is trained purely as a CrossTransformer.\nHowever, we ﬁnd beneﬁts on some datasets (notably DTD) if we add an auxiliary loss that matches\nthe loss used in concurrent work [82]. That is, we compute Φ(x) for each image x in the support set,\nand globally pool this feature. We then apply a ﬁxed classiﬁer on top of this feature, which performs\nthe 712-way classiﬁcation for the 712 ImageNet-train categories. We add this classiﬁcation loss to\nthe CTX loss (without weight) at training time.\nC.4\nAugmentation\nFor most experiments in the paper, we use no augmentation for images that aren’t a part of SimCLR\nEpisodes, following prior work [85]. However, BOHB [68] studies augmentation extensively,\nand so we adopt similar augmentation for some experiments. BOHB optimizes parameters for\naugmentation in a similar style to AutoAugment [19], using 2 randomly-selected stages, where each\nstage may consist of rotation, posterizing, solarizing, color shifts, contrast, brightness, sharpness,\nshear, translation, and cutout, with settings discovered via validation on Meta-Dataset’s ImageNet\nval split. We use these only for experiments labeled as “+Aug” in Table 2 and Table 3. We found\nqualitatively, however, that even with these changes, the network could still be quite sensitive to\ninput images which are resized from very small images, especially when the input resolution is high\n(224×224 in our case). Therefore, we add one more stage that the augmentation function can select,\nwhich randomly resizes the image by a ratio sampled uniformly from 1 all the way down to a ratio that\nwould produce a 10-pixel-wide image. Then we compress the image with jpeg, with a compression\nquality uniformly sampled between 75 and 100, before decompressing and resizing to the original\nresolution. These parameters were chosen once, and we ran no hyperparameter sweep to tune them.\nWe expect that properly tuning them on validation data following BOHB [68] would yield further\nimprovements, but we leave this for future work.\nOur implementation of SimCLR follows the public one released by the original authors, using the\nstandard two augmentation ops of random cropping and color jittering with the default parameters.\nAlso following this implementation, we apply random blur to only one image in each pair of positive\nmatches; in our case, we apply the blur to the query image.\nD\nCorrespondence visualization\nFigures 6–15 visualize the attention inferred by CrossTransformers for all the 10 evaluation datasets\nin Meta-Dataset. We show query images for each dataset, along with three support-set images for\neach. Within each query image, we choose three spatial locations (red, green, and blue squares),\nand plot the CrossTransformer attention weights for each one in the corresponding color (higher\nweight means brighter colors). Both 7×7 and 14×14 attention maps corresponding to CTX7 and\nCTX14 models respectively are presented, with the query points selected at approximately the same\nlocation for both models. The inferred correspondences are semantically meaningful, and often not\none-to-one.\nFinally, Figure 5 shows some qualitative examples of a few challenging cases which suggests areas\nfor improvement. In particular, our method sometimes produces conﬁdent correspondences even\nwhen the true correspondence is unclear; in such cases, we might prefer that the method falls back\nto global comparisons. Conversely, the algorithm may not always ﬁnd correspondences when they\nare available, if, for example, there is a large difference in appearance between corresponding points.\nThis suggests that the correspondence itself may be overﬁtting, and suggests a possible avenue for\nfuture research.\n17\n",
    "Query\nCorrespondence\nin support set\n(1)\n(2)\n(3)\n(4)\n(5)\n(6)\nFigure 5: Failure cases. We visualize the attention correspondences for a few challenging cases for\nour method. In (1) incorrect parts of the swings are matched with high conﬁdence. In the (2) the\nhandle of the ladle is not localized. A shortcoming of our approach is that the model is tasked to\nmatch images which cannot be/are difﬁcult to put into correspondence. For example, in (3) and (4)\nno clear correspondence is feasible, and the model matches parts which are plausibly in the same\ngeometric location. Finally, in (5) and (6), the model localizes the instances correctly, but due to large\nvariance in the instance shapes fails at detailed matching of the sub-parts.\nE\nFive-shot\nWe also report ﬁve-shot results. As the standard Meta-Dataset evaluation speciﬁes a broad range of\npossible shots (e.g., up to 100 examples in extremely rare cases), we believe that 5-shot results can\naid in interpretability. The ‘ways’ are sampled as before (i.e., the standard for Meta-Dataset), but the\nnumber of examples per category is set to exactly ﬁve. This means that both support set and query set\nare class balanced (unlike the standard evaluation, where the query set is balanced but the support set\nis not). We use the same checkpoints that were used above (i.e., no additional validation to choose a\ncheckpoint for ﬁve-shot evaluation). Results are shown in Table 4. We see that the performance is\nsomewhat lower, but the overall trends of performance on the datasets are similar.\nF\nConﬁdence intervals\nTable 5 shows conﬁdence intervals for most experiments in this paper, to enable future comparisons\nlike the tables shown in this work.\n18\n",
    "7×7 attention\n14×14 attention\nQuery\nCorrespondence in support set\nQuery\nCorrespondence in support set\nFigure 6: Aircraft. Various aircraft parts (e.g., wings, head, tail, engine, landing wheels) are matched across instances with\nlarge differences in viewpoint/pose and scale.\n7×7 attention\n14×14 attention\nQuery\nCorrespondence in support set\nQuery\nCorrespondence in support set\nFigure 7: CU-Birds. Beak, body, and feet are matched for different species.\n",
    "7×7 attention\n14×14 attention\nQuery\nCorrespondence in support set\nQuery\nCorrespondence in support set\nFigure 8: Describable Textures (DTD). Textures do not have localized parts/sub-parts, and hence, the correspondence is quite\ndiffuse (e.g., the ﬁrst above). However, speciﬁc features, if present, are matched: in the last four examples, the donut’s sprinkles\nand icing, the net-like pattern, the facial features, and the pumpkin’s stem, surface and cavities, are matched respectively..\n7×7 attention\n14×14 attention\nQuery\nCorrespondence in support set\nQuery\nCorrespondence in support set\nFigure 9: FGVCx Fungi. The caps and stem are matched across variations in pose/rotation, and also number of fungi exhibit\none-to-many matches..\n",
    "7×7 attention\n14×14 attention\nQuery\nCorrespondence in support set\nQuery\nCorrespondence in support set\nFigure 10: VGG Flower. The central disk and petals are matched with all instances of ﬂowers present in the support-set images.\nThe background is diffuse and separated from the central object.\n7×7 attention\n14×14 attention\nQuery\nCorrespondence in support set\nQuery\nCorrespondence in support set\nFigure 11: ImageNet. Correspondence is established in the presence of distractors and large variations in object pose and\nshape.\n",
    "7×7 attention\n14×14 attention\nQuery\nCorrespondence in support set\nQuery\nCorrespondence in support set\nFigure 12: MSCOCO. We observe some detailed matching for different object categories: animal parts (ﬁrst two), giraffe\nossicones (ﬁrst), bus, large variations in the pose of rackets (fourth), and different letters in the trafﬁc sign (last).\n7×7 attention\n14×14 attention\nQuery\nCorrespondence in support set\nQuery\nCorrespondence in support set\nFigure 13: Omniglot. Corresponding parts of various character glyphs are matched.\n",
    "7×7 attention\n14×14 attention\nQuery\nCorrespondence in support set\nQuery\nCorrespondence in support set\nFigure 14: Trafﬁc Signs. Corresponding parts are matched, even when the support-image is ﬂipped horizontally (last).\n7×7 attention\n14×14 attention\nQuery\nCorrespondence in support set\nQuery\nCorrespondence in support set\nFigure 15: Quick Draw. Matches across deformations of doodles are observed.\n",
    "Table 4: Five-Shot. For interpretability, we also compute 5-shot results for all Meta-Dataset datasets, and include (accuracy (%) ± conﬁdence (%)) for the sake of\nfuture comparison for CTX models (with the 14x14 feature grid). These are the standard conﬁdence intervals computed for Meta-Dataset: i.e., the standard error\ncomputed from the episode-to-episode variability in accuracy across 600 test episodes.\nILSVRC\nOmniglot\nAircraft\nBirds\nTextures\nQuick Draw\nFungi\nVGG Flowers\nTrafﬁc\nMSCOCO\nRank\nPrototypical (ours)\n41.87±0.89\n61.33±1.13\n39.40±0.78\n65.57±0.73\n59.06±0.60\n47.86±0.80\n41.64±1.02\n83.88±0.48 44.84±0.88\n41.14±0.82\n4.00\nCTX\n51.70±0.90\n84.24±0.79\n62.29±0.73\n79.38±0.54\n65.86±0.58\n63.36±0.73 49.43±0.98\n92.74±0.29\n68.31±0.71 48.63±0.79\n2.25\nCTX+SimCLR Eps\n51.29±0.89\n86.14±0.74 69.74±0.67\n74.85±0.62\n63.84±0.62\n64.11±0.67\n48.87±0.91\n93.00±0.30\n70.62±0.68 48.45±0.83\n2.10\nCTX+SimCLR Eps+Aug 52.56±0.86\n87.53±0.61 64.28±0.71\n73.27±0.63\n64.72±0.63\n66.90±0.66 48.22±0.94\n93.23±0.28\n78.45±0.60\n56.61±0.78\n1.65\nTable 5: Conﬁdence intervals for quantitative results. We report the conﬁdence intervals in addition to the mean accuracy (accuracy (%) ± conﬁdence (%)) for\nthe models introduced in this work for the sake of future comparison. All versions shown here use 224 resolution, ResNet34, and exponential moving average (EMA)\nfor test-time batch norm (BN), unless otherwise speciﬁed.\nILSVRC\nOmniglot\nAircraft\nBirds\nTextures\nQuick Draw\nFungi\nVGG Flowers\nTrafﬁc\nMSCOCO\nProtoNets (ours)\n51.66±1.10\n57.22±1.34\n51.63±0.93\n71.73±1.02\n69.72±0.76\n53.81±1.04\n42.07±1.14\n87.29±0.69 47.45±0.92 44.38±1.03\nProtoNets (ours)+SimCLR Eps\n49.67±1.06\n65.21±1.23\n54.46±0.91\n60.94±0.94\n63.96±0.77\n50.64±1.05\n37.84±1.06\n88.70±0.60 51.61±1.00 42.97±1.04\nProtoNets (ours)+SimCLR Eps (no BN EMA) 53.69±1.10\n67.44±1.26\n57.10±0.99\n74.07±0.91\n69.46±0.75\n51.83±1.02\n41.67±1.21\n86.93±0.65\n57.41±1.04\n41.43±1.10\nCTX7\n59.73±1.08\n74.11±1.22\n70.90±0.99\n80.29±0.86\n73.91±0.70\n65.61±0.82\n48.53±1.09\n91.98±0.52 68.81±0.99 50.62±1.03\nCTX14\n61.94±1.04\n76.52±1.14\n79.65±0.91\n84.06±0.85\n76.26±0.70\n65.67±0.91\n52.53±1.16\n94.11±0.44 70.47±0.92 53.51±1.06\nCTX7+SimCLR Eps\n60.69±0.99\n79.22±1.16\n76.64±0.88\n77.86±0.92\n77.31±0.66\n67.43±0.88\n47.37±1.15\n93.30±0.43\n69.56±0.99\n52.35±1.01\nCTX14+SimCLR Eps\n63.79±1.00\n80.83±1.07\n82.05±0.83\n82.01±0.89\n75.76±0.76\n68.84±0.88\n52.01±1.13\n94.62±0.43 75.01±0.93 52.76±1.03\nCTX7+SimCLR Eps+Aug\n61.20±1.04\n87.26±0.65\n77.98±0.89\n68.31±0.71\n72.70±0.71\n73.32±0.77\n44.12±0.94\n93.29±0.43 80.03±0.80 57.88±1.04\nCTX14+SimCLR Eps+Aug\n62.76±0.99\n82.21±1.00\n79.49±0.89\n80.63±0.88\n75.57±0.64\n72.68±0.82\n51.58±1.11\n95.34±0.37 82.65±0.76 59.90±1.02\nCTX14+SimCLR Eps+Aug+LR\n62.25±0.96\n82.03±0.98\n77.41±0.84\n76.66±0.87\n80.29±0.72\n72.24±0.81\n49.39±1.17\n93.05±0.50 75.25±0.93 60.35±1.06\n24\n"
  ],
  "full_text": "CrossTransformers: spatially-aware few-shot transfer\nCarl Doersch∗\nAnkush Gupta∗\nAndrew Zisserman∗†\n∗DeepMind, London\n† VGG, Department of Engineering Science, University of Oxford\nAbstract\nGiven new tasks with very little data—such as new classes in a classiﬁcation\nproblem or a domain shift in the input—performance of modern vision systems\ndegrades remarkably quickly. In this work, we illustrate how the neural network\nrepresentations which underpin modern vision systems are subject to supervision\ncollapse, whereby they lose any information that is not necessary for performing\nthe training task, including information that may be necessary for transfer to new\ntasks or domains. We then propose two methods to mitigate this problem. First,\nwe employ self-supervised learning to encourage general-purpose features that\ntransfer better. Second, we propose a novel Transformer based neural network\narchitecture called CrossTransformers, which can take a small number of labeled\nimages and an unlabeled query, ﬁnd coarse spatial correspondence between the\nquery and the labeled images, and then infer class membership by computing\ndistances between spatially-corresponding features. The result is a classiﬁer that\nis more robust to task and domain shift, which we demonstrate via state-of-the-\nart performance on Meta-Dataset, a recent dataset for evaluating transfer from\nImageNet to many other vision datasets. Code and pretrained checkpoints available\nat: https://github.com/google-research/meta-dataset.\n1\nIntroduction\nGeneral-purpose vision systems must be adaptable. Home robots must be able to operate in new,\nunseen homes; photo-organizing software must recognize unseen objects (e.g., to ﬁnd examples of\n“my sixth-grade son’s abstract art project”); industrial quality-assurance systems must spot defects in\nnew products. Deep neural network representations can bring some visual knowledge from datasets\nlike ImageNet [67] to bear on different tasks beyond ImageNet [14, 31, 61], but empirically, this\nrequires a non-trivial amount of labeled data in the new task. With too little labeled data, or for a\nlarge change in distribution, such systems empirically perform poorly.\nResearch on meta-learning directly benchmarks adaptability. At training time, the algorithm receives\na large amount of data and accompanying supervision (e.g., labels). At test time, however, the\nalgorithm receives a series of episodes, each of which consists of a small number of datapoints from a\ndifferent distribution than the training set (e.g., a different domain or different classes). Only a subset\nof this data has the accompanying supervision (called the support set); the algorithm must make\npredictions about the rest (the query set). Meta-Dataset [85] is particularly relevant for vision, as the\nchallenge is few-shot ﬁne-grained image classiﬁcation. The training data is a subset of ImageNet\nclasses. At test time, each episode either contains images from the other ImageNet classes, or from\none of nine other visually distinct ﬁne-grained recognition datasets. The algorithm must rapidly adapt\nits representations to the new classes and domains.\nSimple centroid-based algorithms like Prototypical Nets [16, 75] are near state-of-the-art on Meta-\nDataset, achieving around 50% accuracy on the held-out ImageNet classes in Meta-Dataset’s valida-\ntion set (chance is roughly 1 in 20). An equivalent classiﬁer trained on those validation classes can\nachieve roughly 84% accuracy on the same challenge. What accounts for the enormous discrepancy\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2007.11498v5  [cs.CV]  17 Feb 2021\n\n\nQuery\nNearest Neighbors\ngila monster\ngila monster hognose snake gila monster\ngila monster\ngila monster\nnight snake\nhognose snake gila monster\nknot\nletter opener\nletter opener\nscrewdriver\nletter opener\nballpoint\nfountain pen\nbuckle\nletter opener\nletter opener\nbassoon\nhammer\nhammer\nmicrophone\nhammer\nscrew\nhammer\nsyringe\nshovel\nhook\nscrew\nbuckeye\nbuckeye\nbuckeye\nhip\npomegranate\nsunglasses\nbuckeye\nbuckeye\nbuckeye\nscrew\n  : in test split\nFigure 1: Illustration of supervision collapse with nearest neighbors. In each row, the leftmost\nimage is a query taken from the Meta-Dataset ImageNet test classes, and the rest are the top 9\nnearest neighbors from both training and test support set classes, using the embedding learned by a\nPrototypical Net (training details in Appendix B). Images belonging to the test split are indicated by a\nnear the bottom left corner; rest are from the training split. For a simple classiﬁer to work well on\nthese test classes, semantically similar images should have similar representations, and so we hope the\nnearest neighbors would come from the same—or semantically similar—classes. Instead, we observe\nthat only 5% of matches for test-set queries are from the same class as the query. Furthermore,\nmany matches are all from the same incorrect training class (highlighted in red). We see a knot is\nmatched with several gila monsters (and other reptiles); a bassoon with letter openers (and pens); a\nscrew with hammers; another screw with buckeyes. The errors within that wrong class often have\nwidely different appearance: for example, the bottom-most screw is matched with single buckeyes\nand also a pile of buckeyes. One interpretation is that the network picks up on image patterns during\ntraining that allow images of each class to be tightly grouped in the feature space, minimizing other\nways that the image might be similar to other classes in preparation for a conﬁdent classiﬁcation. For\nout-of-domain samples, the network can then overemphasize a spurious image pattern that suggests\nmembership in one training-set class. This is the consequence of supervision collapse, where image\npatterns that might help make the correct associations are lost.\nbetween performance on within-distribution samples and out-of-distribution samples? We hypothe-\nsize that because the neural network backbone of Prototypical Nets is designed for classiﬁcation, they\ndo just this: represent only an image’s (training-set) class, and discard information that might help\nwith out-of-distribution classes. Doing so minimizes the losses for many meta-learning algorithms,\nincluding Prototypical Nets. We call this problem supervision collapse, and illustrate it in Figure 1.\nOur ﬁrst contribution is to explore using self-supervision to overcome supervision collapse. We\nemploy SimCLR [15], which learns embeddings that discriminate between every image in the dataset\nwhile maintaining invariance to transformations (e.g., cropping and color shifts), thus capturing more\nthan just classes. However, rather than treat SimCLR as an auxiliary loss, we reformulate SimCLR as\n“episodes” that can be classiﬁed in the same manner as a training episode.\nOur second contribution is a novel architecture called CrossTransformers, which extends Trans-\nformers [87] to few-shot ﬁne-grained classiﬁcation. Our key insight is that objects and scenes are\ngenerally composed of smaller parts, with local appearance that may be similar to what has been seen\nat training time. The classical example of this is the centaur that appeared in several early papers on\nvisual representation [11, 40, 88], where the parts from the human and horse composed the centaur.\nCrossTransformers operationalize this insight of (i) local part-based comparisons, and (ii) accounting\nfor spatial alignment, resulting in a procedure for comparing images which is more agnostic to the\nunderlying classes. In more detail, ﬁrst a coarse alignment between geometric or functional parts in\nthe query- and support-set images is established using attention as in Transformers. Then, given this\nalignment, distances between corresponding local features are computed to inform classiﬁcation. We\ndemonstrate this improves generalization to unseen classes and domains.\n2\n\n\nIn summary, our contributions in this paper are: (i) We improve the robustness of our local features\nwith a self-supervised technique, modifying the state-of-the-art SimCLR [15] algorithm. (ii) We\npropose the CrossTransformer, a network architecture that is spatially aware and performs few-shot\nclassiﬁcation using more local features, which improves transfer. Finally, (iii) we evaluate and\nablate how the choices in these algorithms impact Meta-Dataset [85] performance, and demonstrate\nstate-of-the-art results on nearly every dataset within it, often by large margins.\n2\nRelated Work\nFew-shot image classiﬁcation.\nFew-shot learning [25, 35, 46, 53] has recently been primarily\naddressed in the meta-learning framework [57, 71, 80], where a model learns an update rule for\nthe parameters of a base-learner model [5, 6, 72] through a sequence of training episodes [79, 89].\nThe meta-learner either learns to produce new parameters directly from the new data [8, 33, 56, 62,\n64, 72, 73], or learns to produce an update rule to iteratively optimize the base learner to ﬁt the\nnew data [1, 5, 7, 38, 63, 98]. [27, 51, 59] do not use any explicit meta-learner model, but instead\nunroll the base-learner gradient updates and optimize for model initializations which generalize\nwell on novel tasks. Matching-based methods [28, 75, 78, 90] instead learn representations for\nsimilarity functions [10, 16, 17, 44, 82], in the hope that the similarities will generalize to new\ndata. CrossTransformers fall in this category, and share much of their architecture with Prototypical\nNets [75].\nAttention for few-shot learning.\nCrossTransformers attend [3] individually over each class’s\nsupport set to establish local correspondences, whereas Matching Networks [90] attend over the\nwhole support set to “point to” matching instances. [54] extend this idea to larger contexts using\ntemporally dilated convolutions [86]. In the limit, attention over long-term experiences accumulated\nin memories [43, 56, 69, 76] can augment more traditional learning.\nCorrespondences for visual recognition.\nCrossTransformers perform classiﬁcation by matching\nmore local parts. Discriminative parts [4, 9, 22, 34, 42, 83] and visual words [74, 101] have a\nrich history, and have found applications in deformable-parts models [26], classiﬁcation [70, 101],\nand retrieval [12, 84]. Part-based correspondences for recognition [104] have been particularly\nsuccessful in ﬁne-grained face retrieval and recognition [48, 96]. CrossTransformers establish\nsoft correspondences between pixels in the query- and support-set images; such dense pairwise\ninteractions [93] have recently proved useful for generative networks [100], semantic matching [66]\nand tracking [91]. [49] learns spatially dense classiﬁers for few-shot classiﬁcation, but pools the\nspatial dimensions of the prototypes, and hence does not have a notion of correspondence.\nSelf-supervised learning for few-shot.\nOur work on SimCLR episodes inherits from a line of\nself-supervised learning research, which typically deal with transfer from pretext tasks to semantic\nones and must therefore represent more than their training data [2, 13, 15, 21, 24, 30, 36, 47,\n60, 102, 103]. Some recent works [29, 77] demonstrate that this can improve few-shot learning,\nalthough these use self-supervised auxiliary losses rather than integrating self-supervised instance\ndiscrimination [15, 24, 55, 81, 95] into episodic training. Also particularly relevant are methods\nthat use self-supervision for correspondence [41, 50, 91], which may in future work improve the\ncorrespondences that CrossTransformers use.\n3\nStopping Collapse: SimCLR Episodes and CrossTransformers\nWe take a two-pronged approach to dealing with the supervision collapse problem. Modern ap-\nproaches to few-shot learning typically involve learning an embedding for each image, followed by a\nclassiﬁer that aggregates information across an episode’s support set in order to classify the episode’s\nqueries. Our ﬁrst step aims to use self-supervised learning to improve the embedding so it expresses\ninformation beyond the classes, in a way that is as algorithm-agnostic as possible. Once we have these\nembeddings, we build a classiﬁer called a CrossTransformer. CrossTransformers use Prototypical\nNets [75] as a blueprint, chosen due to their simplicity and strong performance; the main modiﬁcation\nis to aggregate information in a spatially-aware way. We begin by reviewing Prototypical Nets, and\nthen describe the two approaches.\n3\n\n\nPrototypical Nets are episodic learners, which means training is performed on the same kind of\nepisodes that will be presented at test time: a query set Q of images, and a support set S which can\nbe partitioned into classes c ∈{1, 2, . . . , C}: each Sc = {xc\ni}N\ni=1 is composed of N example images\nxc\ni. Prototypical Nets learn a distance function between the query and each subset Sc. Both the\nquery- and support-set images are ﬁrst encoded into a D-dimensional representation Φ(x), using a\nshared ConvNet Φ : RH×W ×3 7→RD, where H, W are the height and width respectively. Then a\n“prototype” tc ∈RD for the class c is obtained by averaging the representations of the support set Sc,\ntc =\n1\n|Sc|\nP\nx∈Sc Φ(x). Finally, a distribution of classes is obtained using softmax over the distances\nbetween the query image and class prototypes: p(y = c|xq) =\nexp(−d(Φ(xq),tc))\nPC\nc′=1 exp(−d(Φ(xq),tc′)). In practice,\nthe distance function d is ﬁxed to be the squared Euclidean distance d(xq, Sc) = ||Φ(xq) −tc||2\n2.\nThe learning objective is to train the embedding network Φ to maximize the probability of the correct\nclass for each query.\n3.1\nSelf-supervised training with SimCLR\nOur ﬁrst challenge is to improve the neural network embedding Φ: after all, if these features have\ncollapsed to represent little information beyond the classes, then a subsequent classiﬁer cannot can\nrecover this information. But how can we train features to represent things beyond labels when our\nonly supervision is the labels? Our solution is self-supervised learning, which invents “pretext tasks”\nthat train representations without labels [21, 24], and better yet, has a reputation for representations\nthat transfer beyond this pretext task. Speciﬁcally we use SimCLR [15], which uses “instance\ndiscrimination” as a pretext task. It works by applying random image transformations (e.g., cropping\nor color shifts) twice to the same image, thus generating two “views” of that image. Then it trains the\nnetwork so that representations of the two views of the same image are more similar to each other\nthan they are to those of different images. Empirically, networks trained in this way become sensitive\nto semantic information, but also learn to discriminate between different images within a single class,\nwhich is useful for combating supervision collapse.\nWhile we could treat SimCLR as an auxiliary loss on the embedding, we instead reformulate SimCLR\nas episodic learning, so that the technique can be applied to all episodic learners with minimal\nhyper-parameters. To do this, we randomly convert 50% of the training episodes into what we call\nSimCLR episodes, by treating every image as its own class. For clarity, we will call the original\nepisodes that have not been converted SimCLR episodes MD-categorization episodes, to emphasize\nthat they use the original categories from Meta-Dataset. Speciﬁcally, let ρ(·) be SimCLR’s (random)\nimage transformation function, and let S = {xi}|S|\ni=1 be a training support set. We generate a\nSimCLR episode by sampling a new support set, transforming each image in the original support\nset S′ = {ρ(xi)}|S|\ni=1, and then generating query images by sampling other transformations from the\nsame support set: Q′ = {ρ(random_sample(S))}|Q|\ni=1, where random_sample just takes a random\nimage from the set.1 The original query set Q is discarded. The label for an image in the SimCLR\nepisode is its index in the original support set, resulting in an |S|-way classiﬁcation for each query.\nNote that for a SimCLR episode, the ‘prototypes’ in Prototypical Nets average over just a single\nimage, and therefore the Prototypical Net loss can be written as\nexp(−d(Φ(ρ(xq)),Φ(ρ(xq))))\nPn\ni=1 exp(−d(Φ(ρ(xq)),Φ(ρ(xi))). If we\ndeﬁne d as the cosine distance rather than Euclidean, this loss is identical to the one used in SimCLR.\n3.2\nCrossTransformers\nGiven a query image xq and a support set Sc = {xc\ni}N\ni=1 for the class c, CrossTransformers aim to\nbuild a representation which enables local part-based comparisons between them.\nCrossTransformers start by making the image representation a spatial tensor, and then assemble\nquery-aligned class prototypes by putting the support-set images Sc in correspondence with the\nquery image. The distance between the query image and the query-aligned prototype for each\nis then computed and used in a similar way to Prototypical Nets. In practice, we establish soft\ncorrespondences using attention [3] based Transformers [87]. In contrast, Prototypical Nets use ﬂat\nvector representations which lose the location of image features, and have a ﬁxed class prototype\nwhich is independent of the query image.\n1We enforce that the sampled queries have the same class distribution as Q, and have no repeats.\n4\n\n\np\nΓ\nΓ\nΩ\nm\nn\nDot-product query features at location \np against all support set (Sc) spatial \nfeatures in class\nΛ\nΛ\nSoftmax across all\nspatial features in class\np\nKey Heads\nValue Heads\nQuery-aligned\nprototype (tc)\nweighted sum\nSoftmax normalized\nattention weights\nSupport-set (Sc) image \nfeatures for the category c\nQuery Head\ndv\ndv\ndk\ndk\nQuery image \nfeatures\nm\nn\na2\n    c\na1\n    c\n~a2\n    c\n~a1\n    c\nKeys (kc)\nValues (vc)\nQueries (q)\nFigure 2: CrossTransformers. Construction of query-aligned class prototype vector tc\np for the class\nc and the query image xq, focusing on the spatial location p in xq. The query vector qp is compared\nagainst keys kc from all spatial locations in the support set Sc to obtain attention scores ac, which are\nsoftmax normalized before being used to aggregate the values vc for the aligned prototype vector tc\np.\nConcretely, CrossTransformers remove the ﬁnal spatial pooling in Prototypical Nets’ embedding\nnetwork Φ(·), such that the spatial dimensions H′, W ′ are preserved: Φ(x) ∈RH′×W ′×D. Following\nTransformers, key-value pairs are then generated for each image in the support set using two\nindependent linear maps: the key-head Γ : RD 7→Rdk, and the value-head Λ : RD 7→Rdv\nrespectively. Similarly, the query image features Φ(xq) are embedded using the query-head Ω:\nRD 7→Rdk. Dot-product attention scores are then obtained between keys and queries, followed\nby softmax normalization across all the images and locations in Sc. This attention serves as our\ncoarse correspondence (see example attention visualizations in Figure 3 and Appendix D), and is\nused to aggregate the support-set features into alignment with the query. This process is visualized in\nFigure 2.\nMathematically, let kc\njm = Γ · Φ(xc\nj)m be the key for the jth image in the support set for class c at\nspatial position m (index over the two dimensions H′, W ′), and similarly let qp = Ω· Φ(xq)p be the\nquery vector at spatial position p in the query image xq. The attention ˜ac\njmp ∈R between the two is\nthen obtained as:\n˜ac\njmp =\nexp(ac\njmp/τ)\nP\ni,n exp(ac\ninp/τ),\nwhere\nac\njmp = kc\njm · qp,\nand\nτ =\np\ndk.\n(1)\nNext, the aligned prototype vector tc\np corresponding to spatial location p in the query is obtained by\naggregating the support-set values vc\njm = Λ · Φ(xc\nj)m using the attention weights above:\ntc\np =\nX\njm\n˜ac\njmpvc\njm\n(2)\nFinally, squared Euclidean distances between aligned local features from the above prototype and\ncorresponding query image values wp = Λ · Φ(xq)p are aggregated as below. This scalar distance\nacts as a negative logit for a distribution over classes as in Prototypical Nets.\nd(xq, Sc) =\n1\nH′W ′\nX\np\n||tc\np −wp||2\n2\n(3)\nNote we apply the same value-head Λ to both the query and support-set images. This ensures that\nthe CrossTransformer behaves somewhat like a distance. That is, imagine a trivial case where, for\none class, all images in Sc are identical to xq. We would want d(xq, Sc) to approach 0 even if the\nnetwork is untrained, or if these images are highly dissimilar from those used for training. Sharing Λ\nbetween the support and query sets helps accomplish this: in fact, if ˜ac\njmp is 1 where p = m and 0\nelsewhere for all j, then d(xq, Sc) will be identically 0 under this architecture, no matter the network\n5\n\n\nQuery\nCorrespondence in support set\nQuery\nCorrespondence in support set\nFigure 3: Visualization of the attention ˜a. We show four query images, along with three support-set\nimages for each. Within each query image, we choose three spatial locations (red, green, and blue\nsquares), and plot the CrossTransformer attention weights for each one in the corresponding color\n(brighter colors mean higher weight). The four examples are from Aircraft, CU-Birds, VGG Flowers,\nand ImNet test sets respectively (clockwise, starting from top-left). No matter which dataset, the\nattention masks are semantically relevant, even when the correspondence is not one-to-one. More\nvisualizations are given in Appendix D.\nweights. To encourage this behavior for the attention ˜a, we also set Γ = Ω, i.e., the key and query\nheads are the same. This way, in our trivial case, the attention is likely to be maximal for spatial\nlocations that correspond, because kc\njm and qp will be the same for p = m.\nFor one experiment, we also augment the CrossTransformer with a global feature, which can help for\nsome datasets like DTD (Describable Textures Dataset) with less spatial structure.\n4\nExperiments\nWe evaluate on Meta-Dataset [85], speciﬁcally the setting where the training is performed on the\nImageNet train split only, which is 712 classes (plus 158 classes for validation, which are not used\nfor training but only to perform early stopping). We then test on the remaining 130 held-out classes\nfrom ImageNet, as well as 9 other image datasets. Note that this is in contrast to another popular (and\neasier) setting, where the training also uses a subset of categories from more of these datasets: usually\nall datasets except Trafﬁc Signs and COCO. For clarity, we’ll use “Meta-Dataset Train-on-ILSVRC”\nto denote training on ImageNet only, and “Meta-Dataset Train-on-all” to denote when training occurs\non more datasets. Test time consists of a series of episodes, each of which contains: (1) a support set\nbetween 50 and 500 labeled images which come from between 5 and 50 classes; and (2) an unlabeled\nquery set with 10 images per class. Meta-Dataset aims for ﬁne-grained recognition, so the classes in\neach episode are mutually similar: one episode may contain only musical instruments, another may\ncontain only birds, etc.\nMeta-Dataset is useful for studying transfer because different test datasets encapsulate different\nkinds of transfer challenges. For test datasets like CU-Birds [92], there are numerous similar classes\nin ImageNet train (20 bird classes in ImageNet train, versus 100 in the CU-birds test dataset). In\ncontrast, for test datasets like Aircraft [52], there is just a single corresponding class in ImageNet\ntrain; therefore, algorithms which don’t represent the intra-class variability for this class will be\npenalized. The ImageNet test set has images in a similar domain to the ImageNet train set but with\ndifferent classes, while test datasets like COCO contain many similar classes to ImageNet, but with\ndomain shift (in COCO, instances are generally not the subject of their photographs, and may be\nlow-resolution or occluded). Finally, test datasets like OmniGlot combine these challenges, i.e.,\ndifferent classes in a substantially different domain.\n4.1\nImplementation details\nTo ensure comparability, we followed the public implementation of Prototypical Nets for Meta-\nDataset [85] wherever possible. This includes using the same hyperparameters, unless otherwise\nnoted. For the hyperparameters that were chosen with a sweep on the validation set (learning rate\nschedule and weight decay), we simply used the best values discovered for Prototypical Nets for all\nthe experiments in this paper. See Appendix C.2 for details of the CrossTransformer architecture. We\n6\n\n\nuse no pretraining for CrossTransformers, although to be consistent with prior work [85] we use it\nfor the experiments involving Prototypical Nets.\nWe incorporate two improvements from Meta-Baseline [16], which at test time is similar to Proto-\ntypical Nets (though it isn’t trained as an episodic learner). The ﬁrst is to keep exponential moving\naverages for Batch Norm statistics during training, and use those for Batch Norm at test time. Second,\nwe note that Meta-Baseline does not train on ﬁne-grained episodes sampled from the ImageNet hier-\narchy, as Prototypical Nets does, but rather on batches with uniformly-sampled classes. Empirically,\nPrototypical Nets trained only on ﬁne-grained episodes struggle to do coarse-grained recognition, as\nrequired for datasets like COCO. Therefore, we only use the ImageNet hierarchy to make 50% of\nepisodes ﬁne-grained; the rest have categories sampled uniformly.\nChoice of network.\nPrior implementations of networks like Prototypical Nets use relatively small\nnetworks (e.g., ResNet-18) with small input images (e.g. 126×126 pixels), and report that measures\nto increase capacity (e.g., Wide ResNets [99]) provide minimal beneﬁts. This is surprising given that\nstandard networks show improvements for increasing capacity (e.g., ResNet-34 outperforms ResNet-\n18 by 3% on ImageNet [37]). Making our networks spatially-aware requires higher-resolution, and\nalso higher-capacity networks are especially important in self-supervised learning [23, 45]. Therefore,\nour experiments increase resolution to the standard 224×224 and use ResNet-34, and we also use\nnormalized stochastic gradient descent [18, 58], which we found improved stability when ﬁne-tuning\nmore complex networks. Table 1 compares the Prototypical Nets performance of this network to that\nof using a ResNet-18. Increased capacity leads to only slight performance improvements, which are\nmore pronounced for datasets that are similar to ImageNet; it harms, e.g., OmniGlot. Further details\nin Appendix C.1.\nFor experiments with CrossTransformers, we also increased the resolution of the convolutional feature\nmap by setting the stride of ﬁnal block of the ResNet to 1, and using dilated convolutions to preserve\nthe feature computation [32, 39]. This turns the usual 7×7 feature map for a 224×224 image into a\n14×14 feature map. We ablate this choice in Appendix C.2.\nAugmenting CTX with a global feature.\nRecent works have also shown beneﬁts for applying\nlogistic regression (LR) at test time [82]. In practice, it is too expensive to apply LR to our query-\naligned prototypes (as this would involve a separate classiﬁer for every query). Therefore, we instead\napply logistic regression to a globally-pooled feature and average the logits with those produced by\nthe CrossTransformer. See Appendix C.3 for details.\nAugmentation.\nWhile most experiments use no augmentation (apart from SimCLR episodes) to be\nconsistent with prior work [85], more recent work [16, 68, 82] showed that stronger data augmentation\nis effective. Therefore, for two experiments, we employ augmentation using the settings discovered\nin BOHB [68] (via Auto-Augment [19] on the validation set), with an extra stage that randomly\ndownsamples and then upsamples images, which we ﬁnd helpful as our network operates at higher\nresolution than many of the test datasets. This BOHB augmentation is only applied to the “MD-\ncategorization” episodes, and not to the SimCLR episodes. Note this BOHB augmentation is different\nfrom SimCLR-style augmentation, which is used in SimCLR Episodes as well as in the ablation\n(SC-Aug) in Table 1. See Appendix C.4 for details.\n4.2\nResults for self-supervised learning with SimCLR on Prototypical Nets\nWe ﬁrst analyze the impact of SimCLR Episodes and other architectural choices in Table 1. For\nbaseline Prototypical Nets, SimCLR Episodes generally improve performance, but this depends on\narchitectural choices. Improvements are largest for datasets that are more distant from ImageNet,\ne.g., OmniGlot and Quickdraw, and datasets which require distinguishing between sub-categories\nImageNet categories, e.g., Aircraft and Trafﬁc Signs. In ImageNet, all commercial airplanes fall\nin a single ImageNet class; therefore, the success of SimCLR Episodes here suggests they recover\nfeatures which are lost due to supervision collapse. Strangely, however, SimCLR Episodes interact\nwith Batch Norm: we ﬁnd more robust improvements when computing Batch Norm statistics from\nthe test-time support set, but not when using exponential moving averages (EMA) as suggested by\n[16]. One possible interpretation is that the network has learned to use Batch Norm to communicate\ninformation across the batch: e.g., to distinguish between SimCLR Episodes and MD-categorization\nepisodes. Using EMA at test time may prevent this, which may confuse the network. Interestingly,\n7\n\n\nTable 1: Effects of architecture and SimCLR Episodes on Prototypical Nets, for Meta-Dataset\nTrain-on-ILSVRC. We ablate architectural choices: use of Exponential Moving Averages (EMA) at\ntest time for Batch Norm (versus computing Batch Norm statistics on the support set at test time),\nimage resolution (224, versus the baseline’s 126), ResNet-34 (R34) replacing ResNet-18, SimCLR-\nstyle augmentation (SC-Aug), and the addition of 50% SimCLR Episodes (SC-Eps). The test datasets\nfrom Meta-Dataset are ImNet: Meta-Dataset’s ImageNet Test classes; Omni: OmniGlot drawn\ncharacters; Acraft: Aircraft; Bird: CU-Birds; DTD: Textures; QDraw: Quick Draw drawings; Fungi:\nFGVCx fungi challenge; Flower: VGG Flowers; COCO: Microsoft COCO cropped objects. The best\nnumber in each column is bolded, along with others that are within a conﬁdence interval [85]. Rank\nis the average rank for each method. Using SimCLR Episodes provides improvements on almost\nall datasets, and provides especially large boosts for datasets which are dissimilar from ImageNet,\nsuch as OmniGlot. However, simply using SimCLR transformations without instance discrimination\n(SC-Aug) harms results on almost all datasets. Increased capacity provides small beneﬁts on some\ndatasets, especially the more realistic and ImageNet-like datasets (e.g., birds), but actually harm\nothers like OmniGlot. Note that in this table, QuickDraw uses the split from the original paper [85]\nrather than the (somewhat easier) split published for that paper’s public benchmark. For all other\ntables, we use the split from the published benchmark.\n224 R34 SC-Aug SC-Eps EMA ImNet Omni Acraft\nBird DTD QDraw Fungi Flower\nSign COCO Rank\n\u0013\n49.10 59.27\n49.31 68.43 66.70\n45.83 38.48\n85.34 49.49\n42.88\n5.55\n49.77 55.70\n52.06 68.58 67.27\n49.86 37.68\n84.32 50.27\n41.92\n5.20\n\u0013\n\u0013\n\u0013\n51.66 57.22\n51.63 71.73 69.72\n47.31 42.07\n87.29 47.45\n44.38\n4.35\n\u0013\n\u0013\n52.51 49.87\n56.47 72.81 68.45\n51.41 42.16\n87.92 54.40\n40.60\n3.30\n\u0013\n\u0013\n\u0013\n\u0013\n47.58 55.73\n46.93 57.75 54.88\n42.91 37.42\n83.82 46.88\n43.36\n7.55\n\u0013\n\u0013\n\u0013\n47.94 51.79\n54.58 62.84 58.64\n46.36 36.06\n76.88 48.35\n38.77\n7.45\n\u0013\n\u0013\n\u0013\n\u0013\n49.67 65.21\n54.46 60.94 63.96\n50.64 37.84\n88.70 51.61\n42.97\n4.35\n\u0013\n\u0013\n\u0013\n53.69 68.50\n58.04 74.07 68.76\n53.30 40.73\n86.96 58.11\n41.70\n1.90\nProtoNets [85]\n50.50 59.98\n53.10 68.79 66.56\n48.96 39.71\n85.27 47.12\n41.00\n5.35\nwe will show later that SimCLR Episodes don’t harm CrossTransformers as they harm Prototypical\nNets when using EMA at test time, suggesting the two architectures solve the problem differently.\nRecall that converting an MD-categorization episode into a SimCLR episode makes two changes to\nthe episode: it 1) applies data augmentation, and 2) converts the classiﬁcation problem to “instance\ndiscrimination,” by selecting images from the support set as a new query set, and requiring the\nnetwork to predict the selected indices. To ensure that we are not simply seeing the effect of data\naugmentation, we also implemented a baseline (SC-Aug) that does 1 but not 2 to the input MD-\ncategorization episodes, and does this augmentation for all episodes (rather than 50%, which is\nthe fraction of MD-categorization episodes that are converted to SimCLR episodes for SC-Eps\nexperiments). Indeed, we see no improvements for this change, and in fact non-trivial performance\nloss from this, mirroring the result for supervised learning in the original paper [15]. This reinforces\nthat SimCLR was designed for self-supervised learning, and so the transformations are more severe\nthan is usually optimal for supervised learning.\nFinally, we see small improvements from using larger networks and higher resolution for the baseline\nmodel. While our baseline is overall better than the baseline Prototypical Nets implementation [85],\nit is still below the state-of-the-art for centroid-based methods which rely more heavily on pretraining,\nand use no episodic training [16].\n4.3\nCrossTransformers results\nGiven these performant features, we next turn to CrossTransformers. Table 2 compares CrossTrans-\nformers (CTX) with and without SimCLR episodes to several state-of-the-art methods, including the\nPrototypical Nets on which they are based. We see that CrossTransformers provide strong perfor-\nmance on their own, including having a better average rank than all baselines. With SimCLR episodes\nproviding more versatile features, we see further improvements, with performance on-par or better\nthan the best methods on almost every dataset. We note particularly large improvements on OmniGlot,\nwhich has a large domain gap relative to the training data. We also see strong improvements on Street\nSigns, Aircraft, and Flowers, where multiple test-time categories map to few training-time categories,\nand often exhibit well-deﬁned spatial correspondence.\n8\n\n\nTable 2: CrossTransformers (CTX) comparison to state-of-the-art. We compare four versions\nof CrossTransformers to several state-of-the-art methods, which are the best performers among\nthose evaluated for Meta-Dataset Train-on-ILSVRC. We see that CTX alone has better average rank\nthan any baseline. Adding SimCLR episodes (+SimCLR Eps) and data augmentation inspired by\nBOHB [68] (+Aug) further improves results. Our full model is on-par or above prior methods on all\nbut one dataset, sometimes with large gaps over the best baseline (e.g., +5% on OmniGlot, +13% on\nAircraft, +5% on Signs), and furthermore, each prior method has some datasets where we outperform\nby a larger margin (the next best average rank [82], performs 19% worse on Aircraft and 17% worse\non OmniGlot). Finally, adding a test-time Logistic Regression classiﬁer inspired by Tian et al. [82]\nimproves performance on the one dataset—DTD textures—that was otherwise lacking. Note that\nmost of these methods [16, 68, 82] are unpublished concurrent work.\nImNet Omni Acraft\nBird\nDTD QDraw Fungi Flower\nSign COCO\nRank\nFinetuning [85]\n45.78 60.85\n68.69 57.31 69.05\n42.60 38.20\n85.51 66.79\n34.86 12.20\nProtoNets [85]\n50.50 59.98\n53.10 68.79 66.56\n48.96 39.71\n85.27 47.12\n41.00 12.65\nProtoNets+MAML [85]\n49.53 63.37\n55.95 68.66 66.49\n51.52 39.96\n87.15 48.83\n43.74 11.55\nCNAPS [65]\n50.60 45.20\n36.00 60.70 67.50\n42.30 30.10\n70.70 53.30\n45.20 13.55\nBOHB-L [68]\n50.60 64.09\n57.36 67.68 70.38\n46.26 33.82\n85.51 55.17\n41.58 11.50\nBOHB-NC [68]\n51.92 67.57\n54.12 70.69 68.34\n50.33 41.38\n87.34 51.80\n48.03 10.15\nBOHB-NC Ensemble [68]\n55.39 77.45\n60.85 73.56 72.86\n61.16 44.54\n90.62 57.53\n51.86\n7.45\nDhillon et al. [20]\n-\n-\n68.69 74.26 77.35\n-\n-\n88.14 55.98\n40.62\n-\nMeta-Baseline [16]\n59.20 69.10\n54.10 77.30 76.00\n57.30 45.40\n89.60 66.20\n55.70\n7.20\nTian et al. LR [82]\n60.14 64.92\n63.12 77.69 78.59\n62.48 47.12\n91.60 77.51\n57.00\n5.50\nTian et al. LR-distill [82]\n61.58 64.31\n62.32 79.47 79.28\n60.83 48.53\n91.00 76.33\n59.28\n4.60\nProtoNets (Our implementation)\n51.66 57.22\n51.63 71.73 69.72\n53.81 42.07\n87.29 47.45\n44.38 11.10\nCTX\n61.94 76.52\n79.65 84.06 76.26\n65.67 52.53\n94.11 70.47\n53.51\n3.85\nCTX+SimCLR Eps\n63.79 80.83\n82.05 82.01 75.76\n68.84 52.01\n94.62 75.01\n52.76\n3.05\nCTX+SimCLR Eps+Aug\n62.76 82.21\n79.49 80.63 75.57\n72.68 51.58\n95.34 82.65\n59.90\n2.25\nCTX+SimCLR Eps+Aug+LR\n62.25 82.03\n77.41 76.66 80.29\n72.24 49.39\n93.05 75.25\n60.35\n3.40\nDTD, however, is more challenging for basic CTX, which is unsurprising since textures have little of\nthe kind of spatial correspondence that CTX attempts to ﬁnd. COCO is also challenging, likely due\nto its extremely large intra-class variation (e.g., occlusion) and the fact that many categories overlap\nwith ImageNet-train categories, meaning that simply memorizing categories from the training set\nmay be more useful than using test-time appearance. To explore this trade-off, we applied logistic\nregression at test time to a globally pooled feature (see Appendix C.3), which provides additional\nlogits that are averaged with the CTX logits. We see non-trivial improvements on DTD by using\nthis, although we sacriﬁce some performance on other datasets, such as Signs and Aircraft. This\nimplies that there’s a fundamental tension between learning categories based on global features, and\ndecomposing the task into local features. More work is needed to better combine these two ideas.\nFinally, Figure 3 depicts the correspondence inferred by the CrossTransformer. The attention is often\nsemantically meaningful: object parts are well matched, including heads, bodies, feet, engines, and\nstrings. The attention is often not one-to-one either: for the ﬂower, the single query ﬂower is matched\nto multiple ﬂowers in some of the support images. Furthermore, the matching works even when\nthe ﬁne-grained classes are not the same, such as the different species of birds, suggesting that the\nattention is indeed a coarse-grained matching that has not overﬁt to the training-set classes.\n5\nConclusion\nWithin a single domain, deep networks have a remarkable ability to compose and reuse features in or-\nder to achieve statistical efﬁciency. However, this work shows the hidden problem with such systems:\nthe networks compose features in a way that conﬂates images which have different appearance but the\nsame label, i.e., it loses information about intra-class variation that may be necessary to understand\nnovel classes. We propose two techniques that help resolve this problem: self-supervised learning,\nwhich prevents features from losing that intra-class variation, and CrossTransformers, which help\nneural networks classify images using local features that are more likely to generalize. However,\nthis problem is far from resolved. In particular, our algorithm provides less beneﬁt when less spatial\nstructure is available, when knowledge of train-time categories can be useful (as in, e.g., COCO), or\nwhen higher-level reasoning is required (e.g., ﬁnding conjunctions of multiple objects). Allowing this\nalgorithm to use spatial structure only where relevant remains an open problem.\n9\n\n\nBroader Impact\nThe algorithm presented in this paper most directly applies to few-shot recognition, which has\nnumerous uses in industry, including vision systems for robotics that must adapt to new objects,\nand photo-organizing software which must infer the presence of new classes of objects on-the-ﬂy.\nUnfamiliar objects are ubiquitous in many real-world vision applications due to the so-called ‘long\ntail’ [94] of objects that occur in real scenes, and therefore we expect our algorithm to improve\nthe robustness of visual recognition systems. While our current work only addresses classiﬁcation,\nmany other tasks in computer vision, such as object detection and segmentation, use neural network\nrepresentations that can likewise be made more robust using the kind of architectures presented here.\nOur algorithm attempts to build representations which factorize the object recognition problem\ninto sub-problems (feature correspondence and feature comparison) that will each transfer correctly\nto new datasets. We hope that further research in this direction may help address dataset biases,\nincluding biases regarding race, gender, or other attributes [97], by helping to disentangle the truly\nmeaningful traits from the spurious correlations. Finally, while this algorithm presents an advance\nto state-of-the-art in understanding rare objects, the general performance of such systems is still far\nbelow human performance. For safety-critical applications (e.g., surgery or self-driving cars), relying\non the ability of vision systems to correctly interpret unusual situations is risky with current systems,\neven with the advances presented here.\nFunding Disclosure\nThis work was funded by DeepMind.\nAcknowledgments\nThe authors would like to thank Pascal Lamblin for help with Meta-Dataset, Olivier Hénaff for\nhelp with SimCLR, Yonglong Tian for help in reproducing baselines, and Relja Arandjelovi´c for\ninvaluable advice on the paper. They are also grateful to Jean-Baptiste Alayrac, Joao Carreira,\nMateusz Malinowski, Viorica P˘atr˘aucean, Adria Recasens, and Lucas Smaira for helpful discussions,\nsupport, and feedback on the project.\nReferences\n[1] M. Andrychowicz, M. Denil, S. Gomez, M. W. Hoffman, D. Pfau, T. Schaul, B. Shillingford, and\nN. De Freitas. Learning to learn by gradient descent by gradient descent. In NeurIPS, 2016.\n[2] P. Bachman, R. D. Hjelm, and W. Buchwalter. Learning representations by maximizing mutual information\nacross views. In NeurIPS, 2019.\n[3] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate.\nIn Proc. ICLR, 2015.\n[4] E. Bart and S. Ullman. Cross-generalization: Learning novel classes from a single example by feature\nreplacement. In Proc. CVPR, 2005.\n[5] S. Bengio, Y. Bengio, J. Cloutier, and J. Gecsei. On the optimization of a synaptic learning rule. In\nPreprints Conf. Optimality in Artiﬁcial and Biological Neural Networks, volume 2. Univ. of Texas, 1992.\n[6] Y. Bengio, S. Bengio, and J. Cloutier. Learning a synaptic learning rule. Citeseer, 1990.\n[7] L. Bertinetto, J. F. Henriques, P. H. Torr, and A. Vedaldi. Meta-learning with differentiable closed-form\nsolvers. In Proc. ICLR, 2019.\n[8] L. Bertinetto, J. F. Henriques, J. Valmadre, P. Torr, and A. Vedaldi. Learning feed-forward one-shot\nlearners. In NeurIPS, 2016.\n[9] L. Bourdev and J. Malik. Poselets: Body part detectors trained using 3d human pose annotations. In Proc.\nICCV, 2009.\n[10] J. Bromley, I. Guyon, Y. LeCun, E. Säckinger, and R. Shah. Signature veriﬁcation using a\" siamese\" time\ndelay neural network. In NeurIPS, 1994.\n[11] A. M. Bronstein, M. M. Bronstein, A. M. Bruckstein, and R. Kimmel. Partial similarity of objects, or\nhow to compare a centaur to a horse. Proc. ICCV, 2009.\n[12] B. Cao, A. Araujo, and J. Sim. Unifying deep local and global features for image search. In Proc. ECCV,\n2020.\n[13] M. Caron, P. Bojanowski, A. Joulin, and M. Douze. Deep clustering for unsupervised learning of visual\nfeatures. In Proc. ECCV, 2018.\n[14] K. Chatﬁeld, K. Simonyan, and A. Zisserman. Efﬁcient on-the-ﬂy category retrieval using convnets and\ngpus. In Asian Conference on Computer Vision, 2014.\n10\n\n\n[15] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning of visual\nrepresentations. In Proc. ICML, 2020.\n[16] Y. Chen, X. Wang, Z. Liu, H. Xu, and T. Darrell. A new meta-baseline for few-shot learning. arXiv\npreprint arXiv:2003.04390, 2020.\n[17] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity metric discriminatively, with application to\nface veriﬁcation. In Proc. CVPR, 2005.\n[18] J. Cortés. Finite-time convergent gradient ﬂows with applications to network consensus. Automatica,\n42(11), 2006.\n[19] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le. Autoaugment: Learning augmentation\nstrategies from data. In Proc. CVPR, 2019.\n[20] G. S. Dhillon et al. A baseline for few-shot image classiﬁcation. Proc. ICLR, 2020.\n[21] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised visual representation learning by context prediction.\nIn Proc. ICCV, 2015.\n[22] C. Doersch, S. Singh, A. Gupta, J. Sivic, and A. A. Efros. What makes paris look like paris? Proc. ACM\nSIGGRAPH, 31(4), 2012.\n[23] C. Doersch and A. Zisserman. Multi-task self-supervised visual learning. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pages 2051–2060, 2017.\n[24] A. Dosovitskiy, J. T. Springenberg, M. Riedmiller, and T. Brox. Discriminative unsupervised feature\nlearning with convolutional neural networks. In NeurIPS. 2014.\n[25] L. Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object categories. IEEE PAMI, 2006.\n[26] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively\ntrained part-based models. IEEE PAMI, 32(9), 2009.\n[27] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In\nProc. ICML, 2017.\n[28] V. Garcia and J. Bruna. Few-shot learning with graph neural networks. In Proc. ICLR, 2018.\n[29] S. Gidaris, A. Bursuc, N. Komodakis, P. Perez, and M. Cord. Boosting few-shot visual learning with\nself-supervision. In The IEEE International Conference on Computer Vision (ICCV), October 2019.\n[30] S. Gidaris, P. Singh, and N. Komodakis. Unsupervised representation learning by predicting image\nrotations. In Proc. ICLR, 2018.\n[31] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection\nand semantic segmentation. In Proc. CVPR, 2014.\n[32] A. Giusti, D. C. Cire¸san, J. Masci, L. M. Gambardella, and J. Schmidhuber. Fast image scanning with\ndeep max-pooling convolutional neural networks. In Intl. Conf. Image Proc., 2013.\n[33] D. Ha, A. Dai, and Q. V. Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016.\n[34] X. Han, T. Leung, Y. Jia, R. Sukthankar, and A. C. Berg. Matchnet: Unifying feature and metric learning\nfor patch-based matching. In Proc. CVPR, 2015.\n[35] B. Hariharan and R. Girshick. Low-shot visual recognition by shrinking and hallucinating features. In\nProc. CVPR, 2017.\n[36] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation\nlearning. In Proc. CVPR, 2020.\n[37] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proc. CVPR, 2016.\n[38] S. Hochreiter, A. S. Younger, and P. R. Conwell. Learning to learn using gradient descent. In International\nConference on Artiﬁcial Neural Networks. Springer, 2001.\n[39] M. Holschneider, R. Kronland-Martinet, J. Morlet, and P. Tchamitchian. A real-time algorithm for signal\nanalysis with the help of the wavelet transform. In Wavelets, pages 286–297. 1990.\n[40] D. Jacobs, D. Weinshall, and Y. Gdalyahu. Class representation and image retrieval with non-metric\ndistances. IEEE PAMI, 22(6):583–600, 2000.\n[41] J. Y. Jason, A. W. Harley, and K. G. Derpanis. Back to basics: Unsupervised learning of optical ﬂow via\nbrightness constancy and motion smoothness. In Proc. CVPR, 2016.\n[42] M. Juneja, A. Vedaldi, C. Jawahar, and A. Zisserman. Blocks that shout: Distinctive parts for scene\nclassiﬁcation. In Proc. CVPR, 2013.\n[43] Ł. Kaiser, O. Nachum, A. Roy, and S. Bengio. Learning to remember rare events. In Proc. ICLR, 2017.\n[44] G. Koch, R. Zemel, and R. Salakhutdinov. Siamese neural networks for one-shot image recognition. In\nICML deep learning workshop, volume 2. Lille, 2015.\n[45] A. Kolesnikov, X. Zhai, and L. Beyer. Revisiting self-supervised visual representation learning. In Proc.\nCVPR, 2019.\n[46] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level concept learning through probabilistic\nprogram induction. Science, 350, 2015.\n[47] G. Larsson, M. Maire, and G. Shakhnarovich. Learning representations for automatic colorization. In\nProc. ECCV, 2016.\n[48] H. Li, G. Hua, Z. Lin, J. Brandt, and J. Yang. Probabilistic elastic matching for pose variant face\nveriﬁcation. In Proc. CVPR, 2013.\n[49] Y. Lifchitz, Y. Avrithis, S. Picard, and A. Bursuc. Dense classiﬁcation and implanting for few-shot\nlearning. In Proc. CVPR, 2019.\n[50] P. Liu, M. Lyu, I. King, and J. Xu. Selﬂow: Self-supervised learning of optical ﬂow. In Proc. CVPR,\n2019.\n[51] D. Maclaurin, D. Duvenaud, and R. Adams. Gradient-based hyperparameter optimization through\nreversible learning. In International Conference on Machine Learning, 2015.\n11\n\n\n[52] S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi. Fine-grained visual classiﬁcation of aircraft.\narXiv preprint arXiv:1306.5151, 2013.\n[53] E. G. Miller, N. E. Matsakis, and P. A. Viola. Learning from one example through shared densities on\ntransforms. In Proc. CVPR, 2000.\n[54] N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel. A simple neural attentive meta-learner. In Proc.\nICLR, 2017.\n[55] I. Misra and L. v. d. Maaten. Self-supervised learning of pretext-invariant representations. In Proc. CVPR,\n2020.\n[56] T. Munkhdalai and H. Yu. Meta networks. In Proc. ICML, 2017.\n[57] D. K. Naik and R. J. Mammone. Meta-neural networks that learn by learning. In [Proceedings 1992]\nIJCNN International Joint Conference on Neural Networks, volume 1. IEEE, 1992.\n[58] Y. E. Nesterov. Minimization methods for nonsmooth convex and quasiconvex functions. Matekon, 29,\n1984.\n[59] J. Nichol, Alex any Andrychowicz ed Achiam and J. Schulman. On ﬁrst-order meta-learning algorithms.\narXiv preprint arXiv:1803.02999, 2018.\n[60] M. Noroozi and P. Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In\nProc. ECCV, 2016.\n[61] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning and transferring mid-level image representations\nusing convolutional neural networks. In Proc. CVPR, 2014.\n[62] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville. Film: Visual reasoning with a general\nconditioning layer. In Proc. AAAI, 2018.\n[63] S. Ravi and H. Larochelle. Optimization as a model for few-shot learning. In Proc. ICLR, 2017.\n[64] S.-A. Rebufﬁ, H. Bilen, and A. Vedaldi. Learning multiple visual domains with residual adapters. In\nNeurIPS, 2017.\n[65] J. Requeima, J. Gordon, J. Bronskill, S. Nowozin, and R. E. Turner. Fast and ﬂexible multi-task\nclassiﬁcation using conditional neural adaptive processes. In NeurIPS, 2019.\n[66] I. Rocco, M. Cimpoi, R. Arandjelovi´c, A. Torii, T. Pajdla, and J. Sivic. Neighbourhood consensus\nnetworks. In NeurIPS, pages 1651–1662, 2018.\n[67] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla,\nM. Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 115(3), 2015.\n[68] T. Saikia, T. Brox, and C. Schmid. Optimized generic feature learning for few-shot classiﬁcation across\ndomains. arXiv preprint arXiv:2001.07926, 2020.\n[69] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and T. Lillicrap. Meta-learning with memory-\naugmented neural networks. In Proc. ICML, 2016.\n[70] S. Savarese, J. Winn, and A. Criminisi. Discriminative object class models of appearance and shape by\ncorrelatons. In Proc. CVPR, 2006.\n[71] J. Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: the\nmeta-meta-... hook. PhD thesis, Technische Universität München, 1987.\n[72] J. Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks.\nNeural Computation, 4(1):131–139, 1992.\n[73] J. Schmidhuber. A neural network that embeds its own meta-levels. In IEEE International Conference on\nNeural Networks, pages 407–412. IEEE, 1993.\n[74] J. Sivic and A. Zisserman. Video Google: A text retrieval approach to object matching in videos. In Proc.\nICCV, 2003.\n[75] J. Snell, K. Swersky, and R. Zemel. Prototypical networks for few-shot learning. In NeurIPS, 2017.\n[76] P. Sprechmann, S. M. Jayakumar, J. W. Rae, A. Pritzel, A. P. Badia, B. Uria, O. Vinyals, D. Hassabis,\nR. Pascanu, and C. Blundell. Memory-based parameter adaptation. In Proc. ICLR, 2018.\n[77] J.-C. Su, S. Maji, and B. Hariharan. When does self-supervision improve few-shot learning? In Proc.\nECCV, 2020.\n[78] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and T. M. Hospedales. Learning to compare: Relation\nnetwork for few-shot learning. In Proc. CVPR, 2018.\n[79] S. Thrun. Lifelong learning algorithms. In Learning to learn. Springer, 1998.\n[80] S. Thrun and L. Pratt. Learning to learn. Springer Science & Business Media, 1998.\n[81] Y. Tian, D. Krishnan, and P. Isola. Contrastive multiview coding. arXiv preprint arXiv:1906.05849, 2019.\n[82] Y. Tian, Y. Wang, D. Krishnan, J. B. Tenenbaum, and P. Isola. Rethinking few-shot image classiﬁcation:\na good embedding is all you need? In Proc. ECCV, 2020.\n[83] P. Tokmakov, Y.-X. Wang, and M. Hebert. Learning compositional representations for few-shot recogni-\ntion. In Proc. ICCV, 2019.\n[84] G. Tolias, T. Jenicek, and O. Chum. Learning and aggregating deep local descriptors for instance-level\nrecognition. In Proc. ECCV, 2020.\n[85] E. Triantaﬁllou, T. Zhu, V. Dumoulin, P. Lamblin, U. Evci, K. Xu, R. Goroshin, C. Gelada, K. J. Swersky,\nP.-A. Manzagol, and H. Larochelle. Meta-dataset: A dataset of datasets for learning to learn from few\nexamples. In Proc. ICLR, 2020.\n[86] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior,\nand K. Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499,\n2016.\n[87] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.\nAttention is all you need. In NeurIPS, 2017.\n12\n\n\n[88] R. C. Veltkamp. Shape matching: Similarity measures and algorithms. In International Conference on\nShape Modeling and Applications, 2001.\n[89] R. Vilalta and Y. Drissi. A perspective view and survey of meta-learning. Artiﬁcial intelligence review,\n18, 2002.\n[90] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and D. Wierstra. Matching networks for one shot\nlearning. In NeurIPS, 2016.\n[91] C. Vondrick, A. Shrivastava, A. Fathi, S. Guadarrama, and K. Murphy. Tracking emerges by colorizing\nvideos. In Proc. ECCV, 2018.\n[92] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 dataset.\n2011.\n[93] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural networks. In Proc. CVPR, 2018.\n[94] Y.-X. Wang, D. Ramanan, and M. Hebert. Learning to model the tail. In NeurIPS, 2017.\n[95] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin. Unsupervised feature learning via non-parametric instance\ndiscrimination. In Proc. CVPR, 2018.\n[96] W. Xie, L. Shen, and A. Zisserman. Comparator networks. In Proc. ECCV, 2018.\n[97] K. Yang, K. Qinami, L. Fei-Fei, J. Deng, and O. Russakovsky. Towards fairer datasets: Filtering and\nbalancing the distribution of the people subtree in the imagenet hierarchy. In Conference on Fairness,\nAccountability, and Transparency, 2020.\n[98] A. S. Younger, S. Hochreiter, and P. R. Conwell. Meta-learning with backpropagation. In IJCNN’01.\nInternational Joint Conference on Neural Networks. Proceedings (Cat. No. 01CH37222), volume 3. IEEE,\n2001.\n[99] S. Zagoruyko and N. Komodakis. Wide residual networks. In Proc. BMVC., 2016.\n[100] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena. Self-attention generative adversarial networks. In\nProc. ICML, 2019.\n[101] J. Zhang, M. Marszałek, S. Lazebnik, and C. Schmid. Local features and kernels for classiﬁcation of\ntexture and object categories: A comprehensive study. IJCV, 73(2), 2007.\n[102] R. Zhang, P. Isola, and A. A. Efros. Colorful image colorization. In Proc. ECCV. Springer, 2016.\n[103] R. Zhang, P. Isola, and A. A. Efros. Split-brain autoencoders: Unsupervised learning by cross-channel\nprediction. In Proc. CVPR, 2017.\n[104] H. Zheng, J. Fu, T. Mei, and J. Luo. Learning multi-attention convolutional neural network for ﬁne-grained\nimage recognition. In Proc. ICCV, 2017.\n13\n\n\nA\nOut-of-domain vs. within-domain classiﬁcation implementation details\nIn the introduction, we state that algorithms like Prototypical Nets [75] achieve 50% accuracy on\nImageNet held-out categories (out-of-domain), versus 84% accuracy for a similar challenge given\nsupervised training data (within-domain).\nTo arrive at these numbers, we used the Meta-Dataset [85] validation classes. For Prototypical\nNets out-of-domain classiﬁcation, we ﬁnd that 50% is an upper bound: the performance from our\nreimplementation of Prototypical Nets (using 224×224 inputs, ResNet 34, and Normalized SGD)\non this set is actually 46.4%. For the supervised within-domain classiﬁcation baseline we trained\na ResNet-34 on the full ImageNet train set (all classes) with 224×224 images, using modern best\npractices for training, and found a top-1 performance of 73.0% on ImageNet’s standard val set\n(i.e., held-out images, but not held-out categories). We then applied this network to ﬁne-grained\nclassiﬁcation by constructing query sets, using images from the standard ImageNet val set, but\nfollowed the class distribution in Meta-Dataset’s query sets for the Meta-Dataset ImageNet val set.\nWe then classiﬁed all these images with the network as following: we discarded all logits except\nthose categories that are present in the query set, in order to ensure that chance performance is the\nsame for the Prototypical Nets and this baseline classiﬁer. The result was 84.2%.\nB\nSupervision Collapse: nearest neighbor experiments\nComputing nearest neighbors for Prototypical Net representations proved challenging due to another,\nentirely different source of supervision collapse: the default implementation of Prototypical Nets\nactually only produces representations that are comparable within a single episode. This is likely\nbecause baseline Prototypical Nets are trained only on episodes: that is, the network only sees\nﬁne-grained classiﬁcation problems (e.g., classifying insects versus other insects), rather than coarse-\ngrained episodes (e.g., insects versus cars). Therefore, nothing encourages the network to have having\ndistinct, non-overlapping representations of widely different categories (e.g., beetle the insect may\nhave the same representation as beetle the car, without affecting the training loss). Worse, Batch\nNorm allows communication within a support set, and so the ﬁnal representation of each image\ncontains not only information about the image, but also about how it contrasts with other images in\nthe support set.\nAs a result, even ImageNet images from the Meta-Dataset training set, grouped randomly into\nepisodes and fed through prototypical nets, have virtually meaningless representations. In the ﬁle\nbatch_norm_train_nearest_neigbors. html in the paper supplement,2 we show nearest neighbors\nretrieved in this way. The nearest-neighbor retrieval set includes 10% of images from both the\nImageNet train and test sets (Meta-Dataset’s split; speciﬁcally, 130 images per class). These are\npassed in batches of size 256 (Batch Norm is set to train mode) to obtain a feature vector for each\nimage. We use a ResNet-34 Prototypical Net with 224×224 images, trained with normalized SGD.\nFor each row in the HTML ﬁle, we show the query image (left), along with the top 9 nearest neighbors,\nusing Euclidean distance. For batch_norm_train_nearest_neigbors. html, the results are close to\nrandom, even though all query images are taken from the training set. This is not particularly useful\nfor analysis.\nTo ﬁx this problem, we make two modiﬁcations to Prototypical Net training. First, rather than\ntrain only on ﬁne-grained episodes, we train on episodes that contain classes sampled uniformly\nat random from the full ImageNet training set. This means that a single episode can now contain\nboth cars and insects. We also replace Batch Norm with Layer Norm, ensuring that there can no\nlonger be communication within the batch. Results for queries from the training classes are shown\nin layer_norm_train_nearest_neigbors. html. We can see a substantial improvement in the quality\nof the matches, as would be expected for the retrievals using a representation trained with standard\nImageNet classiﬁcation.\nFinally, in layer_norm_test_nearest_neigbors. html, we show the results of using the same retrieval\nprocedure, but using images from test-set classes as queries. We can see that, due to supervision\ncollapse, the nearest neighbors have returned to being quite poor.\n2Supplementary material is available on the NeurIPS 2020 webpage for this paper.\n14\n\n\nSame class as query\nAny train class\nMost frequent train class\nFigure 4: Nearest neighbors statistics. We sample 1000 random queries from the Meta-Dataset test\nset, and ﬁnd the top 9 nearest neighbors in Prototypical Net embedding space, in both training and\ntest sets. We show histograms of nearest neighbors: the x-axis is a count of the number of retrievals\nfor a single query that were of some type, and the bar height is the number of queries for which the\ncount was equal to that x-axis value. Left: the number of nearest neighbors that came from the same\n(test) class as the query. Center: the number of nearest neighbors that came from the train set. Right:\nthe number of retrievals that come from the same train class, for the most frequently-retrieved such\nclass. Note that the 0’th bin of this plot indicates that all retrievals were from the test set.\nB.1\nSupervision collapse: quantitative analysis\nIn Figure 4, we show statistics for a larger set of query images from the test set (1000 queries), which\nunderscore how poor the results are. In Figure 4 left, we see that over 60% of queries had 0 nearest\nneighbors of the correct category, even though 130 images of the correct category are guaranteed\nto exist in the retrieval set. Furthermore, Figure 4 center shows a large proportion of matches for\ntest set images are from the training set. There are 712 training categories and 130 test categories;\ntherefore, at random chance, we would expect 712/(130 + 712) = 84.6% to come from the training\nset (and note that all test-set images are devices, while there are no devices in the train set). Retrievals\nfrom the test set happen more often than chance, but a large fraction do not, and roughly 6% have all\nretrievals from the training set. Finally, in Figure 4 right, we see that often, the nearest neighbors\nfrom the train set are far from random. Having two or more matches from the the same training\nset class is quite common (more than 55.3% of the queries), even more frequent than having even\none match that’s from the correct val set class (34.1% of the queries). Many examples have far\nmore than two matches from the same training set class. In one case, all 9 retrievals were from the\nsame (incorrect) training category. The statistics reafﬁrm our intuition that individual Prototypical\nNet embeddings for held-out images are not likely to capture the correct semantics; instead, the\nembedding overemphasizes features it has in common with one particular category, which skews its\nnotion of similarity.\nWe repeated this experiment after training Prototypical Nets with 50% SimCLR episodes and found\nimprovements: only 43.3% of queries now have 2 or more neighbors from the same train set category,\nand 48.8% have at least one neighbor from the correct class. This suggests that SimCLR episodes\nare effective at reducing supervision collapse, but the problem is far from solved with this technique\nalone.\nC\nCrossTransformer implementation details\nC.1\nTraining\nOur experiments with CrossTransformers use no pretraining, although we use it for the experiments\ninvolving Prototypical Nets to be consistent with prior work [85], which has shown that pretraining\ngives a boost for Prototypical Net models. Speciﬁcally, for Prototypical Nets, the representation\nis pretrained for direct classiﬁcation on the training set, i.e., the network predicts a ﬁxed number\nof logits from batches of images sampled uniformly from the training categories. This is trained\nwith early stopping, where the stopping criterion involves training a linear classiﬁer on validation\ncategories and stopping when this ﬁne-tuning performance begins to decline. Only then is the network\nre-architectured into a Prototypical Net, where it is trained on episodes with support sets and query\nsets with relatively few categories.\n15\n\n\nTable 3: CrossTransformer comparison of feature map spatial resolution. We see that increasing\nthe spatial resolution from 7 (CTX7) to 14 (CTX14) via dilated convolution typically gives a small\nperformance boost, and almost never harms performance.\nImNet Omni Plane\nBird DTD QDraw Fungi Flower\nSign COCO Rank\nCTX7\n59.73 74.11 70.90 80.29 73.91\n65.61 48.53\n91.98 68.81\n50.62\n5.35\nCTX14\n61.94 76.52 79.65 84.06 76.26\n65.67 52.53\n94.11 70.47\n53.51\n3.25\nCTX7+SimCLR Eps\n60.69 79.22 76.64 77.86 77.31\n67.43 43.68\n93.30 69.56\n52.35\n4.10\nCTX14+SimCLR Eps\n63.79 80.83 82.05 82.01 75.76\n68.84 52.01\n94.62 75.01\n52.76\n2.50\nCTX7+SimCLR Eps+Aug\n60.76 87.26 77.56 68.31 71.44\n72.62 44.12\n92.45 81.20\n54.66\n3.85\nCTX+SimCLR Eps+Aug\n62.76 82.21 79.49 80.63 75.57\n72.68 51.58\n95.34 82.65\n59.90\n1.95\nFor other aspects of training, we follow prior work [85] where possible, including sampling episodes\nin the same way, and training the full network using ADAM (applied after normalizing the gradients\nin the case of Normalized SGD). We train until convergence, and select the best checkpoint using\nthe error on the validation set. For the hyperparameters chosen via hyperparameter sweep in the\noriginal paper, we use the best values for Prototypical Nets, which are a weight decay of 8.86e −5,\nand decaying the learning rate by a factor of 0.915 after a ﬁxed episode interval. The exception is\nthe learning rate, where we ﬁnd that 1.21e −3 is too high initially, and learning for CTX doesn’t\ntake off until it has decayed to half its original value (though this doesn’t affect ﬁnal performance).\nTherefore, we use an initial learning rate of 6e −4 for all CTX experiments. For Prototypical Nets,\nthis interval is 500 episodes, but we use a longer interval for CrossTransformers, as they train from\nscratch. For CrossTransformers alone this interval is 2000; we increase this interval by a factor of two\nwhen adding SimCLR Episodes, and another factor of two when adding BOHB-style augmentation,\nas both of these additions make learning more difﬁcult.\nTherefore, the main departures from prior work [85] are that 1) we use ResNet-34, 2) we feed\nimages at a higher resolution (224×224), 3) we use normalized gradient descent, 4) we use 50%\nepisodes where the categories are selected uniformly at random from ImageNet, 5) we use Batch\nNorm statistics in test mode at test time (i.e. exponential moving averages computed during training,\ndecaying at a rate of .9 per episode), and 6) we use no pretraining.\nC.2\nCrossTransformers architecture\nThe output of our ResNet-34 with dilated ﬁnal block has 512 channels and a 14×14 grid. We compute\nkey and value heads with 128 dimensions each, with no non-linearities and no bias. We ﬁnd that\nthe attention maps are rather memory-intensive (they contain all pairs of spatial positions between\nquery and support set). Therefore, we distributed the model across 8 NVIDIA V-100 GPUs, and use\ngradient rematerialization for the CrossTransformer attention maps. Training to convergence requires\nroughly 7 days for our most complex model with SimCLR episodes and BOHB-style augmentations\nenabled.\nTo demonstrate the importance of high-resolution feature maps, Table 3 shows the performance\nof a few versions of CrossTransformer without dilation, which results in stride-32 network with a\n7×7 output grid, like the standard ResNet implementation. We see that higher resolution almost\nalways gives a small boost. The boosts are largest on datasets with non-trivial spatial structure where\nthe distinguishing features may be small, e.g., Aircraft, Birds, and Fungi. On the other hand, the\nincreased resolution makes little difference for DTD textures and QuickDraw, and the the lower\nresolution actually performs best on OmniGlot. Textures lack spatial structure, and OmniGlot and\nQuickdraw contain low-resolution images with few identifying features: therefore, it’s unsurprising\nthat the extra resolution isn’t useful. One possible interpretation is that the network may subdivide\nthe scene into more parts than are justiﬁed, which can degrade performance when correspondences\nare wrong, suggesting that an adaptive mechanism for choosing the resolution might be useful.\nC.3\nAugmenting CTX with a global feature\nConcurrent work [82] showed that applying logistic regression to a globally-pooled feature at test\ntime can improve results. Here, we modify CTX to use the same ideas. We ﬁrst globally pool the\nfeature Φ(x) spatially, which results in a ﬂat 512-dimension vector for each image in both the query\n16\n\n\nand test set. We then train a simple Logistic Regression classiﬁer using the same parameters from [82]\n(sklearn’s implementation with a multinomial loss C=10, applied to ℓ2-normalized features). Running\nthe classiﬁer on the query images produces another set of logits, which we ﬁnd are scaled smaller than\nCTX logits. Therefore, we produce ﬁnal classiﬁer logits via argmax(CTX(S, xq) + λ LR(S, xq)),\nwhere CTX(S, xq) is the logits produced by CTX for query xq and support set S, LR(S, xq) are the\nlogits from the logistic regression classiﬁer, and λ is a scalar constant that we set to 5.\nWe ﬁnd that this provides no beneﬁt if the embedding network is trained purely as a CrossTransformer.\nHowever, we ﬁnd beneﬁts on some datasets (notably DTD) if we add an auxiliary loss that matches\nthe loss used in concurrent work [82]. That is, we compute Φ(x) for each image x in the support set,\nand globally pool this feature. We then apply a ﬁxed classiﬁer on top of this feature, which performs\nthe 712-way classiﬁcation for the 712 ImageNet-train categories. We add this classiﬁcation loss to\nthe CTX loss (without weight) at training time.\nC.4\nAugmentation\nFor most experiments in the paper, we use no augmentation for images that aren’t a part of SimCLR\nEpisodes, following prior work [85]. However, BOHB [68] studies augmentation extensively,\nand so we adopt similar augmentation for some experiments. BOHB optimizes parameters for\naugmentation in a similar style to AutoAugment [19], using 2 randomly-selected stages, where each\nstage may consist of rotation, posterizing, solarizing, color shifts, contrast, brightness, sharpness,\nshear, translation, and cutout, with settings discovered via validation on Meta-Dataset’s ImageNet\nval split. We use these only for experiments labeled as “+Aug” in Table 2 and Table 3. We found\nqualitatively, however, that even with these changes, the network could still be quite sensitive to\ninput images which are resized from very small images, especially when the input resolution is high\n(224×224 in our case). Therefore, we add one more stage that the augmentation function can select,\nwhich randomly resizes the image by a ratio sampled uniformly from 1 all the way down to a ratio that\nwould produce a 10-pixel-wide image. Then we compress the image with jpeg, with a compression\nquality uniformly sampled between 75 and 100, before decompressing and resizing to the original\nresolution. These parameters were chosen once, and we ran no hyperparameter sweep to tune them.\nWe expect that properly tuning them on validation data following BOHB [68] would yield further\nimprovements, but we leave this for future work.\nOur implementation of SimCLR follows the public one released by the original authors, using the\nstandard two augmentation ops of random cropping and color jittering with the default parameters.\nAlso following this implementation, we apply random blur to only one image in each pair of positive\nmatches; in our case, we apply the blur to the query image.\nD\nCorrespondence visualization\nFigures 6–15 visualize the attention inferred by CrossTransformers for all the 10 evaluation datasets\nin Meta-Dataset. We show query images for each dataset, along with three support-set images for\neach. Within each query image, we choose three spatial locations (red, green, and blue squares),\nand plot the CrossTransformer attention weights for each one in the corresponding color (higher\nweight means brighter colors). Both 7×7 and 14×14 attention maps corresponding to CTX7 and\nCTX14 models respectively are presented, with the query points selected at approximately the same\nlocation for both models. The inferred correspondences are semantically meaningful, and often not\none-to-one.\nFinally, Figure 5 shows some qualitative examples of a few challenging cases which suggests areas\nfor improvement. In particular, our method sometimes produces conﬁdent correspondences even\nwhen the true correspondence is unclear; in such cases, we might prefer that the method falls back\nto global comparisons. Conversely, the algorithm may not always ﬁnd correspondences when they\nare available, if, for example, there is a large difference in appearance between corresponding points.\nThis suggests that the correspondence itself may be overﬁtting, and suggests a possible avenue for\nfuture research.\n17\n\n\nQuery\nCorrespondence\nin support set\n(1)\n(2)\n(3)\n(4)\n(5)\n(6)\nFigure 5: Failure cases. We visualize the attention correspondences for a few challenging cases for\nour method. In (1) incorrect parts of the swings are matched with high conﬁdence. In the (2) the\nhandle of the ladle is not localized. A shortcoming of our approach is that the model is tasked to\nmatch images which cannot be/are difﬁcult to put into correspondence. For example, in (3) and (4)\nno clear correspondence is feasible, and the model matches parts which are plausibly in the same\ngeometric location. Finally, in (5) and (6), the model localizes the instances correctly, but due to large\nvariance in the instance shapes fails at detailed matching of the sub-parts.\nE\nFive-shot\nWe also report ﬁve-shot results. As the standard Meta-Dataset evaluation speciﬁes a broad range of\npossible shots (e.g., up to 100 examples in extremely rare cases), we believe that 5-shot results can\naid in interpretability. The ‘ways’ are sampled as before (i.e., the standard for Meta-Dataset), but the\nnumber of examples per category is set to exactly ﬁve. This means that both support set and query set\nare class balanced (unlike the standard evaluation, where the query set is balanced but the support set\nis not). We use the same checkpoints that were used above (i.e., no additional validation to choose a\ncheckpoint for ﬁve-shot evaluation). Results are shown in Table 4. We see that the performance is\nsomewhat lower, but the overall trends of performance on the datasets are similar.\nF\nConﬁdence intervals\nTable 5 shows conﬁdence intervals for most experiments in this paper, to enable future comparisons\nlike the tables shown in this work.\n18\n\n\n7×7 attention\n14×14 attention\nQuery\nCorrespondence in support set\nQuery\nCorrespondence in support set\nFigure 6: Aircraft. Various aircraft parts (e.g., wings, head, tail, engine, landing wheels) are matched across instances with\nlarge differences in viewpoint/pose and scale.\n7×7 attention\n14×14 attention\nQuery\nCorrespondence in support set\nQuery\nCorrespondence in support set\nFigure 7: CU-Birds. Beak, body, and feet are matched for different species.\n\n\n7×7 attention\n14×14 attention\nQuery\nCorrespondence in support set\nQuery\nCorrespondence in support set\nFigure 8: Describable Textures (DTD). Textures do not have localized parts/sub-parts, and hence, the correspondence is quite\ndiffuse (e.g., the ﬁrst above). However, speciﬁc features, if present, are matched: in the last four examples, the donut’s sprinkles\nand icing, the net-like pattern, the facial features, and the pumpkin’s stem, surface and cavities, are matched respectively..\n7×7 attention\n14×14 attention\nQuery\nCorrespondence in support set\nQuery\nCorrespondence in support set\nFigure 9: FGVCx Fungi. The caps and stem are matched across variations in pose/rotation, and also number of fungi exhibit\none-to-many matches..\n\n\n7×7 attention\n14×14 attention\nQuery\nCorrespondence in support set\nQuery\nCorrespondence in support set\nFigure 10: VGG Flower. The central disk and petals are matched with all instances of ﬂowers present in the support-set images.\nThe background is diffuse and separated from the central object.\n7×7 attention\n14×14 attention\nQuery\nCorrespondence in support set\nQuery\nCorrespondence in support set\nFigure 11: ImageNet. Correspondence is established in the presence of distractors and large variations in object pose and\nshape.\n\n\n7×7 attention\n14×14 attention\nQuery\nCorrespondence in support set\nQuery\nCorrespondence in support set\nFigure 12: MSCOCO. We observe some detailed matching for different object categories: animal parts (ﬁrst two), giraffe\nossicones (ﬁrst), bus, large variations in the pose of rackets (fourth), and different letters in the trafﬁc sign (last).\n7×7 attention\n14×14 attention\nQuery\nCorrespondence in support set\nQuery\nCorrespondence in support set\nFigure 13: Omniglot. Corresponding parts of various character glyphs are matched.\n\n\n7×7 attention\n14×14 attention\nQuery\nCorrespondence in support set\nQuery\nCorrespondence in support set\nFigure 14: Trafﬁc Signs. Corresponding parts are matched, even when the support-image is ﬂipped horizontally (last).\n7×7 attention\n14×14 attention\nQuery\nCorrespondence in support set\nQuery\nCorrespondence in support set\nFigure 15: Quick Draw. Matches across deformations of doodles are observed.\n\n\nTable 4: Five-Shot. For interpretability, we also compute 5-shot results for all Meta-Dataset datasets, and include (accuracy (%) ± conﬁdence (%)) for the sake of\nfuture comparison for CTX models (with the 14x14 feature grid). These are the standard conﬁdence intervals computed for Meta-Dataset: i.e., the standard error\ncomputed from the episode-to-episode variability in accuracy across 600 test episodes.\nILSVRC\nOmniglot\nAircraft\nBirds\nTextures\nQuick Draw\nFungi\nVGG Flowers\nTrafﬁc\nMSCOCO\nRank\nPrototypical (ours)\n41.87±0.89\n61.33±1.13\n39.40±0.78\n65.57±0.73\n59.06±0.60\n47.86±0.80\n41.64±1.02\n83.88±0.48 44.84±0.88\n41.14±0.82\n4.00\nCTX\n51.70±0.90\n84.24±0.79\n62.29±0.73\n79.38±0.54\n65.86±0.58\n63.36±0.73 49.43±0.98\n92.74±0.29\n68.31±0.71 48.63±0.79\n2.25\nCTX+SimCLR Eps\n51.29±0.89\n86.14±0.74 69.74±0.67\n74.85±0.62\n63.84±0.62\n64.11±0.67\n48.87±0.91\n93.00±0.30\n70.62±0.68 48.45±0.83\n2.10\nCTX+SimCLR Eps+Aug 52.56±0.86\n87.53±0.61 64.28±0.71\n73.27±0.63\n64.72±0.63\n66.90±0.66 48.22±0.94\n93.23±0.28\n78.45±0.60\n56.61±0.78\n1.65\nTable 5: Conﬁdence intervals for quantitative results. We report the conﬁdence intervals in addition to the mean accuracy (accuracy (%) ± conﬁdence (%)) for\nthe models introduced in this work for the sake of future comparison. All versions shown here use 224 resolution, ResNet34, and exponential moving average (EMA)\nfor test-time batch norm (BN), unless otherwise speciﬁed.\nILSVRC\nOmniglot\nAircraft\nBirds\nTextures\nQuick Draw\nFungi\nVGG Flowers\nTrafﬁc\nMSCOCO\nProtoNets (ours)\n51.66±1.10\n57.22±1.34\n51.63±0.93\n71.73±1.02\n69.72±0.76\n53.81±1.04\n42.07±1.14\n87.29±0.69 47.45±0.92 44.38±1.03\nProtoNets (ours)+SimCLR Eps\n49.67±1.06\n65.21±1.23\n54.46±0.91\n60.94±0.94\n63.96±0.77\n50.64±1.05\n37.84±1.06\n88.70±0.60 51.61±1.00 42.97±1.04\nProtoNets (ours)+SimCLR Eps (no BN EMA) 53.69±1.10\n67.44±1.26\n57.10±0.99\n74.07±0.91\n69.46±0.75\n51.83±1.02\n41.67±1.21\n86.93±0.65\n57.41±1.04\n41.43±1.10\nCTX7\n59.73±1.08\n74.11±1.22\n70.90±0.99\n80.29±0.86\n73.91±0.70\n65.61±0.82\n48.53±1.09\n91.98±0.52 68.81±0.99 50.62±1.03\nCTX14\n61.94±1.04\n76.52±1.14\n79.65±0.91\n84.06±0.85\n76.26±0.70\n65.67±0.91\n52.53±1.16\n94.11±0.44 70.47±0.92 53.51±1.06\nCTX7+SimCLR Eps\n60.69±0.99\n79.22±1.16\n76.64±0.88\n77.86±0.92\n77.31±0.66\n67.43±0.88\n47.37±1.15\n93.30±0.43\n69.56±0.99\n52.35±1.01\nCTX14+SimCLR Eps\n63.79±1.00\n80.83±1.07\n82.05±0.83\n82.01±0.89\n75.76±0.76\n68.84±0.88\n52.01±1.13\n94.62±0.43 75.01±0.93 52.76±1.03\nCTX7+SimCLR Eps+Aug\n61.20±1.04\n87.26±0.65\n77.98±0.89\n68.31±0.71\n72.70±0.71\n73.32±0.77\n44.12±0.94\n93.29±0.43 80.03±0.80 57.88±1.04\nCTX14+SimCLR Eps+Aug\n62.76±0.99\n82.21±1.00\n79.49±0.89\n80.63±0.88\n75.57±0.64\n72.68±0.82\n51.58±1.11\n95.34±0.37 82.65±0.76 59.90±1.02\nCTX14+SimCLR Eps+Aug+LR\n62.25±0.96\n82.03±0.98\n77.41±0.84\n76.66±0.87\n80.29±0.72\n72.24±0.81\n49.39±1.17\n93.05±0.50 75.25±0.93 60.35±1.06\n24\n"
}