{
  "filename": "1512.00567v3.pdf",
  "num_pages": 10,
  "pages": [
    "Rethinking the Inception Architecture for Computer Vision\nChristian Szegedy\nGoogle Inc.\nszegedy@google.com\nVincent Vanhoucke\nvanhoucke@google.com\nSergey Ioffe\nsioffe@google.com\nJonathon Shlens\nshlens@google.com\nZbigniew Wojna\nUniversity College London\nzbigniewwojna@gmail.com\nAbstract\nConvolutional networks are at the core of most state-\nof-the-art computer vision solutions for a wide variety of\ntasks. Since 2014 very deep convolutional networks started\nto become mainstream, yielding substantial gains in vari-\nous benchmarks. Although increased model size and com-\nputational cost tend to translate to immediate quality gains\nfor most tasks (as long as enough labeled data is provided\nfor training), computational efﬁciency and low parameter\ncount are still enabling factors for various use cases such as\nmobile vision and big-data scenarios. Here we are explor-\ning ways to scale up networks in ways that aim at utilizing\nthe added computation as efﬁciently as possible by suitably\nfactorized convolutions and aggressive regularization. We\nbenchmark our methods on the ILSVRC 2012 classiﬁcation\nchallenge validation set demonstrate substantial gains over\nthe state of the art: 21.2% top-1 and 5.6% top-5 error for\nsingle frame evaluation using a network with a computa-\ntional cost of 5 billion multiply-adds per inference and with\nusing less than 25 million parameters. With an ensemble of\n4 models and multi-crop evaluation, we report 3.5% top-5\nerror and 17.3% top-1 error.\n1. Introduction\nSince the 2012 ImageNet competition [16] winning en-\ntry by Krizhevsky et al [9], their network “AlexNet” has\nbeen successfully applied to a larger variety of computer\nvision tasks, for example to object-detection [5], segmen-\ntation [12], human pose estimation [22], video classiﬁca-\ntion [8], object tracking [23], and superresolution [3].\nThese successes spurred a new line of research that fo-\ncused on ﬁnding higher performing convolutional neural\nnetworks. Starting in 2014, the quality of network architec-\ntures signiﬁcantly improved by utilizing deeper and wider\nnetworks. VGGNet [18] and GoogLeNet [20] yielded simi-\nlarly high performance in the 2014 ILSVRC [16] classiﬁca-\ntion challenge. One interesting observation was that gains\nin the classiﬁcation performance tend to transfer to signiﬁ-\ncant quality gains in a wide variety of application domains.\nThis means that architectural improvements in deep con-\nvolutional architecture can be utilized for improving perfor-\nmance for most other computer vision tasks that are increas-\ningly reliant on high quality, learned visual features. Also,\nimprovements in the network quality resulted in new appli-\ncation domains for convolutional networks in cases where\nAlexNet features could not compete with hand engineered,\ncrafted solutions, e.g. proposal generation in detection[4].\nAlthough VGGNet [18] has the compelling feature of\narchitectural simplicity, this comes at a high cost: evalu-\nating the network requires a lot of computation. On the\nother hand, the Inception architecture of GoogLeNet [20]\nwas also designed to perform well even under strict con-\nstraints on memory and computational budget. For exam-\nple, GoogleNet employed only 5 million parameters, which\nrepresented a 12× reduction with respect to its predeces-\nsor AlexNet, which used 60 million parameters. Further-\nmore, VGGNet employed about 3x more parameters than\nAlexNet.\nThe computational cost of Inception is also much lower\nthan VGGNet or its higher performing successors [6]. This\nhas made it feasible to utilize Inception networks in big-data\nscenarios[17], [13], where huge amount of data needed to\nbe processed at reasonable cost or scenarios where memory\nor computational capacity is inherently limited, for example\nin mobile vision settings. It is certainly possible to mitigate\nparts of these issues by applying specialized solutions to tar-\nget memory use [2], [15] or by optimizing the execution of\ncertain operations via computational tricks [10]. However,\nthese methods add extra complexity. Furthermore, these\nmethods could be applied to optimize the Inception archi-\ntecture as well, widening the efﬁciency gap again.\nStill, the complexity of the Inception architecture makes\n1\narXiv:1512.00567v3  [cs.CV]  11 Dec 2015\n",
    "it more difﬁcult to make changes to the network. If the ar-\nchitecture is scaled up naively, large parts of the computa-\ntional gains can be immediately lost. Also, [20] does not\nprovide a clear description about the contributing factors\nthat lead to the various design decisions of the GoogLeNet\narchitecture. This makes it much harder to adapt it to new\nuse-cases while maintaining its efﬁciency.\nFor example,\nif it is deemed necessary to increase the capacity of some\nInception-style model, the simple transformation of just\ndoubling the number of all ﬁlter bank sizes will lead to a\n4x increase in both computational cost and number of pa-\nrameters. This might prove prohibitive or unreasonable in a\nlot of practical scenarios, especially if the associated gains\nare modest. In this paper, we start with describing a few\ngeneral principles and optimization ideas that that proved\nto be useful for scaling up convolution networks in efﬁcient\nways. Although our principles are not limited to Inception-\ntype networks, they are easier to observe in that context as\nthe generic structure of the Inception style building blocks\nis ﬂexible enough to incorporate those constraints naturally.\nThis is enabled by the generous use of dimensional reduc-\ntion and parallel structures of the Inception modules which\nallows for mitigating the impact of structural changes on\nnearby components. Still, one needs to be cautious about\ndoing so, as some guiding principles should be observed to\nmaintain high quality of the models.\n2. General Design Principles\nHere we will describe a few design principles based\non large-scale experimentation with various architectural\nchoices with convolutional networks. At this point, the util-\nity of the principles below are speculative and additional\nfuture experimental evidence will be necessary to assess\ntheir accuracy and domain of validity. Still, grave devia-\ntions from these principles tended to result in deterioration\nin the quality of the networks and ﬁxing situations where\nthose deviations were detected resulted in improved archi-\ntectures in general.\n1. Avoid representational bottlenecks, especially early in\nthe network.\nFeed-forward networks can be repre-\nsented by an acyclic graph from the input layer(s) to\nthe classiﬁer or regressor. This deﬁnes a clear direction\nfor the information ﬂow. For any cut separating the in-\nputs from the outputs, one can access the amount of\ninformation passing though the cut. One should avoid\nbottlenecks with extreme compression. In general the\nrepresentation size should gently decrease from the in-\nputs to the outputs before reaching the ﬁnal represen-\ntation used for the task at hand. Theoretically, infor-\nmation content can not be assessed merely by the di-\nmensionality of the representation as it discards impor-\ntant factors like correlation structure; the dimensional-\nity merely provides a rough estimate of information\ncontent.\n2. Higher dimensional representations are easier to pro-\ncess locally within a network. Increasing the activa-\ntions per tile in a convolutional network allows for\nmore disentangled features.\nThe resulting networks\nwill train faster.\n3. Spatial aggregation can be done over lower dimen-\nsional embeddings without much or any loss in rep-\nresentational power. For example, before performing a\nmore spread out (e.g. 3 × 3) convolution, one can re-\nduce the dimension of the input representation before\nthe spatial aggregation without expecting serious ad-\nverse effects. We hypothesize that the reason for that\nis the strong correlation between adjacent unit results\nin much less loss of information during dimension re-\nduction, if the outputs are used in a spatial aggrega-\ntion context. Given that these signals should be easily\ncompressible, the dimension reduction even promotes\nfaster learning.\n4. Balance the width and depth of the network. Optimal\nperformance of the network can be reached by balanc-\ning the number of ﬁlters per stage and the depth of\nthe network. Increasing both the width and the depth\nof the network can contribute to higher quality net-\nworks. However, the optimal improvement for a con-\nstant amount of computation can be reached if both are\nincreased in parallel. The computational budget should\ntherefore be distributed in a balanced way between the\ndepth and width of the network.\nAlthough these principles might make sense, it is not\nstraightforward to use them to improve the quality of net-\nworks out of box. The idea is to use them judiciously in\nambiguous situations only.\n3. Factorizing Convolutions with Large Filter\nSize\nMuch of the original gains of the GoogLeNet net-\nwork [20] arise from a very generous use of dimension re-\nduction. This can be viewed as a special case of factorizing\nconvolutions in a computationally efﬁcient manner. Con-\nsider for example the case of a 1 × 1 convolutional layer\nfollowed by a 3 × 3 convolutional layer. In a vision net-\nwork, it is expected that the outputs of near-by activations\nare highly correlated. Therefore, we can expect that their\nactivations can be reduced before aggregation and that this\nshould result in similarly expressive local representations.\nHere we explore other ways of factorizing convolutions\nin various settings, especially in order to increase the com-\nputational efﬁciency of the solution. Since Inception net-\nworks are fully convolutional, each weight corresponds to\n",
    "Figure 1. Mini-network replacing the 5 × 5 convolutions.\none multiplication per activation. Therefore, any reduction\nin computational cost results in reduced number of param-\neters. This means that with suitable factorization, we can\nend up with more disentangled parameters and therefore\nwith faster training. Also, we can use the computational\nand memory savings to increase the ﬁlter-bank sizes of our\nnetwork while maintaining our ability to train each model\nreplica on a single computer.\n3.1. Factorization into smaller convolutions\nConvolutions with larger spatial ﬁlters (e.g. 5 × 5 or\n7 × 7) tend to be disproportionally expensive in terms of\ncomputation. For example, a 5 × 5 convolution with n ﬁl-\nters over a grid with m ﬁlters is 25/9 = 2.78 times more\ncomputationally expensive than a 3 × 3 convolution with\nthe same number of ﬁlters. Of course, a 5×5 ﬁlter can cap-\nture dependencies between signals between activations of\nunits further away in the earlier layers, so a reduction of the\ngeometric size of the ﬁlters comes at a large cost of expres-\nsiveness. However, we can ask whether a 5 × 5 convolution\ncould be replaced by a multi-layer network with less pa-\nrameters with the same input size and output depth. If we\nzoom into the computation graph of the 5 × 5 convolution,\nwe see that each output looks like a small fully-connected\nnetwork sliding over 5×5 tiles over its input (see Figure 1).\nSince we are constructing a vision network, it seems natural\nto exploit translation invariance again and replace the fully\nconnected component by a two layer convolutional archi-\ntecture: the ﬁrst layer is a 3 × 3 convolution, the second is a\nfully connected layer on top of the 3 × 3 output grid of the\nﬁrst layer (see Figure 1). Sliding this small network over\nthe input activation grid boils down to replacing the 5 × 5\nconvolution with two layers of 3 × 3 convolution (compare\nFigure 4 with 5).\nThis setup clearly reduces the parameter count by shar-\ning the weights between adjacent tiles. To analyze the ex-\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\nx 10\n6\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nIteration\nTop−1 Accuracy\nFactorization with Linear vs ReLU activation\n \n \nReLU\nLinear\nFigure 2. One of several control experiments between two Incep-\ntion models, one of them uses factorization into linear + ReLU\nlayers, the other uses two ReLU layers. After 3.86 million opera-\ntions, the former settles at 76.2%, while the latter reaches 77.2%\ntop-1 Accuracy on the validation set.\npected computational cost savings, we will make a few sim-\nplifying assumptions that apply for the typical situations:\nWe can assume that n = αm, that is that we want to\nchange the number of activations/unit by a constant alpha\nfactor. Since the 5 × 5 convolution is aggregating, α is\ntypically slightly larger than one (around 1.5 in the case\nof GoogLeNet). Having a two layer replacement for the\n5 × 5 layer, it seems reasonable to reach this expansion in\ntwo steps: increasing the number of ﬁlters by √α in both\nsteps. In order to simplify our estimate by choosing α = 1\n(no expansion), If we would naivly slide a network without\nreusing the computation between neighboring grid tiles, we\nwould increase the computational cost. sliding this network\ncan be represented by two 3 × 3 convolutional layers which\nreuses the activations between adjacent tiles. This way, we\nend up with a net 9+9\n25 × reduction of computation, resulting\nin a relative gain of 28% by this factorization. The exact\nsame saving holds for the parameter count as each parame-\nter is used exactly once in the computation of the activation\nof each unit. Still, this setup raises two general questions:\nDoes this replacement result in any loss of expressiveness?\nIf our main goal is to factorize the linear part of the compu-\ntation, would it not suggest to keep linear activations in the\nﬁrst layer? We have ran several control experiments (for ex-\nample see ﬁgure 2) and using linear activation was always\ninferior to using rectiﬁed linear units in all stages of the fac-\ntorization. We attribute this gain to the enhanced space of\nvariations that the network can learn especially if we batch-\nnormalize [7] the output activations. One can see similar\neffects when using linear activations for the dimension re-\nduction components.\n3.2. Spatial Factorization into Asymmetric Convo-\nlutions\nThe above results suggest that convolutions with ﬁlters\nlarger 3 × 3 a might not be generally useful as they can\nalways be reduced into a sequence of 3 × 3 convolutional\n",
    "Figure 3. Mini-network replacing the 3 × 3 convolutions. The\nlower layer of this network consists of a 3 × 1 convolution with 3\noutput units.\n1x1 \n1x1 \n5x5 \n3x3 \nPool \n1x1 \nBase\nFilter Concat\n1x1 \nFigure 4. Original Inception module as described in [20].\nlayers. Still we can ask the question whether one should\nfactorize them into smaller, for example 2×2 convolutions.\nHowever, it turns out that one can do even better than 2 × 2\nby using asymmetric convolutions, e.g. n × 1. For example\nusing a 3 × 1 convolution followed by a 1 × 3 convolution\nis equivalent to sliding a two layer network with the same\nreceptive ﬁeld as in a 3 × 3 convolution (see ﬁgure 3). Still\nthe two-layer solution is 33% cheaper for the same number\nof output ﬁlters, if the number of input and output ﬁlters is\nequal. By comparison, factorizing a 3 × 3 convolution into\na two 2 × 2 convolution represents only a 11% saving of\ncomputation.\nIn theory, we could go even further and argue that one\ncan replace any n × n convolution by a 1 × n convolu-\n1x1 \n1x1 \n3x3 \n3x3 \nPool \n1x1 \nBase\nFilter Concat\n3x3 \n1x1 \nFigure 5. Inception modules where each 5 × 5 convolution is re-\nplaced by two 3 × 3 convolution, as suggested by principle 3 of\nSection 2.\ntion followed by a n × 1 convolution and the computational\ncost saving increases dramatically as n grows (see ﬁgure 6).\nIn practice, we have found that employing this factorization\ndoes not work well on early layers, but it gives very good re-\nsults on medium grid-sizes (On m×m feature maps, where\nm ranges between 12 and 20). On that level, very good re-\nsults can be achieved by using 1 × 7 convolutions followed\nby 7 × 1 convolutions.\n4. Utility of Auxiliary Classiﬁers\n[20] has introduced the notion of auxiliary classiﬁers to\nimprove the convergence of very deep networks. The origi-\nnal motivation was to push useful gradients to the lower lay-\ners to make them immediately useful and improve the con-\nvergence during training by combating the vanishing gra-\ndient problem in very deep networks. Also Lee et al[11]\nargues that auxiliary classiﬁers promote more stable learn-\ning and better convergence. Interestingly, we found that\nauxiliary classiﬁers did not result in improved convergence\nearly in the training: the training progression of network\nwith and without side head looks virtually identical before\nboth models reach high accuracy. Near the end of training,\nthe network with the auxiliary branches starts to overtake\nthe accuracy of the network without any auxiliary branch\nand reaches a slightly higher plateau.\nAlso [20] used two side-heads at different stages in the\nnetwork. The removal of the lower auxiliary branch did not\nhave any adverse effect on the ﬁnal quality of the network.\nTogether with the earlier observation in the previous para-\n",
    "1x1 \n1x1 \n1xn \nPool \n1x1 \nBase\nFilter Concat\nnx1 \n1xn \nnx1 \n1xn \nnx1 \n1x1 \nFigure 6. Inception modules after the factorization of the n × n\nconvolutions. In our proposed architecture, we chose n = 7 for\nthe 17 × 17 grid. (The ﬁlter sizes are picked using principle 3)\n.\ngraph, this means that original the hypothesis of [20] that\nthese branches help evolving the low-level features is most\nlikely misplaced. Instead, we argue that the auxiliary clas-\nsiﬁers act as regularizer. This is supported by the fact that\nthe main classiﬁer of the network performs better if the side\nbranch is batch-normalized [7] or has a dropout layer. This\nalso gives a weak supporting evidence for the conjecture\nthat batch normalization acts as a regularizer.\n5. Efﬁcient Grid Size Reduction\nTraditionally, convolutional networks used some pooling\noperation to decrease the grid size of the feature maps. In\norder to avoid a representational bottleneck, before apply-\ning maximum or average pooling the activation dimension\nof the network ﬁlters is expanded. For example, starting a\nd×d grid with k ﬁlters, if we would like to arrive at a d\n2 × d\n2\ngrid with 2k ﬁlters, we ﬁrst need to compute a stride-1 con-\nvolution with 2k ﬁlters and then apply an additional pooling\nstep. This means that the overall computational cost is dom-\ninated by the expensive convolution on the larger grid using\n2d2k2 operations. One possibility would be to switch to\npooling with convolution and therefore resulting in 2( d\n2)2k2\n1x1 \n1x1 \n3x3 \nPool \n1x1 \nBase\nFilter Concat\n1x3 \n1x3 \n1x1 \n3x1 \n3x1 \nFigure 7. Inception modules with expanded the ﬁlter bank outputs.\nThis architecture is used on the coarsest (8 × 8) grids to promote\nhigh dimensional representations, as suggested by principle 2 of\nSection 2. We are using this solution only on the coarsest grid,\nsince that is the place where producing high dimensional sparse\nrepresentation is the most critical as the ratio of local processing\n(by 1 × 1 convolutions) is increased compared to the spatial ag-\ngregation.\n17x17x768\n5x5x768\n8x8x1280\nInception\n5x5x128\n1x1x1024\n5x5 Average pooling with stride 3\n1x1 Convolution\n Fully connected\n...\nFigure 8. Auxiliary classiﬁer on top of the last 17×17 layer. Batch\nnormalization[7] of the layers in the side head results in a 0.4%\nabsolute gain in top-1 accuracy. The lower axis shows the number\nof itertions performed, each with batch size 32.\nreducing the computational cost by a quarter. However, this\ncreates a representational bottlenecks as the overall dimen-\nsionality of the representation drops to ( d\n2)2k resulting in\nless expressive networks (see Figure 9). Instead of doing so,\nwe suggest another variant the reduces the computational\ncost even further while removing the representational bot-\ntleneck. (see Figure 10). We can use two parallel stride 2\nblocks: P and C. P is a pooling layer (either average or\nmaximum pooling) the activation, both of them are stride 2\nthe ﬁlter banks of which are concatenated as in ﬁgure 10.\n",
    "35x35x320\n17x17x320\n17x17x640\nPooling\nInception\n35x35x320\n35x35x640\n17x17x640\nInception\nPooling\nFigure 9. Two alternative ways of reducing the grid size. The so-\nlution on the left violates the principle 1 of not introducing an rep-\nresentational bottleneck from Section 2. The version on the right\nis 3 times more expensive computationally.\nPool\nstride 2 \nBase\nFilter Concat\n1x1 \n3x3\nstride 2 \n3x3\nstride 1 \n1x1 \n3x3\nstride 2 \n35x35x320\n17x17x320\n17x17x320\n17x17x640\npool\nconv\nconcat\nFigure 10. Inception module that reduces the grid-size while ex-\npands the ﬁlter banks. It is both cheap and avoids the representa-\ntional bottleneck as is suggested by principle 1. The diagram on\nthe right represents the same solution but from the perspective of\ngrid sizes rather than the operations.\n6. Inception-v2\nHere we are connecting the dots from above and pro-\npose a new architecture with improved performance on the\nILSVRC 2012 classiﬁcation benchmark. The layout of our\nnetwork is given in table 1. Note that we have factorized\nthe traditional 7 × 7 convolution into three 3 × 3 convolu-\ntions based on the same ideas as described in section 3.1.\nFor the Inception part of the network, we have 3 traditional\ninception modules at the 35 × 35 with 288 ﬁlters each. This\nis reduced to a 17 × 17 grid with 768 ﬁlters using the grid\nreduction technique described in section 5. This is is fol-\nlowed by 5 instances of the factorized inception modules as\ndepicted in ﬁgure 5. This is reduced to a 8 × 8 × 1280 grid\nwith the grid reduction technique depicted in ﬁgure 10. At\nthe coarsest 8 × 8 level, we have two Inception modules as\ndepicted in ﬁgure 6, with a concatenated output ﬁlter bank\nsize of 2048 for each tile. The detailed structure of the net-\nwork, including the sizes of ﬁlter banks inside the Inception\nmodules, is given in the supplementary material, given in\nthe model.txt that is in the tar-ﬁle of this submission.\ntype\npatch size/stride\nor remarks\ninput size\nconv\n3×3/2\n299×299×3\nconv\n3×3/1\n149×149×32\nconv padded\n3×3/1\n147×147×32\npool\n3×3/2\n147×147×64\nconv\n3×3/1\n73×73×64\nconv\n3×3/2\n71×71×80\nconv\n3×3/1\n35×35×192\n3×Inception\nAs in ﬁgure 5\n35×35×288\n5×Inception\nAs in ﬁgure 6\n17×17×768\n2×Inception\nAs in ﬁgure 7\n8×8×1280\npool\n8 × 8\n8 × 8 × 2048\nlinear\nlogits\n1 × 1 × 2048\nsoftmax\nclassiﬁer\n1 × 1 × 1000\nTable 1. The outline of the proposed network architecture. The\noutput size of each module is the input size of the next one. We\nare using variations of reduction technique depicted Figure 10 to\nreduce the grid sizes between the Inception blocks whenever ap-\nplicable. We have marked the convolution with 0-padding, which\nis used to maintain the grid size. 0-padding is also used inside\nthose Inception modules that do not reduce the grid size. All other\nlayers do not use padding. The various ﬁlter bank sizes are chosen\nto observe principle 4 from Section 2.\nHowever, we have observed that the quality of the network\nis relatively stable to variations as long as the principles\nfrom Section 2 are observed. Although our network is 42\nlayers deep, our computation cost is only about 2.5 higher\nthan that of GoogLeNet and it is still much more efﬁcient\nthan VGGNet.\n7. Model Regularization via Label Smoothing\nHere we propose a mechanism to regularize the classiﬁer\nlayer by estimating the marginalized effect of label-dropout\nduring training.\nFor each training example x, our model computes the\nprobability of each label k\n∈\n{1 . . . K}:\np(k|x)\n=\nexp(zk)\nPK\ni=1 exp(zi). Here, zi are the logits or unnormalized log-\nprobabilities. Consider the ground-truth distribution over\nlabels q(k|x) for this training example, normalized so that\nP\nk q(k|x) = 1. For brevity, let us omit the dependence\nof p and q on example x. We deﬁne the loss for the ex-\nample as the cross entropy: ℓ= −PK\nk=1 log(p(k))q(k).\nMinimizing this is equivalent to maximizing the expected\nlog-likelihood of a label, where the label is selected accord-\ning to its ground-truth distribution q(k). Cross-entropy loss\nis differentiable with respect to the logits zk and thus can be\nused for gradient training of deep models. The gradient has\na rather simple form:\n∂ℓ\n∂zk = p(k)−q(k), which is bounded\nbetween −1 and 1.\nConsider the case of a single ground-truth label y, so\nthat q(y) = 1 and q(k) = 0 for all k ̸= y. In this case,\n",
    "minimizing the cross entropy is equivalent to maximizing\nthe log-likelihood of the correct label. For a particular ex-\nample x with label y, the log-likelihood is maximized for\nq(k) = δk,y, where δk,y is Dirac delta, which equals 1 for\nk = y and 0 otherwise. This maximum is not achievable\nfor ﬁnite zk but is approached if zy ≫zk for all k ̸= y\n– that is, if the logit corresponding to the ground-truth la-\nbel is much great than all other logits. This, however, can\ncause two problems. First, it may result in over-ﬁtting: if\nthe model learns to assign full probability to the ground-\ntruth label for each training example, it is not guaranteed to\ngeneralize. Second, it encourages the differences between\nthe largest logit and all others to become large, and this,\ncombined with the bounded gradient\n∂ℓ\n∂zk , reduces the abil-\nity of the model to adapt. Intuitively, this happens because\nthe model becomes too conﬁdent about its predictions.\nWe propose a mechanism for encouraging the model to\nbe less conﬁdent. While this may not be desired if the goal\nis to maximize the log-likelihood of training labels, it does\nregularize the model and makes it more adaptable.\nThe\nmethod is very simple. Consider a distribution over labels\nu(k), independent of the training example x, and a smooth-\ning parameter ϵ. For a training example with ground-truth\nlabel y, we replace the label distribution q(k|x) = δk,y with\nq′(k|x) = (1 −ϵ)δk,y + ϵu(k)\nwhich is a mixture of the original ground-truth distribution\nq(k|x) and the ﬁxed distribution u(k), with weights 1 −ϵ\nand ϵ, respectively. This can be seen as the distribution of\nthe label k obtained as follows: ﬁrst, set it to the ground-\ntruth label k = y; then, with probability ϵ, replace k with\na sample drawn from the distribution u(k). We propose to\nuse the prior distribution over labels as u(k). In our exper-\niments, we used the uniform distribution u(k) = 1/K, so\nthat\nq′(k) = (1 −ϵ)δk,y + ϵ\nK .\nWe refer to this change in ground-truth label distribution as\nlabel-smoothing regularization, or LSR.\nNote that LSR achieves the desired goal of preventing\nthe largest logit from becoming much larger than all others.\nIndeed, if this were to happen, then a single q(k) would\napproach 1 while all others would approach 0. This would\nresult in a large cross-entropy with q′(k) because, unlike\nq(k) = δk,y, all q′(k) have a positive lower bound.\nAnother interpretation of LSR can be obtained by con-\nsidering the cross entropy:\nH(q′, p) = −\nK\nX\nk=1\nlog p(k)q′(k) = (1−ϵ)H(q, p)+ϵH(u, p)\nThus, LSR is equivalent to replacing a single cross-entropy\nloss H(q, p) with a pair of such losses H(q, p) and H(u, p).\nThe second loss penalizes the deviation of predicted label\ndistribution p from the prior u, with the relative weight\nϵ\n1−ϵ.\nNote that this deviation could be equivalently captured by\nthe KL divergence, since H(u, p) = DKL(u∥p) + H(u)\nand H(u) is ﬁxed.\nWhen u is the uniform distribution,\nH(u, p) is a measure of how dissimilar the predicted dis-\ntribution p is to uniform, which could also be measured (but\nnot equivalently) by negative entropy −H(p); we have not\nexperimented with this approach.\nIn our ImageNet experiments with K = 1000 classes,\nwe used u(k) = 1/1000 and ϵ = 0.1. For ILSVRC 2012,\nwe have found a consistent improvement of about 0.2% ab-\nsolute both for top-1 error and the top-5 error (cf. Table 3).\n8. Training Methodology\nWe have trained our networks with stochastic gradient\nutilizing the TensorFlow [1] distributed machine learning\nsystem using 50 replicas running each on a NVidia Kepler\nGPU with batch size 32 for 100 epochs. Our earlier experi-\nments used momentum [19] with a decay of 0.9, while our\nbest models were achieved using RMSProp [21] with de-\ncay of 0.9 and ϵ = 1.0. We used a learning rate of 0.045,\ndecayed every two epoch using an exponential rate of 0.94.\nIn addition, gradient clipping [14] with threshold 2.0 was\nfound to be useful to stabilize the training. Model evalua-\ntions are performed using a running average of the parame-\nters computed over time.\n9. Performance on Lower Resolution Input\nA typical use-case of vision networks is for the the post-\nclassiﬁcation of detection, for example in the Multibox [4]\ncontext. This includes the analysis of a relative small patch\nof the image containing a single object with some context.\nThe tasks is to decide whether the center part of the patch\ncorresponds to some object and determine the class of the\nobject if it does. The challenge is that objects tend to be\nrelatively small and low-resolution. This raises the question\nof how to properly deal with lower resolution input.\nThe common wisdom is that models employing higher\nresolution receptive ﬁelds tend to result in signiﬁcantly im-\nproved recognition performance. However it is important to\ndistinguish between the effect of the increased resolution of\nthe ﬁrst layer receptive ﬁeld and the effects of larger model\ncapacitance and computation. If we just change the reso-\nlution of the input without further adjustment to the model,\nthen we end up using computationally much cheaper mod-\nels to solve more difﬁcult tasks. Of course, it is natural,\nthat these solutions loose out already because of the reduced\ncomputational effort. In order to make an accurate assess-\nment, the model needs to analyze vague hints in order to\nbe able to “hallucinate” the ﬁne details. This is computa-\ntionally costly. The question remains therefore: how much\n",
    "Receptive Field Size\nTop-1 Accuracy (single frame)\n79 × 79\n75.2%\n151 × 151\n76.4%\n299 × 299\n76.6%\nTable 2. Comparison of recognition performance when the size of\nthe receptive ﬁeld varies, but the computational cost is constant.\ndoes higher input resolution helps if the computational ef-\nfort is kept constant. One simple way to ensure constant\neffort is to reduce the strides of the ﬁrst two layer in the\ncase of lower resolution input, or by simply removing the\nﬁrst pooling layer of the network.\nFor this purpose we have performed the following three\nexperiments:\n1. 299 × 299 receptive ﬁeld with stride 2 and maximum\npooling after the ﬁrst layer.\n2. 151 × 151 receptive ﬁeld with stride 1 and maximum\npooling after the ﬁrst layer.\n3. 79 × 79 receptive ﬁeld with stride 1 and without pool-\ning after the ﬁrst layer.\nAll three networks have almost identical computational\ncost. Although the third network is slightly cheaper, the\ncost of the pooling layer is marginal and (within 1% of the\ntotal cost of the)network. In each case, the networks were\ntrained until convergence and their quality was measured on\nthe validation set of the ImageNet ILSVRC 2012 classiﬁca-\ntion benchmark. The results can be seen in table 2. Al-\nthough the lower-resolution networks take longer to train,\nthe quality of the ﬁnal result is quite close to that of their\nhigher resolution counterparts.\nHowever, if one would just naively reduce the network\nsize according to the input resolution, then network would\nperform much more poorly. However this would an unfair\ncomparison as we would are comparing a 16 times cheaper\nmodel on a more difﬁcult task.\nAlso these results of table 2 suggest, one might con-\nsider using dedicated high-cost low resolution networks for\nsmaller objects in the R-CNN [5] context.\n10. Experimental Results and Comparisons\nTable 3 shows the experimental results about the recog-\nnition performance of our proposed architecture (Inception-\nv2) as described in Section 6. Each Inception-v2 line shows\nthe result of the cumulative changes including the high-\nlighted new modiﬁcation plus all the earlier ones. Label\nSmoothing refers to method described in Section 7. Fac-\ntorized 7 × 7 includes a change that factorizes the ﬁrst\n7 × 7 convolutional layer into a sequence of 3 × 3 convo-\nlutional layers. BN-auxiliary refers to the version in which\nNetwork\nTop-1\nError\nTop-5\nError\nCost\nBn Ops\nGoogLeNet [20]\n29%\n9.2%\n1.5\nBN-GoogLeNet\n26.8%\n-\n1.5\nBN-Inception [7]\n25.2%\n7.8\n2.0\nInception-v2\n23.4%\n-\n3.8\nInception-v2\nRMSProp\n23.1%\n6.3\n3.8\nInception-v2\nLabel Smoothing\n22.8%\n6.1\n3.8\nInception-v2\nFactorized 7 × 7\n21.6%\n5.8\n4.8\nInception-v2\nBN-auxiliary\n21.2%\n5.6%\n4.8\nTable 3. Single crop experimental results comparing the cumula-\ntive effects on the various contributing factors. We compare our\nnumbers with the best published single-crop inference for Ioffe at\nal [7]. For the “Inception-v2” lines, the changes are cumulative\nand each subsequent line includes the new change in addition to\nthe previous ones. The last line is referring to all the changes is\nwhat we refer to as “Inception-v3” below. Unfortunately, He et\nal [6] reports the only 10-crop evaluation results, but not single\ncrop results, which is reported in the Table 4 below.\nNetwork\nCrops\nEvaluated\nTop-5\nError\nTop-1\nError\nGoogLeNet [20]\n10\n-\n9.15%\nGoogLeNet [20]\n144\n-\n7.89%\nVGG [18]\n-\n24.4%\n6.8%\nBN-Inception [7]\n144\n22%\n5.82%\nPReLU [6]\n10\n24.27%\n7.38%\nPReLU [6]\n-\n21.59%\n5.71%\nInception-v3\n12\n19.47%\n4.48%\nInception-v3\n144\n18.77%\n4.2%\nTable 4. Single-model, multi-crop experimental results compar-\ning the cumulative effects on the various contributing factors. We\ncompare our numbers with the best published single-model infer-\nence results on the ILSVRC 2012 classiﬁcation benchmark.\nthe fully connected layer of the auxiliary classiﬁer is also\nbatch-normalized, not just the convolutions. We are refer-\nring to the model in last row of Table 3 as Inception-v3 and\nevaluate its performance in the multi-crop and ensemble set-\ntings.\nAll our evaluations are done on the 48238 non-\nblacklisted examples on the ILSVRC-2012 validation set,\nas suggested by [16]. We have evaluated all the 50000 ex-\namples as well and the results were roughly 0.1% worse in\ntop-5 error and around 0.2% in top-1 error. In the upcom-\ning version of this paper, we will verify our ensemble result\non the test set, but at the time of our last evaluation of BN-\nInception in spring [7] indicates that the test and validation\nset error tends to correlate very well.\n",
    "Network\nModels\nEvaluated\nCrops\nEvaluated\nTop-1\nError\nTop-5\nError\nVGGNet [18]\n2\n-\n23.7%\n6.8%\nGoogLeNet [20]\n7\n144\n-\n6.67%\nPReLU [6]\n-\n-\n-\n4.94%\nBN-Inception [7]\n6\n144\n20.1%\n4.9%\nInception-v3\n4\n144\n17.2%\n3.58%∗\nTable 5. Ensemble evaluation results comparing multi-model,\nmulti-crop reported results. Our numbers are compared with the\nbest published ensemble inference results on the ILSVRC 2012\nclassiﬁcation benchmark.\n∗All results, but the top-5 ensemble\nresult reported are on the validation set. The ensemble yielded\n3.46% top-5 error on the validation set.\n11. Conclusions\nWe have provided several design principles to scale up\nconvolutional networks and studied them in the context of\nthe Inception architecture. This guidance can lead to high\nperformance vision networks that have a relatively mod-\nest computation cost compared to simpler, more monolithic\narchitectures. Our highest quality version of Inception-v3\nreaches 21.2%, top-1 and 5.6% top-5 error for single crop\nevaluation on the ILSVR 2012 classiﬁcation, setting a new\nstate of the art.\nThis is achieved with relatively modest\n(2.5×) increase in computational cost compared to the net-\nwork described in Ioffe et al [7]. Still our solution uses\nmuch less computation than the best published results based\non denser networks: our model outperforms the results of\nHe et al [6] – cutting the top-5 (top-1) error by 25% (14%)\nrelative, respectively – while being six times cheaper com-\nputationally and using at least ﬁve times less parameters\n(estimated).\nOur ensemble of four Inception-v3 models\nreaches 3.5% with multi-crop evaluation reaches 3.5% top-\n5 error which represents an over 25% reduction to the best\npublished results and is almost half of the error of ILSVRC\n2014 winining GoogLeNet ensemble.\nWe have also demonstrated that high quality results can\nbe reached with receptive ﬁeld resolution as low as 79×79.\nThis might prove to be helpful in systems for detecting rel-\natively small objects. We have studied how factorizing con-\nvolutions and aggressive dimension reductions inside neural\nnetwork can result in networks with relatively low computa-\ntional cost while maintaining high quality. The combination\nof lower parameter count and additional regularization with\nbatch-normalized auxiliary classiﬁers and label-smoothing\nallows for training high quality networks on relatively mod-\nest sized training sets.\nReferences\n[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,\nC. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghe-\nmawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,\nR. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man´e,\nR. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,\nJ. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker,\nV. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals, P. War-\nden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. Tensor-\nFlow: Large-scale machine learning on heterogeneous sys-\ntems, 2015. Software available from tensorﬂow.org.\n[2] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and\nY. Chen.\nCompressing neural networks with the hashing\ntrick. In Proceedings of The 32nd International Conference\non Machine Learning, 2015.\n[3] C. Dong, C. C. Loy, K. He, and X. Tang. Learning a deep\nconvolutional network for image super-resolution. In Com-\nputer Vision–ECCV 2014, pages 184–199. Springer, 2014.\n[4] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable\nobject detection using deep neural networks. In Computer\nVision and Pattern Recognition (CVPR), 2014 IEEE Confer-\nence on, pages 2155–2162. IEEE, 2014.\n[5] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\nture hierarchies for accurate object detection and semantic\nsegmentation. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2014.\n[6] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into\nrectiﬁers: Surpassing human-level performance on imagenet\nclassiﬁcation. arXiv preprint arXiv:1502.01852, 2015.\n[7] S. Ioffe and C. Szegedy. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift. In\nProceedings of The 32nd International Conference on Ma-\nchine Learning, pages 448–456, 2015.\n[8] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,\nand L. Fei-Fei. Large-scale video classiﬁcation with con-\nvolutional neural networks.\nIn Computer Vision and Pat-\ntern Recognition (CVPR), 2014 IEEE Conference on, pages\n1725–1732. IEEE, 2014.\n[9] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImagenet\nclassiﬁcation with deep convolutional neural networks. In\nAdvances in neural information processing systems, pages\n1097–1105, 2012.\n[10] A. Lavin. Fast algorithms for convolutional neural networks.\narXiv preprint arXiv:1509.09308, 2015.\n[11] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-\nsupervised nets. arXiv preprint arXiv:1409.5185, 2014.\n[12] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional\nnetworks for semantic segmentation. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 3431–3440, 2015.\n[13] Y. Movshovitz-Attias, Q. Yu, M. C. Stumpe, V. Shet,\nS. Arnoud, and L. Yatziv. Ontological supervision for ﬁne\ngrained classiﬁcation of street view storefronts. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 1693–1702, 2015.\n[14] R. Pascanu, T. Mikolov, and Y. Bengio.\nOn the difﬁ-\nculty of training recurrent neural networks. arXiv preprint\narXiv:1211.5063, 2012.\n[15] D. C. Psichogios and L. H. Ungar. Svd-net: an algorithm\nthat automatically selects network structure. IEEE transac-\ntions on neural networks/a publication of the IEEE Neural\nNetworks Council, 5(3):513–515, 1993.\n",
    "[16] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\net al.\nImagenet large scale visual recognition challenge.\n2014.\n[17] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uni-\nﬁed embedding for face recognition and clustering. arXiv\npreprint arXiv:1503.03832, 2015.\n[18] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. arXiv preprint\narXiv:1409.1556, 2014.\n[19] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the\nimportance of initialization and momentum in deep learning.\nIn Proceedings of the 30th International Conference on Ma-\nchine Learning (ICML-13), volume 28, pages 1139–1147.\nJMLR Workshop and Conference Proceedings, May 2013.\n[20] C. Szegedy,\nW. Liu,\nY. Jia,\nP. Sermanet,\nS. Reed,\nD. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.\nGoing deeper with convolutions. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\npages 1–9, 2015.\n[21] T. Tieleman and G. Hinton. Divide the gradient by a run-\nning average of its recent magnitude. COURSERA: Neural\nNetworks for Machine Learning, 4, 2012. Accessed: 2015-\n11-05.\n[22] A. Toshev and C. Szegedy. Deeppose: Human pose estima-\ntion via deep neural networks. In Computer Vision and Pat-\ntern Recognition (CVPR), 2014 IEEE Conference on, pages\n1653–1660. IEEE, 2014.\n[23] N. Wang and D.-Y. Yeung. Learning a deep compact image\nrepresentation for visual tracking.\nIn Advances in Neural\nInformation Processing Systems, pages 809–817, 2013.\n"
  ],
  "full_text": "Rethinking the Inception Architecture for Computer Vision\nChristian Szegedy\nGoogle Inc.\nszegedy@google.com\nVincent Vanhoucke\nvanhoucke@google.com\nSergey Ioffe\nsioffe@google.com\nJonathon Shlens\nshlens@google.com\nZbigniew Wojna\nUniversity College London\nzbigniewwojna@gmail.com\nAbstract\nConvolutional networks are at the core of most state-\nof-the-art computer vision solutions for a wide variety of\ntasks. Since 2014 very deep convolutional networks started\nto become mainstream, yielding substantial gains in vari-\nous benchmarks. Although increased model size and com-\nputational cost tend to translate to immediate quality gains\nfor most tasks (as long as enough labeled data is provided\nfor training), computational efﬁciency and low parameter\ncount are still enabling factors for various use cases such as\nmobile vision and big-data scenarios. Here we are explor-\ning ways to scale up networks in ways that aim at utilizing\nthe added computation as efﬁciently as possible by suitably\nfactorized convolutions and aggressive regularization. We\nbenchmark our methods on the ILSVRC 2012 classiﬁcation\nchallenge validation set demonstrate substantial gains over\nthe state of the art: 21.2% top-1 and 5.6% top-5 error for\nsingle frame evaluation using a network with a computa-\ntional cost of 5 billion multiply-adds per inference and with\nusing less than 25 million parameters. With an ensemble of\n4 models and multi-crop evaluation, we report 3.5% top-5\nerror and 17.3% top-1 error.\n1. Introduction\nSince the 2012 ImageNet competition [16] winning en-\ntry by Krizhevsky et al [9], their network “AlexNet” has\nbeen successfully applied to a larger variety of computer\nvision tasks, for example to object-detection [5], segmen-\ntation [12], human pose estimation [22], video classiﬁca-\ntion [8], object tracking [23], and superresolution [3].\nThese successes spurred a new line of research that fo-\ncused on ﬁnding higher performing convolutional neural\nnetworks. Starting in 2014, the quality of network architec-\ntures signiﬁcantly improved by utilizing deeper and wider\nnetworks. VGGNet [18] and GoogLeNet [20] yielded simi-\nlarly high performance in the 2014 ILSVRC [16] classiﬁca-\ntion challenge. One interesting observation was that gains\nin the classiﬁcation performance tend to transfer to signiﬁ-\ncant quality gains in a wide variety of application domains.\nThis means that architectural improvements in deep con-\nvolutional architecture can be utilized for improving perfor-\nmance for most other computer vision tasks that are increas-\ningly reliant on high quality, learned visual features. Also,\nimprovements in the network quality resulted in new appli-\ncation domains for convolutional networks in cases where\nAlexNet features could not compete with hand engineered,\ncrafted solutions, e.g. proposal generation in detection[4].\nAlthough VGGNet [18] has the compelling feature of\narchitectural simplicity, this comes at a high cost: evalu-\nating the network requires a lot of computation. On the\nother hand, the Inception architecture of GoogLeNet [20]\nwas also designed to perform well even under strict con-\nstraints on memory and computational budget. For exam-\nple, GoogleNet employed only 5 million parameters, which\nrepresented a 12× reduction with respect to its predeces-\nsor AlexNet, which used 60 million parameters. Further-\nmore, VGGNet employed about 3x more parameters than\nAlexNet.\nThe computational cost of Inception is also much lower\nthan VGGNet or its higher performing successors [6]. This\nhas made it feasible to utilize Inception networks in big-data\nscenarios[17], [13], where huge amount of data needed to\nbe processed at reasonable cost or scenarios where memory\nor computational capacity is inherently limited, for example\nin mobile vision settings. It is certainly possible to mitigate\nparts of these issues by applying specialized solutions to tar-\nget memory use [2], [15] or by optimizing the execution of\ncertain operations via computational tricks [10]. However,\nthese methods add extra complexity. Furthermore, these\nmethods could be applied to optimize the Inception archi-\ntecture as well, widening the efﬁciency gap again.\nStill, the complexity of the Inception architecture makes\n1\narXiv:1512.00567v3  [cs.CV]  11 Dec 2015\n\n\nit more difﬁcult to make changes to the network. If the ar-\nchitecture is scaled up naively, large parts of the computa-\ntional gains can be immediately lost. Also, [20] does not\nprovide a clear description about the contributing factors\nthat lead to the various design decisions of the GoogLeNet\narchitecture. This makes it much harder to adapt it to new\nuse-cases while maintaining its efﬁciency.\nFor example,\nif it is deemed necessary to increase the capacity of some\nInception-style model, the simple transformation of just\ndoubling the number of all ﬁlter bank sizes will lead to a\n4x increase in both computational cost and number of pa-\nrameters. This might prove prohibitive or unreasonable in a\nlot of practical scenarios, especially if the associated gains\nare modest. In this paper, we start with describing a few\ngeneral principles and optimization ideas that that proved\nto be useful for scaling up convolution networks in efﬁcient\nways. Although our principles are not limited to Inception-\ntype networks, they are easier to observe in that context as\nthe generic structure of the Inception style building blocks\nis ﬂexible enough to incorporate those constraints naturally.\nThis is enabled by the generous use of dimensional reduc-\ntion and parallel structures of the Inception modules which\nallows for mitigating the impact of structural changes on\nnearby components. Still, one needs to be cautious about\ndoing so, as some guiding principles should be observed to\nmaintain high quality of the models.\n2. General Design Principles\nHere we will describe a few design principles based\non large-scale experimentation with various architectural\nchoices with convolutional networks. At this point, the util-\nity of the principles below are speculative and additional\nfuture experimental evidence will be necessary to assess\ntheir accuracy and domain of validity. Still, grave devia-\ntions from these principles tended to result in deterioration\nin the quality of the networks and ﬁxing situations where\nthose deviations were detected resulted in improved archi-\ntectures in general.\n1. Avoid representational bottlenecks, especially early in\nthe network.\nFeed-forward networks can be repre-\nsented by an acyclic graph from the input layer(s) to\nthe classiﬁer or regressor. This deﬁnes a clear direction\nfor the information ﬂow. For any cut separating the in-\nputs from the outputs, one can access the amount of\ninformation passing though the cut. One should avoid\nbottlenecks with extreme compression. In general the\nrepresentation size should gently decrease from the in-\nputs to the outputs before reaching the ﬁnal represen-\ntation used for the task at hand. Theoretically, infor-\nmation content can not be assessed merely by the di-\nmensionality of the representation as it discards impor-\ntant factors like correlation structure; the dimensional-\nity merely provides a rough estimate of information\ncontent.\n2. Higher dimensional representations are easier to pro-\ncess locally within a network. Increasing the activa-\ntions per tile in a convolutional network allows for\nmore disentangled features.\nThe resulting networks\nwill train faster.\n3. Spatial aggregation can be done over lower dimen-\nsional embeddings without much or any loss in rep-\nresentational power. For example, before performing a\nmore spread out (e.g. 3 × 3) convolution, one can re-\nduce the dimension of the input representation before\nthe spatial aggregation without expecting serious ad-\nverse effects. We hypothesize that the reason for that\nis the strong correlation between adjacent unit results\nin much less loss of information during dimension re-\nduction, if the outputs are used in a spatial aggrega-\ntion context. Given that these signals should be easily\ncompressible, the dimension reduction even promotes\nfaster learning.\n4. Balance the width and depth of the network. Optimal\nperformance of the network can be reached by balanc-\ning the number of ﬁlters per stage and the depth of\nthe network. Increasing both the width and the depth\nof the network can contribute to higher quality net-\nworks. However, the optimal improvement for a con-\nstant amount of computation can be reached if both are\nincreased in parallel. The computational budget should\ntherefore be distributed in a balanced way between the\ndepth and width of the network.\nAlthough these principles might make sense, it is not\nstraightforward to use them to improve the quality of net-\nworks out of box. The idea is to use them judiciously in\nambiguous situations only.\n3. Factorizing Convolutions with Large Filter\nSize\nMuch of the original gains of the GoogLeNet net-\nwork [20] arise from a very generous use of dimension re-\nduction. This can be viewed as a special case of factorizing\nconvolutions in a computationally efﬁcient manner. Con-\nsider for example the case of a 1 × 1 convolutional layer\nfollowed by a 3 × 3 convolutional layer. In a vision net-\nwork, it is expected that the outputs of near-by activations\nare highly correlated. Therefore, we can expect that their\nactivations can be reduced before aggregation and that this\nshould result in similarly expressive local representations.\nHere we explore other ways of factorizing convolutions\nin various settings, especially in order to increase the com-\nputational efﬁciency of the solution. Since Inception net-\nworks are fully convolutional, each weight corresponds to\n\n\nFigure 1. Mini-network replacing the 5 × 5 convolutions.\none multiplication per activation. Therefore, any reduction\nin computational cost results in reduced number of param-\neters. This means that with suitable factorization, we can\nend up with more disentangled parameters and therefore\nwith faster training. Also, we can use the computational\nand memory savings to increase the ﬁlter-bank sizes of our\nnetwork while maintaining our ability to train each model\nreplica on a single computer.\n3.1. Factorization into smaller convolutions\nConvolutions with larger spatial ﬁlters (e.g. 5 × 5 or\n7 × 7) tend to be disproportionally expensive in terms of\ncomputation. For example, a 5 × 5 convolution with n ﬁl-\nters over a grid with m ﬁlters is 25/9 = 2.78 times more\ncomputationally expensive than a 3 × 3 convolution with\nthe same number of ﬁlters. Of course, a 5×5 ﬁlter can cap-\nture dependencies between signals between activations of\nunits further away in the earlier layers, so a reduction of the\ngeometric size of the ﬁlters comes at a large cost of expres-\nsiveness. However, we can ask whether a 5 × 5 convolution\ncould be replaced by a multi-layer network with less pa-\nrameters with the same input size and output depth. If we\nzoom into the computation graph of the 5 × 5 convolution,\nwe see that each output looks like a small fully-connected\nnetwork sliding over 5×5 tiles over its input (see Figure 1).\nSince we are constructing a vision network, it seems natural\nto exploit translation invariance again and replace the fully\nconnected component by a two layer convolutional archi-\ntecture: the ﬁrst layer is a 3 × 3 convolution, the second is a\nfully connected layer on top of the 3 × 3 output grid of the\nﬁrst layer (see Figure 1). Sliding this small network over\nthe input activation grid boils down to replacing the 5 × 5\nconvolution with two layers of 3 × 3 convolution (compare\nFigure 4 with 5).\nThis setup clearly reduces the parameter count by shar-\ning the weights between adjacent tiles. To analyze the ex-\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\nx 10\n6\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nIteration\nTop−1 Accuracy\nFactorization with Linear vs ReLU activation\n \n \nReLU\nLinear\nFigure 2. One of several control experiments between two Incep-\ntion models, one of them uses factorization into linear + ReLU\nlayers, the other uses two ReLU layers. After 3.86 million opera-\ntions, the former settles at 76.2%, while the latter reaches 77.2%\ntop-1 Accuracy on the validation set.\npected computational cost savings, we will make a few sim-\nplifying assumptions that apply for the typical situations:\nWe can assume that n = αm, that is that we want to\nchange the number of activations/unit by a constant alpha\nfactor. Since the 5 × 5 convolution is aggregating, α is\ntypically slightly larger than one (around 1.5 in the case\nof GoogLeNet). Having a two layer replacement for the\n5 × 5 layer, it seems reasonable to reach this expansion in\ntwo steps: increasing the number of ﬁlters by √α in both\nsteps. In order to simplify our estimate by choosing α = 1\n(no expansion), If we would naivly slide a network without\nreusing the computation between neighboring grid tiles, we\nwould increase the computational cost. sliding this network\ncan be represented by two 3 × 3 convolutional layers which\nreuses the activations between adjacent tiles. This way, we\nend up with a net 9+9\n25 × reduction of computation, resulting\nin a relative gain of 28% by this factorization. The exact\nsame saving holds for the parameter count as each parame-\nter is used exactly once in the computation of the activation\nof each unit. Still, this setup raises two general questions:\nDoes this replacement result in any loss of expressiveness?\nIf our main goal is to factorize the linear part of the compu-\ntation, would it not suggest to keep linear activations in the\nﬁrst layer? We have ran several control experiments (for ex-\nample see ﬁgure 2) and using linear activation was always\ninferior to using rectiﬁed linear units in all stages of the fac-\ntorization. We attribute this gain to the enhanced space of\nvariations that the network can learn especially if we batch-\nnormalize [7] the output activations. One can see similar\neffects when using linear activations for the dimension re-\nduction components.\n3.2. Spatial Factorization into Asymmetric Convo-\nlutions\nThe above results suggest that convolutions with ﬁlters\nlarger 3 × 3 a might not be generally useful as they can\nalways be reduced into a sequence of 3 × 3 convolutional\n\n\nFigure 3. Mini-network replacing the 3 × 3 convolutions. The\nlower layer of this network consists of a 3 × 1 convolution with 3\noutput units.\n1x1 \n1x1 \n5x5 \n3x3 \nPool \n1x1 \nBase\nFilter Concat\n1x1 \nFigure 4. Original Inception module as described in [20].\nlayers. Still we can ask the question whether one should\nfactorize them into smaller, for example 2×2 convolutions.\nHowever, it turns out that one can do even better than 2 × 2\nby using asymmetric convolutions, e.g. n × 1. For example\nusing a 3 × 1 convolution followed by a 1 × 3 convolution\nis equivalent to sliding a two layer network with the same\nreceptive ﬁeld as in a 3 × 3 convolution (see ﬁgure 3). Still\nthe two-layer solution is 33% cheaper for the same number\nof output ﬁlters, if the number of input and output ﬁlters is\nequal. By comparison, factorizing a 3 × 3 convolution into\na two 2 × 2 convolution represents only a 11% saving of\ncomputation.\nIn theory, we could go even further and argue that one\ncan replace any n × n convolution by a 1 × n convolu-\n1x1 \n1x1 \n3x3 \n3x3 \nPool \n1x1 \nBase\nFilter Concat\n3x3 \n1x1 \nFigure 5. Inception modules where each 5 × 5 convolution is re-\nplaced by two 3 × 3 convolution, as suggested by principle 3 of\nSection 2.\ntion followed by a n × 1 convolution and the computational\ncost saving increases dramatically as n grows (see ﬁgure 6).\nIn practice, we have found that employing this factorization\ndoes not work well on early layers, but it gives very good re-\nsults on medium grid-sizes (On m×m feature maps, where\nm ranges between 12 and 20). On that level, very good re-\nsults can be achieved by using 1 × 7 convolutions followed\nby 7 × 1 convolutions.\n4. Utility of Auxiliary Classiﬁers\n[20] has introduced the notion of auxiliary classiﬁers to\nimprove the convergence of very deep networks. The origi-\nnal motivation was to push useful gradients to the lower lay-\ners to make them immediately useful and improve the con-\nvergence during training by combating the vanishing gra-\ndient problem in very deep networks. Also Lee et al[11]\nargues that auxiliary classiﬁers promote more stable learn-\ning and better convergence. Interestingly, we found that\nauxiliary classiﬁers did not result in improved convergence\nearly in the training: the training progression of network\nwith and without side head looks virtually identical before\nboth models reach high accuracy. Near the end of training,\nthe network with the auxiliary branches starts to overtake\nthe accuracy of the network without any auxiliary branch\nand reaches a slightly higher plateau.\nAlso [20] used two side-heads at different stages in the\nnetwork. The removal of the lower auxiliary branch did not\nhave any adverse effect on the ﬁnal quality of the network.\nTogether with the earlier observation in the previous para-\n\n\n1x1 \n1x1 \n1xn \nPool \n1x1 \nBase\nFilter Concat\nnx1 \n1xn \nnx1 \n1xn \nnx1 \n1x1 \nFigure 6. Inception modules after the factorization of the n × n\nconvolutions. In our proposed architecture, we chose n = 7 for\nthe 17 × 17 grid. (The ﬁlter sizes are picked using principle 3)\n.\ngraph, this means that original the hypothesis of [20] that\nthese branches help evolving the low-level features is most\nlikely misplaced. Instead, we argue that the auxiliary clas-\nsiﬁers act as regularizer. This is supported by the fact that\nthe main classiﬁer of the network performs better if the side\nbranch is batch-normalized [7] or has a dropout layer. This\nalso gives a weak supporting evidence for the conjecture\nthat batch normalization acts as a regularizer.\n5. Efﬁcient Grid Size Reduction\nTraditionally, convolutional networks used some pooling\noperation to decrease the grid size of the feature maps. In\norder to avoid a representational bottleneck, before apply-\ning maximum or average pooling the activation dimension\nof the network ﬁlters is expanded. For example, starting a\nd×d grid with k ﬁlters, if we would like to arrive at a d\n2 × d\n2\ngrid with 2k ﬁlters, we ﬁrst need to compute a stride-1 con-\nvolution with 2k ﬁlters and then apply an additional pooling\nstep. This means that the overall computational cost is dom-\ninated by the expensive convolution on the larger grid using\n2d2k2 operations. One possibility would be to switch to\npooling with convolution and therefore resulting in 2( d\n2)2k2\n1x1 \n1x1 \n3x3 \nPool \n1x1 \nBase\nFilter Concat\n1x3 \n1x3 \n1x1 \n3x1 \n3x1 \nFigure 7. Inception modules with expanded the ﬁlter bank outputs.\nThis architecture is used on the coarsest (8 × 8) grids to promote\nhigh dimensional representations, as suggested by principle 2 of\nSection 2. We are using this solution only on the coarsest grid,\nsince that is the place where producing high dimensional sparse\nrepresentation is the most critical as the ratio of local processing\n(by 1 × 1 convolutions) is increased compared to the spatial ag-\ngregation.\n17x17x768\n5x5x768\n8x8x1280\nInception\n5x5x128\n1x1x1024\n5x5 Average pooling with stride 3\n1x1 Convolution\n Fully connected\n...\nFigure 8. Auxiliary classiﬁer on top of the last 17×17 layer. Batch\nnormalization[7] of the layers in the side head results in a 0.4%\nabsolute gain in top-1 accuracy. The lower axis shows the number\nof itertions performed, each with batch size 32.\nreducing the computational cost by a quarter. However, this\ncreates a representational bottlenecks as the overall dimen-\nsionality of the representation drops to ( d\n2)2k resulting in\nless expressive networks (see Figure 9). Instead of doing so,\nwe suggest another variant the reduces the computational\ncost even further while removing the representational bot-\ntleneck. (see Figure 10). We can use two parallel stride 2\nblocks: P and C. P is a pooling layer (either average or\nmaximum pooling) the activation, both of them are stride 2\nthe ﬁlter banks of which are concatenated as in ﬁgure 10.\n\n\n35x35x320\n17x17x320\n17x17x640\nPooling\nInception\n35x35x320\n35x35x640\n17x17x640\nInception\nPooling\nFigure 9. Two alternative ways of reducing the grid size. The so-\nlution on the left violates the principle 1 of not introducing an rep-\nresentational bottleneck from Section 2. The version on the right\nis 3 times more expensive computationally.\nPool\nstride 2 \nBase\nFilter Concat\n1x1 \n3x3\nstride 2 \n3x3\nstride 1 \n1x1 \n3x3\nstride 2 \n35x35x320\n17x17x320\n17x17x320\n17x17x640\npool\nconv\nconcat\nFigure 10. Inception module that reduces the grid-size while ex-\npands the ﬁlter banks. It is both cheap and avoids the representa-\ntional bottleneck as is suggested by principle 1. The diagram on\nthe right represents the same solution but from the perspective of\ngrid sizes rather than the operations.\n6. Inception-v2\nHere we are connecting the dots from above and pro-\npose a new architecture with improved performance on the\nILSVRC 2012 classiﬁcation benchmark. The layout of our\nnetwork is given in table 1. Note that we have factorized\nthe traditional 7 × 7 convolution into three 3 × 3 convolu-\ntions based on the same ideas as described in section 3.1.\nFor the Inception part of the network, we have 3 traditional\ninception modules at the 35 × 35 with 288 ﬁlters each. This\nis reduced to a 17 × 17 grid with 768 ﬁlters using the grid\nreduction technique described in section 5. This is is fol-\nlowed by 5 instances of the factorized inception modules as\ndepicted in ﬁgure 5. This is reduced to a 8 × 8 × 1280 grid\nwith the grid reduction technique depicted in ﬁgure 10. At\nthe coarsest 8 × 8 level, we have two Inception modules as\ndepicted in ﬁgure 6, with a concatenated output ﬁlter bank\nsize of 2048 for each tile. The detailed structure of the net-\nwork, including the sizes of ﬁlter banks inside the Inception\nmodules, is given in the supplementary material, given in\nthe model.txt that is in the tar-ﬁle of this submission.\ntype\npatch size/stride\nor remarks\ninput size\nconv\n3×3/2\n299×299×3\nconv\n3×3/1\n149×149×32\nconv padded\n3×3/1\n147×147×32\npool\n3×3/2\n147×147×64\nconv\n3×3/1\n73×73×64\nconv\n3×3/2\n71×71×80\nconv\n3×3/1\n35×35×192\n3×Inception\nAs in ﬁgure 5\n35×35×288\n5×Inception\nAs in ﬁgure 6\n17×17×768\n2×Inception\nAs in ﬁgure 7\n8×8×1280\npool\n8 × 8\n8 × 8 × 2048\nlinear\nlogits\n1 × 1 × 2048\nsoftmax\nclassiﬁer\n1 × 1 × 1000\nTable 1. The outline of the proposed network architecture. The\noutput size of each module is the input size of the next one. We\nare using variations of reduction technique depicted Figure 10 to\nreduce the grid sizes between the Inception blocks whenever ap-\nplicable. We have marked the convolution with 0-padding, which\nis used to maintain the grid size. 0-padding is also used inside\nthose Inception modules that do not reduce the grid size. All other\nlayers do not use padding. The various ﬁlter bank sizes are chosen\nto observe principle 4 from Section 2.\nHowever, we have observed that the quality of the network\nis relatively stable to variations as long as the principles\nfrom Section 2 are observed. Although our network is 42\nlayers deep, our computation cost is only about 2.5 higher\nthan that of GoogLeNet and it is still much more efﬁcient\nthan VGGNet.\n7. Model Regularization via Label Smoothing\nHere we propose a mechanism to regularize the classiﬁer\nlayer by estimating the marginalized effect of label-dropout\nduring training.\nFor each training example x, our model computes the\nprobability of each label k\n∈\n{1 . . . K}:\np(k|x)\n=\nexp(zk)\nPK\ni=1 exp(zi). Here, zi are the logits or unnormalized log-\nprobabilities. Consider the ground-truth distribution over\nlabels q(k|x) for this training example, normalized so that\nP\nk q(k|x) = 1. For brevity, let us omit the dependence\nof p and q on example x. We deﬁne the loss for the ex-\nample as the cross entropy: ℓ= −PK\nk=1 log(p(k))q(k).\nMinimizing this is equivalent to maximizing the expected\nlog-likelihood of a label, where the label is selected accord-\ning to its ground-truth distribution q(k). Cross-entropy loss\nis differentiable with respect to the logits zk and thus can be\nused for gradient training of deep models. The gradient has\na rather simple form:\n∂ℓ\n∂zk = p(k)−q(k), which is bounded\nbetween −1 and 1.\nConsider the case of a single ground-truth label y, so\nthat q(y) = 1 and q(k) = 0 for all k ̸= y. In this case,\n\n\nminimizing the cross entropy is equivalent to maximizing\nthe log-likelihood of the correct label. For a particular ex-\nample x with label y, the log-likelihood is maximized for\nq(k) = δk,y, where δk,y is Dirac delta, which equals 1 for\nk = y and 0 otherwise. This maximum is not achievable\nfor ﬁnite zk but is approached if zy ≫zk for all k ̸= y\n– that is, if the logit corresponding to the ground-truth la-\nbel is much great than all other logits. This, however, can\ncause two problems. First, it may result in over-ﬁtting: if\nthe model learns to assign full probability to the ground-\ntruth label for each training example, it is not guaranteed to\ngeneralize. Second, it encourages the differences between\nthe largest logit and all others to become large, and this,\ncombined with the bounded gradient\n∂ℓ\n∂zk , reduces the abil-\nity of the model to adapt. Intuitively, this happens because\nthe model becomes too conﬁdent about its predictions.\nWe propose a mechanism for encouraging the model to\nbe less conﬁdent. While this may not be desired if the goal\nis to maximize the log-likelihood of training labels, it does\nregularize the model and makes it more adaptable.\nThe\nmethod is very simple. Consider a distribution over labels\nu(k), independent of the training example x, and a smooth-\ning parameter ϵ. For a training example with ground-truth\nlabel y, we replace the label distribution q(k|x) = δk,y with\nq′(k|x) = (1 −ϵ)δk,y + ϵu(k)\nwhich is a mixture of the original ground-truth distribution\nq(k|x) and the ﬁxed distribution u(k), with weights 1 −ϵ\nand ϵ, respectively. This can be seen as the distribution of\nthe label k obtained as follows: ﬁrst, set it to the ground-\ntruth label k = y; then, with probability ϵ, replace k with\na sample drawn from the distribution u(k). We propose to\nuse the prior distribution over labels as u(k). In our exper-\niments, we used the uniform distribution u(k) = 1/K, so\nthat\nq′(k) = (1 −ϵ)δk,y + ϵ\nK .\nWe refer to this change in ground-truth label distribution as\nlabel-smoothing regularization, or LSR.\nNote that LSR achieves the desired goal of preventing\nthe largest logit from becoming much larger than all others.\nIndeed, if this were to happen, then a single q(k) would\napproach 1 while all others would approach 0. This would\nresult in a large cross-entropy with q′(k) because, unlike\nq(k) = δk,y, all q′(k) have a positive lower bound.\nAnother interpretation of LSR can be obtained by con-\nsidering the cross entropy:\nH(q′, p) = −\nK\nX\nk=1\nlog p(k)q′(k) = (1−ϵ)H(q, p)+ϵH(u, p)\nThus, LSR is equivalent to replacing a single cross-entropy\nloss H(q, p) with a pair of such losses H(q, p) and H(u, p).\nThe second loss penalizes the deviation of predicted label\ndistribution p from the prior u, with the relative weight\nϵ\n1−ϵ.\nNote that this deviation could be equivalently captured by\nthe KL divergence, since H(u, p) = DKL(u∥p) + H(u)\nand H(u) is ﬁxed.\nWhen u is the uniform distribution,\nH(u, p) is a measure of how dissimilar the predicted dis-\ntribution p is to uniform, which could also be measured (but\nnot equivalently) by negative entropy −H(p); we have not\nexperimented with this approach.\nIn our ImageNet experiments with K = 1000 classes,\nwe used u(k) = 1/1000 and ϵ = 0.1. For ILSVRC 2012,\nwe have found a consistent improvement of about 0.2% ab-\nsolute both for top-1 error and the top-5 error (cf. Table 3).\n8. Training Methodology\nWe have trained our networks with stochastic gradient\nutilizing the TensorFlow [1] distributed machine learning\nsystem using 50 replicas running each on a NVidia Kepler\nGPU with batch size 32 for 100 epochs. Our earlier experi-\nments used momentum [19] with a decay of 0.9, while our\nbest models were achieved using RMSProp [21] with de-\ncay of 0.9 and ϵ = 1.0. We used a learning rate of 0.045,\ndecayed every two epoch using an exponential rate of 0.94.\nIn addition, gradient clipping [14] with threshold 2.0 was\nfound to be useful to stabilize the training. Model evalua-\ntions are performed using a running average of the parame-\nters computed over time.\n9. Performance on Lower Resolution Input\nA typical use-case of vision networks is for the the post-\nclassiﬁcation of detection, for example in the Multibox [4]\ncontext. This includes the analysis of a relative small patch\nof the image containing a single object with some context.\nThe tasks is to decide whether the center part of the patch\ncorresponds to some object and determine the class of the\nobject if it does. The challenge is that objects tend to be\nrelatively small and low-resolution. This raises the question\nof how to properly deal with lower resolution input.\nThe common wisdom is that models employing higher\nresolution receptive ﬁelds tend to result in signiﬁcantly im-\nproved recognition performance. However it is important to\ndistinguish between the effect of the increased resolution of\nthe ﬁrst layer receptive ﬁeld and the effects of larger model\ncapacitance and computation. If we just change the reso-\nlution of the input without further adjustment to the model,\nthen we end up using computationally much cheaper mod-\nels to solve more difﬁcult tasks. Of course, it is natural,\nthat these solutions loose out already because of the reduced\ncomputational effort. In order to make an accurate assess-\nment, the model needs to analyze vague hints in order to\nbe able to “hallucinate” the ﬁne details. This is computa-\ntionally costly. The question remains therefore: how much\n\n\nReceptive Field Size\nTop-1 Accuracy (single frame)\n79 × 79\n75.2%\n151 × 151\n76.4%\n299 × 299\n76.6%\nTable 2. Comparison of recognition performance when the size of\nthe receptive ﬁeld varies, but the computational cost is constant.\ndoes higher input resolution helps if the computational ef-\nfort is kept constant. One simple way to ensure constant\neffort is to reduce the strides of the ﬁrst two layer in the\ncase of lower resolution input, or by simply removing the\nﬁrst pooling layer of the network.\nFor this purpose we have performed the following three\nexperiments:\n1. 299 × 299 receptive ﬁeld with stride 2 and maximum\npooling after the ﬁrst layer.\n2. 151 × 151 receptive ﬁeld with stride 1 and maximum\npooling after the ﬁrst layer.\n3. 79 × 79 receptive ﬁeld with stride 1 and without pool-\ning after the ﬁrst layer.\nAll three networks have almost identical computational\ncost. Although the third network is slightly cheaper, the\ncost of the pooling layer is marginal and (within 1% of the\ntotal cost of the)network. In each case, the networks were\ntrained until convergence and their quality was measured on\nthe validation set of the ImageNet ILSVRC 2012 classiﬁca-\ntion benchmark. The results can be seen in table 2. Al-\nthough the lower-resolution networks take longer to train,\nthe quality of the ﬁnal result is quite close to that of their\nhigher resolution counterparts.\nHowever, if one would just naively reduce the network\nsize according to the input resolution, then network would\nperform much more poorly. However this would an unfair\ncomparison as we would are comparing a 16 times cheaper\nmodel on a more difﬁcult task.\nAlso these results of table 2 suggest, one might con-\nsider using dedicated high-cost low resolution networks for\nsmaller objects in the R-CNN [5] context.\n10. Experimental Results and Comparisons\nTable 3 shows the experimental results about the recog-\nnition performance of our proposed architecture (Inception-\nv2) as described in Section 6. Each Inception-v2 line shows\nthe result of the cumulative changes including the high-\nlighted new modiﬁcation plus all the earlier ones. Label\nSmoothing refers to method described in Section 7. Fac-\ntorized 7 × 7 includes a change that factorizes the ﬁrst\n7 × 7 convolutional layer into a sequence of 3 × 3 convo-\nlutional layers. BN-auxiliary refers to the version in which\nNetwork\nTop-1\nError\nTop-5\nError\nCost\nBn Ops\nGoogLeNet [20]\n29%\n9.2%\n1.5\nBN-GoogLeNet\n26.8%\n-\n1.5\nBN-Inception [7]\n25.2%\n7.8\n2.0\nInception-v2\n23.4%\n-\n3.8\nInception-v2\nRMSProp\n23.1%\n6.3\n3.8\nInception-v2\nLabel Smoothing\n22.8%\n6.1\n3.8\nInception-v2\nFactorized 7 × 7\n21.6%\n5.8\n4.8\nInception-v2\nBN-auxiliary\n21.2%\n5.6%\n4.8\nTable 3. Single crop experimental results comparing the cumula-\ntive effects on the various contributing factors. We compare our\nnumbers with the best published single-crop inference for Ioffe at\nal [7]. For the “Inception-v2” lines, the changes are cumulative\nand each subsequent line includes the new change in addition to\nthe previous ones. The last line is referring to all the changes is\nwhat we refer to as “Inception-v3” below. Unfortunately, He et\nal [6] reports the only 10-crop evaluation results, but not single\ncrop results, which is reported in the Table 4 below.\nNetwork\nCrops\nEvaluated\nTop-5\nError\nTop-1\nError\nGoogLeNet [20]\n10\n-\n9.15%\nGoogLeNet [20]\n144\n-\n7.89%\nVGG [18]\n-\n24.4%\n6.8%\nBN-Inception [7]\n144\n22%\n5.82%\nPReLU [6]\n10\n24.27%\n7.38%\nPReLU [6]\n-\n21.59%\n5.71%\nInception-v3\n12\n19.47%\n4.48%\nInception-v3\n144\n18.77%\n4.2%\nTable 4. Single-model, multi-crop experimental results compar-\ning the cumulative effects on the various contributing factors. We\ncompare our numbers with the best published single-model infer-\nence results on the ILSVRC 2012 classiﬁcation benchmark.\nthe fully connected layer of the auxiliary classiﬁer is also\nbatch-normalized, not just the convolutions. We are refer-\nring to the model in last row of Table 3 as Inception-v3 and\nevaluate its performance in the multi-crop and ensemble set-\ntings.\nAll our evaluations are done on the 48238 non-\nblacklisted examples on the ILSVRC-2012 validation set,\nas suggested by [16]. We have evaluated all the 50000 ex-\namples as well and the results were roughly 0.1% worse in\ntop-5 error and around 0.2% in top-1 error. In the upcom-\ning version of this paper, we will verify our ensemble result\non the test set, but at the time of our last evaluation of BN-\nInception in spring [7] indicates that the test and validation\nset error tends to correlate very well.\n\n\nNetwork\nModels\nEvaluated\nCrops\nEvaluated\nTop-1\nError\nTop-5\nError\nVGGNet [18]\n2\n-\n23.7%\n6.8%\nGoogLeNet [20]\n7\n144\n-\n6.67%\nPReLU [6]\n-\n-\n-\n4.94%\nBN-Inception [7]\n6\n144\n20.1%\n4.9%\nInception-v3\n4\n144\n17.2%\n3.58%∗\nTable 5. Ensemble evaluation results comparing multi-model,\nmulti-crop reported results. Our numbers are compared with the\nbest published ensemble inference results on the ILSVRC 2012\nclassiﬁcation benchmark.\n∗All results, but the top-5 ensemble\nresult reported are on the validation set. The ensemble yielded\n3.46% top-5 error on the validation set.\n11. Conclusions\nWe have provided several design principles to scale up\nconvolutional networks and studied them in the context of\nthe Inception architecture. This guidance can lead to high\nperformance vision networks that have a relatively mod-\nest computation cost compared to simpler, more monolithic\narchitectures. Our highest quality version of Inception-v3\nreaches 21.2%, top-1 and 5.6% top-5 error for single crop\nevaluation on the ILSVR 2012 classiﬁcation, setting a new\nstate of the art.\nThis is achieved with relatively modest\n(2.5×) increase in computational cost compared to the net-\nwork described in Ioffe et al [7]. Still our solution uses\nmuch less computation than the best published results based\non denser networks: our model outperforms the results of\nHe et al [6] – cutting the top-5 (top-1) error by 25% (14%)\nrelative, respectively – while being six times cheaper com-\nputationally and using at least ﬁve times less parameters\n(estimated).\nOur ensemble of four Inception-v3 models\nreaches 3.5% with multi-crop evaluation reaches 3.5% top-\n5 error which represents an over 25% reduction to the best\npublished results and is almost half of the error of ILSVRC\n2014 winining GoogLeNet ensemble.\nWe have also demonstrated that high quality results can\nbe reached with receptive ﬁeld resolution as low as 79×79.\nThis might prove to be helpful in systems for detecting rel-\natively small objects. We have studied how factorizing con-\nvolutions and aggressive dimension reductions inside neural\nnetwork can result in networks with relatively low computa-\ntional cost while maintaining high quality. The combination\nof lower parameter count and additional regularization with\nbatch-normalized auxiliary classiﬁers and label-smoothing\nallows for training high quality networks on relatively mod-\nest sized training sets.\nReferences\n[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,\nC. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghe-\nmawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,\nR. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man´e,\nR. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,\nJ. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker,\nV. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals, P. War-\nden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. Tensor-\nFlow: Large-scale machine learning on heterogeneous sys-\ntems, 2015. Software available from tensorﬂow.org.\n[2] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and\nY. Chen.\nCompressing neural networks with the hashing\ntrick. In Proceedings of The 32nd International Conference\non Machine Learning, 2015.\n[3] C. Dong, C. C. Loy, K. He, and X. Tang. Learning a deep\nconvolutional network for image super-resolution. In Com-\nputer Vision–ECCV 2014, pages 184–199. Springer, 2014.\n[4] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable\nobject detection using deep neural networks. In Computer\nVision and Pattern Recognition (CVPR), 2014 IEEE Confer-\nence on, pages 2155–2162. IEEE, 2014.\n[5] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\nture hierarchies for accurate object detection and semantic\nsegmentation. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2014.\n[6] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into\nrectiﬁers: Surpassing human-level performance on imagenet\nclassiﬁcation. arXiv preprint arXiv:1502.01852, 2015.\n[7] S. Ioffe and C. Szegedy. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift. In\nProceedings of The 32nd International Conference on Ma-\nchine Learning, pages 448–456, 2015.\n[8] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,\nand L. Fei-Fei. Large-scale video classiﬁcation with con-\nvolutional neural networks.\nIn Computer Vision and Pat-\ntern Recognition (CVPR), 2014 IEEE Conference on, pages\n1725–1732. IEEE, 2014.\n[9] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImagenet\nclassiﬁcation with deep convolutional neural networks. In\nAdvances in neural information processing systems, pages\n1097–1105, 2012.\n[10] A. Lavin. Fast algorithms for convolutional neural networks.\narXiv preprint arXiv:1509.09308, 2015.\n[11] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-\nsupervised nets. arXiv preprint arXiv:1409.5185, 2014.\n[12] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional\nnetworks for semantic segmentation. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 3431–3440, 2015.\n[13] Y. Movshovitz-Attias, Q. Yu, M. C. Stumpe, V. Shet,\nS. Arnoud, and L. Yatziv. Ontological supervision for ﬁne\ngrained classiﬁcation of street view storefronts. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 1693–1702, 2015.\n[14] R. Pascanu, T. Mikolov, and Y. Bengio.\nOn the difﬁ-\nculty of training recurrent neural networks. arXiv preprint\narXiv:1211.5063, 2012.\n[15] D. C. Psichogios and L. H. Ungar. Svd-net: an algorithm\nthat automatically selects network structure. IEEE transac-\ntions on neural networks/a publication of the IEEE Neural\nNetworks Council, 5(3):513–515, 1993.\n\n\n[16] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\net al.\nImagenet large scale visual recognition challenge.\n2014.\n[17] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uni-\nﬁed embedding for face recognition and clustering. arXiv\npreprint arXiv:1503.03832, 2015.\n[18] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. arXiv preprint\narXiv:1409.1556, 2014.\n[19] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the\nimportance of initialization and momentum in deep learning.\nIn Proceedings of the 30th International Conference on Ma-\nchine Learning (ICML-13), volume 28, pages 1139–1147.\nJMLR Workshop and Conference Proceedings, May 2013.\n[20] C. Szegedy,\nW. Liu,\nY. Jia,\nP. Sermanet,\nS. Reed,\nD. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.\nGoing deeper with convolutions. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\npages 1–9, 2015.\n[21] T. Tieleman and G. Hinton. Divide the gradient by a run-\nning average of its recent magnitude. COURSERA: Neural\nNetworks for Machine Learning, 4, 2012. Accessed: 2015-\n11-05.\n[22] A. Toshev and C. Szegedy. Deeppose: Human pose estima-\ntion via deep neural networks. In Computer Vision and Pat-\ntern Recognition (CVPR), 2014 IEEE Conference on, pages\n1653–1660. IEEE, 2014.\n[23] N. Wang and D.-Y. Yeung. Learning a deep compact image\nrepresentation for visual tracking.\nIn Advances in Neural\nInformation Processing Systems, pages 809–817, 2013.\n"
}