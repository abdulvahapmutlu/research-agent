{
  "filename": "2110.09446v1.pdf",
  "num_pages": 33,
  "pages": [
    "Squeezing Backbone Feature Distributions to the Max\nfor Eﬃcient Few-Shot Learning\nYuqing Hua,b, St´ephane Pateuxa, Vincent Griponb\naOrange Labs, Rennes, France\nbIMT Atlantique, Lab-STICC, UMR CNRS 6285, F-29238, France\nAbstract\nFew-shot classiﬁcation is a challenging problem due to the uncertainty caused\nby using few labelled samples.\nIn the past few years, many methods have\nbeen proposed with the common aim of transferring knowledge acquired on a\npreviously solved task, what is often achieved by using a pretrained feature\nextractor. Following this vein, in this paper we propose a novel transfer-based\nmethod which aims at processing the feature vectors so that they become closer\nto Gaussian-like distributions, resulting in increased accuracy. In the case of\ntransductive few-shot learning where unlabelled test samples are available during\ntraining, we also introduce an optimal-transport inspired algorithm to boost\neven further the achieved performance. Using standardized vision benchmarks,\nwe show the ability of the proposed methodology to achieve state-of-the-art\naccuracy with various datasets, backbone architectures and few-shot settings.\nKeywords:\nFew-Shot learning, Inductive and Transductive Learning, Transfer\nLearning, Optimal Transport\n1. Introduction\nThanks to their outstanding performance, Deep Learning methods have\nbeen widely considered for vision tasks such as image classiﬁcation and object\ndetection. In order to reach top performance, these systems are typically trained\nusing very large labelled datasets that are representative enough of the inputs\nto be processed afterwards.\nPreprint submitted to Arxiv\nOctober 19, 2021\narXiv:2110.09446v1  [cs.LG]  18 Oct 2021\n",
    "However, in many applications, it is costly to acquire or to annotate data,\nresulting in the impossibility to create such large labelled datasets. Under this\ncondition, it is challenging to optimize Deep Learning architectures considering\nthe fact they typically are made of way more parameters than the dataset can\neﬃciently tune. This is the reason why in the past few years, few-shot learning\n(i.e. the problem of learning with few labelled examples) has become a trending\nresearch subject in the ﬁeld. In more details, there are two settings that authors\noften consider: a) “inductive few-shot”, where only a few labelled samples\nare available during training and prediction is performed on each test input\nindependently, and b) “transductive few-shot”, where prediction is performed\non a batch of (non-labelled) test inputs, allowing to take into account their joint\ndistribution.\nMany works in the domain are built based on a “learning to learn” guidance,\nwhere the pipeline is to train an optimizer [1, 2, 3] with diﬀerent tasks of\nlimited data so that the model is able to learn generic experience for novel\ntasks. Namely, the model learns a set of initialization parameters that are in an\nadvantageous position for the model to adapt to a new (small) dataset. Recently,\nthe trend evolved towards using well-thought-out transfer architectures (called\nbackbones) [4, 5, 6, 7, 8, 9] trained one time on the same training data, but seen\nas a unique large dataset.\nA main problem of using features extracted using a backbone pretrained\narchitecture is that their distribution is likely to be unorthodox, as the problem\nthe backbone has been optimized for most of the time diﬀers from that it is then\nused upon. As such, methods that rely on strong assumptions about the feature\ndistributions tend to have limitations on leveraging their quality. In this paper,\nwe propose an eﬃcient feature preprocessing methodology that allows to boost\nthe accuracy in few-shot transfer settings. In the case of transductive few-shot\nlearning, we also propose an optimal transport based algorithm that allows\nreaching even better performance. Using standardized benchmarks in the ﬁeld,\nwe demonstrate the ability of the proposed method to obtain state-of-the-art\naccuracy, for various problems and backbone architectures.\n2\n",
    "Oﬄine training of a generic\nfeature extractor using\na large available dataset\nProposed PEME-\nBMS to learn to\nclassify the considered\nfew-shot dataset\nFeature extraction\nPreprocessing\nBoosted Min-size Sinkhorn\nlarge dataset Dbase\nTrain feature extractor\nx 7→fϕ(x) ∈(R+)d\nfϕ\nPEME\nS ∪Q ∈Dnovel\nMin-size Sinkhorn\nInitialized wj\nWeight update\nnsteps\nPrediction\nfQ\nfS ∪fQ\nP\nwj\nAccuracy\nhj(k)\n˜hj(k)\nPEME\nFigure 1: Illustration of the proposed method. First we train a feature extractor fϕ using\nDbase that has a large number of labelled data. Then we extract feature vectors of all the\ninputs (support set S and query set Q) in Dnovel (the considered few-shot dataset). We\npreprocess them with proposed PEME, which contains power transform that has the eﬀect\nof mapping a skewed feature distribution into a gaussian-like distribution (hj(k) denotes the\nhistogram of feature k in class j). The result feature vectors are denoted by fS ∪fQ. In the\ncase of transductive learning, we introduce another step called Boosted Min-size Sinkhorn\n(BMS), where we perform a modiﬁed Sinkhorn algorithm with class weight parameters wj\ninitialized on labelled feature vectors fS to obtain the class allocation matrix P for the inputs,\nand we update the weight parameters for the next iteration. After nsteps we evaluate the\naccuracy on fQ.\n2. Related work\nA large volume of works in few-shot classiﬁcation is based on meta learning [3]\nmethods, where the training data is transformed into few-shot learning episodes\nto better ﬁt in the context of few examples. In this branch, optimization based\nmethods [3, 1, 2, 10, 11, 12] train a well-initialized optimizer so that it quickly\nadapts to unseen classes with a few epochs of training. Other works [13, 14] apply\ndata augmentation techniques to artiﬁcially increase the size of the training data\nin order for the model to generalize better to unseen data.\nIn the past few years, there have been a growing interest in transfer-based\nmethods. The main idea consists in training feature extractors able to eﬃciently\nsegregate novel classes it never saw before. For example, in [6] the authors train\nthe backbone with a distance-based classiﬁer [15] that takes into account the inter-\nclass distance. In [7], the authors utilize self-supervised learning techniques [16]\nto co-train an extra rotation classiﬁer for the output features, improving the\naccuracy in few-shot settings. Aside from approaches focused on training a\n3\n",
    "more robust model, other approaches are built on top of a pre-trained feature\nextractor (backbone). For instance, in [17] the authors implement a nearest class\nmean classiﬁer to associate an input with a class whose centroid is the closest\nin terms of the ℓ2 distance. In [18] an iterative approach is used to adjust the\nclass prototypes. In [8] the authors build a graph neural network to gather the\nfeature information from similar samples. Generally, transfer-based techniques\noften reach the best performance on standardized benchmarks.\nAlthough many works involve feature extraction, few have explored the\nfeatures in terms of their distribution [19, 20, 7]. Often, assumptions are made\nthat the features in a class align to a certain distribution, even though these\nassumptions are seldom experimentally discussed. In our work, we analyze the\nimpact of the features distributions and how they can be transformed for better\nprocessing and accuracy. We also introduce a new algorithm to improve the\nquality of the association between input features and corresponding classes in\ntypical few-shot settings.\nContributions. Let us highlight the main contributions of this work. (1)\nWe propose to preprocess the raw extracted features in order to make them more\naligned with Gaussian assumptions. Namely we introduce transforms of the\nfeatures so that they become less skewed. (2) We use a Wasserstein-based method\nto better align the distribution of features with that of the considered classes.\n(3) We show that the proposed method can bring large increase in accuracy with\na variety of feature extractors and datasets, leading to state-of-the-art results\nin the considered benchmarks. This work is an extended version of [9], with\nthe main diﬀerence that here we consider the broader case where we do not\nknow the proportion of samples belonging to each considered class in the case\nof transductive few-shot, leading to a new algorithm called Boosted Min-size\nSinkhorn. We also propose more eﬃcient preprocessing steps, leading to overall\nbetter performance in both inductive and transductive settings. Finally, we\nintroduce the use of Logistic Regression in our methodology instead of a simple\nNearest Class Mean classiﬁer.\n4\n",
    "3. Methodology\nIn this section we introduce the problem statement. We also discuss the\nvarious steps of the proposed method, including training the feature extractors,\npreprocessing the feature representations, and classifying them. Note that we\nmade the code of our method available at https://github.com/yhu01/BMS.\n3.1. Problem statement\nWe consider a typical few-shot learning problem. Namely, we are given a base\ndataset Dbase and a novel dataset Dnovel such that Dbase ∩Dnovel = ∅. Dbase\ncontains a large number of labelled examples from K diﬀerent classes and can\nbe used to train a generic feature extractor. Dnovel, also referred to as a task or\nepisode in other works, contains a small number of labelled examples (support\nset S), along with some unlabelled ones (query set Q), all from n new classes\nthat are distinct from the K classes in Dbase. Our goal is to predict the classes of\nunlabelled examples in the query set. The following parameters are of particular\nimportance to deﬁne such a few-shot problem: the number of classes in the\nnovel dataset n (called n-way), the number of labelled samples per class s (called\ns-shot) and the number of unlabelled samples per class q. Therefore, the novel\ndataset contains a total of l + u samples, where l = ns are labelled, and u = nq\nare unlabelled. In the case of inductive few-shot, the prediction is performed\nindependently on each one of the query samples. In the case of transductive\nfew-shot [21, 18], the prediction is performed considering all unlabelled samples\ntogether. Contrary to our previous work [9], we do not consider knowing the\nproportion of samples in each class in the case of transductive few-shot.\n3.2. Feature extraction\nThe ﬁrst step is to train a neural network backbone model using only the\nbase dataset. In this work we consider multiple backbones, with various training\nprocedures. Once the considered backbone is trained, we obtain robust embed-\ndings that should generalize well to novel classes. We denote by fϕ the backbone\nfunction, obtained by extracting the output of the penultimate layer from the\n5\n",
    "considered architecture, with ϕ being the trained architecture parameters. Thus\nconsidering an input vector x, fϕ(x) is a feature vector with d dimensions that\ncan be thought of as a simpler-to-manipulate representation of x. Note that\nimportantly, in all backbone architectures used in the experiments of this work,\nthe penultimate layers are obtained by applying a ReLU function, so that all\nfeature components coming out of fϕ are nonnegative.\n3.3. Feature preprocessing\nAs mentioned in Section 2, many works hypothesize, explicitly or not, that\nthe features from the same class are aligned with a speciﬁc distribution (often\nGaussian-like). But this aspect is rarely experimentally veriﬁed. In fact, it is very\nlikely that features obtained using the backbone architecture are not Gaussian.\nIndeed, usually the features are obtained after applying a ReLU function [22],\nand exhibit a positive and yet skewed distribution mostly concentrated around 0\n(more details can be found in the next section).\nMultiple works in the domain [17, 18] discuss the diﬀerent statistical methods\n(e.g. batch normalization) to better ﬁt the features into a model. Although\nthese methods may have provable assets for some distributions, they could\nworsen the process if applied to an unexpected input distribution.\nThis is\nwhy we propose to preprocess the obtained raw feature vectors so that they\nbetter align with typical distribution assumptions in the ﬁeld. Denote fϕ(x) =\n[f 1\nϕ(x), ..., f h\nϕ(x), ..., f d\nϕ(x)] ∈(R+)d, x ∈Dnovel as the obtained features on\nDnovel, and f h\nϕ(x), 1 ≤h ≤d denotes its value in the hth position.\nThe\npreprocessing methods applied in our proposed algorithms are as follows:\nEuclidean normalization. Also known as L2-normalization that is widely\nused in many related works [17, 20, 8], this step scales the features to the\nsame area so that large variance feature vectors do not predominate the others.\nEuclidean normalization can be given by:\nfϕ(x) ←\nfϕ(x)\n∥fϕ(x)∥2\n(1)\nPower transform. Power transform method [23, 9] simply consists of taking\n6\n",
    "the power of each feature vector coordinate. The formula is given by:\nf h\nϕ(x) ←(f h\nϕ(x) + ϵ)β, β ̸= 0\n(2)\nwhere ϵ = 1e −6 is used to make sure that fϕ(x) + ϵ is strictly positive in every\nposition, and β is a hyper-parameter. The rationale of the preprocessing above\nis that power transform, often used in combination with euclidean normalization,\nhas the functionality of reducing the skew of a distribution and mapping it to\na close-to-gaussian distribution, adjusted by β. After experiments, we found\nthat β = 0.5 gives the most consistent results for our considered experiments,\nwhich corresponds to a square-root function that has a wide range of usage on\nfeatures [24]. We will analyse this ability and the eﬀect of power transform in\nmore details in Section 4. Note that power transform can only be applied if\nconsidered feature vectors contain nonnegative entries, which will always be the\ncase in the remaining of this work.\nMean subtraction. With mean subtraction, each sample is translated\nusing m ∈(R+)d, the projection center. This is often used in combination with\neuclidean normalization in order to reduce the task bias and better align the\nfeature distributions [18]. The formula is given by:\nfϕ(x) ←fϕ(x) −m\n(3)\nThe projection center is often computed as the mean values of feature vectors\nrelated to the problem [17, 18]. In this paper we compute it either as the mean\nfeature vector of the base dataset (denoted as Mb) or the mean vector of the\nnovel dataset (denoted as Mn), depending on the few-shot settings. Of course,\nin both of these cases, the rationale is to consider a proxy to what would be the\nexact mean value of feature vectors on the considered task.\nIn our proposed method we deploy these preprocessing steps in the following\norder: Power transform (P) on the raw features, followed by an Euclidean\nnormalization (E). Then we perform Mean subtraction (M) followed by another\nEuclidean normalization at the end. For simplicity we denote PEME as our\nproposed preprocessing order, in which M can be either Mb or Mn as mentioned\n7\n",
    "above. In our experiments, we found that using Mb in the case of inductive few-\nshot learning and Mn in the case of transductive few-shot learning consistently led\nto the most competitive results. More details on why we used this methodology\nare available in the experiment section.\nWhen facing an inductive problem, a simple classiﬁer such as a Nearest-Class-\nMean classiﬁer (NCM) can be used directly after this preprocessing step. The\nresulting methodology is denoted PEMbE-NCM. But in the case of transductive\nsettings, we also introduce an iterative procedure, denoted BMS for Boosted\nMin-size Sinkhorn, meant to leverage the joint distribution of unlabelled samples.\nThe resulting methodology is denoted PEMnE-BMS. The details of the BMS\nprocedure are presented thereafter.\n3.4. Boosted Min-size Sinkhorn\nIn the case of transductive few-shot, we introduce a method that consists in\niteratively reﬁning estimates for the probability each unlabelled sample belong\nto any of the considered classes. This method is largely based on the one we\nintroduced in [9], except it does not require priors about samples distribution\nin each of the considered class. Denote i ∈[1, ..., l + u] as the sample index in\nDnovel and j ∈[1, ..., n] as the class index, the goal is to maximize the following\nlog post-posterior function:\nL(θ) =\nX\ni\nlog P(l(xi) = j|xi; θ)\n=\nX\ni\nlog P(xi, l(xi) = j; θ)\nP(xi; θ)\n∝\nX\ni\nlog P(xi|l(xi) = j; θ)\nP(xi; θ)\n,\n(4)\nhere l(xi) denotes the class label for sample xi ∈Q ∪S, P(xi; θ) denotes\nthe marginal probability, and θ represents the model parameters to estimate.\nAssuming a gaussian distribution on the input features for each class, here we\ndeﬁne θ = wj, ∀j where wj ∈Rd stand for the weight parameters for class j.\nWe observe that Eq. 4 can be related to the cost function utilized in Optimal\nTransport [25], which is often considered to solve classiﬁcation problems, with\n8\n",
    "constrains on the sample distribution over classes. To that end, a well-known\nSinkhorn [26] mapping method is proposed. The algorithm aims at computing\na class allocation matrix among novel class data for a minimum Wasserstein\ndistance. Namely, an allocation matrix P ∈R(l+u)×n\n+\nis deﬁned where P[i, j]\ndenotes the assigned portion for sample i to class j, and it is computed as follows:\nP = Sinkhorn(C, p, q, λ)\n= argmin\n˜P∈U(p,q)\nX\nij\n˜P[i, j]C[i, j] + λH(˜P),\n(5)\nwhere U(p, q) ∈R(l+u)×n\n+\nis a set of positive matrices for which the rows sum to\np and the columns sum to q, p denotes the distribution of the amount that each\nsample uses for class allocation, and q denotes the distribution of the amount of\nsamples allocated to each class. Therefore, U(p, q) contains all the possible ways\nof allocation. In the same equation, C can be viewed as a cost matrix that is of\nthe same size as P, each element in C indicates the cost of its corresponding\nposition in P. We will deﬁne the particular formula of the cost function for each\nposition C[i, j], ∀i, j in details later on in the section. As for the second term on\nthe right of 5, it stands for the entropy of ˜P: H(˜P) = −P\nij ˜P[i, j] log ˜P[i, j],\nregularized by a hyper-parameter λ. Increasing λ would force the entropy to\nbecome smaller, so that the mapping is less diluted. This term also makes the\nobjective function strictly convex [26, 27] and thus a practical and eﬀective\ncomputation. From lemma 2 in [26], the result of Sinkhorn allocation has the\ntypical form P = diag(u) · exp(−C/λ) · diag(v). It is worth noting that here we\nassume a soft class allocation, meaning that each sample can be “sliced” into\ndiﬀerent classes. We will present our proposed method in details in the next\nparagraphs.\nGiven all that are presented above, in this paper we propose an Expecta-\ntion–Maximization (EM ) [28] based method which alternates between updating\nthe allocation matrix P and estimating the parameter θ of the designed model,\nin order to minimize Eq. 5 and maximize Eq. 4. For a starter, we deﬁne a weight\nmatrix W with n columns (i.e one per class) and d rows (i.e one per dimension\n9\n",
    "of feature vectors), for column j in W we denote it as the weight parameters\nwj ∈Rd for class j in correspondence with Eq. 4. And it is initialized as follows:\nwj = W[:, j] = cj/∥cj∥2,\n(6)\nwhere\ncj = 1\ns\nX\nx∈S,ℓ(x)=j\nfϕ(x).\n(7)\nWe can see that W contains the average of feature vectors in the support set for\neach class, followed by a L2-normalization on each column so that ∥wj∥2 = 1, ∀j.\nThen, we iterate multiple steps that we describe thereafter.\na. Computing costs\nAs previously stated, the proposed algorithm is an EM -like one that iterately\nupdates model parameters for optimal estimates. Therefore, this step along with\nMin-size Sinkhorn presented in the next step, is considered as the E-step of our\nproposed method. The goal is to ﬁnd membership probabilities for the input\nsamples, namely, we compute P that minimizes Eq. 5.\nHere we assume gaussian distributions, features in each class have the same\nvariance and are independent from one another (covariance matrix Σ = Iσ2).\nWe observe that, ignoring the marginal probability, Eq. 4 can be boiled down to\nnegative L2 distances between extracted samples fϕ(xi), ∀i and wj, ∀j, which\nis initialized in Eq. 6 in our proposed method. Therefore, based on the fact\nthat wj and fϕ(xi) are both normalized to be unit length vectors (fϕ(xi) being\npreprocessed using PEME introduced in the previous section), here we deﬁne\nthe cost between sample i and class j to be the following equation:\nC[i, j] ∝(fϕ(xi) −wj)2\n= 1 −wT\nj fϕ(xi),\n(8)\nwhich corresponds to the cosine distance.\nb. Min-size Sinkhorn\nIn [9], we proposed a Wasserstein distance based method in which the\nSinkhorn algorithm is applied at each iteration so that the class prototypes are\n10\n",
    "Algorithm 1 Min-size Sinkhorn\nInputs: C, p = 1l+u, q = k1n, λ\nInitializations: P = Softmax(−λC)\nfor iter = 1 to 50 do\nP[i, :] ←p[i] ·\nP[i,:]\nP\nj P[i,j], ∀i\nP[:, j] ←q[j] ·\nP[:,j]\nP\ni P[i,j] if P\ni P[i, j] < q[j], ∀j\nend for\nreturn P\nupdated iteratively in order to ﬁnd their best estimates. Although the method\nshowed promising results, it is established on the condition that the distribution\nof the query set is known, e.g. a uniform distribution among classes on the query\nset. This is not ideal given the fact that any priors about Q should be supposedly\nkept unknown when applying a method. The methodology introduced in this\npaper can be seen as a generalization of that introduced in [9] that does not\nrequire priors about Q.\nIn the classical settings, Sinkhorn algorithm aims at ﬁnding the optimal\nmatrix P, given the cost matrix C and regulation parameter λ presented in\nEq. 4). Typically it initiates P from a softmax operation over the rows in C,\nthen it iterates between normalizing columns and rows of P, until the resulting\nmatrix becomes close-to doubly stochastic according to p and q. However, in\nour case we do not know the distribution of samples over classes. To address\nthis, we ﬁrstly introduce the parameter k, initialized so that k ←s, meant to\ntrack an estimate of the cardinal of the class containing the least number of\nsamples in the considered task. Then we propose the following modiﬁcation to\nbe applied to the matrix P once initialized: we normalize each row as in the\nclassical case, but only normalize the columns of P for which the sum is less\nthan the previously computed min-size k [18]. This ensures at least k elements\nallocated for each class, but not exactly k samples as in the balanced case.\nThe principle of this modiﬁed Sinkhorn solution is presented in Algorithm 1.\nc. Updating weights\n11\n",
    "This step is considered as the M -step of the proposed algorithm, in which\nwe use a variant of the Logistic Regression algorithm in order to ﬁnd the model\nparameter θ in the form of weight parameters wj for each class. Note that wj,\nif normalized, is equivalent to the prototype for class j in this case. Given the\nfact that in Eq. 4 we also take into account the marginal probability, which can\nbe further broken down as:\nP(xi; θ) =\nX\nj\nP(xi|l(xi) = j; θ)P(l(xi) = j),\n(9)\nwe observe that Eq. 4 corresponds to applying a softmax function on the negative\nlogits computed through a L2-distance function between samples and class\nprototypes (normalized). This ﬁts the formulation of a linear hypothesis between\nfϕ(xi) and wj for logit calculations, hence the rationale for utilizing Logistic\nRegression in our proposed method.\nThe procedure of this step is as follows: now that we have a polished allocation\nmatrix P, we ﬁrstly initialize the weights wj as follows:\nwj ←uj/∥uj∥2,\n(10)\nwhere\nuj ←\nX\ni\nP[i, j]fϕ(xi)/\nX\ni\nP[i, j].\n(11)\nWe can see that elements in P are used as coeﬃcients for feature vectors to\nlinearly adjust the class prototypes [9]. Similar to Eq. 6, here wj is the normalized\nnewly-computed class prototype that is a vector of length 1.\nNext we further adjust weights by applying a logistic regression, the opti-\nmization is performed by minimizing the following loss:\n1\nl + u ·\nX\ni\nX\nj\n−log(\nexp (S[i, j])\nPn\nγ=1 exp (S[i, γ])) · P[i, j],\n(12)\nwhere S ∈R(l+u)×n contains the logits, each element is computed as:\nS[i, j] = κ · wT\nj fϕ(xi)\n∥wj∥2\n.\n(13)\n12\n",
    "Note that κ is a scaling parameter, it can also be seen as a temperature parameter\nthat adjusts the conﬁdence metric to be associated to each sample. And it is\nlearnt jointly with W.\nThe deployed Logistic Regression comes with hyperparameters on its own.\nIn our experiments, we use an SGD optimizer with a gradient step of 0.1 and\n0.8 as the momentum parameter, and we train over e epochs. Here we point\nout that e ≥0 is considered an inﬂuential hyperparameter in our proposed\nalgorithm, e = 0 indicates a simple update of W as the normalized adjusted class\nprototypes (Eq. 10) computed from P in Eq. 11, without further adjustment of\nlogistic regression. And also note that when e > 0 we project columns of W to\nthe unit hypersphere at the end of each epoch.\nd. Estimating the class minimum size\nWe can now reﬁne our estimate for the min-size k for the next iteration. To\nthis end, we ﬁrstly compute the predicted label of each sample as follows:\nˆℓ(xi) = arg max\nj (P[i, j]),\n(14)\nwhich can be seen as the current (temporary) class prediction.\nThen, we compute:\nk = min\nj\n{kj},\n(15)\nwhere kj = #{i, ˆℓ(xi) = j}, #{·} representing the cardinal of a set.\nSummary of the proposed method. All steps of the proposed method\nare summarized in Algorithm 2. In our experiments, we also report the results\nobtained when using a prior about Q as in [9]. In this case, k does not have to\nbe estimated throughout the iterations and can be replaced with the actual exact\ntargets for the Sinkhorn. We denote this prior-dependent version PEMnE-BMS∗\n(with an added ∗).\n4. Experiments\n4.1. Datasets\nWe evaluate the performance of the proposed method using standardized few-\nshot classiﬁcation datasets: miniImageNet [29], tieredImageNet [30], CUB [31]\n13\n",
    "Algorithm 2 Boosted Min-size Sinkhorn (BMS)\nParameters: λ, e\nInputs: Preprocessed fϕ(x), ∀x ∈Dnovel = Q ∪S\nInitializations: W as normalized mean vectors over the support set for each\nclass (Eq. 6); Min-size k ←s.\nfor iter = 1 to 20 do\nCompute cost matrix C using W (Eq. 8). # E-step\nApply Min-size Sinkhorn to compute P (Algorithm 1). # E-step\nUpdate weights W using P with logistic regression (Eq. 10-13). # M-step\nEstimate class predictions ˆℓand min-size k using P (Eq. 14-15).\nend for\nreturn ˆℓ\nand CIFAR-FS [12]. The miniImageNet dataset contains 100 classes randomly\nchosen from ILSVRC- 2012 [32] and 600 images of size 84 × 84 pixels per\nclass. It is split into 64 base classes, 16 validation classes and 20 novel classes.\nThe tieredImageNet dataset is another subset of ImageNet, it consists of 34\nhigh-level categories with 608 classes in total. These categories are split into\n20 meta-training superclasses, 6 meta-validation superclasses and 8 meta-test\nsuperclasses, which corresponds to 351 base classes, 97 validation classes and\n160 novel classes respectively. The CUB dataset contains 200 classes of birds\nand has 11,788 images of size 84 × 84 pixels in total, it is split into 100 base\nclasses, 50 validation classes and 50 novel classes. The CIFAR-FS dataset has\n100 classes, each class contains 600 images of size 32 × 32 pixels. The splits of\nthis dataset are the same as those in miniImageNet.\n4.2. Implementation details\nIn order to stress the genericity of our proposed method with regards to the\nchosen backbone architecture and training strategy, we perform experiments\nusing WRN [33], ResNet18 and ResNet12 [34], along with some other pre-\ntrained backbones (e.g. DenseNet [35, 17]). For each dataset we train the\n14\n",
    "feature extractor with base classes and test the performance using novel classes.\nTherefore, for each test run, n classes are drawn uniformly at random among\nnovel classes. Among these n classes, s labelled examples and q unlabelled ex-\namples per class are uniformly drawn at random to form Dnovel. The WRN and\nResNet are trained following [7]. In the inductive setting, we use our proposed\npreprocessing steps PEMbE followed by a basic Nearest Class Mean (NCM)\nclassiﬁer. In the transductive setting, the preprocessing steps are denoted as\nPEMnE in that we use the mean vector of novel dataset for mean subtraction,\nfollowed by BMS or BMS∗depending on whether we have prior knowledge on\nthe distribution of query set Q among classes. Note that we perform a QR\ndecomposition on preprocessed features in order to speed up the computation for\nthe classiﬁer that follows. All our experiments are performed using n = 5, q = 15,\ns = 1 or 5. We run 10,000 random draws to obtain mean accuracy score and\nindicate conﬁdence scores (95%) when relevant. For our proposed PEMnE-BMS,\nwe train e = 0 epoch in the case of 1-shot and e = 40 epochs in the case of\n5-shot. As for PEMnE-BMS∗we set e = 20 for 1-shot and e = 40 for 5-shot.\nAs for the regularization parameter λ in Eq. 5, it is ﬁxed to 8.5 for all settings.\nImpact of these hyperparameters is detailed in the next sections.\n4.3. Comparison with state-of-the-art methods\nPerformance on standardized benchmarks. In the ﬁrst experiment, we\nconduct our proposed method on diﬀerent benchmarks and compare the perfor-\nmance with other state-of-the-art solutions. The results are presented in Table 1\nand 2, we observe that our method reaches the state-of-the-art performance\nin both inductive and transductive settings on all the few-shot classiﬁcation\nbenchmarks. Particularly, the proposed PEMnE-BMS∗brings important gains\nin both 1-shot and 5-shot settings, and the prior-independent PEMnE-BMS\nalso obtains competitive results on 5-shot. Note that for tieredImageNet we\nimplement our method based on a pre-trained DenseNet121 backbone following\nthe procedure described in [17]. From these experiments we conclude that the\nproposed method can bring an increase of accuracy with a variety of backbones\n15\n",
    "and datasets, leading to state-of-the-art performance. In terms of execution\ntime, we measured an average of 0.004s per run.\nPerformance on cross-domain settings. In this experiment we test our\nmethod in a cross-domain setting, where the backbone is trained with the base\nclasses in miniImageNet but tested with the novel classes in CUB dataset. As\nshown in Table 3, the proposed method gives the best accuracy both in the case\nof 1-shot and 5-shot, for both inductive and transductive settings.\n4.4. Ablation studies\nGeneralization to backbone architectures. To further stress the inter-\nest of the ingredients on the proposed method reaching top performance, in\nTable 4 we investigate the impact of our proposed method on diﬀerent backbone\narchitectures and benchmarks in the transductive setting. For comparison pur-\npose we also replace our proposed BMS algorithm with a standard K-Means\nalgorithm where class prototypes are initialized with the available labelled sam-\nples for each class. We can observe that: 1) the proposed method consistently\nachieves the best results for any ﬁxed backbone architecture, 2) the feature\nextractor trained on WRN outperforms the others with our proposed method on\ndiﬀerent benchmarks, 3) there are signiﬁcant drops in accuracy with K-Means,\nwhich stresses the interest of BMS, and 4) the prior on Q (BMS vs BMS∗) has\nmajor interest for 1-shot, boosting the performance by an approximation of 1%\non all tested feature extractors.\nPreprocessing impact. In Table 5 we compare our proposed PEME with\nother preprocessing techniques such as Batch Normalization and the ones being\nused in [17]. The experiment is conducted on miniImageNet (backbone: WRN).\nFor all that are put into comparison, we run either a NCM classiﬁer or BMS after\npreprocessing, depending on the settings. The obtained results clearly show the\ninterest of PEME compared with existing alternatives, we also observe that the\npower transform helps increase the accuracy on both inductive and transductive\nsettings. We will further study its impact in details.\nEﬀect of power transform. We ﬁrstly conduct a Gaussian hypothesis test\n16\n",
    "Table 1: 1-shot and 5-shot accuracy of state-of-the-art methods in the literature on miniIma-\ngeNet and tieredImageNet, compared with the proposed solution.\nminiImageNet\nSetting\nMethod\nBackbone\n1-shot\n5-shot\nInductive\nMatching Networks [29]\nWRN\n64.03 ± 0.20%\n76.32 ± 0.16%\nSimpleShot [17]\nDenseNet121\n64.29 ± 0.20%\n81.50 ± 0.14%\nS2M2 R [7]\nWRN\n64.93 ± 0.18%\n83.18 ± 0.11%\nPT+NCM [9]\nWRN\n65.35 ± 0.20%\n83.87 ± 0.13%\nDeepEMD[36]\nResNet12\n65.91 ± 0.82%\n82.41 ± 0.56%\nFEAT[37]\nResNet12\n66.78 ± 0.20%\n82.05 ± 0.14%\nPEMbE-NCM (ours)\nWRN\n68.43 ± 0.20%\n84.67 ± 0.13%\nTransductive\nBD-CSPN [38]\nWRN\n70.31 ± 0.93%\n81.89 ± 0.60%\nLaplacianShot [39]\nDenseNet121\n75.57 ± 0.19%\n87.72 ± 0.13%\nTransfer+SGC [8]\nWRN\n76.47 ± 0.23%\n85.23 ± 0.13%\nTAFSSL [18]\nDenseNet121\n77.06 ± 0.26%\n84.99 ± 0.14%\nTIM-GD [40]\nWRN\n77.80%\n87.40%\nMCT [41]\nResNet12\n78.55 ± 0.86%\n86.03 ± 0.42%\nEPNet [42]\nWRN\n79.22 ± 0.92%\n88.05 ± 0.51%\nPT+MAP [9]\nWRN\n82.92 ± 0.26%\n88.82 ± 0.13%\nPEMnE-BMS (ours)\nWRN\n82.07 ± 0.25%\n89.51 ± 0.13%\nPEMnE-BMS∗(ours)\nWRN\n83.35 ± 0.25%\n89.53 ± 0.13%\ntieredImageNet\nSetting\nMethod\nBackbone\n1-shot\n5-shot\nInductive\nProtoNet [43]\nConvNet4\n53.31 ± 0.89%\n72.69 ± 0.74%\nLEO [44]\nWRN\n66.33 ± 0.05%\n81.44 ± 0.09%\nSimpleShot [17]\nDenseNet121\n71.32 ± 0.22%\n86.66 ± 0.15%\nPT+NCM [9]\nDenseNet121\n69.96 ± 0.22%\n86.45 ± 0.15%\nFEAT[37]\nResNet12\n70.80 ± 0.23%\n84.79 ± 0.16%\nDeepEMD[36]\nResNet12\n71.16 ± 0.87%\n86.03 ± 0.58%\nRENet[45]\nResNet12\n71.61 ± 0.51%\n85.28 ± 0.35%\nPEMbE-NCM (ours)\nDenseNet121\n71.86 ± 0.21%\n87.09 ± 0.15%\nTransductive\nBD-CSPN [38]\nWRN\n78.74 ± 0.95%\n86.92 ± 0.63%\nLaplacianShot [39]\nDenseNet121\n80.30 ± 0.22%\n87.93 ± 0.15%\nMCT [41]\nResNet12\n82.32 ± 0.81%\n87.36 ± 0.50%\nTIM-GD [40]\nWRN\n82.10%\n89.80%\nTAFSSL [18]\nDenseNet121\n84.29 ± 0.25%\n89.31 ± 0.15%\nPT+MAP [9]\nDenseNet121\n85.75 ± 0.26%\n90.43 ± 0.14%\nPEMnE-BMS (ours)\nDenseNet121\n85.08 ± 0.25%\n91.08 ± 0.14%\nPEMnE-BMS∗(ours)\nDenseNet121\n86.07 ± 0.25%\n91.09 ± 0.14%\n17\n",
    "Table 2: 1-shot and 5-shot accuracy of state-of-the-art methods on CUB and CIFAR-FS.\nCUB\nSetting\nMethod\nBackbone\n1-shot\n5-shot\nInductive\nBaseline++ [6]\nResNet10\n69.55 ± 0.89%\n85.17 ± 0.50%\nMAML [1]\nResNet10\n70.32 ± 0.99%\n80.93 ± 0.71%\nProtoNet [43]\nResNet18\n72.99 ± 0.88%\n86.64 ± 0.51%\nMatching Networks [29]\nResNet18\n73.49 ± 0.89%\n84.45 ± 0.58%\nFEAT[37]\nResNet12\n73.27 ± 0.22%\n85.77 ± 0.14%\nDeepEMD[36]\nResNet12\n75.65 ± 0.83%\n88.69 ± 0.50%\nRENet[45]\nResNet12\n79.49 ± 0.44%\n91.11 ± 0.24%\nS2M2 R [7]\nWRN\n80.68 ± 0.81%\n90.85 ± 0.44%\nPT+NCM [9]\nWRN\n80.57 ± 0.20%\n91.15 ± 0.10%\nPEMbE-NCM (ours)\nWRN\n80.82 ± 0.19%\n91.46 ± 0.10%\nTransductive\nLaplacianShot [39]\nResNet18\n80.96%\n88.68%\nTIM-GD [40]\nResNet18\n82.20%\n90.80%\nBD-CSPN [38]\nWRN\n87.45%\n91.74%\nTransfer+SGC [8]\nWRN\n88.35 ± 0.19%\n92.14 ± 0.10%\nPT+MAP [9]\nWRN\n91.55 ± 0.19%\n93.99 ± 0.10%\nLST+MAP [46]\nWRN\n91.68 ± 0.19%\n94.09 ± 0.10%\nPEMnE-BMS (ours)\nWRN\n91.01 ± 0.19%\n94.60 ± 0.09%\nPEMnE-BMS∗(ours)\nWRN\n91.91 ± 0.18%\n94.62 ± 0.09%\nCIFAR-FS\nSetting\nMethod\nBackbone\n1-shot\n5-shot\nInductive\nProtoNet [43]\nConvNet64\n55.50 ± 0.70%\n72.00 ± 0.60%\nMAML [1]\nConvNet32\n58.90 ± 1.90%\n71.50 ± 1.00%\nRENet[45]\nResNet12\n74.51 ± 0.46%\n86.60 ± 0.32%\nBD-CSPN [38]\nWRN\n72.13 ± 1.01%\n82.28 ± 0.69%\nS2M2 R [7]\nWRN\n74.81 ± 0.19%\n87.47 ± 0.13%\nPT+NCM [9]\nWRN\n74.64 ± 0.21%\n87.64 ± 0.15%\nPEMbE-NCM (ours)\nWRN\n74.84 ± 0.21%\n87.73 ± 0.15%\nTransductive\nDSN-MR [47]\nResNet12\n78.00 ± 0.90%\n87.30 ± 0.60%\nTransfer+SGC [8]\nWRN\n83.90 ± 0.22%\n88.76 ± 0.15%\nMCT [41]\nResNet12\n87.28 ± 0.70%\n90.50 ± 0.43%\nPT+MAP [9]\nWRN\n87.69 ± 0.23%\n90.68 ± 0.15%\nLST+MAP [46]\nWRN\n87.79 ± 0.23%\n90.73 ± 0.15%\nPEMnE-BMS (ours)\nWRN\n86.93 ± 0.23%\n91.18 ± 0.15%\nPEMnE-BMS∗(ours)\nWRN\n87.83 ± 0.22%\n91.20 ± 0.15%\non each of the 640 coordinates of raw extracted features (backbone: WRN) for\neach of the 20 novel classes (dataset: miniImageNet). Following D’Agostino and\n18\n",
    "Table 3: 1-shot and 5-shot accuracy of state-of-the-art methods when performing cross-domain\nclassiﬁcation (backbone: WRN).\nSetting\nMethod\n1-shot\n5-shot\nInductive\nBaseline++ [6]\n40.44 ± 0.75%\n56.64 ± 0.72%\nManifold Mixup [48]\n46.21 ± 0.77%\n66.03 ± 0.71%\nS2M2 R [7]\n48.24 ± 0.84%\n70.44 ± 0.75%\nPT+NCM [9]\n48.37 ± 0.19%\n70.22 ± 0.17%\nPEMbE-NCM (ours)\n50.71 ± 0.19%\n73.15 ± 0.16%\nTransductive\nLaplacianShot [39]\n55.46%\n66.33%\nTransfer+SGC [8]\n58.63 ± 0.25%\n73.46 ± 0.17%\nPT+MAP [9]\n63.17 ± 0.31%\n76.43 ± 0.19%\nPEMnE-BMS (ours)\n62.93 ± 0.28%\n79.10 ± 0.18%\nPEMnE-BMS∗(ours)\n63.90 ± 0.31%\n79.15 ± 0.18%\nTable 4: 1-shot and 5-shot accuracy of proposed method on diﬀerent backbones and benchmarks.\nComparison with k-means algorithm.\nminiImageNet\nCUB\nCIFAR-FS\nMethod\nBackbone\n1-shot\n5-shot\n1-shot\n5-shot\n1-shot\n5-shot\nK-MEANS\nResNet12\n72.73 ± 0.23%\n84.05 ± 0.14%\n87.35 ± 0.19%\n92.31 ± 0.10%\n78.39 ± 0.24%\n85.73 ± 0.16%\nResNet18\n73.08 ± 0.22%\n84.67 ± 0.14%\n87.16 ± 0.19%\n91.97 ± 0.09%\n79.95 ± 0.23%\n86.74 ± 0.16%\nWRN\n76.67 ± 0.22%\n86.73 ± 0.13%\n88.28 ± 0.19%\n92.37 ± 0.10%\n83.69 ± 0.22%\n89.19 ± 0.15%\nBMS (ours)\nResNet12\n77.62 ± 0.28%\n86.95 ± 0.15%\n90.14 ± 0.19%\n94.30 ± 0.10%\n81.65 ± 0.25%\n88.38 ± 0.16%\nResNet18\n79.30 ± 0.27%\n87.94 ± 0.14%\n90.50 ± 0.19%\n94.29 ± 0.09%\n84.16 ± 0.24%\n89.39 ± 0.15%\nWRN\n82.07 ± 0.25%\n89.51 ± 0.13%\n91.01 ± 0.18%\n94.60 ± 0.09%\n86.93 ± 0.23%\n91.18 ± 0.15%\nBMS∗(ours)\nResNet12\n79.03 ± 0.28%\n87.01 ± 0.15%\n91.34 ± 0.19%\n94.32 ± 0.09%\n82.87 ± 0.27%\n88.43 ± 0.16%\nResNet18\n80.56 ± 0.27%\n87.98 ± 0.14%\n91.39 ± 0.19%\n94.31 ± 0.09%\n85.17 ± 0.25%\n89.42 ± 0.16%\nWRN\n83.35 ± 0.25%\n89.53 ± 0.13%\n91.91 ± 0.18%\n94.62 ± 0.09%\n87.83 ± 0.22%\n91.20 ± 0.15%\nPearson’s methodology [50, 51] and p = 1e −3, only one of the 640 × 20 = 12800\ntests return positive, suggesting a very low pass rate for raw features. However,\nafter applying the power transform we record a pass rate that surpasses 50%,\nsuggesting a considerably increased number of positive results for Gaussian tests.\nThis experiment shows the eﬀect of power transform being able to adjust feature\ndistributions into more gaussian-like ones.\nTo better show the eﬀect of this proposed technique on feature distributions,\nwe depict in Figure 2 the distributions of an arbitrarily selected feature for\n19\n",
    "Table 5: 1-shot and 5-shot accuracy on miniImageNet (backbone: WRN) with diﬀerent\npreprocessings on the extracted features.\nInductive (NCM)\nTransductive (BMS)\nPreprocessing\n1-shot\n5-shot\n1-shot\n5-shot\nNone\n55.30 ± 0.21%\n78.34 ± 0.15%\n77.62 ± 0.26%\n87.96 ± 0.13%\nBatch Norm [49]\n66.81 ± 0.20%\n83.57 ± 0.13%\n73.74 ± 0.21%\n88.07 ± 0.13%\nL2N [17]\n65.37 ± 0.20%\n83.46 ± 0.13%\n73.84 ± 0.21%\n88.15 ± 0.13%\nCL2N [17]\n63.88 ± 0.20%\n80.85 ± 0.14%\n73.12 ± 0.28%\n86.47 ± 0.15%\nEMbE\n68.05 ± 0.20%\n83.76 ± 0.13%\n80.28 ± 0.26%\n88.36 ± 0.13%\nPEMbE\n68.43 ± 0.20%\n84.67 ± 0.13%\n82.01 ± 0.26%\n89.50 ± 0.13%\nEMnE\n\\\n\\\n80.14 ± 0.27%\n88.39 ± 0.13%\nPEMnE\n\\\n\\\n82.07 ± 0.25%\n89.51 ± 0.13%\n3 randomly selected novel classes of miniImageNet when using WRN, before\nand after applying power transform.\nWe observe quite clearly that 1) raw\nfeatures exhibit a positive distribution mostly concentrated around 0, and 2)\npower transform is able to reshape the feature distributions to close-to-gaussian\ndistributions. We observe similar behaviors with other datasets as well. Moreover,\nin order to visualize the impact of this technique with respect to the position of\nfeature points, in Figure 3 we plot the feature vectors of randomly selected 3\nclasses from Dnovel. Note that all feature vectors in this experiment are reduced\nto a 3-dimensional ones corresponding to their largest eigenvalues. From Figure 3\nwe can observe that power transform, often followed by a L2-normalization, can\nhelp shape the class distributions to become more gathered and Gaussian-like [46].\nInﬂuence of the number of unlabelled samples. In order to better\nunderstand the gain in accuracy due to having access to more unlabelled samples,\nwe depict in Figure 4 the evolution of accuracy as a function of q, when the\nnumber of classes n = 5 is ﬁxed. Interestingly, the accuracy quickly reaches a\nclose-to-asymptotical plateau, emphasizing the ability of the method to quickly\nexploit available information in the task.\nInﬂuence of hyperparameters. In order to test how much impact the\nhyperparameters could have on our proposed method in terms of prediction\n20\n",
    "0\n0.1\n0.2\n0.3\n0.4\n0.5\n0\n20\n40\n60\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0\n20\n40\n60\n80\nClass 1\nClass 2\nClass 3\nFigure 2: Distributions of an arbitrarily chosen feature for 3 novel classes before (left) and\nafter (right) power transform.\nFigure 3: Plot of feature vectors (extracted from WRN) from randomly selected 3 classes.\n(left) Naive features. (right) Preprocessed features using power transform.\naccuracy, here we select two important hyperparameters that are used in BMS\nand observe their impact. Namely the number of training epochs e in logistic\nregression and the regulation parameter λ used for computing the prediction\nmatrix P. In Figure 5 we show the accuracy of our proposed method as a\nfunction of e (top) and λ (bottom). Results are reported for BMS∗in 1-shot\nsettings, and for BMS in 5-shot settings. From the ﬁgure we can see a slight\nuptick of accuracy as e or λ increases, followed by a downhill when they become\nlarger, implying an overﬁtting of the classiﬁer.\nProposed method on backbones pre-trained with external data. In\nthis experiment, we compare our proposed method BMS∗with the work in [52]\n21\n",
    "10\n30\n50\n75\n100\n150\n80\n85\n90\n5q\nAccuracy\ncub\ncifar −fs\nmini\nFigure 4: Accuracy of 5-way, 1-shot classiﬁcation setting on miniImageNet, CUB and CIFAR-FS\nas a function of q.\n0\n10\n20\n30\n40\n50\n60\n70\n80\n80\n85\n90\ne\nAccuracy\nBMS∗, 1-shot\nBMS, 5-shot\n3 4 5 6 7 8 9 10\n12\n15\n20\n82\n84\n86\n88\n90\nλ\nAccuracy\nBMS∗, 1-shot\nBMS, 5-shot\nFigure 5: Accuracy of proposed method on miniImageNet (backbone: WRN) as a function of\ntraining epoch e (top) and regulation parameter λ (bottom).\nthat pre-trains the backbone with the help of external illumination data for\naugmentation, followed by PT+MAP in [9] for class center estimation. Here\nwe use the same backbones as [52], and replace PT+MAP with our proposed\nBMS∗at the same conditions. Results are presented in Table 6. Note that we\nalso show the re-implemented results of [52], and our method reaches superior\nperformance on all tested benchmarks using external data in [52].\nProposed method on Few-Shot Open-Set Recognition.\nFew-Shot\nOpen-Set Recognition (FSOR) as a new trending topic deals with the fact that\nthere are open data mixed in query set Q that do not belong to any of the\n22\n",
    "Table 6: Proposed method on backbones pre-trained with external data. Note that -re denotes\nthe re-implementation of an existing method.\nBenchmark\nMethod\n1-shot\n5-shot\nminiImageNet\nIllu-Aug [52]\n82.99 ± 0.23%\n89.14 ± 0.12%\nIllu-Aug-re\n83.53 ± 0.25%\n89.38 ± 0.12%\nPEMnE-BMS∗(ours)\n83.85 ± 0.25%\n90.07 ± 0.12%\nCUB\nIllu-Aug [52]\n94.73 ± 0.14%\n96.28 ± 0.08%\nIllu-Aug-re\n94.63 ± 0.15%\n96.06 ± 0.08%\nPEMnE-BMS∗(ours)\n94.78 ± 0.15%\n96.43 ± 0.07%\nCIFAR-FS\nIllu-Aug [52]\n87.73 ± 0.22%\n91.09 ± 0.15%\nIllu-Aug-re\n87.76 ± 0.23%\n91.04 ± 0.15%\nPEMnE-BMS∗(ours)\n87.83 ± 0.23%\n91.49 ± 0.15%\nsupposed classes used for label predictions. Therefore this often requires a\nrobust classiﬁer that is able to classify correctly the non-open data as well as\nrejecting the open ones. In Table 7 we apply our proposed PEME for feature\npreprocessing, followed by an NCM classiﬁer, and compare the results with other\nstate-of-the-art alternatives. We observe that our proposed method is able to\nsurpass the others in terms of Accuracy and AUROC.\nTable 7: Accuracy and AUROC of Proposed method for Few-Shot Open-Set Recognition.\nminiImageNet\ntieredImageNet\n1-shot\n5-shot\n1-shot\n5-shot\nMethod\nAcc\nAUROC\nAcc\nAUROC\nAcc\nAUROC\nAcc\nAUROC\nProtoNet [43]\n64.01%\n51.81%\n80.09%\n60.39%\n68.26%\n60.73%\n83.40%\n64.96%\nFEAT [37]\n67.02%\n57.01%\n82.02%\n63.18%\n70.52%\n63.54%\n84.74%\n70.74%\nNN [53]\n63.82%\n56.96%\n80.12%\n63.43%\n67.73%\n62.70%\n83.43%\n69.77%\nOpenMax [54]\n63.69%\n62.64%\n80.56%\n62.27%\n68.28%\n60.13%\n83.48%\n65.51%\nPEELER [55]\n65.86%\n60.57%\n80.61%\n67.35%\n69.51%\n65.20%\n84.10%\n73.27%\nSnaTCHer [56]\n67.60%\n70.17%\n82.36%\n77.42%\n70.85%\n74.95%\n85.23%\n82.03%\nPEMbE-NCM (ours)\n68.43%\n72.10%\n84.67%\n80.04%\n71.87%\n75.44%\n87.09%\n83.85%\n23\n",
    "4.5. Proposed method on merged features\nIn this section we investigate the eﬀect of our proposed method on merged\nfeatures. Namely, we perform a direct concatenation of raw feature vectors\nextracted from multiple backbones at the beginning, followed by BMS. In\nTable 8 we chose the feature vectors from three backbones (WRN, ResNet18\nand ResNet12) and evaluated the performance with diﬀerent combinations. We\nobserve that 1) a direct concatenation, depending on the backbones, can bring\nabout 1% gain in both 1-shot and 5-shot settings compared with the results\nin Table 4 with feature vectors extracted from one single feature extractor. 2)\nBMS∗reached new state-of-the-art results on few-shot learning benchmarks with\nfeature vectors concatenated from WRN, ResNet18 and ResNet12, given that\nno external data is used.\nTo further study the impact of the number of backbones on prediction\naccuracy, in Figure 6 we depict the performance of our proposed method as a\nfunction of the number of backbones. Note that here we operate on feature\nvectors of 6 WRN backbones (dataset: miniImageNet) concatenated one after\nanother, which makes a total of 6 slots corresponding to a 640 × 6 = 3840\nfeature size. Each of them is trained the same way as in [7], and we randomly\nselect the multiples of 640 coordinates within the slots to denote the number\nof concatenated backbones used. The performance result is the average of 100\nrandom selections and we test with both BMS and BMS∗for 1-shot, and BMS∗\nfor 5-shot. From Figure 6 we observe that, as the number of backbones increases,\nthere is a relatively steady growth in terms of accuracy in multiple settings of\nour proposed method, indicating the interest of BMS in merged features.\n5. Conclusion\nIn this paper we introduced a new pipeline to solve the few-shot classiﬁcation\nproblem. Namely, we proposed to ﬁrstly preprocess the raw feature vectors\nto better align to a Gaussian distribution and then we designed an optimal-\ntransport inspired iterative algorithm to estimate the class prototypes for the\n24\n",
    "Table 8: 1-shot and 5-shot accuracy on miniImageNet, CUB and CIFAR-FS on our proposed\nPEMnE-BMS with multi-backbones (backbone training procedure follows [7], ’+’ denotes a\nconcatenation of backbone features).\nminiImageNet\nCUB\nCIFAR-FS\nBackbone\n1-shot\n5-shot\n1-shot\n5-shot\n1-shot\n5-shot\nRN18+RN12\n80.32%\n89.07%\n92.31%\n95.62%\n85.44%\n90.58%\nWRN+RN12\n82.63%\n90.43%\n92.69%\n95.96%\n87.11%\n91.50%\nWRN+RN18\n83.05%\n90.57%\n92.66%\n95.79%\n87.53%\n91.70%\nWRN+RN18+RN12\n82.90%\n90.64%\n93.32%\n96.31%\n87.62%\n91.84%\nWRN+RN18+RN12∗\n84.37%\n90.69%\n94.26%\n96.32%\n88.44%\n91.86%\n6×WRN∗\n85.54%\n91.53%\n\\\n\\\n\\\n\\\n∗: BMS∗.\n1\n2\n3\n4\n5\n6\n85\n90\n# of backbones\nAccuracy\nBMS, 1-shot\nBMS∗, 1-shot\nBMS∗, 5-shot\nFigure 6: Accuracy of proposed method in diﬀerent settings as a function of the number of\nbackbones (dataset: miniImageNet).\ntransductive setting. Our experimental results on standard vision benchmarks\nreach state-of-the-art accuracy, with important gains in both 1-shot and 5-shot\nclassiﬁcation settings. Moreover, the proposed method can bring gains with a\nvariety of feature extractors, with few extra hyperparameters. Thus we believe\nthat the proposed method is applicable to many practical problems. We also\nprovide two versions of our proposed method, one being prior-dependent and\none that does not require any knowledge on unlabelled data, and they both are\nable to bring important gains in accuracy.\n25\n",
    "References\n[1] C. Finn, P. Abbeel, S. Levine, Model-agnostic meta-learning for fast adapta-\ntion of deep networks, in: Proceedings of the 34th International Conference\non Machine Learning-Volume 70, JMLR. org, 2017, pp. 1126–1135.\n[2] S. Ravi, H. Larochelle, Optimization as a model for few-shot learning, in: 5th\nInternational Conference on Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Proceedings, OpenReview.net,\n2017.\nURL https://openreview.net/forum?id=rJY0-Kcll\n[3] S. Thrun, L. Pratt, Learning to learn, Springer Science & Business Media,\n2012.\n[4] L. Torrey, J. Shavlik, Transfer learning, in: Handbook of research on machine\nlearning applications and trends: algorithms, methods, and techniques, IGI\nGlobal, 2010, pp. 242–264.\n[5] D. Das, C. S. G. Lee, A two-stage approach to few-shot learning for image\nrecognition, IEEE Trans. Image Process. 29 (2020) 3336–3350. doi:10.\n1109/TIP.2019.2959254.\nURL https://doi.org/10.1109/TIP.2019.2959254\n[6] W. Chen, Y. Liu, Z. Kira, Y. F. Wang, J. Huang, A closer look at few-shot\nclassiﬁcation, in: 7th International Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019, OpenReview.net, 2019.\nURL https://openreview.net/forum?id=HkxLXnAcFQ\n[7] P. Mangla, N. Kumari, A. Sinha, M. Singh, B. Krishnamurthy, V. N.\nBalasubramanian, Charting the right manifold: Manifold mixup for few-\nshot learning, in: The IEEE Winter Conference on Applications of Computer\nVision, 2020, pp. 2218–2227.\n26\n",
    "[8] Y. Hu, V. Gripon, S. Pateux, Graph-based interpolation of feature vectors\nfor accurate few-shot classiﬁcation, in: 2020 25th International Conference\non Pattern Recognition (ICPR), IEEE, 2021, pp. 8164–8171.\n[9] Y. Hu, V. Gripon, S. Pateux, Leveraging the feature distribution in transfer-\nbased few-shot learning, in: International Conference on Artiﬁcial Neural\nNetworks, Springer, 2021, pp. 487–499.\n[10] Z. Li, F. Zhou, F. Chen, H. Li, Meta-sgd: Learning to learn quickly for few\nshot learning, CoRR abs/1707.09835. arXiv:1707.09835.\nURL http://arxiv.org/abs/1707.09835\n[11] A. Antoniou, H. Edwards, A. J. Storkey, How to train your MAML, in: 7th\nInternational Conference on Learning Representations, ICLR 2019, New\nOrleans, LA, USA, May 6-9, 2019, OpenReview.net, 2019.\nURL https://openreview.net/forum?id=HJGven05Y7\n[12] L. Bertinetto, J. F. Henriques, P. H. S. Torr, A. Vedaldi, Meta-learning\nwith diﬀerentiable closed-form solvers, in: 7th International Conference on\nLearning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019, OpenReview.net, 2019.\nURL https://openreview.net/forum?id=HyxnZh0ct7\n[13] H. Zhang, J. Zhang, P. Koniusz, Few-shot learning via saliency-guided hal-\nlucination of samples, in: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2019, pp. 2770–2779.\n[14] Z. Chen, Y. Fu, Y.-X. Wang, L. Ma, W. Liu, M. Hebert, Image deformation\nmeta-networks for one-shot learning, in: Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2019, pp. 8680–8689.\n[15] T. Mensink, J. Verbeek, F. Perronnin, G. Csurka, Metric learning for large\nscale image classiﬁcation: Generalizing to new classes at near-zero cost, in:\nEuropean Conference on Computer Vision, Springer, 2012, pp. 488–501.\n27\n",
    "[16] O. Chapelle, B. Scholkopf, A. Zien, Semi-supervised learning (chapelle, o.\net al., eds.; 2006)[book reviews], IEEE Transactions on Neural Networks\n20 (3) (2009) 542–542.\n[17] Y. Wang, W. Chao, K. Q. Weinberger, L. van der Maaten, Simpleshot:\nRevisiting nearest-neighbor classiﬁcation for few-shot learning, CoRR\nabs/1911.04623. arXiv:1911.04623.\nURL http://arxiv.org/abs/1911.04623\n[18] M. Lichtenstein, P. Sattigeri, R. Feris, R. Giryes, L. Karlinsky, Tafssl: Task-\nadaptive feature sub-space learning for few-shot classiﬁcation, in: European\nConference on Computer Vision, Springer, 2020, pp. 522–539.\n[19] V. Gripon, G. B. Hacene, M. L¨owe, F. Vermet, Improving accuracy of non-\nparametric transfer learning via vector segmentation, in: 2018 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Processing (ICASSP),\n2018, pp. 2966–2970.\n[20] S. Yang, L. Liu, M. Xu, Free lunch for few-shot learning: Distribution\ncalibration, in: 9th International Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021, OpenReview.net, 2021.\nURL https://openreview.net/forum?id=JWOiYxMG92s\n[21] Y. Liu, J. Lee, M. Park, S. Kim, E. Yang, S. J. Hwang, Y. Yang, Learning to\npropagate labels: Transductive propagation network for few-shot learning,\nin: 7th International Conference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019, OpenReview.net, 2019.\nURL https://openreview.net/forum?id=SyVuRiC5K7\n[22] A. F. Agarap, Deep learning using rectiﬁed linear units (relu), CoRR\nabs/1803.08375. arXiv:1803.08375.\nURL http://arxiv.org/abs/1803.08375\n[23] J. W. Tukey, Exploratory data analysis, Vol. 2, Reading, Mass., 1977.\n28\n",
    "[24] R. G. Cinbis, J. Verbeek, C. Schmid, Approximate ﬁsher kernels of non-\niid image models for image categorization, IEEE transactions on pattern\nanalysis and machine intelligence 38 (6) (2015) 1084–1098.\n[25] C. Villani, Optimal transport: old and new, Vol. 338, Springer Science &\nBusiness Media, 2008.\n[26] M. Cuturi, Sinkhorn distances: Lightspeed computation of optimal trans-\nport, in: Advances in neural information processing systems, 2013, pp.\n2292–2300.\n[27] J. Solomon, F. De Goes, G. Peyr´e, M. Cuturi, A. Butscher, A. Nguyen,\nT. Du, L. Guibas, Convolutional wasserstein distances: Eﬃcient optimal\ntransportation on geometric domains, ACM Transactions on Graphics\n(TOG) 34 (4) (2015) 1–11.\n[28] A. P. Dempster, N. M. Laird, D. B. Rubin, Maximum likelihood from\nincomplete data via the em algorithm, Journal of the Royal Statistical\nSociety: Series B (Methodological) 39 (1) (1977) 1–22.\n[29] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al., Matching networks\nfor one shot learning, in: Advances in neural information processing systems,\n2016, pp. 3630–3638.\n[30] M. Ren, E. Triantaﬁllou, S. Ravi, J. Snell, K. Swersky, J. B. Tenenbaum,\nH. Larochelle, R. S. Zemel, Meta-learning for semi-supervised few-shot\nclassiﬁcation, in: 6th International Conference on Learning Representations,\nICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference\nTrack Proceedings, OpenReview.net, 2018.\nURL https://openreview.net/forum?id=HJcSzz-CZ\n[31] C. Wah, S. Branson, P. Welinder, P. Perona, S. Belongie, The Caltech-UCSD\nBirds-200-2011 Dataset, Tech. Rep. CNS-TR-2011-001, California Institute\nof Technology (2011).\n29\n",
    "[32] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,\nA. Karpathy, A. Khosla, M. Bernstein, et al., Imagenet large scale visual\nrecognition challenge, International journal of computer vision 115 (3) (2015)\n211–252.\n[33] S. Zagoruyko, N. Komodakis, Wide residual networks, in: R. C. Wilson,\nE. R. Hancock, W. A. P. Smith (Eds.), Proceedings of the British Machine\nVision Conference 2016, BMVC 2016, York, UK, September 19-22, 2016,\nBMVA Press, 2016.\nURL http://www.bmva.org/bmvc/2016/papers/paper087/index.html\n[34] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition,\nin: Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2016, pp. 770–778.\n[35] G. Huang, Z. Liu, L. Van Der Maaten, K. Q. Weinberger, Densely connected\nconvolutional networks, in: Proceedings of the IEEE conference on computer\nvision and pattern recognition, 2017, pp. 4700–4708.\n[36] C. Zhang, Y. Cai, G. Lin, C. Shen, Deepemd: Few-shot image classiﬁcation\nwith diﬀerentiable earth mover’s distance and structured classiﬁers, in:\nProceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, 2020, pp. 12203–12213.\n[37] H.-J. Ye, H. Hu, D.-C. Zhan, F. Sha, Few-shot learning via embedding\nadaptation with set-to-set functions, in: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2020, pp. 8808–\n8817.\n[38] J. Liu, L. Song, Y. Qin, Prototype rectiﬁcation for few-shot learning, in:\nComputer Vision–ECCV 2020: 16th European Conference, Glasgow, UK,\nAugust 23–28, 2020, Proceedings, Part I 16, Springer, 2020, pp. 741–756.\n[39] I. Ziko, J. Dolz, E. Granger, I. B. Ayed, Laplacian regularized few-shot\n30\n",
    "learning, in: International Conference on Machine Learning, PMLR, 2020,\npp. 11660–11670.\n[40] M. Boudiaf, I. M. Ziko, J. Rony, J. Dolz, P. Piantanida, I. B. Ayed, Transduc-\ntive information maximization for few-shot learning, CoRR abs/2008.11297.\narXiv:2008.11297.\nURL https://arxiv.org/abs/2008.11297\n[41] S. M. Kye, H. Lee, H. Kim, S. J. Hwang, Transductive few-shot learning\nwith meta-learned conﬁdence, CoRR abs/2002.12017. arXiv:2002.12017.\nURL https://arxiv.org/abs/2002.12017\n[42] P. Rodr´ıguez, I. Laradji, A. Drouin, A. Lacoste, Embedding propagation:\nSmoother manifold for few-shot classiﬁcation, in: European Conference on\nComputer Vision, Springer, 2020, pp. 121–138.\n[43] J. Snell, K. Swersky, R. Zemel, Prototypical networks for few-shot learning,\nin: Advances in Neural Information Processing Systems, 2017, pp. 4077–\n4087.\n[44] A. A. Rusu, D. Rao, J. Sygnowski, O. Vinyals, R. Pascanu, S. Osindero,\nR. Hadsell, Meta-learning with latent embedding optimization, in: 7th\nInternational Conference on Learning Representations, ICLR 2019, New\nOrleans, LA, USA, May 6-9, 2019, OpenReview.net, 2019.\nURL https://openreview.net/forum?id=BJgklhAcK7\n[45] D. Kang, H. Kwon, J. Min, M. Cho, Relational embedding for few-shot\nclassiﬁcation, CoRR abs/2108.09666. arXiv:2108.09666.\nURL https://arxiv.org/abs/2108.09666\n[46] T. Chobola, D. Vasata, P. Kord´ık, Transfer learning based few-shot classiﬁ-\ncation using optimal transport mapping from preprocessed latent space of\nbackbone neural network, CoRR abs/2102.05176. arXiv:2102.05176.\nURL https://arxiv.org/abs/2102.05176\n31\n",
    "[47] C. Simon, P. Koniusz, R. Nock, M. Harandi, Adaptive subspaces for few-shot\nlearning, in: Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2020, pp. 4136–4145.\n[48] V. Verma, A. Lamb, C. Beckham, A. Najaﬁ, I. Mitliagkas, D. Lopez-Paz,\nY. Bengio, Manifold mixup: Better representations by interpolating hidden\nstates, in: International Conference on Machine Learning, PMLR, 2019, pp.\n6438–6447.\n[49] S. Ioﬀe, C. Szegedy, Batch normalization: Accelerating deep network train-\ning by reducing internal covariate shift, in: International conference on\nmachine learning, PMLR, 2015, pp. 448–456.\n[50] R. DIAgostino, An omnibus test of normality for moderate and large sample\nsizes, Biometrika 58 (34) (1971) 1–348.\n[51] R. D’AGOSTINO, E. S. Pearson, Tests for departure from normality. em-\npirical results for the distributions of b 2 and\n√\nb, Biometrika 60 (3) (1973)\n613–622.\n[52] H. Zhang, Z. Cao, Z. Yan, C. Zhang, Sill-net: Feature augmentation with\nseparated illumination representation, CoRR abs/2102.03539. arXiv:2102.\n03539.\nURL https://arxiv.org/abs/2102.03539\n[53] P. R. M. J´unior, R. M. De Souza, R. d. O. Werneck, B. V. Stein, D. V.\nPazinato, W. R. de Almeida, O. A. Penatti, R. d. S. Torres, A. Rocha,\nNearest neighbors distance ratio open-set classiﬁer, Machine Learning 106 (3)\n(2017) 359–386.\n[54] A. Bendale, T. E. Boult, Towards open set deep networks, in: Proceedings\nof the IEEE conference on computer vision and pattern recognition, 2016,\npp. 1563–1572.\n32\n",
    "[55] B. Liu, H. Kang, H. Li, G. Hua, N. Vasconcelos, Few-shot open-set recogni-\ntion using meta-learning, in: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2020, pp. 8798–8807.\n[56] M. Jeong, S. Choi, C. Kim, Few-shot open-set recognition by transformation\nconsistency, in: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2021, pp. 12566–12575.\n33\n"
  ],
  "full_text": "Squeezing Backbone Feature Distributions to the Max\nfor Eﬃcient Few-Shot Learning\nYuqing Hua,b, St´ephane Pateuxa, Vincent Griponb\naOrange Labs, Rennes, France\nbIMT Atlantique, Lab-STICC, UMR CNRS 6285, F-29238, France\nAbstract\nFew-shot classiﬁcation is a challenging problem due to the uncertainty caused\nby using few labelled samples.\nIn the past few years, many methods have\nbeen proposed with the common aim of transferring knowledge acquired on a\npreviously solved task, what is often achieved by using a pretrained feature\nextractor. Following this vein, in this paper we propose a novel transfer-based\nmethod which aims at processing the feature vectors so that they become closer\nto Gaussian-like distributions, resulting in increased accuracy. In the case of\ntransductive few-shot learning where unlabelled test samples are available during\ntraining, we also introduce an optimal-transport inspired algorithm to boost\neven further the achieved performance. Using standardized vision benchmarks,\nwe show the ability of the proposed methodology to achieve state-of-the-art\naccuracy with various datasets, backbone architectures and few-shot settings.\nKeywords:\nFew-Shot learning, Inductive and Transductive Learning, Transfer\nLearning, Optimal Transport\n1. Introduction\nThanks to their outstanding performance, Deep Learning methods have\nbeen widely considered for vision tasks such as image classiﬁcation and object\ndetection. In order to reach top performance, these systems are typically trained\nusing very large labelled datasets that are representative enough of the inputs\nto be processed afterwards.\nPreprint submitted to Arxiv\nOctober 19, 2021\narXiv:2110.09446v1  [cs.LG]  18 Oct 2021\n\n\nHowever, in many applications, it is costly to acquire or to annotate data,\nresulting in the impossibility to create such large labelled datasets. Under this\ncondition, it is challenging to optimize Deep Learning architectures considering\nthe fact they typically are made of way more parameters than the dataset can\neﬃciently tune. This is the reason why in the past few years, few-shot learning\n(i.e. the problem of learning with few labelled examples) has become a trending\nresearch subject in the ﬁeld. In more details, there are two settings that authors\noften consider: a) “inductive few-shot”, where only a few labelled samples\nare available during training and prediction is performed on each test input\nindependently, and b) “transductive few-shot”, where prediction is performed\non a batch of (non-labelled) test inputs, allowing to take into account their joint\ndistribution.\nMany works in the domain are built based on a “learning to learn” guidance,\nwhere the pipeline is to train an optimizer [1, 2, 3] with diﬀerent tasks of\nlimited data so that the model is able to learn generic experience for novel\ntasks. Namely, the model learns a set of initialization parameters that are in an\nadvantageous position for the model to adapt to a new (small) dataset. Recently,\nthe trend evolved towards using well-thought-out transfer architectures (called\nbackbones) [4, 5, 6, 7, 8, 9] trained one time on the same training data, but seen\nas a unique large dataset.\nA main problem of using features extracted using a backbone pretrained\narchitecture is that their distribution is likely to be unorthodox, as the problem\nthe backbone has been optimized for most of the time diﬀers from that it is then\nused upon. As such, methods that rely on strong assumptions about the feature\ndistributions tend to have limitations on leveraging their quality. In this paper,\nwe propose an eﬃcient feature preprocessing methodology that allows to boost\nthe accuracy in few-shot transfer settings. In the case of transductive few-shot\nlearning, we also propose an optimal transport based algorithm that allows\nreaching even better performance. Using standardized benchmarks in the ﬁeld,\nwe demonstrate the ability of the proposed method to obtain state-of-the-art\naccuracy, for various problems and backbone architectures.\n2\n\n\nOﬄine training of a generic\nfeature extractor using\na large available dataset\nProposed PEME-\nBMS to learn to\nclassify the considered\nfew-shot dataset\nFeature extraction\nPreprocessing\nBoosted Min-size Sinkhorn\nlarge dataset Dbase\nTrain feature extractor\nx 7→fϕ(x) ∈(R+)d\nfϕ\nPEME\nS ∪Q ∈Dnovel\nMin-size Sinkhorn\nInitialized wj\nWeight update\nnsteps\nPrediction\nfQ\nfS ∪fQ\nP\nwj\nAccuracy\nhj(k)\n˜hj(k)\nPEME\nFigure 1: Illustration of the proposed method. First we train a feature extractor fϕ using\nDbase that has a large number of labelled data. Then we extract feature vectors of all the\ninputs (support set S and query set Q) in Dnovel (the considered few-shot dataset). We\npreprocess them with proposed PEME, which contains power transform that has the eﬀect\nof mapping a skewed feature distribution into a gaussian-like distribution (hj(k) denotes the\nhistogram of feature k in class j). The result feature vectors are denoted by fS ∪fQ. In the\ncase of transductive learning, we introduce another step called Boosted Min-size Sinkhorn\n(BMS), where we perform a modiﬁed Sinkhorn algorithm with class weight parameters wj\ninitialized on labelled feature vectors fS to obtain the class allocation matrix P for the inputs,\nand we update the weight parameters for the next iteration. After nsteps we evaluate the\naccuracy on fQ.\n2. Related work\nA large volume of works in few-shot classiﬁcation is based on meta learning [3]\nmethods, where the training data is transformed into few-shot learning episodes\nto better ﬁt in the context of few examples. In this branch, optimization based\nmethods [3, 1, 2, 10, 11, 12] train a well-initialized optimizer so that it quickly\nadapts to unseen classes with a few epochs of training. Other works [13, 14] apply\ndata augmentation techniques to artiﬁcially increase the size of the training data\nin order for the model to generalize better to unseen data.\nIn the past few years, there have been a growing interest in transfer-based\nmethods. The main idea consists in training feature extractors able to eﬃciently\nsegregate novel classes it never saw before. For example, in [6] the authors train\nthe backbone with a distance-based classiﬁer [15] that takes into account the inter-\nclass distance. In [7], the authors utilize self-supervised learning techniques [16]\nto co-train an extra rotation classiﬁer for the output features, improving the\naccuracy in few-shot settings. Aside from approaches focused on training a\n3\n\n\nmore robust model, other approaches are built on top of a pre-trained feature\nextractor (backbone). For instance, in [17] the authors implement a nearest class\nmean classiﬁer to associate an input with a class whose centroid is the closest\nin terms of the ℓ2 distance. In [18] an iterative approach is used to adjust the\nclass prototypes. In [8] the authors build a graph neural network to gather the\nfeature information from similar samples. Generally, transfer-based techniques\noften reach the best performance on standardized benchmarks.\nAlthough many works involve feature extraction, few have explored the\nfeatures in terms of their distribution [19, 20, 7]. Often, assumptions are made\nthat the features in a class align to a certain distribution, even though these\nassumptions are seldom experimentally discussed. In our work, we analyze the\nimpact of the features distributions and how they can be transformed for better\nprocessing and accuracy. We also introduce a new algorithm to improve the\nquality of the association between input features and corresponding classes in\ntypical few-shot settings.\nContributions. Let us highlight the main contributions of this work. (1)\nWe propose to preprocess the raw extracted features in order to make them more\naligned with Gaussian assumptions. Namely we introduce transforms of the\nfeatures so that they become less skewed. (2) We use a Wasserstein-based method\nto better align the distribution of features with that of the considered classes.\n(3) We show that the proposed method can bring large increase in accuracy with\na variety of feature extractors and datasets, leading to state-of-the-art results\nin the considered benchmarks. This work is an extended version of [9], with\nthe main diﬀerence that here we consider the broader case where we do not\nknow the proportion of samples belonging to each considered class in the case\nof transductive few-shot, leading to a new algorithm called Boosted Min-size\nSinkhorn. We also propose more eﬃcient preprocessing steps, leading to overall\nbetter performance in both inductive and transductive settings. Finally, we\nintroduce the use of Logistic Regression in our methodology instead of a simple\nNearest Class Mean classiﬁer.\n4\n\n\n3. Methodology\nIn this section we introduce the problem statement. We also discuss the\nvarious steps of the proposed method, including training the feature extractors,\npreprocessing the feature representations, and classifying them. Note that we\nmade the code of our method available at https://github.com/yhu01/BMS.\n3.1. Problem statement\nWe consider a typical few-shot learning problem. Namely, we are given a base\ndataset Dbase and a novel dataset Dnovel such that Dbase ∩Dnovel = ∅. Dbase\ncontains a large number of labelled examples from K diﬀerent classes and can\nbe used to train a generic feature extractor. Dnovel, also referred to as a task or\nepisode in other works, contains a small number of labelled examples (support\nset S), along with some unlabelled ones (query set Q), all from n new classes\nthat are distinct from the K classes in Dbase. Our goal is to predict the classes of\nunlabelled examples in the query set. The following parameters are of particular\nimportance to deﬁne such a few-shot problem: the number of classes in the\nnovel dataset n (called n-way), the number of labelled samples per class s (called\ns-shot) and the number of unlabelled samples per class q. Therefore, the novel\ndataset contains a total of l + u samples, where l = ns are labelled, and u = nq\nare unlabelled. In the case of inductive few-shot, the prediction is performed\nindependently on each one of the query samples. In the case of transductive\nfew-shot [21, 18], the prediction is performed considering all unlabelled samples\ntogether. Contrary to our previous work [9], we do not consider knowing the\nproportion of samples in each class in the case of transductive few-shot.\n3.2. Feature extraction\nThe ﬁrst step is to train a neural network backbone model using only the\nbase dataset. In this work we consider multiple backbones, with various training\nprocedures. Once the considered backbone is trained, we obtain robust embed-\ndings that should generalize well to novel classes. We denote by fϕ the backbone\nfunction, obtained by extracting the output of the penultimate layer from the\n5\n\n\nconsidered architecture, with ϕ being the trained architecture parameters. Thus\nconsidering an input vector x, fϕ(x) is a feature vector with d dimensions that\ncan be thought of as a simpler-to-manipulate representation of x. Note that\nimportantly, in all backbone architectures used in the experiments of this work,\nthe penultimate layers are obtained by applying a ReLU function, so that all\nfeature components coming out of fϕ are nonnegative.\n3.3. Feature preprocessing\nAs mentioned in Section 2, many works hypothesize, explicitly or not, that\nthe features from the same class are aligned with a speciﬁc distribution (often\nGaussian-like). But this aspect is rarely experimentally veriﬁed. In fact, it is very\nlikely that features obtained using the backbone architecture are not Gaussian.\nIndeed, usually the features are obtained after applying a ReLU function [22],\nand exhibit a positive and yet skewed distribution mostly concentrated around 0\n(more details can be found in the next section).\nMultiple works in the domain [17, 18] discuss the diﬀerent statistical methods\n(e.g. batch normalization) to better ﬁt the features into a model. Although\nthese methods may have provable assets for some distributions, they could\nworsen the process if applied to an unexpected input distribution.\nThis is\nwhy we propose to preprocess the obtained raw feature vectors so that they\nbetter align with typical distribution assumptions in the ﬁeld. Denote fϕ(x) =\n[f 1\nϕ(x), ..., f h\nϕ(x), ..., f d\nϕ(x)] ∈(R+)d, x ∈Dnovel as the obtained features on\nDnovel, and f h\nϕ(x), 1 ≤h ≤d denotes its value in the hth position.\nThe\npreprocessing methods applied in our proposed algorithms are as follows:\nEuclidean normalization. Also known as L2-normalization that is widely\nused in many related works [17, 20, 8], this step scales the features to the\nsame area so that large variance feature vectors do not predominate the others.\nEuclidean normalization can be given by:\nfϕ(x) ←\nfϕ(x)\n∥fϕ(x)∥2\n(1)\nPower transform. Power transform method [23, 9] simply consists of taking\n6\n\n\nthe power of each feature vector coordinate. The formula is given by:\nf h\nϕ(x) ←(f h\nϕ(x) + ϵ)β, β ̸= 0\n(2)\nwhere ϵ = 1e −6 is used to make sure that fϕ(x) + ϵ is strictly positive in every\nposition, and β is a hyper-parameter. The rationale of the preprocessing above\nis that power transform, often used in combination with euclidean normalization,\nhas the functionality of reducing the skew of a distribution and mapping it to\na close-to-gaussian distribution, adjusted by β. After experiments, we found\nthat β = 0.5 gives the most consistent results for our considered experiments,\nwhich corresponds to a square-root function that has a wide range of usage on\nfeatures [24]. We will analyse this ability and the eﬀect of power transform in\nmore details in Section 4. Note that power transform can only be applied if\nconsidered feature vectors contain nonnegative entries, which will always be the\ncase in the remaining of this work.\nMean subtraction. With mean subtraction, each sample is translated\nusing m ∈(R+)d, the projection center. This is often used in combination with\neuclidean normalization in order to reduce the task bias and better align the\nfeature distributions [18]. The formula is given by:\nfϕ(x) ←fϕ(x) −m\n(3)\nThe projection center is often computed as the mean values of feature vectors\nrelated to the problem [17, 18]. In this paper we compute it either as the mean\nfeature vector of the base dataset (denoted as Mb) or the mean vector of the\nnovel dataset (denoted as Mn), depending on the few-shot settings. Of course,\nin both of these cases, the rationale is to consider a proxy to what would be the\nexact mean value of feature vectors on the considered task.\nIn our proposed method we deploy these preprocessing steps in the following\norder: Power transform (P) on the raw features, followed by an Euclidean\nnormalization (E). Then we perform Mean subtraction (M) followed by another\nEuclidean normalization at the end. For simplicity we denote PEME as our\nproposed preprocessing order, in which M can be either Mb or Mn as mentioned\n7\n\n\nabove. In our experiments, we found that using Mb in the case of inductive few-\nshot learning and Mn in the case of transductive few-shot learning consistently led\nto the most competitive results. More details on why we used this methodology\nare available in the experiment section.\nWhen facing an inductive problem, a simple classiﬁer such as a Nearest-Class-\nMean classiﬁer (NCM) can be used directly after this preprocessing step. The\nresulting methodology is denoted PEMbE-NCM. But in the case of transductive\nsettings, we also introduce an iterative procedure, denoted BMS for Boosted\nMin-size Sinkhorn, meant to leverage the joint distribution of unlabelled samples.\nThe resulting methodology is denoted PEMnE-BMS. The details of the BMS\nprocedure are presented thereafter.\n3.4. Boosted Min-size Sinkhorn\nIn the case of transductive few-shot, we introduce a method that consists in\niteratively reﬁning estimates for the probability each unlabelled sample belong\nto any of the considered classes. This method is largely based on the one we\nintroduced in [9], except it does not require priors about samples distribution\nin each of the considered class. Denote i ∈[1, ..., l + u] as the sample index in\nDnovel and j ∈[1, ..., n] as the class index, the goal is to maximize the following\nlog post-posterior function:\nL(θ) =\nX\ni\nlog P(l(xi) = j|xi; θ)\n=\nX\ni\nlog P(xi, l(xi) = j; θ)\nP(xi; θ)\n∝\nX\ni\nlog P(xi|l(xi) = j; θ)\nP(xi; θ)\n,\n(4)\nhere l(xi) denotes the class label for sample xi ∈Q ∪S, P(xi; θ) denotes\nthe marginal probability, and θ represents the model parameters to estimate.\nAssuming a gaussian distribution on the input features for each class, here we\ndeﬁne θ = wj, ∀j where wj ∈Rd stand for the weight parameters for class j.\nWe observe that Eq. 4 can be related to the cost function utilized in Optimal\nTransport [25], which is often considered to solve classiﬁcation problems, with\n8\n\n\nconstrains on the sample distribution over classes. To that end, a well-known\nSinkhorn [26] mapping method is proposed. The algorithm aims at computing\na class allocation matrix among novel class data for a minimum Wasserstein\ndistance. Namely, an allocation matrix P ∈R(l+u)×n\n+\nis deﬁned where P[i, j]\ndenotes the assigned portion for sample i to class j, and it is computed as follows:\nP = Sinkhorn(C, p, q, λ)\n= argmin\n˜P∈U(p,q)\nX\nij\n˜P[i, j]C[i, j] + λH(˜P),\n(5)\nwhere U(p, q) ∈R(l+u)×n\n+\nis a set of positive matrices for which the rows sum to\np and the columns sum to q, p denotes the distribution of the amount that each\nsample uses for class allocation, and q denotes the distribution of the amount of\nsamples allocated to each class. Therefore, U(p, q) contains all the possible ways\nof allocation. In the same equation, C can be viewed as a cost matrix that is of\nthe same size as P, each element in C indicates the cost of its corresponding\nposition in P. We will deﬁne the particular formula of the cost function for each\nposition C[i, j], ∀i, j in details later on in the section. As for the second term on\nthe right of 5, it stands for the entropy of ˜P: H(˜P) = −P\nij ˜P[i, j] log ˜P[i, j],\nregularized by a hyper-parameter λ. Increasing λ would force the entropy to\nbecome smaller, so that the mapping is less diluted. This term also makes the\nobjective function strictly convex [26, 27] and thus a practical and eﬀective\ncomputation. From lemma 2 in [26], the result of Sinkhorn allocation has the\ntypical form P = diag(u) · exp(−C/λ) · diag(v). It is worth noting that here we\nassume a soft class allocation, meaning that each sample can be “sliced” into\ndiﬀerent classes. We will present our proposed method in details in the next\nparagraphs.\nGiven all that are presented above, in this paper we propose an Expecta-\ntion–Maximization (EM ) [28] based method which alternates between updating\nthe allocation matrix P and estimating the parameter θ of the designed model,\nin order to minimize Eq. 5 and maximize Eq. 4. For a starter, we deﬁne a weight\nmatrix W with n columns (i.e one per class) and d rows (i.e one per dimension\n9\n\n\nof feature vectors), for column j in W we denote it as the weight parameters\nwj ∈Rd for class j in correspondence with Eq. 4. And it is initialized as follows:\nwj = W[:, j] = cj/∥cj∥2,\n(6)\nwhere\ncj = 1\ns\nX\nx∈S,ℓ(x)=j\nfϕ(x).\n(7)\nWe can see that W contains the average of feature vectors in the support set for\neach class, followed by a L2-normalization on each column so that ∥wj∥2 = 1, ∀j.\nThen, we iterate multiple steps that we describe thereafter.\na. Computing costs\nAs previously stated, the proposed algorithm is an EM -like one that iterately\nupdates model parameters for optimal estimates. Therefore, this step along with\nMin-size Sinkhorn presented in the next step, is considered as the E-step of our\nproposed method. The goal is to ﬁnd membership probabilities for the input\nsamples, namely, we compute P that minimizes Eq. 5.\nHere we assume gaussian distributions, features in each class have the same\nvariance and are independent from one another (covariance matrix Σ = Iσ2).\nWe observe that, ignoring the marginal probability, Eq. 4 can be boiled down to\nnegative L2 distances between extracted samples fϕ(xi), ∀i and wj, ∀j, which\nis initialized in Eq. 6 in our proposed method. Therefore, based on the fact\nthat wj and fϕ(xi) are both normalized to be unit length vectors (fϕ(xi) being\npreprocessed using PEME introduced in the previous section), here we deﬁne\nthe cost between sample i and class j to be the following equation:\nC[i, j] ∝(fϕ(xi) −wj)2\n= 1 −wT\nj fϕ(xi),\n(8)\nwhich corresponds to the cosine distance.\nb. Min-size Sinkhorn\nIn [9], we proposed a Wasserstein distance based method in which the\nSinkhorn algorithm is applied at each iteration so that the class prototypes are\n10\n\n\nAlgorithm 1 Min-size Sinkhorn\nInputs: C, p = 1l+u, q = k1n, λ\nInitializations: P = Softmax(−λC)\nfor iter = 1 to 50 do\nP[i, :] ←p[i] ·\nP[i,:]\nP\nj P[i,j], ∀i\nP[:, j] ←q[j] ·\nP[:,j]\nP\ni P[i,j] if P\ni P[i, j] < q[j], ∀j\nend for\nreturn P\nupdated iteratively in order to ﬁnd their best estimates. Although the method\nshowed promising results, it is established on the condition that the distribution\nof the query set is known, e.g. a uniform distribution among classes on the query\nset. This is not ideal given the fact that any priors about Q should be supposedly\nkept unknown when applying a method. The methodology introduced in this\npaper can be seen as a generalization of that introduced in [9] that does not\nrequire priors about Q.\nIn the classical settings, Sinkhorn algorithm aims at ﬁnding the optimal\nmatrix P, given the cost matrix C and regulation parameter λ presented in\nEq. 4). Typically it initiates P from a softmax operation over the rows in C,\nthen it iterates between normalizing columns and rows of P, until the resulting\nmatrix becomes close-to doubly stochastic according to p and q. However, in\nour case we do not know the distribution of samples over classes. To address\nthis, we ﬁrstly introduce the parameter k, initialized so that k ←s, meant to\ntrack an estimate of the cardinal of the class containing the least number of\nsamples in the considered task. Then we propose the following modiﬁcation to\nbe applied to the matrix P once initialized: we normalize each row as in the\nclassical case, but only normalize the columns of P for which the sum is less\nthan the previously computed min-size k [18]. This ensures at least k elements\nallocated for each class, but not exactly k samples as in the balanced case.\nThe principle of this modiﬁed Sinkhorn solution is presented in Algorithm 1.\nc. Updating weights\n11\n\n\nThis step is considered as the M -step of the proposed algorithm, in which\nwe use a variant of the Logistic Regression algorithm in order to ﬁnd the model\nparameter θ in the form of weight parameters wj for each class. Note that wj,\nif normalized, is equivalent to the prototype for class j in this case. Given the\nfact that in Eq. 4 we also take into account the marginal probability, which can\nbe further broken down as:\nP(xi; θ) =\nX\nj\nP(xi|l(xi) = j; θ)P(l(xi) = j),\n(9)\nwe observe that Eq. 4 corresponds to applying a softmax function on the negative\nlogits computed through a L2-distance function between samples and class\nprototypes (normalized). This ﬁts the formulation of a linear hypothesis between\nfϕ(xi) and wj for logit calculations, hence the rationale for utilizing Logistic\nRegression in our proposed method.\nThe procedure of this step is as follows: now that we have a polished allocation\nmatrix P, we ﬁrstly initialize the weights wj as follows:\nwj ←uj/∥uj∥2,\n(10)\nwhere\nuj ←\nX\ni\nP[i, j]fϕ(xi)/\nX\ni\nP[i, j].\n(11)\nWe can see that elements in P are used as coeﬃcients for feature vectors to\nlinearly adjust the class prototypes [9]. Similar to Eq. 6, here wj is the normalized\nnewly-computed class prototype that is a vector of length 1.\nNext we further adjust weights by applying a logistic regression, the opti-\nmization is performed by minimizing the following loss:\n1\nl + u ·\nX\ni\nX\nj\n−log(\nexp (S[i, j])\nPn\nγ=1 exp (S[i, γ])) · P[i, j],\n(12)\nwhere S ∈R(l+u)×n contains the logits, each element is computed as:\nS[i, j] = κ · wT\nj fϕ(xi)\n∥wj∥2\n.\n(13)\n12\n\n\nNote that κ is a scaling parameter, it can also be seen as a temperature parameter\nthat adjusts the conﬁdence metric to be associated to each sample. And it is\nlearnt jointly with W.\nThe deployed Logistic Regression comes with hyperparameters on its own.\nIn our experiments, we use an SGD optimizer with a gradient step of 0.1 and\n0.8 as the momentum parameter, and we train over e epochs. Here we point\nout that e ≥0 is considered an inﬂuential hyperparameter in our proposed\nalgorithm, e = 0 indicates a simple update of W as the normalized adjusted class\nprototypes (Eq. 10) computed from P in Eq. 11, without further adjustment of\nlogistic regression. And also note that when e > 0 we project columns of W to\nthe unit hypersphere at the end of each epoch.\nd. Estimating the class minimum size\nWe can now reﬁne our estimate for the min-size k for the next iteration. To\nthis end, we ﬁrstly compute the predicted label of each sample as follows:\nˆℓ(xi) = arg max\nj (P[i, j]),\n(14)\nwhich can be seen as the current (temporary) class prediction.\nThen, we compute:\nk = min\nj\n{kj},\n(15)\nwhere kj = #{i, ˆℓ(xi) = j}, #{·} representing the cardinal of a set.\nSummary of the proposed method. All steps of the proposed method\nare summarized in Algorithm 2. In our experiments, we also report the results\nobtained when using a prior about Q as in [9]. In this case, k does not have to\nbe estimated throughout the iterations and can be replaced with the actual exact\ntargets for the Sinkhorn. We denote this prior-dependent version PEMnE-BMS∗\n(with an added ∗).\n4. Experiments\n4.1. Datasets\nWe evaluate the performance of the proposed method using standardized few-\nshot classiﬁcation datasets: miniImageNet [29], tieredImageNet [30], CUB [31]\n13\n\n\nAlgorithm 2 Boosted Min-size Sinkhorn (BMS)\nParameters: λ, e\nInputs: Preprocessed fϕ(x), ∀x ∈Dnovel = Q ∪S\nInitializations: W as normalized mean vectors over the support set for each\nclass (Eq. 6); Min-size k ←s.\nfor iter = 1 to 20 do\nCompute cost matrix C using W (Eq. 8). # E-step\nApply Min-size Sinkhorn to compute P (Algorithm 1). # E-step\nUpdate weights W using P with logistic regression (Eq. 10-13). # M-step\nEstimate class predictions ˆℓand min-size k using P (Eq. 14-15).\nend for\nreturn ˆℓ\nand CIFAR-FS [12]. The miniImageNet dataset contains 100 classes randomly\nchosen from ILSVRC- 2012 [32] and 600 images of size 84 × 84 pixels per\nclass. It is split into 64 base classes, 16 validation classes and 20 novel classes.\nThe tieredImageNet dataset is another subset of ImageNet, it consists of 34\nhigh-level categories with 608 classes in total. These categories are split into\n20 meta-training superclasses, 6 meta-validation superclasses and 8 meta-test\nsuperclasses, which corresponds to 351 base classes, 97 validation classes and\n160 novel classes respectively. The CUB dataset contains 200 classes of birds\nand has 11,788 images of size 84 × 84 pixels in total, it is split into 100 base\nclasses, 50 validation classes and 50 novel classes. The CIFAR-FS dataset has\n100 classes, each class contains 600 images of size 32 × 32 pixels. The splits of\nthis dataset are the same as those in miniImageNet.\n4.2. Implementation details\nIn order to stress the genericity of our proposed method with regards to the\nchosen backbone architecture and training strategy, we perform experiments\nusing WRN [33], ResNet18 and ResNet12 [34], along with some other pre-\ntrained backbones (e.g. DenseNet [35, 17]). For each dataset we train the\n14\n\n\nfeature extractor with base classes and test the performance using novel classes.\nTherefore, for each test run, n classes are drawn uniformly at random among\nnovel classes. Among these n classes, s labelled examples and q unlabelled ex-\namples per class are uniformly drawn at random to form Dnovel. The WRN and\nResNet are trained following [7]. In the inductive setting, we use our proposed\npreprocessing steps PEMbE followed by a basic Nearest Class Mean (NCM)\nclassiﬁer. In the transductive setting, the preprocessing steps are denoted as\nPEMnE in that we use the mean vector of novel dataset for mean subtraction,\nfollowed by BMS or BMS∗depending on whether we have prior knowledge on\nthe distribution of query set Q among classes. Note that we perform a QR\ndecomposition on preprocessed features in order to speed up the computation for\nthe classiﬁer that follows. All our experiments are performed using n = 5, q = 15,\ns = 1 or 5. We run 10,000 random draws to obtain mean accuracy score and\nindicate conﬁdence scores (95%) when relevant. For our proposed PEMnE-BMS,\nwe train e = 0 epoch in the case of 1-shot and e = 40 epochs in the case of\n5-shot. As for PEMnE-BMS∗we set e = 20 for 1-shot and e = 40 for 5-shot.\nAs for the regularization parameter λ in Eq. 5, it is ﬁxed to 8.5 for all settings.\nImpact of these hyperparameters is detailed in the next sections.\n4.3. Comparison with state-of-the-art methods\nPerformance on standardized benchmarks. In the ﬁrst experiment, we\nconduct our proposed method on diﬀerent benchmarks and compare the perfor-\nmance with other state-of-the-art solutions. The results are presented in Table 1\nand 2, we observe that our method reaches the state-of-the-art performance\nin both inductive and transductive settings on all the few-shot classiﬁcation\nbenchmarks. Particularly, the proposed PEMnE-BMS∗brings important gains\nin both 1-shot and 5-shot settings, and the prior-independent PEMnE-BMS\nalso obtains competitive results on 5-shot. Note that for tieredImageNet we\nimplement our method based on a pre-trained DenseNet121 backbone following\nthe procedure described in [17]. From these experiments we conclude that the\nproposed method can bring an increase of accuracy with a variety of backbones\n15\n\n\nand datasets, leading to state-of-the-art performance. In terms of execution\ntime, we measured an average of 0.004s per run.\nPerformance on cross-domain settings. In this experiment we test our\nmethod in a cross-domain setting, where the backbone is trained with the base\nclasses in miniImageNet but tested with the novel classes in CUB dataset. As\nshown in Table 3, the proposed method gives the best accuracy both in the case\nof 1-shot and 5-shot, for both inductive and transductive settings.\n4.4. Ablation studies\nGeneralization to backbone architectures. To further stress the inter-\nest of the ingredients on the proposed method reaching top performance, in\nTable 4 we investigate the impact of our proposed method on diﬀerent backbone\narchitectures and benchmarks in the transductive setting. For comparison pur-\npose we also replace our proposed BMS algorithm with a standard K-Means\nalgorithm where class prototypes are initialized with the available labelled sam-\nples for each class. We can observe that: 1) the proposed method consistently\nachieves the best results for any ﬁxed backbone architecture, 2) the feature\nextractor trained on WRN outperforms the others with our proposed method on\ndiﬀerent benchmarks, 3) there are signiﬁcant drops in accuracy with K-Means,\nwhich stresses the interest of BMS, and 4) the prior on Q (BMS vs BMS∗) has\nmajor interest for 1-shot, boosting the performance by an approximation of 1%\non all tested feature extractors.\nPreprocessing impact. In Table 5 we compare our proposed PEME with\nother preprocessing techniques such as Batch Normalization and the ones being\nused in [17]. The experiment is conducted on miniImageNet (backbone: WRN).\nFor all that are put into comparison, we run either a NCM classiﬁer or BMS after\npreprocessing, depending on the settings. The obtained results clearly show the\ninterest of PEME compared with existing alternatives, we also observe that the\npower transform helps increase the accuracy on both inductive and transductive\nsettings. We will further study its impact in details.\nEﬀect of power transform. We ﬁrstly conduct a Gaussian hypothesis test\n16\n\n\nTable 1: 1-shot and 5-shot accuracy of state-of-the-art methods in the literature on miniIma-\ngeNet and tieredImageNet, compared with the proposed solution.\nminiImageNet\nSetting\nMethod\nBackbone\n1-shot\n5-shot\nInductive\nMatching Networks [29]\nWRN\n64.03 ± 0.20%\n76.32 ± 0.16%\nSimpleShot [17]\nDenseNet121\n64.29 ± 0.20%\n81.50 ± 0.14%\nS2M2 R [7]\nWRN\n64.93 ± 0.18%\n83.18 ± 0.11%\nPT+NCM [9]\nWRN\n65.35 ± 0.20%\n83.87 ± 0.13%\nDeepEMD[36]\nResNet12\n65.91 ± 0.82%\n82.41 ± 0.56%\nFEAT[37]\nResNet12\n66.78 ± 0.20%\n82.05 ± 0.14%\nPEMbE-NCM (ours)\nWRN\n68.43 ± 0.20%\n84.67 ± 0.13%\nTransductive\nBD-CSPN [38]\nWRN\n70.31 ± 0.93%\n81.89 ± 0.60%\nLaplacianShot [39]\nDenseNet121\n75.57 ± 0.19%\n87.72 ± 0.13%\nTransfer+SGC [8]\nWRN\n76.47 ± 0.23%\n85.23 ± 0.13%\nTAFSSL [18]\nDenseNet121\n77.06 ± 0.26%\n84.99 ± 0.14%\nTIM-GD [40]\nWRN\n77.80%\n87.40%\nMCT [41]\nResNet12\n78.55 ± 0.86%\n86.03 ± 0.42%\nEPNet [42]\nWRN\n79.22 ± 0.92%\n88.05 ± 0.51%\nPT+MAP [9]\nWRN\n82.92 ± 0.26%\n88.82 ± 0.13%\nPEMnE-BMS (ours)\nWRN\n82.07 ± 0.25%\n89.51 ± 0.13%\nPEMnE-BMS∗(ours)\nWRN\n83.35 ± 0.25%\n89.53 ± 0.13%\ntieredImageNet\nSetting\nMethod\nBackbone\n1-shot\n5-shot\nInductive\nProtoNet [43]\nConvNet4\n53.31 ± 0.89%\n72.69 ± 0.74%\nLEO [44]\nWRN\n66.33 ± 0.05%\n81.44 ± 0.09%\nSimpleShot [17]\nDenseNet121\n71.32 ± 0.22%\n86.66 ± 0.15%\nPT+NCM [9]\nDenseNet121\n69.96 ± 0.22%\n86.45 ± 0.15%\nFEAT[37]\nResNet12\n70.80 ± 0.23%\n84.79 ± 0.16%\nDeepEMD[36]\nResNet12\n71.16 ± 0.87%\n86.03 ± 0.58%\nRENet[45]\nResNet12\n71.61 ± 0.51%\n85.28 ± 0.35%\nPEMbE-NCM (ours)\nDenseNet121\n71.86 ± 0.21%\n87.09 ± 0.15%\nTransductive\nBD-CSPN [38]\nWRN\n78.74 ± 0.95%\n86.92 ± 0.63%\nLaplacianShot [39]\nDenseNet121\n80.30 ± 0.22%\n87.93 ± 0.15%\nMCT [41]\nResNet12\n82.32 ± 0.81%\n87.36 ± 0.50%\nTIM-GD [40]\nWRN\n82.10%\n89.80%\nTAFSSL [18]\nDenseNet121\n84.29 ± 0.25%\n89.31 ± 0.15%\nPT+MAP [9]\nDenseNet121\n85.75 ± 0.26%\n90.43 ± 0.14%\nPEMnE-BMS (ours)\nDenseNet121\n85.08 ± 0.25%\n91.08 ± 0.14%\nPEMnE-BMS∗(ours)\nDenseNet121\n86.07 ± 0.25%\n91.09 ± 0.14%\n17\n\n\nTable 2: 1-shot and 5-shot accuracy of state-of-the-art methods on CUB and CIFAR-FS.\nCUB\nSetting\nMethod\nBackbone\n1-shot\n5-shot\nInductive\nBaseline++ [6]\nResNet10\n69.55 ± 0.89%\n85.17 ± 0.50%\nMAML [1]\nResNet10\n70.32 ± 0.99%\n80.93 ± 0.71%\nProtoNet [43]\nResNet18\n72.99 ± 0.88%\n86.64 ± 0.51%\nMatching Networks [29]\nResNet18\n73.49 ± 0.89%\n84.45 ± 0.58%\nFEAT[37]\nResNet12\n73.27 ± 0.22%\n85.77 ± 0.14%\nDeepEMD[36]\nResNet12\n75.65 ± 0.83%\n88.69 ± 0.50%\nRENet[45]\nResNet12\n79.49 ± 0.44%\n91.11 ± 0.24%\nS2M2 R [7]\nWRN\n80.68 ± 0.81%\n90.85 ± 0.44%\nPT+NCM [9]\nWRN\n80.57 ± 0.20%\n91.15 ± 0.10%\nPEMbE-NCM (ours)\nWRN\n80.82 ± 0.19%\n91.46 ± 0.10%\nTransductive\nLaplacianShot [39]\nResNet18\n80.96%\n88.68%\nTIM-GD [40]\nResNet18\n82.20%\n90.80%\nBD-CSPN [38]\nWRN\n87.45%\n91.74%\nTransfer+SGC [8]\nWRN\n88.35 ± 0.19%\n92.14 ± 0.10%\nPT+MAP [9]\nWRN\n91.55 ± 0.19%\n93.99 ± 0.10%\nLST+MAP [46]\nWRN\n91.68 ± 0.19%\n94.09 ± 0.10%\nPEMnE-BMS (ours)\nWRN\n91.01 ± 0.19%\n94.60 ± 0.09%\nPEMnE-BMS∗(ours)\nWRN\n91.91 ± 0.18%\n94.62 ± 0.09%\nCIFAR-FS\nSetting\nMethod\nBackbone\n1-shot\n5-shot\nInductive\nProtoNet [43]\nConvNet64\n55.50 ± 0.70%\n72.00 ± 0.60%\nMAML [1]\nConvNet32\n58.90 ± 1.90%\n71.50 ± 1.00%\nRENet[45]\nResNet12\n74.51 ± 0.46%\n86.60 ± 0.32%\nBD-CSPN [38]\nWRN\n72.13 ± 1.01%\n82.28 ± 0.69%\nS2M2 R [7]\nWRN\n74.81 ± 0.19%\n87.47 ± 0.13%\nPT+NCM [9]\nWRN\n74.64 ± 0.21%\n87.64 ± 0.15%\nPEMbE-NCM (ours)\nWRN\n74.84 ± 0.21%\n87.73 ± 0.15%\nTransductive\nDSN-MR [47]\nResNet12\n78.00 ± 0.90%\n87.30 ± 0.60%\nTransfer+SGC [8]\nWRN\n83.90 ± 0.22%\n88.76 ± 0.15%\nMCT [41]\nResNet12\n87.28 ± 0.70%\n90.50 ± 0.43%\nPT+MAP [9]\nWRN\n87.69 ± 0.23%\n90.68 ± 0.15%\nLST+MAP [46]\nWRN\n87.79 ± 0.23%\n90.73 ± 0.15%\nPEMnE-BMS (ours)\nWRN\n86.93 ± 0.23%\n91.18 ± 0.15%\nPEMnE-BMS∗(ours)\nWRN\n87.83 ± 0.22%\n91.20 ± 0.15%\non each of the 640 coordinates of raw extracted features (backbone: WRN) for\neach of the 20 novel classes (dataset: miniImageNet). Following D’Agostino and\n18\n\n\nTable 3: 1-shot and 5-shot accuracy of state-of-the-art methods when performing cross-domain\nclassiﬁcation (backbone: WRN).\nSetting\nMethod\n1-shot\n5-shot\nInductive\nBaseline++ [6]\n40.44 ± 0.75%\n56.64 ± 0.72%\nManifold Mixup [48]\n46.21 ± 0.77%\n66.03 ± 0.71%\nS2M2 R [7]\n48.24 ± 0.84%\n70.44 ± 0.75%\nPT+NCM [9]\n48.37 ± 0.19%\n70.22 ± 0.17%\nPEMbE-NCM (ours)\n50.71 ± 0.19%\n73.15 ± 0.16%\nTransductive\nLaplacianShot [39]\n55.46%\n66.33%\nTransfer+SGC [8]\n58.63 ± 0.25%\n73.46 ± 0.17%\nPT+MAP [9]\n63.17 ± 0.31%\n76.43 ± 0.19%\nPEMnE-BMS (ours)\n62.93 ± 0.28%\n79.10 ± 0.18%\nPEMnE-BMS∗(ours)\n63.90 ± 0.31%\n79.15 ± 0.18%\nTable 4: 1-shot and 5-shot accuracy of proposed method on diﬀerent backbones and benchmarks.\nComparison with k-means algorithm.\nminiImageNet\nCUB\nCIFAR-FS\nMethod\nBackbone\n1-shot\n5-shot\n1-shot\n5-shot\n1-shot\n5-shot\nK-MEANS\nResNet12\n72.73 ± 0.23%\n84.05 ± 0.14%\n87.35 ± 0.19%\n92.31 ± 0.10%\n78.39 ± 0.24%\n85.73 ± 0.16%\nResNet18\n73.08 ± 0.22%\n84.67 ± 0.14%\n87.16 ± 0.19%\n91.97 ± 0.09%\n79.95 ± 0.23%\n86.74 ± 0.16%\nWRN\n76.67 ± 0.22%\n86.73 ± 0.13%\n88.28 ± 0.19%\n92.37 ± 0.10%\n83.69 ± 0.22%\n89.19 ± 0.15%\nBMS (ours)\nResNet12\n77.62 ± 0.28%\n86.95 ± 0.15%\n90.14 ± 0.19%\n94.30 ± 0.10%\n81.65 ± 0.25%\n88.38 ± 0.16%\nResNet18\n79.30 ± 0.27%\n87.94 ± 0.14%\n90.50 ± 0.19%\n94.29 ± 0.09%\n84.16 ± 0.24%\n89.39 ± 0.15%\nWRN\n82.07 ± 0.25%\n89.51 ± 0.13%\n91.01 ± 0.18%\n94.60 ± 0.09%\n86.93 ± 0.23%\n91.18 ± 0.15%\nBMS∗(ours)\nResNet12\n79.03 ± 0.28%\n87.01 ± 0.15%\n91.34 ± 0.19%\n94.32 ± 0.09%\n82.87 ± 0.27%\n88.43 ± 0.16%\nResNet18\n80.56 ± 0.27%\n87.98 ± 0.14%\n91.39 ± 0.19%\n94.31 ± 0.09%\n85.17 ± 0.25%\n89.42 ± 0.16%\nWRN\n83.35 ± 0.25%\n89.53 ± 0.13%\n91.91 ± 0.18%\n94.62 ± 0.09%\n87.83 ± 0.22%\n91.20 ± 0.15%\nPearson’s methodology [50, 51] and p = 1e −3, only one of the 640 × 20 = 12800\ntests return positive, suggesting a very low pass rate for raw features. However,\nafter applying the power transform we record a pass rate that surpasses 50%,\nsuggesting a considerably increased number of positive results for Gaussian tests.\nThis experiment shows the eﬀect of power transform being able to adjust feature\ndistributions into more gaussian-like ones.\nTo better show the eﬀect of this proposed technique on feature distributions,\nwe depict in Figure 2 the distributions of an arbitrarily selected feature for\n19\n\n\nTable 5: 1-shot and 5-shot accuracy on miniImageNet (backbone: WRN) with diﬀerent\npreprocessings on the extracted features.\nInductive (NCM)\nTransductive (BMS)\nPreprocessing\n1-shot\n5-shot\n1-shot\n5-shot\nNone\n55.30 ± 0.21%\n78.34 ± 0.15%\n77.62 ± 0.26%\n87.96 ± 0.13%\nBatch Norm [49]\n66.81 ± 0.20%\n83.57 ± 0.13%\n73.74 ± 0.21%\n88.07 ± 0.13%\nL2N [17]\n65.37 ± 0.20%\n83.46 ± 0.13%\n73.84 ± 0.21%\n88.15 ± 0.13%\nCL2N [17]\n63.88 ± 0.20%\n80.85 ± 0.14%\n73.12 ± 0.28%\n86.47 ± 0.15%\nEMbE\n68.05 ± 0.20%\n83.76 ± 0.13%\n80.28 ± 0.26%\n88.36 ± 0.13%\nPEMbE\n68.43 ± 0.20%\n84.67 ± 0.13%\n82.01 ± 0.26%\n89.50 ± 0.13%\nEMnE\n\\\n\\\n80.14 ± 0.27%\n88.39 ± 0.13%\nPEMnE\n\\\n\\\n82.07 ± 0.25%\n89.51 ± 0.13%\n3 randomly selected novel classes of miniImageNet when using WRN, before\nand after applying power transform.\nWe observe quite clearly that 1) raw\nfeatures exhibit a positive distribution mostly concentrated around 0, and 2)\npower transform is able to reshape the feature distributions to close-to-gaussian\ndistributions. We observe similar behaviors with other datasets as well. Moreover,\nin order to visualize the impact of this technique with respect to the position of\nfeature points, in Figure 3 we plot the feature vectors of randomly selected 3\nclasses from Dnovel. Note that all feature vectors in this experiment are reduced\nto a 3-dimensional ones corresponding to their largest eigenvalues. From Figure 3\nwe can observe that power transform, often followed by a L2-normalization, can\nhelp shape the class distributions to become more gathered and Gaussian-like [46].\nInﬂuence of the number of unlabelled samples. In order to better\nunderstand the gain in accuracy due to having access to more unlabelled samples,\nwe depict in Figure 4 the evolution of accuracy as a function of q, when the\nnumber of classes n = 5 is ﬁxed. Interestingly, the accuracy quickly reaches a\nclose-to-asymptotical plateau, emphasizing the ability of the method to quickly\nexploit available information in the task.\nInﬂuence of hyperparameters. In order to test how much impact the\nhyperparameters could have on our proposed method in terms of prediction\n20\n\n\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0\n20\n40\n60\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0\n20\n40\n60\n80\nClass 1\nClass 2\nClass 3\nFigure 2: Distributions of an arbitrarily chosen feature for 3 novel classes before (left) and\nafter (right) power transform.\nFigure 3: Plot of feature vectors (extracted from WRN) from randomly selected 3 classes.\n(left) Naive features. (right) Preprocessed features using power transform.\naccuracy, here we select two important hyperparameters that are used in BMS\nand observe their impact. Namely the number of training epochs e in logistic\nregression and the regulation parameter λ used for computing the prediction\nmatrix P. In Figure 5 we show the accuracy of our proposed method as a\nfunction of e (top) and λ (bottom). Results are reported for BMS∗in 1-shot\nsettings, and for BMS in 5-shot settings. From the ﬁgure we can see a slight\nuptick of accuracy as e or λ increases, followed by a downhill when they become\nlarger, implying an overﬁtting of the classiﬁer.\nProposed method on backbones pre-trained with external data. In\nthis experiment, we compare our proposed method BMS∗with the work in [52]\n21\n\n\n10\n30\n50\n75\n100\n150\n80\n85\n90\n5q\nAccuracy\ncub\ncifar −fs\nmini\nFigure 4: Accuracy of 5-way, 1-shot classiﬁcation setting on miniImageNet, CUB and CIFAR-FS\nas a function of q.\n0\n10\n20\n30\n40\n50\n60\n70\n80\n80\n85\n90\ne\nAccuracy\nBMS∗, 1-shot\nBMS, 5-shot\n3 4 5 6 7 8 9 10\n12\n15\n20\n82\n84\n86\n88\n90\nλ\nAccuracy\nBMS∗, 1-shot\nBMS, 5-shot\nFigure 5: Accuracy of proposed method on miniImageNet (backbone: WRN) as a function of\ntraining epoch e (top) and regulation parameter λ (bottom).\nthat pre-trains the backbone with the help of external illumination data for\naugmentation, followed by PT+MAP in [9] for class center estimation. Here\nwe use the same backbones as [52], and replace PT+MAP with our proposed\nBMS∗at the same conditions. Results are presented in Table 6. Note that we\nalso show the re-implemented results of [52], and our method reaches superior\nperformance on all tested benchmarks using external data in [52].\nProposed method on Few-Shot Open-Set Recognition.\nFew-Shot\nOpen-Set Recognition (FSOR) as a new trending topic deals with the fact that\nthere are open data mixed in query set Q that do not belong to any of the\n22\n\n\nTable 6: Proposed method on backbones pre-trained with external data. Note that -re denotes\nthe re-implementation of an existing method.\nBenchmark\nMethod\n1-shot\n5-shot\nminiImageNet\nIllu-Aug [52]\n82.99 ± 0.23%\n89.14 ± 0.12%\nIllu-Aug-re\n83.53 ± 0.25%\n89.38 ± 0.12%\nPEMnE-BMS∗(ours)\n83.85 ± 0.25%\n90.07 ± 0.12%\nCUB\nIllu-Aug [52]\n94.73 ± 0.14%\n96.28 ± 0.08%\nIllu-Aug-re\n94.63 ± 0.15%\n96.06 ± 0.08%\nPEMnE-BMS∗(ours)\n94.78 ± 0.15%\n96.43 ± 0.07%\nCIFAR-FS\nIllu-Aug [52]\n87.73 ± 0.22%\n91.09 ± 0.15%\nIllu-Aug-re\n87.76 ± 0.23%\n91.04 ± 0.15%\nPEMnE-BMS∗(ours)\n87.83 ± 0.23%\n91.49 ± 0.15%\nsupposed classes used for label predictions. Therefore this often requires a\nrobust classiﬁer that is able to classify correctly the non-open data as well as\nrejecting the open ones. In Table 7 we apply our proposed PEME for feature\npreprocessing, followed by an NCM classiﬁer, and compare the results with other\nstate-of-the-art alternatives. We observe that our proposed method is able to\nsurpass the others in terms of Accuracy and AUROC.\nTable 7: Accuracy and AUROC of Proposed method for Few-Shot Open-Set Recognition.\nminiImageNet\ntieredImageNet\n1-shot\n5-shot\n1-shot\n5-shot\nMethod\nAcc\nAUROC\nAcc\nAUROC\nAcc\nAUROC\nAcc\nAUROC\nProtoNet [43]\n64.01%\n51.81%\n80.09%\n60.39%\n68.26%\n60.73%\n83.40%\n64.96%\nFEAT [37]\n67.02%\n57.01%\n82.02%\n63.18%\n70.52%\n63.54%\n84.74%\n70.74%\nNN [53]\n63.82%\n56.96%\n80.12%\n63.43%\n67.73%\n62.70%\n83.43%\n69.77%\nOpenMax [54]\n63.69%\n62.64%\n80.56%\n62.27%\n68.28%\n60.13%\n83.48%\n65.51%\nPEELER [55]\n65.86%\n60.57%\n80.61%\n67.35%\n69.51%\n65.20%\n84.10%\n73.27%\nSnaTCHer [56]\n67.60%\n70.17%\n82.36%\n77.42%\n70.85%\n74.95%\n85.23%\n82.03%\nPEMbE-NCM (ours)\n68.43%\n72.10%\n84.67%\n80.04%\n71.87%\n75.44%\n87.09%\n83.85%\n23\n\n\n4.5. Proposed method on merged features\nIn this section we investigate the eﬀect of our proposed method on merged\nfeatures. Namely, we perform a direct concatenation of raw feature vectors\nextracted from multiple backbones at the beginning, followed by BMS. In\nTable 8 we chose the feature vectors from three backbones (WRN, ResNet18\nand ResNet12) and evaluated the performance with diﬀerent combinations. We\nobserve that 1) a direct concatenation, depending on the backbones, can bring\nabout 1% gain in both 1-shot and 5-shot settings compared with the results\nin Table 4 with feature vectors extracted from one single feature extractor. 2)\nBMS∗reached new state-of-the-art results on few-shot learning benchmarks with\nfeature vectors concatenated from WRN, ResNet18 and ResNet12, given that\nno external data is used.\nTo further study the impact of the number of backbones on prediction\naccuracy, in Figure 6 we depict the performance of our proposed method as a\nfunction of the number of backbones. Note that here we operate on feature\nvectors of 6 WRN backbones (dataset: miniImageNet) concatenated one after\nanother, which makes a total of 6 slots corresponding to a 640 × 6 = 3840\nfeature size. Each of them is trained the same way as in [7], and we randomly\nselect the multiples of 640 coordinates within the slots to denote the number\nof concatenated backbones used. The performance result is the average of 100\nrandom selections and we test with both BMS and BMS∗for 1-shot, and BMS∗\nfor 5-shot. From Figure 6 we observe that, as the number of backbones increases,\nthere is a relatively steady growth in terms of accuracy in multiple settings of\nour proposed method, indicating the interest of BMS in merged features.\n5. Conclusion\nIn this paper we introduced a new pipeline to solve the few-shot classiﬁcation\nproblem. Namely, we proposed to ﬁrstly preprocess the raw feature vectors\nto better align to a Gaussian distribution and then we designed an optimal-\ntransport inspired iterative algorithm to estimate the class prototypes for the\n24\n\n\nTable 8: 1-shot and 5-shot accuracy on miniImageNet, CUB and CIFAR-FS on our proposed\nPEMnE-BMS with multi-backbones (backbone training procedure follows [7], ’+’ denotes a\nconcatenation of backbone features).\nminiImageNet\nCUB\nCIFAR-FS\nBackbone\n1-shot\n5-shot\n1-shot\n5-shot\n1-shot\n5-shot\nRN18+RN12\n80.32%\n89.07%\n92.31%\n95.62%\n85.44%\n90.58%\nWRN+RN12\n82.63%\n90.43%\n92.69%\n95.96%\n87.11%\n91.50%\nWRN+RN18\n83.05%\n90.57%\n92.66%\n95.79%\n87.53%\n91.70%\nWRN+RN18+RN12\n82.90%\n90.64%\n93.32%\n96.31%\n87.62%\n91.84%\nWRN+RN18+RN12∗\n84.37%\n90.69%\n94.26%\n96.32%\n88.44%\n91.86%\n6×WRN∗\n85.54%\n91.53%\n\\\n\\\n\\\n\\\n∗: BMS∗.\n1\n2\n3\n4\n5\n6\n85\n90\n# of backbones\nAccuracy\nBMS, 1-shot\nBMS∗, 1-shot\nBMS∗, 5-shot\nFigure 6: Accuracy of proposed method in diﬀerent settings as a function of the number of\nbackbones (dataset: miniImageNet).\ntransductive setting. Our experimental results on standard vision benchmarks\nreach state-of-the-art accuracy, with important gains in both 1-shot and 5-shot\nclassiﬁcation settings. Moreover, the proposed method can bring gains with a\nvariety of feature extractors, with few extra hyperparameters. Thus we believe\nthat the proposed method is applicable to many practical problems. We also\nprovide two versions of our proposed method, one being prior-dependent and\none that does not require any knowledge on unlabelled data, and they both are\nable to bring important gains in accuracy.\n25\n\n\nReferences\n[1] C. Finn, P. Abbeel, S. Levine, Model-agnostic meta-learning for fast adapta-\ntion of deep networks, in: Proceedings of the 34th International Conference\non Machine Learning-Volume 70, JMLR. org, 2017, pp. 1126–1135.\n[2] S. Ravi, H. Larochelle, Optimization as a model for few-shot learning, in: 5th\nInternational Conference on Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Proceedings, OpenReview.net,\n2017.\nURL https://openreview.net/forum?id=rJY0-Kcll\n[3] S. Thrun, L. Pratt, Learning to learn, Springer Science & Business Media,\n2012.\n[4] L. Torrey, J. Shavlik, Transfer learning, in: Handbook of research on machine\nlearning applications and trends: algorithms, methods, and techniques, IGI\nGlobal, 2010, pp. 242–264.\n[5] D. Das, C. S. G. Lee, A two-stage approach to few-shot learning for image\nrecognition, IEEE Trans. Image Process. 29 (2020) 3336–3350. doi:10.\n1109/TIP.2019.2959254.\nURL https://doi.org/10.1109/TIP.2019.2959254\n[6] W. Chen, Y. Liu, Z. Kira, Y. F. Wang, J. Huang, A closer look at few-shot\nclassiﬁcation, in: 7th International Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019, OpenReview.net, 2019.\nURL https://openreview.net/forum?id=HkxLXnAcFQ\n[7] P. Mangla, N. Kumari, A. Sinha, M. Singh, B. Krishnamurthy, V. N.\nBalasubramanian, Charting the right manifold: Manifold mixup for few-\nshot learning, in: The IEEE Winter Conference on Applications of Computer\nVision, 2020, pp. 2218–2227.\n26\n\n\n[8] Y. Hu, V. Gripon, S. Pateux, Graph-based interpolation of feature vectors\nfor accurate few-shot classiﬁcation, in: 2020 25th International Conference\non Pattern Recognition (ICPR), IEEE, 2021, pp. 8164–8171.\n[9] Y. Hu, V. Gripon, S. Pateux, Leveraging the feature distribution in transfer-\nbased few-shot learning, in: International Conference on Artiﬁcial Neural\nNetworks, Springer, 2021, pp. 487–499.\n[10] Z. Li, F. Zhou, F. Chen, H. Li, Meta-sgd: Learning to learn quickly for few\nshot learning, CoRR abs/1707.09835. arXiv:1707.09835.\nURL http://arxiv.org/abs/1707.09835\n[11] A. Antoniou, H. Edwards, A. J. Storkey, How to train your MAML, in: 7th\nInternational Conference on Learning Representations, ICLR 2019, New\nOrleans, LA, USA, May 6-9, 2019, OpenReview.net, 2019.\nURL https://openreview.net/forum?id=HJGven05Y7\n[12] L. Bertinetto, J. F. Henriques, P. H. S. Torr, A. Vedaldi, Meta-learning\nwith diﬀerentiable closed-form solvers, in: 7th International Conference on\nLearning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019, OpenReview.net, 2019.\nURL https://openreview.net/forum?id=HyxnZh0ct7\n[13] H. Zhang, J. Zhang, P. Koniusz, Few-shot learning via saliency-guided hal-\nlucination of samples, in: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2019, pp. 2770–2779.\n[14] Z. Chen, Y. Fu, Y.-X. Wang, L. Ma, W. Liu, M. Hebert, Image deformation\nmeta-networks for one-shot learning, in: Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2019, pp. 8680–8689.\n[15] T. Mensink, J. Verbeek, F. Perronnin, G. Csurka, Metric learning for large\nscale image classiﬁcation: Generalizing to new classes at near-zero cost, in:\nEuropean Conference on Computer Vision, Springer, 2012, pp. 488–501.\n27\n\n\n[16] O. Chapelle, B. Scholkopf, A. Zien, Semi-supervised learning (chapelle, o.\net al., eds.; 2006)[book reviews], IEEE Transactions on Neural Networks\n20 (3) (2009) 542–542.\n[17] Y. Wang, W. Chao, K. Q. Weinberger, L. van der Maaten, Simpleshot:\nRevisiting nearest-neighbor classiﬁcation for few-shot learning, CoRR\nabs/1911.04623. arXiv:1911.04623.\nURL http://arxiv.org/abs/1911.04623\n[18] M. Lichtenstein, P. Sattigeri, R. Feris, R. Giryes, L. Karlinsky, Tafssl: Task-\nadaptive feature sub-space learning for few-shot classiﬁcation, in: European\nConference on Computer Vision, Springer, 2020, pp. 522–539.\n[19] V. Gripon, G. B. Hacene, M. L¨owe, F. Vermet, Improving accuracy of non-\nparametric transfer learning via vector segmentation, in: 2018 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Processing (ICASSP),\n2018, pp. 2966–2970.\n[20] S. Yang, L. Liu, M. Xu, Free lunch for few-shot learning: Distribution\ncalibration, in: 9th International Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021, OpenReview.net, 2021.\nURL https://openreview.net/forum?id=JWOiYxMG92s\n[21] Y. Liu, J. Lee, M. Park, S. Kim, E. Yang, S. J. Hwang, Y. Yang, Learning to\npropagate labels: Transductive propagation network for few-shot learning,\nin: 7th International Conference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019, OpenReview.net, 2019.\nURL https://openreview.net/forum?id=SyVuRiC5K7\n[22] A. F. Agarap, Deep learning using rectiﬁed linear units (relu), CoRR\nabs/1803.08375. arXiv:1803.08375.\nURL http://arxiv.org/abs/1803.08375\n[23] J. W. Tukey, Exploratory data analysis, Vol. 2, Reading, Mass., 1977.\n28\n\n\n[24] R. G. Cinbis, J. Verbeek, C. Schmid, Approximate ﬁsher kernels of non-\niid image models for image categorization, IEEE transactions on pattern\nanalysis and machine intelligence 38 (6) (2015) 1084–1098.\n[25] C. Villani, Optimal transport: old and new, Vol. 338, Springer Science &\nBusiness Media, 2008.\n[26] M. Cuturi, Sinkhorn distances: Lightspeed computation of optimal trans-\nport, in: Advances in neural information processing systems, 2013, pp.\n2292–2300.\n[27] J. Solomon, F. De Goes, G. Peyr´e, M. Cuturi, A. Butscher, A. Nguyen,\nT. Du, L. Guibas, Convolutional wasserstein distances: Eﬃcient optimal\ntransportation on geometric domains, ACM Transactions on Graphics\n(TOG) 34 (4) (2015) 1–11.\n[28] A. P. Dempster, N. M. Laird, D. B. Rubin, Maximum likelihood from\nincomplete data via the em algorithm, Journal of the Royal Statistical\nSociety: Series B (Methodological) 39 (1) (1977) 1–22.\n[29] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al., Matching networks\nfor one shot learning, in: Advances in neural information processing systems,\n2016, pp. 3630–3638.\n[30] M. Ren, E. Triantaﬁllou, S. Ravi, J. Snell, K. Swersky, J. B. Tenenbaum,\nH. Larochelle, R. S. Zemel, Meta-learning for semi-supervised few-shot\nclassiﬁcation, in: 6th International Conference on Learning Representations,\nICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference\nTrack Proceedings, OpenReview.net, 2018.\nURL https://openreview.net/forum?id=HJcSzz-CZ\n[31] C. Wah, S. Branson, P. Welinder, P. Perona, S. Belongie, The Caltech-UCSD\nBirds-200-2011 Dataset, Tech. Rep. CNS-TR-2011-001, California Institute\nof Technology (2011).\n29\n\n\n[32] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,\nA. Karpathy, A. Khosla, M. Bernstein, et al., Imagenet large scale visual\nrecognition challenge, International journal of computer vision 115 (3) (2015)\n211–252.\n[33] S. Zagoruyko, N. Komodakis, Wide residual networks, in: R. C. Wilson,\nE. R. Hancock, W. A. P. Smith (Eds.), Proceedings of the British Machine\nVision Conference 2016, BMVC 2016, York, UK, September 19-22, 2016,\nBMVA Press, 2016.\nURL http://www.bmva.org/bmvc/2016/papers/paper087/index.html\n[34] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition,\nin: Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2016, pp. 770–778.\n[35] G. Huang, Z. Liu, L. Van Der Maaten, K. Q. Weinberger, Densely connected\nconvolutional networks, in: Proceedings of the IEEE conference on computer\nvision and pattern recognition, 2017, pp. 4700–4708.\n[36] C. Zhang, Y. Cai, G. Lin, C. Shen, Deepemd: Few-shot image classiﬁcation\nwith diﬀerentiable earth mover’s distance and structured classiﬁers, in:\nProceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, 2020, pp. 12203–12213.\n[37] H.-J. Ye, H. Hu, D.-C. Zhan, F. Sha, Few-shot learning via embedding\nadaptation with set-to-set functions, in: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2020, pp. 8808–\n8817.\n[38] J. Liu, L. Song, Y. Qin, Prototype rectiﬁcation for few-shot learning, in:\nComputer Vision–ECCV 2020: 16th European Conference, Glasgow, UK,\nAugust 23–28, 2020, Proceedings, Part I 16, Springer, 2020, pp. 741–756.\n[39] I. Ziko, J. Dolz, E. Granger, I. B. Ayed, Laplacian regularized few-shot\n30\n\n\nlearning, in: International Conference on Machine Learning, PMLR, 2020,\npp. 11660–11670.\n[40] M. Boudiaf, I. M. Ziko, J. Rony, J. Dolz, P. Piantanida, I. B. Ayed, Transduc-\ntive information maximization for few-shot learning, CoRR abs/2008.11297.\narXiv:2008.11297.\nURL https://arxiv.org/abs/2008.11297\n[41] S. M. Kye, H. Lee, H. Kim, S. J. Hwang, Transductive few-shot learning\nwith meta-learned conﬁdence, CoRR abs/2002.12017. arXiv:2002.12017.\nURL https://arxiv.org/abs/2002.12017\n[42] P. Rodr´ıguez, I. Laradji, A. Drouin, A. Lacoste, Embedding propagation:\nSmoother manifold for few-shot classiﬁcation, in: European Conference on\nComputer Vision, Springer, 2020, pp. 121–138.\n[43] J. Snell, K. Swersky, R. Zemel, Prototypical networks for few-shot learning,\nin: Advances in Neural Information Processing Systems, 2017, pp. 4077–\n4087.\n[44] A. A. Rusu, D. Rao, J. Sygnowski, O. Vinyals, R. Pascanu, S. Osindero,\nR. Hadsell, Meta-learning with latent embedding optimization, in: 7th\nInternational Conference on Learning Representations, ICLR 2019, New\nOrleans, LA, USA, May 6-9, 2019, OpenReview.net, 2019.\nURL https://openreview.net/forum?id=BJgklhAcK7\n[45] D. Kang, H. Kwon, J. Min, M. Cho, Relational embedding for few-shot\nclassiﬁcation, CoRR abs/2108.09666. arXiv:2108.09666.\nURL https://arxiv.org/abs/2108.09666\n[46] T. Chobola, D. Vasata, P. Kord´ık, Transfer learning based few-shot classiﬁ-\ncation using optimal transport mapping from preprocessed latent space of\nbackbone neural network, CoRR abs/2102.05176. arXiv:2102.05176.\nURL https://arxiv.org/abs/2102.05176\n31\n\n\n[47] C. Simon, P. Koniusz, R. Nock, M. Harandi, Adaptive subspaces for few-shot\nlearning, in: Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2020, pp. 4136–4145.\n[48] V. Verma, A. Lamb, C. Beckham, A. Najaﬁ, I. Mitliagkas, D. Lopez-Paz,\nY. Bengio, Manifold mixup: Better representations by interpolating hidden\nstates, in: International Conference on Machine Learning, PMLR, 2019, pp.\n6438–6447.\n[49] S. Ioﬀe, C. Szegedy, Batch normalization: Accelerating deep network train-\ning by reducing internal covariate shift, in: International conference on\nmachine learning, PMLR, 2015, pp. 448–456.\n[50] R. DIAgostino, An omnibus test of normality for moderate and large sample\nsizes, Biometrika 58 (34) (1971) 1–348.\n[51] R. D’AGOSTINO, E. S. Pearson, Tests for departure from normality. em-\npirical results for the distributions of b 2 and\n√\nb, Biometrika 60 (3) (1973)\n613–622.\n[52] H. Zhang, Z. Cao, Z. Yan, C. Zhang, Sill-net: Feature augmentation with\nseparated illumination representation, CoRR abs/2102.03539. arXiv:2102.\n03539.\nURL https://arxiv.org/abs/2102.03539\n[53] P. R. M. J´unior, R. M. De Souza, R. d. O. Werneck, B. V. Stein, D. V.\nPazinato, W. R. de Almeida, O. A. Penatti, R. d. S. Torres, A. Rocha,\nNearest neighbors distance ratio open-set classiﬁer, Machine Learning 106 (3)\n(2017) 359–386.\n[54] A. Bendale, T. E. Boult, Towards open set deep networks, in: Proceedings\nof the IEEE conference on computer vision and pattern recognition, 2016,\npp. 1563–1572.\n32\n\n\n[55] B. Liu, H. Kang, H. Li, G. Hua, N. Vasconcelos, Few-shot open-set recogni-\ntion using meta-learning, in: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2020, pp. 8798–8807.\n[56] M. Jeong, S. Choi, C. Kim, Few-shot open-set recognition by transformation\nconsistency, in: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2021, pp. 12566–12575.\n33\n"
}