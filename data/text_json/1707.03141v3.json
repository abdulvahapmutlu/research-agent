{
  "filename": "1707.03141v3.pdf",
  "num_pages": 17,
  "pages": [
    "Accepted as a conference paper at ICLR 2018\nA SIMPLE NEURAL ATTENTIVE META-LEARNER\nNikhil Mishra ∗†\nMostafa Rohaninejad∗\nXi Chen†\nPieter Abbeel†\nUC Berkeley, Department of Electrical Engineering and Computer Science\nEmbodied Intelligence\n{nmishra, rohaninejadm, c.xi, pabbeel}@berkeley.edu\nABSTRACT\nDeep neural networks excel in regimes with large amounts of data, but tend to\nstruggle when data is scarce or when they need to adapt quickly to changes in the\ntask. In response, recent work in meta-learning proposes training a meta-learner\non a distribution of similar tasks, in the hopes of generalization to novel but related\ntasks by learning a high-level strategy that captures the essence of the problem it is\nasked to solve. However, many recent meta-learning approaches are extensively\nhand-designed, either using architectures specialized to a particular application, or\nhard-coding algorithmic components that constrain how the meta-learner solves\nthe task. We propose a class of simple and generic meta-learner architectures that\nuse a novel combination of temporal convolutions and soft attention; the former to\naggregate information from past experience and the latter to pinpoint speciﬁc pieces\nof information. In the most extensive set of meta-learning experiments to date,\nwe evaluate the resulting Simple Neural AttentIve Learner (or SNAIL) on several\nheavily-benchmarked tasks. On all tasks, in both supervised and reinforcement\nlearning, SNAIL attains state-of-the-art performance by signiﬁcant margins.\n1\nINTRODUCTION\nThe ability to learn quickly is a key characteristic that distinguishes human intelligence from its\nartiﬁcial counterpart. Humans effectively utilize prior knowledge and experiences to learn new skills\nquickly. However, artiﬁcial learners trained with traditional supervised-learning or reinforcement-\nlearning methods generally perform poorly when only a small amount of data is available or when\nthey need to adapt to a changing task.\nMeta-learning seeks to resolve this deﬁciency by broadening the learner’s scope to a distribution of\nrelated tasks. Rather than training the learner on a single task (with the goal of generalizing to unseen\nsamples from a similar data distribution) a meta-learner is trained on a distribution of similar tasks,\nwith the goal of learning a strategy that generalizes to related but unseen tasks from a similar task\ndistribution. Traditionally, a successful learner discovers a rule that generalizes across data points,\nwhile a successful meta-learner learns an algorithm that generalizes across tasks.\nMany recently-proposed meta-learning methods demonstrate improved performance at the expense of\nbeing hand-designed at either the architectural or algorithmic level. Some have been engineered with\na particular application in mind, while others have aspects of a particular high-level strategy already\nbuilt into them. However, the optimal strategy for an arbitrary range of tasks may not be obvious to\nthe humans designing a meta-learner, in which case the meta-learner should have the ﬂexibility to\nlearn the best way to solve the tasks it is presented with. Such a meta-learner would need to have an\nexpressive, versatile model architecture, in order to learn a range of strategies in a variety of domains.\nMeta-learning can be formalized as a sequence-to-sequence problem; in existing approaches that\nadopt this view, the bottleneck is in the meta-learner’s ability to internalize and refer to past experience.\nThus, we propose a class of model architectures that addresses this shortcoming: we combine temporal\nconvolutions, which enable the meta-learner to aggregate contextual information from past experience,\nwith causal attention, which allow it to pinpoint speciﬁc pieces of information within that context. We\nevaluate this Simple Neural AttenIve Learner (SNAIL) on several heavily-benchmarked meta-learning\ntasks, including the Omniglot and mini-Imagenet datasets in supervised learning, and multi-armed\nbandits, tabular Markov Decision processes (MDPs), visual navigation, and continuous control in\nreinforcement learning. In all domains, SNAIL achieves state-of-the-art performance by signiﬁcant\nmargins, outperforming methods that are domain-speciﬁc or rely on built-in algorithmic priors.\n∗Authors contributed equally and are listed in alphabetical order.\n†Part of this work was done at OpenAI.\n1\narXiv:1707.03141v3  [cs.AI]  25 Feb 2018\n",
    "Accepted as a conference paper at ICLR 2018\n2\nMETA-LEARNING PRELIMINARIES\nBefore we describe SNAIL in detail, we will introduce notation and formalize the meta-learning\nproblem. As brieﬂy discussed in Section 1, the goal of meta-learning is generalization across tasks\nrather than across data points. Each task Ti is episodic and deﬁned by inputs xt, outputs at, a\nloss function Li(xt, at), a transition distribution Pi(xt|xt−1, at−1), and an episode length Hi. A\nmeta-learner (with parameters θ) models the distribution π(at|x1, . . . , xt; θ). Given a distribution\nover tasks T = P(Ti), the meta-learner’s objective is to minimize its expected loss with respect to θ.\nmin\nθ\nETi∼T\n\u0014 Hi\nX\nt=0\nLi(xt, at)\n\u0015\n,\nwhere xt ∼Pi(xt|xt−1, at−1), at ∼π(at|x1, . . . , xt; θ)\nA meta-learner is trained by optimizing this expected loss over tasks (or mini-batches of tasks)\nsampled from T . During testing, the meta-learner is evaluated on unseen tasks from a different task\ndistribution eT = P(eTi) that is similar to the training task distribution T .\n3\nA SIMPLE NEURAL ATTENTIVE LEARNER\nThe key principle motivating our approach is simplicity and versatility: a meta-learner should be\nuniversally applicable to domains in both supervised and reinforcement learning. It should be generic\nand expressive enough to learn an optimal strategy, rather than having the strategy already built-in.\nSantoro et al. (2016) considered a similar formulation of the meta-learning problem, and explored\nusing recurrent neural networks (RNNs) to implement a meta-learner. Although simple and generic,\ntheir approach is signiﬁcantly outperformed by methods that are hand-designed to exploit domain\nor algorithmic knowledge (methods which we survey in Section 4). We hypothesize that this is\nbecause traditional RNN architectures propagate information by keeping it in their hidden state from\none timestep to the next; this temporally-linear dependency bottlenecks their capacity to perform\nsophisticated computation on a stream of inputs.\nvan den Oord et al. (2016a) introduced a class of architectures that generate sequential data (in their\ncase, audio) by performing dilated 1D-convolutions over the temporal dimension. These temporal\nconvolutions (TC) are causal, so that the generated values at the next timestep are only inﬂuenced\nby past timesteps and not future ones. Compared to traditional RNNs, they offer more direct, high-\nbandwidth access to past information, allowing them to perform more sophisticated computation over\na temporal context of ﬁxed size. However, to scale to long sequences, the dilation rates generally\nincrease exponentially, so that the required number of layers scales logarithmically with the sequence\nlength. Hence, they have coarser access to inputs that are further back in time; their bounded capacity\nand positional dependence can be undesirable in a meta-learner, which should be able to fully utilize\nincreasingly large amounts of experience.\nIn contrast, soft attention (in particular, the style used by Vaswani et al. (2017a)) allows a model\nto pinpoint a speciﬁc piece of information from a potentially inﬁnitely-large context. It treats the\ncontext as an unordered key-value store which it can query based on the content of each element.\nHowever, the lack of positional dependence can also be undesirable, especially in reinforcement\nlearning, where the observations, actions, and rewards are intrinsically sequential.\nDespite their individual shortcomings, temporal convolutions and attention complement each other:\nwhile the former provide high-bandwidth access at the expense of ﬁnite context size, the latter provide\npinpoint access over an inﬁnitely large context. Hence, we construct SNAIL by combining the two:\nwe use temporal convolutions to produce the context over which we use a causal attention operation.\nBy interleaving TC layers with causal attention layers, SNAIL can have high-bandwidth access over\nits past experience without constraints on the amount of experience it can effectively use. By using\nattention at multiple stages within a model that is trained end-to-end, SNAIL can learn what pieces\nof information to pick out from the experience it gathers, as well as a feature representation that is\namenable to doing so easily. As an additional beneﬁt, SNAIL architectures are easier to train than\ntraditional RNNs such as LSTM or GRUs (where the underlying optimization can be difﬁcult because\nof the temporally-linear hidden state dependency) and can be efﬁciently implemented so that an entire\nsequence can be processed in a single forward pass. Figure 1 provides an illustration of SNAIL, and\nwe discuss architectural components in Section 3.1.\n2\n",
    "Accepted as a conference paper at ICLR 2018\nSupervised Learning\nReinforcement Learning\n(Examples, \n Labels)\nxt-1\nyt-1\nxt-2\nyt-2\nxt\n--\nxt-3\nyt-3\nPredicted Label\nt\n(Observations,\nActions,\nRewards)\not\nat-1\nrt-1\not-3\n--\n--\not-2\nat-3\nrt-3\not-1\nat-2\nrt-2\nActions\nat-3\nat\nat-2\nat-1\nFigure 1: Overview of our simple neural attentive learner (SNAIL); in this example, two blocks of\nTC layers (orange) are interleaved with two causal attention layers (green). The same class of model\narchitectures can be applied to both supervised and reinforcement learning.\nIn\nsupervised\nsettings,\nSNAIL\nreceives\nas\ninput\na\nsequence\nof\nexample-label\npairs\n(x1, y1), . . . , (xt−1, yt−1) for timesteps 1, . . . , t −1, followed by an unlabeled example (xt, −).\nIt then outputs its prediction for xt based on the previous labeled examples it has seen.\nIn reinforcement-learning settings, it receives a sequence of observation-action-reward tuples\n(o1, −, −), . . . , (ot, at−1, rt−1). At each time t, it outputs a distribution over actions at based on the\ncurrent observation ot as well as previous observations, actions, and rewards. Crucially, following\nexisting work in meta-RL (Duan et al., 2016; Wang et al., 2016), we preserve the internal state of a\nSNAIL across episode boundaries, which allows it to have memory that spans multiple episodes. The\nobservations also contain a binary input that indicates episode termination.\n3.1\nMODULAR BUILDING BLOCKS\nWe compose SNAIL architectures using a few primary building blocks. Below, we provide pseu-\ndocode for applying each block to a matrix (\"inputs\" in the pseudocode) of size (sequence length) ×\n(input dimensionality). Note that, if any of the inputs are images, we employ an additional (spatial)\nconvolutional network that converts the image into a feature vector before it is passed into the SNAIL.\nFigure 2 illustrates the different blocks visually.\nMany techniques have been proposed to increase the capacity or accelerate the training of deep convo-\nlutional architectures, including batch normalization (Ioffe & Szegedy (2015)), residual connections\n(He et al. (2016)), and dense connections (Huang et al. (2016)). We found that these techniques\ngreatly improved the expressive capacity and training speed of SNAILs, but that no particular choice\nof residual/dense conﬁgurations was essential for good performance (we explore the robustness of\nSNAILs to architectural choices in Appendix B).\nA dense block applies a single causal 1D-convolution with dilation rate R and D ﬁlters (we used\nkernel size 2 in all experiments), and then concatenates the result with its input. We used the gated\nactivation function (line 3) introduced by van den Oord et al. (2016a;b).\n1: function DENSEBLOCK(inputs, dilation rate R, number of ﬁlters D):\n2:\nxf, xg = CausalConv(inputs, R, D), CausalConv(inputs, R, D)\n3:\nactivations = tanh(xf) * sigmoid(xg)\n4:\nreturn concat(inputs, activations)\n3\n",
    "Accepted as a conference paper at ICLR 2018\nA TC block consists of a series of dense blocks whose dilation rates increase exponentially until their\nreceptive ﬁeld exceeds the desired sequence length:\n1: function TCBLOCK(inputs, sequence length T, number of ﬁlters D):\n2:\nfor i in 1, . . . , ⌈log2 T⌉do\n3:\ninputs = DenseBlock(inputs, 2i, D)\n4:\nreturn inputs\nA attention block performs a single key-value lookup; we style this operation after the self-attention\nmechanism proposed by Vaswani et al. (2017a):\n1: function ATTENTIONBLOCK(inputs, key size K, value size V ):\n2:\nkeys, query = afﬁne(inputs, K), afﬁne(inputs, K)\n3:\nlogits = matmul(query, transpose(keys))\n4:\nprobs = CausallyMaskedSoftmax(logits /\n√\nK)\n5:\nvalues = afﬁne(inputs, V )\n6:\nread = matmul(probs, values)\n7:\nreturn concat(inputs, read)\nwhere CausallyMaskedSoftmax(·) zeros out the appropriate probabilities before normalization, so\nthat a particular timestep’s query cannot have access to future keys/values.\n(a) Dense Block (dilation rate R, D lters)\nconcatenate\ninputs, shape [T, C]\noutputs, shape [T, C + D] \ncausal conv, kernel 2\ndilation R, D lters\n(b) Attention Block (key size K, value size V)\nconcatenate\ninputs, shape [T, C]\noutputs, shape [T, C + V] \na\nne, output size K \n(query)\na\nne, output size K\n(keys)\na\nne, output size V \n(values)\nmatmul, masked softmax\nmatmul\nFigure 2: Two of the building blocks that compose SNAIL architectures. (a) A dense block applies\na causal 1D-convolution, and then concatenates the output to its input. A TC block (not pictured)\napplies a series of dense blocks with exponentially-increasing dilation rates. (b) A attention block\nperforms a (causal) key-value lookup, and also concatenates the output to the input.\n4\nRELATED WORK\nPioneered by Schmidhuber (1987); Naik & Mammone (1992); Thrun & Pratt (1998), meta-learning\nis not a new idea. A key tradeoff central to many recent meta-learning approaches is between\nperformance and generality; we discuss several notable methods and how they ﬁt into this paradigm.\nGraves et al. (2014) investigated the use of recurrent neural networks (RNNs) to solve algorithmic\ntasks. They experimented with a meta-learner implemented by an LSTM, but their results suggested\nthat LSTM architectures are ill-equipped for these kinds of tasks. They then designed a more\nsophisticated RNN architecture, where an LSTM controller was coupled to an external memory bank\nfrom which it can read and write, and demonstrated that these memory-augmented neural networks\n(MANNs) achieved substantially better performance than LSTMs. Santoro et al. (2016) evaluated\nboth LSTM and MANN meta-learners on few-shot image classiﬁcation, and conﬁrm the inadequacy\n4\n",
    "Accepted as a conference paper at ICLR 2018\nof the LSTM architecture. These approaches are generic, but MANNs feature a complicated memory-\naddressing architecture that is difﬁcult to train – they still suffer from the same temporally-linear\nhidden-state dependencies as LSTMs.\nIn response, several approaches have demonstrated good performance in few-shot classiﬁcation with\nspecialized neural network architectures. Koch (2015) used a Siamese network that was trained to\npredict whether two images belong to the same class. Vinyals et al. (2016) learned an embedding\nfunction and used cosine distance in an attention kernel to judge image similarity. Snell et al. (2017)\nemployed a similar approach to Vinyals et al. (2016), based on Euclidean distance metrics. All three\nmethods work well within the context of classiﬁcation, but are not readily applicable to other domains,\nsuch as reinforcement learning. They perform well because their architectures have been designed\nto exploit domain knowledge, but ideally we would like a meta-learner that is not constrained to a\nparticular problem type.\nA number of methods consider a meta-learner that makes updates to the parameters of a traditional\nlearner (Bengio et al., 1992; Hochreiter et al., 2001). Andrychowicz et al. (2016) and Li & Malik\n(2017) investigated the setting of learning to optimize, where the learner is an objective function to\nminimize, and the meta-learner uses the gradients of the learner to perform the optimization. Their\nmeta-learner was implemented by an LSTM and the strategy that it learned can be interpreted as\na gradient-based optimization algorithm; however, it is unclear whether the learned optimizers are\nsubstantially better than existing SGD-based methods.\nRavi & Larochelle (2017) extended this idea, using a similar LSTM meta-learner in a few-shot\nclassiﬁcation setting, where the traditional learner was a convolutional-network-based classiﬁer. In\nthis setting, the meta-learning algorithm is decomposed into two parts: the traditional learner’s initial\nparameters are trained to be suitable for fast gradient-based adaptation; the LSTM meta-learner is\ntrained to be an optimization algorithm adapted for meta-learning tasks. Finn et al. (2017) explored\na special case where the meta-learner is constrained to use ordinary gradient descent to update the\nlearner and showed that this simpliﬁed model (known as MAML) can achieve equivalent performance.\nMunkhdalai & Yu (2017) explored a more sophisticated weight update scheme that yielded minor\nperformance improvements on few-shot classiﬁcation.\nAll of the methods discussed in the previous paragraph have the beneﬁt of being domain independent,\nbut they explicitly encode a particular strategy for the meta-learner to follow (namely, adaptation via\ngradient descent at test time). In a particular domain, there may exist better strategies that exploit the\nstructure of the task, but gradient-based methods will be unable to discover them. In contrast, SNAIL\npresents an alternative paradigm where a generic architecture has the capacity to learn an algorithm\nthat exploits domain-speciﬁc task structure.\nDuan et al. (2016) and Wang et al. (2016) both investigated meta-learning in reinforcement-learning\ndomains using traditional RNN architectures (GRUs and LSTMs). In addition, Finn et al. (2017)\nexperimented with fast adaptation of policies in continuous control, where the meta-learner was\ntrained on a distribution of closely-related locomotion tasks. In Section 5.2, we benchmark SNAIL\nagainst MAML and an LSTM-based meta-learner on the tasks considered by these works.\n5\nEXPERIMENTS\nOur experiments were designed to investigate the following questions:\n• How does SNAIL’s generality affect its performance on a range of meta-learning tasks?\n• How does its performance compare to existing approaches that are specialized to a particular\ntask domain, or have elements of a high-level strategy already built-in?\n• How does SNAIL scale with high-dimensional inputs and long-term temporal dependencies?\n5.1\nFEW-SHOT IMAGE CLASSIFICATION\nIn the few-shot classiﬁcation setting, we wish to classify data points into N classes when we only\nhave a small number (K) of labeled examples per class. A meta-learner is readily applicable, because\nit learns how to compare input points, rather than memorize a speciﬁc mapping from points to classes.\nThe Omniglot and mini-ImageNet datasets for few-shot image classiﬁcation are the standard bench-\nmarks in supervised meta-learning. Introduced by Lake et al. (2011), Omniglot consists of black-\n5\n",
    "Accepted as a conference paper at ICLR 2018\nand-white images of handwritten characters gathered from 50 languages, for a total of 1632 different\nclasses with 20 instances per class. Like prior works, we downsampled the images to 28 × 28\nand randomly selected 1200 classes for training and 432 for testing. We performed the same data\naugmentation proposed by Santoro et al. (2016), forming new classes by rotating each member of an\nexisting class by a multiple of 90 degrees.\nMini-ImageNet is a more difﬁcult benchmark; a subset of the well-known ImageNet dataset, it\nconsists of 84 × 84 color images from 100 different classes with 600 instances per class. We used the\nsplit released by Ravi & Larochelle (2017) and used by a number of other works, with 64 classes for\ntraining, 16 for validation, and 20 for testing.\nTo evaluate a SNAIL on the N-way, K-shot problem, we sample N classes from the overall dataset\nand K examples of each class. We then feed the corresponding NK example-label pairs to the\nSNAIL in a random order, followed by a new, unlabeled example from one of the N classes. We\nreport the average accuracy on this last, (NK + 1)-th timestep.\nWe tested SNAIL on 5-way Omniglot, 20-way Omniglot, and 5-way mini-ImageNet. For each of\nthese three splits, we trained the SNAIL on episodes where the number of shots K was chosen\nuniformly at random from 1 to 5 (note that this is unlike prior works, who train separate models\nfor each shot). For a K-shot episode within an N-way problem, the loss was simply the average\ncross-entropy between the predicted and true label on the (NK + 1)-th timestep. We train both the\nSNAIL and the feature-extracting embedding network in an end-to-end fashion using Adam (Kingma\n& Ba, 2015) For a complete description of the speciﬁcs SNAIL and embedding architectures we used,\nwe refer the reader to Appendix A.\nTable 1 displays our results on 5-way and 20-way Omniglot, and Table 2 respectively for 5-way\nmini-ImageNet. We see that SNAIL outperforms state-of-the-art methods that are extensively hand-\ndesigned, and/or domain-speciﬁc. It signiﬁcantly exceeds the performance of methods such as\nSantoro et al. (2016) that are similarly simple and generic. In Appendix B, we conduct a number of\nablations to analyse SNAIL’s performance.\nTable 1:\n5-way and 20-way, 1-shot and 5-shot classiﬁcation accuracies on Omniglot, with 95%\nconﬁdence intervals where available. For each task, the best-performing method is highlighted, along\nwith any others whose conﬁdence intervals overlap.\nMethod\n5-Way Omniglot\n20-Way Omniglot\n1-shot\n5-shot\n1-shot\n5-shot\nSantoro et al. (2016)\n82.8%\n94.9%\n–\n–\nKoch (2015)\n97.3%\n98.4%\n88.2%\n97.0%\nVinyals et al. (2016)\n98.1%\n98.9%\n93.8%\n98.5%\nFinn et al. (2017)\n98.7% ± 0.4%\n99.9% ± 0.3%\n95.8% ± 0.3%\n98.9% ± 0.2%\nSnell et al. (2017)\n97.4%\n99.3%\n96.0%\n98.9%\nMunkhdalai & Yu (2017)\n98.9%\n–\n97.0%\n–\nSNAIL, Ours\n99.07% ± 0.16%\n99.78% ± 0.09%\n97.64% ± 0.30%\n99.36% ± 0.18%\nTable 2: 5-way, 1-shot and 5-shot classiﬁcation accuracies on mini-ImageNet, with 95% conﬁdence\nintervals where available. For each task, the best-performing method is highlighted, along with any\nothers whose conﬁdence intervals overlap.\nMethod\n5-Way Mini-ImageNet\n1-shot\n5-shot\nVinyals et al. (2016)\n43.6%\n55.3%\nFinn et al. (2017)\n48.7% ± 1.84%\n63.1% ± 0.92%\nRavi & Larochelle (2017)\n43.4% ± 0.77%\n60.2% ± 0.71%\nSnell et al. (2017)\n46.61% ± 0.78%\n65.77% ± 0.70%\nMunkhdalai & Yu (2017)\n49.21% ± 0.96%\n–\nSNAIL, Ours\n55.71% ± 0.99%\n68.88% ± 0.92%\n6\n",
    "Accepted as a conference paper at ICLR 2018\n5.2\nREINFORCEMENT LEARNING\nReinforcement learning features a number of challenges that supervised learning does not, including\nlong-term temporal dependencies (as the experienced states and rewards may depend on actions taken\nmany timesteps ago) as well as balancing exploration and exploitation. To explore SNAIL‘s ability to\nlearn RL algorithms, we evaluate it on four different domains from prior work in meta-RL1:\n• Multi-armed bandits (Duan et al., 2016; Wang et al., 2016): the agent interacts with a set of\narms whose reward distributions are unknown. Although its actions do not affect its state,\nexploration and exploitation are both essential: an optimal agent must initially explore by\nsampling different arms, but later exploit its knowledge by repeatedly selecting the best arm.\n• Tabular MDPs (Duan et al., 2016; Wang et al., 2016): we procedurally generate random\nMDPs and allow the agent to act within each one for multiple episodes. Since every MDP is\ndifferent, a meta-learner cannot simply memorize the ones it is trained on; it must actually\nlearn an algorithm for solving MDPs.\n• Visual navigation (Duan et al., 2016; Wang et al., 2016): the agent must navigate randomly-\ngenerated mazes to ﬁnd a randomly-located goal, using only visual observations as input. It\nis allowed to interact with the same maze/goal conﬁguration for two episodes, so an optimal\nagent should explore the maze on the ﬁrst episode to ﬁnd the goal, and then go directly to\nthe goal on the second episode. This task features many of the common challenges in deep\nRL, including high-dimensional observations, partial observability, and sparse rewards.\n• Continuous control (Finn et al., 2017): we consider a suite of simulated locomotion tasks.\nAlthough the environment dynamics are complex, the underlying task distribution is quite\nnarrow. As a result, there is signiﬁcant task structure for a meta-learner to exploit; the\noptimal strategy is closer to task-identiﬁcation than a true RL algorithm.\nOn each of these domains, we trained a SNAIL, along with two meta-learning baselines:\n• An LSTM-based meta-learner, as concurrently proposed by Duan et al. (2016); Wang et al.\n(2016). We refer to this method as “LSTM\" in the tables and ﬁgures in subsequent sections.\n• MAML, the method introduced by Finn et al. (2017). It trains the initial parameters of a\npolicy to achieve maximal performance after one (policy) gradient update on a new task.\nWe also conducted some ablation experiments, which are detailed in Appendix D.\nIn all domains, we trained the meta-learners using trust region policy optimization with generalized\nadvantage estimation (TRPO with GAE; Schulman et al. (2015; 2016)); the SNAIL architectures and\nTRPO/GAE hyperparameters are detailed in Appendix C.\nIn the bandit and MDP domains, there exist a number of human-designed algorithms with various\noptimality guarantees (which we discuss in more depth in the subsequent sections). Although there\nisn’t much task structure for a meta-learner to exploit, the existence of upper bounds on asymptotic\nperformance let us evaluate the optimality of a meta-learned algorithm.\nHowever, the true utility of a meta-learner is that it can learn an algorithm specialized to the particular\ndistribution of tasks it is trained on. We evaluate this in the visual navigation and continuous control\ndomains, where there is signiﬁcant task structure for the meta-learner to exploit, but no optimal\nalgorithms are known to exist due to the task complexity.\n5.2.1\nMULTI-ARMED BANDITS\nIn our bandit experiments (styled after Duan et al. (2016)), each of K arms gives rewards according\nto a Bernoulli distribution whose parameter p ∈[0, 1] is chosen randomly at the start of each episode\nof length N. At each timestep, the meta-learner receives previous timestep’s reward, along with a\none-hot encoding of the corresponding arm selected. It outputs a discrete probability distribution over\nthe K arms; the selected arm is determined by sampling from this distribution.\nAs an oracle, we consider the Gittins index (Gittins, 1979), the Bayes optimal solution in the\ndiscounted, inﬁnite horizon setting. Since it is only optimal as N →∞, a meta-learner can\noutperform it for smaller N by choosing to exploit sooner.\nFollowing Duan et al. (2016), we tested all combinations of N = 10, 100, 500 and K = 5, 10, 50.\nWe also tested the additional case of N = 1000, K = 50 to further evaluate the scalability of SNAIL\nto longer sequences. We report the mean reward per episode for each setting; the results are given\nin Table 3 with 95% conﬁdence intervals where available. We found that training MAML was too\ncomputationally expensive for N = 500, 1000; hence we omit those results from Table 3.\n1Some video results can be found at https://sites.google.com/view/snail-iclr-2018/.\n7\n",
    "Accepted as a conference paper at ICLR 2018\nTable 3: Results on multi-arm bandit problems. For each, we highlighted the best performing method,\nand any others whose performance is not statistically-signiﬁcantly different (based on a one-sided\nt-test with p = 0.05). Except for SNAIL and MAML, we report the results from Duan et al. (2016).\nSetup\nMethod\n(N, K)\nGittins\n(optimal as N →∞)\nRandom\nLSTM\nMAML\nSNAIL (ours)\n10, 5\n6.6\n5.0\n6.7\n6.5 ± 0.1\n6.6 ± 0.1\n10, 10\n6.6\n5.0\n6.7\n6.6 ± 0.1\n6.7 ± 0.1\n10, 50\n6.5\n5.1\n6.8\n6.6 ± 0.1\n6.7 ± 0.1\n100, 5\n78.3\n49.9\n78.7\n67.1 ± 1.1\n79.1 ± 1.0\n100, 10\n82.8\n49.9\n83.5\n70.1 ± 0.6\n83.5 ± 0.8\n100, 50\n85.2\n49.8\n84.9\n70.3 ± 0.4\n85.1 ± 0.6\n500, 5\n405.8\n249.8\n401.5\n–\n408.1 ± 4.9\n500, 10\n437.8\n249.0\n432.5\n–\n432.4 ± 3.5\n500, 50\n463.7\n249.6\n438.9\n–\n442.6 ± 2.5\n1000, 50\n944.1\n499.8\n847.43\n–\n889.8 ± 5.6\n5.2.2\nTABULAR MDPS\nIn our tabular MDP experiments (also following Duan et al. (2016)), each MDP had 10 states and 5\nactions (both discrete); the reward for each (state, action)-pair followed a normal distribution with\nunit variance where the mean was sampled from N(1, 1), and the transitions are sampled from a ﬂat\nDirichlet distribution (the latter is a commonly used prior in Bayesian RL) with random parameters.\nWe allowed each meta-learner to interact with an MDP for N episodes of length 10. As input, they\nreceived one-hot encodings of the current state and previous action, the previous reward received,\nand a binary ﬂag indicating termination of the current episode.\nIn addition to a random agent, we consider the follow human-designed algorithms as baselines.\n• PSRL (Strens, 2000): a Bayesian method that estimate the belief over the current MDP\nparameters. At the start of each of the N episodes, it samples an MDP from the current\nposterior, and acts according to the optimal policy for the rest of the episode.\n• OPSRL (Osband & Van Roy, 2017): an optimistic variant of PSRL.\n• UCRL2 (Jaksch et al., 2010): uses an extended value iteration procedure to compute an\noptimistic MDP under the current belief.\n• ϵ-greedy: with probability 1 −ϵ, act optimally against the MAP estimate according to the\ncurrent posterior (which is updated once per episode).\nAs an oracle, we run value iteration for 10 iterations (the episode length) on each MDP. Value\niteration is optimal when the MDP parameters (reward function, transition probabilities) are known;\nthus, the resulting values provide an upper bound on the performance of any algorithm, whether\nhuman-designed or meta-learned (which do not receive the MDP parameters).\nWe tested N = 10, 25, 50, 75, 100; in Table 4, we report the performance normalized by the value-\niteration upper bound. As N increases, performance should approach 1, as the algorithm learns more\nabout the current MDP. Similarly to the bandit experiments, we could not train MAML successfully\nfor N = 50, 75, 100. In Figure 3, we show learning curves of SNAIL and LSTM.\n0\n1500\nn = 10\n0\n6000\nn = 25\n0\n6000\nn = 50\n0\n6000\nn = 75\n0\n6000\nn = 100\nLSTM\nSNAIL (ours)\nFigure 3: Learning curves of SNAIL (red) and LSTM (blue) on the random MDP task for different\nvalues of N. The horizontal axis is the TRPO iteration, and the vertical is average reward.\n8\n",
    "Accepted as a conference paper at ICLR 2018\nTable 4: Performance on tabular MDPs, scaled by the average reward achieved by value iteration.\nAs before, we highlight the best-performing method, and any others whose performance is not\nstatistically-signiﬁcantly different (using the same one-sided t-test with p = 0.05). Except for SNAIL\nand MAML, we report the values from Duan et al. (2016).\nN\nMethod\nRandom\nϵ-greedy\nPSRL\nOPSRL\nUCRL2\nLSTM\nMAML\nSNAIL (ours)\n10\n0.482\n0.640\n0.665\n0.694\n0.706\n0.752\n0.563\n0.766 ± 0.001\n25\n0.482\n0.727\n0.788\n0.819\n0.817\n0.859\n0.591\n0.862 ± 0.001\n50\n0.481\n0.793\n0.871\n0.897\n0.885\n0.902\n–\n0.908 ± 0.003\n75\n0.482\n0.831\n0.910\n0.931\n0.917\n0.918\n–\n0.930 ± 0.002\n100\n0.481\n0.857\n0.934\n0.951\n0.936\n0.922\n–\n0.941 ± 0.003\n5.2.3\nCONTINUOUS CONTROL\nWe consider the set of tasks introduced by Finn et al. (2017), in which two simulated robots (a planar\ncheetah and a 3D-quadruped ant) have to run in a particular direction or at a speciﬁed velocity (the\ndirection or velocity are chosen randomly and not told to the agent). In the goal direction experiments,\nthe reward is the magnitude of the robot’s velocity in either the forward or backward direction, and in\nthe goal velocity experiments, the reward is the negative absolute value between its current forward\nvelocity and the goal. The observations are the robot’s joint angles and velocities, and the actions\nare its joint torques. For each of these four task distributions ({ant, cheetah} × {goal velocity, goal\ndirection}), Finn et al. (2017) trained a policy to maximize its performance after one policy gradient\nupdate using 20 episodes (40 for ant), of 200 timesteps each, on a newly sampled task.\nWe trained both SNAIL and LSTM on each of these four task categories. Since they do not update\ntheir parameters at test time (instead incorporating experience through their hidden state), SNAIL\nand LSTM receive as input the previous action, previous reward, and an episode-termination ﬂag in\naddition to the current observation. We found that two episodes of interaction was sufﬁcient for these\nmeta-learners to adapt to a task, and that unrolling them for longer did not improve performance.\nIn Figure 4, we show how the different methods adapt to a new task. As an oracle, we sampled tasks\nfrom each distribution, and trained a separate policy for each task. We plot the average performance\nof the oracle policies for each task distribution as an upper bound on a meta-learner’s performance.\nQualitatively, we can think of MAML as applying a general-purpose strategy (namely, gradient\ndescent) to a distribution of highly-structured tasks. In contrast, SNAIL and LSTM are able to\nspecialize themselves based on the shared task structure, enabling them to identify the task within the\ninitial timesteps of the ﬁrst episode, and then act optimally thereafter.\n0\n20\n40\n60\n80\n100\n120\nnumber of ro  outs\n40\n60\n80\n100\n120\n140\n160\n180\naverage return\nant, goa  ve ocity\n0\n10\n20\n30\n40\n50\n60\nnumber of ro  outs\n0\n100\n200\n300\n400\n500\n600\n700\naverage return\ncheetah, forward/backward\n0\n20\n40\n60\n80\n100\n120\nnumber of ro  outs\n0\n200\n400\n600\n800\n1000\naverage return\nant, forward/backward\n0\n10\n20\n30\n40\n50\n60\nnumber of ro  outs\n−120\n−100\n−80\n−60\n−40\naverage return\ncheetah, goa  ve ocity\nLSTM\nSNAIL (ours)\nMAML\nOracle\nFigure 4: Test-time adaptation curves on simulated locomotion tasks for SNAIL, LSTM, and MAML\n(which was unrolled for three policy gradient updates). Since SNAIL incorporates experience through\nits hidden state, it can exploit common task structure to perform optimally within a few timesteps.\n5.2.4\nVISUAL NAVIGATION\nBoth Duan et al. (2016) and Wang et al. (2016) consider the task of visual navigation, where the agent\nmust ﬁnd a target in a maze using only visual inputs. The former used randomly-generated mazes\nand target positions, while the latter used a ﬁxed maze and only four different target positions. Hence,\nwe evaluated SNAIL on the former, more challenging task. The observations the agent receives are\n9\n",
    "Accepted as a conference paper at ICLR 2018\n30 × 40 ﬁrst-person images, and the actions it can take are {step forward, turn slightly left, turn\nslightly right}. We constructed a training dataset and two test datasets (unseen mazes of the same and\nlarger size, respectively), each with 1000 mazes. The agents were allowed to interact with each maze\nfor 2 episodes, with episode length 250 (1000 in the larger mazes). The starting and goal locations\nwere chosen randomly for each trial but remained ﬁxed within each pair of episodes. The agents\nreceived rewards of +1 for reaching the target (which resulted in the episode terminating), -0.01 at\neach timestep, to encourage it to reach the goal faster, and -0.001 for hitting the wall. Figure 5 depicts\nan example of the observations as well as sample maze layouts.\nWe evaluate each method using the average episode length, for both the ﬁrst and second episode\nwithin a trial. The results are displayed in Table 5. Since MAML scaled poorly to long sequences\nin the bandit and MDP domains, we did not evaluate it on this domain; the computational expense\nwas prohibitively high. Qualitatively, we observe that the optimal strategy does indeed emerge: the\nSNAIL agent explores the maze during the ﬁrst episode, and then, after ﬁnding the goal, goes directly\nthere on the second episode (the LSTM agent also exhibits this behavior, but has a harder time\nremembering where the goal is). An illustration is depicted in Figure 5.\nTable 5: Average time to ﬁnd the goal on each episode in the small and large mazes. SNAIL solves\nthe mazes the fastest, and improves the most from the ﬁrst to second episode.\nMethod\nSmall Maze\nLarge Maze\nEpisode 1\nEpisode 2\nEpisode 1\nEpisode 2\nRandom\n188.6 ± 3.5\n187.7 ± 3.5\n420.2 ± 1.2\n420.8 ± 1.2\nLSTM\n52.4 ± 1.3\n39.1 ± 0.9\n180.1 ± 6.0\n150.6 ± 5.9\nSNAIL (ours)\n50.3 ± 0.3\n34.8 ± 0.2\n140.5 ± 4.2\n105.9 ± 2.4\nFigure 5: From left to right: (a) A (higher-resolution) example of the observations the agent receives.\n(b) An example of the mazes used for training (goal shown in blue). (c) The movement of the SNAIL\non its ﬁrst episode in a larger maze, exploring the maze until it ﬁnds the goal. (d) The SNAIL’s path\nduring its second episode in the same maze as (c). Remembering the goal location, it navigates there\ndirectly on the second episode. Maps like in (b), (c), (d) are used for visualization but not available to\nthe agent. In (c), (d), the color progression from red to blue indicates the passage of time (red earlier).\n6\nCONCLUSION AND FUTURE WORK\nWe presented a simple and generic class of architectures for meta-learning, motivated by the need\nfor a meta-learner to quickly incorporate and refer to past experience. Our simple neural attentive\nlearner (SNAIL) utilizes a novel combination of temporal convolutions and causal attention, two\nbuilding blocks of sequence-to-sequence models that have complementary strengths and weaknesses.\nWe demonstrate that SNAIL achieves state-of-the-art performance by signiﬁcant margins on all of\nthe most-widely benchmarked meta-learning tasks in both supervised and reinforcement learning,\nwithout relying on any application-speciﬁc architectural components or algorithmic priors.\nAlthough we designed SNAIL with meta-learning in mind, it would likely excel at other sequence-to-\nsequence tasks, such as language modeling or translation; we plan to explore this in future work.\nAnother interesting idea would be to train an meta-learner that can attend over its entire lifetime\nof experience (rather than only a few recent episodes, as in this work). An agent with this lifelong\nmemory could learn faster and generalize better; however, to keep the computational requirements\npractical, it would also need to learn how to decide what experiences are worth remembering.\n10\n",
    "Accepted as a conference paper at ICLR 2018\nREFERENCES\nMarcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,\nand Nando de Freitas. Learning to learn by gradient descent by gradient descent. In Advances in\nNeural Information Processing Systems (NIPS), 2016.\nSamy Bengio, Yoshua Bengio, Jocelyn Cloutier, and Jan Gecsei. On the optimization of a synaptic\nlearning rule. In Optimality in Artiﬁcial and Biological Neural Networks, pp. 6–8. Univ. of Texas,\n1992.\nYan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl$ˆ2$:\nFast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779,\n2016.\nChelsea Finn, Pieter Abbeel, and Sergy Levine. Model-agnostic meta learning. International\nConference on Machine Learning (ICML), 2017.\nJ.C. Gittins. Bandit processes and dynamic allocation indices. Journal of the Royal Statistical Society.\nSeries B (Methodological), 1979.\nAlex Graves, Greg Wayne, and Ivo Danihelka.\nNeural turing machines.\narXiv preprint\narXiv:1410.5401, 2014.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\nSepp Hochreiter, A Younger, and Peter Conwell. Learning to learn using gradient descent. Artiﬁcial\nNeural Networks, ICANN, 2001.\nGao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected\nconvolutional networks. arXiv preprint arXiv:1608.06993, 2016.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. International Conference on Machine Learning (ICML), 2015.\nThomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement\nlearning. Journal of Machine Learning Research, 2010.\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International\nConference on Learning Representations (ICLR), 2015.\nGregory Koch. Siamese neural networks for one-shot image recognition. PhD thesis, University of\nToronto, 2015.\nBrenden M Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua B Tenenbaum. One shot learning\nof simple visual concepts. In CogSci, 2011.\nKe Li and Jitendra Malik. Learning to optimize. International Conference on Learning Representa-\ntions (ICLR), 2017.\nTsendsuren Munkhdalai and Hong Yu. Meta networks. International Conference on Machine\nLearning (ICML), 2017.\nDevang K Naik and RJ Mammone. Meta-neural networks that learn by learning. In Neural Networks,\n1992. IJCNN., International Joint Conference on, volume 1, pp. 437–442. IEEE, 1992.\nIan Osband and Benjamin Van Roy. Why is posterior sampling better than optimism for reinforcement\nlearning. International Conference on Machine Learning (ICML), 2017.\nSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International\nConference on Learning Representations (ICLR), 2017.\nAdam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-\nlearning with memory-augmented neural networks. In International Conference on Machine\nLearning (ICML), 2016.\n11\n",
    "Accepted as a conference paper at ICLR 2018\nJurgen Schmidhuber. Evolutionary principles in self-referential learning. On learning how to learn:\nThe meta-meta-... hook.) Diploma thesis, Institut f. Informatik, Tech. Univ. Munich, 1987.\nJohn Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel. Trust region\npolicy optimization. International Conference on Machine Learning (ICML), 2015.\nJohn Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional\ncontinuous control using generalized advantage estimation. International Conference on Learning\nRepresentations (ICLR), 2016.\nJake Snell, Kevin Swersky, and Richard S Zemel. Prototypical networks for few-shot learning. arXiv\npreprint arXiv:1703.05175, 2017.\nMalcolm Strens. A bayesian framework for reinforcement learning. In International Conference on\nMachine Learning (ICML), 2000.\nSebastian Thrun and Lorien Pratt. Learning to learn: Introduction and overview. In Learning to learn.\nSpringer, 1998.\nAaron van den Oord, Sander Dieleman, Heig Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal\nKalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw\naudio. CoRR, abs/1609.03499, 2016a.\nAaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional\nimage generation with pixelcnn decoders. In Advances in Neural Information Processing Systems\n(NIPS), 2016b.\nAshish Vaswani, Noah Shazeer, Jakob Uszkoreit, Llion Jones, Aidan Gomez N., Lukas Kaiser, and\nIllia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017a.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017b.\nOriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one shot\nlearning. In Advances in Neural Information Processing Systems (NIPS), 2016.\nJane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos,\nCharles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv\npreprint arXiv:1611.05763, 2016.\n12\n",
    "Accepted as a conference paper at ICLR 2018\nAPPENDIX\nA\nFEW-SHOT CLASSIFICATION ARCHITECTURES\nWith the building blocks deﬁned in Section 3.1, we can concisely describe SNAIL architectures. We\nused the same SNAIL architecture for both Omniglot and mini-Imagenet. For the N-way, K-shot\nproblem, the sequence length is T = NK + 1, and we used the following: AttentionBlock(64, 32),\nTCBlock(T, 128), AttentionBlock(256, 128), TCBlock(T, 128), AttentionBlock(512, 256), followed\nby a ﬁnal 1 × 1 convolution with N ﬁlters.\nFor the Omniglot dataset, we used the same embedding network architecture as all prior works, which\nrepeat the following block four times { 3x3 conv (64 channels), batch norm, ReLU, 2x2 max pool },\nand then apply a single fully-connected layer to output a 64-dimensional feature vector.\nFor mini-Imagenet, existing gradient-descent-based methods (Ravi & Larochelle, 2017; Finn et al.,\n2017), which update their model’s weights during testing, used the same network structure as the\nOmniglot network but reduced the number of channels to 32, in spite of the signiﬁcantly-increased\ncomplexity of the images. We found that this shallow embedding network did not make adequate\nuse of SNAIL’s expressive capacity, and opted to to use a deeper embedding network to prevent\nunderﬁtting (in Appendix B, we conduct ablations regarding this decision). Illustrated in Figure 6,\nour embedding was a smaller version of the ResNet (He et al., 2016) architectures commonly used\nfor the full Imagenet dataset.\n(b) mini-Imagenet embedding\n(a) Residual Block, D lters\n2x2 max-pool,\ndropout 0.9\n3x3 conv, D lters\nbatch norm\nleaky ReLU (leak 0.1)\nAdd\n1x1 conv,\nD lters\n3x3 conv, D lters\nbatch norm\nleaky ReLU (leak 0.1)\n3x3 conv, D lters\nbatch norm\nleaky ReLU (leak 0.1)\n384-dimensional \nfeature vector\n1x1 conv, 2048 lters\n6x6 mean pool, ReLU\ndropout 0.9\n1x1 conv, 384 lters\ninput image, 84x84x3\nResidual Block\n64 lters\nResidual Block\n96 lters\nResidual Block\n128 lters\nResidual Block\n256 lters\n(a) Residual Block, D lters\nFigure 6: (a) A residual block within our mini-Imagenet embedding. (b) The embedding, a smaller\nversion of ResNet (He et al., 2016), uses several of the residual blocks depicted in (a).\n13\n",
    "Accepted as a conference paper at ICLR 2018\nB\nFEW-SHOT CLASSIFICATION: ABLATIONS\nTo investigate the contribution of different components (TC, attention, and deeper embedding in the\ncase of mini-Imagenet) to SNAIL’s performance, we conducted a number of ablations, which are\nsummarized in Table 6. From these ablations, we draw two conclusions:\n• Both TC and attention layers are essential for maximal performance. When we remove\neither one, the resulting model is still competitive with other state-of-the-art methods, but\nthe combination yields the best performance. Notably, compared to the full model, using\nonly TC layers results in similar 1-shot performance but worse 5-shot. In Section 3 we\ndiscussed how temporal convolutions have coarser access to inputs farther back in time; this\nillustrates that this effect is relevant even at sequence length 26.\n• SNAIL’s improved performance is not purely a result of the deeper embedding network.\nGradient-based methods (we tested MAML; Finn et al. (2017)) overﬁt signiﬁcantly when\nthey use our embedding, and domain-speciﬁc RNN-based methods (Vinyals et al., 2016)\ndon’t utilize the extra capacity as well as SNAIL does.\nTable 6: The ablations we conducted on the few-shot classiﬁcation task. From these, we conclude\nthat (i) both TC and attention are essential for the best performance, and (ii) SNAIL‘s improved\nperformance cannot be entirely explained by the deeper embedding.\nAblation\nResult\nReplace SNAIL with stacked LSTM.\nVaried number of layers and their sizes\n(with similar number of parameters to\nSNAIL).\n5-way Omniglot: 78.1% and 90.8% (1-shot, 5-shot).\nWe were unable to successfully train this method on mini-\nImagenet.\nSNAIL with shallow mini-Imagenet em-\nbedding.\n5-way mini-Imagenet: 45.1% and 55.2% (1-shot, 5-shot).\nMAML (Finn et al., 2017), a state-of-\nthe-art gradient-based method, with our\ndeeper mini-Imagenet embedding.\nIt overﬁts tremendously; for 1-shot, 5-way mini-ImageNet:\n30.1% & 75.2% on the test and training set respectively.\nMAML trains separate models for 1-shot and 5-shot; we\ndidn’t train a 5-shot model because the 1-shot did so poorly.\nSNAIl, no TC layers (only attention).\nThis is a generalization of the method\nused by Vinyals et al. (2016), as they\nonly use a single attentive read and ex-\nplicitly force the keys to be features of\nthe image and the values to be the labels.\nWe experimented with multiple paral-\nlel reads (often referred to as multiple\nheads) as well as up to three consecutive\nattentive blocks.\nOn 5-way and 20-way Omniglot: equivalent performance to\nthe full model.\n5-way mini-Imagenet: 49.9% and 63.9% (1-shot, 5-shot).\nSNAIL, no attention (TC layers only).\nOn 5-way Omniglot: 98.8% and 99.2% (1-shot, 5-shot).\nWe were unable to train 20-way Omniglot using this method.\nOn 5-way mini-Imagenet: 55.1% and 61.2%.\nIn an attempt to analyse the learned feature representation, we tried using the features learned by\nthe Omniglot embedding in a nearest-neighbor classiﬁer, using both cosine and Euclidean distance.\nOn 5-way Omniglot, this achieves 65.1% and 67.1% (1-shot and 5-shot) for Euclidean distance and\n67.7% and 68.3% for cosine. Although SNAIL must be comparing images in order to successfully\nmake few-shot predictions, this suggests that the strategy it learns is more sophisticated than either\nof these distance metrics. We contrast this with Vinyals et al. (2016) and Snell et al. (2017), who\nexplicitly enforce such representations on the meta-learned strategy.\n14\n",
    "Accepted as a conference paper at ICLR 2018\nIn addition, we investigated how sensitive SNAILs are to architectural design choices by sampling\nrandom permutations of the different components introduced in Section 3.1. We chose each com-\nponent uniformly at random from six options: { AttentionBlock(128, 64), DenseBlock(R, 128)\nfor R ∈{1, 2, 4, 8, 16}}. We sampled architectures with 13 layers each (for consistency with our\nprimary model), and trained them on 5-way Omniglot. Averaged across 3 runs, these SNAILs\nachieved 98.62% ± 0.13% and 99.71% ± 0.08% for 1-shot and 5-shot, essentially matching the\nstate-of-the-art performance of our primary architecture.\nFinally, we explored the dependence of the classiﬁcation strategy learned by SNAIL on the dataset it\nwas trained on. If it truly learned an algorithm for few-shot classiﬁcation, then a SNAIL trained on\nimages from a particular domain should easily transfer to a new domain (such as between Omniglot\nand mini-Imagenet). To test this hypothesis:\n• First, we took a SNAIL trained on 5-way Omniglot, ﬁxed its weights, and re-learned an\nembedding for mini-Imagenet. Despite the SNAIL weights not being trained for mini-\nImagenet, this method was able to achieve 50.62% and 62.34% on 1-shot and 5-shot.\n• Then, we tried this in the reverse direction (freezing the SNAIL weights from mini-Imagenet,\nand re-learning an embedding for 5-way Omniglot), and this attained 98.66% and 99.56%.\n• Lastly, we combined an embedding trained on 5-way Omniglot with a SNAIL trained on\n5-way mini-Imagenet (with a single linear layer in between, to handle the difference in\nfeature vector dimensionality). We trained this model on 5-way Omniglot, where only the\nweights of the intermediate linear layer could be updated. It achieved 98.5% and 99.5%.\nAll of these results are very competitive with the state-of-the-art, suggesting a strong degree of\ntransferability of the algorithm and feature representation learned by SNAIL. An interesting idea\nfor future work in zero-shot learning would be to learn embeddings for multiple datasets in an\nunsupervised manner, but with some mild distributional constraints imposed on the output feature\nrepresentation. Then, one could train a SNAIL on one dataset, and have it transfer to new datasets\nwithout a single labeled example.\n15\n",
    "Accepted as a conference paper at ICLR 2018\nC\nREINFORCEMENT LEARNING\nC.1\nMULTI-ARMED BANDIT AND TABULAR MDP ARCHITECTURES\nFor the N-timestep, K-arm bandit problem, the total trajectory length is T = N. For the MDP\nproblem with N episodes per MDP, it is T = 10N (since each episode lasts for 10 timesteps).\nFor multi-arm bandits and tabular MDPs, we used the same architecture. First, we applied a fully-\nconnected layer with 32 outputs that was shared between the policy and value function. Then the\npolicy used: TCBlock(T, 32), TCBlock(T, 32), AttentionBlock(32, 32). The value function used:\nTCBlock(T, 16), TCBlock(T, 16), AttentionBlock(16, 16).\nWe found that removing the attention blocks made no difference in performance on the bandit\nproblems, whereas SNAILs without attention could not learn to solve MDPs.\nC.2\nCONTINUOUS CONTROL ARCHITECTURES\nFor each simulation locomotion task, the total trajectory length was T = 400 (2 episodes of 200\ntimesteps each). We used the same architecture (shared between policy and value function) for\nall tasks: two fully-connected layers of size 256 with tanh nonlinearities, AttentionBlock(32, 32),\nTCBlock(T, 16), TCBlock(T, 16), AttentionBlock(32, 32). Then the policy and value function\napplied separate fully-connected layers to produce the requisite output dimensionalities.\nC.3\nVISUAL NAVIGATION ARCHITECTURES\nUnlike the other RL tasks we considered, the observations in this domain include images. We\npreprocess the images using the same convolutional architecture as Duan et al. (2016): two layers\nwith {kernel size 5 × 5, 16 ﬁlters, stride 2, ReLU nonlinearity}, whose output is then ﬂattened and\nthen passed to a fully-connected layer to produce a feature vector of size 256.\nThe total trajectory length was T = 500 (2 episodes of 250 timesteps each). For the policy, we used:\nTCBlock(T, 32), AttentionBlock(16, 16), TCBlock(T, 32), AttentionBlock(16, 16). For the value\nfunction we used: TCBlock(T, 16), TCBlock(T, 16).\nC.4\nADDITIONAL REINFORCEMENT LEARNING HYPERPARAMETERS\nAs discussed in Section 5.2, we trained all policies using trust-region policy optimization with gener-\nalized advantage estimation (TRPO with GAE, Schulman et al. (2015; 2016)). The hyperparameters\nare listed in Table 7. For multi-armed bandits, tabular MDPs, and visual navigation, we used the\nsame hyperparamters as Duan et al. (2016) to make our results directly comparable; additional tuning\ncould potentially improve SNAIL’s performance.\nTable 7: The TRPO + GAE hyperparameters we used in our RL experiments.\nHyperparameter\nMulti-armed Bandits\nTabular MDPs\nContinuous Control\nVisual Navigation\nBatch Size (timesteps)\n250K\n250K\n50K\n50K\nDiscount\n0.99\n0.99\n0.99\n0.99\nGAE λ\n0.3\n0.3\n0.97\n0.99\nMean KL\n0.01\n0.01\n0.01\n0.01\n16\n",
    "Accepted as a conference paper at ICLR 2018\nD\nREINFORCEMENT LEARNING: ABLATIONS\nHere we conduct a few ablations on RL tasks: we explore whether an agent relying only on TC layers\nor only on attention layers can solve the multi-armed bandit or MDP tasks from in Section 5.2.\nFirst, we consider an SNAIL agent without attention layers (only TC layers, which amounts to a\nvariant of the WaveNet architecture introduced by van den Oord et al. (2016a)).\nWhen applied to the bandit domain, we found that this TC-only model performed just as well as a\ncomplete SNAIL. This is likely due to the simplicity of this task domain, as successful performance\non bandit problems does not require maintaining a large memory of past experience. Indeed, many\nhuman designed algorithms (including the asymptotically optimal Gittins index) simply update\nrunning statistics at each timestep.\nHowever, this model struggled in the MDP domain, where a more sophisticated algorithm is required.\nThe results are in the table below (with those of a random agent, SNAIL, LSTM and MAML\nduplicated from Table 4 for reference). This agent’s asymptotic suboptimality suggests that its ability\nto internalize past experience is being saturated.\nTable 8: Ablations of SNAIL in the MDP domain.\nN\nMethod\nRandom\nLSTM\nMAML\nSNAIL\nSNAIL, TC-only\n10\n0.482\n0.752\n0.563\n0.766 ± 0.001\n0.616 ±0.001\n25\n0.482\n0.859\n0.591\n0.862 ± 0.001\n0.684 ±0.001\n50\n0.481\n0.902\n–\n0.908 ± 0.003\n0.699 ±0.002\n75\n0.482\n0.918\n–\n0.930 ± 0.002\n0.726 ±0.002\n100\n0.481\n0.922\n–\n0.941 ± 0.003\n0.728 ±0.003\nNext, we considered a SNAIL agent without TC layers (only attention). Due to the sequential nature\nof RL tasks, we employed the positional encoding proposed by Vaswani et al. (2017b). This model,\nwhich is equivalent to their Transformer architecture, could not solve the bandit or MDP tasks. In both\ndomains, its performance was no better than random. To no avail, we experimented with multiple\nblocks of attention and multiple heads per block.\nWe hypothesize that this architecture’s inadequacy stems from the fact that pure attentive lookups\ncannot easily process sequential information. Despite their inﬁnite receptive ﬁeld, they cannot directly\ncompare two adjacent timesteps (such as a single state-action-state transition) in the same way as a\nsingle convolution can. The TC layers are essential because they allow the agent to locally analyse\ncontiguous parts of a sequence to produce a better contextual representation over which to attend.\n17\n"
  ],
  "full_text": "Accepted as a conference paper at ICLR 2018\nA SIMPLE NEURAL ATTENTIVE META-LEARNER\nNikhil Mishra ∗†\nMostafa Rohaninejad∗\nXi Chen†\nPieter Abbeel†\nUC Berkeley, Department of Electrical Engineering and Computer Science\nEmbodied Intelligence\n{nmishra, rohaninejadm, c.xi, pabbeel}@berkeley.edu\nABSTRACT\nDeep neural networks excel in regimes with large amounts of data, but tend to\nstruggle when data is scarce or when they need to adapt quickly to changes in the\ntask. In response, recent work in meta-learning proposes training a meta-learner\non a distribution of similar tasks, in the hopes of generalization to novel but related\ntasks by learning a high-level strategy that captures the essence of the problem it is\nasked to solve. However, many recent meta-learning approaches are extensively\nhand-designed, either using architectures specialized to a particular application, or\nhard-coding algorithmic components that constrain how the meta-learner solves\nthe task. We propose a class of simple and generic meta-learner architectures that\nuse a novel combination of temporal convolutions and soft attention; the former to\naggregate information from past experience and the latter to pinpoint speciﬁc pieces\nof information. In the most extensive set of meta-learning experiments to date,\nwe evaluate the resulting Simple Neural AttentIve Learner (or SNAIL) on several\nheavily-benchmarked tasks. On all tasks, in both supervised and reinforcement\nlearning, SNAIL attains state-of-the-art performance by signiﬁcant margins.\n1\nINTRODUCTION\nThe ability to learn quickly is a key characteristic that distinguishes human intelligence from its\nartiﬁcial counterpart. Humans effectively utilize prior knowledge and experiences to learn new skills\nquickly. However, artiﬁcial learners trained with traditional supervised-learning or reinforcement-\nlearning methods generally perform poorly when only a small amount of data is available or when\nthey need to adapt to a changing task.\nMeta-learning seeks to resolve this deﬁciency by broadening the learner’s scope to a distribution of\nrelated tasks. Rather than training the learner on a single task (with the goal of generalizing to unseen\nsamples from a similar data distribution) a meta-learner is trained on a distribution of similar tasks,\nwith the goal of learning a strategy that generalizes to related but unseen tasks from a similar task\ndistribution. Traditionally, a successful learner discovers a rule that generalizes across data points,\nwhile a successful meta-learner learns an algorithm that generalizes across tasks.\nMany recently-proposed meta-learning methods demonstrate improved performance at the expense of\nbeing hand-designed at either the architectural or algorithmic level. Some have been engineered with\na particular application in mind, while others have aspects of a particular high-level strategy already\nbuilt into them. However, the optimal strategy for an arbitrary range of tasks may not be obvious to\nthe humans designing a meta-learner, in which case the meta-learner should have the ﬂexibility to\nlearn the best way to solve the tasks it is presented with. Such a meta-learner would need to have an\nexpressive, versatile model architecture, in order to learn a range of strategies in a variety of domains.\nMeta-learning can be formalized as a sequence-to-sequence problem; in existing approaches that\nadopt this view, the bottleneck is in the meta-learner’s ability to internalize and refer to past experience.\nThus, we propose a class of model architectures that addresses this shortcoming: we combine temporal\nconvolutions, which enable the meta-learner to aggregate contextual information from past experience,\nwith causal attention, which allow it to pinpoint speciﬁc pieces of information within that context. We\nevaluate this Simple Neural AttenIve Learner (SNAIL) on several heavily-benchmarked meta-learning\ntasks, including the Omniglot and mini-Imagenet datasets in supervised learning, and multi-armed\nbandits, tabular Markov Decision processes (MDPs), visual navigation, and continuous control in\nreinforcement learning. In all domains, SNAIL achieves state-of-the-art performance by signiﬁcant\nmargins, outperforming methods that are domain-speciﬁc or rely on built-in algorithmic priors.\n∗Authors contributed equally and are listed in alphabetical order.\n†Part of this work was done at OpenAI.\n1\narXiv:1707.03141v3  [cs.AI]  25 Feb 2018\n\n\nAccepted as a conference paper at ICLR 2018\n2\nMETA-LEARNING PRELIMINARIES\nBefore we describe SNAIL in detail, we will introduce notation and formalize the meta-learning\nproblem. As brieﬂy discussed in Section 1, the goal of meta-learning is generalization across tasks\nrather than across data points. Each task Ti is episodic and deﬁned by inputs xt, outputs at, a\nloss function Li(xt, at), a transition distribution Pi(xt|xt−1, at−1), and an episode length Hi. A\nmeta-learner (with parameters θ) models the distribution π(at|x1, . . . , xt; θ). Given a distribution\nover tasks T = P(Ti), the meta-learner’s objective is to minimize its expected loss with respect to θ.\nmin\nθ\nETi∼T\n\u0014 Hi\nX\nt=0\nLi(xt, at)\n\u0015\n,\nwhere xt ∼Pi(xt|xt−1, at−1), at ∼π(at|x1, . . . , xt; θ)\nA meta-learner is trained by optimizing this expected loss over tasks (or mini-batches of tasks)\nsampled from T . During testing, the meta-learner is evaluated on unseen tasks from a different task\ndistribution eT = P(eTi) that is similar to the training task distribution T .\n3\nA SIMPLE NEURAL ATTENTIVE LEARNER\nThe key principle motivating our approach is simplicity and versatility: a meta-learner should be\nuniversally applicable to domains in both supervised and reinforcement learning. It should be generic\nand expressive enough to learn an optimal strategy, rather than having the strategy already built-in.\nSantoro et al. (2016) considered a similar formulation of the meta-learning problem, and explored\nusing recurrent neural networks (RNNs) to implement a meta-learner. Although simple and generic,\ntheir approach is signiﬁcantly outperformed by methods that are hand-designed to exploit domain\nor algorithmic knowledge (methods which we survey in Section 4). We hypothesize that this is\nbecause traditional RNN architectures propagate information by keeping it in their hidden state from\none timestep to the next; this temporally-linear dependency bottlenecks their capacity to perform\nsophisticated computation on a stream of inputs.\nvan den Oord et al. (2016a) introduced a class of architectures that generate sequential data (in their\ncase, audio) by performing dilated 1D-convolutions over the temporal dimension. These temporal\nconvolutions (TC) are causal, so that the generated values at the next timestep are only inﬂuenced\nby past timesteps and not future ones. Compared to traditional RNNs, they offer more direct, high-\nbandwidth access to past information, allowing them to perform more sophisticated computation over\na temporal context of ﬁxed size. However, to scale to long sequences, the dilation rates generally\nincrease exponentially, so that the required number of layers scales logarithmically with the sequence\nlength. Hence, they have coarser access to inputs that are further back in time; their bounded capacity\nand positional dependence can be undesirable in a meta-learner, which should be able to fully utilize\nincreasingly large amounts of experience.\nIn contrast, soft attention (in particular, the style used by Vaswani et al. (2017a)) allows a model\nto pinpoint a speciﬁc piece of information from a potentially inﬁnitely-large context. It treats the\ncontext as an unordered key-value store which it can query based on the content of each element.\nHowever, the lack of positional dependence can also be undesirable, especially in reinforcement\nlearning, where the observations, actions, and rewards are intrinsically sequential.\nDespite their individual shortcomings, temporal convolutions and attention complement each other:\nwhile the former provide high-bandwidth access at the expense of ﬁnite context size, the latter provide\npinpoint access over an inﬁnitely large context. Hence, we construct SNAIL by combining the two:\nwe use temporal convolutions to produce the context over which we use a causal attention operation.\nBy interleaving TC layers with causal attention layers, SNAIL can have high-bandwidth access over\nits past experience without constraints on the amount of experience it can effectively use. By using\nattention at multiple stages within a model that is trained end-to-end, SNAIL can learn what pieces\nof information to pick out from the experience it gathers, as well as a feature representation that is\namenable to doing so easily. As an additional beneﬁt, SNAIL architectures are easier to train than\ntraditional RNNs such as LSTM or GRUs (where the underlying optimization can be difﬁcult because\nof the temporally-linear hidden state dependency) and can be efﬁciently implemented so that an entire\nsequence can be processed in a single forward pass. Figure 1 provides an illustration of SNAIL, and\nwe discuss architectural components in Section 3.1.\n2\n\n\nAccepted as a conference paper at ICLR 2018\nSupervised Learning\nReinforcement Learning\n(Examples, \n Labels)\nxt-1\nyt-1\nxt-2\nyt-2\nxt\n--\nxt-3\nyt-3\nPredicted Label\nt\n(Observations,\nActions,\nRewards)\not\nat-1\nrt-1\not-3\n--\n--\not-2\nat-3\nrt-3\not-1\nat-2\nrt-2\nActions\nat-3\nat\nat-2\nat-1\nFigure 1: Overview of our simple neural attentive learner (SNAIL); in this example, two blocks of\nTC layers (orange) are interleaved with two causal attention layers (green). The same class of model\narchitectures can be applied to both supervised and reinforcement learning.\nIn\nsupervised\nsettings,\nSNAIL\nreceives\nas\ninput\na\nsequence\nof\nexample-label\npairs\n(x1, y1), . . . , (xt−1, yt−1) for timesteps 1, . . . , t −1, followed by an unlabeled example (xt, −).\nIt then outputs its prediction for xt based on the previous labeled examples it has seen.\nIn reinforcement-learning settings, it receives a sequence of observation-action-reward tuples\n(o1, −, −), . . . , (ot, at−1, rt−1). At each time t, it outputs a distribution over actions at based on the\ncurrent observation ot as well as previous observations, actions, and rewards. Crucially, following\nexisting work in meta-RL (Duan et al., 2016; Wang et al., 2016), we preserve the internal state of a\nSNAIL across episode boundaries, which allows it to have memory that spans multiple episodes. The\nobservations also contain a binary input that indicates episode termination.\n3.1\nMODULAR BUILDING BLOCKS\nWe compose SNAIL architectures using a few primary building blocks. Below, we provide pseu-\ndocode for applying each block to a matrix (\"inputs\" in the pseudocode) of size (sequence length) ×\n(input dimensionality). Note that, if any of the inputs are images, we employ an additional (spatial)\nconvolutional network that converts the image into a feature vector before it is passed into the SNAIL.\nFigure 2 illustrates the different blocks visually.\nMany techniques have been proposed to increase the capacity or accelerate the training of deep convo-\nlutional architectures, including batch normalization (Ioffe & Szegedy (2015)), residual connections\n(He et al. (2016)), and dense connections (Huang et al. (2016)). We found that these techniques\ngreatly improved the expressive capacity and training speed of SNAILs, but that no particular choice\nof residual/dense conﬁgurations was essential for good performance (we explore the robustness of\nSNAILs to architectural choices in Appendix B).\nA dense block applies a single causal 1D-convolution with dilation rate R and D ﬁlters (we used\nkernel size 2 in all experiments), and then concatenates the result with its input. We used the gated\nactivation function (line 3) introduced by van den Oord et al. (2016a;b).\n1: function DENSEBLOCK(inputs, dilation rate R, number of ﬁlters D):\n2:\nxf, xg = CausalConv(inputs, R, D), CausalConv(inputs, R, D)\n3:\nactivations = tanh(xf) * sigmoid(xg)\n4:\nreturn concat(inputs, activations)\n3\n\n\nAccepted as a conference paper at ICLR 2018\nA TC block consists of a series of dense blocks whose dilation rates increase exponentially until their\nreceptive ﬁeld exceeds the desired sequence length:\n1: function TCBLOCK(inputs, sequence length T, number of ﬁlters D):\n2:\nfor i in 1, . . . , ⌈log2 T⌉do\n3:\ninputs = DenseBlock(inputs, 2i, D)\n4:\nreturn inputs\nA attention block performs a single key-value lookup; we style this operation after the self-attention\nmechanism proposed by Vaswani et al. (2017a):\n1: function ATTENTIONBLOCK(inputs, key size K, value size V ):\n2:\nkeys, query = afﬁne(inputs, K), afﬁne(inputs, K)\n3:\nlogits = matmul(query, transpose(keys))\n4:\nprobs = CausallyMaskedSoftmax(logits /\n√\nK)\n5:\nvalues = afﬁne(inputs, V )\n6:\nread = matmul(probs, values)\n7:\nreturn concat(inputs, read)\nwhere CausallyMaskedSoftmax(·) zeros out the appropriate probabilities before normalization, so\nthat a particular timestep’s query cannot have access to future keys/values.\n(a) Dense Block (dilation rate R, D lters)\nconcatenate\ninputs, shape [T, C]\noutputs, shape [T, C + D] \ncausal conv, kernel 2\ndilation R, D lters\n(b) Attention Block (key size K, value size V)\nconcatenate\ninputs, shape [T, C]\noutputs, shape [T, C + V] \na\nne, output size K \n(query)\na\nne, output size K\n(keys)\na\nne, output size V \n(values)\nmatmul, masked softmax\nmatmul\nFigure 2: Two of the building blocks that compose SNAIL architectures. (a) A dense block applies\na causal 1D-convolution, and then concatenates the output to its input. A TC block (not pictured)\napplies a series of dense blocks with exponentially-increasing dilation rates. (b) A attention block\nperforms a (causal) key-value lookup, and also concatenates the output to the input.\n4\nRELATED WORK\nPioneered by Schmidhuber (1987); Naik & Mammone (1992); Thrun & Pratt (1998), meta-learning\nis not a new idea. A key tradeoff central to many recent meta-learning approaches is between\nperformance and generality; we discuss several notable methods and how they ﬁt into this paradigm.\nGraves et al. (2014) investigated the use of recurrent neural networks (RNNs) to solve algorithmic\ntasks. They experimented with a meta-learner implemented by an LSTM, but their results suggested\nthat LSTM architectures are ill-equipped for these kinds of tasks. They then designed a more\nsophisticated RNN architecture, where an LSTM controller was coupled to an external memory bank\nfrom which it can read and write, and demonstrated that these memory-augmented neural networks\n(MANNs) achieved substantially better performance than LSTMs. Santoro et al. (2016) evaluated\nboth LSTM and MANN meta-learners on few-shot image classiﬁcation, and conﬁrm the inadequacy\n4\n\n\nAccepted as a conference paper at ICLR 2018\nof the LSTM architecture. These approaches are generic, but MANNs feature a complicated memory-\naddressing architecture that is difﬁcult to train – they still suffer from the same temporally-linear\nhidden-state dependencies as LSTMs.\nIn response, several approaches have demonstrated good performance in few-shot classiﬁcation with\nspecialized neural network architectures. Koch (2015) used a Siamese network that was trained to\npredict whether two images belong to the same class. Vinyals et al. (2016) learned an embedding\nfunction and used cosine distance in an attention kernel to judge image similarity. Snell et al. (2017)\nemployed a similar approach to Vinyals et al. (2016), based on Euclidean distance metrics. All three\nmethods work well within the context of classiﬁcation, but are not readily applicable to other domains,\nsuch as reinforcement learning. They perform well because their architectures have been designed\nto exploit domain knowledge, but ideally we would like a meta-learner that is not constrained to a\nparticular problem type.\nA number of methods consider a meta-learner that makes updates to the parameters of a traditional\nlearner (Bengio et al., 1992; Hochreiter et al., 2001). Andrychowicz et al. (2016) and Li & Malik\n(2017) investigated the setting of learning to optimize, where the learner is an objective function to\nminimize, and the meta-learner uses the gradients of the learner to perform the optimization. Their\nmeta-learner was implemented by an LSTM and the strategy that it learned can be interpreted as\na gradient-based optimization algorithm; however, it is unclear whether the learned optimizers are\nsubstantially better than existing SGD-based methods.\nRavi & Larochelle (2017) extended this idea, using a similar LSTM meta-learner in a few-shot\nclassiﬁcation setting, where the traditional learner was a convolutional-network-based classiﬁer. In\nthis setting, the meta-learning algorithm is decomposed into two parts: the traditional learner’s initial\nparameters are trained to be suitable for fast gradient-based adaptation; the LSTM meta-learner is\ntrained to be an optimization algorithm adapted for meta-learning tasks. Finn et al. (2017) explored\na special case where the meta-learner is constrained to use ordinary gradient descent to update the\nlearner and showed that this simpliﬁed model (known as MAML) can achieve equivalent performance.\nMunkhdalai & Yu (2017) explored a more sophisticated weight update scheme that yielded minor\nperformance improvements on few-shot classiﬁcation.\nAll of the methods discussed in the previous paragraph have the beneﬁt of being domain independent,\nbut they explicitly encode a particular strategy for the meta-learner to follow (namely, adaptation via\ngradient descent at test time). In a particular domain, there may exist better strategies that exploit the\nstructure of the task, but gradient-based methods will be unable to discover them. In contrast, SNAIL\npresents an alternative paradigm where a generic architecture has the capacity to learn an algorithm\nthat exploits domain-speciﬁc task structure.\nDuan et al. (2016) and Wang et al. (2016) both investigated meta-learning in reinforcement-learning\ndomains using traditional RNN architectures (GRUs and LSTMs). In addition, Finn et al. (2017)\nexperimented with fast adaptation of policies in continuous control, where the meta-learner was\ntrained on a distribution of closely-related locomotion tasks. In Section 5.2, we benchmark SNAIL\nagainst MAML and an LSTM-based meta-learner on the tasks considered by these works.\n5\nEXPERIMENTS\nOur experiments were designed to investigate the following questions:\n• How does SNAIL’s generality affect its performance on a range of meta-learning tasks?\n• How does its performance compare to existing approaches that are specialized to a particular\ntask domain, or have elements of a high-level strategy already built-in?\n• How does SNAIL scale with high-dimensional inputs and long-term temporal dependencies?\n5.1\nFEW-SHOT IMAGE CLASSIFICATION\nIn the few-shot classiﬁcation setting, we wish to classify data points into N classes when we only\nhave a small number (K) of labeled examples per class. A meta-learner is readily applicable, because\nit learns how to compare input points, rather than memorize a speciﬁc mapping from points to classes.\nThe Omniglot and mini-ImageNet datasets for few-shot image classiﬁcation are the standard bench-\nmarks in supervised meta-learning. Introduced by Lake et al. (2011), Omniglot consists of black-\n5\n\n\nAccepted as a conference paper at ICLR 2018\nand-white images of handwritten characters gathered from 50 languages, for a total of 1632 different\nclasses with 20 instances per class. Like prior works, we downsampled the images to 28 × 28\nand randomly selected 1200 classes for training and 432 for testing. We performed the same data\naugmentation proposed by Santoro et al. (2016), forming new classes by rotating each member of an\nexisting class by a multiple of 90 degrees.\nMini-ImageNet is a more difﬁcult benchmark; a subset of the well-known ImageNet dataset, it\nconsists of 84 × 84 color images from 100 different classes with 600 instances per class. We used the\nsplit released by Ravi & Larochelle (2017) and used by a number of other works, with 64 classes for\ntraining, 16 for validation, and 20 for testing.\nTo evaluate a SNAIL on the N-way, K-shot problem, we sample N classes from the overall dataset\nand K examples of each class. We then feed the corresponding NK example-label pairs to the\nSNAIL in a random order, followed by a new, unlabeled example from one of the N classes. We\nreport the average accuracy on this last, (NK + 1)-th timestep.\nWe tested SNAIL on 5-way Omniglot, 20-way Omniglot, and 5-way mini-ImageNet. For each of\nthese three splits, we trained the SNAIL on episodes where the number of shots K was chosen\nuniformly at random from 1 to 5 (note that this is unlike prior works, who train separate models\nfor each shot). For a K-shot episode within an N-way problem, the loss was simply the average\ncross-entropy between the predicted and true label on the (NK + 1)-th timestep. We train both the\nSNAIL and the feature-extracting embedding network in an end-to-end fashion using Adam (Kingma\n& Ba, 2015) For a complete description of the speciﬁcs SNAIL and embedding architectures we used,\nwe refer the reader to Appendix A.\nTable 1 displays our results on 5-way and 20-way Omniglot, and Table 2 respectively for 5-way\nmini-ImageNet. We see that SNAIL outperforms state-of-the-art methods that are extensively hand-\ndesigned, and/or domain-speciﬁc. It signiﬁcantly exceeds the performance of methods such as\nSantoro et al. (2016) that are similarly simple and generic. In Appendix B, we conduct a number of\nablations to analyse SNAIL’s performance.\nTable 1:\n5-way and 20-way, 1-shot and 5-shot classiﬁcation accuracies on Omniglot, with 95%\nconﬁdence intervals where available. For each task, the best-performing method is highlighted, along\nwith any others whose conﬁdence intervals overlap.\nMethod\n5-Way Omniglot\n20-Way Omniglot\n1-shot\n5-shot\n1-shot\n5-shot\nSantoro et al. (2016)\n82.8%\n94.9%\n–\n–\nKoch (2015)\n97.3%\n98.4%\n88.2%\n97.0%\nVinyals et al. (2016)\n98.1%\n98.9%\n93.8%\n98.5%\nFinn et al. (2017)\n98.7% ± 0.4%\n99.9% ± 0.3%\n95.8% ± 0.3%\n98.9% ± 0.2%\nSnell et al. (2017)\n97.4%\n99.3%\n96.0%\n98.9%\nMunkhdalai & Yu (2017)\n98.9%\n–\n97.0%\n–\nSNAIL, Ours\n99.07% ± 0.16%\n99.78% ± 0.09%\n97.64% ± 0.30%\n99.36% ± 0.18%\nTable 2: 5-way, 1-shot and 5-shot classiﬁcation accuracies on mini-ImageNet, with 95% conﬁdence\nintervals where available. For each task, the best-performing method is highlighted, along with any\nothers whose conﬁdence intervals overlap.\nMethod\n5-Way Mini-ImageNet\n1-shot\n5-shot\nVinyals et al. (2016)\n43.6%\n55.3%\nFinn et al. (2017)\n48.7% ± 1.84%\n63.1% ± 0.92%\nRavi & Larochelle (2017)\n43.4% ± 0.77%\n60.2% ± 0.71%\nSnell et al. (2017)\n46.61% ± 0.78%\n65.77% ± 0.70%\nMunkhdalai & Yu (2017)\n49.21% ± 0.96%\n–\nSNAIL, Ours\n55.71% ± 0.99%\n68.88% ± 0.92%\n6\n\n\nAccepted as a conference paper at ICLR 2018\n5.2\nREINFORCEMENT LEARNING\nReinforcement learning features a number of challenges that supervised learning does not, including\nlong-term temporal dependencies (as the experienced states and rewards may depend on actions taken\nmany timesteps ago) as well as balancing exploration and exploitation. To explore SNAIL‘s ability to\nlearn RL algorithms, we evaluate it on four different domains from prior work in meta-RL1:\n• Multi-armed bandits (Duan et al., 2016; Wang et al., 2016): the agent interacts with a set of\narms whose reward distributions are unknown. Although its actions do not affect its state,\nexploration and exploitation are both essential: an optimal agent must initially explore by\nsampling different arms, but later exploit its knowledge by repeatedly selecting the best arm.\n• Tabular MDPs (Duan et al., 2016; Wang et al., 2016): we procedurally generate random\nMDPs and allow the agent to act within each one for multiple episodes. Since every MDP is\ndifferent, a meta-learner cannot simply memorize the ones it is trained on; it must actually\nlearn an algorithm for solving MDPs.\n• Visual navigation (Duan et al., 2016; Wang et al., 2016): the agent must navigate randomly-\ngenerated mazes to ﬁnd a randomly-located goal, using only visual observations as input. It\nis allowed to interact with the same maze/goal conﬁguration for two episodes, so an optimal\nagent should explore the maze on the ﬁrst episode to ﬁnd the goal, and then go directly to\nthe goal on the second episode. This task features many of the common challenges in deep\nRL, including high-dimensional observations, partial observability, and sparse rewards.\n• Continuous control (Finn et al., 2017): we consider a suite of simulated locomotion tasks.\nAlthough the environment dynamics are complex, the underlying task distribution is quite\nnarrow. As a result, there is signiﬁcant task structure for a meta-learner to exploit; the\noptimal strategy is closer to task-identiﬁcation than a true RL algorithm.\nOn each of these domains, we trained a SNAIL, along with two meta-learning baselines:\n• An LSTM-based meta-learner, as concurrently proposed by Duan et al. (2016); Wang et al.\n(2016). We refer to this method as “LSTM\" in the tables and ﬁgures in subsequent sections.\n• MAML, the method introduced by Finn et al. (2017). It trains the initial parameters of a\npolicy to achieve maximal performance after one (policy) gradient update on a new task.\nWe also conducted some ablation experiments, which are detailed in Appendix D.\nIn all domains, we trained the meta-learners using trust region policy optimization with generalized\nadvantage estimation (TRPO with GAE; Schulman et al. (2015; 2016)); the SNAIL architectures and\nTRPO/GAE hyperparameters are detailed in Appendix C.\nIn the bandit and MDP domains, there exist a number of human-designed algorithms with various\noptimality guarantees (which we discuss in more depth in the subsequent sections). Although there\nisn’t much task structure for a meta-learner to exploit, the existence of upper bounds on asymptotic\nperformance let us evaluate the optimality of a meta-learned algorithm.\nHowever, the true utility of a meta-learner is that it can learn an algorithm specialized to the particular\ndistribution of tasks it is trained on. We evaluate this in the visual navigation and continuous control\ndomains, where there is signiﬁcant task structure for the meta-learner to exploit, but no optimal\nalgorithms are known to exist due to the task complexity.\n5.2.1\nMULTI-ARMED BANDITS\nIn our bandit experiments (styled after Duan et al. (2016)), each of K arms gives rewards according\nto a Bernoulli distribution whose parameter p ∈[0, 1] is chosen randomly at the start of each episode\nof length N. At each timestep, the meta-learner receives previous timestep’s reward, along with a\none-hot encoding of the corresponding arm selected. It outputs a discrete probability distribution over\nthe K arms; the selected arm is determined by sampling from this distribution.\nAs an oracle, we consider the Gittins index (Gittins, 1979), the Bayes optimal solution in the\ndiscounted, inﬁnite horizon setting. Since it is only optimal as N →∞, a meta-learner can\noutperform it for smaller N by choosing to exploit sooner.\nFollowing Duan et al. (2016), we tested all combinations of N = 10, 100, 500 and K = 5, 10, 50.\nWe also tested the additional case of N = 1000, K = 50 to further evaluate the scalability of SNAIL\nto longer sequences. We report the mean reward per episode for each setting; the results are given\nin Table 3 with 95% conﬁdence intervals where available. We found that training MAML was too\ncomputationally expensive for N = 500, 1000; hence we omit those results from Table 3.\n1Some video results can be found at https://sites.google.com/view/snail-iclr-2018/.\n7\n\n\nAccepted as a conference paper at ICLR 2018\nTable 3: Results on multi-arm bandit problems. For each, we highlighted the best performing method,\nand any others whose performance is not statistically-signiﬁcantly different (based on a one-sided\nt-test with p = 0.05). Except for SNAIL and MAML, we report the results from Duan et al. (2016).\nSetup\nMethod\n(N, K)\nGittins\n(optimal as N →∞)\nRandom\nLSTM\nMAML\nSNAIL (ours)\n10, 5\n6.6\n5.0\n6.7\n6.5 ± 0.1\n6.6 ± 0.1\n10, 10\n6.6\n5.0\n6.7\n6.6 ± 0.1\n6.7 ± 0.1\n10, 50\n6.5\n5.1\n6.8\n6.6 ± 0.1\n6.7 ± 0.1\n100, 5\n78.3\n49.9\n78.7\n67.1 ± 1.1\n79.1 ± 1.0\n100, 10\n82.8\n49.9\n83.5\n70.1 ± 0.6\n83.5 ± 0.8\n100, 50\n85.2\n49.8\n84.9\n70.3 ± 0.4\n85.1 ± 0.6\n500, 5\n405.8\n249.8\n401.5\n–\n408.1 ± 4.9\n500, 10\n437.8\n249.0\n432.5\n–\n432.4 ± 3.5\n500, 50\n463.7\n249.6\n438.9\n–\n442.6 ± 2.5\n1000, 50\n944.1\n499.8\n847.43\n–\n889.8 ± 5.6\n5.2.2\nTABULAR MDPS\nIn our tabular MDP experiments (also following Duan et al. (2016)), each MDP had 10 states and 5\nactions (both discrete); the reward for each (state, action)-pair followed a normal distribution with\nunit variance where the mean was sampled from N(1, 1), and the transitions are sampled from a ﬂat\nDirichlet distribution (the latter is a commonly used prior in Bayesian RL) with random parameters.\nWe allowed each meta-learner to interact with an MDP for N episodes of length 10. As input, they\nreceived one-hot encodings of the current state and previous action, the previous reward received,\nand a binary ﬂag indicating termination of the current episode.\nIn addition to a random agent, we consider the follow human-designed algorithms as baselines.\n• PSRL (Strens, 2000): a Bayesian method that estimate the belief over the current MDP\nparameters. At the start of each of the N episodes, it samples an MDP from the current\nposterior, and acts according to the optimal policy for the rest of the episode.\n• OPSRL (Osband & Van Roy, 2017): an optimistic variant of PSRL.\n• UCRL2 (Jaksch et al., 2010): uses an extended value iteration procedure to compute an\noptimistic MDP under the current belief.\n• ϵ-greedy: with probability 1 −ϵ, act optimally against the MAP estimate according to the\ncurrent posterior (which is updated once per episode).\nAs an oracle, we run value iteration for 10 iterations (the episode length) on each MDP. Value\niteration is optimal when the MDP parameters (reward function, transition probabilities) are known;\nthus, the resulting values provide an upper bound on the performance of any algorithm, whether\nhuman-designed or meta-learned (which do not receive the MDP parameters).\nWe tested N = 10, 25, 50, 75, 100; in Table 4, we report the performance normalized by the value-\niteration upper bound. As N increases, performance should approach 1, as the algorithm learns more\nabout the current MDP. Similarly to the bandit experiments, we could not train MAML successfully\nfor N = 50, 75, 100. In Figure 3, we show learning curves of SNAIL and LSTM.\n0\n1500\nn = 10\n0\n6000\nn = 25\n0\n6000\nn = 50\n0\n6000\nn = 75\n0\n6000\nn = 100\nLSTM\nSNAIL (ours)\nFigure 3: Learning curves of SNAIL (red) and LSTM (blue) on the random MDP task for different\nvalues of N. The horizontal axis is the TRPO iteration, and the vertical is average reward.\n8\n\n\nAccepted as a conference paper at ICLR 2018\nTable 4: Performance on tabular MDPs, scaled by the average reward achieved by value iteration.\nAs before, we highlight the best-performing method, and any others whose performance is not\nstatistically-signiﬁcantly different (using the same one-sided t-test with p = 0.05). Except for SNAIL\nand MAML, we report the values from Duan et al. (2016).\nN\nMethod\nRandom\nϵ-greedy\nPSRL\nOPSRL\nUCRL2\nLSTM\nMAML\nSNAIL (ours)\n10\n0.482\n0.640\n0.665\n0.694\n0.706\n0.752\n0.563\n0.766 ± 0.001\n25\n0.482\n0.727\n0.788\n0.819\n0.817\n0.859\n0.591\n0.862 ± 0.001\n50\n0.481\n0.793\n0.871\n0.897\n0.885\n0.902\n–\n0.908 ± 0.003\n75\n0.482\n0.831\n0.910\n0.931\n0.917\n0.918\n–\n0.930 ± 0.002\n100\n0.481\n0.857\n0.934\n0.951\n0.936\n0.922\n–\n0.941 ± 0.003\n5.2.3\nCONTINUOUS CONTROL\nWe consider the set of tasks introduced by Finn et al. (2017), in which two simulated robots (a planar\ncheetah and a 3D-quadruped ant) have to run in a particular direction or at a speciﬁed velocity (the\ndirection or velocity are chosen randomly and not told to the agent). In the goal direction experiments,\nthe reward is the magnitude of the robot’s velocity in either the forward or backward direction, and in\nthe goal velocity experiments, the reward is the negative absolute value between its current forward\nvelocity and the goal. The observations are the robot’s joint angles and velocities, and the actions\nare its joint torques. For each of these four task distributions ({ant, cheetah} × {goal velocity, goal\ndirection}), Finn et al. (2017) trained a policy to maximize its performance after one policy gradient\nupdate using 20 episodes (40 for ant), of 200 timesteps each, on a newly sampled task.\nWe trained both SNAIL and LSTM on each of these four task categories. Since they do not update\ntheir parameters at test time (instead incorporating experience through their hidden state), SNAIL\nand LSTM receive as input the previous action, previous reward, and an episode-termination ﬂag in\naddition to the current observation. We found that two episodes of interaction was sufﬁcient for these\nmeta-learners to adapt to a task, and that unrolling them for longer did not improve performance.\nIn Figure 4, we show how the different methods adapt to a new task. As an oracle, we sampled tasks\nfrom each distribution, and trained a separate policy for each task. We plot the average performance\nof the oracle policies for each task distribution as an upper bound on a meta-learner’s performance.\nQualitatively, we can think of MAML as applying a general-purpose strategy (namely, gradient\ndescent) to a distribution of highly-structured tasks. In contrast, SNAIL and LSTM are able to\nspecialize themselves based on the shared task structure, enabling them to identify the task within the\ninitial timesteps of the ﬁrst episode, and then act optimally thereafter.\n0\n20\n40\n60\n80\n100\n120\nnumber of ro  outs\n40\n60\n80\n100\n120\n140\n160\n180\naverage return\nant, goa  ve ocity\n0\n10\n20\n30\n40\n50\n60\nnumber of ro  outs\n0\n100\n200\n300\n400\n500\n600\n700\naverage return\ncheetah, forward/backward\n0\n20\n40\n60\n80\n100\n120\nnumber of ro  outs\n0\n200\n400\n600\n800\n1000\naverage return\nant, forward/backward\n0\n10\n20\n30\n40\n50\n60\nnumber of ro  outs\n−120\n−100\n−80\n−60\n−40\naverage return\ncheetah, goa  ve ocity\nLSTM\nSNAIL (ours)\nMAML\nOracle\nFigure 4: Test-time adaptation curves on simulated locomotion tasks for SNAIL, LSTM, and MAML\n(which was unrolled for three policy gradient updates). Since SNAIL incorporates experience through\nits hidden state, it can exploit common task structure to perform optimally within a few timesteps.\n5.2.4\nVISUAL NAVIGATION\nBoth Duan et al. (2016) and Wang et al. (2016) consider the task of visual navigation, where the agent\nmust ﬁnd a target in a maze using only visual inputs. The former used randomly-generated mazes\nand target positions, while the latter used a ﬁxed maze and only four different target positions. Hence,\nwe evaluated SNAIL on the former, more challenging task. The observations the agent receives are\n9\n\n\nAccepted as a conference paper at ICLR 2018\n30 × 40 ﬁrst-person images, and the actions it can take are {step forward, turn slightly left, turn\nslightly right}. We constructed a training dataset and two test datasets (unseen mazes of the same and\nlarger size, respectively), each with 1000 mazes. The agents were allowed to interact with each maze\nfor 2 episodes, with episode length 250 (1000 in the larger mazes). The starting and goal locations\nwere chosen randomly for each trial but remained ﬁxed within each pair of episodes. The agents\nreceived rewards of +1 for reaching the target (which resulted in the episode terminating), -0.01 at\neach timestep, to encourage it to reach the goal faster, and -0.001 for hitting the wall. Figure 5 depicts\nan example of the observations as well as sample maze layouts.\nWe evaluate each method using the average episode length, for both the ﬁrst and second episode\nwithin a trial. The results are displayed in Table 5. Since MAML scaled poorly to long sequences\nin the bandit and MDP domains, we did not evaluate it on this domain; the computational expense\nwas prohibitively high. Qualitatively, we observe that the optimal strategy does indeed emerge: the\nSNAIL agent explores the maze during the ﬁrst episode, and then, after ﬁnding the goal, goes directly\nthere on the second episode (the LSTM agent also exhibits this behavior, but has a harder time\nremembering where the goal is). An illustration is depicted in Figure 5.\nTable 5: Average time to ﬁnd the goal on each episode in the small and large mazes. SNAIL solves\nthe mazes the fastest, and improves the most from the ﬁrst to second episode.\nMethod\nSmall Maze\nLarge Maze\nEpisode 1\nEpisode 2\nEpisode 1\nEpisode 2\nRandom\n188.6 ± 3.5\n187.7 ± 3.5\n420.2 ± 1.2\n420.8 ± 1.2\nLSTM\n52.4 ± 1.3\n39.1 ± 0.9\n180.1 ± 6.0\n150.6 ± 5.9\nSNAIL (ours)\n50.3 ± 0.3\n34.8 ± 0.2\n140.5 ± 4.2\n105.9 ± 2.4\nFigure 5: From left to right: (a) A (higher-resolution) example of the observations the agent receives.\n(b) An example of the mazes used for training (goal shown in blue). (c) The movement of the SNAIL\non its ﬁrst episode in a larger maze, exploring the maze until it ﬁnds the goal. (d) The SNAIL’s path\nduring its second episode in the same maze as (c). Remembering the goal location, it navigates there\ndirectly on the second episode. Maps like in (b), (c), (d) are used for visualization but not available to\nthe agent. In (c), (d), the color progression from red to blue indicates the passage of time (red earlier).\n6\nCONCLUSION AND FUTURE WORK\nWe presented a simple and generic class of architectures for meta-learning, motivated by the need\nfor a meta-learner to quickly incorporate and refer to past experience. Our simple neural attentive\nlearner (SNAIL) utilizes a novel combination of temporal convolutions and causal attention, two\nbuilding blocks of sequence-to-sequence models that have complementary strengths and weaknesses.\nWe demonstrate that SNAIL achieves state-of-the-art performance by signiﬁcant margins on all of\nthe most-widely benchmarked meta-learning tasks in both supervised and reinforcement learning,\nwithout relying on any application-speciﬁc architectural components or algorithmic priors.\nAlthough we designed SNAIL with meta-learning in mind, it would likely excel at other sequence-to-\nsequence tasks, such as language modeling or translation; we plan to explore this in future work.\nAnother interesting idea would be to train an meta-learner that can attend over its entire lifetime\nof experience (rather than only a few recent episodes, as in this work). An agent with this lifelong\nmemory could learn faster and generalize better; however, to keep the computational requirements\npractical, it would also need to learn how to decide what experiences are worth remembering.\n10\n\n\nAccepted as a conference paper at ICLR 2018\nREFERENCES\nMarcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,\nand Nando de Freitas. Learning to learn by gradient descent by gradient descent. In Advances in\nNeural Information Processing Systems (NIPS), 2016.\nSamy Bengio, Yoshua Bengio, Jocelyn Cloutier, and Jan Gecsei. On the optimization of a synaptic\nlearning rule. In Optimality in Artiﬁcial and Biological Neural Networks, pp. 6–8. Univ. of Texas,\n1992.\nYan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl$ˆ2$:\nFast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779,\n2016.\nChelsea Finn, Pieter Abbeel, and Sergy Levine. Model-agnostic meta learning. International\nConference on Machine Learning (ICML), 2017.\nJ.C. Gittins. Bandit processes and dynamic allocation indices. Journal of the Royal Statistical Society.\nSeries B (Methodological), 1979.\nAlex Graves, Greg Wayne, and Ivo Danihelka.\nNeural turing machines.\narXiv preprint\narXiv:1410.5401, 2014.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\nSepp Hochreiter, A Younger, and Peter Conwell. Learning to learn using gradient descent. Artiﬁcial\nNeural Networks, ICANN, 2001.\nGao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected\nconvolutional networks. arXiv preprint arXiv:1608.06993, 2016.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. International Conference on Machine Learning (ICML), 2015.\nThomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement\nlearning. Journal of Machine Learning Research, 2010.\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International\nConference on Learning Representations (ICLR), 2015.\nGregory Koch. Siamese neural networks for one-shot image recognition. PhD thesis, University of\nToronto, 2015.\nBrenden M Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua B Tenenbaum. One shot learning\nof simple visual concepts. In CogSci, 2011.\nKe Li and Jitendra Malik. Learning to optimize. International Conference on Learning Representa-\ntions (ICLR), 2017.\nTsendsuren Munkhdalai and Hong Yu. Meta networks. International Conference on Machine\nLearning (ICML), 2017.\nDevang K Naik and RJ Mammone. Meta-neural networks that learn by learning. In Neural Networks,\n1992. IJCNN., International Joint Conference on, volume 1, pp. 437–442. IEEE, 1992.\nIan Osband and Benjamin Van Roy. Why is posterior sampling better than optimism for reinforcement\nlearning. International Conference on Machine Learning (ICML), 2017.\nSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International\nConference on Learning Representations (ICLR), 2017.\nAdam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-\nlearning with memory-augmented neural networks. In International Conference on Machine\nLearning (ICML), 2016.\n11\n\n\nAccepted as a conference paper at ICLR 2018\nJurgen Schmidhuber. Evolutionary principles in self-referential learning. On learning how to learn:\nThe meta-meta-... hook.) Diploma thesis, Institut f. Informatik, Tech. Univ. Munich, 1987.\nJohn Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel. Trust region\npolicy optimization. International Conference on Machine Learning (ICML), 2015.\nJohn Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional\ncontinuous control using generalized advantage estimation. International Conference on Learning\nRepresentations (ICLR), 2016.\nJake Snell, Kevin Swersky, and Richard S Zemel. Prototypical networks for few-shot learning. arXiv\npreprint arXiv:1703.05175, 2017.\nMalcolm Strens. A bayesian framework for reinforcement learning. In International Conference on\nMachine Learning (ICML), 2000.\nSebastian Thrun and Lorien Pratt. Learning to learn: Introduction and overview. In Learning to learn.\nSpringer, 1998.\nAaron van den Oord, Sander Dieleman, Heig Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal\nKalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw\naudio. CoRR, abs/1609.03499, 2016a.\nAaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional\nimage generation with pixelcnn decoders. In Advances in Neural Information Processing Systems\n(NIPS), 2016b.\nAshish Vaswani, Noah Shazeer, Jakob Uszkoreit, Llion Jones, Aidan Gomez N., Lukas Kaiser, and\nIllia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017a.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017b.\nOriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one shot\nlearning. In Advances in Neural Information Processing Systems (NIPS), 2016.\nJane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos,\nCharles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv\npreprint arXiv:1611.05763, 2016.\n12\n\n\nAccepted as a conference paper at ICLR 2018\nAPPENDIX\nA\nFEW-SHOT CLASSIFICATION ARCHITECTURES\nWith the building blocks deﬁned in Section 3.1, we can concisely describe SNAIL architectures. We\nused the same SNAIL architecture for both Omniglot and mini-Imagenet. For the N-way, K-shot\nproblem, the sequence length is T = NK + 1, and we used the following: AttentionBlock(64, 32),\nTCBlock(T, 128), AttentionBlock(256, 128), TCBlock(T, 128), AttentionBlock(512, 256), followed\nby a ﬁnal 1 × 1 convolution with N ﬁlters.\nFor the Omniglot dataset, we used the same embedding network architecture as all prior works, which\nrepeat the following block four times { 3x3 conv (64 channels), batch norm, ReLU, 2x2 max pool },\nand then apply a single fully-connected layer to output a 64-dimensional feature vector.\nFor mini-Imagenet, existing gradient-descent-based methods (Ravi & Larochelle, 2017; Finn et al.,\n2017), which update their model’s weights during testing, used the same network structure as the\nOmniglot network but reduced the number of channels to 32, in spite of the signiﬁcantly-increased\ncomplexity of the images. We found that this shallow embedding network did not make adequate\nuse of SNAIL’s expressive capacity, and opted to to use a deeper embedding network to prevent\nunderﬁtting (in Appendix B, we conduct ablations regarding this decision). Illustrated in Figure 6,\nour embedding was a smaller version of the ResNet (He et al., 2016) architectures commonly used\nfor the full Imagenet dataset.\n(b) mini-Imagenet embedding\n(a) Residual Block, D lters\n2x2 max-pool,\ndropout 0.9\n3x3 conv, D lters\nbatch norm\nleaky ReLU (leak 0.1)\nAdd\n1x1 conv,\nD lters\n3x3 conv, D lters\nbatch norm\nleaky ReLU (leak 0.1)\n3x3 conv, D lters\nbatch norm\nleaky ReLU (leak 0.1)\n384-dimensional \nfeature vector\n1x1 conv, 2048 lters\n6x6 mean pool, ReLU\ndropout 0.9\n1x1 conv, 384 lters\ninput image, 84x84x3\nResidual Block\n64 lters\nResidual Block\n96 lters\nResidual Block\n128 lters\nResidual Block\n256 lters\n(a) Residual Block, D lters\nFigure 6: (a) A residual block within our mini-Imagenet embedding. (b) The embedding, a smaller\nversion of ResNet (He et al., 2016), uses several of the residual blocks depicted in (a).\n13\n\n\nAccepted as a conference paper at ICLR 2018\nB\nFEW-SHOT CLASSIFICATION: ABLATIONS\nTo investigate the contribution of different components (TC, attention, and deeper embedding in the\ncase of mini-Imagenet) to SNAIL’s performance, we conducted a number of ablations, which are\nsummarized in Table 6. From these ablations, we draw two conclusions:\n• Both TC and attention layers are essential for maximal performance. When we remove\neither one, the resulting model is still competitive with other state-of-the-art methods, but\nthe combination yields the best performance. Notably, compared to the full model, using\nonly TC layers results in similar 1-shot performance but worse 5-shot. In Section 3 we\ndiscussed how temporal convolutions have coarser access to inputs farther back in time; this\nillustrates that this effect is relevant even at sequence length 26.\n• SNAIL’s improved performance is not purely a result of the deeper embedding network.\nGradient-based methods (we tested MAML; Finn et al. (2017)) overﬁt signiﬁcantly when\nthey use our embedding, and domain-speciﬁc RNN-based methods (Vinyals et al., 2016)\ndon’t utilize the extra capacity as well as SNAIL does.\nTable 6: The ablations we conducted on the few-shot classiﬁcation task. From these, we conclude\nthat (i) both TC and attention are essential for the best performance, and (ii) SNAIL‘s improved\nperformance cannot be entirely explained by the deeper embedding.\nAblation\nResult\nReplace SNAIL with stacked LSTM.\nVaried number of layers and their sizes\n(with similar number of parameters to\nSNAIL).\n5-way Omniglot: 78.1% and 90.8% (1-shot, 5-shot).\nWe were unable to successfully train this method on mini-\nImagenet.\nSNAIL with shallow mini-Imagenet em-\nbedding.\n5-way mini-Imagenet: 45.1% and 55.2% (1-shot, 5-shot).\nMAML (Finn et al., 2017), a state-of-\nthe-art gradient-based method, with our\ndeeper mini-Imagenet embedding.\nIt overﬁts tremendously; for 1-shot, 5-way mini-ImageNet:\n30.1% & 75.2% on the test and training set respectively.\nMAML trains separate models for 1-shot and 5-shot; we\ndidn’t train a 5-shot model because the 1-shot did so poorly.\nSNAIl, no TC layers (only attention).\nThis is a generalization of the method\nused by Vinyals et al. (2016), as they\nonly use a single attentive read and ex-\nplicitly force the keys to be features of\nthe image and the values to be the labels.\nWe experimented with multiple paral-\nlel reads (often referred to as multiple\nheads) as well as up to three consecutive\nattentive blocks.\nOn 5-way and 20-way Omniglot: equivalent performance to\nthe full model.\n5-way mini-Imagenet: 49.9% and 63.9% (1-shot, 5-shot).\nSNAIL, no attention (TC layers only).\nOn 5-way Omniglot: 98.8% and 99.2% (1-shot, 5-shot).\nWe were unable to train 20-way Omniglot using this method.\nOn 5-way mini-Imagenet: 55.1% and 61.2%.\nIn an attempt to analyse the learned feature representation, we tried using the features learned by\nthe Omniglot embedding in a nearest-neighbor classiﬁer, using both cosine and Euclidean distance.\nOn 5-way Omniglot, this achieves 65.1% and 67.1% (1-shot and 5-shot) for Euclidean distance and\n67.7% and 68.3% for cosine. Although SNAIL must be comparing images in order to successfully\nmake few-shot predictions, this suggests that the strategy it learns is more sophisticated than either\nof these distance metrics. We contrast this with Vinyals et al. (2016) and Snell et al. (2017), who\nexplicitly enforce such representations on the meta-learned strategy.\n14\n\n\nAccepted as a conference paper at ICLR 2018\nIn addition, we investigated how sensitive SNAILs are to architectural design choices by sampling\nrandom permutations of the different components introduced in Section 3.1. We chose each com-\nponent uniformly at random from six options: { AttentionBlock(128, 64), DenseBlock(R, 128)\nfor R ∈{1, 2, 4, 8, 16}}. We sampled architectures with 13 layers each (for consistency with our\nprimary model), and trained them on 5-way Omniglot. Averaged across 3 runs, these SNAILs\nachieved 98.62% ± 0.13% and 99.71% ± 0.08% for 1-shot and 5-shot, essentially matching the\nstate-of-the-art performance of our primary architecture.\nFinally, we explored the dependence of the classiﬁcation strategy learned by SNAIL on the dataset it\nwas trained on. If it truly learned an algorithm for few-shot classiﬁcation, then a SNAIL trained on\nimages from a particular domain should easily transfer to a new domain (such as between Omniglot\nand mini-Imagenet). To test this hypothesis:\n• First, we took a SNAIL trained on 5-way Omniglot, ﬁxed its weights, and re-learned an\nembedding for mini-Imagenet. Despite the SNAIL weights not being trained for mini-\nImagenet, this method was able to achieve 50.62% and 62.34% on 1-shot and 5-shot.\n• Then, we tried this in the reverse direction (freezing the SNAIL weights from mini-Imagenet,\nand re-learning an embedding for 5-way Omniglot), and this attained 98.66% and 99.56%.\n• Lastly, we combined an embedding trained on 5-way Omniglot with a SNAIL trained on\n5-way mini-Imagenet (with a single linear layer in between, to handle the difference in\nfeature vector dimensionality). We trained this model on 5-way Omniglot, where only the\nweights of the intermediate linear layer could be updated. It achieved 98.5% and 99.5%.\nAll of these results are very competitive with the state-of-the-art, suggesting a strong degree of\ntransferability of the algorithm and feature representation learned by SNAIL. An interesting idea\nfor future work in zero-shot learning would be to learn embeddings for multiple datasets in an\nunsupervised manner, but with some mild distributional constraints imposed on the output feature\nrepresentation. Then, one could train a SNAIL on one dataset, and have it transfer to new datasets\nwithout a single labeled example.\n15\n\n\nAccepted as a conference paper at ICLR 2018\nC\nREINFORCEMENT LEARNING\nC.1\nMULTI-ARMED BANDIT AND TABULAR MDP ARCHITECTURES\nFor the N-timestep, K-arm bandit problem, the total trajectory length is T = N. For the MDP\nproblem with N episodes per MDP, it is T = 10N (since each episode lasts for 10 timesteps).\nFor multi-arm bandits and tabular MDPs, we used the same architecture. First, we applied a fully-\nconnected layer with 32 outputs that was shared between the policy and value function. Then the\npolicy used: TCBlock(T, 32), TCBlock(T, 32), AttentionBlock(32, 32). The value function used:\nTCBlock(T, 16), TCBlock(T, 16), AttentionBlock(16, 16).\nWe found that removing the attention blocks made no difference in performance on the bandit\nproblems, whereas SNAILs without attention could not learn to solve MDPs.\nC.2\nCONTINUOUS CONTROL ARCHITECTURES\nFor each simulation locomotion task, the total trajectory length was T = 400 (2 episodes of 200\ntimesteps each). We used the same architecture (shared between policy and value function) for\nall tasks: two fully-connected layers of size 256 with tanh nonlinearities, AttentionBlock(32, 32),\nTCBlock(T, 16), TCBlock(T, 16), AttentionBlock(32, 32). Then the policy and value function\napplied separate fully-connected layers to produce the requisite output dimensionalities.\nC.3\nVISUAL NAVIGATION ARCHITECTURES\nUnlike the other RL tasks we considered, the observations in this domain include images. We\npreprocess the images using the same convolutional architecture as Duan et al. (2016): two layers\nwith {kernel size 5 × 5, 16 ﬁlters, stride 2, ReLU nonlinearity}, whose output is then ﬂattened and\nthen passed to a fully-connected layer to produce a feature vector of size 256.\nThe total trajectory length was T = 500 (2 episodes of 250 timesteps each). For the policy, we used:\nTCBlock(T, 32), AttentionBlock(16, 16), TCBlock(T, 32), AttentionBlock(16, 16). For the value\nfunction we used: TCBlock(T, 16), TCBlock(T, 16).\nC.4\nADDITIONAL REINFORCEMENT LEARNING HYPERPARAMETERS\nAs discussed in Section 5.2, we trained all policies using trust-region policy optimization with gener-\nalized advantage estimation (TRPO with GAE, Schulman et al. (2015; 2016)). The hyperparameters\nare listed in Table 7. For multi-armed bandits, tabular MDPs, and visual navigation, we used the\nsame hyperparamters as Duan et al. (2016) to make our results directly comparable; additional tuning\ncould potentially improve SNAIL’s performance.\nTable 7: The TRPO + GAE hyperparameters we used in our RL experiments.\nHyperparameter\nMulti-armed Bandits\nTabular MDPs\nContinuous Control\nVisual Navigation\nBatch Size (timesteps)\n250K\n250K\n50K\n50K\nDiscount\n0.99\n0.99\n0.99\n0.99\nGAE λ\n0.3\n0.3\n0.97\n0.99\nMean KL\n0.01\n0.01\n0.01\n0.01\n16\n\n\nAccepted as a conference paper at ICLR 2018\nD\nREINFORCEMENT LEARNING: ABLATIONS\nHere we conduct a few ablations on RL tasks: we explore whether an agent relying only on TC layers\nor only on attention layers can solve the multi-armed bandit or MDP tasks from in Section 5.2.\nFirst, we consider an SNAIL agent without attention layers (only TC layers, which amounts to a\nvariant of the WaveNet architecture introduced by van den Oord et al. (2016a)).\nWhen applied to the bandit domain, we found that this TC-only model performed just as well as a\ncomplete SNAIL. This is likely due to the simplicity of this task domain, as successful performance\non bandit problems does not require maintaining a large memory of past experience. Indeed, many\nhuman designed algorithms (including the asymptotically optimal Gittins index) simply update\nrunning statistics at each timestep.\nHowever, this model struggled in the MDP domain, where a more sophisticated algorithm is required.\nThe results are in the table below (with those of a random agent, SNAIL, LSTM and MAML\nduplicated from Table 4 for reference). This agent’s asymptotic suboptimality suggests that its ability\nto internalize past experience is being saturated.\nTable 8: Ablations of SNAIL in the MDP domain.\nN\nMethod\nRandom\nLSTM\nMAML\nSNAIL\nSNAIL, TC-only\n10\n0.482\n0.752\n0.563\n0.766 ± 0.001\n0.616 ±0.001\n25\n0.482\n0.859\n0.591\n0.862 ± 0.001\n0.684 ±0.001\n50\n0.481\n0.902\n–\n0.908 ± 0.003\n0.699 ±0.002\n75\n0.482\n0.918\n–\n0.930 ± 0.002\n0.726 ±0.002\n100\n0.481\n0.922\n–\n0.941 ± 0.003\n0.728 ±0.003\nNext, we considered a SNAIL agent without TC layers (only attention). Due to the sequential nature\nof RL tasks, we employed the positional encoding proposed by Vaswani et al. (2017b). This model,\nwhich is equivalent to their Transformer architecture, could not solve the bandit or MDP tasks. In both\ndomains, its performance was no better than random. To no avail, we experimented with multiple\nblocks of attention and multiple heads per block.\nWe hypothesize that this architecture’s inadequacy stems from the fact that pure attentive lookups\ncannot easily process sequential information. Despite their inﬁnite receptive ﬁeld, they cannot directly\ncompare two adjacent timesteps (such as a single state-action-state transition) in the same way as a\nsingle convolution can. The TC layers are essential because they allow the agent to locally analyse\ncontiguous parts of a sequence to produce a better contextual representation over which to attend.\n17\n"
}