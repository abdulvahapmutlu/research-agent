{
  "filename": "2006.15486v3.pdf",
  "num_pages": 11,
  "pages": [
    "Laplacian Regularized Few-Shot Learning\nImtiaz Masud Ziko 1 Jose Dolz 1 Eric Granger 1 Ismail Ben Ayed 1\nAbstract\nWe propose a transductive Laplacian-regularized\ninference for few-shot tasks. Given any feature\nembedding learned from the base classes, we\nminimize a quadratic binary-assignment function\ncontaining two terms: (1) a unary term assign-\ning query samples to the nearest class prototype,\nand (2) a pairwise Laplacian term encouraging\nnearby query samples to have consistent label as-\nsignments. Our transductive inference does not\nre-train the base model, and can be viewed as a\ngraph clustering of the query set, subject to super-\nvision constraints from the support set. We derive\na computationally efﬁcient bound optimizer of a\nrelaxation of our function, which computes inde-\npendent (parallel) updates for each query sample,\nwhile guaranteeing convergence. Following a sim-\nple cross-entropy training on the base classes, and\nwithout complex meta-learning strategies, we con-\nducted comprehensive experiments over ﬁve few-\nshot learning benchmarks. Our LaplacianShot\nconsistently outperforms state-of-the-art methods\nby signiﬁcant margins across different models,\nsettings, and data sets. Furthermore, our trans-\nductive inference is very fast, with computational\ntimes that are close to inductive inference, and\ncan be used for large-scale few-shot tasks.\n1. Introduction\nDeep learning models have achieved human-level perfor-\nmances in various tasks. The success of these models rely\nconsiderably on exhaustive learning from large-scale labeled\ndata sets. Nevertheless, they still have difﬁculty general-\nizing to novel classes unseen during training, given only\na few labeled instances for these new classes. In contrast,\nhumans can learn new tasks easily from a handful of ex-\namples, by leveraging prior experience and related context.\n1 ´ETS Montreal, Canada. Correspondence to: Imtiaz Masud\nZiko <imtiaz-masud.ziko.1@etsmtl.ca>.\nProceedings of the 37 th International Conference on Machine\nLearning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by\nthe author(s).\nFew-shot learning (Fei-Fei et al., 2006; Miller et al., 2000;\nVinyals et al., 2016) has emerged as an appealing paradigm\nto bridge this gap. Under standard few-shot learning scenar-\nios, a model is ﬁrst trained on substantial labeled data over\nan initial set of classes, often referred to as the base classes.\nThen, supervision for novel classes, which are unseen during\nbase training, is limited to just one or few labeled exam-\nples per class. The model is evaluated over few-shot tasks,\neach one supervised by a few labeled examples per novel\nclass (the support set) and containing unlabeled samples for\nevaluation (the query set).\nThe problem has recently received substantial research in-\nterests, with a large body of work based on complex meta-\nlearning and episodic-training strategies. The meta-learning\nsetting uses the base training data to create a set of few-shot\ntasks (or episodes), with support and query samples that\nsimulate generalization difﬁculties during test times, and\ntrain the model to generalize well on these artiﬁcial tasks.\nFor example, (Vinyals et al., 2016) introduced matching\nnetwork, which employs an attention mechanism to pre-\ndict the unknown query samples as a linear combination\nof the support labels, while using episodic training and\nmemory architectures. Prototypical networks (Snell et al.,\n2017) maintain a single prototype representation for each\nclass in the embedding space, and minimize the negative\nlog-probability of the query features with episodic training.\nRavi & Larochelle (2017) viewed optimization as a model\nfor few-shot learning, and used an LSTM meta-learner to\nupdate classiﬁer parameters. Finn et al. (2017) proposed\nMAML, a meta-learning strategy that attempts to make a\nmodel “easy” to ﬁne-tune. These widely adopted works\nwere recently followed by an abundant meta-learning litera-\nture, for instance, (Sung et al., 2018; Oreshkin et al., 2018;\nMishra et al., 2018; Rusu et al., 2019; Liu et al., 2019b; Hou\net al., 2019; Ye et al., 2020), among many others.\nSeveral recent studies explored transductive inference for\nfew-shot tasks, e.g., (Liu et al., 2019b; Hou et al., 2019;\nDhillon et al., 2020; Hu et al., 2020; Kim et al., 2019;\nQiao et al., 2019), among others. Given a few-shot task\nat test time, transductive inference performs class predic-\ntions jointly for all the unlabeled query samples of the task,\nrather than one sample at a time as in inductive inference.\nFor instance, TPN (Liu et al., 2019b) used label propaga-\ntion (Zhou et al., 2004) along with episodic training and a\narXiv:2006.15486v3  [cs.LG]  28 Apr 2021\n",
    "Laplacian Regularized Few-Shot Learning\nspeciﬁc network architecture, so as to learn how to prop-\nagate labels from labeled to unlabeled samples. CAN-T\n(Hou et al., 2019) is another meta-learning based transduc-\ntive method, which uses attention mechanisms to propagate\nlabels to unlabeled query samples. The transductive ﬁne-\ntuning method by (Dhillon et al., 2020) re-train the network\nby minimizing an additional entropy loss, which encour-\nages peaked (conﬁdent) class predictions at unlabeled query\npoints, in conjunction with a standard cross-entropy loss\ndeﬁned on the labeled support set.\nTransductive few-shot methods typically perform better than\ntheir inductive counterparts. However, this may come at the\nprice of a much heavier computational complexity during\ninference. For example, the entropy ﬁne-tuning in (Dhillon\net al., 2020) re-trains the network, performing gradient up-\ndates over all the parameters during inference. Also, the\nlabel propagation in (Liu et al., 2019b) requires a matrix\ninversion, which has a computational overhead that is cubic\nwith respect to the number of query samples. This may\nbe an impediment for deployment for large-scale few-shot\ntasks.\nWe propose a transductive Laplacian-regularized inference\nfor few-shot tasks. Given any feature embedding learned\nfrom the base data, our method minimizes a quadratic\nbinary-assignment function integrating two types of poten-\ntials: (1) unary potentials assigning query samples to the\nnearest class prototype, and (2) pairwise potentials favor-\ning consistent label assignments for nearby query samples.\nOur transductive inference can be viewed as a graph clus-\ntering of the query set, subject to supervision constraints\nfrom the support set, and does not re-train the base model.\nFollowing a relaxation of our function, we derive a compu-\ntationally efﬁcient bound optimizer, which computes inde-\npendent (parallel) label-assignment updates for each query\npoint, with guaranteed convergence. We conducted compre-\nhensive experiments on ﬁve few-shot learning benchmarks,\nwith different levels of difﬁculties. Using a simple cross-\nentropy training on the base classes, and without complex\nmeta-learning strategies, our LaplacianShot outperforms\nstate-of-the-art methods by signiﬁcant margins, consistently\nproviding improvements across different settings, data sets,\nand training models. Furthermore, our transductive infer-\nence is very fast, with computational times that are close to\ninductive inference, and can be used for large-scale tasks.\n2. Laplacian Regularized Few-Shot Learning\n2.1. Proposed Formulation\nIn the few-shot setting, we are given a labeled support set\nXs = SC\nc=1 Xc\ns with C test classes, where each novel class c\nhas |Xc\ns| labeled examples, for instance, |Xc\ns| = 1 for 1-shot\nand |Xc\ns| = 5 for 5-shot. The objective of few-shot learn-\ning is, therefore, to accurately classify unlabeled unseen\nquery sample set Xq = SC\nc=1 Xc\nq from these C test classes.\nThis setting is referred to as the |Xc\ns|-shot C-way few-shot\nlearning.\nLet fθ denotes the embedding function of a deep convolu-\ntional neural network, with parameters θ and xq = fθ(zq) ∈\nRM encoding the features of a given data point zq. Embed-\nding fθ is learned from a labeled training set Xbase, with\nbase classes that are different from the few-shot classes of\nXs and Xq. In our work, parameters θ are learned through\na basic network training with the standard cross-entropy\nloss deﬁned over Xbase, without resorting to any complex\nepisodic-training or meta-learning strategy. For each query\nfeature point xq in a few-shot task, we deﬁne a latent bi-\nnary assignment vector yq = [yq,1, . . . , yq,C]t ∈{0, 1}C,\nwhich is within the C-dimensional probability simplex\n∇C = {y ∈[0, 1]C | 1ty = 1}: binary yq,c is equal\nto 1 if xq belongs to class c, and equal to 0 otherwise. t is\nused as the transpose operator. Let Y denotes the N × C\nmatrix whose rows are formed by yq, where N is the num-\nber of query points in Xq. We propose a transductive few-\nshot inference, which minimizes a Laplacian-regularization\nobjective for few-shot tasks w.r.t assignment variables Y,\nsubject to simplex and integer constraints yq ∈∇C and\nyq ∈{0, 1}C, ∀q:\nE(Y)\n=\nN(Y) + λ\n2 L(Y)\n(1)\nN(Y)\n=\nN\nX\nq=1\nC\nX\nc=1\nyq,cd(xq −mc)\nL(Y)\n=\n1\n2\nX\nq,p\nw(xq, xp)∥yq −yp∥2\nIn (1), the ﬁrst term N(Y) is minimized globally when\neach query point is assigned to the class of the nearest\nprototype mc from the support set, using a distance metric\nd(xq, mc), such as the Euclidean distance. In the 1-shot\nsetting, prototype mc is the support example of class c,\nwhereas in multi-shot, mc can be the mean of the support\nexamples. In fact, mc can be further rectiﬁed by integrating\ninformation from the query features, as we will detail later\nin our experiments.\nThe second term L(Y) is the well-known Laplacian regular-\nizer, which can be equivalently written as tr(YtLY), where\nL is the Laplacian matrix1 corresponding to afﬁnity matrix\nW = [w(xq, xp)], and tr denotes the trace operator. Pair-\nwise potential w(xq, xp) evaluates the similarity between\nfeature vectors xq and xp, and can be computed using some\nkernel function. The Laplacian term encourages nearby\n1The Laplacian matrix corresponding to afﬁnity matrix W =\n[w(xq, xp)] is L = D −W, with D the diagonal matrix whose\ndiagonal elements are given by: Dq = P\np w(xq, xp).\n",
    "Laplacian Regularized Few-Shot Learning\nAlgorithm 1 Proposed Algorithm for LaplacianShot\nInput: Xs, Xq, λ, fθ\nOutput: Labels ∈{1, .., C}N for Xq\nGet prototypes mc.\nCompute aq using (8a) ∀xq ∈Xq.\nInitialize i = 1.\nInitialize yi\nq =\nexp(−aq)\n1t exp(−aq).\nrepeat\nCompute yi+1\nq\nusing (12)\nyi\nq ←yi+1\nq\n.\nY = [yi\nq]; ∀q.\ni = i + 1.\nuntil Bi(Y) in (7) does not change\nlq = arg max\nc\nyq; ∀yq ∈Y.\nLabels = {lq}N\nq=1\npoints (xq, xp) in the feature space to have the same latent\nlabel assignment, thereby regularizing predictions at query\nsamples for few-shot tasks. As we will show later in our\ncomprehensive experiments, the pairwise Laplacian term\ncomplements the unary potentials in N(Y), substantially\nincreasing the predictive performance of few-shot learning\nacross different networks, and various benchmark datasets\nwith different levels of difﬁculty.\nMore generally, Laplacian regularization is widely used\nin the contexts of graph clustering (Von Luxburg, 2007;\nShi & Malik, 2000; Ziko et al., 2018; Wang & Carreira-\nPerpin´an, 2014) and semi-supervised learning (Weston et al.,\n2012; Belkin et al., 2006). For instance, popular spectral\ngraph clustering techniques (Von Luxburg, 2007; Shi & Ma-\nlik, 2000) optimize the Laplacian term subject to partition-\nbalance constraints. In this connection, our transductive\ninference can be viewed as a graph clustering of the query\nset, subject to supervision constraints from the support set.\nRegularization parameter λ controls the trade-off between\nthe two terms. It is worth noting that the recent nearest-\nprototype classiﬁcation in (Wang et al., 2019) corresponds\nto the particular case of λ = 0 of our model in (1). It\nassigns a query sample xq to the label of the closest support\nprototype in the feature space, thereby minimizing N(Y):\nyq,c∗= 1\nif\nc∗= arg min\nc∈{1,...,C}\nd(xq, mc)\n(2)\n2.2. Optimization\nIn this section, we propose an efﬁcient bound-optimization\ntechnique for solving a relaxed version of our objective\nin (1), which guarantees convergence, while computing in-\ndependent closed-form updates for each query sample in\nfew-shot tasks. It is well known that minimizing pairwise\nfunctions over binary variables is NP-hard (Tian et al., 2014),\nand a standard approach in the context of clustering algo-\nrithms is to relax the integer constraints, for instance, using\na convex (Wang & Carreira-Perpin´an, 2014) or a concave\nrelaxation (Ziko et al., 2018). In fact, by relaxing integer\nconstraints yq ∈{0, 1}C, our objective in (1) becomes a\nconvex quadratic problem. However, this would require\nsolving for the N × C assignment variables all together,\nwith additional projections steps for handling the simplex\nconstraints. In this work, we use a concave relaxation of\nthe Laplacian-regularized objective in (1), which, as we will\nlater show, yields fast independent and closed-form updates\nfor each assignment variable, with convergence guarantee.\nFurthermore, it enables us to draw interesting connections\nbetween Laplacian regularization and attention mechanisms\nin few-shot learning (Vinyals et al., 2016).\nIt is easy to verify that, for binary (integer) simplex variables,\nthe Laplacian term in (1) can be written as follows, after\nsome simple manipulations:\nL(Y) =\nX\nq\nDq −\nX\nq,p\nw(xq, xp)yt\nqyp\n(3)\nwhere Dq = P\np w(xq, xp) denotes the degree of query\nsample xq. By relaxing integer constraints yq ∈{0, 1}C,\nthe expression in Eq. (3) can be viewed as a concave relax-\nation2 for Laplacian term L(Y) when symmetric afﬁnity\nmatrix W = [w(xq, xp)] is positive semi-deﬁnite. As we\nwill see in the next paragraph, concavity is important to\nderive an efﬁcient bound optimizer for our model, with\nindependent and closed-form updates for each query sam-\nple. Notice that the ﬁrst term in relaxation (3) is a constant\nindependent of the soft (relaxed) assignment variables.\nWe further augment relaxation (3) with a convex negative-\nentropy barrier function yt\nq log yq, which avoids expensive\nprojection steps and Lagrangian-dual inner iterations for the\nsimplex constraints of each query point. Such a barrier3\nremoves the need for extra dual variables for constraints\nyq ≥0 by restricting the domain of each assignment vari-\nable to non-negative values, and yields closed-form updates\nfor the dual variables of constraints 1tyq = 1. Notice\nthat this barrier function is null at the vertices of the sim-\nplex. Putting all together, and omitting the additive constant\nP\nq Dq in (3), we minimize the following concave-convex\nrelaxation of our objective in (1) w.r.t soft assignment vari-\nables Y, subject to simplex constraints yq ∈∇C, ∀q:\nR(Y) = Yt log Y + N(Y) + λ\n2\n˜L(Y)\n(4)\n2Equality (3) holds in for points on the vertices of the simplex,\ni.e., yq ∈{0, 1}C, but is an approximation for points within the\nsimplex (soft assignments), i.e., yq ∈]0, 1[C.\n3Note that entropy-like barriers are known in the context of\nBregman-proximal optimization (Yuan et al., 2017), and have\nwell-known computational beneﬁts when dealing with simplex\nconstraints.\n",
    "Laplacian Regularized Few-Shot Learning\nwhere ˜L(Y) = −P\nq,p w(xq, xp)yt\nqyp.\nBound optimization:\nIn the following, we detail an\niterative bound-optimization solution for relaxation (4).\nBound optimization, often referred to as MM (Majorize-\nMinimization) framework (Lange et al., 2000; Zhang et al.,\n2007), is a general optimization principle4. At each iteration,\nit updates the variable as the minimum of a surrogate func-\ntion, i.e., an upper bound on the original objective, which\nis tight at the current iteration. This guarantees that the\noriginal objective does not increase at each iteration.\nRe-arranging the soft assignment matrix Y in vector form\nY = [yq] ∈RNC, relaxation ˜L(Y) can be written conve-\nniently in the following form:\n˜L(Y) = −\nX\nq,p\nw(xq, xp)yt\nqyp = YtΨY\n(5)\nwith Ψ = −W⊗I, where ⊗denotes the Kronecker product\nand I is the N × N identity matrix. Note that Ψ is negative\nsemi-deﬁnite for a positive semi-deﬁnite W. Therefore,\nYtΨY is a concave function, and the ﬁrst-order approxima-\ntion of (5) at a current solution Yi (i is the iteration index)\ngives the following tight upper bound on ˜L(Y):\n˜L(Y) = YtΨY ≤(Yi)tΨYi + 2 (ΨYi)t(Y −Yi) (6)\nTherefore, using unary potentials N(Y) and the negative\nentropy barrier in conjunction with the upper bound in (6),\nwe obtain the following surrogate function Bi(Y) for relax-\nation R(Y) at current solution Yi:\nR(Y) ≤Bi(Y) c=\nN\nX\nq=1\nyt\nq(log(yq) + aq −λbi\nq)\n(7)\nwhere c= means equality up to an additive constant5 that is\nindependent of variable Y, and aq and bi\nq are the following\nC-dimensional vectors:\naq = [aq,1, . . . , aq,C]t; aq,c = d(xq, mc)\n(8a)\nbi\nq = [bi\nq,1, . . . , bi\nq,C]t; bi\nq,c =\nX\np\nw(xq, xp)yi\np,c\n(8b)\nIt is straightforward to verify that upper bound Bi(Y) is\ntight at the current iteration, i.e., Bi(Yi) = R(Yi). This\n4The general MM principle is widely used in machine learn-\ning in various problems as it enables to replace a difﬁcult opti-\nmization problem with a sequence of easier sub-problems (Zhang\net al., 2007). Examples of well-known bound optimizers include\nexpectation-maximization (EM) algorithms, the concave-convex\nprocedure (CCCP) (Yuille & Rangarajan, 2001) and submodular-\nsupermodular procedures (SSP) (Narasimhan & Bilmes, 2005),\namong many others.\n5The additive constant in Bi(Y) is a term that depends only on\nYi. This term comes from the Laplacian upper bound in (6).\ncan be seen easily from the ﬁrst-order approximation in\n(6). We iteratively optimize the surrogate function at each\niteration i:\nYi+1 = arg min\nY\nBi(Y)\n(9)\nBecause of upper-bound condition R(Y) ≤Bi(Y), ∀Y,\ntightness condition Bi(Yi) = R(Yi) at the current solu-\ntion, and the fact that Bi(Yi+1) ≤Bi(Yi) due to minimiza-\ntion (9), it is easy to verify that updates (9) guarantee that\nrelaxation R(Y) does not increase at each iteration:\nR(Yi+1) ≤Bi(Yi+1) ≤Bi(Yi) = R(Yi)\nClosed-form solutions of the surrogate functions: No-\ntice that Bi(Y) is a sum of independent functions of each\nassignment variable. Therefore, we can solve (9) for each\nyq independently, while satisfying the simplex constraint:\nmin\nyq∈∇C yt\nq(log(yq) + aq −λbi\nq), ∀q\n(10)\nThe negative entropy barrier term yt\nq log yq in (10) restricts\nyq to be non-negative, removing the need of extra dual vari-\nables for the constraints yq > 0. Also, simplex constraint\n1tyq = 1 is afﬁne. Thus, the solution of the following\nKarush-Kuhn-Tucker (KKT) condition provide the mini-\nmum of (10):\nlog yq + aq −λbi\nq + β1 = 0\n(11)\nwith β the Lagrange multiplier for the simplex constraint.\nThis provides, for each q, closed-form solutions for both the\nprimal and dual variables, yielding the following indepen-\ndent updates of the assignment variables:\nyi+1\nq\n=\nexp(−ai\nq + λbi\nq)\n1t exp(−aiq + λbiq) ∀q\n(12)\n2.3. Proposed Algorithm\nThe overall proposed algorithm is simpliﬁed in Algorithm\n1. Once the network function fθ is learned using the base\ndataset Xbase, our algorithm proceeds with the extracted\nfeatures xq. Before the iterative bound updates, each soft\nassignment y1\nq is initialized as a softmax probability of aq,\nwhich is based on the distances to prototypes mc. The itera-\ntive bound optimization is guaranteed to converge, typically\nless than 15 iterations in our experiments (Figure 2). Also\nthe independent point-wise bound updates yield a parallel\nstructure of the algorithm, which makes it very efﬁcient (and\nconvenient for large-scale few-shot tasks). We refer to our\nmethod as LaplacianShot in the experiments.\nLink to attention mechanisms: Our Laplacian-regularized\nmodel has interesting connection to the popular attention\nmechanism in (Vaswani et al., 2017). In fact, MatchingNet\n",
    "Laplacian Regularized Few-Shot Learning\nTable 1. Average accuracy (in %) in miniImageNet and tieredImageNet. The best results are reported in bold font.\nminiImageNet\ntieredImageNet\nMethods\nNetwork\n1-shot\n5-shot\n1-shot\n5-shot\nMAML (Finn et al., 2017)\nResNet-18\n49.61 ± 0.92\n65.72 ± 0.77\n-\n-\nChen (Chen et al., 2019)\nResNet-18\n51.87 ± 0.77\n75.68 ± 0.63\n-\n-\nRelationNet (Sung et al., 2018)\nResNet-18\n52.48 ± 0.86\n69.83 ± 0.68\n-\n-\nMatchingNet (Vinyals et al., 2016)\nResNet-18\n52.91 ± 0.88\n68.88 ± 0.69\n-\n-\nProtoNet (Snell et al., 2017)\nResNet-18\n54.16 ± 0.82\n73.68 ± 0.65\n-\n-\nGidaris (Gidaris & Komodakis, 2018)\nResNet-15\n55.45 ± 0.89\n70.13 ± 0.68\n-\n-\nSNAIL (Mishra et al., 2018)\nResNet-15\n55.71 ± 0.99\n68.88 ± 0.92\n-\n-\nAdaCNN (Munkhdalai et al., 2018)\nResNet-15\n56.88 ± 0.62\n71.94 ± 0.57\n-\n-\nTADAM (Oreshkin et al., 2018)\nResNet-15\n58.50 ± 0.30\n76.70 ± 0.30\n-\n-\nCAML (Jiang et al., 2019)\nResNet-12\n59.23 ± 0.99\n72.35 ± 0.71\n-\n-\nTPN (Liu et al., 2019b)\nResNet-12\n59.46\n75.64\n-\n-\nTEAM (Qiao et al., 2019)\nResNet-18\n60.07\n75.90\n-\n-\nMTL (Sun et al., 2019)\nResNet-18\n61.20 ± 1.80\n75.50 ± 0.80\n-\n-\nVariationalFSL (Zhang et al., 2019)\nResNet-18\n61.23 ± 0.26\n77.69 ± 0.17\n-\n-\nTransductive tuning (Dhillon et al., 2020)\nResNet-12\n62.35 ± 0.66\n74.53 ± 0.54\n-\n-\nMetaoptNet (Lee et al., 2019)\nResNet-18\n62.64 ± 0.61\n78.63 ± 0.46\n65.99 ± 0.72\n81.56 ± 0.53\nSimpleShot (Wang et al., 2019)\nResNet-18\n63.10 ± 0.20\n79.92 ± 0.14\n69.68 ± 0.22\n84.56 ± 0.16\nCAN+T (Hou et al., 2019)\nResNet-12\n67.19 ± 0.55\n80.64 ± 0.35\n73.21 ± 0.58\n84.93 ± 0.38\nLaplacianShot (ours)\nResNet-18\n72.11 ± 0.19\n82.31 ± 0.14\n78.98 ± 0.21\n86.39 ± 0.16\nQiao (Qiao et al., 2018)\nWRN\n59.60 ± 0.41\n73.74 ± 0.19\n-\n-\nLEO (Rusu et al., 2019)\nWRN\n61.76 ± 0.08\n77.59 ± 0.12\n66.33 ± 0.05\n81.44 ± 0.09\nProtoNet (Snell et al., 2017)\nWRN\n62.60 ± 0.20\n79.97 ± 0.14\n-\n-\nCC+rot (Gidaris et al., 2019)\nWRN\n62.93 ± 0.45\n79.87 ± 0.33\n70.53 ± 0.51\n84.98 ± 0.36\nMatchingNet (Vinyals et al., 2016)\nWRN\n64.03 ± 0.20\n76.32 ± 0.16\n-\n-\nFEAT (Ye et al., 2020)\nWRN\n65.10 ± 0.20\n81.11 ± 0.14\n70.41 ± 0.23\n84.38 ± 0.16\nTransductive tuning (Dhillon et al., 2020)\nWRN\n65.73 ± 0.68\n78.40 ± 0.52\n73.34 ± 0.71\n85.50 ± 0.50\nSimpleShot (Wang et al., 2019)\nWRN\n65.87± 0.20\n82.09 ± 0.14\n70.90 ± 0.22\n85.76 ± 0.15\nSIB (Hu et al., 2020)\nWRN\n70.0 ± 0.6\n79.2 ± 0.4\n-\n-\nBD-CSPN (Liu et al., 2019a)\nWRN\n70.31 ± 0.93\n81.89 ± 0.60\n78.74 ± 0.95\n86.92 ± 0.63\nLaplacianShot (ours)\nWRN\n74.86 ± 0.19\n84.13 ± 0.14\n80.18 ± 0.21\n87.56± 0.15\nSimpleShot (Wang et al., 2019)\nMobileNet\n61.55 ± 0.20\n77.70 ± 0.15\n69.50 ± 0.22\n84.91 ± 0.15\nLaplacianShot (ours)\nMobileNet\n70.27 ± 0.19\n80.10 ± 0.15\n79.13 ± 0.21\n86.75 ± 0.15\nSimpleShot (Wang et al., 2019)\nDenseNet\n65.77 ± 0.19\n82.23 ± 0.13\n71.20 ± 0.22\n86.33 ± 0.15\nLaplacianShot (ours)\nDenseNet\n75.57 ± 0.19\n84.72 ± 0.13\n80.30 ± 0.22\n87.93 ± 0.15\n(Vinyals et al., 2016) predicted the labels of the query sam-\nples xq as a linear combination of the support labels. The ex-\npression of bi\nq,c that we obtained in (8b), which stems from\nour bound optimizer and the concave relaxation of the Lapla-\ncian, also takes the form of a combination of labels at each it-\neration i in our model: bi\nq,c = P\np w(xq, xp)yi\np,c. However,\nthere are important differences with (Vinyals et al., 2016):\nFirst, the attention in our formulation is non-parametric as\nit considers only the feature relationships among the query\nsamples in Xq, not the support examples. Second, unlike\nour approach, the attention mechanism in (Vinyals et al.,\n2016) is employed during training for learning embedding\nfunction fθ with a meta-learning approach.\n3. Experiments\nIn this section, we describe our experimental setup. An\nimplementation of our LaplacianShot is publicly available6.\n6https://github.com/imtiazziko/LaplacianShot\n3.1. Datasets\nWe used ﬁve benchmarks for few-shot classiﬁcation:\nminiImageNet, tieredImageNet, CUB, cross-domain CUB\n(with base training on miniImageNet) and iNat.\nThe miniImageNet benchmark is a subset of the larger\nILSVRC-12 dataset (Russakovsky et al., 2015). It has a\ntotal of 60,000 color images with 100 classes, where each\nclass has 600 images of size 84 × 84, following (Vinyals\net al., 2016). We use the standard split of 64 base, 16 vali-\ndation and 20 test classes (Ravi & Larochelle, 2017; Wang\net al., 2019). The tieredImageNet benchmark (Ren et al.,\n2018) is also a subset of ILSVRC-12 dataset but with 608\nclasses instead. We follow standard splits with 351 base,\n97 validation and 160 test classes for the experiments. The\nimages are also resized to 84 × 84 pixels. CUB-200-2011\n(Wah et al., 2011) is a ﬁne-grained image classiﬁcation\ndataset. We follow (Chen et al., 2019) for few-shot classi-\nﬁcation on CUB, which splits into 100 base, 50 validation\n",
    "Laplacian Regularized Few-Shot Learning\nTable 2. Results for CUB and cross-domain results on miniImagenet →CUB.\nMethods\nNetwork\nCUB\nminiImagenet →CUB\n1-shot\n5-shot\n1-shot\n5-shot\nMatchingNet (Vinyals et al., 2016)\nResNet-18\n73.49\n84.45\n-\n53.07\nMAML (Finn et al., 2017)\nResNet-18\n68.42\n83.47\n-\n51.34\nProtoNet (Snell et al., 2017)\nResNet-18\n72.99\n86.64\n-\n62.02\nRelationNet (Sung et al., 2018)\nResNet-18\n68.58\n84.05\n-\n57.71\nChen (Chen et al., 2019)\nResNet-18\n67.02\n83.58\n-\n65.57\nSimpleShot (Wang et al., 2019)\nResNet-18\n70.28\n86.37\n48.56\n65.63\nLaplacianShot(ours)\nResNet-18\n80.96\n88.68\n55.46\n66.33\nTable 3. Average accuracy (in %) in iNat benchmark for SimpleShot (Wang et al., 2019) and the proposed LaplacianShot. The best results\nare reported in bold font. Note that, for iNat, we do not utilize the rectiﬁed prototypes. [The best reported result of (Wertheimer &\nHariharan, 2019) with ResNet50 is: Per Class: 46.04%, Mean: 51.25%.]\nMethods\nNetwork\nUN\nL2\nCL2\nPer Class\nMean\nPer Class\nMean\nPer Class\nMean\nSimpleShot\nResNet-18\n55.80\n58.56\n57.15\n59.56\n56.35\n58.63\nLaplacianShot\nResNet-18\n62.80\n66.40\n58.72\n61.14\n58.49\n60.81\nSimpleShot\nResNet-50\n58.45\n61.07\n59.68\n61.99\n58.83\n60.98\nLaplacianShot\nResNet-50\n65.96\n69.13\n61.40\n63.66\n61.08\n63.18\nSimpleShot\nWRN\n62.44\n65.08\n64.26\n66.25\n63.03\n65.17\nLaplacianShot\nWRN\n71.55\n74.97\n65.78\n67.82\n65.32\n67.43\nand 50 test classes for the experiments. The images are\nalso resized to 84 × 84 pixels, as in miniImageNet. The\niNat benchmark, introduced recently for few-shot classiﬁca-\ntion in (Wertheimer & Hariharan, 2019), contains images of\n1,135 animal species. It introduces a more challenging few-\nshot scenario, with different numbers of support examples\nper class, which simulates more realistic class-imbalance\nscenarios, and with semantically related classes that are\nnot easily separable. Following (Wertheimer & Hariharan,\n2019), the dataset is split into 908 base classes and 227 test\nclasses, with images of size 84 × 84.\n3.2. Evaluation Protocol\nIn the case of miniImageNet, CUB and tieredImageNet,\nwe evaluate 10,000 ﬁve-way 1-shot and ﬁve-way 5-shot\nclassiﬁcation tasks, randomly sampled from the test classes,\nfollowing standard few-shot evaluation settings (Wang et al.,\n2019; Rusu et al., 2019). This means that, for each of\nthe ﬁve-way few-shot tasks, C = 5 classes are randomly\nselected, with |Xc\ns| = 1 (1-shot) and |Xc\ns| = 5 (5-shot) ex-\namples selected per class, to serve as support set Xs. Query\nset Xq contains 15 images per class. Therefore, the evalua-\ntion is performed over N = 75 query images per task. The\naverage accuracy of these 10,000 few shot tasks are reported\nalong with the 95% conﬁdence interval. For the iNat bench-\nmark, the number of support examples |Xc\ns| per class varies.\nWe performed 227-way multi-shot evaluation, and report\nthe top-1 accuracy averaged over the test images per class\n(Per Class in Table 3), as well as the average over all test\nimages (Mean in Table 3), following the same procedure as\nin (Wertheimer & Hariharan, 2019; Wang et al., 2019).\n3.3. Network Models\nWe evaluate LaplacianShot on four different backbone net-\nwork models to learn feature extractor fθ:\nResNet-18/50 is based on the deep residual network archi-\ntecture (He et al., 2016), where the ﬁrst two down-sampling\nlayers are removed, setting the stride to 1 in the ﬁrst convolu-\ntional layer and removing the ﬁrst max-pool layer. The ﬁrst\nconvolutional layer is used with a kernel of size 3×3 instead\nof 7×7. ResNet-18 has 8 basic residual blocks, and ResNet-\n50 has 16 bottleneck blocks. For all the networks, the dimen-\nsion of the extracted features is 512. MobileNet (Howard\net al., 2017) was initially proposed as a light-weight con-\nvolutional network for mobile-vision applications. In our\nsetting, we remove the ﬁrst two down-sampling operations,\nwhich results in a feature embedding of size 1024. WRN\n(Zagoruyko & Komodakis, 2016) widens the residual blocks\nby adding more convolutional layers and feature planes. In\nour case, we used 28 convolutional layers, with a widen-\ning factor of 10 and an extracted-feature dimension of 640.\nFinally, we used the standard 121-layer DenseNet (Huang\net al., 2017), omitting the ﬁrst two down-sampling layers\nand setting the stride to 1. We changed the kernel size of\nthe ﬁrst convolutional layer to 3 × 3. The extracted feature\nvector is of dimension 1024.\n",
    "Laplacian Regularized Few-Shot Learning\nFigure 1. We tune regularization parameter λ over values ranging from 0.1 to 1.5. In the above plots, we show the impact of choosing λ\non both validation and test accuracies. The values of λ based on the best validation accuracies correspond to good accuracies in the test\nclasses. The results are shown for different networks on miniImageNet dataset, for both 1-shot (top row) and 5-shot (bottom row).\nFigure 2. Convergence of Algorithm 1: Bounds Bi(Y) vs. iteration numbers for features from different networks. Here, the plots are\nproduced by setting λ = 1.0, for a single 5-way 5 shot task from the miniImageNet test set.\n3.4. Implementation Details\nNetwork model training: We trained the network models\nusing the standard cross-entropy loss on the base classes,\nwith a label-smoothing (Szegedy et al., 2016) parameter set\nto 0.1. Note that the base training did not involve any meta-\nlearning or episodic-training strategy. We used the SGD\noptimizer to train the models, with mini-batch size set to 256\nfor all the networks, except for WRN and DenseNet, where\nwe used mini-batch sizes of 128 and 100, respectively. We\nused two 16GB P100 GPUs for network training with base\nclasses. For miniImageNet, CUB and tieredImageNet, we\nused early stopping by evaluating the the nearest-prototype\nclassiﬁcation accuracy on the validation classes, with L2\nnormalized features.\nPrototype estimation and feature transformation: Dur-\ning the inference on test classes, SimpleShot (Wang et al.,\n2019) performs the following feature transformations: L2\nnormalization, xq := xq/∥xq∥2 and CL2, which computes\nthe mean of the base class features ¯x =\n1\n|Xbase|\nP\nx∈Xbase x\nand centers the extracted features as xq := xq −¯x, which\nis followed by an L2 normalization. We report the results\nin Table 1 and 2 with CL2 normalized features. In Table 3\nfor the iNat dataset, we provide the results with both nor-\nmalized and unnormalized (UN) features for a comparative\nanalysis. We reproduced the results of SimpleShot with our\ntrained network models. In the 1-shot setting, prototype mc\nis just the support example xq ∈Xc\ns of class c, whereas in\nmulti-shot, mc is the simple mean of the support examples\nof class c. Another option is to use rectiﬁed prototypes, i.e.,\na weighted combination of features from both the support\nexamples in Xc\ns and query samples in Xc\nq, which are initially\n",
    "Laplacian Regularized Few-Shot Learning\npredicted as belonging to class c using Eq. (2):\n˜mc =\n1\n|Xcs| + |Xcq|\nX\nxp∈{Xcs ,Xcq }\nexp(cos(xp, mc))\nPC\nc=1 exp(cos(xp, mc))\nxp,\nwhere cos denotes the cosine similarity. And, for a given\nfew-shot task, we compute the cross-domain shift ∆as\nthe difference between the mean of features within the\nsupport set and the mean of features within the query set:\n∆=\n1\n|Xs|\nP\nxp∈Xs xp −\n1\n|Xq|\nP\nxq∈Xq xq. Then, we rectify\neach query point xp ∈Xq in the few-shot task as follows:\nxp = xp + ∆. This shift correction is similar to the pro-\ntotype rectiﬁcation in (Liu et al., 2019a). Note that our\nLaplacianShot model in Eq. (1) is agnostic to the way of\nestimating the prototypes: It can be used either with the\nstandard prototypes (mc) or with the rectiﬁed ones ( ˜mc).\nWe report the results of LaplacianShot with the rectiﬁed pro-\ntotypes in Table 1 and 2, for miniImagenet, tieredImagenet\nand CUB. We do not report the results with the rectiﬁed\nprototypes in Table 3 for iNat, as rectiﬁcation drastically\nworsen the performance.\nFor W, we used the k-nearest neighbor afﬁnities as follows:\nw(xq, xp) = 1 if xp is within the k nearest neighbor of\nxq, and w(xq, xp) = 0 otherwise. In our experiments, k is\nsimply chosen from three typical values (3, 5 or 10) tuned\nover 500 few-shot tasks from the base training classes (i.e.,\nwe did not use test data for choosing k). We used k = 3\nfor miniImageNet, CUB and tieredImageNet and k = 10\nfor iNat benchmark. Regularization parameter λ is chosen\nbased on the validation class accuracy for miniImageNet,\nCUB and tieredImageNet. This will be discussed in more\ndetails in section 3.6. For the iNat experiments, we simply\nﬁx λ = 1.0, as there is no validation set for this benchmark.\n3.5. Results\nWe evaluated LaplacianShot over ﬁve different bench-\nmarks, with different scenarios and difﬁculties: Generic\nimage classiﬁcation, ﬁne-grained image classiﬁcation, cross-\ndomain adaptation, and imbalanced class distributions.\nWe report the results of LaplacianShot for miniImageNet,\ntieredImageNet, CUB and iNat datasets, in Tables 1, 2 and\n3, along with comparisons with state-of-the-art methods.\nGeneric image classiﬁcation: Table 1 reports the results of\ngeneric image classiﬁcation for the standard miniImageNet\nand tieredImageNet few-shot benchmarks. We can clearly\nobserve that LaplacianShot outperforms state-of-the-art\nmethods by large margins, with gains that are consistent\nacross different settings and network models. It is worth\nmentioning that, for challenging scenarios, e.g., 1-shot with\nlow-capacity models, LaplacianShot outperforms complex\nmeta-learning methods by more than 9%. For instance,\ncompared to well-known MAML (Finn et al., 2017) and\nProtoNet (Snell et al., 2017), and to the recent MetaoptNet\n(Lee et al., 2019), LaplacianShot brings improvements of\nnearly 22%, 17%, and 9%, respectively, under the same\nevaluation conditions. Furthermore, it outperforms the very\nrecent transductive approaches in (Dhillon et al., 2020; Liu\net al., 2019a;b) by signiﬁcant margins. With better learned\nfeatures with WRN and DenseNet, LaplacianShot brings sig-\nniﬁcant performance boosts, yielding state-of-the art results\nin few-shot classiﬁcation, without meta-learning.\nFine-grained image classiﬁcation: Table 2 reports the re-\nsults of ﬁne-grained few-shot classiﬁcation on CUB, with\nResnet-18 network. LaplacianShot outperforms the best\nperforming method in this setting by a 7% margin.\nCross-domain (mini-ImageNet →CUB): We perform the\nvery interesting few-shot experiment, with a cross-domain\nscenario, following the setting in (Chen et al., 2019). We\nused the ResNet-18 model trained on the miniImagenet base\nclasses, while evaluation is performed on CUB few-shot\ntasks, with 50 test classes. Table 2 (rightmost column)\nreports the results. In this cross-domain setting, and consis-\ntently with the standard settings, LaplacianShot outperforms\ncomplex meta-learning methods by substantial margins.\nImbalanced class distribution: Table 3 reports the results\nfor the more challenging, class-imbalanced iNat benchmark,\nwith different numbers of support examples per class and,\nalso, with high visual similarities between the different\nclasses, making class separation difﬁcult. To our knowledge,\nonly (Wertheimer & Hariharan, 2019; Wang et al., 2019)\nreport performances on this benchmark, and SimpleShot\n(Wang et al., 2019) represents the state-of-the-art. We com-\npared with SimpleShot using unnormalized extracted fea-\ntures (UN), L2 and CL2 normalized features. Our Laplacian\nregularization yields signiﬁcant improvements, regardless\nof the network model and feature normalization. However,\nunlike SimpleShot, our method reaches its best performance\nwith the unnormalized features. Note that, for iNat, we\ndid not use the rectiﬁed prototypes. These results clearly\nhighlight the beneﬁt Laplacian regularization brings in chal-\nlenging class-imbalance scenarios.\n3.6. Ablation Study\nChoosing the Value of λ: In LaplacianShot, we need to\nchoose the value of regularization parameter λ, which con-\ntrols the trade-off between the nearest-prototype classiﬁer\nterm aq and Laplacian regularizer bi\nq. We tuned this param-\neter using the validation classes by sampling 500 few-shot\ntasks. LaplacianShot is used in each few-shot task with the\nfollowing values of λ: [0.1, 0.3, 0.5, 0.7, 0.8, 1.0, 1.2, 1.5].\nThe best λ corresponding to the best average 1-shot and\n5-shot accuracy over validation classes/data is selected for\ninference over the test classes/data. To examine experi-\nmentally whether the chosen values of λ based on the best\nvalidation accuracies correspond to good accuracies in the\n",
    "Laplacian Regularized Few-Shot Learning\nTable 4. Ablation study on the effect of each term corresponding to nearest prototype N(Y), Laplacian L(Y) and rectiﬁed prototype ˜mc.\nResults are reported with ResNet-18 network. Note that, the Laplacian regularization L(Y) improve the results consistently.\nmini-ImageNet\ntiered-ImageNet\nCUB\nN(Y)\nL(Y)\n˜mc\n1-shot\n5-shot\n1-shot\n5-shot\n1shot\n5-shot\n\u0013\n\u0017\n\u0017\n63.10\n79.92\n69.68\n84.56\n70.28\n86.37\n\u0013\n\u0013\n\u0017\n66.20\n80.75\n72.89\n85.25\n74.46\n86.86\n\u0013\n\u0017\n\u0013\n69.74\n82.01\n76.73\n85.74\n78.76\n88.55\n\u0013\n\u0013\n\u0013\n72.11\n82.31\n78.98\n86.39\n80.96\n88.68\nTable 5. Average inference time (in seconds) for the 5-shot tasks\nin miniImagenet dataset.\nMethods\ninference time\nSimpleShot (Wang et al., 2019)\n0.009\nTransductive tuning (Dhillon et al., 2020)\n20.7\nLaplacianShot (ours)\n0.012\ntest classes, we plotted both the validation and test class ac-\ncuracies vs. different values of λ for miniImageNet (Figure\n1). The results are intuitive, with a consistent trend in both\n1-shot and 5-shot settings. Particularly, for 1-shot tasks,\nλ = 0.7 provides the best results in both validation and test\naccuracies. In 5-shot tasks, the best test results are obtained\nmostly with λ = 0.1, while the best validation accuracies\nwere reached with higher values of λ. Nevertheless, we re-\nport the results of LaplacianShot with the values of λ chosen\nbased on the best validation accuracies.\nEffects of Laplacian regularization: We conducted an ab-\nlation study on the effect of each term in our model, i.e.,\nnearest-prototype classiﬁer N(Y) and Laplacian regular-\nizer L(Y). We also examined the effect of using prototype\nrectiﬁcation, i.e., ˜mc instead of mc. Table 4 reports the\nresults, using the ResNet-18 network. The ﬁrst row corre-\nsponds to the prediction of the nearest neighbor classiﬁer\n(λ = 0), and the second shows the effect of adding Lapla-\ncian regularization. In the 1-shot case, the latter boosts the\nperformances by at least 3%. Prototype rectiﬁcation (third\nand fourth rows) also boosts the performances. Again, in\nthis case, the improvement that the Laplacian term brings is\nsigniﬁcant, particularly in the 1-shot case (2 to 3%).\nConvergence of transductive LaplacianShot inference:\nThe proposed algorithm belongs to the family of bound op-\ntimizers or MM algorithms. In fact, the MM principle can\nbe viewed as a generalization of expectation-maximization\n(EM). Therefore, in general, MM algorithms inherit the\nmonotonicity and convergence properties of EM algorithms\n(Vaida, 2005), which are well-studied in the literature. In\nfact, Theorem 3 in (Vaida, 2005) states a simple condition\nfor convergence of the general MM procedure, which is al-\nmost always satisﬁed in practice: The surrogate function has\na unique global minimum. In Fig. 2, we plotted surrogates\nBi(Y), up to a constant, i.e., Eq. (7), as functions of the iter-\nation numbers, for different networks. One can see that the\nvalue of Bi(Y) decreases monotonically at each iteration,\nand converges, typically, within less than 15 iterations.\nInference time: We computed the average inference time\nrequired for each 5-shot task. Table 5 reports these infer-\nence times for miniImageNet with the WRN network. The\npurpose of this is to check whether there exist a signiﬁcant\ncomputational overhead added by our Laplacian-regularized\ntransductive inference, in comparison to inductive inference.\nNote that the computational complexity of the proposed\ninference is O(NkC) for a few-shot task, where k is the\nneighborhood size for afﬁnity matrix W. The inference\ntime per few-shot task for LaplacianShot is close to induc-\ntive SimpleShot run-time (LaplacianShot is only 1-order of\nmagnitude slower), and is 3-order-of-magnitude faster than\nthe transductive ﬁne-tuning in (Dhillon et al., 2020).\n4. Conclusion\nWithout meta-learning, we provide state-of-the-art results,\noutperforming signiﬁcantly a large number of sophisticated\nfew-shot learning methods, in all benchmarks. Our trans-\nductive inference is a simple constrained graph clustering\nof the query features. It can be used in conjunction with any\nbase-class training model, consistently yielding improve-\nments. Our results are in line with several recent baselines\n(Dhillon et al., 2020; Chen et al., 2019; Wang et al., 2019)\nthat reported competitive performances, without resorting\nto complex meta-learning strategies. This recent line of sim-\nple methods emphasizes the limitations of current few-shot\nbenchmarks, and questions the viability of a large body of\nconvoluted few-shot learning techniques in the recent lit-\nerature. As pointed out in Fig. 1 in (Dhillon et al., 2020),\nthe progress made by an abundant recent few-shot literature,\nmostly based on meta-learning, may be illusory. Classical\nand simple regularizers, such as the entropy in (Dhillon\net al., 2020) or our Laplacian term, well-established in semi-\nsupervised learning and clustering, achieve outstanding per-\nformances. We do not claim to hold the ultimate solution for\nfew-shot learning, but we believe that our model-agnostic\ntransductive inference should be used as a strong baseline\nfor future few-shot learning research.\n",
    "Laplacian Regularized Few-Shot Learning\nReferences\nBelkin, M., Niyogi, P., and Sindhwani, V. Manifold regular-\nization: A geometric framework for learning from labeled\nand unlabeled examples. Journal of Machine Learning\nResearch, 7:2399–2434, 2006.\nChen, W.-Y., Liu, Y.-C., Kira, Z., Wang, Y.-C. F., and Huang,\nJ.-B. A closer look at few-shot classiﬁcation. In Interna-\ntional Conference on Learning Representations (ICLR),\n2019.\nDhillon, G. S., Chaudhari, P., Ravichandran, A., and Soatto,\nS. A baseline for few-shot image classiﬁcation. In Inter-\nnational Conference on Learning Representations (ICLR),\n2020.\nFei-Fei, L., Fergus, R., and Perona, P. One-shot learning of\nobject categories. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 28:594–611, 2006.\nFinn, C., Abbeel, P., and Levine, S. Model-agnostic meta-\nlearning for fast adaptation of deep networks. In Interna-\ntional Conference on Machine Learning (ICML), 2017.\nGidaris, S. and Komodakis, N. Dynamic few-shot visual\nlearning without forgetting. In Conference on Computer\nVision and Pattern Recognition (CVPR), 2018.\nGidaris, S., Bursuc, A., Komodakis, N., P´erez, P., and\nCord, M. Boosting few-shot visual learning with self-\nsupervision. In International Conference on Computer\nVision (ICCV), 2019.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In Conference on Computer\nVision and Pattern Recognition (CVPR), 2016.\nHou, R., Chang, H., Bingpeng, M., Shan, S., and Chen, X.\nCross attention network for few-shot classiﬁcation. In\nNeural Information Processing Systems (NeurIPS), 2019.\nHoward, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang,\nW., Weyand, T., Andreetto, M., and Adam, H. Mobilenets:\nEfﬁcient convolutional neural networks for mobile vision\napplications. Preprint arXiv:1704.04861, 2017.\nHu, S. X., Moreno, P. G., Xiao, Y., Shen, X., Obozinski,\nG., Lawrence, N. D., and Damianou, A. Empirical bayes\ntransductive meta-learning with synthetic gradients. In\nInternational Conference on Learning Representations\n(ICLR), 2020.\nHuang, G., Liu, Z., Van Der Maaten, L., and Weinberger,\nK. Q. Densely connected convolutional networks. In\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2017.\nJiang, X., Havaei, M., Varno, F., Chartrand, G., Chapa-\ndos, N., and Matwin, S. Learning to learn with condi-\ntional class dependencies. In International Conference\non Learning Representations (ICLR), 2019.\nKim, J., Kim, T., Kim, S., and Yoo, C. D. Edge-labeling\ngraph neural network for few-shot learning.\nIn Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), 2019.\nLange, K., Hunter, D. R., and Yang, I. Optimization transfer\nusing surrogate objective functions. Journal of computa-\ntional and graphical statistics, 9(1):1–20, 2000.\nLee, K., Maji, S., Ravichandran, A., and Soatto, S. Meta-\nlearning with differentiable convex optimization. In Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), 2019.\nLiu, J., Song, L., and Qin, Y. Prototype rectiﬁcation for\nfew-shot learning. Preprint arXiv:1911.10713, 2019a.\nLiu, Y., Lee, J., Park, M., Kim, S., Yang, E., Hwang, S., and\nYang, Y. Learning to propagate labels: Transductive prop-\nagation network for few-shot learning. In International\nConference on Learning Representations (ICLR), 2019b.\nMiller, E., Matsakis, N., and Viola, P. Learning from one\nexample through shared densities on transforms. Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), 2000.\nMishra, N., Rohaninejad, M., Chen, X., and Abbeel, P. A\nsimple neural attentive meta-learner. In International\nConference on Learning Representations (ICLR), 2018.\nMunkhdalai, T., Yuan, X., Mehri, S., and Trischler, A. Rapid\nadaptation with conditionally shifted neurons. In Interna-\ntional Conference on Machine Learning (ICML), 2018.\nNarasimhan, M. and Bilmes, J. A submodular-supermodular\nprocedure with applications to discriminative structure\nlearning. In Conference on Uncertainty in Artiﬁcial Intel-\nligence (UAI), 2005.\nOreshkin, B., L´opez, P. R., and Lacoste, A. Tadam: Task de-\npendent adaptive metric for improved few-shot learning.\nIn Neural Information Processing Systems (NeurIPS),\n2018.\nQiao, L., Shi, Y., Li, J., Wang, Y., Huang, T., and Tian,\nY. Transductive episodic-wise adaptive metric for few-\nshot learning. In International Conference on Computer\nVision (ICCV), 2019.\nQiao, S., Liu, C., Shen, W., and Yuille, A. L. Few-shot\nimage recognition by predicting parameters from activa-\ntions. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2018.\n",
    "Laplacian Regularized Few-Shot Learning\nRavi, S. and Larochelle, H. Optimization as a model for few-\nshot learning. In International Conference on Learning\nRepresentations (ICLR), 2017.\nRen, M., Triantaﬁllou, E., Ravi, S., Snell, J., Swersky, K.,\nTenenbaum, J. B., Larochelle, H., and Zemel, R. S. Meta-\nlearning for semi-supervised few-shot classiﬁcation. In\nInternational Conference on Learning Representations\nICLR, 2018.\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,\nMa, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,\nM., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale\nVisual Recognition Challenge. International Journal of\nComputer Vision (IJCV), 115(3):211–252, 2015.\nRusu, A. A., Rao, D., Sygnowski, J., Vinyals, O., Pascanu,\nR., Osindero, S., and Hadsell, R. Meta-learning with\nlatent embedding optimization. In International Confer-\nence on Learning Representations (ICLR), 2019.\nShi, J. and Malik, J. Normalized cuts and image segmenta-\ntion. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 22(8):888–905, 2000.\nSnell, J., Swersky, K., and Zemel, R. Prototypical networks\nfor few-shot learning. In Neural Information Processing\nSystems (NeurIPS), 2017.\nSun, Q., Liu, Y., Chua, T., and Schiele, B. Meta-transfer\nlearning for few-shot learning. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR), June 2019.\nSung, F., Yang, Y., Zhang, L., Xiang, T., Torr, P. H., and\nHospedales, T. M. Learning to compare: Relation net-\nwork for few-shot learning. In Conference on Computer\nVision and Pattern Recognition (CVPR), 2018.\nSzegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna,\nZ. Rethinking the inception architecture for computer\nvision. In Conference on Computer Vision and Pattern\nRecognition, 2016.\nTian, F., Gao, B., Cui, Q., Chen, E., and Liu, T.-Y. Learn-\ning deep representations for graph clustering. In AAAI\nConference on Artiﬁcial Intelligence, 2014.\nVaida, F. Parameter convergence for em and mm algorithms.\nStatistica Sinica, 15:831–840, 2005.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, u., and Polosukhin, I. Attention\nis all you need. In Neural Information Processing Systems\n(NeurIPS), 2017.\nVinyals, O., Blundell, C., Lillicrap, T. P., Kavukcuoglu, K.,\nand Wierstra, D. Matching networks for one shot learning.\nIn Neural Information Processing Systems (NeurIPS),\n2016.\nVon Luxburg, U. A tutorial on spectral clustering. Statistics\nand computing, 17(4):395–416, 2007.\nWah, C., Branson, S., Welinder, P., Perona, P., and Belongie,\nS. The caltech-ucsd birds-200-2011 dataset. 2011.\nWang, W. and Carreira-Perpin´an, M. A.\nThe lapla-\ncian k-modes algorithm for clustering.\nPreprint\narXiv:1406.3895, 2014.\nWang,\nY.,\nChao,\nW.-L.,\nWeinberger,\nK. Q.,\nand\nvan der Maaten, L.\nSimpleshot: Revisiting nearest-\nneighbor classiﬁcation for few-shot learning. Preprint\narXiv:1911.04623, 2019.\nWertheimer, D. and Hariharan, B. Few-shot learning with\nlocalization in realistic settings. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2019.\nWeston, J., Ratle, F., Mobahi, H., and Collobert, R. Deep\nlearning via semi-supervised embedding. In Neural net-\nworks: Tricks of the trade, pp. 639–655. Springer, 2012.\nYe, H.-J., Hu, H., Zhan, D.-C., and Sha, F. Few-shot learning\nvia embedding adaptation with set-to-set functions. In\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2020.\nYuan, J., Yin, K., Bai, Y., Feng, X., and Tai, X. Bregman-\nproximal augmented lagrangian approach to multiphase\nimage segmentation. In Scale Space and Variational\nMethods in Computer Vision (SSVM), 2017.\nYuille, A. L. and Rangarajan, A. The concave-convex proce-\ndure (CCCP). In Neural Information Processing Systems\n(NeurIPS), 2001.\nZagoruyko, S. and Komodakis, N. Wide residual networks.\nIn British Machine Vision Conference (BMVC), 2016.\nZhang, J., Zhao, C., Ni, B., Xu, M., and Yang, X. Varia-\ntional few-shot learning. In International Conference on\nComputer Vision (ICCV), 2019.\nZhang, Z., Kwok, J. T., and Yeung, D.-Y. Surrogate max-\nimization/minimization algorithms and extensions. Ma-\nchine Learning, 69:1–33, 2007.\nZhou, D., Bousquet, O., Lal, T. N., Weston, J., and\nSch¨olkopf, B. Learning with local and global consistency.\nIn Neural Information Processing Systems (NeurIPS),\n2004.\nZiko, I., Granger, E., and Ben Ayed, I. Scalable lapla-\ncian k-modes. In Neural Information Processing Systems\n(NeurIPS), 2018.\n"
  ],
  "full_text": "Laplacian Regularized Few-Shot Learning\nImtiaz Masud Ziko 1 Jose Dolz 1 Eric Granger 1 Ismail Ben Ayed 1\nAbstract\nWe propose a transductive Laplacian-regularized\ninference for few-shot tasks. Given any feature\nembedding learned from the base classes, we\nminimize a quadratic binary-assignment function\ncontaining two terms: (1) a unary term assign-\ning query samples to the nearest class prototype,\nand (2) a pairwise Laplacian term encouraging\nnearby query samples to have consistent label as-\nsignments. Our transductive inference does not\nre-train the base model, and can be viewed as a\ngraph clustering of the query set, subject to super-\nvision constraints from the support set. We derive\na computationally efﬁcient bound optimizer of a\nrelaxation of our function, which computes inde-\npendent (parallel) updates for each query sample,\nwhile guaranteeing convergence. Following a sim-\nple cross-entropy training on the base classes, and\nwithout complex meta-learning strategies, we con-\nducted comprehensive experiments over ﬁve few-\nshot learning benchmarks. Our LaplacianShot\nconsistently outperforms state-of-the-art methods\nby signiﬁcant margins across different models,\nsettings, and data sets. Furthermore, our trans-\nductive inference is very fast, with computational\ntimes that are close to inductive inference, and\ncan be used for large-scale few-shot tasks.\n1. Introduction\nDeep learning models have achieved human-level perfor-\nmances in various tasks. The success of these models rely\nconsiderably on exhaustive learning from large-scale labeled\ndata sets. Nevertheless, they still have difﬁculty general-\nizing to novel classes unseen during training, given only\na few labeled instances for these new classes. In contrast,\nhumans can learn new tasks easily from a handful of ex-\namples, by leveraging prior experience and related context.\n1 ´ETS Montreal, Canada. Correspondence to: Imtiaz Masud\nZiko <imtiaz-masud.ziko.1@etsmtl.ca>.\nProceedings of the 37 th International Conference on Machine\nLearning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by\nthe author(s).\nFew-shot learning (Fei-Fei et al., 2006; Miller et al., 2000;\nVinyals et al., 2016) has emerged as an appealing paradigm\nto bridge this gap. Under standard few-shot learning scenar-\nios, a model is ﬁrst trained on substantial labeled data over\nan initial set of classes, often referred to as the base classes.\nThen, supervision for novel classes, which are unseen during\nbase training, is limited to just one or few labeled exam-\nples per class. The model is evaluated over few-shot tasks,\neach one supervised by a few labeled examples per novel\nclass (the support set) and containing unlabeled samples for\nevaluation (the query set).\nThe problem has recently received substantial research in-\nterests, with a large body of work based on complex meta-\nlearning and episodic-training strategies. The meta-learning\nsetting uses the base training data to create a set of few-shot\ntasks (or episodes), with support and query samples that\nsimulate generalization difﬁculties during test times, and\ntrain the model to generalize well on these artiﬁcial tasks.\nFor example, (Vinyals et al., 2016) introduced matching\nnetwork, which employs an attention mechanism to pre-\ndict the unknown query samples as a linear combination\nof the support labels, while using episodic training and\nmemory architectures. Prototypical networks (Snell et al.,\n2017) maintain a single prototype representation for each\nclass in the embedding space, and minimize the negative\nlog-probability of the query features with episodic training.\nRavi & Larochelle (2017) viewed optimization as a model\nfor few-shot learning, and used an LSTM meta-learner to\nupdate classiﬁer parameters. Finn et al. (2017) proposed\nMAML, a meta-learning strategy that attempts to make a\nmodel “easy” to ﬁne-tune. These widely adopted works\nwere recently followed by an abundant meta-learning litera-\nture, for instance, (Sung et al., 2018; Oreshkin et al., 2018;\nMishra et al., 2018; Rusu et al., 2019; Liu et al., 2019b; Hou\net al., 2019; Ye et al., 2020), among many others.\nSeveral recent studies explored transductive inference for\nfew-shot tasks, e.g., (Liu et al., 2019b; Hou et al., 2019;\nDhillon et al., 2020; Hu et al., 2020; Kim et al., 2019;\nQiao et al., 2019), among others. Given a few-shot task\nat test time, transductive inference performs class predic-\ntions jointly for all the unlabeled query samples of the task,\nrather than one sample at a time as in inductive inference.\nFor instance, TPN (Liu et al., 2019b) used label propaga-\ntion (Zhou et al., 2004) along with episodic training and a\narXiv:2006.15486v3  [cs.LG]  28 Apr 2021\n\n\nLaplacian Regularized Few-Shot Learning\nspeciﬁc network architecture, so as to learn how to prop-\nagate labels from labeled to unlabeled samples. CAN-T\n(Hou et al., 2019) is another meta-learning based transduc-\ntive method, which uses attention mechanisms to propagate\nlabels to unlabeled query samples. The transductive ﬁne-\ntuning method by (Dhillon et al., 2020) re-train the network\nby minimizing an additional entropy loss, which encour-\nages peaked (conﬁdent) class predictions at unlabeled query\npoints, in conjunction with a standard cross-entropy loss\ndeﬁned on the labeled support set.\nTransductive few-shot methods typically perform better than\ntheir inductive counterparts. However, this may come at the\nprice of a much heavier computational complexity during\ninference. For example, the entropy ﬁne-tuning in (Dhillon\net al., 2020) re-trains the network, performing gradient up-\ndates over all the parameters during inference. Also, the\nlabel propagation in (Liu et al., 2019b) requires a matrix\ninversion, which has a computational overhead that is cubic\nwith respect to the number of query samples. This may\nbe an impediment for deployment for large-scale few-shot\ntasks.\nWe propose a transductive Laplacian-regularized inference\nfor few-shot tasks. Given any feature embedding learned\nfrom the base data, our method minimizes a quadratic\nbinary-assignment function integrating two types of poten-\ntials: (1) unary potentials assigning query samples to the\nnearest class prototype, and (2) pairwise potentials favor-\ning consistent label assignments for nearby query samples.\nOur transductive inference can be viewed as a graph clus-\ntering of the query set, subject to supervision constraints\nfrom the support set, and does not re-train the base model.\nFollowing a relaxation of our function, we derive a compu-\ntationally efﬁcient bound optimizer, which computes inde-\npendent (parallel) label-assignment updates for each query\npoint, with guaranteed convergence. We conducted compre-\nhensive experiments on ﬁve few-shot learning benchmarks,\nwith different levels of difﬁculties. Using a simple cross-\nentropy training on the base classes, and without complex\nmeta-learning strategies, our LaplacianShot outperforms\nstate-of-the-art methods by signiﬁcant margins, consistently\nproviding improvements across different settings, data sets,\nand training models. Furthermore, our transductive infer-\nence is very fast, with computational times that are close to\ninductive inference, and can be used for large-scale tasks.\n2. Laplacian Regularized Few-Shot Learning\n2.1. Proposed Formulation\nIn the few-shot setting, we are given a labeled support set\nXs = SC\nc=1 Xc\ns with C test classes, where each novel class c\nhas |Xc\ns| labeled examples, for instance, |Xc\ns| = 1 for 1-shot\nand |Xc\ns| = 5 for 5-shot. The objective of few-shot learn-\ning is, therefore, to accurately classify unlabeled unseen\nquery sample set Xq = SC\nc=1 Xc\nq from these C test classes.\nThis setting is referred to as the |Xc\ns|-shot C-way few-shot\nlearning.\nLet fθ denotes the embedding function of a deep convolu-\ntional neural network, with parameters θ and xq = fθ(zq) ∈\nRM encoding the features of a given data point zq. Embed-\nding fθ is learned from a labeled training set Xbase, with\nbase classes that are different from the few-shot classes of\nXs and Xq. In our work, parameters θ are learned through\na basic network training with the standard cross-entropy\nloss deﬁned over Xbase, without resorting to any complex\nepisodic-training or meta-learning strategy. For each query\nfeature point xq in a few-shot task, we deﬁne a latent bi-\nnary assignment vector yq = [yq,1, . . . , yq,C]t ∈{0, 1}C,\nwhich is within the C-dimensional probability simplex\n∇C = {y ∈[0, 1]C | 1ty = 1}: binary yq,c is equal\nto 1 if xq belongs to class c, and equal to 0 otherwise. t is\nused as the transpose operator. Let Y denotes the N × C\nmatrix whose rows are formed by yq, where N is the num-\nber of query points in Xq. We propose a transductive few-\nshot inference, which minimizes a Laplacian-regularization\nobjective for few-shot tasks w.r.t assignment variables Y,\nsubject to simplex and integer constraints yq ∈∇C and\nyq ∈{0, 1}C, ∀q:\nE(Y)\n=\nN(Y) + λ\n2 L(Y)\n(1)\nN(Y)\n=\nN\nX\nq=1\nC\nX\nc=1\nyq,cd(xq −mc)\nL(Y)\n=\n1\n2\nX\nq,p\nw(xq, xp)∥yq −yp∥2\nIn (1), the ﬁrst term N(Y) is minimized globally when\neach query point is assigned to the class of the nearest\nprototype mc from the support set, using a distance metric\nd(xq, mc), such as the Euclidean distance. In the 1-shot\nsetting, prototype mc is the support example of class c,\nwhereas in multi-shot, mc can be the mean of the support\nexamples. In fact, mc can be further rectiﬁed by integrating\ninformation from the query features, as we will detail later\nin our experiments.\nThe second term L(Y) is the well-known Laplacian regular-\nizer, which can be equivalently written as tr(YtLY), where\nL is the Laplacian matrix1 corresponding to afﬁnity matrix\nW = [w(xq, xp)], and tr denotes the trace operator. Pair-\nwise potential w(xq, xp) evaluates the similarity between\nfeature vectors xq and xp, and can be computed using some\nkernel function. The Laplacian term encourages nearby\n1The Laplacian matrix corresponding to afﬁnity matrix W =\n[w(xq, xp)] is L = D −W, with D the diagonal matrix whose\ndiagonal elements are given by: Dq = P\np w(xq, xp).\n\n\nLaplacian Regularized Few-Shot Learning\nAlgorithm 1 Proposed Algorithm for LaplacianShot\nInput: Xs, Xq, λ, fθ\nOutput: Labels ∈{1, .., C}N for Xq\nGet prototypes mc.\nCompute aq using (8a) ∀xq ∈Xq.\nInitialize i = 1.\nInitialize yi\nq =\nexp(−aq)\n1t exp(−aq).\nrepeat\nCompute yi+1\nq\nusing (12)\nyi\nq ←yi+1\nq\n.\nY = [yi\nq]; ∀q.\ni = i + 1.\nuntil Bi(Y) in (7) does not change\nlq = arg max\nc\nyq; ∀yq ∈Y.\nLabels = {lq}N\nq=1\npoints (xq, xp) in the feature space to have the same latent\nlabel assignment, thereby regularizing predictions at query\nsamples for few-shot tasks. As we will show later in our\ncomprehensive experiments, the pairwise Laplacian term\ncomplements the unary potentials in N(Y), substantially\nincreasing the predictive performance of few-shot learning\nacross different networks, and various benchmark datasets\nwith different levels of difﬁculty.\nMore generally, Laplacian regularization is widely used\nin the contexts of graph clustering (Von Luxburg, 2007;\nShi & Malik, 2000; Ziko et al., 2018; Wang & Carreira-\nPerpin´an, 2014) and semi-supervised learning (Weston et al.,\n2012; Belkin et al., 2006). For instance, popular spectral\ngraph clustering techniques (Von Luxburg, 2007; Shi & Ma-\nlik, 2000) optimize the Laplacian term subject to partition-\nbalance constraints. In this connection, our transductive\ninference can be viewed as a graph clustering of the query\nset, subject to supervision constraints from the support set.\nRegularization parameter λ controls the trade-off between\nthe two terms. It is worth noting that the recent nearest-\nprototype classiﬁcation in (Wang et al., 2019) corresponds\nto the particular case of λ = 0 of our model in (1). It\nassigns a query sample xq to the label of the closest support\nprototype in the feature space, thereby minimizing N(Y):\nyq,c∗= 1\nif\nc∗= arg min\nc∈{1,...,C}\nd(xq, mc)\n(2)\n2.2. Optimization\nIn this section, we propose an efﬁcient bound-optimization\ntechnique for solving a relaxed version of our objective\nin (1), which guarantees convergence, while computing in-\ndependent closed-form updates for each query sample in\nfew-shot tasks. It is well known that minimizing pairwise\nfunctions over binary variables is NP-hard (Tian et al., 2014),\nand a standard approach in the context of clustering algo-\nrithms is to relax the integer constraints, for instance, using\na convex (Wang & Carreira-Perpin´an, 2014) or a concave\nrelaxation (Ziko et al., 2018). In fact, by relaxing integer\nconstraints yq ∈{0, 1}C, our objective in (1) becomes a\nconvex quadratic problem. However, this would require\nsolving for the N × C assignment variables all together,\nwith additional projections steps for handling the simplex\nconstraints. In this work, we use a concave relaxation of\nthe Laplacian-regularized objective in (1), which, as we will\nlater show, yields fast independent and closed-form updates\nfor each assignment variable, with convergence guarantee.\nFurthermore, it enables us to draw interesting connections\nbetween Laplacian regularization and attention mechanisms\nin few-shot learning (Vinyals et al., 2016).\nIt is easy to verify that, for binary (integer) simplex variables,\nthe Laplacian term in (1) can be written as follows, after\nsome simple manipulations:\nL(Y) =\nX\nq\nDq −\nX\nq,p\nw(xq, xp)yt\nqyp\n(3)\nwhere Dq = P\np w(xq, xp) denotes the degree of query\nsample xq. By relaxing integer constraints yq ∈{0, 1}C,\nthe expression in Eq. (3) can be viewed as a concave relax-\nation2 for Laplacian term L(Y) when symmetric afﬁnity\nmatrix W = [w(xq, xp)] is positive semi-deﬁnite. As we\nwill see in the next paragraph, concavity is important to\nderive an efﬁcient bound optimizer for our model, with\nindependent and closed-form updates for each query sam-\nple. Notice that the ﬁrst term in relaxation (3) is a constant\nindependent of the soft (relaxed) assignment variables.\nWe further augment relaxation (3) with a convex negative-\nentropy barrier function yt\nq log yq, which avoids expensive\nprojection steps and Lagrangian-dual inner iterations for the\nsimplex constraints of each query point. Such a barrier3\nremoves the need for extra dual variables for constraints\nyq ≥0 by restricting the domain of each assignment vari-\nable to non-negative values, and yields closed-form updates\nfor the dual variables of constraints 1tyq = 1. Notice\nthat this barrier function is null at the vertices of the sim-\nplex. Putting all together, and omitting the additive constant\nP\nq Dq in (3), we minimize the following concave-convex\nrelaxation of our objective in (1) w.r.t soft assignment vari-\nables Y, subject to simplex constraints yq ∈∇C, ∀q:\nR(Y) = Yt log Y + N(Y) + λ\n2\n˜L(Y)\n(4)\n2Equality (3) holds in for points on the vertices of the simplex,\ni.e., yq ∈{0, 1}C, but is an approximation for points within the\nsimplex (soft assignments), i.e., yq ∈]0, 1[C.\n3Note that entropy-like barriers are known in the context of\nBregman-proximal optimization (Yuan et al., 2017), and have\nwell-known computational beneﬁts when dealing with simplex\nconstraints.\n\n\nLaplacian Regularized Few-Shot Learning\nwhere ˜L(Y) = −P\nq,p w(xq, xp)yt\nqyp.\nBound optimization:\nIn the following, we detail an\niterative bound-optimization solution for relaxation (4).\nBound optimization, often referred to as MM (Majorize-\nMinimization) framework (Lange et al., 2000; Zhang et al.,\n2007), is a general optimization principle4. At each iteration,\nit updates the variable as the minimum of a surrogate func-\ntion, i.e., an upper bound on the original objective, which\nis tight at the current iteration. This guarantees that the\noriginal objective does not increase at each iteration.\nRe-arranging the soft assignment matrix Y in vector form\nY = [yq] ∈RNC, relaxation ˜L(Y) can be written conve-\nniently in the following form:\n˜L(Y) = −\nX\nq,p\nw(xq, xp)yt\nqyp = YtΨY\n(5)\nwith Ψ = −W⊗I, where ⊗denotes the Kronecker product\nand I is the N × N identity matrix. Note that Ψ is negative\nsemi-deﬁnite for a positive semi-deﬁnite W. Therefore,\nYtΨY is a concave function, and the ﬁrst-order approxima-\ntion of (5) at a current solution Yi (i is the iteration index)\ngives the following tight upper bound on ˜L(Y):\n˜L(Y) = YtΨY ≤(Yi)tΨYi + 2 (ΨYi)t(Y −Yi) (6)\nTherefore, using unary potentials N(Y) and the negative\nentropy barrier in conjunction with the upper bound in (6),\nwe obtain the following surrogate function Bi(Y) for relax-\nation R(Y) at current solution Yi:\nR(Y) ≤Bi(Y) c=\nN\nX\nq=1\nyt\nq(log(yq) + aq −λbi\nq)\n(7)\nwhere c= means equality up to an additive constant5 that is\nindependent of variable Y, and aq and bi\nq are the following\nC-dimensional vectors:\naq = [aq,1, . . . , aq,C]t; aq,c = d(xq, mc)\n(8a)\nbi\nq = [bi\nq,1, . . . , bi\nq,C]t; bi\nq,c =\nX\np\nw(xq, xp)yi\np,c\n(8b)\nIt is straightforward to verify that upper bound Bi(Y) is\ntight at the current iteration, i.e., Bi(Yi) = R(Yi). This\n4The general MM principle is widely used in machine learn-\ning in various problems as it enables to replace a difﬁcult opti-\nmization problem with a sequence of easier sub-problems (Zhang\net al., 2007). Examples of well-known bound optimizers include\nexpectation-maximization (EM) algorithms, the concave-convex\nprocedure (CCCP) (Yuille & Rangarajan, 2001) and submodular-\nsupermodular procedures (SSP) (Narasimhan & Bilmes, 2005),\namong many others.\n5The additive constant in Bi(Y) is a term that depends only on\nYi. This term comes from the Laplacian upper bound in (6).\ncan be seen easily from the ﬁrst-order approximation in\n(6). We iteratively optimize the surrogate function at each\niteration i:\nYi+1 = arg min\nY\nBi(Y)\n(9)\nBecause of upper-bound condition R(Y) ≤Bi(Y), ∀Y,\ntightness condition Bi(Yi) = R(Yi) at the current solu-\ntion, and the fact that Bi(Yi+1) ≤Bi(Yi) due to minimiza-\ntion (9), it is easy to verify that updates (9) guarantee that\nrelaxation R(Y) does not increase at each iteration:\nR(Yi+1) ≤Bi(Yi+1) ≤Bi(Yi) = R(Yi)\nClosed-form solutions of the surrogate functions: No-\ntice that Bi(Y) is a sum of independent functions of each\nassignment variable. Therefore, we can solve (9) for each\nyq independently, while satisfying the simplex constraint:\nmin\nyq∈∇C yt\nq(log(yq) + aq −λbi\nq), ∀q\n(10)\nThe negative entropy barrier term yt\nq log yq in (10) restricts\nyq to be non-negative, removing the need of extra dual vari-\nables for the constraints yq > 0. Also, simplex constraint\n1tyq = 1 is afﬁne. Thus, the solution of the following\nKarush-Kuhn-Tucker (KKT) condition provide the mini-\nmum of (10):\nlog yq + aq −λbi\nq + β1 = 0\n(11)\nwith β the Lagrange multiplier for the simplex constraint.\nThis provides, for each q, closed-form solutions for both the\nprimal and dual variables, yielding the following indepen-\ndent updates of the assignment variables:\nyi+1\nq\n=\nexp(−ai\nq + λbi\nq)\n1t exp(−aiq + λbiq) ∀q\n(12)\n2.3. Proposed Algorithm\nThe overall proposed algorithm is simpliﬁed in Algorithm\n1. Once the network function fθ is learned using the base\ndataset Xbase, our algorithm proceeds with the extracted\nfeatures xq. Before the iterative bound updates, each soft\nassignment y1\nq is initialized as a softmax probability of aq,\nwhich is based on the distances to prototypes mc. The itera-\ntive bound optimization is guaranteed to converge, typically\nless than 15 iterations in our experiments (Figure 2). Also\nthe independent point-wise bound updates yield a parallel\nstructure of the algorithm, which makes it very efﬁcient (and\nconvenient for large-scale few-shot tasks). We refer to our\nmethod as LaplacianShot in the experiments.\nLink to attention mechanisms: Our Laplacian-regularized\nmodel has interesting connection to the popular attention\nmechanism in (Vaswani et al., 2017). In fact, MatchingNet\n\n\nLaplacian Regularized Few-Shot Learning\nTable 1. Average accuracy (in %) in miniImageNet and tieredImageNet. The best results are reported in bold font.\nminiImageNet\ntieredImageNet\nMethods\nNetwork\n1-shot\n5-shot\n1-shot\n5-shot\nMAML (Finn et al., 2017)\nResNet-18\n49.61 ± 0.92\n65.72 ± 0.77\n-\n-\nChen (Chen et al., 2019)\nResNet-18\n51.87 ± 0.77\n75.68 ± 0.63\n-\n-\nRelationNet (Sung et al., 2018)\nResNet-18\n52.48 ± 0.86\n69.83 ± 0.68\n-\n-\nMatchingNet (Vinyals et al., 2016)\nResNet-18\n52.91 ± 0.88\n68.88 ± 0.69\n-\n-\nProtoNet (Snell et al., 2017)\nResNet-18\n54.16 ± 0.82\n73.68 ± 0.65\n-\n-\nGidaris (Gidaris & Komodakis, 2018)\nResNet-15\n55.45 ± 0.89\n70.13 ± 0.68\n-\n-\nSNAIL (Mishra et al., 2018)\nResNet-15\n55.71 ± 0.99\n68.88 ± 0.92\n-\n-\nAdaCNN (Munkhdalai et al., 2018)\nResNet-15\n56.88 ± 0.62\n71.94 ± 0.57\n-\n-\nTADAM (Oreshkin et al., 2018)\nResNet-15\n58.50 ± 0.30\n76.70 ± 0.30\n-\n-\nCAML (Jiang et al., 2019)\nResNet-12\n59.23 ± 0.99\n72.35 ± 0.71\n-\n-\nTPN (Liu et al., 2019b)\nResNet-12\n59.46\n75.64\n-\n-\nTEAM (Qiao et al., 2019)\nResNet-18\n60.07\n75.90\n-\n-\nMTL (Sun et al., 2019)\nResNet-18\n61.20 ± 1.80\n75.50 ± 0.80\n-\n-\nVariationalFSL (Zhang et al., 2019)\nResNet-18\n61.23 ± 0.26\n77.69 ± 0.17\n-\n-\nTransductive tuning (Dhillon et al., 2020)\nResNet-12\n62.35 ± 0.66\n74.53 ± 0.54\n-\n-\nMetaoptNet (Lee et al., 2019)\nResNet-18\n62.64 ± 0.61\n78.63 ± 0.46\n65.99 ± 0.72\n81.56 ± 0.53\nSimpleShot (Wang et al., 2019)\nResNet-18\n63.10 ± 0.20\n79.92 ± 0.14\n69.68 ± 0.22\n84.56 ± 0.16\nCAN+T (Hou et al., 2019)\nResNet-12\n67.19 ± 0.55\n80.64 ± 0.35\n73.21 ± 0.58\n84.93 ± 0.38\nLaplacianShot (ours)\nResNet-18\n72.11 ± 0.19\n82.31 ± 0.14\n78.98 ± 0.21\n86.39 ± 0.16\nQiao (Qiao et al., 2018)\nWRN\n59.60 ± 0.41\n73.74 ± 0.19\n-\n-\nLEO (Rusu et al., 2019)\nWRN\n61.76 ± 0.08\n77.59 ± 0.12\n66.33 ± 0.05\n81.44 ± 0.09\nProtoNet (Snell et al., 2017)\nWRN\n62.60 ± 0.20\n79.97 ± 0.14\n-\n-\nCC+rot (Gidaris et al., 2019)\nWRN\n62.93 ± 0.45\n79.87 ± 0.33\n70.53 ± 0.51\n84.98 ± 0.36\nMatchingNet (Vinyals et al., 2016)\nWRN\n64.03 ± 0.20\n76.32 ± 0.16\n-\n-\nFEAT (Ye et al., 2020)\nWRN\n65.10 ± 0.20\n81.11 ± 0.14\n70.41 ± 0.23\n84.38 ± 0.16\nTransductive tuning (Dhillon et al., 2020)\nWRN\n65.73 ± 0.68\n78.40 ± 0.52\n73.34 ± 0.71\n85.50 ± 0.50\nSimpleShot (Wang et al., 2019)\nWRN\n65.87± 0.20\n82.09 ± 0.14\n70.90 ± 0.22\n85.76 ± 0.15\nSIB (Hu et al., 2020)\nWRN\n70.0 ± 0.6\n79.2 ± 0.4\n-\n-\nBD-CSPN (Liu et al., 2019a)\nWRN\n70.31 ± 0.93\n81.89 ± 0.60\n78.74 ± 0.95\n86.92 ± 0.63\nLaplacianShot (ours)\nWRN\n74.86 ± 0.19\n84.13 ± 0.14\n80.18 ± 0.21\n87.56± 0.15\nSimpleShot (Wang et al., 2019)\nMobileNet\n61.55 ± 0.20\n77.70 ± 0.15\n69.50 ± 0.22\n84.91 ± 0.15\nLaplacianShot (ours)\nMobileNet\n70.27 ± 0.19\n80.10 ± 0.15\n79.13 ± 0.21\n86.75 ± 0.15\nSimpleShot (Wang et al., 2019)\nDenseNet\n65.77 ± 0.19\n82.23 ± 0.13\n71.20 ± 0.22\n86.33 ± 0.15\nLaplacianShot (ours)\nDenseNet\n75.57 ± 0.19\n84.72 ± 0.13\n80.30 ± 0.22\n87.93 ± 0.15\n(Vinyals et al., 2016) predicted the labels of the query sam-\nples xq as a linear combination of the support labels. The ex-\npression of bi\nq,c that we obtained in (8b), which stems from\nour bound optimizer and the concave relaxation of the Lapla-\ncian, also takes the form of a combination of labels at each it-\neration i in our model: bi\nq,c = P\np w(xq, xp)yi\np,c. However,\nthere are important differences with (Vinyals et al., 2016):\nFirst, the attention in our formulation is non-parametric as\nit considers only the feature relationships among the query\nsamples in Xq, not the support examples. Second, unlike\nour approach, the attention mechanism in (Vinyals et al.,\n2016) is employed during training for learning embedding\nfunction fθ with a meta-learning approach.\n3. Experiments\nIn this section, we describe our experimental setup. An\nimplementation of our LaplacianShot is publicly available6.\n6https://github.com/imtiazziko/LaplacianShot\n3.1. Datasets\nWe used ﬁve benchmarks for few-shot classiﬁcation:\nminiImageNet, tieredImageNet, CUB, cross-domain CUB\n(with base training on miniImageNet) and iNat.\nThe miniImageNet benchmark is a subset of the larger\nILSVRC-12 dataset (Russakovsky et al., 2015). It has a\ntotal of 60,000 color images with 100 classes, where each\nclass has 600 images of size 84 × 84, following (Vinyals\net al., 2016). We use the standard split of 64 base, 16 vali-\ndation and 20 test classes (Ravi & Larochelle, 2017; Wang\net al., 2019). The tieredImageNet benchmark (Ren et al.,\n2018) is also a subset of ILSVRC-12 dataset but with 608\nclasses instead. We follow standard splits with 351 base,\n97 validation and 160 test classes for the experiments. The\nimages are also resized to 84 × 84 pixels. CUB-200-2011\n(Wah et al., 2011) is a ﬁne-grained image classiﬁcation\ndataset. We follow (Chen et al., 2019) for few-shot classi-\nﬁcation on CUB, which splits into 100 base, 50 validation\n\n\nLaplacian Regularized Few-Shot Learning\nTable 2. Results for CUB and cross-domain results on miniImagenet →CUB.\nMethods\nNetwork\nCUB\nminiImagenet →CUB\n1-shot\n5-shot\n1-shot\n5-shot\nMatchingNet (Vinyals et al., 2016)\nResNet-18\n73.49\n84.45\n-\n53.07\nMAML (Finn et al., 2017)\nResNet-18\n68.42\n83.47\n-\n51.34\nProtoNet (Snell et al., 2017)\nResNet-18\n72.99\n86.64\n-\n62.02\nRelationNet (Sung et al., 2018)\nResNet-18\n68.58\n84.05\n-\n57.71\nChen (Chen et al., 2019)\nResNet-18\n67.02\n83.58\n-\n65.57\nSimpleShot (Wang et al., 2019)\nResNet-18\n70.28\n86.37\n48.56\n65.63\nLaplacianShot(ours)\nResNet-18\n80.96\n88.68\n55.46\n66.33\nTable 3. Average accuracy (in %) in iNat benchmark for SimpleShot (Wang et al., 2019) and the proposed LaplacianShot. The best results\nare reported in bold font. Note that, for iNat, we do not utilize the rectiﬁed prototypes. [The best reported result of (Wertheimer &\nHariharan, 2019) with ResNet50 is: Per Class: 46.04%, Mean: 51.25%.]\nMethods\nNetwork\nUN\nL2\nCL2\nPer Class\nMean\nPer Class\nMean\nPer Class\nMean\nSimpleShot\nResNet-18\n55.80\n58.56\n57.15\n59.56\n56.35\n58.63\nLaplacianShot\nResNet-18\n62.80\n66.40\n58.72\n61.14\n58.49\n60.81\nSimpleShot\nResNet-50\n58.45\n61.07\n59.68\n61.99\n58.83\n60.98\nLaplacianShot\nResNet-50\n65.96\n69.13\n61.40\n63.66\n61.08\n63.18\nSimpleShot\nWRN\n62.44\n65.08\n64.26\n66.25\n63.03\n65.17\nLaplacianShot\nWRN\n71.55\n74.97\n65.78\n67.82\n65.32\n67.43\nand 50 test classes for the experiments. The images are\nalso resized to 84 × 84 pixels, as in miniImageNet. The\niNat benchmark, introduced recently for few-shot classiﬁca-\ntion in (Wertheimer & Hariharan, 2019), contains images of\n1,135 animal species. It introduces a more challenging few-\nshot scenario, with different numbers of support examples\nper class, which simulates more realistic class-imbalance\nscenarios, and with semantically related classes that are\nnot easily separable. Following (Wertheimer & Hariharan,\n2019), the dataset is split into 908 base classes and 227 test\nclasses, with images of size 84 × 84.\n3.2. Evaluation Protocol\nIn the case of miniImageNet, CUB and tieredImageNet,\nwe evaluate 10,000 ﬁve-way 1-shot and ﬁve-way 5-shot\nclassiﬁcation tasks, randomly sampled from the test classes,\nfollowing standard few-shot evaluation settings (Wang et al.,\n2019; Rusu et al., 2019). This means that, for each of\nthe ﬁve-way few-shot tasks, C = 5 classes are randomly\nselected, with |Xc\ns| = 1 (1-shot) and |Xc\ns| = 5 (5-shot) ex-\namples selected per class, to serve as support set Xs. Query\nset Xq contains 15 images per class. Therefore, the evalua-\ntion is performed over N = 75 query images per task. The\naverage accuracy of these 10,000 few shot tasks are reported\nalong with the 95% conﬁdence interval. For the iNat bench-\nmark, the number of support examples |Xc\ns| per class varies.\nWe performed 227-way multi-shot evaluation, and report\nthe top-1 accuracy averaged over the test images per class\n(Per Class in Table 3), as well as the average over all test\nimages (Mean in Table 3), following the same procedure as\nin (Wertheimer & Hariharan, 2019; Wang et al., 2019).\n3.3. Network Models\nWe evaluate LaplacianShot on four different backbone net-\nwork models to learn feature extractor fθ:\nResNet-18/50 is based on the deep residual network archi-\ntecture (He et al., 2016), where the ﬁrst two down-sampling\nlayers are removed, setting the stride to 1 in the ﬁrst convolu-\ntional layer and removing the ﬁrst max-pool layer. The ﬁrst\nconvolutional layer is used with a kernel of size 3×3 instead\nof 7×7. ResNet-18 has 8 basic residual blocks, and ResNet-\n50 has 16 bottleneck blocks. For all the networks, the dimen-\nsion of the extracted features is 512. MobileNet (Howard\net al., 2017) was initially proposed as a light-weight con-\nvolutional network for mobile-vision applications. In our\nsetting, we remove the ﬁrst two down-sampling operations,\nwhich results in a feature embedding of size 1024. WRN\n(Zagoruyko & Komodakis, 2016) widens the residual blocks\nby adding more convolutional layers and feature planes. In\nour case, we used 28 convolutional layers, with a widen-\ning factor of 10 and an extracted-feature dimension of 640.\nFinally, we used the standard 121-layer DenseNet (Huang\net al., 2017), omitting the ﬁrst two down-sampling layers\nand setting the stride to 1. We changed the kernel size of\nthe ﬁrst convolutional layer to 3 × 3. The extracted feature\nvector is of dimension 1024.\n\n\nLaplacian Regularized Few-Shot Learning\nFigure 1. We tune regularization parameter λ over values ranging from 0.1 to 1.5. In the above plots, we show the impact of choosing λ\non both validation and test accuracies. The values of λ based on the best validation accuracies correspond to good accuracies in the test\nclasses. The results are shown for different networks on miniImageNet dataset, for both 1-shot (top row) and 5-shot (bottom row).\nFigure 2. Convergence of Algorithm 1: Bounds Bi(Y) vs. iteration numbers for features from different networks. Here, the plots are\nproduced by setting λ = 1.0, for a single 5-way 5 shot task from the miniImageNet test set.\n3.4. Implementation Details\nNetwork model training: We trained the network models\nusing the standard cross-entropy loss on the base classes,\nwith a label-smoothing (Szegedy et al., 2016) parameter set\nto 0.1. Note that the base training did not involve any meta-\nlearning or episodic-training strategy. We used the SGD\noptimizer to train the models, with mini-batch size set to 256\nfor all the networks, except for WRN and DenseNet, where\nwe used mini-batch sizes of 128 and 100, respectively. We\nused two 16GB P100 GPUs for network training with base\nclasses. For miniImageNet, CUB and tieredImageNet, we\nused early stopping by evaluating the the nearest-prototype\nclassiﬁcation accuracy on the validation classes, with L2\nnormalized features.\nPrototype estimation and feature transformation: Dur-\ning the inference on test classes, SimpleShot (Wang et al.,\n2019) performs the following feature transformations: L2\nnormalization, xq := xq/∥xq∥2 and CL2, which computes\nthe mean of the base class features ¯x =\n1\n|Xbase|\nP\nx∈Xbase x\nand centers the extracted features as xq := xq −¯x, which\nis followed by an L2 normalization. We report the results\nin Table 1 and 2 with CL2 normalized features. In Table 3\nfor the iNat dataset, we provide the results with both nor-\nmalized and unnormalized (UN) features for a comparative\nanalysis. We reproduced the results of SimpleShot with our\ntrained network models. In the 1-shot setting, prototype mc\nis just the support example xq ∈Xc\ns of class c, whereas in\nmulti-shot, mc is the simple mean of the support examples\nof class c. Another option is to use rectiﬁed prototypes, i.e.,\na weighted combination of features from both the support\nexamples in Xc\ns and query samples in Xc\nq, which are initially\n\n\nLaplacian Regularized Few-Shot Learning\npredicted as belonging to class c using Eq. (2):\n˜mc =\n1\n|Xcs| + |Xcq|\nX\nxp∈{Xcs ,Xcq }\nexp(cos(xp, mc))\nPC\nc=1 exp(cos(xp, mc))\nxp,\nwhere cos denotes the cosine similarity. And, for a given\nfew-shot task, we compute the cross-domain shift ∆as\nthe difference between the mean of features within the\nsupport set and the mean of features within the query set:\n∆=\n1\n|Xs|\nP\nxp∈Xs xp −\n1\n|Xq|\nP\nxq∈Xq xq. Then, we rectify\neach query point xp ∈Xq in the few-shot task as follows:\nxp = xp + ∆. This shift correction is similar to the pro-\ntotype rectiﬁcation in (Liu et al., 2019a). Note that our\nLaplacianShot model in Eq. (1) is agnostic to the way of\nestimating the prototypes: It can be used either with the\nstandard prototypes (mc) or with the rectiﬁed ones ( ˜mc).\nWe report the results of LaplacianShot with the rectiﬁed pro-\ntotypes in Table 1 and 2, for miniImagenet, tieredImagenet\nand CUB. We do not report the results with the rectiﬁed\nprototypes in Table 3 for iNat, as rectiﬁcation drastically\nworsen the performance.\nFor W, we used the k-nearest neighbor afﬁnities as follows:\nw(xq, xp) = 1 if xp is within the k nearest neighbor of\nxq, and w(xq, xp) = 0 otherwise. In our experiments, k is\nsimply chosen from three typical values (3, 5 or 10) tuned\nover 500 few-shot tasks from the base training classes (i.e.,\nwe did not use test data for choosing k). We used k = 3\nfor miniImageNet, CUB and tieredImageNet and k = 10\nfor iNat benchmark. Regularization parameter λ is chosen\nbased on the validation class accuracy for miniImageNet,\nCUB and tieredImageNet. This will be discussed in more\ndetails in section 3.6. For the iNat experiments, we simply\nﬁx λ = 1.0, as there is no validation set for this benchmark.\n3.5. Results\nWe evaluated LaplacianShot over ﬁve different bench-\nmarks, with different scenarios and difﬁculties: Generic\nimage classiﬁcation, ﬁne-grained image classiﬁcation, cross-\ndomain adaptation, and imbalanced class distributions.\nWe report the results of LaplacianShot for miniImageNet,\ntieredImageNet, CUB and iNat datasets, in Tables 1, 2 and\n3, along with comparisons with state-of-the-art methods.\nGeneric image classiﬁcation: Table 1 reports the results of\ngeneric image classiﬁcation for the standard miniImageNet\nand tieredImageNet few-shot benchmarks. We can clearly\nobserve that LaplacianShot outperforms state-of-the-art\nmethods by large margins, with gains that are consistent\nacross different settings and network models. It is worth\nmentioning that, for challenging scenarios, e.g., 1-shot with\nlow-capacity models, LaplacianShot outperforms complex\nmeta-learning methods by more than 9%. For instance,\ncompared to well-known MAML (Finn et al., 2017) and\nProtoNet (Snell et al., 2017), and to the recent MetaoptNet\n(Lee et al., 2019), LaplacianShot brings improvements of\nnearly 22%, 17%, and 9%, respectively, under the same\nevaluation conditions. Furthermore, it outperforms the very\nrecent transductive approaches in (Dhillon et al., 2020; Liu\net al., 2019a;b) by signiﬁcant margins. With better learned\nfeatures with WRN and DenseNet, LaplacianShot brings sig-\nniﬁcant performance boosts, yielding state-of-the art results\nin few-shot classiﬁcation, without meta-learning.\nFine-grained image classiﬁcation: Table 2 reports the re-\nsults of ﬁne-grained few-shot classiﬁcation on CUB, with\nResnet-18 network. LaplacianShot outperforms the best\nperforming method in this setting by a 7% margin.\nCross-domain (mini-ImageNet →CUB): We perform the\nvery interesting few-shot experiment, with a cross-domain\nscenario, following the setting in (Chen et al., 2019). We\nused the ResNet-18 model trained on the miniImagenet base\nclasses, while evaluation is performed on CUB few-shot\ntasks, with 50 test classes. Table 2 (rightmost column)\nreports the results. In this cross-domain setting, and consis-\ntently with the standard settings, LaplacianShot outperforms\ncomplex meta-learning methods by substantial margins.\nImbalanced class distribution: Table 3 reports the results\nfor the more challenging, class-imbalanced iNat benchmark,\nwith different numbers of support examples per class and,\nalso, with high visual similarities between the different\nclasses, making class separation difﬁcult. To our knowledge,\nonly (Wertheimer & Hariharan, 2019; Wang et al., 2019)\nreport performances on this benchmark, and SimpleShot\n(Wang et al., 2019) represents the state-of-the-art. We com-\npared with SimpleShot using unnormalized extracted fea-\ntures (UN), L2 and CL2 normalized features. Our Laplacian\nregularization yields signiﬁcant improvements, regardless\nof the network model and feature normalization. However,\nunlike SimpleShot, our method reaches its best performance\nwith the unnormalized features. Note that, for iNat, we\ndid not use the rectiﬁed prototypes. These results clearly\nhighlight the beneﬁt Laplacian regularization brings in chal-\nlenging class-imbalance scenarios.\n3.6. Ablation Study\nChoosing the Value of λ: In LaplacianShot, we need to\nchoose the value of regularization parameter λ, which con-\ntrols the trade-off between the nearest-prototype classiﬁer\nterm aq and Laplacian regularizer bi\nq. We tuned this param-\neter using the validation classes by sampling 500 few-shot\ntasks. LaplacianShot is used in each few-shot task with the\nfollowing values of λ: [0.1, 0.3, 0.5, 0.7, 0.8, 1.0, 1.2, 1.5].\nThe best λ corresponding to the best average 1-shot and\n5-shot accuracy over validation classes/data is selected for\ninference over the test classes/data. To examine experi-\nmentally whether the chosen values of λ based on the best\nvalidation accuracies correspond to good accuracies in the\n\n\nLaplacian Regularized Few-Shot Learning\nTable 4. Ablation study on the effect of each term corresponding to nearest prototype N(Y), Laplacian L(Y) and rectiﬁed prototype ˜mc.\nResults are reported with ResNet-18 network. Note that, the Laplacian regularization L(Y) improve the results consistently.\nmini-ImageNet\ntiered-ImageNet\nCUB\nN(Y)\nL(Y)\n˜mc\n1-shot\n5-shot\n1-shot\n5-shot\n1shot\n5-shot\n\u0013\n\u0017\n\u0017\n63.10\n79.92\n69.68\n84.56\n70.28\n86.37\n\u0013\n\u0013\n\u0017\n66.20\n80.75\n72.89\n85.25\n74.46\n86.86\n\u0013\n\u0017\n\u0013\n69.74\n82.01\n76.73\n85.74\n78.76\n88.55\n\u0013\n\u0013\n\u0013\n72.11\n82.31\n78.98\n86.39\n80.96\n88.68\nTable 5. Average inference time (in seconds) for the 5-shot tasks\nin miniImagenet dataset.\nMethods\ninference time\nSimpleShot (Wang et al., 2019)\n0.009\nTransductive tuning (Dhillon et al., 2020)\n20.7\nLaplacianShot (ours)\n0.012\ntest classes, we plotted both the validation and test class ac-\ncuracies vs. different values of λ for miniImageNet (Figure\n1). The results are intuitive, with a consistent trend in both\n1-shot and 5-shot settings. Particularly, for 1-shot tasks,\nλ = 0.7 provides the best results in both validation and test\naccuracies. In 5-shot tasks, the best test results are obtained\nmostly with λ = 0.1, while the best validation accuracies\nwere reached with higher values of λ. Nevertheless, we re-\nport the results of LaplacianShot with the values of λ chosen\nbased on the best validation accuracies.\nEffects of Laplacian regularization: We conducted an ab-\nlation study on the effect of each term in our model, i.e.,\nnearest-prototype classiﬁer N(Y) and Laplacian regular-\nizer L(Y). We also examined the effect of using prototype\nrectiﬁcation, i.e., ˜mc instead of mc. Table 4 reports the\nresults, using the ResNet-18 network. The ﬁrst row corre-\nsponds to the prediction of the nearest neighbor classiﬁer\n(λ = 0), and the second shows the effect of adding Lapla-\ncian regularization. In the 1-shot case, the latter boosts the\nperformances by at least 3%. Prototype rectiﬁcation (third\nand fourth rows) also boosts the performances. Again, in\nthis case, the improvement that the Laplacian term brings is\nsigniﬁcant, particularly in the 1-shot case (2 to 3%).\nConvergence of transductive LaplacianShot inference:\nThe proposed algorithm belongs to the family of bound op-\ntimizers or MM algorithms. In fact, the MM principle can\nbe viewed as a generalization of expectation-maximization\n(EM). Therefore, in general, MM algorithms inherit the\nmonotonicity and convergence properties of EM algorithms\n(Vaida, 2005), which are well-studied in the literature. In\nfact, Theorem 3 in (Vaida, 2005) states a simple condition\nfor convergence of the general MM procedure, which is al-\nmost always satisﬁed in practice: The surrogate function has\na unique global minimum. In Fig. 2, we plotted surrogates\nBi(Y), up to a constant, i.e., Eq. (7), as functions of the iter-\nation numbers, for different networks. One can see that the\nvalue of Bi(Y) decreases monotonically at each iteration,\nand converges, typically, within less than 15 iterations.\nInference time: We computed the average inference time\nrequired for each 5-shot task. Table 5 reports these infer-\nence times for miniImageNet with the WRN network. The\npurpose of this is to check whether there exist a signiﬁcant\ncomputational overhead added by our Laplacian-regularized\ntransductive inference, in comparison to inductive inference.\nNote that the computational complexity of the proposed\ninference is O(NkC) for a few-shot task, where k is the\nneighborhood size for afﬁnity matrix W. The inference\ntime per few-shot task for LaplacianShot is close to induc-\ntive SimpleShot run-time (LaplacianShot is only 1-order of\nmagnitude slower), and is 3-order-of-magnitude faster than\nthe transductive ﬁne-tuning in (Dhillon et al., 2020).\n4. Conclusion\nWithout meta-learning, we provide state-of-the-art results,\noutperforming signiﬁcantly a large number of sophisticated\nfew-shot learning methods, in all benchmarks. Our trans-\nductive inference is a simple constrained graph clustering\nof the query features. It can be used in conjunction with any\nbase-class training model, consistently yielding improve-\nments. Our results are in line with several recent baselines\n(Dhillon et al., 2020; Chen et al., 2019; Wang et al., 2019)\nthat reported competitive performances, without resorting\nto complex meta-learning strategies. This recent line of sim-\nple methods emphasizes the limitations of current few-shot\nbenchmarks, and questions the viability of a large body of\nconvoluted few-shot learning techniques in the recent lit-\nerature. As pointed out in Fig. 1 in (Dhillon et al., 2020),\nthe progress made by an abundant recent few-shot literature,\nmostly based on meta-learning, may be illusory. Classical\nand simple regularizers, such as the entropy in (Dhillon\net al., 2020) or our Laplacian term, well-established in semi-\nsupervised learning and clustering, achieve outstanding per-\nformances. We do not claim to hold the ultimate solution for\nfew-shot learning, but we believe that our model-agnostic\ntransductive inference should be used as a strong baseline\nfor future few-shot learning research.\n\n\nLaplacian Regularized Few-Shot Learning\nReferences\nBelkin, M., Niyogi, P., and Sindhwani, V. Manifold regular-\nization: A geometric framework for learning from labeled\nand unlabeled examples. Journal of Machine Learning\nResearch, 7:2399–2434, 2006.\nChen, W.-Y., Liu, Y.-C., Kira, Z., Wang, Y.-C. F., and Huang,\nJ.-B. A closer look at few-shot classiﬁcation. In Interna-\ntional Conference on Learning Representations (ICLR),\n2019.\nDhillon, G. S., Chaudhari, P., Ravichandran, A., and Soatto,\nS. A baseline for few-shot image classiﬁcation. In Inter-\nnational Conference on Learning Representations (ICLR),\n2020.\nFei-Fei, L., Fergus, R., and Perona, P. One-shot learning of\nobject categories. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 28:594–611, 2006.\nFinn, C., Abbeel, P., and Levine, S. Model-agnostic meta-\nlearning for fast adaptation of deep networks. In Interna-\ntional Conference on Machine Learning (ICML), 2017.\nGidaris, S. and Komodakis, N. Dynamic few-shot visual\nlearning without forgetting. In Conference on Computer\nVision and Pattern Recognition (CVPR), 2018.\nGidaris, S., Bursuc, A., Komodakis, N., P´erez, P., and\nCord, M. Boosting few-shot visual learning with self-\nsupervision. In International Conference on Computer\nVision (ICCV), 2019.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In Conference on Computer\nVision and Pattern Recognition (CVPR), 2016.\nHou, R., Chang, H., Bingpeng, M., Shan, S., and Chen, X.\nCross attention network for few-shot classiﬁcation. In\nNeural Information Processing Systems (NeurIPS), 2019.\nHoward, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang,\nW., Weyand, T., Andreetto, M., and Adam, H. Mobilenets:\nEfﬁcient convolutional neural networks for mobile vision\napplications. Preprint arXiv:1704.04861, 2017.\nHu, S. X., Moreno, P. G., Xiao, Y., Shen, X., Obozinski,\nG., Lawrence, N. D., and Damianou, A. Empirical bayes\ntransductive meta-learning with synthetic gradients. In\nInternational Conference on Learning Representations\n(ICLR), 2020.\nHuang, G., Liu, Z., Van Der Maaten, L., and Weinberger,\nK. Q. Densely connected convolutional networks. In\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2017.\nJiang, X., Havaei, M., Varno, F., Chartrand, G., Chapa-\ndos, N., and Matwin, S. Learning to learn with condi-\ntional class dependencies. In International Conference\non Learning Representations (ICLR), 2019.\nKim, J., Kim, T., Kim, S., and Yoo, C. D. Edge-labeling\ngraph neural network for few-shot learning.\nIn Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), 2019.\nLange, K., Hunter, D. R., and Yang, I. Optimization transfer\nusing surrogate objective functions. Journal of computa-\ntional and graphical statistics, 9(1):1–20, 2000.\nLee, K., Maji, S., Ravichandran, A., and Soatto, S. Meta-\nlearning with differentiable convex optimization. In Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), 2019.\nLiu, J., Song, L., and Qin, Y. Prototype rectiﬁcation for\nfew-shot learning. Preprint arXiv:1911.10713, 2019a.\nLiu, Y., Lee, J., Park, M., Kim, S., Yang, E., Hwang, S., and\nYang, Y. Learning to propagate labels: Transductive prop-\nagation network for few-shot learning. In International\nConference on Learning Representations (ICLR), 2019b.\nMiller, E., Matsakis, N., and Viola, P. Learning from one\nexample through shared densities on transforms. Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), 2000.\nMishra, N., Rohaninejad, M., Chen, X., and Abbeel, P. A\nsimple neural attentive meta-learner. In International\nConference on Learning Representations (ICLR), 2018.\nMunkhdalai, T., Yuan, X., Mehri, S., and Trischler, A. Rapid\nadaptation with conditionally shifted neurons. In Interna-\ntional Conference on Machine Learning (ICML), 2018.\nNarasimhan, M. and Bilmes, J. A submodular-supermodular\nprocedure with applications to discriminative structure\nlearning. In Conference on Uncertainty in Artiﬁcial Intel-\nligence (UAI), 2005.\nOreshkin, B., L´opez, P. R., and Lacoste, A. Tadam: Task de-\npendent adaptive metric for improved few-shot learning.\nIn Neural Information Processing Systems (NeurIPS),\n2018.\nQiao, L., Shi, Y., Li, J., Wang, Y., Huang, T., and Tian,\nY. Transductive episodic-wise adaptive metric for few-\nshot learning. In International Conference on Computer\nVision (ICCV), 2019.\nQiao, S., Liu, C., Shen, W., and Yuille, A. L. Few-shot\nimage recognition by predicting parameters from activa-\ntions. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2018.\n\n\nLaplacian Regularized Few-Shot Learning\nRavi, S. and Larochelle, H. Optimization as a model for few-\nshot learning. In International Conference on Learning\nRepresentations (ICLR), 2017.\nRen, M., Triantaﬁllou, E., Ravi, S., Snell, J., Swersky, K.,\nTenenbaum, J. B., Larochelle, H., and Zemel, R. S. Meta-\nlearning for semi-supervised few-shot classiﬁcation. In\nInternational Conference on Learning Representations\nICLR, 2018.\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,\nMa, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,\nM., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale\nVisual Recognition Challenge. International Journal of\nComputer Vision (IJCV), 115(3):211–252, 2015.\nRusu, A. A., Rao, D., Sygnowski, J., Vinyals, O., Pascanu,\nR., Osindero, S., and Hadsell, R. Meta-learning with\nlatent embedding optimization. In International Confer-\nence on Learning Representations (ICLR), 2019.\nShi, J. and Malik, J. Normalized cuts and image segmenta-\ntion. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 22(8):888–905, 2000.\nSnell, J., Swersky, K., and Zemel, R. Prototypical networks\nfor few-shot learning. In Neural Information Processing\nSystems (NeurIPS), 2017.\nSun, Q., Liu, Y., Chua, T., and Schiele, B. Meta-transfer\nlearning for few-shot learning. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR), June 2019.\nSung, F., Yang, Y., Zhang, L., Xiang, T., Torr, P. H., and\nHospedales, T. M. Learning to compare: Relation net-\nwork for few-shot learning. In Conference on Computer\nVision and Pattern Recognition (CVPR), 2018.\nSzegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna,\nZ. Rethinking the inception architecture for computer\nvision. In Conference on Computer Vision and Pattern\nRecognition, 2016.\nTian, F., Gao, B., Cui, Q., Chen, E., and Liu, T.-Y. Learn-\ning deep representations for graph clustering. In AAAI\nConference on Artiﬁcial Intelligence, 2014.\nVaida, F. Parameter convergence for em and mm algorithms.\nStatistica Sinica, 15:831–840, 2005.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, u., and Polosukhin, I. Attention\nis all you need. In Neural Information Processing Systems\n(NeurIPS), 2017.\nVinyals, O., Blundell, C., Lillicrap, T. P., Kavukcuoglu, K.,\nand Wierstra, D. Matching networks for one shot learning.\nIn Neural Information Processing Systems (NeurIPS),\n2016.\nVon Luxburg, U. A tutorial on spectral clustering. Statistics\nand computing, 17(4):395–416, 2007.\nWah, C., Branson, S., Welinder, P., Perona, P., and Belongie,\nS. The caltech-ucsd birds-200-2011 dataset. 2011.\nWang, W. and Carreira-Perpin´an, M. A.\nThe lapla-\ncian k-modes algorithm for clustering.\nPreprint\narXiv:1406.3895, 2014.\nWang,\nY.,\nChao,\nW.-L.,\nWeinberger,\nK. Q.,\nand\nvan der Maaten, L.\nSimpleshot: Revisiting nearest-\nneighbor classiﬁcation for few-shot learning. Preprint\narXiv:1911.04623, 2019.\nWertheimer, D. and Hariharan, B. Few-shot learning with\nlocalization in realistic settings. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2019.\nWeston, J., Ratle, F., Mobahi, H., and Collobert, R. Deep\nlearning via semi-supervised embedding. In Neural net-\nworks: Tricks of the trade, pp. 639–655. Springer, 2012.\nYe, H.-J., Hu, H., Zhan, D.-C., and Sha, F. Few-shot learning\nvia embedding adaptation with set-to-set functions. In\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2020.\nYuan, J., Yin, K., Bai, Y., Feng, X., and Tai, X. Bregman-\nproximal augmented lagrangian approach to multiphase\nimage segmentation. In Scale Space and Variational\nMethods in Computer Vision (SSVM), 2017.\nYuille, A. L. and Rangarajan, A. The concave-convex proce-\ndure (CCCP). In Neural Information Processing Systems\n(NeurIPS), 2001.\nZagoruyko, S. and Komodakis, N. Wide residual networks.\nIn British Machine Vision Conference (BMVC), 2016.\nZhang, J., Zhao, C., Ni, B., Xu, M., and Yang, X. Varia-\ntional few-shot learning. In International Conference on\nComputer Vision (ICCV), 2019.\nZhang, Z., Kwok, J. T., and Yeung, D.-Y. Surrogate max-\nimization/minimization algorithms and extensions. Ma-\nchine Learning, 69:1–33, 2007.\nZhou, D., Bousquet, O., Lal, T. N., Weston, J., and\nSch¨olkopf, B. Learning with local and global consistency.\nIn Neural Information Processing Systems (NeurIPS),\n2004.\nZiko, I., Granger, E., and Ben Ayed, I. Scalable lapla-\ncian k-modes. In Neural Information Processing Systems\n(NeurIPS), 2018.\n"
}