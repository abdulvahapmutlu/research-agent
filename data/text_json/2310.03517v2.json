{
  "filename": "2310.03517v2.pdf",
  "num_pages": 14,
  "pages": [
    "Highlights\nPrototypeFormer: Learning to Explore Prototype Relationships for Few-shot Image Classi-\nfication\nMeijuan Su, Feihong He, Fanzhang Li\nâ€¢ Prototype Extraction Module. We introduce a novel and efficient transformer-based architecture specifically\ndesigned for few-shot learning. This module, termed the Prototype Extraction Module, leverages the self-\nattention mechanism of transformers to capture intricate relationships among intra-class samples. By treating\nclass prototypes as learnable tokens and integrating them with support set embeddings, the module extracts\nhighly discriminative prototype representations. Unlike traditional methods that rely on global average pooling\nor local descriptors, our approach provides a comprehensive global perspective, enabling the model to better\ncapture task-specific feature relationships. This module is simple yet powerful, significantly enhancing the\nmodelâ€™s ability to generalize in few-shot scenarios.\nâ€¢ Prototype Contrastive Loss. We form sub-prototypes by employing linear combinations of the support set.\nSubsequently, we optimize the model using the prototype contrastive loss based on these sub-prototypes to\nobtain more robust prototype representations. This approach ensures that similar class embeddings are pulled\ncloser together, while dissimilar ones are pushed apart, leading to more robust and generalizable prototype\nrepresentations. The contrastive loss is particularly effective in few-shot settings, where limited data makes\ntraditional methods prone to overfitting.\nâ€¢ Achieving State-of-the-Art Performance. We extensively evaluate our method on multiple widely used few-\nshot learning benchmarks. Our experiments demonstrate that PrototypeFormer consistently outperforms existing\nstate-of-the-art methods across these datasets. Notably, on the miniImageNet dataset, our method achieves\nremarkable accuracy improvements of 0.57% and 6.84% for 5-way 5-shot and 5-way 1-shot tasks, respectively.\nThese results highlight the effectiveness of our approach in addressing the challenges of few-shot learning,\nparticularly in scenarios with limited labeled data. The success of our method is further validated by its strong\nperformance on fine-grained classification tasks, such as those in the CUB-200 dataset.\narXiv:2310.03517v2  [cs.CV]  17 Feb 2025\n",
    "PrototypeFormer: Learning to Explore Prototype Relationships for\nFew-shot Image Classificationâ‹†\nMeijuan Sua, Feihong Heb and Fanzhang Li a\naSchool of Computer Science and Technology, Soochow University, 215000, Suzhou, China\nbSchool of Cyberspace Security, Sun Yat-sen University, 518107, Shenzhen, China\nA R T I C L E I N F O\nKeywords:\nfew-shot learning\nmetric learning\ntransformer\ncontrastive loss\nA B S T R A C T\nFew-shot image classification has received considerable attention for overcoming the challenge\nof limited classification performance with limited samples in novel classes. Most existing works\nemploy sophisticated learning strategies and feature learning modules to alleviate this challenge.\nIn this paper, we propose a novel method called PrototypeFormer, exploring the relationships\namong category prototypes in the few-shot scenario. Specifically, we utilize a transformer\narchitecture to build a prototype extraction module, aiming to extract class representations that\nare more discriminative for few-shot classification. Besides, during the model training process,\nwe propose a contrastive learning-based optimization approach to optimize prototype features in\nfew-shot learning scenarios. Despite its simplicity, our method performs remarkably well, with\nno bells and whistles. We have experimented with our approach on several popular few-shot\nimage classification benchmark datasets, which shows that our method outperforms all current\nstate-of-the-art methods. In particular, our method achieves 97.07% and 90.88% on 5-way 5-\nshot and 5-way 1-shot tasks of miniImageNet, which surpasses the state-of-the-art results with\naccuracy of 0.57% and 6.84%, respectively. The code will be released later.\n1. Introduction\nNeural networks have been remarkably successful in large-scale image classification. However, the domain of\nfew-shot image classification, where models must rapidly adapt to new data distributions with limited labeled samples\n(e.g., five or one sample for each class), remains a challenge. As a result of its promising applications in diverse fields\nsuch as medical image analysis and robotics, few-shot learning [1] has captivated the attention of the computer vision\nand machine learning community.\nRecent few-shot learning approaches mainly improve the generalization by augmenting the samples/features\nor facilitating feature representation with novel neural modules. A multitude of methods [2â€“6] utilizes generative\nmodels to generate new samples or augment feature space, aiming to approximate the actual distribution. Devising\nsophisticated feature representation modules is also a meaningful way to improve the model performance on low-\nshot categories. Specifically, CAN [7] leverages cross-attention mechanisms to acquire enriched sample embeddings\nwith enhanced class-specific features in a transductive way, while DN4 [8], DMN4 [9], and MCL [10] adopt\nlocal feature representations instead of global representations to obtain more discriminative feature representations.\nFollowing the line of feature representation learning approaches, we introduce a prototype extraction module to\nenhance the prototype embeddings. Contrary to earlier feature representation methodologies, our study delves into\nthe intricate interconnections both within each class and across the entire task to derive more discriminative prototype\nrepresentations.\nLearning prototype embedding [11, 12] is useful for few-shot classification. ProtoNet [11] introduces a method-\nology employing prototype points to encapsulate the feature embeddings of entire categories, and [12] proposes to\nenhance the notion of prototype points. However, they significantly ignore the prototype relationships for learning\nrobust class features. In this paper, we delve into the interconnections between prototype points, considering both\nintra-class and inter-class relationships. We first introduce a novel prototype extraction module to learn the relationship\nof intra-class samples through the self-attention of sub-prototypes. This module excels at obtaining a comprehensive\nâ‹†\nâˆ—Corresponding author\nORCID(s):\n: Preprint submitted to Elsevier\nPage 1 of 13\n",
    "Same Class\nÂ Similar Feature\nTask-specific Features\nDifferent Classes\nDiscriminative Features\nFigure 1: Samples from different categories exhibit both shared features and distinctive features. For example, the red\nrectangle indicates the similarity features among different categories, while the purple rectangle represents dissimilar features\nacross different categories.\nglobal perspective, enabling the extraction of robust class features based on relationships among categories throughout\nthe entire task.\nTo further fortify the robustness of class features in few-shot scenarios, we introduce prototype contrastive loss, a\nnovel contrastive loss designed explicitly to capture interactions among inter-class prototypes. One important concept\nin our approach is sub-prototypes, representing the average features of subsets of samples within each category. By\nemploying these sub-prototypes within a contrastive learning framework, we aim to cultivate more discriminative\nrepresentations. Specifically, the contrastive learning strategy ensures that similar class embeddings are drawn closer in\nthe feature space, while dissimilar ones are pushed apart, thus enhancing the discriminative power of our representative\nprototypes.\nMoreover, some works [13, 14] have demonstrated the impressive feature extraction capabilities of the CLIP pre-\ntrained model in few-shot learning. As a result, we integrate CLIP into our approach, undertaking only a limited amount\nof parameter training. We conclude our contribution as follows:\nâ€¢ Prototype Extraction Module. We introduce a novel and simple transformer-based architecture for few-shot\nlearning, employing a learnable prototype extraction module to extract prototype representations.\nâ€¢ Prototype Contrastive Loss. We form sub-prototypes by employing linear combinations of the support set.\nSubsequently, we optimize the model using the prototype contrastive loss based on these sub-prototypes to obtain\nmore robust prototype representations.\nâ€¢ Achieving State-of-the-Art Performance. We evaluate our method on multiple publicly few-shot benchmark\ndatasets, and the results demonstrate that the proposed method in this paper outperforms state-of-the-art few-shot\nlearning methods across these datasets, achieving a remarkable improvement of up to 6.84%.\n2. Related Work\n2.1. Few-shot Learning\nThe rapid development of deep neural networks in recent years has primarily benefited from large-scale labeled\ndatasets. However, the high cost of data collection and manual labeling has brought few-shot learning to the forefront\nof widespread interest. Few-shot learning is usually classified into optimization-based and metric-based methods. The\nmain idea of metric-based methods is to define specific metrics to classify samples in a way similar to the nearest\nneighbor algorithm. The Siamese Network[15] employs shared feature extractors to derive feature representations\nfrom both support sets and query sets. Subsequently, it computes classification similarity individually for each pair of\nsupport set and query set. Furthermore, the Siamese Network effectively distinguishes between different categories by\n: Preprint submitted to Elsevier\nPage 2 of 13\n",
    "C1K\nLoss\n=\n......\nSupport Set\nQuery\nCLIP\n......\n......\nPrototype\nExtraction\nmodule\n......\n......\n...\n......\n......\n...\nNegative Pair\nPositive Pair\nLPrototype\nLClassifier\n+\n...\nSub-prototype\nPrototype token\nOverview Diagram of PrototypeFormer\nFigure 2: This figure presents the overall process flowchart of the method proposed in this paper. We linearly combine\nthe support set and obtain sub-prototypes through the prototype extraction module. The sub-prototypes are utilized for\ncomputing the prototype contrastive loss ğ¿ğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’, while the prototype is employed for calculating the classification loss\nğ¿ğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ğ‘–ğ‘’ğ‘Ÿ. We sum the ğ¿ğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’and ğ¿ğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ğ‘–ğ‘’ğ‘Ÿto obtain the final optimization objective.\ncomparing deep features of the support and query sets, maintaining high classification accuracy even when differences\nbetween categories are subtle. The Prototypical Network [11] computes prototype points for each class of samples, and\nthe query samples are categorized by calculating the L2 distance to each prototype point. In Relation Network [16],\nthe incorporation of learnable nonlinear classifiers for sample classification is done innovatively. CAN [7] has\nimproved model performance by computing cross-attention on samples to enhance the networkâ€™s focus on classification\ntargets. Also, to reduce sample background interference, local descriptors that do not contain classification targets\nare eliminated in DN4 [8] and DMN4 [9] by comparing the similarity between local descriptors. COSOC [17],\nas a similar endeavor, seeks to enhance classification performance by distinguishing between classification targets\nand background elements. HCTransformers [18] propose a hierarchical cascading transformer architecture, aiming\nto address the overfitting challenges faced by large-scale models in few-shot learning. Meanwhile, FewTURE [19]\nsimilarly employs transformer architecture to extract key features from the main subjects within images. In the realm\nof generalized few-shot learning, a substantial body of work [13, 20] has already leveraged pre-trained models to\nenhance the efficacy of few-shot learning. In our research, we have also incorporated the pre-trained CLIP [21] model\nto enhance the feature extraction capabilities of our model. The critical distinction, however, lies in the fact that our\nmodel is trained using a meta-learning approach.\n2.2. Sample Relation\nThere exist diverse sample relationships among different class samples, and currently, most models are built\nupon the foundation of establishing these sample relationships. Numerous studies aim for models to achieve strong\ngeneralization performance across various class sample relationships, thereby minimizing vicinal risk. CAN [7] and\nOLTR [22] incorporate sample-specific relationships within the shared context by leveraging the correlations among\nindividual samples. IEM [23] analyzes local correlations among samples and performs memory storage updates for\nthese correlations. IRM [24] achieves a reduced vicinal risk by exploring the correlation between sample invariant\nfeatures and spurious features. In cross-domain tasks, [25] explores the transferability of sample relationships across\ndifferent domains by discarding specific sample relationships. Similar to [25], [26] explores domain-invariant and\nclass-invariant relationships by employing the deep adversarial disentangled autoencoder to achieve cross-domain\nclassification tasks. BatchFormer [27] has achieved significant improvements across various data scarcity tasks by\nimplicitly exploring the relationships among mini-batch samples during training. In mixup [3], samples are linearly\n: Preprint submitted to Elsevier\nPage 3 of 13\n",
    "interpolated to capture the class-invariant relationships between samples. In our work, we perform linear combinations\nof samples to explore task-relevant relationships among them.\n2.3. Contrative Learning\nContrastive learning has achieved significant success in recent years. InstDisc [28] proposes the utilization of\ninstance discrimination tasks as an alternative to class-based discrimination tasks within the framework of unsupervised\nlearning. MOCO [29] achieves favorable transferability to downstream tasks through the strategy of constructing a\ndynamic dictionary and performing momentum-based updates. Contrastive learning has exhibited its generality and\nflexibility in time series tasks, encompassing domains like audio and textual data. An abundance of work [21, 29, 30]\nhas demonstrated the positive impact of contrastive learning in both unsupervised learning and generalization research\nwithin the realm of computer vision. The objective of contrastive learning is to bring together samples of the same\nclass while separating those from different classes, thus constructing suitable patterns for sample feature extraction. In\nepisodic training, we utilize contrastive learning methods to extract class relationships within the task, enhancing the\nclassification performance for few-shot learning.\n3. Method\nIn this section, we first describe the problem definition related to few-shot learning. Subsequently, an exposition of\nour proposed methodology is presented. Conclusively, we delve into a comprehensive discussion on the two important\ncomponents of our method: Prototype Extraction Module and Prototype Contrastive Loss.\n3.1. Problem Formulation\nEpisodic training differs from the deep neural networks training approach. In the traditional training of deep neural\nnetworks, we usually train the neural network on a sample-by-sample basis. In episodic training, we typically train the\nneural network on a task-by-task basis. The episodic training mechanism [31] has been demonstrated to facilitate the\nlearning of transferable knowledge across classes.\nIn few-shot learning, we usually divide the dataset into training, validation, and test sets. The training set, validation\nset, and test set have no overlapping classes. Therefore, we refer to the classes in the training set as seen classes, while\nthe classes in the validation set and test set are termed unseen classes. During the training phase, we randomly sample\nfrom the training set to create the support set and the query set. We use ğ‘†to represent the support set and ğ‘„to define\nthe query set. In the support set ğ‘†, there are ğ‘classes, and each class contains ğ¾samples. We treat the query set ğ‘„as\nunlabeled samples and perform classification on the unlabeled samples in ğ‘„using the labeled samples in the support\nset ğ‘†, which contains ğ‘classes, each with ğ¾samples. During the testing phase, we follow the same procedure and\ndivide the test set into a support set and a query set, similar to what we did during the training phase. This allows us\nto evaluate the few-shot learning performance of the model on unseen classes in a manner consistent with the training\nprocess. We typically refer to tasks that satisfy the above settings as N-way K-shot tasks. In our work, we train and\nevaluate the model using the aforementioned problem formulation.\n3.2. Overview\nWe linearly combine the support set and apply non-linear mapping through the prototype extraction module.\nFurthermore, we optimize the prototype extraction module using contrastive learning strategies to attain improved\nprototype representations. As illustrated in Figure 2, we process both the support set and query samples through a\nfrozen CLIP feature extraction network to obtain image embeddings. Subsequently, we perform linear combinations\non the support set samples to generate ğ¶1\nğ¾sub-support sets. Simultaneously, a prototype token is added to each support\nset and sub-support set, derived by computing the average of the respective embedding collection. Individually, each\nsupport set and sub-support set is fed into the prototype extraction module to obtain encoded prototypes and sub-\nprototypes. We retain the prototypes and sub-prototypes while discarding the sample embeddings from the support\nsets. We compute ğ¿ğ‘ƒğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’using the retained sub-prototypes through contrastive loss, while ğ¿ğ¶ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ğ‘–ğ‘’ğ‘Ÿis obtained\nby calculating the embeddings of query samples and prototypes. Finally, we sum up ğ¿ğ‘ƒğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’and ğ¿ğ¶ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ğ‘–ğ‘’ğ‘Ÿto create\nthe ultimate optimization objective.\n: Preprint submitted to Elsevier\nPage 4 of 13\n",
    "Support Images\nÂ Embedding\nClass Token\nPrototype Extraction module\nEmbedded\nImage\nNorm\nMulti-Head\nAttention\nNorm\nMLP\nEncoder Block\nPrototype Extraction\nmodule\nClass Feature\nEmbedding\nLÃ—\nFigure 3: The prototype extraction module adopts the transformer structure [32], taking the prototype token and\nembeddings of same-class images from the support set as inputs to obtain the prototype and sub-prototype for that\nclass.\n3.3. Prototype Extraction Module\nIn this section, we will provide a comprehensive exposition of our proposed prototype extraction module.\nAdditionally, we will conduct a comparative analysis between our method and existing class feature extraction\napproaches found in the paper.\nFirst we introduce the prototype representation, the earliest class feature representation to appear in few-shot\nlearning. In the N-way K-shot task, we assume the existence of a class C, and in the support set ğ‘†, there exists a\nsubset ğ‘†ğ¶= {ğ‘¥1, ğ‘¥2, â€¦ ğ‘¥ğ¾âˆ£ğ‘¦= ğ¶}. We refer to the feature extraction network as ğ‘“. In that case, we can express the\nclass feature representation in the prototypical networks [11] as follows:\nğ‘ƒğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’(ğ¶) = 1\nğ¾\nğ¾\nâˆ‘\nğ‘–=1\nğ‘“(ğ‘¥ğ‘–\n) , ğ‘¥ğ‘–âŠ‚ğ‘†ğ¶\n(1)\nThe method of prototype points provides a simple and effective way to express class features. Absolutely, the global\naverage pooling layer used in the feature extraction network can introduce noise into the prototype points, causing\nthem to deviate from their true representation and leading to bias. To address this issue, DN4 [8] and DMN4 [9]\nremove the global average pooling layer from the feature extraction network. They employ local descriptors to replace\nthe global feature representation of images and utilize a discriminative nearest neighbor algorithm to obtain the most\nrepresentative local descriptors in the images as the feature representation for samples.\nHowever, we believe that the image background has a certain influence on the image classification performance and\nalso provides some category-related contextual features. Therefore, we propose a novel class feature extraction module\nreferred to as prototype extraction module to replace the current few-shot class feature representation. In ViT [33],\nthe image is divided into patches, and transformer [32] is utilized to compute the correlations between these patches,\nresulting in the overall feature representation of the entire image. Inspired by ViT, we simply treat the image as a set\nof patches input to the transformer, thereby obtaining the feature representation for the entire class. The fundamental\narchitecture of prototype extraction module is illustrated in Figure 3. We use ğœ™to represent the prototype extraction\nmodule, and we can express it in the following form:\nğ‘ƒğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’(ğ¶) = ğœ™(ğ‘¥ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›, ğ‘“(ğ‘¥1\n) , ğ‘“(ğ‘¥2\n) , â€¦ ğ‘“(ğ‘¥ğ¾\n)) , ğ‘¥ğ‘–âŠ‚ğ‘†ğ¶\n(2)\nIn the formula, ğ‘¥ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›represents the prototype token for that class, and it can be expressed as:\nğ‘¥ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›= 1\nğ¾\nğ¾\nâˆ‘\nğ‘–=1\nğ‘“(ğ‘¥ğ‘–\n) , ğ‘¥ğ‘–âŠ‚ğ‘†ğ¶\n(3)\nFinally, we use a simple metric learning classification method to classify the query samples. Specifically, we calculate\nthe distance between the embeddings of the query samples and the prototype points in the feature space to measure the\n: Preprint submitted to Elsevier\nPage 5 of 13\n",
    "similarity between the query samples and each class. This distance metric is used for classification, where the query\nsample is assigned to the class with the closest feature embedding in the feature space. This classification approach\ncan be formalized with the following formula:\nğ‘ğ‘Ÿğ‘”ğ‘šğ‘–ğ‘›ğ‘âŠ‚ğ¶ğ¿2\n(ğ‘¥ğ‘ğ‘¢ğ‘’ğ‘Ÿğ‘¦, ğ‘ƒğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’(ğ‘)\n)\n(4)\nThe classification loss is optimized using the cross-entropy loss, and the formula for the classification loss is as follows:\nğ¿ğ‘œğ‘ ğ‘ ğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ğ‘¦= âˆ’\nğ‘\nâˆ‘\nğ‘=1\nğ‘¦ğ‘ğ‘™ğ‘œğ‘”\n(\nğ‘’âˆ’ğ¿2\n(ğ‘¥ğ‘ğ‘¢ğ‘’ğ‘Ÿğ‘¦,ğ‘ƒğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’(ğ‘)\n)\nâˆ‘ğ‘\nğ‘–=1 ğ‘’âˆ’ğ¿2\n(ğ‘¥ğ‘ğ‘¢ğ‘’ğ‘Ÿğ‘¦,ğ‘ƒğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’(ğ‘–)\n)\n)\n(5)\nThe ğ‘¦ğ‘is the one-hot encoding of the true class label for the sample.\n3.4. Prototype Contrastive Loss\nTo enhance the generalization capability of the prototype extraction module, we drew inspiration from contrastive\nlearning and proposed prototype contrastive loss. The contrastive loss was first introduced by\n[34] and laid the\nfoundation for subsequent highly successful contrastive learning [29, 30]. The main idea of the contrastive loss is\nto construct positive and negative sample pairs, where positive pairs are brought closer together in the feature space,\nwhile negative pairs are pushed further apart.\nIn few-shot learning, by extracting K-1 samples from the same class in the support set ğ‘†, we can obtain ğ¾different\nsub-support set of samples ğ‘†ğ‘ğ‘–= {ğ‘¥ğ‘1, â€¦ , ğ‘¥ğ‘ğ‘–âˆ’1, ğ‘¥ğ‘ğ‘–+1 â€¦ , ğ‘¥ğ‘ğ¾\n} , ğ‘–= 1, 2 â€¦ ğ¾, ğ‘âŠ‚ğ¶. Then, we pass each of these\nK sub-support sets constructed from the same class samples through the prototype extraction module to obtain K sub-\nprototypes for that class. We use the K sub-prototypes obtained from the same-class support set samples as positive\npairs. At the same time, we use the sub-prototypes obtained from different-class sub-support sets as negative pairs. We\nrepresent the constructed positive sample pairs as follows:\nğ‘ƒğ‘œğ‘ ğ‘= {ğ‘ğ‘1, ğ‘ğ‘2, â€¦ ğ‘ğ‘ğ¾\n} , ğ¶= 1, 2 â€¦ ğ‘\n(6)\nThus, we can obtain the prototype contrastive loss using the constructed positive and negative pairs as follows:\nğ¿ğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’= ğ‘’ğ‘¥ğ‘\n(\n1\nğ‘â‹…\nâˆ‘ğ¾\nğ‘–,ğ‘—=1 ğ¿2\n(ğ‘ğ‘ğ‘–, ğ‘ğ‘ğ‘—\n) + ğ¼\nâˆ‘\nğ‘šâ‰ ğ‘›\nâˆ‘ğ¾\nğ‘–,ğ‘—=1 ğ¿2\n(ğ‘ğ‘š,ğ‘–, ğ‘ğ‘›,ğ‘—\n) + ğ¼\n)\n(7)\nBecause when ğ¾= 1, the support set contains only one sample per class, leading to âˆ‘ğ¾\nğ‘–,ğ‘—=1 ğ¿2\n(ğ‘ğ‘ğ‘–, ğ‘ğ‘ğ‘—\n) = 0. To\navoid this situation, we add the identity element I to prevent it from happening. The overall loss of the model during\nthe training phase is as follows:\nğ¿ğ‘œğ‘ ğ‘ = ğ¿ğ‘œğ‘ ğ‘ ğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ğ‘–ğ‘’ğ‘Ÿ+ ğ¿ğ‘œğ‘ ğ‘ ğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’\n(8)\nFinally, we present the pseudocode for the training process of PrototypeFormer in Algorithm 1.\n4. Experiments\nIn this section, we will evaluate the proposed method on multiple few-shot benchmark datasets and compare it with\nstate-of-the-art methods. Additionally, we will conduct ablation experiments and visualization experiments to further\nanalyze and validate the effectiveness of our proposed approach.\n4.1. Datasets\nminiImageNet [31] is a subset of the larger ImageNet dataset and is widely used in few-shot learning research. It\nconsists of 100 classes, with each class containing 600 images, resulting in a total of 60,000 images. The dataset is\ndivided into 64 classes for the training set, 16 classes for the validation set, and 20 classes for the test set.\ntieredImagenet is a larger subset of the ImageNet dataset compared to miniImagenet. The dataset consists of 608\nclasses with a total of 779,165 images. For few-shot learning, it is divided into three subsets, with 351 classes used for\nthe training set, 97 classes for the validation set, and 160 classes for the testing set.\nCaltech-UCSD Birds-200-2011 [47], also known as CUB, is the benchmark image dataset for current fine-grained\nclassification and recognition research. The dataset contains 11,788 bird images, encompassing 200 subclasses of bird\nspecies. We split it into 100, 50, and 50 classes for training, validation, and testing, respectively.\n: Preprint submitted to Elsevier\nPage 6 of 13\n",
    "Algorithm 1 Training Process of PrototypeFormer\n1: Input:\n2:\nSupport set ğ‘†\n=\n{ğ‘†1, ğ‘†2, â€¦ , ğ‘†ğ‘}, where ğ‘†ğ‘\n=\n{ğ‘¥ğ‘1, ğ‘¥ğ‘2, â€¦ , ğ‘¥ğ‘ğ¾} for class ğ‘. Query set ğ‘„\n=\n{ğ‘¥ğ‘1, ğ‘¥ğ‘2, â€¦ , ğ‘¥ğ‘ğ‘€}. Pre-trained CLIP feature extractor ğ‘“(frozen). Prototype extraction module ğœ™(Transformer-\nbased). Number of classes ğ‘, number of shots ğ¾.\n3: Output:\n4:\nClassification results for query set ğ‘„.\n5: Step 1: Extract features for support and query sets\n6: for each ğ‘¥âˆˆğ‘†âˆªğ‘„do\n7:\nğ‘¥embedding, ğ‘¥ğ‘embedding = ğ‘“(ğ‘¥)\nâŠ³Extract features using CLIP\n8: end for\n9: Step 2: Generate sub-support sets and sub-prototypes\n10: for each class ğ‘= 1, 2, â€¦ , ğ‘do\n11:\nğ‘†ğ‘= {ğ‘¥ğ‘1, ğ‘¥ğ‘2, â€¦ , ğ‘¥ğ‘ğ¾}\nâŠ³Support set for class ğ‘\n12:\nsub_Sğ‘= generate_sub_support_sets(ğ‘†ğ‘, ğ¾)\nâŠ³Generate ğ¾sub-support sets\n13:\nfor each sub-support set sub_set âˆˆsub_Sğ‘do\n14:\nğ‘¥token =\n1\nğ¾âˆ’1\nâˆ‘\nğ‘¥ğ‘–âˆˆsub_set ğ‘“(ğ‘¥ğ‘–)\nâŠ³Compute prototype token\n15:\nsub_prototype = ğœ™(ğ‘¥token, sub_set)\nâŠ³Extract sub-prototype\n16:\nsub_prototypesğ‘.append(sub_prototype)\nâŠ³Store sub-prototype\n17:\nend for\n18: end for\n19: Step 3: Compute prototype contrastive loss\n20: ğ¿prototype = 0\n21: for each class ğ‘= 1, 2, â€¦ , ğ‘do\n22:\nPositive pairs, Negative pairs â†sub-prototypes of the same class, sub-prototypes of different classes\n23:\nğ¿prototype+ = contrastive_loss(pos_pairs, neg_pairs)\nâŠ³Compute contrastive loss\n24: end for\n25: Step 4: Compute classification loss\n26: prototypes =\n{\n1\nğ¾\nâˆ‘\nğ‘âˆˆsub_prototypesğ‘ğ‘âˆ£ğ‘= 1, 2, â€¦ , ğ‘\n}\nâŠ³Compute prototypes\n27: ğ¿classifier = 0\n28: for each query sample ğ‘¥ğ‘âˆˆğ‘„do\n29:\ndistances =\n{\nğ¿2(ğ‘¥ğ‘embedding, prototypesğ‘) âˆ£ğ‘= 1, 2, â€¦ , ğ‘\n}\nâŠ³Compute distances\n30:\nğ¿classifier+ = cross_entropy_loss(distances, true_label)\nâŠ³Compute classification loss\n31: end for\n32: Step 5: Optimize the model\n33: Loss = ğ¿classifier + ğ¿prototype\n4.2. Experimental Settings\nTo obtain better image features, we use ViT-Large/14 as the backbone for image feature extraction and pair it with\nthe same CLIP pre-trained model used in CoOp [13] and Clip-Adapter [14]. Due to the limited data in the context\nof few-shot learning, prototype extraction module adopts a two-layer transformer architecture without incorporating\npositional encoding. During the training phase, we freeze the feature extraction network and only train the prototype\nextraction module proposed in this paper to preserve the image feature extraction capabilities of the pre-trained CLIP\nmodel and obtain a prototype extraction module with excellent class feature representations.\nDuring the training phase, we maintain the traditional episodic training approach and conduct training on 5-way\n5-shot and 5-way 1-shot task settings. Additionally, we use the Adam [48] optimizer to optimize the model. We set\nthe initial learning rate of the optimizer to 0.0001. The momentum weight coefficients ğ›½1 and ğ›½2, as well as the ğœ–\nparameter of the optimizer, are set to their default values of 0.9, 0.999, and 1e-8, respectively. In the gradient updating\nstrategy, we adopt the gradient accumulation algorithm, where we accumulate gradients over every 10 batches before\nperforming a parameter update. We train the model for 100 epochs, where each epoch consisted of 500 batches, and\n: Preprint submitted to Elsevier\nPage 7 of 13\n",
    "Table 1\nFew-shot learning classification accuracies(%) on miniImageNet, tieredImagenet and CUB-200 under the setting of 5-way\n1-shot and 5-way 5-shot with 95% confidence interval. (â€˜-â€™ not reported)\nModel\nminiImageNet\ntieredImagenet\nCUB-200\n5-way 5-shot\n5-way 1-shot\n5-way 5-shot\n5-way 1-shot\n5-way 5-shot\n5-way 1-shot\nMAML [35]\n64.31Â±1.1\n47.78Â±1.75\n71.10Â±1.67\n52.07Â±0.91\n-\n-\nPrototypical Network [11]\n78.44Â±0.21\n60.76Â±0.39\n80.11Â±0.91\n66.25Â±0.34\n-\n-\nHCTransformers [18]\n89.19 Â± 0.13\n74.62 Â± 0.20\n91.72 Â± 0.11\n79.57 Â± 0.20\n-\n-\nDeepEMD [36]\n82.41 Â± 0.56\n65.91 Â± 0.82\n86.03 Â± 0.58\n71.16 Â± 0.87\n88.69 Â± 0.50\n75.65 Â± 0.83\nMCL [10]\n83.99\n67.51\n86.02\n72.01\n93.18\n85.63\nPOODLE [37]\n85.81\n77.56\n86.96\n79.67\n93.80\n89.88\nFRN [38]\n82.83Â±0.13\n66.45Â±0.19\n86.89Â±0.14\n72.06Â±0.22\n92.92Â±0.10\n83.55Â±0.19\nPTN [39]\n88.43Â±0.67\n82.66Â±0.97\n89.14Â±0.71\n84.70Â±1.14\n-\n-\nFewTURE [19]\n86.38Â±0.49\n72.40Â±0.78\n89.96Â±0.55\n76.32Â±0.87\n-\n-\nEASY [40]\n89.14 Â± 0.1\n84.04 Â± 0.2\n89.76 Â± 0.14\n84.29 Â± 0.24\n93.79 Â± 0.10\n90.56 Â± 0.19\niLPC [41]\n88.82Â±0.42\n83.05Â±0.79\n92.46Â±0.42\n88.50Â±0.75\n94.11Â±0.30\n91.03Â±0.63\nSimple CNAPS [42]\n89.80\n82.16\n89.01\n78.29\n-\n-\nMBSS [43]\n86.32 Â± 0.44\n78.93 Â± 0.82\n91.41 Â± 0.48\n87.42 Â± 0.82\n90.83Â±0.39\n86.26Â±0.74\nBRAVE [44]\n88.93 Â± 0.32\n68.55 Â± 0.28\n89.05 Â± 0.24\n73.79 Â± 0.44\n-\n-\nFGFD GNN [45]\n96.50 Â± 0.25\n81.65 Â± 0.98\n-\n-\n91.56 Â± 0.24\n78.93 Â± 0.42\nFeatWalk [46]\n87.38 Â± 0.27\n70.21 Â± 0.44\n89.92 Â± 0.29\n75.25 Â± 0.48\n95.44 Â± 0.16\n85.67 Â± 0.38\nOurs\n97.07 Â± 0.11\n90.88 Â± 0.31\n95.00 Â± 0.19\n87.26 Â± 0.40\n94.25 Â± 0.16\n89.04 Â± 0.35\nFigure 4: We randomly select eight task sets from the test dataset and visualize their feature embeddings using t-SNE [49].\nIn the visualization, circular points represent query samples, triangles represent prototype points obtained by averaging the\nsupport set, and pentagrams represent class feature embeddings obtained through our proposed method in this paper.\neach batch represented a task. In image augmentation, we resize the images and then apply center cropping to obtain\n224 Ã— 224 pixel image inputs.\nIn the testing phase, to ensure fairness, we adhere to the evaluation methodology of few-shot learning without\nmaking any changes. We randomly sample 2000 tasks from the test set. For each task, we extract 15 query samples per\nclass to evaluate our method. We report the average accuracy with a 95% confidence interval to ensure the reliability\nof our results.\n: Preprint submitted to Elsevier\nPage 8 of 13\n",
    "Table 2\nThis ablation experiment aims to validate the effectiveness of the prototype extraction module.\nModel\nminiImageNet\ntieredImagenet\nCUB-200\n5-way 5-shot\n5-way 1-shot\n5-way 5-shot\n5-way 1-shot\n5-way 5-shot\n5-way 1-shot\nCLIP\n95.13 Â± 0.14%\n83.86 Â± 0.40%\n92.25 Â± 0.24%\n79.24 Â± 0.46%\n89.20 Â± 0.24%\n72.51 Â± 0.51%\nOurs\n97.07 Â± 0.11%\n90.88 Â± 0.31%\n95.00 Â± 0.19%\n87.26 Â± 0.40%\n94.25 Â± 0.16%\n89.04 Â± 0.35%\nTable 3\nThe table presents a comparative experiment on whether to include the prototype contrastive loss in the model.\nModel\nminiImageNet\n5-Way 5-Shot\n5-Way 1-Shot\nL_classifier\n96.24 Â± 0.11%\n89.13 Â± 0.32%\nL_classifier+L_prototype\n97.07 Â± 0.11%\n90.88 Â± 0.31%\n4.3. Results\nFollowing the few-shot standard experimental settings, we conduct experiments on both 5-way 1-shot and 5-way\n5-shot tasks to evaluate our method. The experimental results are presented in Table 1.\nAs shown in the table 1, our method outperforms the current state-of-the-art results on both 5-way 5-shot and\n5-way 1-shot tasks in the miniImageNet dataset. Excitingly, our method achieve an accuracy improvement of 0.57%\nover the current state-of-the-art method in the 5-way 5-shot task on this dataset. At the same time, our method also\nachieve a 6.84% accuracy improvement in the 5-way 1-shot task compared to the current state-of-the-art method. Our\nmethod achieve significant improvements in the 5-way 5-shot task on both the tieredImagenet dataset and the CUB-\n200 dataset compared to the existing methods. Observing the table, we can notice that compared to the 5-way 5-shot\ntasks, our methodâ€™s performance is slightly inferior in the 5-way 1-shot tasks. We believe that this is due to the lack\nof positive pairs in the 5-way 1-shot task, which hinders the prototype extraction moduleâ€™s ability to represent class\nfeatures accurately.\n4.4. Ablation Study\nTo validate the effectiveness of our method, we conduct ablation experiments from various perspectives on the\nproposed approach.\nTo validate the effectiveness of prototype extraction module, we conduct ablation experiments under two\nconditions: removing the prototype extraction module and retaining the prototype extraction module as part of our\nmethod. The experimental results are shown in Table 2, where â€œCLIP\" represents the condition where we remove\nthe prototype extraction module and retain only the CLIP pre-trained model. From the Table 2, we can observe that\nthe CLIP pre-trained model itself exhibits good few-shot image classification performance due to its strong zero-shot\nknowledge transfer ability in few-shot learning. Furthermore, our proposed method shows significant performance\nimprovement compared to the comparative methods in the ablation experiments.\nAs shown in Table 3, we conduct experiments on the miniImageNet dataset in both 5-way 5-shot and 5-way 1-shot\nsettings with and without the inclusion of the prototype contrastive loss. The experimental results indicate that the\nprototype loss has a positive impact on model optimization. Additionally, in Table 4, we conduct ablation experiments\non prototype extraction modules with 2, 4 and 6 layers of transformer blocks.\n4.5. Visualization\nIn this section, we delve into a comprehensive visualization analysis based on the model trained on the 5-way 5-shot\ntask of the miniImageNet dataset. The visualization, depicted in Figure 4, involves the random extraction of samples\nfrom 8 tasks in the test set, showcasing them using t-SNE. The visualization emphasizes the 15 query set samples\nthrough circular symbols, while triangular symbols signify the prototype points derived by averaging the embeddings\nof support set samples. Additionally, pentagram symbols denote the prototypes obtained using the prototype extraction\nmodule introduced in this paper.\n: Preprint submitted to Elsevier\nPage 9 of 13\n",
    "ï¼ˆa) miniImageNet\n(b) tieredImagenet\n(c) CUB-200\nCLIP\nOurs\nFigure 5: We randomly choose 5 categories from the test set, with 15 samples in each category, and create their similarity\nmatrix. In the visualization, yellow areas show correct classifications, while blue areas indicate misclassifications.\nTable 4\nAblation experiments of prototype extraction module with 2, 4 and 6 transformer blocks on miniImageNet dataset.\nLÃ—block\nminiImageNet\n5-way 5-shot\n5-way 1-shot\n2\n97.07 Â± 0.11%\n90.88 Â± 0.31%\n4\n95.96 Â± 0.13%\n90.03 Â± 0.33%\n6\n94.44 Â± 0.17%\n88.33 Â± 0.35%\nUpon careful observation of Figure 4, a notable distinction emerges. Class embeddings obtained through the\nprototype point calculation method, as seen in prototypical networks [11], tend to be positioned relatively closer to the\ncenter of their respective classes. In contrast, the class embeddings derived from our proposed method are strategically\npositioned towards the edges of the respective classes. This distinction arises from the underlying objectives of the\ntwo methods. The prototype point calculation method aims to represent the inherent characteristics of each class,\npositioning prototype points at the center to describe the class distribution in the feature space. On the other hand, our\nmethod strategically places class embeddings towards the edges, aiming to maximize the separation from other class\nsamples while staying close to samples of the same class for effective classification.\nTo further underscore the efficacy of our approach, we conduct a matrix similarity visualization comparing our\nmethod with the traditional prototype point approach, as illustrated in Figure 5. Notably, the term \"CLIP\" refers to the\nconventional prototype point representation using the CLIP pre-trained model as the backbone. These experiments are\nconducted separately on the miniImageNet, tieredImagenet, and CUB-200 datasets. The results showcased in Figure\n5 unequivocally highlight the substantial enhancement achieved by our proposed method in the domain of few-shot\nclassification.\n5. Conclusions\nWe propose PrototypeFormer, a simple transformer-based backbone for exploring the relationships among\nprototypes of few-shot classes to enhance the capability of robust feature learning. To further enhance the discriminative\ncharacteristics of prototype features, we introduce prototype contrastive learning for the optimization of prototypes.\nIn contrast to instance discrimination, we treat sub-prototypes from the same category as positive samples and sub-\nprototypes from different categories as negative samples. We evaluate PrototypeFormer on several popular few-\nshot image classification benchmark datasets and conduct comprehensive analyses through ablation experiments and\n: Preprint submitted to Elsevier\nPage 10 of 13\n",
    "visualization techniques. The experimental results demonstrate that our approach significantly outperforms the current\nstate-of-the-art methods. The success of PrototypeFormer is further evidenced by its ability to generalize well across\ndiverse datasets, showcasing its robustness and versatility in various image classification challenges. We hope that our\nwork encourages further exploration into sample relations, prototype relations, and class relations in few-shot learning.\nCRediT authorship contribution statement\nMeijuan Su: Conceptualization, Methodology, Software, Validation, Writing - review and editing. Feihong\nHe: Conceptualization, Methodology, Software, Writing - original draft, Writing - review and editing, Validation.\nFanzhang Li : Investigation, Methodology, Software, Project administration, Writing - review. .\nDeclaration of competing interest\nThe authors declare that they have no known competing financial interests or personal relationships that could have\nappeared to influence the work reported in this paper.\nAcknowledgements\nThis work was supported in part by the Priority Academic Program Development of Jiangsu Higher Education\nInstitutions, and by the National Natural Science Foundation of China under Grant No.61672364, No.62176172 and\nNo.61902269.\nData Availability\nThe datasets used during this study are available upon reasonable request to the authors.\nReferences\n[1] Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Generalizing from a few examples: A survey on few-shot learning. ACM\ncomputing surveys (csur), 53(3):1â€“34, 2020.\n[2] Ruixiang Zhang, Tong Che, Zoubin Ghahramani, Yoshua Bengio, and Yangqiu Song. Metagan: An adversarial approach to few-shot learning.\nAdvances in neural information processing systems, 31, 2018.\n[3] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint\narXiv:1710.09412, 2017.\n[4] Zitian Chen, Yanwei Fu, Yinda Zhang, Yu-Gang Jiang, Xiangyang Xue, and Leonid Sigal. Multi-level semantic feature augmentation for\none-shot learning. IEEE Transactions on Image Processing, 28(9):4594â€“4605, 2019.\n[5] Eli Schwartz, Leonid Karlinsky, Joseph Shtok, Sivan Harary, Mattias Marder, Abhishek Kumar, Rogerio Feris, Raja Giryes, and Alex\nBronstein. Delta-encoder: an effective sample synthesis method for few-shot object recognition. Advances in neural information processing\nsystems, 31, 2018.\n[6] Shuo Yang, Lu Liu, and Min Xu. Free lunch for few-shot learning: Distribution calibration. arXiv preprint arXiv:2101.06395, 2021.\n[7] Ruibing Hou, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. Cross attention network for few-shot classification. Advances in\nneural information processing systems, 32, 2019.\n[8] Wenbin Li, Lei Wang, Jinglin Xu, Jing Huo, Yang Gao, and Jiebo Luo. Revisiting local descriptor based image-to-class measure for few-shot\nlearning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7260â€“7268, 2019.\n[9] Yang Liu, Tu Zheng, Jie Song, Deng Cai, and Xiaofei He. Dmn4: Few-shot learning via discriminative mutual nearest neighbor neural network.\nIn Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 1828â€“1836, 2022.\n[10] Yang Liu, Weifeng Zhang, Chao Xiang, Tu Zheng, Deng Cai, and Xiaofei He. Learning to affiliate: Mutual centralized learning for few-shot\nclassification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14411â€“14420, 2022.\n[11] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in neural information processing\nsystems, 30, 2017.\n[12] Baoquan Zhang, Xutao Li, Yunming Ye, Zhichao Huang, and Lisai Zhang. Prototype completion with primitive knowledge for few-shot\nlearning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3754â€“3762, 2021.\n[13] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of\nComputer Vision, 130(9):2337â€“2348, 2022.\n[14] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-\nlanguage models with feature adapters. arXiv preprint arXiv:2110.04544, 2021.\n[15] Gregory Koch, Richard Zemel, Ruslan Salakhutdinov, et al. Siamese neural networks for one-shot image recognition. In ICML deep learning\nworkshop, volume 2. Lille, 2015.\n[16] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales. Learning to compare: Relation network for\nfew-shot learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1199â€“1208, 2018.\n[17] Xu Luo, Longhui Wei, Liangjian Wen, Jinrong Yang, Lingxi Xie, Zenglin Xu, and Qi Tian. Rectifying the shortcut learning of background\nfor few-shot learning. Advances in Neural Information Processing Systems, 34:13073â€“13085, 2021.\n[18] Yangji He, Weihan Liang, Dongyang Zhao, Hong-Yu Zhou, Weifeng Ge, Yizhou Yu, and Wenqiang Zhang. Attribute surrogates learning and\nspectral tokens pooling in transformers for few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 9119â€“9129, 2022.\n: Preprint submitted to Elsevier\nPage 11 of 13\n",
    "[19] Markus Hiller, Rongkai Ma, Mehrtash Harandi, and Tom Drummond. Rethinking generalization in few-shot classification. Advances in Neural\nInformation Processing Systems, 35:3582â€“3595, 2022.\n[20] Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free\nadaption of clip for few-shot classification. In European Conference on Computer Vision, pages 493â€“510. Springer, 2022.\n[21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela\nMishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine\nlearning, pages 8748â€“8763. PMLR, 2021.\n[22] Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X Yu. Large-scale long-tailed recognition in an open world.\nIn Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2537â€“2546, 2019.\n[23] Linchao Zhu and Yi Yang. Inflated episodic memory with region self-attention for long-tailed visual recognition. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4344â€“4353, 2020.\n[24] Martin Arjovsky, LÃ©on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.\n[25] Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In Computer Visionâ€“ECCV\n2020: 16th European Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part II 16, pages 124â€“140. Springer, 2020.\n[26] Xingchao Peng, Zijun Huang, Ximeng Sun, and Kate Saenko. Domain agnostic learning with disentangled representations. In International\nConference on Machine Learning, pages 5102â€“5112. PMLR, 2019.\n[27] Zhi Hou, Baosheng Yu, and Dacheng Tao. Batchformer: Learning to explore sample relationships for robust representation learning. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7256â€“7266, 2022.\n[28] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 3733â€“3742, 2018.\n[29] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729â€“9738, 2020.\n[30] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representa-\ntions. In International conference on machine learning, pages 1597â€“1607. PMLR, 2020.\n[31] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. Advances in neural\ninformation processing systems, 29, 2016.\n[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention\nis all you need. Advances in neural information processing systems, 30, 2017.\n[33] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[34] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE computer society\nconference on computer vision and pattern recognition (CVPRâ€™06), volume 2, pages 1735â€“1742. IEEE, 2006.\n[35] Chelsea Finn, Pieter Abbeel, and Sergey Levine.\nModel-agnostic meta-learning for fast adaptation of deep networks.\nIn International\nconference on machine learning, pages 1126â€“1135. PMLR, 2017.\n[36] Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen. Deepemd: Differentiable earth moverâ€™s distance for few-shot learning. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 45(5):5632â€“5648, 2022.\n[37] Duong Le, Khoi Duc Nguyen, Khoi Nguyen, Quoc-Huy Tran, Rang Nguyen, and Binh-Son Hua. Poodle: Improving few-shot learning via\npenalizing out-of-distribution samples. Advances in Neural Information Processing Systems, 34:23942â€“23955, 2021.\n[38] Davis Wertheimer, Luming Tang, and Bharath Hariharan. Few-shot classification with feature map reconstruction networks. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern recognition, pages 8012â€“8021, 2021.\n[39] Huaxi Huang, Junjie Zhang, Jian Zhang, Qiang Wu, and Chang Xu. Ptn: A poisson transfer network for semi-supervised few-shot learning.\nIn Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 1602â€“1609, 2021.\n[40] Yassir Bendou, Yuqing Hu, Raphael Lafargue, Giulia Lioi, Bastien Pasdeloup, StÃ©phane Pateux, and Vincent Gripon.\nEasyâ€”ensemble\naugmented-shot-y-shaped learning: State-of-the-art few-shot classification with simple components. Journal of Imaging, 8(7):179, 2022.\n[41] Michalis Lazarou, Tania Stathaki, and Yannis Avrithis. Iterative label cleaning for transductive and semi-supervised few-shot learning. In\nProceedings of the IEEE/CVF International Conference on Computer Vision, pages 8751â€“8760, 2021.\n[42] Peyman Bateni, Raghav Goyal, Vaden Masrani, Frank Wood, and Leonid Sigal. Improved few-shot visual classification. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14493â€“14502, 2020.\n[43] Jun Cheng, Fusheng Hao, Fengxiang He, Liu Liu, and Qieshi Zhang. Mixer-based semantic spread for few-shot learning. IEEE Transactions\non Multimedia, 25:191â€“202, 2021.\n[44] Huayi Ji, Linkai Luo, and Hong Peng. Brave: A cascaded generative model with sample attention for robust few shot image classification.\nNeurocomputing, 610:128585, 2024.\n[45] Priyanka Ganesan, Senthil Kumar Jagatheesaperumal, Mohammad Mehedi Hassan, Francesco Pupo, and Giancarlo Fortino. Few-shot image\nclassification using graph neural network with fine-grained feature descriptors. Neurocomputing, 610:128448, 2024.\n[46] Dalong Chen, Jianjia Zhang, Wei-Shi Zheng, and Ruixuan Wang. Featwalk: Enhancing few-shot classification through local view leveraging.\nIn Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1019â€“1027, 2024.\n[47] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.\n[48] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n[49] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.\n[50] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan. Supervision exists everywhere:\nA data efficient contrastive language-image pre-training paradigm. arXiv preprint arXiv:2110.05208, 2021.\n: Preprint submitted to Elsevier\nPage 12 of 13\n",
    "[51] Hao Zhu and Piotr Koniusz.\nTransductive few-shot learning with prototype-based label propagation by iterative graph refinement.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 23996â€“24006, 2023.\n[52] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.\nDecision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084â€“15097,\n2021.\n[53] Michael I Jordan. Serial order: A parallel distributed processing approach. In Advances in psychology, volume 121, pages 471â€“495. Elsevier,\n1997.\n[54] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735â€“1780, 1997.\n[55] Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of\nthe IEEE, 86(11):2278â€“2324, 1998.\n[56] Christian Simon, Piotr Koniusz, Richard Nock, and Mehrtash Harandi. Adaptive subspaces for few-shot learning. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, pages 4136â€“4145, 2020.\n[57] James Requeima, Jonathan Gordon, John Bronskill, Sebastian Nowozin, and Richard E Turner. Fast and flexible multi-task classification using\nconditional neural adaptive processes. Advances in Neural Information Processing Systems, 32, 2019.\n[58] Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Learning to model the tail. Advances in neural information processing systems, 30,\n2017.\n: Preprint submitted to Elsevier\nPage 13 of 13\n"
  ],
  "full_text": "Highlights\nPrototypeFormer: Learning to Explore Prototype Relationships for Few-shot Image Classi-\nfication\nMeijuan Su, Feihong He, Fanzhang Li\nâ€¢ Prototype Extraction Module. We introduce a novel and efficient transformer-based architecture specifically\ndesigned for few-shot learning. This module, termed the Prototype Extraction Module, leverages the self-\nattention mechanism of transformers to capture intricate relationships among intra-class samples. By treating\nclass prototypes as learnable tokens and integrating them with support set embeddings, the module extracts\nhighly discriminative prototype representations. Unlike traditional methods that rely on global average pooling\nor local descriptors, our approach provides a comprehensive global perspective, enabling the model to better\ncapture task-specific feature relationships. This module is simple yet powerful, significantly enhancing the\nmodelâ€™s ability to generalize in few-shot scenarios.\nâ€¢ Prototype Contrastive Loss. We form sub-prototypes by employing linear combinations of the support set.\nSubsequently, we optimize the model using the prototype contrastive loss based on these sub-prototypes to\nobtain more robust prototype representations. This approach ensures that similar class embeddings are pulled\ncloser together, while dissimilar ones are pushed apart, leading to more robust and generalizable prototype\nrepresentations. The contrastive loss is particularly effective in few-shot settings, where limited data makes\ntraditional methods prone to overfitting.\nâ€¢ Achieving State-of-the-Art Performance. We extensively evaluate our method on multiple widely used few-\nshot learning benchmarks. Our experiments demonstrate that PrototypeFormer consistently outperforms existing\nstate-of-the-art methods across these datasets. Notably, on the miniImageNet dataset, our method achieves\nremarkable accuracy improvements of 0.57% and 6.84% for 5-way 5-shot and 5-way 1-shot tasks, respectively.\nThese results highlight the effectiveness of our approach in addressing the challenges of few-shot learning,\nparticularly in scenarios with limited labeled data. The success of our method is further validated by its strong\nperformance on fine-grained classification tasks, such as those in the CUB-200 dataset.\narXiv:2310.03517v2  [cs.CV]  17 Feb 2025\n\n\nPrototypeFormer: Learning to Explore Prototype Relationships for\nFew-shot Image Classificationâ‹†\nMeijuan Sua, Feihong Heb and Fanzhang Li a\naSchool of Computer Science and Technology, Soochow University, 215000, Suzhou, China\nbSchool of Cyberspace Security, Sun Yat-sen University, 518107, Shenzhen, China\nA R T I C L E I N F O\nKeywords:\nfew-shot learning\nmetric learning\ntransformer\ncontrastive loss\nA B S T R A C T\nFew-shot image classification has received considerable attention for overcoming the challenge\nof limited classification performance with limited samples in novel classes. Most existing works\nemploy sophisticated learning strategies and feature learning modules to alleviate this challenge.\nIn this paper, we propose a novel method called PrototypeFormer, exploring the relationships\namong category prototypes in the few-shot scenario. Specifically, we utilize a transformer\narchitecture to build a prototype extraction module, aiming to extract class representations that\nare more discriminative for few-shot classification. Besides, during the model training process,\nwe propose a contrastive learning-based optimization approach to optimize prototype features in\nfew-shot learning scenarios. Despite its simplicity, our method performs remarkably well, with\nno bells and whistles. We have experimented with our approach on several popular few-shot\nimage classification benchmark datasets, which shows that our method outperforms all current\nstate-of-the-art methods. In particular, our method achieves 97.07% and 90.88% on 5-way 5-\nshot and 5-way 1-shot tasks of miniImageNet, which surpasses the state-of-the-art results with\naccuracy of 0.57% and 6.84%, respectively. The code will be released later.\n1. Introduction\nNeural networks have been remarkably successful in large-scale image classification. However, the domain of\nfew-shot image classification, where models must rapidly adapt to new data distributions with limited labeled samples\n(e.g., five or one sample for each class), remains a challenge. As a result of its promising applications in diverse fields\nsuch as medical image analysis and robotics, few-shot learning [1] has captivated the attention of the computer vision\nand machine learning community.\nRecent few-shot learning approaches mainly improve the generalization by augmenting the samples/features\nor facilitating feature representation with novel neural modules. A multitude of methods [2â€“6] utilizes generative\nmodels to generate new samples or augment feature space, aiming to approximate the actual distribution. Devising\nsophisticated feature representation modules is also a meaningful way to improve the model performance on low-\nshot categories. Specifically, CAN [7] leverages cross-attention mechanisms to acquire enriched sample embeddings\nwith enhanced class-specific features in a transductive way, while DN4 [8], DMN4 [9], and MCL [10] adopt\nlocal feature representations instead of global representations to obtain more discriminative feature representations.\nFollowing the line of feature representation learning approaches, we introduce a prototype extraction module to\nenhance the prototype embeddings. Contrary to earlier feature representation methodologies, our study delves into\nthe intricate interconnections both within each class and across the entire task to derive more discriminative prototype\nrepresentations.\nLearning prototype embedding [11, 12] is useful for few-shot classification. ProtoNet [11] introduces a method-\nology employing prototype points to encapsulate the feature embeddings of entire categories, and [12] proposes to\nenhance the notion of prototype points. However, they significantly ignore the prototype relationships for learning\nrobust class features. In this paper, we delve into the interconnections between prototype points, considering both\nintra-class and inter-class relationships. We first introduce a novel prototype extraction module to learn the relationship\nof intra-class samples through the self-attention of sub-prototypes. This module excels at obtaining a comprehensive\nâ‹†\nâˆ—Corresponding author\nORCID(s):\n: Preprint submitted to Elsevier\nPage 1 of 13\n\n\nSame Class\nÂ Similar Feature\nTask-specific Features\nDifferent Classes\nDiscriminative Features\nFigure 1: Samples from different categories exhibit both shared features and distinctive features. For example, the red\nrectangle indicates the similarity features among different categories, while the purple rectangle represents dissimilar features\nacross different categories.\nglobal perspective, enabling the extraction of robust class features based on relationships among categories throughout\nthe entire task.\nTo further fortify the robustness of class features in few-shot scenarios, we introduce prototype contrastive loss, a\nnovel contrastive loss designed explicitly to capture interactions among inter-class prototypes. One important concept\nin our approach is sub-prototypes, representing the average features of subsets of samples within each category. By\nemploying these sub-prototypes within a contrastive learning framework, we aim to cultivate more discriminative\nrepresentations. Specifically, the contrastive learning strategy ensures that similar class embeddings are drawn closer in\nthe feature space, while dissimilar ones are pushed apart, thus enhancing the discriminative power of our representative\nprototypes.\nMoreover, some works [13, 14] have demonstrated the impressive feature extraction capabilities of the CLIP pre-\ntrained model in few-shot learning. As a result, we integrate CLIP into our approach, undertaking only a limited amount\nof parameter training. We conclude our contribution as follows:\nâ€¢ Prototype Extraction Module. We introduce a novel and simple transformer-based architecture for few-shot\nlearning, employing a learnable prototype extraction module to extract prototype representations.\nâ€¢ Prototype Contrastive Loss. We form sub-prototypes by employing linear combinations of the support set.\nSubsequently, we optimize the model using the prototype contrastive loss based on these sub-prototypes to obtain\nmore robust prototype representations.\nâ€¢ Achieving State-of-the-Art Performance. We evaluate our method on multiple publicly few-shot benchmark\ndatasets, and the results demonstrate that the proposed method in this paper outperforms state-of-the-art few-shot\nlearning methods across these datasets, achieving a remarkable improvement of up to 6.84%.\n2. Related Work\n2.1. Few-shot Learning\nThe rapid development of deep neural networks in recent years has primarily benefited from large-scale labeled\ndatasets. However, the high cost of data collection and manual labeling has brought few-shot learning to the forefront\nof widespread interest. Few-shot learning is usually classified into optimization-based and metric-based methods. The\nmain idea of metric-based methods is to define specific metrics to classify samples in a way similar to the nearest\nneighbor algorithm. The Siamese Network[15] employs shared feature extractors to derive feature representations\nfrom both support sets and query sets. Subsequently, it computes classification similarity individually for each pair of\nsupport set and query set. Furthermore, the Siamese Network effectively distinguishes between different categories by\n: Preprint submitted to Elsevier\nPage 2 of 13\n\n\nC1K\nLoss\n=\n......\nSupport Set\nQuery\nCLIP\n......\n......\nPrototype\nExtraction\nmodule\n......\n......\n...\n......\n......\n...\nNegative Pair\nPositive Pair\nLPrototype\nLClassifier\n+\n...\nSub-prototype\nPrototype token\nOverview Diagram of PrototypeFormer\nFigure 2: This figure presents the overall process flowchart of the method proposed in this paper. We linearly combine\nthe support set and obtain sub-prototypes through the prototype extraction module. The sub-prototypes are utilized for\ncomputing the prototype contrastive loss ğ¿ğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’, while the prototype is employed for calculating the classification loss\nğ¿ğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ğ‘–ğ‘’ğ‘Ÿ. We sum the ğ¿ğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’and ğ¿ğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ğ‘–ğ‘’ğ‘Ÿto obtain the final optimization objective.\ncomparing deep features of the support and query sets, maintaining high classification accuracy even when differences\nbetween categories are subtle. The Prototypical Network [11] computes prototype points for each class of samples, and\nthe query samples are categorized by calculating the L2 distance to each prototype point. In Relation Network [16],\nthe incorporation of learnable nonlinear classifiers for sample classification is done innovatively. CAN [7] has\nimproved model performance by computing cross-attention on samples to enhance the networkâ€™s focus on classification\ntargets. Also, to reduce sample background interference, local descriptors that do not contain classification targets\nare eliminated in DN4 [8] and DMN4 [9] by comparing the similarity between local descriptors. COSOC [17],\nas a similar endeavor, seeks to enhance classification performance by distinguishing between classification targets\nand background elements. HCTransformers [18] propose a hierarchical cascading transformer architecture, aiming\nto address the overfitting challenges faced by large-scale models in few-shot learning. Meanwhile, FewTURE [19]\nsimilarly employs transformer architecture to extract key features from the main subjects within images. In the realm\nof generalized few-shot learning, a substantial body of work [13, 20] has already leveraged pre-trained models to\nenhance the efficacy of few-shot learning. In our research, we have also incorporated the pre-trained CLIP [21] model\nto enhance the feature extraction capabilities of our model. The critical distinction, however, lies in the fact that our\nmodel is trained using a meta-learning approach.\n2.2. Sample Relation\nThere exist diverse sample relationships among different class samples, and currently, most models are built\nupon the foundation of establishing these sample relationships. Numerous studies aim for models to achieve strong\ngeneralization performance across various class sample relationships, thereby minimizing vicinal risk. CAN [7] and\nOLTR [22] incorporate sample-specific relationships within the shared context by leveraging the correlations among\nindividual samples. IEM [23] analyzes local correlations among samples and performs memory storage updates for\nthese correlations. IRM [24] achieves a reduced vicinal risk by exploring the correlation between sample invariant\nfeatures and spurious features. In cross-domain tasks, [25] explores the transferability of sample relationships across\ndifferent domains by discarding specific sample relationships. Similar to [25], [26] explores domain-invariant and\nclass-invariant relationships by employing the deep adversarial disentangled autoencoder to achieve cross-domain\nclassification tasks. BatchFormer [27] has achieved significant improvements across various data scarcity tasks by\nimplicitly exploring the relationships among mini-batch samples during training. In mixup [3], samples are linearly\n: Preprint submitted to Elsevier\nPage 3 of 13\n\n\ninterpolated to capture the class-invariant relationships between samples. In our work, we perform linear combinations\nof samples to explore task-relevant relationships among them.\n2.3. Contrative Learning\nContrastive learning has achieved significant success in recent years. InstDisc [28] proposes the utilization of\ninstance discrimination tasks as an alternative to class-based discrimination tasks within the framework of unsupervised\nlearning. MOCO [29] achieves favorable transferability to downstream tasks through the strategy of constructing a\ndynamic dictionary and performing momentum-based updates. Contrastive learning has exhibited its generality and\nflexibility in time series tasks, encompassing domains like audio and textual data. An abundance of work [21, 29, 30]\nhas demonstrated the positive impact of contrastive learning in both unsupervised learning and generalization research\nwithin the realm of computer vision. The objective of contrastive learning is to bring together samples of the same\nclass while separating those from different classes, thus constructing suitable patterns for sample feature extraction. In\nepisodic training, we utilize contrastive learning methods to extract class relationships within the task, enhancing the\nclassification performance for few-shot learning.\n3. Method\nIn this section, we first describe the problem definition related to few-shot learning. Subsequently, an exposition of\nour proposed methodology is presented. Conclusively, we delve into a comprehensive discussion on the two important\ncomponents of our method: Prototype Extraction Module and Prototype Contrastive Loss.\n3.1. Problem Formulation\nEpisodic training differs from the deep neural networks training approach. In the traditional training of deep neural\nnetworks, we usually train the neural network on a sample-by-sample basis. In episodic training, we typically train the\nneural network on a task-by-task basis. The episodic training mechanism [31] has been demonstrated to facilitate the\nlearning of transferable knowledge across classes.\nIn few-shot learning, we usually divide the dataset into training, validation, and test sets. The training set, validation\nset, and test set have no overlapping classes. Therefore, we refer to the classes in the training set as seen classes, while\nthe classes in the validation set and test set are termed unseen classes. During the training phase, we randomly sample\nfrom the training set to create the support set and the query set. We use ğ‘†to represent the support set and ğ‘„to define\nthe query set. In the support set ğ‘†, there are ğ‘classes, and each class contains ğ¾samples. We treat the query set ğ‘„as\nunlabeled samples and perform classification on the unlabeled samples in ğ‘„using the labeled samples in the support\nset ğ‘†, which contains ğ‘classes, each with ğ¾samples. During the testing phase, we follow the same procedure and\ndivide the test set into a support set and a query set, similar to what we did during the training phase. This allows us\nto evaluate the few-shot learning performance of the model on unseen classes in a manner consistent with the training\nprocess. We typically refer to tasks that satisfy the above settings as N-way K-shot tasks. In our work, we train and\nevaluate the model using the aforementioned problem formulation.\n3.2. Overview\nWe linearly combine the support set and apply non-linear mapping through the prototype extraction module.\nFurthermore, we optimize the prototype extraction module using contrastive learning strategies to attain improved\nprototype representations. As illustrated in Figure 2, we process both the support set and query samples through a\nfrozen CLIP feature extraction network to obtain image embeddings. Subsequently, we perform linear combinations\non the support set samples to generate ğ¶1\nğ¾sub-support sets. Simultaneously, a prototype token is added to each support\nset and sub-support set, derived by computing the average of the respective embedding collection. Individually, each\nsupport set and sub-support set is fed into the prototype extraction module to obtain encoded prototypes and sub-\nprototypes. We retain the prototypes and sub-prototypes while discarding the sample embeddings from the support\nsets. We compute ğ¿ğ‘ƒğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’using the retained sub-prototypes through contrastive loss, while ğ¿ğ¶ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ğ‘–ğ‘’ğ‘Ÿis obtained\nby calculating the embeddings of query samples and prototypes. Finally, we sum up ğ¿ğ‘ƒğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’and ğ¿ğ¶ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ğ‘–ğ‘’ğ‘Ÿto create\nthe ultimate optimization objective.\n: Preprint submitted to Elsevier\nPage 4 of 13\n\n\nSupport Images\nÂ Embedding\nClass Token\nPrototype Extraction module\nEmbedded\nImage\nNorm\nMulti-Head\nAttention\nNorm\nMLP\nEncoder Block\nPrototype Extraction\nmodule\nClass Feature\nEmbedding\nLÃ—\nFigure 3: The prototype extraction module adopts the transformer structure [32], taking the prototype token and\nembeddings of same-class images from the support set as inputs to obtain the prototype and sub-prototype for that\nclass.\n3.3. Prototype Extraction Module\nIn this section, we will provide a comprehensive exposition of our proposed prototype extraction module.\nAdditionally, we will conduct a comparative analysis between our method and existing class feature extraction\napproaches found in the paper.\nFirst we introduce the prototype representation, the earliest class feature representation to appear in few-shot\nlearning. In the N-way K-shot task, we assume the existence of a class C, and in the support set ğ‘†, there exists a\nsubset ğ‘†ğ¶= {ğ‘¥1, ğ‘¥2, â€¦ ğ‘¥ğ¾âˆ£ğ‘¦= ğ¶}. We refer to the feature extraction network as ğ‘“. In that case, we can express the\nclass feature representation in the prototypical networks [11] as follows:\nğ‘ƒğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’(ğ¶) = 1\nğ¾\nğ¾\nâˆ‘\nğ‘–=1\nğ‘“(ğ‘¥ğ‘–\n) , ğ‘¥ğ‘–âŠ‚ğ‘†ğ¶\n(1)\nThe method of prototype points provides a simple and effective way to express class features. Absolutely, the global\naverage pooling layer used in the feature extraction network can introduce noise into the prototype points, causing\nthem to deviate from their true representation and leading to bias. To address this issue, DN4 [8] and DMN4 [9]\nremove the global average pooling layer from the feature extraction network. They employ local descriptors to replace\nthe global feature representation of images and utilize a discriminative nearest neighbor algorithm to obtain the most\nrepresentative local descriptors in the images as the feature representation for samples.\nHowever, we believe that the image background has a certain influence on the image classification performance and\nalso provides some category-related contextual features. Therefore, we propose a novel class feature extraction module\nreferred to as prototype extraction module to replace the current few-shot class feature representation. In ViT [33],\nthe image is divided into patches, and transformer [32] is utilized to compute the correlations between these patches,\nresulting in the overall feature representation of the entire image. Inspired by ViT, we simply treat the image as a set\nof patches input to the transformer, thereby obtaining the feature representation for the entire class. The fundamental\narchitecture of prototype extraction module is illustrated in Figure 3. We use ğœ™to represent the prototype extraction\nmodule, and we can express it in the following form:\nğ‘ƒğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’(ğ¶) = ğœ™(ğ‘¥ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›, ğ‘“(ğ‘¥1\n) , ğ‘“(ğ‘¥2\n) , â€¦ ğ‘“(ğ‘¥ğ¾\n)) , ğ‘¥ğ‘–âŠ‚ğ‘†ğ¶\n(2)\nIn the formula, ğ‘¥ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›represents the prototype token for that class, and it can be expressed as:\nğ‘¥ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›= 1\nğ¾\nğ¾\nâˆ‘\nğ‘–=1\nğ‘“(ğ‘¥ğ‘–\n) , ğ‘¥ğ‘–âŠ‚ğ‘†ğ¶\n(3)\nFinally, we use a simple metric learning classification method to classify the query samples. Specifically, we calculate\nthe distance between the embeddings of the query samples and the prototype points in the feature space to measure the\n: Preprint submitted to Elsevier\nPage 5 of 13\n\n\nsimilarity between the query samples and each class. This distance metric is used for classification, where the query\nsample is assigned to the class with the closest feature embedding in the feature space. This classification approach\ncan be formalized with the following formula:\nğ‘ğ‘Ÿğ‘”ğ‘šğ‘–ğ‘›ğ‘âŠ‚ğ¶ğ¿2\n(ğ‘¥ğ‘ğ‘¢ğ‘’ğ‘Ÿğ‘¦, ğ‘ƒğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’(ğ‘)\n)\n(4)\nThe classification loss is optimized using the cross-entropy loss, and the formula for the classification loss is as follows:\nğ¿ğ‘œğ‘ ğ‘ ğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ğ‘¦= âˆ’\nğ‘\nâˆ‘\nğ‘=1\nğ‘¦ğ‘ğ‘™ğ‘œğ‘”\n(\nğ‘’âˆ’ğ¿2\n(ğ‘¥ğ‘ğ‘¢ğ‘’ğ‘Ÿğ‘¦,ğ‘ƒğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’(ğ‘)\n)\nâˆ‘ğ‘\nğ‘–=1 ğ‘’âˆ’ğ¿2\n(ğ‘¥ğ‘ğ‘¢ğ‘’ğ‘Ÿğ‘¦,ğ‘ƒğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’(ğ‘–)\n)\n)\n(5)\nThe ğ‘¦ğ‘is the one-hot encoding of the true class label for the sample.\n3.4. Prototype Contrastive Loss\nTo enhance the generalization capability of the prototype extraction module, we drew inspiration from contrastive\nlearning and proposed prototype contrastive loss. The contrastive loss was first introduced by\n[34] and laid the\nfoundation for subsequent highly successful contrastive learning [29, 30]. The main idea of the contrastive loss is\nto construct positive and negative sample pairs, where positive pairs are brought closer together in the feature space,\nwhile negative pairs are pushed further apart.\nIn few-shot learning, by extracting K-1 samples from the same class in the support set ğ‘†, we can obtain ğ¾different\nsub-support set of samples ğ‘†ğ‘ğ‘–= {ğ‘¥ğ‘1, â€¦ , ğ‘¥ğ‘ğ‘–âˆ’1, ğ‘¥ğ‘ğ‘–+1 â€¦ , ğ‘¥ğ‘ğ¾\n} , ğ‘–= 1, 2 â€¦ ğ¾, ğ‘âŠ‚ğ¶. Then, we pass each of these\nK sub-support sets constructed from the same class samples through the prototype extraction module to obtain K sub-\nprototypes for that class. We use the K sub-prototypes obtained from the same-class support set samples as positive\npairs. At the same time, we use the sub-prototypes obtained from different-class sub-support sets as negative pairs. We\nrepresent the constructed positive sample pairs as follows:\nğ‘ƒğ‘œğ‘ ğ‘= {ğ‘ğ‘1, ğ‘ğ‘2, â€¦ ğ‘ğ‘ğ¾\n} , ğ¶= 1, 2 â€¦ ğ‘\n(6)\nThus, we can obtain the prototype contrastive loss using the constructed positive and negative pairs as follows:\nğ¿ğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’= ğ‘’ğ‘¥ğ‘\n(\n1\nğ‘â‹…\nâˆ‘ğ¾\nğ‘–,ğ‘—=1 ğ¿2\n(ğ‘ğ‘ğ‘–, ğ‘ğ‘ğ‘—\n) + ğ¼\nâˆ‘\nğ‘šâ‰ ğ‘›\nâˆ‘ğ¾\nğ‘–,ğ‘—=1 ğ¿2\n(ğ‘ğ‘š,ğ‘–, ğ‘ğ‘›,ğ‘—\n) + ğ¼\n)\n(7)\nBecause when ğ¾= 1, the support set contains only one sample per class, leading to âˆ‘ğ¾\nğ‘–,ğ‘—=1 ğ¿2\n(ğ‘ğ‘ğ‘–, ğ‘ğ‘ğ‘—\n) = 0. To\navoid this situation, we add the identity element I to prevent it from happening. The overall loss of the model during\nthe training phase is as follows:\nğ¿ğ‘œğ‘ ğ‘ = ğ¿ğ‘œğ‘ ğ‘ ğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ğ‘–ğ‘’ğ‘Ÿ+ ğ¿ğ‘œğ‘ ğ‘ ğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’\n(8)\nFinally, we present the pseudocode for the training process of PrototypeFormer in Algorithm 1.\n4. Experiments\nIn this section, we will evaluate the proposed method on multiple few-shot benchmark datasets and compare it with\nstate-of-the-art methods. Additionally, we will conduct ablation experiments and visualization experiments to further\nanalyze and validate the effectiveness of our proposed approach.\n4.1. Datasets\nminiImageNet [31] is a subset of the larger ImageNet dataset and is widely used in few-shot learning research. It\nconsists of 100 classes, with each class containing 600 images, resulting in a total of 60,000 images. The dataset is\ndivided into 64 classes for the training set, 16 classes for the validation set, and 20 classes for the test set.\ntieredImagenet is a larger subset of the ImageNet dataset compared to miniImagenet. The dataset consists of 608\nclasses with a total of 779,165 images. For few-shot learning, it is divided into three subsets, with 351 classes used for\nthe training set, 97 classes for the validation set, and 160 classes for the testing set.\nCaltech-UCSD Birds-200-2011 [47], also known as CUB, is the benchmark image dataset for current fine-grained\nclassification and recognition research. The dataset contains 11,788 bird images, encompassing 200 subclasses of bird\nspecies. We split it into 100, 50, and 50 classes for training, validation, and testing, respectively.\n: Preprint submitted to Elsevier\nPage 6 of 13\n\n\nAlgorithm 1 Training Process of PrototypeFormer\n1: Input:\n2:\nSupport set ğ‘†\n=\n{ğ‘†1, ğ‘†2, â€¦ , ğ‘†ğ‘}, where ğ‘†ğ‘\n=\n{ğ‘¥ğ‘1, ğ‘¥ğ‘2, â€¦ , ğ‘¥ğ‘ğ¾} for class ğ‘. Query set ğ‘„\n=\n{ğ‘¥ğ‘1, ğ‘¥ğ‘2, â€¦ , ğ‘¥ğ‘ğ‘€}. Pre-trained CLIP feature extractor ğ‘“(frozen). Prototype extraction module ğœ™(Transformer-\nbased). Number of classes ğ‘, number of shots ğ¾.\n3: Output:\n4:\nClassification results for query set ğ‘„.\n5: Step 1: Extract features for support and query sets\n6: for each ğ‘¥âˆˆğ‘†âˆªğ‘„do\n7:\nğ‘¥embedding, ğ‘¥ğ‘embedding = ğ‘“(ğ‘¥)\nâŠ³Extract features using CLIP\n8: end for\n9: Step 2: Generate sub-support sets and sub-prototypes\n10: for each class ğ‘= 1, 2, â€¦ , ğ‘do\n11:\nğ‘†ğ‘= {ğ‘¥ğ‘1, ğ‘¥ğ‘2, â€¦ , ğ‘¥ğ‘ğ¾}\nâŠ³Support set for class ğ‘\n12:\nsub_Sğ‘= generate_sub_support_sets(ğ‘†ğ‘, ğ¾)\nâŠ³Generate ğ¾sub-support sets\n13:\nfor each sub-support set sub_set âˆˆsub_Sğ‘do\n14:\nğ‘¥token =\n1\nğ¾âˆ’1\nâˆ‘\nğ‘¥ğ‘–âˆˆsub_set ğ‘“(ğ‘¥ğ‘–)\nâŠ³Compute prototype token\n15:\nsub_prototype = ğœ™(ğ‘¥token, sub_set)\nâŠ³Extract sub-prototype\n16:\nsub_prototypesğ‘.append(sub_prototype)\nâŠ³Store sub-prototype\n17:\nend for\n18: end for\n19: Step 3: Compute prototype contrastive loss\n20: ğ¿prototype = 0\n21: for each class ğ‘= 1, 2, â€¦ , ğ‘do\n22:\nPositive pairs, Negative pairs â†sub-prototypes of the same class, sub-prototypes of different classes\n23:\nğ¿prototype+ = contrastive_loss(pos_pairs, neg_pairs)\nâŠ³Compute contrastive loss\n24: end for\n25: Step 4: Compute classification loss\n26: prototypes =\n{\n1\nğ¾\nâˆ‘\nğ‘âˆˆsub_prototypesğ‘ğ‘âˆ£ğ‘= 1, 2, â€¦ , ğ‘\n}\nâŠ³Compute prototypes\n27: ğ¿classifier = 0\n28: for each query sample ğ‘¥ğ‘âˆˆğ‘„do\n29:\ndistances =\n{\nğ¿2(ğ‘¥ğ‘embedding, prototypesğ‘) âˆ£ğ‘= 1, 2, â€¦ , ğ‘\n}\nâŠ³Compute distances\n30:\nğ¿classifier+ = cross_entropy_loss(distances, true_label)\nâŠ³Compute classification loss\n31: end for\n32: Step 5: Optimize the model\n33: Loss = ğ¿classifier + ğ¿prototype\n4.2. Experimental Settings\nTo obtain better image features, we use ViT-Large/14 as the backbone for image feature extraction and pair it with\nthe same CLIP pre-trained model used in CoOp [13] and Clip-Adapter [14]. Due to the limited data in the context\nof few-shot learning, prototype extraction module adopts a two-layer transformer architecture without incorporating\npositional encoding. During the training phase, we freeze the feature extraction network and only train the prototype\nextraction module proposed in this paper to preserve the image feature extraction capabilities of the pre-trained CLIP\nmodel and obtain a prototype extraction module with excellent class feature representations.\nDuring the training phase, we maintain the traditional episodic training approach and conduct training on 5-way\n5-shot and 5-way 1-shot task settings. Additionally, we use the Adam [48] optimizer to optimize the model. We set\nthe initial learning rate of the optimizer to 0.0001. The momentum weight coefficients ğ›½1 and ğ›½2, as well as the ğœ–\nparameter of the optimizer, are set to their default values of 0.9, 0.999, and 1e-8, respectively. In the gradient updating\nstrategy, we adopt the gradient accumulation algorithm, where we accumulate gradients over every 10 batches before\nperforming a parameter update. We train the model for 100 epochs, where each epoch consisted of 500 batches, and\n: Preprint submitted to Elsevier\nPage 7 of 13\n\n\nTable 1\nFew-shot learning classification accuracies(%) on miniImageNet, tieredImagenet and CUB-200 under the setting of 5-way\n1-shot and 5-way 5-shot with 95% confidence interval. (â€˜-â€™ not reported)\nModel\nminiImageNet\ntieredImagenet\nCUB-200\n5-way 5-shot\n5-way 1-shot\n5-way 5-shot\n5-way 1-shot\n5-way 5-shot\n5-way 1-shot\nMAML [35]\n64.31Â±1.1\n47.78Â±1.75\n71.10Â±1.67\n52.07Â±0.91\n-\n-\nPrototypical Network [11]\n78.44Â±0.21\n60.76Â±0.39\n80.11Â±0.91\n66.25Â±0.34\n-\n-\nHCTransformers [18]\n89.19 Â± 0.13\n74.62 Â± 0.20\n91.72 Â± 0.11\n79.57 Â± 0.20\n-\n-\nDeepEMD [36]\n82.41 Â± 0.56\n65.91 Â± 0.82\n86.03 Â± 0.58\n71.16 Â± 0.87\n88.69 Â± 0.50\n75.65 Â± 0.83\nMCL [10]\n83.99\n67.51\n86.02\n72.01\n93.18\n85.63\nPOODLE [37]\n85.81\n77.56\n86.96\n79.67\n93.80\n89.88\nFRN [38]\n82.83Â±0.13\n66.45Â±0.19\n86.89Â±0.14\n72.06Â±0.22\n92.92Â±0.10\n83.55Â±0.19\nPTN [39]\n88.43Â±0.67\n82.66Â±0.97\n89.14Â±0.71\n84.70Â±1.14\n-\n-\nFewTURE [19]\n86.38Â±0.49\n72.40Â±0.78\n89.96Â±0.55\n76.32Â±0.87\n-\n-\nEASY [40]\n89.14 Â± 0.1\n84.04 Â± 0.2\n89.76 Â± 0.14\n84.29 Â± 0.24\n93.79 Â± 0.10\n90.56 Â± 0.19\niLPC [41]\n88.82Â±0.42\n83.05Â±0.79\n92.46Â±0.42\n88.50Â±0.75\n94.11Â±0.30\n91.03Â±0.63\nSimple CNAPS [42]\n89.80\n82.16\n89.01\n78.29\n-\n-\nMBSS [43]\n86.32 Â± 0.44\n78.93 Â± 0.82\n91.41 Â± 0.48\n87.42 Â± 0.82\n90.83Â±0.39\n86.26Â±0.74\nBRAVE [44]\n88.93 Â± 0.32\n68.55 Â± 0.28\n89.05 Â± 0.24\n73.79 Â± 0.44\n-\n-\nFGFD GNN [45]\n96.50 Â± 0.25\n81.65 Â± 0.98\n-\n-\n91.56 Â± 0.24\n78.93 Â± 0.42\nFeatWalk [46]\n87.38 Â± 0.27\n70.21 Â± 0.44\n89.92 Â± 0.29\n75.25 Â± 0.48\n95.44 Â± 0.16\n85.67 Â± 0.38\nOurs\n97.07 Â± 0.11\n90.88 Â± 0.31\n95.00 Â± 0.19\n87.26 Â± 0.40\n94.25 Â± 0.16\n89.04 Â± 0.35\nFigure 4: We randomly select eight task sets from the test dataset and visualize their feature embeddings using t-SNE [49].\nIn the visualization, circular points represent query samples, triangles represent prototype points obtained by averaging the\nsupport set, and pentagrams represent class feature embeddings obtained through our proposed method in this paper.\neach batch represented a task. In image augmentation, we resize the images and then apply center cropping to obtain\n224 Ã— 224 pixel image inputs.\nIn the testing phase, to ensure fairness, we adhere to the evaluation methodology of few-shot learning without\nmaking any changes. We randomly sample 2000 tasks from the test set. For each task, we extract 15 query samples per\nclass to evaluate our method. We report the average accuracy with a 95% confidence interval to ensure the reliability\nof our results.\n: Preprint submitted to Elsevier\nPage 8 of 13\n\n\nTable 2\nThis ablation experiment aims to validate the effectiveness of the prototype extraction module.\nModel\nminiImageNet\ntieredImagenet\nCUB-200\n5-way 5-shot\n5-way 1-shot\n5-way 5-shot\n5-way 1-shot\n5-way 5-shot\n5-way 1-shot\nCLIP\n95.13 Â± 0.14%\n83.86 Â± 0.40%\n92.25 Â± 0.24%\n79.24 Â± 0.46%\n89.20 Â± 0.24%\n72.51 Â± 0.51%\nOurs\n97.07 Â± 0.11%\n90.88 Â± 0.31%\n95.00 Â± 0.19%\n87.26 Â± 0.40%\n94.25 Â± 0.16%\n89.04 Â± 0.35%\nTable 3\nThe table presents a comparative experiment on whether to include the prototype contrastive loss in the model.\nModel\nminiImageNet\n5-Way 5-Shot\n5-Way 1-Shot\nL_classifier\n96.24 Â± 0.11%\n89.13 Â± 0.32%\nL_classifier+L_prototype\n97.07 Â± 0.11%\n90.88 Â± 0.31%\n4.3. Results\nFollowing the few-shot standard experimental settings, we conduct experiments on both 5-way 1-shot and 5-way\n5-shot tasks to evaluate our method. The experimental results are presented in Table 1.\nAs shown in the table 1, our method outperforms the current state-of-the-art results on both 5-way 5-shot and\n5-way 1-shot tasks in the miniImageNet dataset. Excitingly, our method achieve an accuracy improvement of 0.57%\nover the current state-of-the-art method in the 5-way 5-shot task on this dataset. At the same time, our method also\nachieve a 6.84% accuracy improvement in the 5-way 1-shot task compared to the current state-of-the-art method. Our\nmethod achieve significant improvements in the 5-way 5-shot task on both the tieredImagenet dataset and the CUB-\n200 dataset compared to the existing methods. Observing the table, we can notice that compared to the 5-way 5-shot\ntasks, our methodâ€™s performance is slightly inferior in the 5-way 1-shot tasks. We believe that this is due to the lack\nof positive pairs in the 5-way 1-shot task, which hinders the prototype extraction moduleâ€™s ability to represent class\nfeatures accurately.\n4.4. Ablation Study\nTo validate the effectiveness of our method, we conduct ablation experiments from various perspectives on the\nproposed approach.\nTo validate the effectiveness of prototype extraction module, we conduct ablation experiments under two\nconditions: removing the prototype extraction module and retaining the prototype extraction module as part of our\nmethod. The experimental results are shown in Table 2, where â€œCLIP\" represents the condition where we remove\nthe prototype extraction module and retain only the CLIP pre-trained model. From the Table 2, we can observe that\nthe CLIP pre-trained model itself exhibits good few-shot image classification performance due to its strong zero-shot\nknowledge transfer ability in few-shot learning. Furthermore, our proposed method shows significant performance\nimprovement compared to the comparative methods in the ablation experiments.\nAs shown in Table 3, we conduct experiments on the miniImageNet dataset in both 5-way 5-shot and 5-way 1-shot\nsettings with and without the inclusion of the prototype contrastive loss. The experimental results indicate that the\nprototype loss has a positive impact on model optimization. Additionally, in Table 4, we conduct ablation experiments\non prototype extraction modules with 2, 4 and 6 layers of transformer blocks.\n4.5. Visualization\nIn this section, we delve into a comprehensive visualization analysis based on the model trained on the 5-way 5-shot\ntask of the miniImageNet dataset. The visualization, depicted in Figure 4, involves the random extraction of samples\nfrom 8 tasks in the test set, showcasing them using t-SNE. The visualization emphasizes the 15 query set samples\nthrough circular symbols, while triangular symbols signify the prototype points derived by averaging the embeddings\nof support set samples. Additionally, pentagram symbols denote the prototypes obtained using the prototype extraction\nmodule introduced in this paper.\n: Preprint submitted to Elsevier\nPage 9 of 13\n\n\nï¼ˆa) miniImageNet\n(b) tieredImagenet\n(c) CUB-200\nCLIP\nOurs\nFigure 5: We randomly choose 5 categories from the test set, with 15 samples in each category, and create their similarity\nmatrix. In the visualization, yellow areas show correct classifications, while blue areas indicate misclassifications.\nTable 4\nAblation experiments of prototype extraction module with 2, 4 and 6 transformer blocks on miniImageNet dataset.\nLÃ—block\nminiImageNet\n5-way 5-shot\n5-way 1-shot\n2\n97.07 Â± 0.11%\n90.88 Â± 0.31%\n4\n95.96 Â± 0.13%\n90.03 Â± 0.33%\n6\n94.44 Â± 0.17%\n88.33 Â± 0.35%\nUpon careful observation of Figure 4, a notable distinction emerges. Class embeddings obtained through the\nprototype point calculation method, as seen in prototypical networks [11], tend to be positioned relatively closer to the\ncenter of their respective classes. In contrast, the class embeddings derived from our proposed method are strategically\npositioned towards the edges of the respective classes. This distinction arises from the underlying objectives of the\ntwo methods. The prototype point calculation method aims to represent the inherent characteristics of each class,\npositioning prototype points at the center to describe the class distribution in the feature space. On the other hand, our\nmethod strategically places class embeddings towards the edges, aiming to maximize the separation from other class\nsamples while staying close to samples of the same class for effective classification.\nTo further underscore the efficacy of our approach, we conduct a matrix similarity visualization comparing our\nmethod with the traditional prototype point approach, as illustrated in Figure 5. Notably, the term \"CLIP\" refers to the\nconventional prototype point representation using the CLIP pre-trained model as the backbone. These experiments are\nconducted separately on the miniImageNet, tieredImagenet, and CUB-200 datasets. The results showcased in Figure\n5 unequivocally highlight the substantial enhancement achieved by our proposed method in the domain of few-shot\nclassification.\n5. Conclusions\nWe propose PrototypeFormer, a simple transformer-based backbone for exploring the relationships among\nprototypes of few-shot classes to enhance the capability of robust feature learning. To further enhance the discriminative\ncharacteristics of prototype features, we introduce prototype contrastive learning for the optimization of prototypes.\nIn contrast to instance discrimination, we treat sub-prototypes from the same category as positive samples and sub-\nprototypes from different categories as negative samples. We evaluate PrototypeFormer on several popular few-\nshot image classification benchmark datasets and conduct comprehensive analyses through ablation experiments and\n: Preprint submitted to Elsevier\nPage 10 of 13\n\n\nvisualization techniques. The experimental results demonstrate that our approach significantly outperforms the current\nstate-of-the-art methods. The success of PrototypeFormer is further evidenced by its ability to generalize well across\ndiverse datasets, showcasing its robustness and versatility in various image classification challenges. We hope that our\nwork encourages further exploration into sample relations, prototype relations, and class relations in few-shot learning.\nCRediT authorship contribution statement\nMeijuan Su: Conceptualization, Methodology, Software, Validation, Writing - review and editing. Feihong\nHe: Conceptualization, Methodology, Software, Writing - original draft, Writing - review and editing, Validation.\nFanzhang Li : Investigation, Methodology, Software, Project administration, Writing - review. .\nDeclaration of competing interest\nThe authors declare that they have no known competing financial interests or personal relationships that could have\nappeared to influence the work reported in this paper.\nAcknowledgements\nThis work was supported in part by the Priority Academic Program Development of Jiangsu Higher Education\nInstitutions, and by the National Natural Science Foundation of China under Grant No.61672364, No.62176172 and\nNo.61902269.\nData Availability\nThe datasets used during this study are available upon reasonable request to the authors.\nReferences\n[1] Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Generalizing from a few examples: A survey on few-shot learning. ACM\ncomputing surveys (csur), 53(3):1â€“34, 2020.\n[2] Ruixiang Zhang, Tong Che, Zoubin Ghahramani, Yoshua Bengio, and Yangqiu Song. Metagan: An adversarial approach to few-shot learning.\nAdvances in neural information processing systems, 31, 2018.\n[3] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint\narXiv:1710.09412, 2017.\n[4] Zitian Chen, Yanwei Fu, Yinda Zhang, Yu-Gang Jiang, Xiangyang Xue, and Leonid Sigal. Multi-level semantic feature augmentation for\none-shot learning. IEEE Transactions on Image Processing, 28(9):4594â€“4605, 2019.\n[5] Eli Schwartz, Leonid Karlinsky, Joseph Shtok, Sivan Harary, Mattias Marder, Abhishek Kumar, Rogerio Feris, Raja Giryes, and Alex\nBronstein. Delta-encoder: an effective sample synthesis method for few-shot object recognition. Advances in neural information processing\nsystems, 31, 2018.\n[6] Shuo Yang, Lu Liu, and Min Xu. Free lunch for few-shot learning: Distribution calibration. arXiv preprint arXiv:2101.06395, 2021.\n[7] Ruibing Hou, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. Cross attention network for few-shot classification. Advances in\nneural information processing systems, 32, 2019.\n[8] Wenbin Li, Lei Wang, Jinglin Xu, Jing Huo, Yang Gao, and Jiebo Luo. Revisiting local descriptor based image-to-class measure for few-shot\nlearning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7260â€“7268, 2019.\n[9] Yang Liu, Tu Zheng, Jie Song, Deng Cai, and Xiaofei He. Dmn4: Few-shot learning via discriminative mutual nearest neighbor neural network.\nIn Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 1828â€“1836, 2022.\n[10] Yang Liu, Weifeng Zhang, Chao Xiang, Tu Zheng, Deng Cai, and Xiaofei He. Learning to affiliate: Mutual centralized learning for few-shot\nclassification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14411â€“14420, 2022.\n[11] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in neural information processing\nsystems, 30, 2017.\n[12] Baoquan Zhang, Xutao Li, Yunming Ye, Zhichao Huang, and Lisai Zhang. Prototype completion with primitive knowledge for few-shot\nlearning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3754â€“3762, 2021.\n[13] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of\nComputer Vision, 130(9):2337â€“2348, 2022.\n[14] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-\nlanguage models with feature adapters. arXiv preprint arXiv:2110.04544, 2021.\n[15] Gregory Koch, Richard Zemel, Ruslan Salakhutdinov, et al. Siamese neural networks for one-shot image recognition. In ICML deep learning\nworkshop, volume 2. Lille, 2015.\n[16] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales. Learning to compare: Relation network for\nfew-shot learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1199â€“1208, 2018.\n[17] Xu Luo, Longhui Wei, Liangjian Wen, Jinrong Yang, Lingxi Xie, Zenglin Xu, and Qi Tian. Rectifying the shortcut learning of background\nfor few-shot learning. Advances in Neural Information Processing Systems, 34:13073â€“13085, 2021.\n[18] Yangji He, Weihan Liang, Dongyang Zhao, Hong-Yu Zhou, Weifeng Ge, Yizhou Yu, and Wenqiang Zhang. Attribute surrogates learning and\nspectral tokens pooling in transformers for few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 9119â€“9129, 2022.\n: Preprint submitted to Elsevier\nPage 11 of 13\n\n\n[19] Markus Hiller, Rongkai Ma, Mehrtash Harandi, and Tom Drummond. Rethinking generalization in few-shot classification. Advances in Neural\nInformation Processing Systems, 35:3582â€“3595, 2022.\n[20] Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free\nadaption of clip for few-shot classification. In European Conference on Computer Vision, pages 493â€“510. Springer, 2022.\n[21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela\nMishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine\nlearning, pages 8748â€“8763. PMLR, 2021.\n[22] Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X Yu. Large-scale long-tailed recognition in an open world.\nIn Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2537â€“2546, 2019.\n[23] Linchao Zhu and Yi Yang. Inflated episodic memory with region self-attention for long-tailed visual recognition. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4344â€“4353, 2020.\n[24] Martin Arjovsky, LÃ©on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.\n[25] Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In Computer Visionâ€“ECCV\n2020: 16th European Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part II 16, pages 124â€“140. Springer, 2020.\n[26] Xingchao Peng, Zijun Huang, Ximeng Sun, and Kate Saenko. Domain agnostic learning with disentangled representations. In International\nConference on Machine Learning, pages 5102â€“5112. PMLR, 2019.\n[27] Zhi Hou, Baosheng Yu, and Dacheng Tao. Batchformer: Learning to explore sample relationships for robust representation learning. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7256â€“7266, 2022.\n[28] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 3733â€“3742, 2018.\n[29] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729â€“9738, 2020.\n[30] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representa-\ntions. In International conference on machine learning, pages 1597â€“1607. PMLR, 2020.\n[31] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. Advances in neural\ninformation processing systems, 29, 2016.\n[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention\nis all you need. Advances in neural information processing systems, 30, 2017.\n[33] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[34] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE computer society\nconference on computer vision and pattern recognition (CVPRâ€™06), volume 2, pages 1735â€“1742. IEEE, 2006.\n[35] Chelsea Finn, Pieter Abbeel, and Sergey Levine.\nModel-agnostic meta-learning for fast adaptation of deep networks.\nIn International\nconference on machine learning, pages 1126â€“1135. PMLR, 2017.\n[36] Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen. Deepemd: Differentiable earth moverâ€™s distance for few-shot learning. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 45(5):5632â€“5648, 2022.\n[37] Duong Le, Khoi Duc Nguyen, Khoi Nguyen, Quoc-Huy Tran, Rang Nguyen, and Binh-Son Hua. Poodle: Improving few-shot learning via\npenalizing out-of-distribution samples. Advances in Neural Information Processing Systems, 34:23942â€“23955, 2021.\n[38] Davis Wertheimer, Luming Tang, and Bharath Hariharan. Few-shot classification with feature map reconstruction networks. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern recognition, pages 8012â€“8021, 2021.\n[39] Huaxi Huang, Junjie Zhang, Jian Zhang, Qiang Wu, and Chang Xu. Ptn: A poisson transfer network for semi-supervised few-shot learning.\nIn Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 1602â€“1609, 2021.\n[40] Yassir Bendou, Yuqing Hu, Raphael Lafargue, Giulia Lioi, Bastien Pasdeloup, StÃ©phane Pateux, and Vincent Gripon.\nEasyâ€”ensemble\naugmented-shot-y-shaped learning: State-of-the-art few-shot classification with simple components. Journal of Imaging, 8(7):179, 2022.\n[41] Michalis Lazarou, Tania Stathaki, and Yannis Avrithis. Iterative label cleaning for transductive and semi-supervised few-shot learning. In\nProceedings of the IEEE/CVF International Conference on Computer Vision, pages 8751â€“8760, 2021.\n[42] Peyman Bateni, Raghav Goyal, Vaden Masrani, Frank Wood, and Leonid Sigal. Improved few-shot visual classification. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14493â€“14502, 2020.\n[43] Jun Cheng, Fusheng Hao, Fengxiang He, Liu Liu, and Qieshi Zhang. Mixer-based semantic spread for few-shot learning. IEEE Transactions\non Multimedia, 25:191â€“202, 2021.\n[44] Huayi Ji, Linkai Luo, and Hong Peng. Brave: A cascaded generative model with sample attention for robust few shot image classification.\nNeurocomputing, 610:128585, 2024.\n[45] Priyanka Ganesan, Senthil Kumar Jagatheesaperumal, Mohammad Mehedi Hassan, Francesco Pupo, and Giancarlo Fortino. Few-shot image\nclassification using graph neural network with fine-grained feature descriptors. Neurocomputing, 610:128448, 2024.\n[46] Dalong Chen, Jianjia Zhang, Wei-Shi Zheng, and Ruixuan Wang. Featwalk: Enhancing few-shot classification through local view leveraging.\nIn Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1019â€“1027, 2024.\n[47] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.\n[48] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n[49] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.\n[50] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan. Supervision exists everywhere:\nA data efficient contrastive language-image pre-training paradigm. arXiv preprint arXiv:2110.05208, 2021.\n: Preprint submitted to Elsevier\nPage 12 of 13\n\n\n[51] Hao Zhu and Piotr Koniusz.\nTransductive few-shot learning with prototype-based label propagation by iterative graph refinement.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 23996â€“24006, 2023.\n[52] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.\nDecision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084â€“15097,\n2021.\n[53] Michael I Jordan. Serial order: A parallel distributed processing approach. In Advances in psychology, volume 121, pages 471â€“495. Elsevier,\n1997.\n[54] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735â€“1780, 1997.\n[55] Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of\nthe IEEE, 86(11):2278â€“2324, 1998.\n[56] Christian Simon, Piotr Koniusz, Richard Nock, and Mehrtash Harandi. Adaptive subspaces for few-shot learning. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, pages 4136â€“4145, 2020.\n[57] James Requeima, Jonathan Gordon, John Bronskill, Sebastian Nowozin, and Richard E Turner. Fast and flexible multi-task classification using\nconditional neural adaptive processes. Advances in Neural Information Processing Systems, 32, 2019.\n[58] Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Learning to model the tail. Advances in neural information processing systems, 30,\n2017.\n: Preprint submitted to Elsevier\nPage 13 of 13\n"
}