{
  "filename": "1805.10002v5.pdf",
  "num_pages": 14,
  "pages": [
    "Published as a conference paper at ICLR 2019\nLEARNING TO PROPAGATE LABELS: TRANSDUCTIVE\nPROPAGATION NETWORK FOR FEW-SHOT LEARNING\nYanbin Liu1∗, Juho Lee2,3, Minseop Park3, Saehoon Kim3, Eunho Yang3,4,\nSung Ju Hwang3,4 & Yi Yang1,5†\n1CAI, University of Technology Sydney, 2University of Oxford\n3AITRICS, 4KAIST, 5Baidu Research\ncsyanbin@gmail.com, juho.lee@stats.ox.ac.uk,\n{mike_seop, shkim}@aitrics.com, {eunhoy, sjhwang82}@kaist.ac.kr,\nYi.Yang@uts.edu.au\nABSTRACT\nThe goal of few-shot learning is to learn a classiﬁer that generalizes well even\nwhen trained with a limited number of training instances per class. The recently\nintroduced meta-learning approaches tackle this problem by learning a generic\nclassiﬁer across a large number of multiclass classiﬁcation tasks and generalizing\nthe model to a new task. Yet, even with such meta-learning, the low-data problem\nin the novel classiﬁcation task still remains. In this paper, we propose Transductive\nPropagation Network (TPN), a novel meta-learning framework for transductive\ninference that classiﬁes the entire test set at once to alleviate the low-data problem.\nSpeciﬁcally, we propose to learn to propagate labels from labeled instances to\nunlabeled test instances, by learning a graph construction module that exploits the\nmanifold structure in the data. TPN jointly learns both the parameters of feature\nembedding and the graph construction in an end-to-end manner. We validate TPN\non multiple benchmark datasets, on which it largely outperforms existing few-shot\nlearning approaches and achieves the state-of-the-art results.\n1\nINTRODUCTION\nRecent breakthroughs in deep learning (Krizhevsky et al., 2012; Simonyan and Zisserman, 2015; He\net al., 2016) highly rely on the availability of large amounts of labeled data. However, this reliance\non large data increases the burden of data collection, which hinders its potential applications to the\nlow-data regime where the labeled data is rare and difﬁcult to gather. On the contrary, humans have\nthe ability to recognize new objects after observing only one or few instances (Lake et al., 2011).\nFor example, children can generalize the concept of “apple” after given a single instance of it. This\nsigniﬁcant gap between human and deep learning has reawakened the research interest on few-shot\nlearning (Vinyals et al., 2016; Snell et al., 2017; Finn et al., 2017; Ravi and Larochelle, 2017; Lee\nand Choi, 2018; Xu et al., 2017; Wang et al., 2018).\nFew-shot learning aims to learn a classiﬁer that generalizes well with a few examples of each of\nthese classes. Traditional techniques such as ﬁne-tuning (Jia et al., 2014) that work well with deep\nlearning models would severely overﬁt on this task (Vinyals et al., 2016; Finn et al., 2017), since a\nsingle or only a few labeled instances would not accurately represent the true data distribution and\nwill result in learning classiﬁers with high variance, which will not generalize well to new data.\nIn order to solve this overﬁtting problem, Vinyals et al. (2016) proposed a meta-learning strat-\negy which learns over diverse classiﬁcation tasks over large number of episodes rather than only\non the target classiﬁcation task. In each episode, the algorithm learns the embedding of the few\nlabeled examples (the support set), which can be used to predict classes for the unlabeled points\n(the query set) by distance in the embedding space. The purpose of episodic training is to mimic\n∗This work was done when Yanbin Liu was an intern at AITRICS.\n†Part of this work was done when Yi Yang was visiting Baidu Research during his Professional Experience\nProgram.\n1\narXiv:1805.10002v5  [cs.LG]  8 Feb 2019\n",
    "Published as a conference paper at ICLR 2019\n...\nTransductive Propagation Network\nTask 1\n...\nTask 2\nTest Task\nunlabeled\nlabeled\nMeta-train\nMeta-test\n!\n!\nFigure 1: A conceptual illustration of our transductive meta-learning framework, where lines between nodes\nrepresent graph connections and their colors represent the potential direction of label propagation. The neigh-\nborhood graph is episodic-wisely trained for transductive inference.\nthe real test environment containing few-shot support set and unlabeled query set. The consistency\nbetween training and test environment alleviates the distribution gap and improves generalization.\nThis episodic meta-learning strategy, due to its generalization performance, has been adapted by\nmany follow-up work on few-shot learning. Finn et al. (2017) learned a good initialization that can\nadapt quickly to the target tasks. Snell et al. (2017) used episodes to train a good representation and\npredict classes by computing Euclidean distance with respect to class prototypes.\nAlthough episodic strategy is an effective approach for few-shot learning as it aims at generalizing\nto unseen classiﬁcation tasks, the fundamental difﬁculty with learning with scarce data remains for\na novel classiﬁcation task. One way to achieve larger improvements with limited amount of training\ndata is to consider relationships between instances in the test set and thus predicting them as a whole,\nwhich is referred to as transduction, or transductive inference. In previous work (Joachims, 1999;\nZhou et al., 2004; Vapnik, 1999), transductive inference has shown to outperform inductive methods\nwhich predict test examples one by one, especially in small training sets. One popular approach for\ntransduction is to construct a network on both the labeled and unlabeled data, and propagate labels\nbetween them for joint prediction. However, the main challenge with such label propagation (and\ntransduction) is that the label propagation network is often obtained without consideration of the\nmain task, since it is not possible to learn them at the test time.\nYet, with the meta-learning by episodic training, we can learn the label propagation network as the\nquery examples sampled from the training set can be used to simulate the real test set for transductive\ninference. Motivated by this ﬁnding, we propose Transductive Propagation Network (TPN) to deal\nwith the low-data problem. Instead of applying the inductive inference, we utilize the entire query\nset for transductive inference (see Figure 1). Speciﬁcally, we ﬁrst map the input to an embedding\nspace using a deep neural network. Then a graph construction module is proposed to exploit the\nmanifold structure of the novel class space using the union of support set and query set. According\nto the graph structure, iterative label propagation is applied to propagate labels from the support\nset to the query set and ﬁnally leads to a closed-form solution. With the propagated scores and\nground truth labels of the query set, we compute the cross-entropy loss with respect to the feature\nembedding and graph construction parameters. Finally, all parameters can be updated end-to-end\nusing backpropagation.\nThe main contribution of this work is threefold.\n• To the best of our knowledge, we are the ﬁrst to model transductive inference explicitly\nin few-shot learning. Although Nichol et al. (2018) experimented with a transductive set-\nting, they only share information between test examples by batch normalization rather than\ndirectly proposing a transductive model.\n• In transductive inference, we propose to learn to propagate labels between data instances\nfor unseen classes via episodic meta-learning. This learned label propagation graph is\n2\n",
    "Published as a conference paper at ICLR 2019\nshown to signiﬁcantly outperform naive heuristic-based label propagation methods (Zhou\net al., 2004).\n• We evaluate our approach on two benchmark datasets for few-shot learning, namely\nminiImageNet and tieredImageNet. The experimental results show that our Transductive\nPropagation Network outperforms the state-of-the-art methods on both datasets. Also, with\nsemi-supervised learning, our algorithm achieves even higher performance, outperforming\nall semi-supervised few-shot learning baselines.\n2\nRELATED WORK\nMeta-learning\nIn\nrecent\nworks,\nfew-shot\nlearning\noften\nfollows\nthe\nidea\nof\nmeta-\nlearning (Schmidhuber, 1987; Thrun and Pratt, 2012). Meta-learning tries to optimize over batches\nof tasks rather than batches of data points. Each task corresponds to a learning problem, obtaining\ngood performance on these tasks helps to learn quickly and generalize well to the target few-shot\nproblem without suffering from overﬁtting. The well-known MAML approach (Finn et al., 2017)\naims to ﬁnd more transferable representations with sensitive parameters. A ﬁrst-order meta-learning\napproach named Reptile is proposed by Nichol et al. (2018). It is closely related to ﬁrst-order\nMAML but does not need a training-test split for each task. Compared with the above methods,\nour algorithm has a closed-form solution for label propagation on the query points, thus avoiding\ngradient computation in the inner updateand usually performs more efﬁciently.\nEmbedding and metric learning approaches\nAnother category of few-shot learning approach\naims to optimize the transferable embedding using metric learning approaches.\nMatching net-\nworks (Vinyals et al., 2016) produce a weighted nearest neighbor classiﬁer given the support set\nand adjust feature embedding according to the performance on the query set. Prototypical net-\nworks (Snell et al., 2017) ﬁrst compute a class’s prototype to be the mean of its support set in the\nembedding space. Then the transferability of feature embedding is evaluated by ﬁnding the near-\nest class prototype for embedded query points. An extension of prototypical networks is proposed\nin Ren et al. (2018) to deal with semi-supervised few-shot learning. Relation Network (Sung et al.,\n2018) learns to learn a deep distance metric to compare a small number of images within episodes.\nOur proposed method is similar to these approaches in the sense that we all focus on learning deep\nembeddings with good generalization ability. However, our algorithm assumes a transductive set-\nting, in which we utilize the union of support set and query set to exploit the manifold structure of\nnovel class space by using episodic-wise parameters.\nTransduction\nThe setting of transductive inference was ﬁrst introduced by Vapnik (Vapnik, 1999).\nTransductive Support Vector Machines (TSVMs) (Joachims, 1999) is a margin-based classiﬁcation\nmethod that minimizes errors of a particular test set. It shows substantial improvements over induc-\ntive methods, especially for small training sets. Another category of transduction methods involves\ngraph-based methods (Zhou et al., 2004; Wang and Zhang, 2006; Rohrbach et al., 2013; Fu et al.,\n2015). Label propagation is used in Zhou et al. (2004) to transfer labels from labeled to unlabeled\ndata instances guided by the weighted graph. Label propagation is sensitive to variance parameter\nσ, so Linear Neighborhood Propagation (LNP) (Wang and Zhang, 2006) constructs approximated\nLaplacian matrix to avoid this issue. In Zhu and Ghahramani (2002), minimum spanning tree heuris-\ntic and entropy minimization are used to learn the parameter σ. In all these prior work, the graph\nconstruction is done on a pre-deﬁned feature space using manually selected hyperparamters since\nit is not possible to learn them at test time. Our approach, on the other hand, is able to learn the\ngraph construction network since it is a meta-learning framework with episodic training, where at\neach episode we simulate the test set with a subset of the training set.\nIn few-shot learning, Nichol et al. (2018) experiments with a transductive setting and shows im-\nprovements. However, they only share information between test examples via batch normaliza-\ntion (Ioffe and Szegedy, 2015) rather than explicitly model the transductive setting as in our algo-\nrithm.\n3\n",
    "Published as a conference paper at ICLR 2019\nCNN \nCNN \nSupport\nQuery\nf'\nf'(X)\nσ\ny\ngφ\nWij = exp\n!\n−1\n2d(f'(xi)\nσi\n, f'(xj)\nσj\n)\n\"\nQuery\nLabel\nLOSS\n!\n!\n!\nGraph Construction\nFeature Embedding\nLabel Propagation\nLoss\n!\nFigure 2: The overall framework of our algorithm in which the manifold structure of the entire query set helps to\nlearn better decision boundary. The proposed algorithm is composed of four components: feature embedding,\ngraph construction, label propagation, and loss generation.\n3\nMAIN APPROACH\nIn this section, we introduce the proposed algorithm that utilizes the manifold structure of the given\nfew-shot classiﬁcation task to improve the performance.\n3.1\nPROBLEM DEFINITION\nWe follow the episodic paradigm (Vinyals et al., 2016) that effectively trains a meta-learner for few-\nshot classiﬁcation tasks, which is commonly employed in various literature (Snell et al., 2017; Finn\net al., 2017; Nichol et al., 2018; Sung et al., 2018; Mishra et al., 2018). Given a relatively large\nlabeled dataset with a set of classes Ctrain, the objective of this setting is to train classiﬁers for an\nunseen set of novel classes Ctest, for which only a few labeled examples are available.\nSpeciﬁcally, in each episode, a small subset of N classes are sampled from Ctrain to construct a\nsupport set and a query set. The support set contains K examples from each of the N classes (i.e.,\nN-way K-shot setting) denoted as S = {(x1, y1), (x2, y2), . . . , (xN×K, yN×K)}, while the query\nset Q = {(x∗\n1, y∗\n1), (x∗\n2, y∗\n2), . . . , (x∗\nT , y∗\nT )} includes different examples from the same N classes.\nHere, the support set S in each episode serves as the labeled training set on which the model is\ntrained to minimize the loss of its predictions for the query set Q. This procedure mimics training\nclassiﬁers for Ctest and goes episode by episode until convergence.\nMeta-learning implemented by the episodic training reasonably performs well to few-shot classi-\nﬁcation tasks. Yet, due to the lack of labeled instances (K is usually very small) in the support\nset, we observe that a reliable classiﬁer is still difﬁcult to be obtained. This motivates us to con-\nsider a transductive setting that utilizes the whole query set for the prediction rather than predicting\neach example independently. Taking the entire query set into account, we can alleviate the low-data\nproblem and provide more reliable generalization property.\n3.2\nTRANSDUCTIVE PROPAGATION NETWORK (TPN)\nWe introduce Transductive Propagation Network (TPN) illustrated in Figure 2, which consists of\nfour components: feature embedding with a convolutional neural network; graph construction that\nproduces example-wise parameters to exploit the manifold structure; label propagation that spreads\nlabels from the support set S to the query set Q; a loss generation step that computes a cross-\nentropy loss between propagated labels and the ground-truths on Q to jointly train all parameters in\nthe framework.\n3.2.1\nFEATURE EMBEDDING\nWe employ a convolutional neural network fϕ to extract features of an input xi, where fϕ(xi; ϕ)\nrefers to the feature map and ϕ indicates a parameter of the network. Despite the generality, we adopt\nthe same architecture used in several recent works (Snell et al., 2017; Sung et al., 2018; Vinyals et\nal., 2016). By doing so, we can provide more fair comparisons in the experiments, highlighting\nthe effects of transductive approach. The network is made up of four convolutional blocks where\neach block begins with a 2D convolutional layer with a 3 × 3 kernel and ﬁlter size of 64. Each\n4\n",
    "Published as a conference paper at ICLR 2019\nconvolutional layer is followed by a batch-normalization layer (Ioffe and Szegedy, 2015), a ReLU\nnonlinearity and a 2 × 2 max-pooling layer. We use the same embedding function fϕ for both the\nsupport set S and the query set Q.\n3.2.2\nGRAPH CONSTRUCTION\nManifold learning (Chung and Graham, 1997; Zhou et al., 2004; Yang et al., 2016) discovers the\nembedded low-dimensional subspace in the data, where it is critical to choose an appropriate neigh-\nborhood graph. A common choice is Gaussian similarity function:\nWij = exp\n\u0012\n−d(xi, xj)\n2σ2\n\u0013\n,\n(1)\nwhere d(·, ·) is a distance measure (e.g., Euclidean distance) and σ is the length scale parameter.\nThe neighborhood structure behaves differently with respect to various σ, which means that it needs\nto carefully select the optimal σ for the best performance of label propagation (Wang and Zhang,\n2006; Zhu and Ghahramani, 2002). In addition, we observe that there is no principled way to tune the\nscale parameter in meta-learning framework, though there exist some heuristics for dimensionalty\nreduction methods (Zelnik-Manor and Perona, 2004; Sugiyama, 2007).\nExample-wise length-scale parameter\nTo obtain a proper neighborhood graph in meta-learning,\nwe propose a graph construction module built on the union set of support set and query set: S ∪Q.\nThis module is composed of a convolutional neural network gφ which takes the feature map fϕ(xi)\nfor xi ∈S ∪Q to produce an example-wise length-scale parameter σi = gφ(fϕ(xi)). Note that the\nscale parameter is determined example-wisely and learned in an episodic training procedure, which\nadapts well to different tasks and makes it suitable for few-shot learning. With the example-wise σi,\nour similarity function is then deﬁned as follows:\nWij = exp\n\u0012\n−1\n2d\n\u0010fϕ(xi)\nσi\n, fϕ(xj)\nσj\n\u0011\u0013\n(2)\nwhere W ∈R(N×K+T )×(N×K+T ) for all instances in S ∪Q. We only keep the k-max values\nin each row of W to construct a k-nearest neighbour graph. Then we apply the normalized graph\nLaplacians (Chung and Graham, 1997) on W, that is, S = D−1/2WD−1/2, where D is a diagonal\nmatrix with its (i, i)-value to be the sum of the i-th row of W.\nf'(xi)\nf'(xj)\nσi\nσj\nWij = exp\n✓\n−1\n2d(f'(xi)\nσi\n, f'(xj)\nσj\n)\n◆\n3 ⇥3 conv\nBatchNorm\nReLU\n2 ⇥2 max-pool\n3 ⇥3 conv\nBatchNorm\nReLU\n2 ⇥2 max-pool\ngφ\nFC layer 1\nFC layer 2\nFigure 3: Detailed architecture of the graph construction module, in which the length-scale parameter is\nexample-wisely determined.\nGraph construction structure\nThe structure of the proposed graph construction module is shown\nin Figure 3. It is composed of two convolutional blocks and two fully-connected layers, where\neach block contains a 3-by-3 convolution, batch normalization, ReLU activation, followed by 2-\nby-2 max pooling. The number of ﬁlters in each convolutional block is 64 and 1, respectively. To\nprovide an example-wise scaling parameter, the activation map from the second convolutional block\nis transformed into a scalar by two fully-connected layers in which the number of neurons is 8 and\n1, respectively.\nGraph construction in each episode\nWe follow the episodic paradigm for few-shot meta-learner\ntraining. This means that the graph is individually constructed for each task in each episode, as\nshown in Figure 1. Typically, in 5-way 5-shot training, N = 5, K = 5, T = 75, the dimension of\nW is only 100 × 100, which is quite efﬁcient.\n5\n",
    "Published as a conference paper at ICLR 2019\n3.2.3\nLABEL PROPAGATION\nWe now describe how to get predictions for the query set Q using label propagation, before the last\ncross-entropy loss step. Let F denote the set of (N × K + T) × N matrix with nonnegative entries.\nWe deﬁne a label matrix Y ∈F with Yij = 1 if xi is from the support set and labeled as yi = j,\notherwise Yij = 0. Starting from Y , label propagation iteratively determines the unknown labels of\ninstances in the union set S ∪Q according to the graph structure using the following formulation:\nFt+1 = αSFt + (1 −α)Y ,\n(3)\nwhere Ft ∈F denotes the predicted labels at the timestamp t, S denotes the normalized weight, and\nα ∈(0, 1) controls the amount of propagated information. It is well known that the sequence {Ft}\nhas a closed-form solution as follows:\nF ∗= (I −αS)−1Y ,\n(4)\nwhere I is the identity matrix (Zhou et al., 2004). We directly utilize this result for the label propa-\ngation, making a whole episodic meta-learning procedure more efﬁcient in practice.\nTime complexity\nMatrix inversion originally takes O(n3) time complexity, which is inefﬁcient\nfor large n. However, in our setting, n = N × K + T (80 for 1-shot and 100 for 5-shot) is very\nsmall. Moreover, there is plenty of prior work on the scalability and efﬁciency of label propagation,\nsuch as Liang and Li (2018); Fujiwara and Irie (2014), which can extend our work to large-scale\ndata. More discussions are presented in A.4\n3.2.4\nCLASSIFICATION LOSS GENERATION\nThe objective of this step is to compute the classiﬁcation loss between the predictions of the union\nof support and query set via label propagation and the ground-truths. We compute the cross-entropy\nloss between predicted scores F ∗and ground-truth labels from S ∪Q to learn all parameters in an\nend-to-end fashion, where F ∗is converted to probabilistic score using softmax:\nP( ˜yi = j|xi) =\nexp(F ∗\nij)\nPN\nj=1 exp(F ∗\nij)\n.\n(5)\nHere, ˜yi denotes the ﬁnal predicted label for ith instance in the union of support and query set and\nF ∗\nij denotes the jth component of predicted label from label propagation. Then the loss function is\ncomputed as:\nJ(ϕ, φ) =\nN×K+T\nX\ni=1\nN\nX\nj=1\n−I(yi == j) log(P( ˜yi = j|xi)) ,\n(6)\nwhere yi means the ground-truth label of xi and I(b) is an indicator function, I(b) = 1 if b is true\nand 0 otherwise.\nNote that in Equation (6), the loss is dependent on two set of parameters ϕ, φ (even though the\ndependency is implicit through F ∗\nij). All these parameters are jointly updated by the episodic training\nin an end-to-end manner.\n4\nEXPERIMENTS\nWe evaluate and compare our TPN with state-of-the-art approaches on two datasets, i.e.,\nminiImageNet (Ravi and Larochelle, 2017) and tieredImageNet (Ren et al., 2018). The former\nis the most popular few-shot learning benchmark and the latter is a much larger dataset released\nrecently for few-shot learning.\n4.1\nDATASETS\nminiImageNet. The miniImageNet dataset is a collection of Imagenet (Krizhevsky et al., 2012) for\nfew-shot image recognition. It is composed of 100 classes randomly selected from Imagenet with\neach class containing 600 examples. In order to directly compare with state-of-the-art algorithms for\n6\n",
    "Published as a conference paper at ICLR 2019\nfew-shot learning, we rely on the class splits used by Ravi and Larochelle (2017), which includes\n64 classes for training, 16 for validation, and 20 for test. All images are resized to 84 × 84 pixels.\ntieredImageNet. Similar to miniImageNet , tieredImageNet (Ren et al., 2018) is also a subset of\nImagenet (Krizhevsky et al., 2012), but it has a larger number of classes from ILSVRC-12 (608\nclasses rather than 100 for miniImageNet). Different from miniImageNet, it has a hierarchical struc-\nture of broader categories corresponding to high-level nodes in Imagenet. The top hierarchy has\n34 categories, which are divided into 20 training (351 classes), 6 validation (97 classes) and 8 test\n(160 classes) categories. The average number of examples in each class is 1281. This high-level\nsplit strategy ensures that the training classes are distinct from the test classes semantically. This is\na more challenging and realistic few-shot setting since there is no assumption that training classes\nshould be similar to test classes. Similarly, all images are resized to 84 × 84 pixels.\n4.2\nEXPERIMENTAL SETUP\nFor fair comparison with other methods, we adopt a widely-used CNN (Finn et al., 2017; Snell et\nal., 2017) as the feature embedding function fϕ (Section 3.2.1). The hyper-parameter k of k-nearest\nneighbour graph (Section 3.2.2) is set to 20 and α of label propagation is set to 0.99, as suggested in\nZhou et al. (2004).\nFollowing Snell et al. (2017), we adopt the episodic training procedure, i.e, we sample a set of\nN-way K-shot training tasks to mimic the N-way K-shot test problems. Moreover, Snell et al.\n(2017) proposed a “Higher Way ” training strategy which used more training classes in each episode\nthan test case. However, we ﬁnd that it is beneﬁcial to train with more examples than test phase\n(Appendix A.1). This is denoted as “Higher Shot” in our experiments. For 1-shot and 5-shot test\nproblem, we adopt 5-shot and 10-shot training respectively. In all settings, the query number is set\nto 15 and the performance are averaged over 600 randomly generated episodes from the test set.\nAll our models were trained with Adam (Kingma and Ba, 2015) and an initial learning rate\nof 10−3.\nFor miniImageNet, we cut the learning rate in half every 10, 000 episodes and for\ntieredImageNet, we cut the learning rate every 25, 000 episodes. The reason for larger decay step is\nthat tieredImageNet has more classes and more examples in each class which needs larger training\niterations. We ran the training process until the validation loss reached a plateau.\n4.3\nFEW-SHOT LEARNING RESULTS\nWe compare our method with several state-of-the-art approaches in various settings. Even though\nthe transductive method has never been used explicitly, batch normalization layer was used transduc-\ntively to share information between test examples. For example, in Finn et al. (2017); Nichol et al.\n(2018), they use the query batch statistics rather than global BN parameters for the prediction, which\nleads to performance gain in the query set. Besides, we propose two simple transductive methods\nas baselines that explicitly utilize the query set. First, we propose the MAML+Transduction with\nslight modiﬁcation of loss function to: J (θ) = PT\ni=1 yi log P(byi|xi) + PN×K+T\ni,j=1\nWij∥byi −byj∥2\n2\nfor transductive inference. The additional term serves as transductive regularization. Second, the\nnaive heuristic-based label propagation methods (Zhou et al., 2004) is proposed to explicitly model\nthe transductive inference.\nExperimental results are shown in Table 1 and Table2. Transductive batch normalization methods\ntend to perform better than pure inductive methods except for the “Higher Way” PROTO NET. Label\npropagation without learning to propagate outperforms other baseline methods in most cases, which\nveriﬁes the necessity of transduction. The proposed TPN achieves the state-of-the-art results and\nsurpasses all the others with a large margin even when the model is trained with regular shots. When\n“Higher Shot” is applied, the performance of TPN continues to improve especially for 1-shot case.\nThis conﬁrms that our model effectively ﬁnds the episodic-wise manifold structure of test examples\nthrough learning to construct the graph for label propagation.\nAnother observation is that the advantages of 5-shot classiﬁcation is less signiﬁcant than that of 1-\nshot case. For example, in 5-way miniImageNet , the absolute improvement of TPN over published\nstate-of-the-art is 4.13% for 1-shot and 1.66% for 5-shot. To further investigate this, we experi-\nmented 5-way k-shot (k = 1, 2, · · · , 10) experiments. The results are shown in Figure 4. Our TPN\nperforms consistently better than other methods with varying shots. Moreover, it can be seen that\n7\n",
    "Published as a conference paper at ICLR 2019\nTable 1: Few-shot classiﬁcation accuracies on miniImageNet. All results are averaged over 600 test episodes.\nTop results are highlighted.\n5-way Acc\n10-way Acc\nModel\nTransduction\n1-shot\n5-shot\n1-shot\n5-shot\nMAML (Finn et al., 2017)\nBN\n48.70\n63.11\n31.27\n46.92\nMAML+Transduction\nYes\n50.83\n66.19\n31.83\n48.23\nReptile (Nichol et al., 2018)\nNo\n47.07\n62.74\n31.10\n44.66\nReptile + BN (Nichol et al., 2018)\nBN\n49.97\n65.99\n32.00\n47.60\nPROTO NET (Snell et al., 2017)\nNo\n46.14\n65.77\n32.88\n49.29\nPROTO NET (Higher Way) (Snell et al., 2017)\nNo\n49.42\n68.20\n34.61\n50.09\nRELATION NET (Sung et al., 2018)\nBN\n51.38\n67.07\n34.86\n47.94\nLabel Propagation\nYes\n52.31\n68.18\n35.23\n51.24\nTPN\nYes\n53.75\n69.43\n36.62\n52.32\nTPN (Higher Shot)\nYes\n55.51\n69.86\n38.44\n52.77\n* “Higher Way” means using more classes in training episodes. “Higher Shot” means using more shots\nin training episodes. “BN” means information is shared among test examples using batch normalization.\n† Due to space limitation, we report the accuracy with 95% conﬁdence intervals in Appendix.\nTable 2: Few-shot classiﬁcation accuracies on tieredImageNet.\nAll results are averaged over 600 test\nepisodes. Top results are highlighted.\n5-way Acc\n10-way Acc\nModel\nTransduction\n1-shot\n5-shot\n1-shot\n5-shot\nMAML (Finn et al., 2017)\nBN\n51.67\n70.30\n34.44\n53.32\nMAML + Transduction\nYes\n53.23\n70.83\n34.78\n54.67\nReptile (Nichol et al., 2018)\nNo\n48.97\n66.47\n33.67\n48.04\nReptile + BN (Nichol et al., 2018)\nBN\n52.36\n71.03\n35.32\n51.98\nPROTO NET (Snell et al., 2017)\nNo\n48.58\n69.57\n37.35\n57.83\nPROTO NET (Higher Way) (Snell et al., 2017)\nNo\n53.31\n72.69\n38.62\n58.32\nRELATION NET (Sung et al., 2018)\nBN\n54.48\n71.31\n36.32\n58.05\nLabel Propagation\nYes\n55.23\n70.43\n39.39\n57.89\nTPN\nYes\n57.53\n72.85\n40.93\n59.17\nTPN (Higher Shot)\nYes\n59.91\n73.30\n44.80\n59.44\n* “Higher Way” means using more classes in training episodes. “Higher Shot” means using more shots\nin training episodes. “BN” means information is shared among test examples using batch normalization.\n† Due to space limitation, we report the accuracy with 95% conﬁdence intervals in Appendix.\nTPN outperforms other methods with a large margin in lower shots. With the shot increase, the\nadvantage of transduction narrows since more labelled data are used. This ﬁnding agrees with the\nresults in TSVM (Joachims, 1999): when more training data are available, the bonus of transductive\ninference will be decreased.\n4.4\nCOMPARISON WITH SEMI-SUPERVISED FEW-SHOT LEARNING\nTable 3: Semi-supervised comparison on miniImageNet.\nModel\n1-shot\n5-shot\n1-shot w/D\n5-shot w/D\nSoft k-Means (Ren et al., 2018)\n50.09\n64.59\n48.70\n63.55\nSoft k-Means+Cluster (Ren et al., 2018)\n49.03\n63.08\n48.86\n61.27\nMasked Soft k-Means (Ren et al., 2018)\n50.41\n64.39\n49.04\n62.96\nTPN-semi\n52.78\n66.42\n50.43\n64.95\n* “w/D” means with distraction. In this setting, many of the unlabelled data are from the\nso-called distraction classes , which is different from the classes of labelled data.\n† Due to space limitation, we report the accuracy with 95% conﬁdence intervals in\nAppendix.\nThe main difference of traditional semi-supervised learning and transduction is the source of un-\nlabeled data. Transductive methods directly use test set as unlabeled data while semi-supervised\nlearning usually has an extra unlabeled set. In order to compare with semi-supervised methods,\n8\n",
    "Published as a conference paper at ICLR 2019\nFigure 4: 5-way performance with various training/test shots.\nTable 4: Semi-supervised comparison on tieredImageNet.\nModel\n1-shot\n5-shot\n1-shot w/D\n5-shot w/D\nSoft k-Means (Ren et al., 2018)\n51.52\n70.25\n49.88\n68.32\nSoft k-Means+Cluster (Ren et al., 2018)\n51.85\n69.42\n51.36\n67.56\nMasked Soft k-Means (Ren et al., 2018)\n52.39\n69.88\n51.38\n69.08\nTPN-semi\n55.74\n71.01\n53.45\n69.93\n* “w/D” means with distraction. In this setting, many of the unlabelled data are from the\nso-called distraction classes , which is different from the classes of labelled data.\n† Due to space limitation, we report the accuracy with 95% conﬁdence intervals in\nAppendix.\nwe propose a semi-supervised version of TPN, named TPN-semi, which classiﬁes one test example\neach time by propagating labels from the labeled set and extra unlabeled set.\nWe use miniImageNet and tieredImageNet with the labeled/unlabeled data split proposed by Ren\net al. (2018). Speciﬁcally, they split the images of each class into disjoint labeled and unlabeled\nsets. For miniImageNet, the ratio of labeled/unlabeled data is 40% and 60% in each class. Likewise,\nthe ratio is 10% and 90% for tieredImageNet. All semi-supervised methods (including TPN-semi)\nsample support/query data from the labeled set (e.g, 40% from miniImageNet) and sample unlabeled\ndata from the unlabeled sets (e.g, 60% from miniImageNet). In addition, there is a more challenging\nsituation where many unlabelled examples from other distractor classes (different from labelled\nclasses).\nFollowing Ren et al. (2018), we report the average accuracy over 10 random labeled/unlabeled splits\nand the uncertainty computed in standard error. Results are shown in Table 3 and Table 4. It can\nbe seen that TPN-semi outperforms all other algorithms with a large margin, especially for 1-shot\ncase. Although TPN is originally designed to perform transductive inference, we show that it can\nbe successfully adapted to semi-supervised learning tasks with little modiﬁcation. In certain cases\nwhere we can not get all test data, the TPN-semi can be used as an effective alternative algorithm.\n5\nCONCLUSION\nIn this work, we proposed the transductive setting for few-shot learning. Our proposed approach,\nnamely Transductive Propagation Network (TPN), utilizes the entire test set for transductive infer-\nence. Speciﬁcally, our approach is composed of four steps: feature embedding, graph construction,\nlabel propagation, and loss computation. Graph construction is a key step that produces example-\nwise parameters to exploit the manifold structure in each episode. In our method, all parameters\nare learned end-to-end using cross-entropy loss with respect to the ground truth labels and the\nprediction scores in the query set. We obtained the state-of-the-art results on miniImageNet and\ntieredImageNet. Also, the semi-supervised adaptation of our algorithm achieved higher results than\nother semi-supervised methods. In future work, we are going to explore the episodic-wise distance\nmetric rather than only using example-wise parameters for the Euclidean distance.\n9\n",
    "Published as a conference paper at ICLR 2019\nACKNOWLEDGMENTS\nSaehoon Kim, Minseop Park, and Eunho Yang were supported by Samsung Research Funding &\nIncubation Center of Samsung Electronics under Project Number SRFC-IT1702-15. Yanbin Liu and\nYi Yang are in part supported by AWS Cloud Credits for Research.\nREFERENCES\nFan RK Chung and Fan Chung Graham. Spectral graph theory. American Mathematical Soc., 1997.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep\nnetworks. In International Conference on Machine Learning, pages 1126–1135, 2017.\nYanwei Fu, Timothy M Hospedales, Tao Xiang, and Shaogang Gong. Transductive multi-view zero-shot learn-\ning. IEEE transactions on pattern analysis and machine intelligence, 37(11):2332–2345, 2015.\nYasuhiro Fujiwara and Go Irie. Efﬁcient label propagation. In International Conference on Machine Learning,\npages 784–792, 2014.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\nComputer Vision and Pattern Recognition, pages 770–778, 2016.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. In International Conference on Machine Learning, pages 448–456, 2015.\nYangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadar-\nrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. In ACM Interna-\ntional Conference on Multimedia, pages 675–678. ACM, 2014.\nThorsten Joachims. Transductive inference for text classiﬁcation using support vector machines. In Interna-\ntional Conference on Machine Learning, volume 99, pages 200–209, 1999.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference\non Learning Representations (ICLR), volume 5, 2015.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural\nnetworks. In Advances in Neural Information Processing Systems, pages 1097–1105, 2012.\nBrenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. One shot learning of simple visual\nconcepts. In Conference of the Cognitive Science Society, volume 33, 2011.\nYoonho Lee and Seungjin Choi. Gradient-based meta-learning with learned layerwise metric and subspace. In\nInternational Conference on Machine Learning, pages 2933–2942, 2018.\nDe-Ming Liang and Yu-Feng Li. Lightweight label propagation for large-scale network data. In IJCAI, pages\n3421–3427, 2018.\nBauerïij ˇN Matthias, Rojas-Carulla Mateo, Jakub Bartłomiej ´Swi ˛atkowski, Bernhard Schölkopf, and Richard E\nTurner. Discriminative k-shot learning using probabilistic models. arXiv preprint arXiv:1706.00326, 2017.\nNikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-learner. In\nInternational Conference on Learning Representations, 2018.\nTsendsuren Munkhdalai, Xingdi Yuan, Soroush Mehri, and Adam Trischler. Rapid adaptation with condition-\nally shifted neurons. In International Conference on Machine Learning, pages 3661–3670, 2018.\nAlex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-order meta-learning algorithms. arXiv preprint\narXiv:1803.02999, 2018.\nBoris N Oreshkin, Alexandre Lacoste, and Pau Rodriguez. Tadam: Task dependent adaptive metric for im-\nproved few-shot learning. In Advances in Neural Information Processing Systems, 2018.\nSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. International Conference\non Learning Representations, 2017.\nMengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum, Hugo\nLarochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classiﬁcation. International\nConference on Learning Representations, 2018.\n10\n",
    "Published as a conference paper at ICLR 2019\nMarcus Rohrbach, Sandra Ebert, and Bernt Schiele. Transfer learning in a transductive setting. In Advances in\nNeural Information Processing Systems, pages 46–54, 2013.\nJürgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: the\nmeta-meta-... hook. PhD thesis, Technische Universität München, 1987.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.\nInternational Conference on Learning Representations, 2015.\nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Advances in\nNeural Information Processing Systems, pages 4080–4090, 2017.\nMasashi Sugiyama. Dimensionality reduction of multimodal labeled data by local ﬁsher discriminant analysis.\nJournal of Machine Learning Research, 8:1027–1061, 2007.\nFlood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales. Learning to\ncompare: Relation network for few-shot learning. In Computer Vision and Pattern Recognition, 2018.\nSebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media, 2012.\nVladimir Naumovich Vapnik. An overview of statistical learning theory. IEEE transactions on neural networks,\n10(5):988–999, 1999.\nOriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning.\nIn Advances in Neural Information Processing Systems, pages 3630–3638, 2016.\nFei Wang and Changshui Zhang. Label propagation through linear neighborhoods. In International Conference\non Machine Learning, pages 985–992. ACM, 2006.\nYu-Xiong Wang, Ross Girshick, Martial Hebert, and Bharath Hariharan. Low-shot learning from imaginary\ndata. In Computer Vision and Pattern Recognition, 2018.\nZhongwen Xu, Linchao Zhu, and Yi Yang. Few-shot object recognition from machine-labeled web images. In\nComputer Vision and Pattern Recognition, 2017.\nZhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with graph em-\nbeddings. In International Conference on Machine Learning, pages 40–48, 2016.\nLihi Zelnik-Manor and Pietro Perona. Self-tuning spectral clustering. In Advances in Neural Information\nProcessing Systems, 2004.\nDenny Zhou, Olivier Bousquet, Thomas N Lal, Jason Weston, and Bernhard Schölkopf. Learning with local\nand global consistency. In Advances in Neural Information Processing Systems, pages 321–328, 2004.\nXiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label propagation. Tech.\nRep., Technical Report CMU-CALD-02–107, Carnegie Mellon University, 2002.\n11\n",
    "Published as a conference paper at ICLR 2019\nA\nABLATION STUDY\nIn this section, we performed several ablation studies with respect to training shots and query number.\nA.1\nTRAINING SHOTS\nFigure 5: Model performance with different training shots. The x-axis indicates the number of shots in training,\nand the y-axis indicates 5-way test accuracy for 1-shot and 5-shot. Error bars indicate 95% conﬁdence intervals\nas computed over 600 test episodes.\nA.2\nQUERY NUMBER\nTable 5: Accuracy with various query numbers\nminiImageNet 1-shot\n5\n10\n15\n20\n25\n30\nTrain=15\n52.29\n52.95\n53.75\n53.92\n54.57\n54.47\nTest=15\n53.53\n53.72\n53.75\n52.79\n52.84\n52.47\nTrain=Test\n51.94\n53.47\n53.75\n54.00\n53.59\n53.32\nminiImageNet 5-shot\n5\n10\n15\n20\n25\n30\nTrain=15\n66.97\n69.30\n69.43\n69.92\n70.54\n70.36\nTest=15\n68.50\n68.85\n69.43\n69.26\n69.12\n68.89\nTrain=Test\n67.55\n69.22\n69.43\n69.85\n70.11\n69.94\nAt ﬁrst, we designed three experiments to study the inﬂuence of the query number in both training and test\nphase: (1) ﬁx training query to 15; (2) ﬁx test query to 15; (3) training query equals test query. The results\nare shown in Table 5. Some conclusions can be drawn from this experiment: (1) When training query is ﬁxed,\nincreasing the test query will lead to the performance gain. Moreover, even a small test query (e.g., 5) can\nyield good performance; (2) When test query is ﬁxed, the performance is relatively stable with various training\nquery numbers; (3) If the query number of training matches test, the performance can also be improved with\nincreasing number.\nA.3\nRESULTS ON RESNET\nIn this paper, we use a 4-layer neural network structure as described in Section 3.2.1 to make a fair comparison.\nCurrently, there are two common network architectures in few-shot learning: 4-layer ConvNets (e.g., Finn et\nal. (2017); Snell et al. (2017); Sung et al. (2018)) and 12-layer ResNet (e.g., Mishra et al. (2018); Munkhdalai\net al. (2018); Matthias et al. (2017); Oreshkin et al. (2018)). Our method belongs to the ﬁrst one, which\ncontains much fewer layers than the ResNet setting. Thus, it is more reasonable to compare algorithms such as\nTADAM (Oreshkin et al., 2018) with ResNet version of our method. To make this comparison, we implemented\nour algorithm with ResNet architecture on miniImagenet dataset and show the results in Table 6.\nIt can be seen that we beat TADAM for 1-shot setting. For 5-shot, we outperform all other recent high-\nperformance methods except for TADAM.\n12\n",
    "Published as a conference paper at ICLR 2019\nTable 6: ResNet results on miniImageNet\nMethod\n1-shot\n5-shot\nSNAIL (Mishra et al., 2018)\n55.71\n68.88\nadaResNet (Munkhdalai et al., 2018)\n56.88\n71.94\nDiscriminative k-shot (Matthias et al., 2017)\n56.30\n73.90\nTADAM (Oreshkin et al., 2018)\n58.50\n76.70\nTPN\n59.46\n75.65\nA.4\nCLOSED-FORM SOLUTION VS ITERATIVE UPDATES\nThere is a potential concern that the closed-form solution of label propagation can not scale to large-scale\nmatrix. We relieve this concern from two aspects. On one hand, the few-shot learning problem assumes that\ntraining examples in each class is quite small (only 1 or 5). In this situation, Eq 3 and the closed-form version\ncan be efﬁciently solved, since the dimension of S is only 80 × 80 (5-way, 1-shot, 15-query) or 100 × 100\n(5-way, 5-shot, 15-query). On the other hand, there are plenty of prior work on the scalability and efﬁciency\nof label propagation, such as Liang and Li (2018); Fujiwara and Irie (2014), which can extend our work to\nlarge-scale data.\nFurthermore, on miniImagenet, we performed iterative optimization and got 53.05/68.75 for 1-shot/5-shot ex-\nperiments with only 10 steps. This is slightly worse than closed-form version (53.75/69.43). We attribute this\nslightly worse accuracy to the inaccurate computation and unstable gradients caused by multiple step iterations.\nA.5\nACCURACY WITH 95% CONFIDENCE INTERVALS\nTable 7: Few-shot classiﬁcation accuracies on miniImageNet. All results are averaged over 600 test episodes\nand are reported with 95% conﬁdence intervals. Top results are highlighted.\n5-way Acc\n10-way Acc\nModel\nTransduction\n1-shot\n5-shot\n1-shot\n5-shot\nMAML\nBN\n48.70±1.84\n63.11±0.92\n31.27±1.15\n46.92±1.25\nMAML+Transduction\nYes\n50.83±1.85\n66.19±1.85\n31.83±0.45\n48.23±1.28\nReptile\nNo\n47.07±0.26\n62.74±0.37\n31.10±0.28\n44.66±0.30\nReptile + BN\nBN\n49.97±0.32\n65.99±0.58\n32.00±0.27\n47.60±0.32\nPROTO NET\nNo\n46.14±0.77\n65.77±0.70\n32.88±0.47\n49.29±0.42\nPROTO NET (Higher Way)\nNo\n49.42±0.78\n68.20±0.66\n34.61±0.46\n50.09±0.44\nRELATION NET\nBN\n51.38±0.82\n67.07±0.69\n34.86±0.48\n47.94±0.42\nLabel Propagation\nYes\n52.31±0.85\n68.18±0.67\n35.23±0.51\n51.24±0.43\nTPN\nYes\n53.75±0.86\n69.43±0.67\n36.62±0.50\n52.32±0.44\nTPN (Higher Shot)\nYes\n55.51±0.86\n69.86±0.65\n38.44±0.49\n52.77±0.45\n* “Higher Way” means using more classes in training episodes. “Higher Shot” means using more shots in\ntraining episodes. “BN” means information is shared among test examples using batch normalization.\n13\n",
    "Published as a conference paper at ICLR 2019\nTable 8: Few-shot classiﬁcation accuracies on tieredImageNet. All results are averaged over 600 test episodes\nand are reported with 95% conﬁdence intervals. Top results are highlighted.\n5-way Acc\n10-way Acc\nModel\nTransduction\n1-shot\n5-shot\n1-shot\n5-shot\nMAML\nBN\n51.67±1.81\n70.30±1.75\n34.44±1.19\n53.32±1.33\nMAML + Transduction\nYes\n53.23±1.85\n70.83±1.78\n34.78±1.18\n54.67±1.26\nReptile\nNo\n48.97±0.21\n66.47±0.21\n33.67±0.28\n48.04±0.30\nReptile + BN\nBN\n52.36±0.23\n71.03±0.22\n35.32±0.28\n51.98±0.32\nPROTO NET\nNo\n48.58±0.87\n69.57±0.75\n37.35±0.56\n57.83±0.55\nPROTO NET (Higher Way)\nNo\n53.31±0.89\n72.69±0.74\n38.62±0.57\n58.32±0.55\nRELATION NET\nBN\n54.48±0.93\n71.31±0.78\n36.32±0.62\n58.05±0.59\nLabel Propagation\nYes\n55.23±0.96\n70.43±0.76\n39.39±0.60\n57.89±0.55\nTPN\nYes\n57.53±0.96\n72.85±0.74\n40.93±0.61\n59.17±0.52\nTPN (Higher Shot)\nYes\n59.91±0.94\n73.30±0.75\n44.80±0.62\n59.44±0.51\n* “Higher Way” means using more classes in training episodes. “Higher Shot” means using more shots in\ntraining episodes. “BN” means information is shared among test examples using batch normalization.\nTable 9: Semi-supervised comparison on miniImageNet.\nModel\n1-shot\n5-shot\n1-shot w/D\n5-shot w/D\nSoft k-Means\n50.09±0.45\n64.59±0.28\n48.70±0.32\n63.55±0.28\nSoft k-Means+Cluster\n49.03±0.24\n63.08±0.18\n48.86±0.32\n61.27±0.24\nMasked Soft k-Means\n50.41±0.31\n64.39±0.24\n49.04±0.31\n62.96±0.14\nTPN-semi\n52.78±0.27\n66.42±0.21\n50.43±0.84\n64.95±0.73\n* “w/D” means with distraction. In this setting, many of the unlabelled data are from\nthe so-called distraction classes , which is different from the classes of labelled data.\nTable 10: Semi-supervised comparison on tieredImageNet.\nModel\n1-shot\n5-shot\n1-shot w/D\n5-shot w/D\nSoft k-Means\n51.52±0.36\n70.25±0.31\n49.88±0.52\n68.32±0.22\nSoft k-Means+Cluster\n51.85±0.25\n69.42±0.17\n51.36±0.31\n67.56±0.10\nMasked Soft k-Means\n52.39±0.44\n69.88±0.20\n51.38±0.38\n69.08±0.25\nTPN-semi\n55.74±0.29\n71.01±0.23\n53.45±0.93\n69.93±0.80\n* “w/D” means with distraction. In this setting, many of the unlabelled data are from\nthe so-called distraction classes , which is different from the classes of labelled data.\n14\n"
  ],
  "full_text": "Published as a conference paper at ICLR 2019\nLEARNING TO PROPAGATE LABELS: TRANSDUCTIVE\nPROPAGATION NETWORK FOR FEW-SHOT LEARNING\nYanbin Liu1∗, Juho Lee2,3, Minseop Park3, Saehoon Kim3, Eunho Yang3,4,\nSung Ju Hwang3,4 & Yi Yang1,5†\n1CAI, University of Technology Sydney, 2University of Oxford\n3AITRICS, 4KAIST, 5Baidu Research\ncsyanbin@gmail.com, juho.lee@stats.ox.ac.uk,\n{mike_seop, shkim}@aitrics.com, {eunhoy, sjhwang82}@kaist.ac.kr,\nYi.Yang@uts.edu.au\nABSTRACT\nThe goal of few-shot learning is to learn a classiﬁer that generalizes well even\nwhen trained with a limited number of training instances per class. The recently\nintroduced meta-learning approaches tackle this problem by learning a generic\nclassiﬁer across a large number of multiclass classiﬁcation tasks and generalizing\nthe model to a new task. Yet, even with such meta-learning, the low-data problem\nin the novel classiﬁcation task still remains. In this paper, we propose Transductive\nPropagation Network (TPN), a novel meta-learning framework for transductive\ninference that classiﬁes the entire test set at once to alleviate the low-data problem.\nSpeciﬁcally, we propose to learn to propagate labels from labeled instances to\nunlabeled test instances, by learning a graph construction module that exploits the\nmanifold structure in the data. TPN jointly learns both the parameters of feature\nembedding and the graph construction in an end-to-end manner. We validate TPN\non multiple benchmark datasets, on which it largely outperforms existing few-shot\nlearning approaches and achieves the state-of-the-art results.\n1\nINTRODUCTION\nRecent breakthroughs in deep learning (Krizhevsky et al., 2012; Simonyan and Zisserman, 2015; He\net al., 2016) highly rely on the availability of large amounts of labeled data. However, this reliance\non large data increases the burden of data collection, which hinders its potential applications to the\nlow-data regime where the labeled data is rare and difﬁcult to gather. On the contrary, humans have\nthe ability to recognize new objects after observing only one or few instances (Lake et al., 2011).\nFor example, children can generalize the concept of “apple” after given a single instance of it. This\nsigniﬁcant gap between human and deep learning has reawakened the research interest on few-shot\nlearning (Vinyals et al., 2016; Snell et al., 2017; Finn et al., 2017; Ravi and Larochelle, 2017; Lee\nand Choi, 2018; Xu et al., 2017; Wang et al., 2018).\nFew-shot learning aims to learn a classiﬁer that generalizes well with a few examples of each of\nthese classes. Traditional techniques such as ﬁne-tuning (Jia et al., 2014) that work well with deep\nlearning models would severely overﬁt on this task (Vinyals et al., 2016; Finn et al., 2017), since a\nsingle or only a few labeled instances would not accurately represent the true data distribution and\nwill result in learning classiﬁers with high variance, which will not generalize well to new data.\nIn order to solve this overﬁtting problem, Vinyals et al. (2016) proposed a meta-learning strat-\negy which learns over diverse classiﬁcation tasks over large number of episodes rather than only\non the target classiﬁcation task. In each episode, the algorithm learns the embedding of the few\nlabeled examples (the support set), which can be used to predict classes for the unlabeled points\n(the query set) by distance in the embedding space. The purpose of episodic training is to mimic\n∗This work was done when Yanbin Liu was an intern at AITRICS.\n†Part of this work was done when Yi Yang was visiting Baidu Research during his Professional Experience\nProgram.\n1\narXiv:1805.10002v5  [cs.LG]  8 Feb 2019\n\n\nPublished as a conference paper at ICLR 2019\n...\nTransductive Propagation Network\nTask 1\n...\nTask 2\nTest Task\nunlabeled\nlabeled\nMeta-train\nMeta-test\n!\n!\nFigure 1: A conceptual illustration of our transductive meta-learning framework, where lines between nodes\nrepresent graph connections and their colors represent the potential direction of label propagation. The neigh-\nborhood graph is episodic-wisely trained for transductive inference.\nthe real test environment containing few-shot support set and unlabeled query set. The consistency\nbetween training and test environment alleviates the distribution gap and improves generalization.\nThis episodic meta-learning strategy, due to its generalization performance, has been adapted by\nmany follow-up work on few-shot learning. Finn et al. (2017) learned a good initialization that can\nadapt quickly to the target tasks. Snell et al. (2017) used episodes to train a good representation and\npredict classes by computing Euclidean distance with respect to class prototypes.\nAlthough episodic strategy is an effective approach for few-shot learning as it aims at generalizing\nto unseen classiﬁcation tasks, the fundamental difﬁculty with learning with scarce data remains for\na novel classiﬁcation task. One way to achieve larger improvements with limited amount of training\ndata is to consider relationships between instances in the test set and thus predicting them as a whole,\nwhich is referred to as transduction, or transductive inference. In previous work (Joachims, 1999;\nZhou et al., 2004; Vapnik, 1999), transductive inference has shown to outperform inductive methods\nwhich predict test examples one by one, especially in small training sets. One popular approach for\ntransduction is to construct a network on both the labeled and unlabeled data, and propagate labels\nbetween them for joint prediction. However, the main challenge with such label propagation (and\ntransduction) is that the label propagation network is often obtained without consideration of the\nmain task, since it is not possible to learn them at the test time.\nYet, with the meta-learning by episodic training, we can learn the label propagation network as the\nquery examples sampled from the training set can be used to simulate the real test set for transductive\ninference. Motivated by this ﬁnding, we propose Transductive Propagation Network (TPN) to deal\nwith the low-data problem. Instead of applying the inductive inference, we utilize the entire query\nset for transductive inference (see Figure 1). Speciﬁcally, we ﬁrst map the input to an embedding\nspace using a deep neural network. Then a graph construction module is proposed to exploit the\nmanifold structure of the novel class space using the union of support set and query set. According\nto the graph structure, iterative label propagation is applied to propagate labels from the support\nset to the query set and ﬁnally leads to a closed-form solution. With the propagated scores and\nground truth labels of the query set, we compute the cross-entropy loss with respect to the feature\nembedding and graph construction parameters. Finally, all parameters can be updated end-to-end\nusing backpropagation.\nThe main contribution of this work is threefold.\n• To the best of our knowledge, we are the ﬁrst to model transductive inference explicitly\nin few-shot learning. Although Nichol et al. (2018) experimented with a transductive set-\nting, they only share information between test examples by batch normalization rather than\ndirectly proposing a transductive model.\n• In transductive inference, we propose to learn to propagate labels between data instances\nfor unseen classes via episodic meta-learning. This learned label propagation graph is\n2\n\n\nPublished as a conference paper at ICLR 2019\nshown to signiﬁcantly outperform naive heuristic-based label propagation methods (Zhou\net al., 2004).\n• We evaluate our approach on two benchmark datasets for few-shot learning, namely\nminiImageNet and tieredImageNet. The experimental results show that our Transductive\nPropagation Network outperforms the state-of-the-art methods on both datasets. Also, with\nsemi-supervised learning, our algorithm achieves even higher performance, outperforming\nall semi-supervised few-shot learning baselines.\n2\nRELATED WORK\nMeta-learning\nIn\nrecent\nworks,\nfew-shot\nlearning\noften\nfollows\nthe\nidea\nof\nmeta-\nlearning (Schmidhuber, 1987; Thrun and Pratt, 2012). Meta-learning tries to optimize over batches\nof tasks rather than batches of data points. Each task corresponds to a learning problem, obtaining\ngood performance on these tasks helps to learn quickly and generalize well to the target few-shot\nproblem without suffering from overﬁtting. The well-known MAML approach (Finn et al., 2017)\naims to ﬁnd more transferable representations with sensitive parameters. A ﬁrst-order meta-learning\napproach named Reptile is proposed by Nichol et al. (2018). It is closely related to ﬁrst-order\nMAML but does not need a training-test split for each task. Compared with the above methods,\nour algorithm has a closed-form solution for label propagation on the query points, thus avoiding\ngradient computation in the inner updateand usually performs more efﬁciently.\nEmbedding and metric learning approaches\nAnother category of few-shot learning approach\naims to optimize the transferable embedding using metric learning approaches.\nMatching net-\nworks (Vinyals et al., 2016) produce a weighted nearest neighbor classiﬁer given the support set\nand adjust feature embedding according to the performance on the query set. Prototypical net-\nworks (Snell et al., 2017) ﬁrst compute a class’s prototype to be the mean of its support set in the\nembedding space. Then the transferability of feature embedding is evaluated by ﬁnding the near-\nest class prototype for embedded query points. An extension of prototypical networks is proposed\nin Ren et al. (2018) to deal with semi-supervised few-shot learning. Relation Network (Sung et al.,\n2018) learns to learn a deep distance metric to compare a small number of images within episodes.\nOur proposed method is similar to these approaches in the sense that we all focus on learning deep\nembeddings with good generalization ability. However, our algorithm assumes a transductive set-\nting, in which we utilize the union of support set and query set to exploit the manifold structure of\nnovel class space by using episodic-wise parameters.\nTransduction\nThe setting of transductive inference was ﬁrst introduced by Vapnik (Vapnik, 1999).\nTransductive Support Vector Machines (TSVMs) (Joachims, 1999) is a margin-based classiﬁcation\nmethod that minimizes errors of a particular test set. It shows substantial improvements over induc-\ntive methods, especially for small training sets. Another category of transduction methods involves\ngraph-based methods (Zhou et al., 2004; Wang and Zhang, 2006; Rohrbach et al., 2013; Fu et al.,\n2015). Label propagation is used in Zhou et al. (2004) to transfer labels from labeled to unlabeled\ndata instances guided by the weighted graph. Label propagation is sensitive to variance parameter\nσ, so Linear Neighborhood Propagation (LNP) (Wang and Zhang, 2006) constructs approximated\nLaplacian matrix to avoid this issue. In Zhu and Ghahramani (2002), minimum spanning tree heuris-\ntic and entropy minimization are used to learn the parameter σ. In all these prior work, the graph\nconstruction is done on a pre-deﬁned feature space using manually selected hyperparamters since\nit is not possible to learn them at test time. Our approach, on the other hand, is able to learn the\ngraph construction network since it is a meta-learning framework with episodic training, where at\neach episode we simulate the test set with a subset of the training set.\nIn few-shot learning, Nichol et al. (2018) experiments with a transductive setting and shows im-\nprovements. However, they only share information between test examples via batch normaliza-\ntion (Ioffe and Szegedy, 2015) rather than explicitly model the transductive setting as in our algo-\nrithm.\n3\n\n\nPublished as a conference paper at ICLR 2019\nCNN \nCNN \nSupport\nQuery\nf'\nf'(X)\nσ\ny\ngφ\nWij = exp\n!\n−1\n2d(f'(xi)\nσi\n, f'(xj)\nσj\n)\n\"\nQuery\nLabel\nLOSS\n!\n!\n!\nGraph Construction\nFeature Embedding\nLabel Propagation\nLoss\n!\nFigure 2: The overall framework of our algorithm in which the manifold structure of the entire query set helps to\nlearn better decision boundary. The proposed algorithm is composed of four components: feature embedding,\ngraph construction, label propagation, and loss generation.\n3\nMAIN APPROACH\nIn this section, we introduce the proposed algorithm that utilizes the manifold structure of the given\nfew-shot classiﬁcation task to improve the performance.\n3.1\nPROBLEM DEFINITION\nWe follow the episodic paradigm (Vinyals et al., 2016) that effectively trains a meta-learner for few-\nshot classiﬁcation tasks, which is commonly employed in various literature (Snell et al., 2017; Finn\net al., 2017; Nichol et al., 2018; Sung et al., 2018; Mishra et al., 2018). Given a relatively large\nlabeled dataset with a set of classes Ctrain, the objective of this setting is to train classiﬁers for an\nunseen set of novel classes Ctest, for which only a few labeled examples are available.\nSpeciﬁcally, in each episode, a small subset of N classes are sampled from Ctrain to construct a\nsupport set and a query set. The support set contains K examples from each of the N classes (i.e.,\nN-way K-shot setting) denoted as S = {(x1, y1), (x2, y2), . . . , (xN×K, yN×K)}, while the query\nset Q = {(x∗\n1, y∗\n1), (x∗\n2, y∗\n2), . . . , (x∗\nT , y∗\nT )} includes different examples from the same N classes.\nHere, the support set S in each episode serves as the labeled training set on which the model is\ntrained to minimize the loss of its predictions for the query set Q. This procedure mimics training\nclassiﬁers for Ctest and goes episode by episode until convergence.\nMeta-learning implemented by the episodic training reasonably performs well to few-shot classi-\nﬁcation tasks. Yet, due to the lack of labeled instances (K is usually very small) in the support\nset, we observe that a reliable classiﬁer is still difﬁcult to be obtained. This motivates us to con-\nsider a transductive setting that utilizes the whole query set for the prediction rather than predicting\neach example independently. Taking the entire query set into account, we can alleviate the low-data\nproblem and provide more reliable generalization property.\n3.2\nTRANSDUCTIVE PROPAGATION NETWORK (TPN)\nWe introduce Transductive Propagation Network (TPN) illustrated in Figure 2, which consists of\nfour components: feature embedding with a convolutional neural network; graph construction that\nproduces example-wise parameters to exploit the manifold structure; label propagation that spreads\nlabels from the support set S to the query set Q; a loss generation step that computes a cross-\nentropy loss between propagated labels and the ground-truths on Q to jointly train all parameters in\nthe framework.\n3.2.1\nFEATURE EMBEDDING\nWe employ a convolutional neural network fϕ to extract features of an input xi, where fϕ(xi; ϕ)\nrefers to the feature map and ϕ indicates a parameter of the network. Despite the generality, we adopt\nthe same architecture used in several recent works (Snell et al., 2017; Sung et al., 2018; Vinyals et\nal., 2016). By doing so, we can provide more fair comparisons in the experiments, highlighting\nthe effects of transductive approach. The network is made up of four convolutional blocks where\neach block begins with a 2D convolutional layer with a 3 × 3 kernel and ﬁlter size of 64. Each\n4\n\n\nPublished as a conference paper at ICLR 2019\nconvolutional layer is followed by a batch-normalization layer (Ioffe and Szegedy, 2015), a ReLU\nnonlinearity and a 2 × 2 max-pooling layer. We use the same embedding function fϕ for both the\nsupport set S and the query set Q.\n3.2.2\nGRAPH CONSTRUCTION\nManifold learning (Chung and Graham, 1997; Zhou et al., 2004; Yang et al., 2016) discovers the\nembedded low-dimensional subspace in the data, where it is critical to choose an appropriate neigh-\nborhood graph. A common choice is Gaussian similarity function:\nWij = exp\n\u0012\n−d(xi, xj)\n2σ2\n\u0013\n,\n(1)\nwhere d(·, ·) is a distance measure (e.g., Euclidean distance) and σ is the length scale parameter.\nThe neighborhood structure behaves differently with respect to various σ, which means that it needs\nto carefully select the optimal σ for the best performance of label propagation (Wang and Zhang,\n2006; Zhu and Ghahramani, 2002). In addition, we observe that there is no principled way to tune the\nscale parameter in meta-learning framework, though there exist some heuristics for dimensionalty\nreduction methods (Zelnik-Manor and Perona, 2004; Sugiyama, 2007).\nExample-wise length-scale parameter\nTo obtain a proper neighborhood graph in meta-learning,\nwe propose a graph construction module built on the union set of support set and query set: S ∪Q.\nThis module is composed of a convolutional neural network gφ which takes the feature map fϕ(xi)\nfor xi ∈S ∪Q to produce an example-wise length-scale parameter σi = gφ(fϕ(xi)). Note that the\nscale parameter is determined example-wisely and learned in an episodic training procedure, which\nadapts well to different tasks and makes it suitable for few-shot learning. With the example-wise σi,\nour similarity function is then deﬁned as follows:\nWij = exp\n\u0012\n−1\n2d\n\u0010fϕ(xi)\nσi\n, fϕ(xj)\nσj\n\u0011\u0013\n(2)\nwhere W ∈R(N×K+T )×(N×K+T ) for all instances in S ∪Q. We only keep the k-max values\nin each row of W to construct a k-nearest neighbour graph. Then we apply the normalized graph\nLaplacians (Chung and Graham, 1997) on W, that is, S = D−1/2WD−1/2, where D is a diagonal\nmatrix with its (i, i)-value to be the sum of the i-th row of W.\nf'(xi)\nf'(xj)\nσi\nσj\nWij = exp\n✓\n−1\n2d(f'(xi)\nσi\n, f'(xj)\nσj\n)\n◆\n3 ⇥3 conv\nBatchNorm\nReLU\n2 ⇥2 max-pool\n3 ⇥3 conv\nBatchNorm\nReLU\n2 ⇥2 max-pool\ngφ\nFC layer 1\nFC layer 2\nFigure 3: Detailed architecture of the graph construction module, in which the length-scale parameter is\nexample-wisely determined.\nGraph construction structure\nThe structure of the proposed graph construction module is shown\nin Figure 3. It is composed of two convolutional blocks and two fully-connected layers, where\neach block contains a 3-by-3 convolution, batch normalization, ReLU activation, followed by 2-\nby-2 max pooling. The number of ﬁlters in each convolutional block is 64 and 1, respectively. To\nprovide an example-wise scaling parameter, the activation map from the second convolutional block\nis transformed into a scalar by two fully-connected layers in which the number of neurons is 8 and\n1, respectively.\nGraph construction in each episode\nWe follow the episodic paradigm for few-shot meta-learner\ntraining. This means that the graph is individually constructed for each task in each episode, as\nshown in Figure 1. Typically, in 5-way 5-shot training, N = 5, K = 5, T = 75, the dimension of\nW is only 100 × 100, which is quite efﬁcient.\n5\n\n\nPublished as a conference paper at ICLR 2019\n3.2.3\nLABEL PROPAGATION\nWe now describe how to get predictions for the query set Q using label propagation, before the last\ncross-entropy loss step. Let F denote the set of (N × K + T) × N matrix with nonnegative entries.\nWe deﬁne a label matrix Y ∈F with Yij = 1 if xi is from the support set and labeled as yi = j,\notherwise Yij = 0. Starting from Y , label propagation iteratively determines the unknown labels of\ninstances in the union set S ∪Q according to the graph structure using the following formulation:\nFt+1 = αSFt + (1 −α)Y ,\n(3)\nwhere Ft ∈F denotes the predicted labels at the timestamp t, S denotes the normalized weight, and\nα ∈(0, 1) controls the amount of propagated information. It is well known that the sequence {Ft}\nhas a closed-form solution as follows:\nF ∗= (I −αS)−1Y ,\n(4)\nwhere I is the identity matrix (Zhou et al., 2004). We directly utilize this result for the label propa-\ngation, making a whole episodic meta-learning procedure more efﬁcient in practice.\nTime complexity\nMatrix inversion originally takes O(n3) time complexity, which is inefﬁcient\nfor large n. However, in our setting, n = N × K + T (80 for 1-shot and 100 for 5-shot) is very\nsmall. Moreover, there is plenty of prior work on the scalability and efﬁciency of label propagation,\nsuch as Liang and Li (2018); Fujiwara and Irie (2014), which can extend our work to large-scale\ndata. More discussions are presented in A.4\n3.2.4\nCLASSIFICATION LOSS GENERATION\nThe objective of this step is to compute the classiﬁcation loss between the predictions of the union\nof support and query set via label propagation and the ground-truths. We compute the cross-entropy\nloss between predicted scores F ∗and ground-truth labels from S ∪Q to learn all parameters in an\nend-to-end fashion, where F ∗is converted to probabilistic score using softmax:\nP( ˜yi = j|xi) =\nexp(F ∗\nij)\nPN\nj=1 exp(F ∗\nij)\n.\n(5)\nHere, ˜yi denotes the ﬁnal predicted label for ith instance in the union of support and query set and\nF ∗\nij denotes the jth component of predicted label from label propagation. Then the loss function is\ncomputed as:\nJ(ϕ, φ) =\nN×K+T\nX\ni=1\nN\nX\nj=1\n−I(yi == j) log(P( ˜yi = j|xi)) ,\n(6)\nwhere yi means the ground-truth label of xi and I(b) is an indicator function, I(b) = 1 if b is true\nand 0 otherwise.\nNote that in Equation (6), the loss is dependent on two set of parameters ϕ, φ (even though the\ndependency is implicit through F ∗\nij). All these parameters are jointly updated by the episodic training\nin an end-to-end manner.\n4\nEXPERIMENTS\nWe evaluate and compare our TPN with state-of-the-art approaches on two datasets, i.e.,\nminiImageNet (Ravi and Larochelle, 2017) and tieredImageNet (Ren et al., 2018). The former\nis the most popular few-shot learning benchmark and the latter is a much larger dataset released\nrecently for few-shot learning.\n4.1\nDATASETS\nminiImageNet. The miniImageNet dataset is a collection of Imagenet (Krizhevsky et al., 2012) for\nfew-shot image recognition. It is composed of 100 classes randomly selected from Imagenet with\neach class containing 600 examples. In order to directly compare with state-of-the-art algorithms for\n6\n\n\nPublished as a conference paper at ICLR 2019\nfew-shot learning, we rely on the class splits used by Ravi and Larochelle (2017), which includes\n64 classes for training, 16 for validation, and 20 for test. All images are resized to 84 × 84 pixels.\ntieredImageNet. Similar to miniImageNet , tieredImageNet (Ren et al., 2018) is also a subset of\nImagenet (Krizhevsky et al., 2012), but it has a larger number of classes from ILSVRC-12 (608\nclasses rather than 100 for miniImageNet). Different from miniImageNet, it has a hierarchical struc-\nture of broader categories corresponding to high-level nodes in Imagenet. The top hierarchy has\n34 categories, which are divided into 20 training (351 classes), 6 validation (97 classes) and 8 test\n(160 classes) categories. The average number of examples in each class is 1281. This high-level\nsplit strategy ensures that the training classes are distinct from the test classes semantically. This is\na more challenging and realistic few-shot setting since there is no assumption that training classes\nshould be similar to test classes. Similarly, all images are resized to 84 × 84 pixels.\n4.2\nEXPERIMENTAL SETUP\nFor fair comparison with other methods, we adopt a widely-used CNN (Finn et al., 2017; Snell et\nal., 2017) as the feature embedding function fϕ (Section 3.2.1). The hyper-parameter k of k-nearest\nneighbour graph (Section 3.2.2) is set to 20 and α of label propagation is set to 0.99, as suggested in\nZhou et al. (2004).\nFollowing Snell et al. (2017), we adopt the episodic training procedure, i.e, we sample a set of\nN-way K-shot training tasks to mimic the N-way K-shot test problems. Moreover, Snell et al.\n(2017) proposed a “Higher Way ” training strategy which used more training classes in each episode\nthan test case. However, we ﬁnd that it is beneﬁcial to train with more examples than test phase\n(Appendix A.1). This is denoted as “Higher Shot” in our experiments. For 1-shot and 5-shot test\nproblem, we adopt 5-shot and 10-shot training respectively. In all settings, the query number is set\nto 15 and the performance are averaged over 600 randomly generated episodes from the test set.\nAll our models were trained with Adam (Kingma and Ba, 2015) and an initial learning rate\nof 10−3.\nFor miniImageNet, we cut the learning rate in half every 10, 000 episodes and for\ntieredImageNet, we cut the learning rate every 25, 000 episodes. The reason for larger decay step is\nthat tieredImageNet has more classes and more examples in each class which needs larger training\niterations. We ran the training process until the validation loss reached a plateau.\n4.3\nFEW-SHOT LEARNING RESULTS\nWe compare our method with several state-of-the-art approaches in various settings. Even though\nthe transductive method has never been used explicitly, batch normalization layer was used transduc-\ntively to share information between test examples. For example, in Finn et al. (2017); Nichol et al.\n(2018), they use the query batch statistics rather than global BN parameters for the prediction, which\nleads to performance gain in the query set. Besides, we propose two simple transductive methods\nas baselines that explicitly utilize the query set. First, we propose the MAML+Transduction with\nslight modiﬁcation of loss function to: J (θ) = PT\ni=1 yi log P(byi|xi) + PN×K+T\ni,j=1\nWij∥byi −byj∥2\n2\nfor transductive inference. The additional term serves as transductive regularization. Second, the\nnaive heuristic-based label propagation methods (Zhou et al., 2004) is proposed to explicitly model\nthe transductive inference.\nExperimental results are shown in Table 1 and Table2. Transductive batch normalization methods\ntend to perform better than pure inductive methods except for the “Higher Way” PROTO NET. Label\npropagation without learning to propagate outperforms other baseline methods in most cases, which\nveriﬁes the necessity of transduction. The proposed TPN achieves the state-of-the-art results and\nsurpasses all the others with a large margin even when the model is trained with regular shots. When\n“Higher Shot” is applied, the performance of TPN continues to improve especially for 1-shot case.\nThis conﬁrms that our model effectively ﬁnds the episodic-wise manifold structure of test examples\nthrough learning to construct the graph for label propagation.\nAnother observation is that the advantages of 5-shot classiﬁcation is less signiﬁcant than that of 1-\nshot case. For example, in 5-way miniImageNet , the absolute improvement of TPN over published\nstate-of-the-art is 4.13% for 1-shot and 1.66% for 5-shot. To further investigate this, we experi-\nmented 5-way k-shot (k = 1, 2, · · · , 10) experiments. The results are shown in Figure 4. Our TPN\nperforms consistently better than other methods with varying shots. Moreover, it can be seen that\n7\n\n\nPublished as a conference paper at ICLR 2019\nTable 1: Few-shot classiﬁcation accuracies on miniImageNet. All results are averaged over 600 test episodes.\nTop results are highlighted.\n5-way Acc\n10-way Acc\nModel\nTransduction\n1-shot\n5-shot\n1-shot\n5-shot\nMAML (Finn et al., 2017)\nBN\n48.70\n63.11\n31.27\n46.92\nMAML+Transduction\nYes\n50.83\n66.19\n31.83\n48.23\nReptile (Nichol et al., 2018)\nNo\n47.07\n62.74\n31.10\n44.66\nReptile + BN (Nichol et al., 2018)\nBN\n49.97\n65.99\n32.00\n47.60\nPROTO NET (Snell et al., 2017)\nNo\n46.14\n65.77\n32.88\n49.29\nPROTO NET (Higher Way) (Snell et al., 2017)\nNo\n49.42\n68.20\n34.61\n50.09\nRELATION NET (Sung et al., 2018)\nBN\n51.38\n67.07\n34.86\n47.94\nLabel Propagation\nYes\n52.31\n68.18\n35.23\n51.24\nTPN\nYes\n53.75\n69.43\n36.62\n52.32\nTPN (Higher Shot)\nYes\n55.51\n69.86\n38.44\n52.77\n* “Higher Way” means using more classes in training episodes. “Higher Shot” means using more shots\nin training episodes. “BN” means information is shared among test examples using batch normalization.\n† Due to space limitation, we report the accuracy with 95% conﬁdence intervals in Appendix.\nTable 2: Few-shot classiﬁcation accuracies on tieredImageNet.\nAll results are averaged over 600 test\nepisodes. Top results are highlighted.\n5-way Acc\n10-way Acc\nModel\nTransduction\n1-shot\n5-shot\n1-shot\n5-shot\nMAML (Finn et al., 2017)\nBN\n51.67\n70.30\n34.44\n53.32\nMAML + Transduction\nYes\n53.23\n70.83\n34.78\n54.67\nReptile (Nichol et al., 2018)\nNo\n48.97\n66.47\n33.67\n48.04\nReptile + BN (Nichol et al., 2018)\nBN\n52.36\n71.03\n35.32\n51.98\nPROTO NET (Snell et al., 2017)\nNo\n48.58\n69.57\n37.35\n57.83\nPROTO NET (Higher Way) (Snell et al., 2017)\nNo\n53.31\n72.69\n38.62\n58.32\nRELATION NET (Sung et al., 2018)\nBN\n54.48\n71.31\n36.32\n58.05\nLabel Propagation\nYes\n55.23\n70.43\n39.39\n57.89\nTPN\nYes\n57.53\n72.85\n40.93\n59.17\nTPN (Higher Shot)\nYes\n59.91\n73.30\n44.80\n59.44\n* “Higher Way” means using more classes in training episodes. “Higher Shot” means using more shots\nin training episodes. “BN” means information is shared among test examples using batch normalization.\n† Due to space limitation, we report the accuracy with 95% conﬁdence intervals in Appendix.\nTPN outperforms other methods with a large margin in lower shots. With the shot increase, the\nadvantage of transduction narrows since more labelled data are used. This ﬁnding agrees with the\nresults in TSVM (Joachims, 1999): when more training data are available, the bonus of transductive\ninference will be decreased.\n4.4\nCOMPARISON WITH SEMI-SUPERVISED FEW-SHOT LEARNING\nTable 3: Semi-supervised comparison on miniImageNet.\nModel\n1-shot\n5-shot\n1-shot w/D\n5-shot w/D\nSoft k-Means (Ren et al., 2018)\n50.09\n64.59\n48.70\n63.55\nSoft k-Means+Cluster (Ren et al., 2018)\n49.03\n63.08\n48.86\n61.27\nMasked Soft k-Means (Ren et al., 2018)\n50.41\n64.39\n49.04\n62.96\nTPN-semi\n52.78\n66.42\n50.43\n64.95\n* “w/D” means with distraction. In this setting, many of the unlabelled data are from the\nso-called distraction classes , which is different from the classes of labelled data.\n† Due to space limitation, we report the accuracy with 95% conﬁdence intervals in\nAppendix.\nThe main difference of traditional semi-supervised learning and transduction is the source of un-\nlabeled data. Transductive methods directly use test set as unlabeled data while semi-supervised\nlearning usually has an extra unlabeled set. In order to compare with semi-supervised methods,\n8\n\n\nPublished as a conference paper at ICLR 2019\nFigure 4: 5-way performance with various training/test shots.\nTable 4: Semi-supervised comparison on tieredImageNet.\nModel\n1-shot\n5-shot\n1-shot w/D\n5-shot w/D\nSoft k-Means (Ren et al., 2018)\n51.52\n70.25\n49.88\n68.32\nSoft k-Means+Cluster (Ren et al., 2018)\n51.85\n69.42\n51.36\n67.56\nMasked Soft k-Means (Ren et al., 2018)\n52.39\n69.88\n51.38\n69.08\nTPN-semi\n55.74\n71.01\n53.45\n69.93\n* “w/D” means with distraction. In this setting, many of the unlabelled data are from the\nso-called distraction classes , which is different from the classes of labelled data.\n† Due to space limitation, we report the accuracy with 95% conﬁdence intervals in\nAppendix.\nwe propose a semi-supervised version of TPN, named TPN-semi, which classiﬁes one test example\neach time by propagating labels from the labeled set and extra unlabeled set.\nWe use miniImageNet and tieredImageNet with the labeled/unlabeled data split proposed by Ren\net al. (2018). Speciﬁcally, they split the images of each class into disjoint labeled and unlabeled\nsets. For miniImageNet, the ratio of labeled/unlabeled data is 40% and 60% in each class. Likewise,\nthe ratio is 10% and 90% for tieredImageNet. All semi-supervised methods (including TPN-semi)\nsample support/query data from the labeled set (e.g, 40% from miniImageNet) and sample unlabeled\ndata from the unlabeled sets (e.g, 60% from miniImageNet). In addition, there is a more challenging\nsituation where many unlabelled examples from other distractor classes (different from labelled\nclasses).\nFollowing Ren et al. (2018), we report the average accuracy over 10 random labeled/unlabeled splits\nand the uncertainty computed in standard error. Results are shown in Table 3 and Table 4. It can\nbe seen that TPN-semi outperforms all other algorithms with a large margin, especially for 1-shot\ncase. Although TPN is originally designed to perform transductive inference, we show that it can\nbe successfully adapted to semi-supervised learning tasks with little modiﬁcation. In certain cases\nwhere we can not get all test data, the TPN-semi can be used as an effective alternative algorithm.\n5\nCONCLUSION\nIn this work, we proposed the transductive setting for few-shot learning. Our proposed approach,\nnamely Transductive Propagation Network (TPN), utilizes the entire test set for transductive infer-\nence. Speciﬁcally, our approach is composed of four steps: feature embedding, graph construction,\nlabel propagation, and loss computation. Graph construction is a key step that produces example-\nwise parameters to exploit the manifold structure in each episode. In our method, all parameters\nare learned end-to-end using cross-entropy loss with respect to the ground truth labels and the\nprediction scores in the query set. We obtained the state-of-the-art results on miniImageNet and\ntieredImageNet. Also, the semi-supervised adaptation of our algorithm achieved higher results than\nother semi-supervised methods. In future work, we are going to explore the episodic-wise distance\nmetric rather than only using example-wise parameters for the Euclidean distance.\n9\n\n\nPublished as a conference paper at ICLR 2019\nACKNOWLEDGMENTS\nSaehoon Kim, Minseop Park, and Eunho Yang were supported by Samsung Research Funding &\nIncubation Center of Samsung Electronics under Project Number SRFC-IT1702-15. Yanbin Liu and\nYi Yang are in part supported by AWS Cloud Credits for Research.\nREFERENCES\nFan RK Chung and Fan Chung Graham. Spectral graph theory. American Mathematical Soc., 1997.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep\nnetworks. In International Conference on Machine Learning, pages 1126–1135, 2017.\nYanwei Fu, Timothy M Hospedales, Tao Xiang, and Shaogang Gong. Transductive multi-view zero-shot learn-\ning. IEEE transactions on pattern analysis and machine intelligence, 37(11):2332–2345, 2015.\nYasuhiro Fujiwara and Go Irie. Efﬁcient label propagation. In International Conference on Machine Learning,\npages 784–792, 2014.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\nComputer Vision and Pattern Recognition, pages 770–778, 2016.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. In International Conference on Machine Learning, pages 448–456, 2015.\nYangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadar-\nrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. In ACM Interna-\ntional Conference on Multimedia, pages 675–678. ACM, 2014.\nThorsten Joachims. Transductive inference for text classiﬁcation using support vector machines. In Interna-\ntional Conference on Machine Learning, volume 99, pages 200–209, 1999.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference\non Learning Representations (ICLR), volume 5, 2015.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural\nnetworks. In Advances in Neural Information Processing Systems, pages 1097–1105, 2012.\nBrenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. One shot learning of simple visual\nconcepts. In Conference of the Cognitive Science Society, volume 33, 2011.\nYoonho Lee and Seungjin Choi. Gradient-based meta-learning with learned layerwise metric and subspace. In\nInternational Conference on Machine Learning, pages 2933–2942, 2018.\nDe-Ming Liang and Yu-Feng Li. Lightweight label propagation for large-scale network data. In IJCAI, pages\n3421–3427, 2018.\nBauerïij ˇN Matthias, Rojas-Carulla Mateo, Jakub Bartłomiej ´Swi ˛atkowski, Bernhard Schölkopf, and Richard E\nTurner. Discriminative k-shot learning using probabilistic models. arXiv preprint arXiv:1706.00326, 2017.\nNikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-learner. In\nInternational Conference on Learning Representations, 2018.\nTsendsuren Munkhdalai, Xingdi Yuan, Soroush Mehri, and Adam Trischler. Rapid adaptation with condition-\nally shifted neurons. In International Conference on Machine Learning, pages 3661–3670, 2018.\nAlex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-order meta-learning algorithms. arXiv preprint\narXiv:1803.02999, 2018.\nBoris N Oreshkin, Alexandre Lacoste, and Pau Rodriguez. Tadam: Task dependent adaptive metric for im-\nproved few-shot learning. In Advances in Neural Information Processing Systems, 2018.\nSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. International Conference\non Learning Representations, 2017.\nMengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum, Hugo\nLarochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classiﬁcation. International\nConference on Learning Representations, 2018.\n10\n\n\nPublished as a conference paper at ICLR 2019\nMarcus Rohrbach, Sandra Ebert, and Bernt Schiele. Transfer learning in a transductive setting. In Advances in\nNeural Information Processing Systems, pages 46–54, 2013.\nJürgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: the\nmeta-meta-... hook. PhD thesis, Technische Universität München, 1987.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.\nInternational Conference on Learning Representations, 2015.\nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Advances in\nNeural Information Processing Systems, pages 4080–4090, 2017.\nMasashi Sugiyama. Dimensionality reduction of multimodal labeled data by local ﬁsher discriminant analysis.\nJournal of Machine Learning Research, 8:1027–1061, 2007.\nFlood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales. Learning to\ncompare: Relation network for few-shot learning. In Computer Vision and Pattern Recognition, 2018.\nSebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media, 2012.\nVladimir Naumovich Vapnik. An overview of statistical learning theory. IEEE transactions on neural networks,\n10(5):988–999, 1999.\nOriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning.\nIn Advances in Neural Information Processing Systems, pages 3630–3638, 2016.\nFei Wang and Changshui Zhang. Label propagation through linear neighborhoods. In International Conference\non Machine Learning, pages 985–992. ACM, 2006.\nYu-Xiong Wang, Ross Girshick, Martial Hebert, and Bharath Hariharan. Low-shot learning from imaginary\ndata. In Computer Vision and Pattern Recognition, 2018.\nZhongwen Xu, Linchao Zhu, and Yi Yang. Few-shot object recognition from machine-labeled web images. In\nComputer Vision and Pattern Recognition, 2017.\nZhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with graph em-\nbeddings. In International Conference on Machine Learning, pages 40–48, 2016.\nLihi Zelnik-Manor and Pietro Perona. Self-tuning spectral clustering. In Advances in Neural Information\nProcessing Systems, 2004.\nDenny Zhou, Olivier Bousquet, Thomas N Lal, Jason Weston, and Bernhard Schölkopf. Learning with local\nand global consistency. In Advances in Neural Information Processing Systems, pages 321–328, 2004.\nXiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label propagation. Tech.\nRep., Technical Report CMU-CALD-02–107, Carnegie Mellon University, 2002.\n11\n\n\nPublished as a conference paper at ICLR 2019\nA\nABLATION STUDY\nIn this section, we performed several ablation studies with respect to training shots and query number.\nA.1\nTRAINING SHOTS\nFigure 5: Model performance with different training shots. The x-axis indicates the number of shots in training,\nand the y-axis indicates 5-way test accuracy for 1-shot and 5-shot. Error bars indicate 95% conﬁdence intervals\nas computed over 600 test episodes.\nA.2\nQUERY NUMBER\nTable 5: Accuracy with various query numbers\nminiImageNet 1-shot\n5\n10\n15\n20\n25\n30\nTrain=15\n52.29\n52.95\n53.75\n53.92\n54.57\n54.47\nTest=15\n53.53\n53.72\n53.75\n52.79\n52.84\n52.47\nTrain=Test\n51.94\n53.47\n53.75\n54.00\n53.59\n53.32\nminiImageNet 5-shot\n5\n10\n15\n20\n25\n30\nTrain=15\n66.97\n69.30\n69.43\n69.92\n70.54\n70.36\nTest=15\n68.50\n68.85\n69.43\n69.26\n69.12\n68.89\nTrain=Test\n67.55\n69.22\n69.43\n69.85\n70.11\n69.94\nAt ﬁrst, we designed three experiments to study the inﬂuence of the query number in both training and test\nphase: (1) ﬁx training query to 15; (2) ﬁx test query to 15; (3) training query equals test query. The results\nare shown in Table 5. Some conclusions can be drawn from this experiment: (1) When training query is ﬁxed,\nincreasing the test query will lead to the performance gain. Moreover, even a small test query (e.g., 5) can\nyield good performance; (2) When test query is ﬁxed, the performance is relatively stable with various training\nquery numbers; (3) If the query number of training matches test, the performance can also be improved with\nincreasing number.\nA.3\nRESULTS ON RESNET\nIn this paper, we use a 4-layer neural network structure as described in Section 3.2.1 to make a fair comparison.\nCurrently, there are two common network architectures in few-shot learning: 4-layer ConvNets (e.g., Finn et\nal. (2017); Snell et al. (2017); Sung et al. (2018)) and 12-layer ResNet (e.g., Mishra et al. (2018); Munkhdalai\net al. (2018); Matthias et al. (2017); Oreshkin et al. (2018)). Our method belongs to the ﬁrst one, which\ncontains much fewer layers than the ResNet setting. Thus, it is more reasonable to compare algorithms such as\nTADAM (Oreshkin et al., 2018) with ResNet version of our method. To make this comparison, we implemented\nour algorithm with ResNet architecture on miniImagenet dataset and show the results in Table 6.\nIt can be seen that we beat TADAM for 1-shot setting. For 5-shot, we outperform all other recent high-\nperformance methods except for TADAM.\n12\n\n\nPublished as a conference paper at ICLR 2019\nTable 6: ResNet results on miniImageNet\nMethod\n1-shot\n5-shot\nSNAIL (Mishra et al., 2018)\n55.71\n68.88\nadaResNet (Munkhdalai et al., 2018)\n56.88\n71.94\nDiscriminative k-shot (Matthias et al., 2017)\n56.30\n73.90\nTADAM (Oreshkin et al., 2018)\n58.50\n76.70\nTPN\n59.46\n75.65\nA.4\nCLOSED-FORM SOLUTION VS ITERATIVE UPDATES\nThere is a potential concern that the closed-form solution of label propagation can not scale to large-scale\nmatrix. We relieve this concern from two aspects. On one hand, the few-shot learning problem assumes that\ntraining examples in each class is quite small (only 1 or 5). In this situation, Eq 3 and the closed-form version\ncan be efﬁciently solved, since the dimension of S is only 80 × 80 (5-way, 1-shot, 15-query) or 100 × 100\n(5-way, 5-shot, 15-query). On the other hand, there are plenty of prior work on the scalability and efﬁciency\nof label propagation, such as Liang and Li (2018); Fujiwara and Irie (2014), which can extend our work to\nlarge-scale data.\nFurthermore, on miniImagenet, we performed iterative optimization and got 53.05/68.75 for 1-shot/5-shot ex-\nperiments with only 10 steps. This is slightly worse than closed-form version (53.75/69.43). We attribute this\nslightly worse accuracy to the inaccurate computation and unstable gradients caused by multiple step iterations.\nA.5\nACCURACY WITH 95% CONFIDENCE INTERVALS\nTable 7: Few-shot classiﬁcation accuracies on miniImageNet. All results are averaged over 600 test episodes\nand are reported with 95% conﬁdence intervals. Top results are highlighted.\n5-way Acc\n10-way Acc\nModel\nTransduction\n1-shot\n5-shot\n1-shot\n5-shot\nMAML\nBN\n48.70±1.84\n63.11±0.92\n31.27±1.15\n46.92±1.25\nMAML+Transduction\nYes\n50.83±1.85\n66.19±1.85\n31.83±0.45\n48.23±1.28\nReptile\nNo\n47.07±0.26\n62.74±0.37\n31.10±0.28\n44.66±0.30\nReptile + BN\nBN\n49.97±0.32\n65.99±0.58\n32.00±0.27\n47.60±0.32\nPROTO NET\nNo\n46.14±0.77\n65.77±0.70\n32.88±0.47\n49.29±0.42\nPROTO NET (Higher Way)\nNo\n49.42±0.78\n68.20±0.66\n34.61±0.46\n50.09±0.44\nRELATION NET\nBN\n51.38±0.82\n67.07±0.69\n34.86±0.48\n47.94±0.42\nLabel Propagation\nYes\n52.31±0.85\n68.18±0.67\n35.23±0.51\n51.24±0.43\nTPN\nYes\n53.75±0.86\n69.43±0.67\n36.62±0.50\n52.32±0.44\nTPN (Higher Shot)\nYes\n55.51±0.86\n69.86±0.65\n38.44±0.49\n52.77±0.45\n* “Higher Way” means using more classes in training episodes. “Higher Shot” means using more shots in\ntraining episodes. “BN” means information is shared among test examples using batch normalization.\n13\n\n\nPublished as a conference paper at ICLR 2019\nTable 8: Few-shot classiﬁcation accuracies on tieredImageNet. All results are averaged over 600 test episodes\nand are reported with 95% conﬁdence intervals. Top results are highlighted.\n5-way Acc\n10-way Acc\nModel\nTransduction\n1-shot\n5-shot\n1-shot\n5-shot\nMAML\nBN\n51.67±1.81\n70.30±1.75\n34.44±1.19\n53.32±1.33\nMAML + Transduction\nYes\n53.23±1.85\n70.83±1.78\n34.78±1.18\n54.67±1.26\nReptile\nNo\n48.97±0.21\n66.47±0.21\n33.67±0.28\n48.04±0.30\nReptile + BN\nBN\n52.36±0.23\n71.03±0.22\n35.32±0.28\n51.98±0.32\nPROTO NET\nNo\n48.58±0.87\n69.57±0.75\n37.35±0.56\n57.83±0.55\nPROTO NET (Higher Way)\nNo\n53.31±0.89\n72.69±0.74\n38.62±0.57\n58.32±0.55\nRELATION NET\nBN\n54.48±0.93\n71.31±0.78\n36.32±0.62\n58.05±0.59\nLabel Propagation\nYes\n55.23±0.96\n70.43±0.76\n39.39±0.60\n57.89±0.55\nTPN\nYes\n57.53±0.96\n72.85±0.74\n40.93±0.61\n59.17±0.52\nTPN (Higher Shot)\nYes\n59.91±0.94\n73.30±0.75\n44.80±0.62\n59.44±0.51\n* “Higher Way” means using more classes in training episodes. “Higher Shot” means using more shots in\ntraining episodes. “BN” means information is shared among test examples using batch normalization.\nTable 9: Semi-supervised comparison on miniImageNet.\nModel\n1-shot\n5-shot\n1-shot w/D\n5-shot w/D\nSoft k-Means\n50.09±0.45\n64.59±0.28\n48.70±0.32\n63.55±0.28\nSoft k-Means+Cluster\n49.03±0.24\n63.08±0.18\n48.86±0.32\n61.27±0.24\nMasked Soft k-Means\n50.41±0.31\n64.39±0.24\n49.04±0.31\n62.96±0.14\nTPN-semi\n52.78±0.27\n66.42±0.21\n50.43±0.84\n64.95±0.73\n* “w/D” means with distraction. In this setting, many of the unlabelled data are from\nthe so-called distraction classes , which is different from the classes of labelled data.\nTable 10: Semi-supervised comparison on tieredImageNet.\nModel\n1-shot\n5-shot\n1-shot w/D\n5-shot w/D\nSoft k-Means\n51.52±0.36\n70.25±0.31\n49.88±0.52\n68.32±0.22\nSoft k-Means+Cluster\n51.85±0.25\n69.42±0.17\n51.36±0.31\n67.56±0.10\nMasked Soft k-Means\n52.39±0.44\n69.88±0.20\n51.38±0.38\n69.08±0.25\nTPN-semi\n55.74±0.29\n71.01±0.23\n53.45±0.93\n69.93±0.80\n* “w/D” means with distraction. In this setting, many of the unlabelled data are from\nthe so-called distraction classes , which is different from the classes of labelled data.\n14\n"
}