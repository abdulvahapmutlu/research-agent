{
  "filename": "20226-Article Text-24239-1-2-20220628.pdf",
  "num_pages": 9,
  "pages": [
    "Hybrid Graph Neural Networks for Few-Shot Learning\nTianyuan Yu1,2, Sen He1,3, Yi-Zhe Song1,3, Tao Xiang1,3\n1Center for Vision, Speech and Signal Processing, University of Surrey\n2National University of Defense Technology\n3iFlyTek-Surrey Joint Research Centre on Artiﬁcial Intelligence\n{tianyuan.yu, sen.he, y.song, t.xiang}@surrey.ac.uk\nAbstract\nGraph neural networks (GNNs) have been used to tackle the\nfew-shot learning (FSL) problem and shown great potentials\nunder the transductive setting. However under the inductive\nsetting, existing GNN based methods are less competitive.\nThis is because they use an instance GNN as a label prop-\nagation/classiﬁcation module, which is jointly meta-learned\nwith a feature embedding network. This design is problem-\natic because the classiﬁer needs to adapt quickly to new\ntasks while the embedding does not. To overcome this prob-\nlem, in this paper we propose a novel hybrid GNN (HGNN)\nmodel consisting of two GNNs, an instance GNN and a pro-\ntotype GNN. Instead of label propagation, they act as fea-\nture embedding adaptation modules for quick adaptation of\nthe meta-learned feature embedding to new tasks. Impor-\ntantly they are designed to deal with a fundamental yet of-\nten neglected challenge in FSL, that is, with only a handful\nof shots per class, any few-shot classiﬁer would be sensi-\ntive to badly sampled shots which are either outliers or can\ncause inter-class distribution overlapping. Extensive experi-\nments show that our HGNN obtains new state-of-the-art on\nthree FSL benchmarks. The code and models are available at\nhttps://github.com/TianyuanYu/HGNN.\nIntroduction\nDeep convolutional neural networks (CNNs) have achieved\ngreat successes in various computer vision problems includ-\ning image classiﬁcation (Krizhevsky, Sutskever, and Hinton\n2012), semantic segmentation (Chen et al. 2017), object de-\ntection (Ren et al. 2015) and image captioning (Xu et al.\n2015). However, training deep neural networks requires a\nlarge amount of labeled data (e.g., hundreds of samples per\nclass). Collecting and annotating them is often tedious, ex-\npensive and even infeasible for some rare classes. This thus\nhinders their deployments in real-world applications. One\nwidely studied solution to this problem is few-shot learning\n(FSL) (Vinyals et al. 2016; Finn, Abbeel, and Levine 2017;\nSnell, Swersky, and Zemel 2017; Sung et al. 2018; Sun et al.\n2019; Zhang et al. 2020), which aims to recognize a set of\nnovel classes with only a handful of labeled samples/shots\n(e.g., 1-5 per class) by knowledge transfer from a set of base\nclasses with abundant samples.\nCopyright c⃝2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nTask 1\nTask n\n…\nTask-agnostic classifier\nTask 1\nTask n\nClassifier 1\nClassifier n\nImage feature\nOne-hot label \n…\nTask-specific\n(a) Prior works\n(b) Our model\nInformation\npropagation\nclass 1\nclass 2\nclass 2\nclass 1\nclass 2\nclass 1\nclass 2\nclass 1\nclass 1\nclass 2\nclass 1\nclass 2\nNode (support)\nNode (query)\n…\nFigure 1: Illustration of the differences between our GNN\nand prior GNN in FSL using 2-way 1-shot tasks. (a) Pre-\nvious methods, e.g. (Garcia and Bruna 2018), use GNN for\nlabel propagation, i.e., as a task-agnostic classiﬁer. (b) We\nuse GNN for feature adaptation and leave the query image\nlabel prediction job to task-speciﬁc classiﬁers.\nMost recent FSL approaches follow the paradigm of meta-\nlearning (Hospedales et al. 2020) with episodic training.\nConcretely, a model is trained in each episode with a few-\nshot classiﬁcation task sampled from the base classes. Each\ntask consists of a support set and a query set for inner and\nouter loop training respectively. This is to imitate the meta-\ntest setting, under which only few labeled data are given for\na novel task. The objective is to meta-learn a model capable\nof “learning to learn”, that is, generalizing well to new tasks\ncomposed of unseen classes. Existing approaches differ pri-\nmarily on what is meta-learned – a deep embedding/distance\nmetric (Vinyals et al. 2016; Snell, Swersky, and Zemel 2017;\nSung et al. 2018; Zhang et al. 2020) or an optimization al-\ngorithm (Finn, Abbeel, and Levine 2017).\nAmong various existing FSL approaches, graph neural\nnetwork (GNN) (Kipf and Welling 2017) based FSL meth-\nods (Garcia and Bruna 2018; Luo et al. 2020; Liu et al. 2019;\nKim et al. 2019; Yang et al. 2020) have received increasing\nattention due to their excellent performance under the trans-\nductive setting. These methods, as shown in Figure 1(a), em-\nploy GNN as a label propagation module whereby the graph\nis used for either node label prediction (Garcia and Bruna\n2018; Liu et al. 2019; Luo et al. 2020) or edge label predic-\ntion (Kim et al. 2019; Yang et al. 2020). In other words, a\nGNN is used as a classiﬁer which takes a feature embedding\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n3179\n",
    "(a) Feature embedding\n(b) HGNN-adapted embedding\nFigure 2: (a) Illustration of two issues, i.e., intra-class out-\nliers and inter-class overlapping, caused by badly sampled\ninstances in 2-way 5 shot tasks. (b) With our HGNN, the\noutlier samples’ effects are minimized and the two classes\nbecome well separated. More illustration with real data dis-\ntributions can be found in Figure 4.\nnetwork’s output as input and produces class labels. Both\nthe classiﬁer/GNN parameters and the feature embedding\nnetwork parameters are learned jointly in the outer loop as\ntwo parts of a single model. According to (Garcia and Bruna\n2018; Kim et al. 2019), a GNN is naturally suited for few-\nshot learning due to its ability to aggregate knowledge by\nmessage passing on a graph constructed on the limited sup-\nport set instances as well as the query set instances. How-\never, the efﬁcacy of the existing methods (Garcia and Bruna\n2018; Luo et al. 2020; Liu et al. 2019; Kim et al. 2019; Yang\net al. 2020) under the inductive setting is still lagging behind\nthe state-of-the-art (Zhang et al. 2020; Ye et al. 2020).\nWe believe that it is the joint meta-learning of the classi-\nﬁer and feature embedding that impedes the effectiveness of\nexisting GNN-based FSL methods under the inductive set-\nting. Speciﬁcally, there is an on-going debate (Raghu et al.\n2020; Oh et al. 2020; Tian et al. 2020) on what meta-learning\nis truly about: rapid learning or feature reuse, or both? There\nseems to be little doubt on the importance of learning a good\nfeature embedding, to the extent that it was argued recently\nthat a good embedding is all one needs (Tian et al. 2020).\nMeanwhile, the ability to rapid adaptation to the new task at\nhand can also be critical (Ye et al. 2020). Nevertheless, there\nis an emerging consensus that jointly meta-learning both the\nclassiﬁer which has to be adapted quickly to each new task,\nand the feature embedding network which is intrinsically\ntask-agnostic for feature reuse, is perhaps not a good idea\ndue to their contradictory nature (Raghu et al. 2020; Oh et al.\n2020).\nTo overcome this limitation, in this paper, a novel Hybrid\nGraph Neural Network (HGNN) based FSL framework is\nproposed. As shown in Figure 1(b), different from exist-\ning GNN-FSL methods (Garcia and Bruna 2018; Luo et al.\n2020; Liu et al. 2019; Kim et al. 2019; Yang et al. 2020)\nwhereby GNNs are used as classiﬁers via label propagation\nfrom support to query, our GNNs are used as feature embed-\nding task adaptation modules to address a speciﬁc problem\nthat is often ignored in FSL. That is, when only few support\nset instances are available to represent each class in a sup-\nport set, any classiﬁer built on them would be sensitive to\nbadly drawn samples. More speciﬁcally, as shown in Figure\n2(a), two issues can be caused by bad samples: outliers and\ninter-class overlapping. Outliers, caused by unusual pose/-\nlighting etc (Yu et al. 2019), are problematic for any learn-\ning tasks, but particular so in FSL – with few samples per\nclass, a single outlier could have an immense effect on the\nclass distribution. The issue of inter-class overlapping is also\ncommonplace when the training samples of different classes\nhave very similar background or object pose, or just being\nvisually similar. Through message passing across the whole\nsupports set containing all classes, our GNNs are learned to\nidentify these outliers, minimize their negative effects, and\nre-adjust the class distributions so that each class’ distribu-\ntion is compact and further apart from each other (see Figure\n2(b)).\nConcretely, our HGNN is integrated with a feature em-\nbedding network meta-learned using the popular prototypi-\ncal network (ProtoNet) (Snell, Swersky, and Zemel 2017).\nAs shown in Figure 3, it consists of two GNNs, namely an\nInstance Graph Neural Networks (IGNN) constructed with\nthe whole support set samples/instances plus a single query\nsample as nodes, and a Prototypical Graph Neural Network\n(PGNN) whose nodes correspond to class prototypes. They\nare designed to address the two bad sampling issues respec-\ntively. In particular, the IGNN focuses on outlier identiﬁca-\ntion and neutralization through instance-level message pass-\ning, while the PGNN operates at the class prototype level\nto make sure that different classes are well separable in the\nembedding space adapted by the GNN. These two objectives\nare clearly complementary and our HGNN exploits this us-\ning two graph-speciﬁc losses and an inter-graph consistency\nloss.\nOur contributions are as follows: (1) We propose a new\nframework for using GNNs for FSL which differs from ex-\nisting GNN-FSL methods in that it uses the GNNs for fea-\nture embedding adaptation for new tasks, rather than label\nprediction. (2) As an instantiation, we propose an instance\nGNN which is designed to address the outlying sample prob-\nlem. (3) We further propose a prototype GNN to deal with\noverlapping classes, which, as far as we know, has never\nbeen used before for FSL. (4) These two GNNs are inte-\ngrated into a hybrid graph model which produces new state-\nof-the-art on three widely used FSL datasets.\nRelated Work\nMeta-learning\nMost FSL methods are based on meta-learning, which aims\nto learn, through episodic training a model that can general-\nize to unseen new tasks. Depending on what is meta-learned,\nexisting methods can be divided broadly into two categories,\noptimization-based and metric-based.\nOptimization-based Methods target at learning a good op-\ntimization algorithm that can adapt a deep CNN to a new\ntask represented with few samples. Early works focused\non meta-learning an optimizer. These include the LSTM-\nbased meta-learner (Ravi and Larochelle 2017) and the\nexternal-memory assisted weight updating (Munkhdalai and\nYu 2017). Later, the focus shifted to meta-learning a model\ninitialization suitable for fast adapting the model to a new\ntask by ﬁne-tuning on few support samples with few itera-\ntions. The representative works are MAML (Finn, Abbeel,\nand Levine 2017) and its many variants (Nichol, Achiam,\nand Schulman 2018; Rajeswaran et al. 2019). These meth-\nods are faced with a difﬁcult bi-level optimization problem\n3180\n",
    "due to the inter-dependency of the parameters updated in the\ninner and outer loops in each episode. To overcome this chal-\nlenge, (Sun et al. 2019) proposed to learn task-relevant scal-\ning and shifting functions to dynamically adjust the CNN\nweights.\nMetric-based Methods aim to learn a transferable feature\nembedding or distance metric. In the early works, Match-\ningNet (Vinyals et al. 2016) used an attention mechanism\nover the learned representations and applied nearest neigh-\nbor search for classiﬁcation. ProtoNet (Snell, Swersky, and\nZemel 2017) used the mean of each class’s support set in\nthe embedding space as a prototype without any classi-\nﬁer parameter and meta-learned the feature embedding net-\nwork in the outer loop using a query set. RelationNet (Sung\net al. 2018) learned a distance metric through a neural net-\nwork on top of a feature embedding network. Many recent\nworks, similar to our HGNN, were formulated with the Pro-\ntoNet (Snell, Swersky, and Zemel 2017) as basis, due to\nits simplicity and competitive performance. For example,\n(Allen et al. 2019) represented each class as a set of clus-\nters rather than a single cluster. (Li et al. 2019) replaced the\nglobal feature by a local descriptor. (Afrasiyabi, Lalonde,\nand Gagn´e 2020) introduced the idea of associative align-\nment for leveraging each informative part of the support\ndata. Most existing works used holistic image features. In\ncontrast, DeepEMD (Zhang et al. 2020) showed that image-\npatch features can be useful in addressing the spatial mis-\nalignment issue.\nRapid Learning vs Feature Reuse Despite their theoret-\nical attractiveness, optimization-based methods are in gen-\neral less competitive compared to metric-based methods.\nThis triggered discussions on the merits of rapid learn-\ning and feature reuse (Raghu et al. 2020; Oh et al. 2020;\nTian et al. 2020). The optimization-based methods obvi-\nously focus on rapid learning, while metric-based methods\nare mostly about feature reuse. Yet, it was discovered that\neven those optimization-based methods also heavily rely on\nlearning a good stable feature embedding that can be used\nfor any new task (Raghu et al. 2020; Oh et al. 2020). Based\non this understanding, it was suggested that the classiﬁer\nparameters, which must be adapted rapidly to new tasks,\nshould not be meta-learned jointly with the feature embed-\nding parameters, a principle adopted by our HGNN. Re-\ncently, (Tian et al. 2020) suggested that feature embedding\nlearning is all one needs - pre-training a feature embed-\nding network together with model distillation can bypass the\nepisodic training stage altogether. A similar ﬁnding was re-\nported in (Liu et al. 2020). However, it was argued that a task\nadaptation module (not a classiﬁer), jointly learned with a\nfeature embedding, is the best trade-off between rapid learn-\ning and feature reuse (Oreshkin, L´opez, and Lacoste 2018;\nYe et al. 2020). Our HGNN provides novel task adaptation\nmodules in the form of both instance and prototype GNNs\nand yields clearly better results than (Oreshkin, L´opez, and\nLacoste 2018; Ye et al. 2020).\nGraph Neural Networks in Few-Shot Learning\nGarcia et al.\n(Garcia and Bruna 2018) were the ﬁrst to\nuse GNNs to address few-shot classiﬁcation tasks. In their\nGNNs, each node corresponds to one instance (support or\nquery) and is represented as the concatenation of a feature\nembedding and a label embedding. The ﬁnal layer in their\nmodel is a linear classiﬁcation layer which directly outputs\nthe prediction scores for each query node. (Liu et al. 2019)\nproposed to learn to propagate labels from support nodes\nto query nodes under the transductive setting, by learning\na graph construction module that exploits the manifold of\ndata in a latent space. Similarly focusing on the transduc-\ntive setting but different from these node label prediction\nmodules, EGNN (Kim et al. 2019) learned to predict the\nedge-labels on the graph. Based on EGNN, (Luo et al. 2020)\njointly modeled the long-term inter-task correlations and\nshort-term intra-class adjacency with the derived continual\ngraph neural networks, which can retain and then access im-\nportant prior information associated with newly encountered\nepisodes. Recently, Yang et al. (Yang et al. 2020) proposed\nDPGN, a dual graph network consisting of a point graph and\na distribution graph, in which each instance node is used to\ncombine the distribution-level and instance-level relations.\nMost of these GNN based FSL methods focus on the\ntransductive setting, under which the full test query set can\nbe injected into the graph for label propagation to allevi-\nate the lack of training sample problem. However, their in-\nductive setting performance is lagging behind the state-of-\nthe-art. As mentioned early, we hypothesize that this is be-\ncause label propagation means that these GNNs are essen-\ntially classiﬁers, and jointly meta-learning a classiﬁer and a\nfeature embedding confuses the model on whether to em-\nphasize on rapid learning or feature reuse. In contrast, our\nHGNN removes the label propagation functionality and fo-\ncuses on feature embedding task adaptation. Further, differ-\nent from these instance GNN only methods, we additionally\nintroduce a prototype GNN. As a result, our HGNN pro-\nduces the new state-of-the-art under the inductive setting on\nseveral benchmarks.\nMethodology\nProblem Deﬁnition\nWe follow the standard FSL formulation (Vinyals et al.\n2016; Finn, Abbeel, and Levine 2017; Snell, Swersky, and\nZemel 2017). Concretely, a task is a N-way K-shot classiﬁ-\ncation problem sampled from a meta-test set DTST. Adopt-\ning an episodic-training based meta-learning strategy, N-\nway K-shot tasks are sampled from a meta-training dataset\nDTRN, in order to imitate the few-shot test setting. Note\nthat there is no overlapping between the class label spaces\nof the two sets, i.e., CTRN ∩CTST = ∅. In each episode,\nwe ﬁrst sample N classes CN from the training class space\nrandomly. Training instances are then sampled from these\nclasses to form a support set SN and a query set QN con-\nsisting of K and Q samples from each sampled class in\nCN respectively. The sampled training instances are denoted\nas SN = {(xi, yi) | yi ∈CN, i = 1, . . . , N × K} and\nQN = {(xi, yi) | yi ∈CN, i = 1, . . . , N × Q}, where\nSN ∩QN = ∅. During training, the model uses the support\nset SN to build a classiﬁer in an inner loop, and then the\nquery set QN is used in an outer loop to evaluate and update\nthe model parameters.\n3181\n",
    "CNN\nsupport\nquery\nPGNN\nIGNN\n𝐿!\n𝐿\"\n𝐿#\n𝑓$\n𝐺!\n𝐺\"\nprotos\nFigure 3: Overview of our HGNN model in a 3-way 2-shot case. Features extracted by a feature embedding CNN are fed into a\nPGNN and an IGNN for task adaptation. In the PGNN, each node represents a class prototype, which is initialized by averaging\nthe support set features for that class. The nodes of the IGNN, on the other hand, include all the instances in the support set\nas well as a single query instance. After instance feature adaptation using the IGNN, the updated instance features are used\nto produce another set of class prototypes. Note that in the IGNN, the edge/message passing from instances in the support set\nto the query instance is one-directional from support to query (blue), while the edges between instances in the support set are\nbidirectional (black). These two sets of GNN-updated prototypes are used to predict the class of the query instances and the\npredictions are evaluated using cross-entropy losses (L1 and L2), and a consistency loss (L3) is used to enforce the prediction\nconsistency between the two GNNs.\nLabel Propagation GNNs for FSL\nBefore introducing our framework, we ﬁrst brieﬂy describe\nthe formulation of existing GNN-based FSL methods in or-\nder to highlight the differences in our design. As mentioned\nearlier, all existing GNN-based FSL methods (Garcia and\nBruna 2018; Luo et al. 2020; Liu et al. 2019; Kim et al.\n2019; Yang et al. 2020) use a GNN as a label propagation\nmodule, i.e., a label classiﬁer. To explain this, we use the\nmodel in (Garcia and Bruna 2018) as an example. In this\nmodel, the GNN is a fully-connected graph composed of M\nnodes. Each node represents an instance from either a sup-\nport set or a query set. Under the inductive setting, only one\nquery instance is included in the graph. Therefore for a N-\nway K-shot task, we have M = N × K + 1. In contrast,\nall query instances are used to construct the graph under the\ntransductive setting; we thus have M = N × (K + Q). For\na graph G with L layers, the input with M entries from the\nlth layer is denoted as F l ∈RM×(df +dl), where df is the\ndimension of the instance feature obtained from a feature\nembedding network, and dl is the dimension of label embed-\nding. In other words, each node is represented as a concate-\nnation of a visual feature embedding and a label embedding\nindicating which class each support set instance belongs to.\nThe output of the (l + 1)th layer is given by\nF l+1 = Gl(F l) = ρ\n\u0000Alφl(F l)\n\u0001\n,\n(1)\nwhere ρ is an element-wise non-linear activation function,\nφl is a linear transformation layer , and Al ∈RM×M is an\nadjacency operator in layer l. Each entry in Al is computed\nas\nAl\nij = ψ(φl(Fi), φl(Fj)),\n(2)\nwhere ψ is a neural network. Al is normalized along each\nrow.\nThe ﬁnal class prediction score of a query node is com-\nputed as\npi = F L\ni w,\n(3)\nwhere w ∈R(df +dl)×N parameterizes a linear classiﬁcation\nlayer.\nIt is clear from the formulation of this GNN that, its ob-\njective is to take the support set labels as part of the node\nrepresentation at the input layer of the graph and perform\nlabel propagation on the graph. As a result, the query sam-\nple nodes at the output layer can be used directly for label\nprediction. The alternative designs in (Kim et al. 2019; Luo\net al. 2020; Yang et al. 2020) encode the support set labels on\nthe edges, instead of nodes of the graph, but the main func-\ntionality of the GNN as a label classiﬁer remains the same.\nIn contrast, our HGNN does not encode the label informa-\ntion anywhere in the graph and it serves as a feature embed-\nding task adaptation module as described in detail next.\nHybrid GNN for FSL\nThe proposed GNN based FSL framework is illustrated in\nFigure 3. It consists of a feature extraction backbone and a\nhybrid graph neural network (HGNN) that is further com-\nposed of an instance GNN (IGNN) and a prototype GNN\n(PGNN). In each training episode, we extract the features\nfor the images in the support set SN and the query set QN\nusing a feature embedding CNN. The extracted features are\nthen fed into the GNNs as node features for task adaptation.\nA key difference between the proposed GNNs and the prior\nones in (Garcia and Bruna 2018; Luo et al. 2020; Liu et al.\n3182\n",
    "2019; Kim et al. 2019; Yang et al. 2020) is that our GNNs\ncontain no label information.\nPrototypical Graph Neural Network (PGNN)\nAs illus-\ntrated in Figure 2(a), as each class in the support set SN is\nonly represented by K samples, a FSL model is challenged\nby two issues caused by badly sampled instances, namely\noutliers and class overlapping. Our PGNN is designed to ad-\ndress the class overlapping problem. More speciﬁcally, since\nour HGNN is integrated with a ProtoNet FSL model where\neach class in SN is represented by the class mean or proto-\ntypes, we feed the prototypes into the PGNN and use mes-\nsage passing between them to ensure that class overlapping\nis minimized.\nFormally, as shown in Figure 3, our PGNN GP receives\nthe prototypes’ features FP ∈RN×d of N classes in the sup-\nport set as nodes’ features, where FPn =\n1\nK\nPK\ni=1 fθ(xni),\nwhere fθ(·) is a feature embedding network producing fea-\ntures of d dimensions, and xni is the ith image from the nth\nclass in the support set. To stabilize the training, as per stan-\ndard, we adopt the residual connection (He et al. 2016) and\nthe layer norm (Ba, Kiros, and Hinton 2016) in our GNNs.\nThus the output of an one-layer PGNN is computed as\nˆFP = LayerNorm(FP + ϕP (GP (FP ))),\n(4)\nwhere ϕ is a linear transformation layer to improve the ex-\npressive power of the adapted features with the same dimen-\nsion d, and GP denotes the same operations in Equation 1.\nFinally, the reﬁned prototypes are used to classify the query\nsamples in QN . Concretely, the probability of the ith query\nbelonging to the class j is ﬁrst computed as\npP\ni,j =\nexp(−Ed(fθ(xi), ˆFPj))\nPN\nk=1 exp(−Ed(fθ(xi), ˆFPk)\n,\n(5)\nwhere Ed(·, ·) is the Euclidean distance. To maximize the\nprobability pP\ni,j, the learned PGNN has the incentive to re-\narrange the relative position of the N prototypes so that they\nbecome more separable. This way, it becomes easier to as-\nsign each query image to the correct class with high conﬁ-\ndence.\nInstance Graph Neural Network (IGNN)\nThe PGNN\nfocuses on the inter-class relationship with the class mean or\nprototypes as graph node. It thus has limited ability to deal\nwith the outliers that are best identiﬁed when intra-class in-\nstance relationships are examined. To that end, an IGNN is\nformulated.\nSince we focus on the inductive setting, that is, only one\nquery instance is available for inference at a time, our IGNN\nconsists of the whole support set instances and one query set\ninstance as nodes (see Figure 3). For a N-way K-shot task,\nthere are N × K + 1 nodes in the graph. This means that for\nQ query set samples in a training episode, Q graph needs\nto be constructed. Formally, the ith instance graph takes the\nith query raw feature together with all support set samples’\nfeature extracted by the feature embedding network fθ(·) as\nthe nodes FIi ∈R(N×K+1)×d. Similar to the PGNN, the\nfeatures of nodes in our IGNN are reﬁned by:\nˆFIi = LayerNorm(FIi + ϕI(GI(FIi))).\n(6)\nNote that the updated nodes include all support nodes and\na single query node. However, as shown in Figure 3, the mes-\nsage passing between support and query is one-directional\n(from support to query only) and the query has no effect on\nthe support set node updating. This means that though we\nhave Q graphs, the support set instances only need to be up-\ndated once and the additional computation is only on the\nquery set instances. As a result, once trained the inference\non our IGNN is very efﬁcient.\nWith the updated support set features, to adhere to the\nProtoNet pipeline, we compute another set of prototypes for\neach class by computing the updated support set instance\nclass means. Finally, the probability of the ith query node\nbelonging to class j is\npI\ni,j =\nexp(−Ed( ˆFIiq,\n˜\nFIiP j))\nPN\nk=1 exp(−Ed( ˆFIiq,\n˜\nFIiP k))\n,\n(7)\nwhere ˆFIiq is the updated query node feature in the ith\nIGNN, and\n˜\nFIiP j is the prototype for the jth class. To maxi-\nmize the probability pI\ni,j, the learned IGNN is encouraged to\nidentify the outlying support set samples and use the other\ninstances of the same class to ‘pull’ it closer in the updated\nembedding space. Together with PGNN, IGNN can create a\nmore friendly embedding space for classifying a query sam-\nple by comparing it with the support set samples. This is il-\nlustrated in Figure 2(b) and veriﬁed both quantitatively and\nqualitatively in our experiments.\nTraining Objectives\nDuring training, the two GNNs in our HGNN make predic-\ntions for each query using their respective prototypes, and\ntrained together with the shared feature embedding network\nfθ via cross entropy losses. All the parameters in fθ, PGNN\nGP , and IGNN GI are end-to-end trained.\nSpeciﬁcally, the classiﬁcation losses on PGNN and IGNN\nare\nL1 =\nQ\nX\ni\nN\nX\nj\n−I(yi == j) log(pP\ni,j),\n(8)\nL2 =\nQ\nX\ni\nN\nX\nj\n−I(yi == j) log(pI\ni,j),\n(9)\nwhere yi is the ground truth label of the ith query and I(x)\nis an indicator function: I(x) = 1 when x is true and 0 oth-\nerwise.\nIn addition, to make the prediction scores for each query\nfrom the GNNs to be consistent in our HGNN, a symmetric\nKullback–Leibler (KL) divergence loss is used:\nL3 =\nN\nX\nj\npI\ni,j log pI\ni,j\npP\ni,j\n+\nN\nX\nj\npP\ni,j log pP\ni,j\npI\ni,j\n.\n(10)\nThus, during training, the total loss is:\nL(θ, φI, φP ) = L1 + L2 + L3.\n(11)\n3183\n",
    "Method\nBackbone\n1-shot\n5-shot\nProtoNet∗\nConv4\n52.78 ± 0.45\n71.26 ± 0.36\nMAML\nConv4\n48.70 ± 1.84\n63.10 ± 0.92\nCentroid\nConv4\n53.14 ± 1.06\n71.45 ± 0.72\nNeg-Cosine\nConv4\n52.84 ± 0.76\n70.41 ± 0.66\nFEAT\nConv4\n55.15 ± 0.20\n71.61 ± 0.16\nGNN∗\nConv4\n52.21 ± 0.20\n67.03 ± 0.17\nEGNN\nConv4\n51.65 ± 0.55\n66.85 ± 0.49\nEGNN∗\nConv4\n48.99 ± 0.59\n61.99 ± 0.43\nTPN †\nConv4\n53.75 ± n/a\n69.43 ± n/a\nBGNN∗\nConv4\n52.35 ± 0.42\n67.35 ± 0.35\nDPGN∗\nConv4\n53.22 ± 0.31\n65.34 ± 0.29\nHGNN\nConv4\n55.63 ± 0.20\n72.48 ± 0.16\nProtoNet∗\nResNet-12\n62.41 ± 0.44\n80.49 ± 0.29\nNeg-Cosine\nResNet-12\n63.85 ± 0.81\n81.57 ± 0.56\nDistill\nResNet-12\n64.82 ± 0.60\n82.14 ± 0.43\nDSN-MR\nResNet-12\n64.60 ± 0.72\n79.51 ± 0.50\nDeepEMD\nResNet-12\n65.91 ± 0.82\n82.41 ± 0.56\nFEAT\nResNet-12\n66.78 ± 0.20\n82.05 ± 0.14\nE3BM\nResNet-25\n64.3 ± n/a\n81.0 ± n/a\nMABAS\nResNet-12\n65.08 ±0.86\n82.70 ±0.54\nAPN\nCapsuleNet\n66.43 ±0.26\n82.13 ±0.23\nPSST\nWRN-28-10\n64.16 ±0.44\n80.64 ±0.32\nFRN\nResNet-12\n66.45 ±0.19\n82.83 ±0.13\nHGNN\nResNet-12\n67.02 ± 0.20\n83.00 ± 0.13\nTable 1: 5-way 1/5-shot classiﬁcation accuracy (%) and 95%\nconﬁdence interval on MiniImageNet. ∗indicates our re-\nproduced results with the same pre-trained backbone, and\n† means transductive setting.\nDuring meta-test, the class prediction of a query is given\nby the mean of two prediction scores from the two GNNs in\nour HGNN.\nExperiments\nDatasets and Settings\nDatasets\nThree\nwidely\nused\nFSL\nbenchmarks,\nMiniImageNet (Vinyals et al. 2016), TieredImageNet\n(Ren et al. 2018) and CUB-200-2011 (Wah et al. 2011) are\nused in our experiments. MiniImageNet contains a total\nof 100 classes and 600 images per class. We follow the\nstandard splits provided in (Vinyals et al. 2016), consisting\nof 64 classes for training, and 16 classes and 20 classes\nfor validation and testing respectively. TieredImageNet is\na larger subset of the ImageNet ILSVRC-12, comprising\n779,165 images from 608 classes. They are divided into\n351, 97, and 160 classes for training, validation and testing\nrespectively (Chen et al. 2018). Different from the other\ntwo datasets, CUB-200-2011 is a ﬁne-grained classiﬁcation\ndataset. It includes 11,778 images from 200 different bird\nclasses. The 200 classes are divided into 100, 50, 50 classes\nfor training, validation and testing respectively as in (Liu\net al. 2020; Afrasiyabi, Lalonde, and Gagn´e 2020). In\nall datasets, images are downsampled to 84 × 84 as per\nstandard.\nMethod\nBackbone\n1-shot\n5-shot\nProtoNet∗\nConv4\n50.89 ± 0.21\n69.26 ± 0.18\nMAML\nConv4\n51.67 ± 1.81\n70.30 ± 0.08\nE3BM\nConv4\n52.1 ± n/a\n70.2 ± n/a\nGNN∗\nConv4\n42.37 ± 0.20\n62.54 ± 0.19\nEGNN∗\nConv4\n47.40 ± 0.43\n62.66 ± 0.57\nBGNN∗\nConv4\n49.41 ± 0.43\n65.27 ± 0.35\nDPGN∗\nConv4\n53.99 ± 0.31\n69.86± 0.28\nHGNN\nConv4\n56.05 ± 0.21\n72.82 ± 0.18\nProtoNet∗\nResNet-12\n69.63 ± 0.53\n84.82 ± 0.36\nDistill\nResNet-12\n71.52 ± 0.69\n86.03 ± 0.49\nDSN-MR\nResNet-12\n67.39 ± 0.82\n82.85 ± 0.56\nDeepEMD\nResNet-12\n71.16 ± 0.87\n86.03 ± 0.58\nFEAT\nResNet-12\n70.80 ± 0.23\n84.79 ± 0.16\nE3BM\nResNet-12\n70.0 ± n/a\n85.0 ± n/a\nAPN\nResNet-12\n69.87 ±0.32\n86.35 ±0.41\nFRN\nResNet-12\n71.16 ±0.22\n86.01 ±0.15\nHGNN\nResNet-12\n72.05 ±0.23\n86.49 ± 0.15\nTable 2: Results on TieredImageNet\nMethods\nBackbone\n1-shot\n5-shot\nProtoNet∗\nConv4\n51.25 ± 0.21\n72.26 ± 0.18\nAdversarial\nConv4\n63.30 ± 0.94\n81.35 ± 0.67\nHGNN\nConv4\n69.02 ± 0.22\n83.20 ± 0.15\nProtoNet∗\nResNet-12\n68.11 ± 0.21\n87.33 ± 0.13\nDeepEMD\nResNet-12\n77.14 ± 0.29\n88.98 ± 0.49\nNeg-Cosine\nResNet-18\n72.66 ± 0.85\n89.40 ± 0.43\nCentroid\nResNet-18\n74.22 ± 1.09\n88.65 ± 0.55\nHGNN\nResNet-12\n78.58 ± 0.20\n90.02 ± 0.12\nTable 3: Results on CUB-200-2011\nFeature Embedding Network\nAs in many other CNN-\nbased visual recognition tasks, a feature embedding network\nis required in a FSL model and the choice of its backbone\nhas a major impact on its performance. For fair comparisons\nwith prior works, two widely used backbones are adopted\nin our experiments, namely Conv4 and ResNet-12. Follow-\ning (Snell, Swersky, and Zemel 2017), the Conv4 backbone\nhas four convolutional blocks and its ﬁnal output feature di-\nmension is 64. The ResNet-12 backbone is used in most\nof the state-of-the-art models (Zhang et al. 2020; Ye et al.\n2020; Liu et al. 2020). It consists of four residual blocks, and\nthe output feature dimension is 640. Following the common\npractice (Liu et al. 2020; Ye et al. 2020; Zhang et al. 2020),\nwe pre-train our feature embedding network with supervised\nlearning on the whole training set before the episodic meta-\nlearning stage.\nBaselines\nThree\ntypes\nof\nbaselines\nare\nchosen\nfor\ncomparisons: (1) representative FSL methods including\nMAML (Finn, Abbeel, and Levine 2017) (optimization-\nbased),\nProtoNet\n(Snell,\nSwersky,\nand\nZemel\n2017)\n(embedding-based),\nFEAT\n(Ye\net\nal.\n2020)\n(task-\nadaptation),\nand\nDistill\n(Tian\net\nal.\n2020)\n(without\nepisodic meta-learning), (2) GNN-based methods includ-\n3184\n",
    "(a) PGNN\n(b) IGNN\nFigure 4: Qualitative results on PGNN and IGNN alleviating the class overlapping and outlier issues respectively. The t-SNE\nprojection of instances and prototypes of 5 classes in CUB-200-2011 are shown here. In (a), we use vectors to indicate how class\nprototypes moves from before (CNN protos) to after PGNN updates (PGNN Protos). It can be seen that after the PGNN update,\nthe 5 class prototypes are clearly more separable. In (b), the distance between an outlying instance and its class prototypes\n(before and after IGNN updates) are highlighted using color-coded straight lines (same color means the same outlier). IGNN\nclearly neutralizes the negative impact of outliers by pulling those outliers closer to their prototypes, indicated by the shorter\nlines to IGNN protos.\ning GNN (Garcia and Bruna 2018), EGNN (Kim et al.\n2019), TPN (Liu et al. 2019), DPGN (Yang et al. 2020)\nand BGNN (Luo et al. 2020), and (3) the state-of-the-art\n(SOTA) methods published in 2020 and 2021, including\nCentroid (Afrasiyabi, Lalonde, and Gagn´e 2020), Neg-\nCosine (Liu et al. 2020), DSN-MR (Simon et al. 2020),\nDeepEMD (Zhang et al. 2020), E3BM (Liu, Schiele, and\nSun 2020), MABAS (Kim, Kim, and Kim 2020), APN (Lu,\nPang, and Zhang 2020), Adversarial (Afrasiyabi, Lalonde,\nand Gagn´e 2020), ArL (Zhang et al. 2021), PSST (Chen\net al. 2021), FRN (Wertheimer, Tang, and Hariharan 2021).\nMain Results\nThe comparative results on MiniImageNet, TieredImageNet\nand CUB-200-2011 are shown in Tables 1, 2 and 3 respec-\ntively. The following observations can be made. (1) Our\nHGNN achieves the new SOTA on all three datasets under\nboth the 1-shot and 5-shot settings and with both the Conv4\nand ResNet-12 backbones, validating its effectiveness. (2)\nOur GNN-FSL model signiﬁcantly outperforms all ﬁve ex-\nisting GNN-FSL models (Garcia and Bruna 2018; Luo et al.\n2020; Liu et al. 2019; Kim et al. 2019; Yang et al. 2020) un-\nder the inductive setting1. This veriﬁes our hypothesis that\njointly meta-learning a GNN-based label propagation/clas-\nsiﬁcation module with a feature embedding network con-\nfuses the objectives of rapid learning and feature reuse. This\nresults in inferior performance under the inductive setting\nwhere, without the access to the full query set, the usefulness\nof label propagation is limited. (3) Overall, the advantages\nof our HGNN over the SOTA alternatives under the more\nchallenging 1-shot setting and with the ﬁne-grained CUB-\n200-2011 dataset are more pronounced. This is expected:\nour IGNN and PGNN are designed to address the badly sam-\npled shots problems in the support set. With fewer shots and\n1For fair comparison, we use the same backbone pre-trained on\nthe base training dataset.\nmore inter-class overlapping in the ﬁne-grained cases, these\nproblems are more acute and hence the clearer advantages\nof our HGNN.\nAre PGNN and IGNN Doing Their jobs?\nOur PGNN and IGNN are designed to solve the inter-class\noverlapping and outlier issues caused by badly sampled in-\nstances in each few-shot classiﬁcation task. These two is-\nsues severely impact the performance of a few-shot learning\nmethod, but are rarely discussed before. Figure 4 visualizes\nhow the feature embedding distributions of the support set\ninstances in a task sampled from CUB-200-2011 under the\n5-way 5-shot setting. This qualitative result aims to show\nhow the 5 prototypes and outlying instances are distributed\nbefore and after the output of the feature embedding CNN\nis updated by the two GNNs. We can see clearly that these\ntwo GNNs are indeed addressing the two issues as we antic-\nipated: the PGNN pushes the class prototypes further away\nfrom each other to tackle the class overlapping problem,\nwhile the IGNN produces a more compact intra-class dis-\ntribution by shortening the distance between the outliers and\nprototypes, mitigating the outlier problem.\nConclusions\nWe have proposed a novel GNN-based FSL model. Dif-\nferent from the existing GNN-FSL methods which utilize\nGNN as a label propagation tool to be jointly meta-learned\nwith a feature embedding network, we argue that a GNN\nis best used in FSL as a feature embedding task adaptation\nmodule. In particular, it should address the outlying samples\nand class overlapping problems commonly existing in FSL\nthrough the task adaptation. To that end, an instance GNN\nand a prototype GNN are formulated and their complemen-\ntarity is exploited in a hybrid GNN framework. Extensive\nexperiments demonstrate that our HGNN is indeed effective\nin addressing the poor shot sampling problems, yielding new\nstate-of-the-art on three benchmarks.\n3185\n",
    "References\nAfrasiyabi, A.; Lalonde, J.-F.; and Gagn´e, C. 2020. Associa-\ntive Alignment for Few-shot Image Classiﬁcation. In ECCV.\nAllen, K. R.; Shelhamer, E.; Shin, H.; and Tenenbaum, J. B.\n2019. Inﬁnite mixture prototypes for few-shot learning. In\nICML.\nBa, J. L.; Kiros, J. R.; and Hinton, G. E. 2016. Layer nor-\nmalization. CoRR, 1607.06450.\nChen, L.-C.; Papandreou, G.; Kokkinos, I.; Murphy, K.; and\nYuille, A. L. 2017. Deeplab: Semantic image segmentation\nwith deep convolutional nets, atrous convolution, and fully\nconnected crfs. PAMI, 40(4): 834–848.\nChen, W.-Y.; Liu, Y.-C.; Kira, Z.; Wang, Y.-C. F.; and\nHuang, J.-B. 2018. A Closer Look at Few-shot Classiﬁca-\ntion. In ICLR.\nChen, Z.; Ge, J.; Zhan, H.; Huang, S.; and Wang, D. 2021.\nPareto Self-Supervised Training for Few-Shot Learning. In\nCVPR, 13663–13672.\nFinn, C.; Abbeel, P.; and Levine, S. 2017. Model-Agnostic\nMeta-Learning for Fast Adaptation of Deep Networks. In\nICML.\nGarcia, V.; and Bruna, J. 2018. Few-shot learning with graph\nneural networks. In ICLR.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual\nlearning for image recognition. In CVPR.\nHospedales, T.; Antoniou, A.; Micaelli, P.; and Storkey,\nA. 2020.\nMeta-Learning in Neural Networks: A Survey.\narXiv:2004.05439.\nKim, J.; Kim, H.; and Kim, G. 2020.\nModel-Agnostic\nBoundary-Adversarial Sampling for Test-Time Generaliza-\ntion in Few-Shot learning. In ECCV.\nKim, J.; Kim, T.; Kim, S.; and Yoo, C. D. 2019.\nEdge-\nlabeling graph neural network for few-shot learning.\nIn\nCVPR.\nKipf, T. N.; and Welling, M. 2017. Semi-supervised classi-\nﬁcation with graph convolutional networks. In ICLR.\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Im-\nagenet classiﬁcation with deep convolutional neural net-\nworks. In NeurIPS.\nLi, W.; Wang, L.; Xu, J.; Huo, J.; Gao, Y.; and Luo, J. 2019.\nRevisiting local descriptor based image-to-class measure for\nfew-shot learning. In CVPR.\nLiu, B.; Cao, Y.; Lin, Y.; Li, Q.; Zhang, Z.; Long, M.; and\nHu, H. 2020. Negative Margin Matters: Understanding Mar-\ngin in Few-shot Classiﬁcation. In ECCV.\nLiu, Y.; Lee, J.; Park, M.; Kim, S.; Yang, E.; Hwang, S. J.;\nand Yang, Y. 2019. Learning to Propagate Labels: Transduc-\ntive Propagation Network for Few-shot Learning. In ICLR.\nLiu, Y.; Schiele, B.; and Sun, Q. 2020.\nAn ensemble\nof epoch-wise empirical Bayes for few-shot learning.\nIn\nECCV.\nLu, W.; Pang, C.; and Zhang, B. 2020.\nAttentive Proto-\ntype Few-shot Learning with Capsule Network-based Em-\nbedding. In ECCV.\nLuo, Y.; Huang, Z.; Zhang, Z.; Wang, Z.; Baktashmotlagh,\nM.; and Yang, Y. 2020. Learning from the Past: Continual\nMeta-Learning via Bayesian Graph Modeling. In AAAI.\nMunkhdalai, T.; and Yu, H. 2017. Meta Networks. In ICML.\nNichol, A.; Achiam, J.; and Schulman, J. 2018. On First-\nOrder Meta-Learning Algorithms. CoRR, abs/1803.02999.\nOh, J.; Yoo, H.; Kim, C.; and Yun, S.-Y. 2020. Does MAML\nreally want feature reuse only? CoRR, abs/2008.08882.\nOreshkin, B.; L´opez, P. R.; and Lacoste, A. 2018. Tadam:\nTask dependent adaptive metric for improved few-shot\nlearning. In NeurIPS.\nRaghu, A.; Raghu, M.; Bengio, S.; and Vinyals, O. 2020.\nRapid learning or feature reuse? towards understanding the\neffectiveness of maml. In ICLR.\nRajeswaran, A.; Finn, C.; Kakade, S.; and Levine, S. 2019.\nMeta-Learning with Implicit Gradients. In NeurIPS.\nRavi, S.; and Larochelle, H. 2017. Optimization as a Model\nfor Few-Shot Learning. In ICLR.\nRen, M.; Triantaﬁllou, E.; Ravi, S.; Snell, J.; Swersky, K.;\nTenenbaum, J. B.; Larochelle, H.; and Zemel, R. S. 2018.\nMeta-learning for semi-supervised few-shot classiﬁcation.\nIn ICLR.\nRen, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster r-\ncnn: Towards real-time object detection with region proposal\nnetworks. In NeurIPS.\nSimon, C.; Koniusz, P.; Nock, R.; and Harandi, M. 2020.\nAdaptive Subspaces for Few-Shot Learning. In CVPR.\nSnell, J.; Swersky, K.; and Zemel, R. 2017.\nPrototypical\nnetworks for few-shot learning. In NeurIPS.\nSun, Q.; Liu, Y.; Chua, T.-S.; and Schiele, B. 2019. Meta-\ntransfer learning for few-shot learning. In CVPR.\nSung, F.; Yang, Y.; Zhang, L.; Xiang, T.; Torr, P. H.; and\nHospedales, T. M. 2018. Learning to compare: Relation net-\nwork for few-shot learning. In CVPR.\nTian, Y.; Wang, Y.; Krishnan, D.; Tenenbaum, J. B.; and\nIsola, P. 2020. Rethinking Few-Shot Image Classiﬁcation:\na Good Embedding Is All You Need? In ECCV.\nVinyals, O.; Blundell, C.; Lillicrap, T.; Wierstra, D.; et al.\n2016. Matching networks for one shot learning. In NeurIPS.\nWah, C.; Branson, S.; Welinder, P.; Perona, P.; and Belongie,\nS. 2011.\nThe Caltech-UCSD Birds-200-2011 Dataset.\nTechnical Report CNS-TR-2011-001, California Institute of\nTechnology.\nWertheimer, D.; Tang, L.; and Hariharan, B. 2021. Few-Shot\nClassiﬁcation With Feature Map Reconstruction Networks.\nIn CVPR, 8012–8021.\nXu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A.; Salakhudi-\nnov, R.; Zemel, R.; and Bengio, Y. 2015. Show, attend and\ntell: Neural image caption generation with visual attention.\nIn ICML.\nYang, L.; Li, L.; Zhang, Z.; Zhou, X.; Zhou, E.; and Liu, Y.\n2020. DPGN: Distribution Propagation Graph Network for\nFew-shot Learning. In CVPR.\n3186\n",
    "Ye, H.-J.; Hu, H.; Zhan, D.-C.; and Sha, F. 2020. Few-shot\nlearning via embedding adaptation with set-to-set functions.\nIn CVPR.\nYu, T.; Li, D.; Yang, Y.; Hospedales, T. M.; and Xiang, T.\n2019. Robust person re-identiﬁcation by modelling feature\nuncertainty. In ICCV, 552–561.\nZhang, C.; Cai, Y.; Lin, G.; and Shen, C. 2020. DeepEMD:\nFew-Shot Image Classiﬁcation with Differentiable Earth\nMover’s Distance and Structured Classiﬁers. In CVPR.\nZhang, H.; Koniusz, P.; Jian, S.; Li, H.; and Torr, P. H.\n2021.\nRethinking Class Relations: Absolute-relative Su-\npervised and Unsupervised Few-shot Learning. In CVPR,\n9432–9441.\n3187\n"
  ],
  "full_text": "Hybrid Graph Neural Networks for Few-Shot Learning\nTianyuan Yu1,2, Sen He1,3, Yi-Zhe Song1,3, Tao Xiang1,3\n1Center for Vision, Speech and Signal Processing, University of Surrey\n2National University of Defense Technology\n3iFlyTek-Surrey Joint Research Centre on Artiﬁcial Intelligence\n{tianyuan.yu, sen.he, y.song, t.xiang}@surrey.ac.uk\nAbstract\nGraph neural networks (GNNs) have been used to tackle the\nfew-shot learning (FSL) problem and shown great potentials\nunder the transductive setting. However under the inductive\nsetting, existing GNN based methods are less competitive.\nThis is because they use an instance GNN as a label prop-\nagation/classiﬁcation module, which is jointly meta-learned\nwith a feature embedding network. This design is problem-\natic because the classiﬁer needs to adapt quickly to new\ntasks while the embedding does not. To overcome this prob-\nlem, in this paper we propose a novel hybrid GNN (HGNN)\nmodel consisting of two GNNs, an instance GNN and a pro-\ntotype GNN. Instead of label propagation, they act as fea-\nture embedding adaptation modules for quick adaptation of\nthe meta-learned feature embedding to new tasks. Impor-\ntantly they are designed to deal with a fundamental yet of-\nten neglected challenge in FSL, that is, with only a handful\nof shots per class, any few-shot classiﬁer would be sensi-\ntive to badly sampled shots which are either outliers or can\ncause inter-class distribution overlapping. Extensive experi-\nments show that our HGNN obtains new state-of-the-art on\nthree FSL benchmarks. The code and models are available at\nhttps://github.com/TianyuanYu/HGNN.\nIntroduction\nDeep convolutional neural networks (CNNs) have achieved\ngreat successes in various computer vision problems includ-\ning image classiﬁcation (Krizhevsky, Sutskever, and Hinton\n2012), semantic segmentation (Chen et al. 2017), object de-\ntection (Ren et al. 2015) and image captioning (Xu et al.\n2015). However, training deep neural networks requires a\nlarge amount of labeled data (e.g., hundreds of samples per\nclass). Collecting and annotating them is often tedious, ex-\npensive and even infeasible for some rare classes. This thus\nhinders their deployments in real-world applications. One\nwidely studied solution to this problem is few-shot learning\n(FSL) (Vinyals et al. 2016; Finn, Abbeel, and Levine 2017;\nSnell, Swersky, and Zemel 2017; Sung et al. 2018; Sun et al.\n2019; Zhang et al. 2020), which aims to recognize a set of\nnovel classes with only a handful of labeled samples/shots\n(e.g., 1-5 per class) by knowledge transfer from a set of base\nclasses with abundant samples.\nCopyright c⃝2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nTask 1\nTask n\n…\nTask-agnostic classifier\nTask 1\nTask n\nClassifier 1\nClassifier n\nImage feature\nOne-hot label \n…\nTask-specific\n(a) Prior works\n(b) Our model\nInformation\npropagation\nclass 1\nclass 2\nclass 2\nclass 1\nclass 2\nclass 1\nclass 2\nclass 1\nclass 1\nclass 2\nclass 1\nclass 2\nNode (support)\nNode (query)\n…\nFigure 1: Illustration of the differences between our GNN\nand prior GNN in FSL using 2-way 1-shot tasks. (a) Pre-\nvious methods, e.g. (Garcia and Bruna 2018), use GNN for\nlabel propagation, i.e., as a task-agnostic classiﬁer. (b) We\nuse GNN for feature adaptation and leave the query image\nlabel prediction job to task-speciﬁc classiﬁers.\nMost recent FSL approaches follow the paradigm of meta-\nlearning (Hospedales et al. 2020) with episodic training.\nConcretely, a model is trained in each episode with a few-\nshot classiﬁcation task sampled from the base classes. Each\ntask consists of a support set and a query set for inner and\nouter loop training respectively. This is to imitate the meta-\ntest setting, under which only few labeled data are given for\na novel task. The objective is to meta-learn a model capable\nof “learning to learn”, that is, generalizing well to new tasks\ncomposed of unseen classes. Existing approaches differ pri-\nmarily on what is meta-learned – a deep embedding/distance\nmetric (Vinyals et al. 2016; Snell, Swersky, and Zemel 2017;\nSung et al. 2018; Zhang et al. 2020) or an optimization al-\ngorithm (Finn, Abbeel, and Levine 2017).\nAmong various existing FSL approaches, graph neural\nnetwork (GNN) (Kipf and Welling 2017) based FSL meth-\nods (Garcia and Bruna 2018; Luo et al. 2020; Liu et al. 2019;\nKim et al. 2019; Yang et al. 2020) have received increasing\nattention due to their excellent performance under the trans-\nductive setting. These methods, as shown in Figure 1(a), em-\nploy GNN as a label propagation module whereby the graph\nis used for either node label prediction (Garcia and Bruna\n2018; Liu et al. 2019; Luo et al. 2020) or edge label predic-\ntion (Kim et al. 2019; Yang et al. 2020). In other words, a\nGNN is used as a classiﬁer which takes a feature embedding\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n3179\n\n\n(a) Feature embedding\n(b) HGNN-adapted embedding\nFigure 2: (a) Illustration of two issues, i.e., intra-class out-\nliers and inter-class overlapping, caused by badly sampled\ninstances in 2-way 5 shot tasks. (b) With our HGNN, the\noutlier samples’ effects are minimized and the two classes\nbecome well separated. More illustration with real data dis-\ntributions can be found in Figure 4.\nnetwork’s output as input and produces class labels. Both\nthe classiﬁer/GNN parameters and the feature embedding\nnetwork parameters are learned jointly in the outer loop as\ntwo parts of a single model. According to (Garcia and Bruna\n2018; Kim et al. 2019), a GNN is naturally suited for few-\nshot learning due to its ability to aggregate knowledge by\nmessage passing on a graph constructed on the limited sup-\nport set instances as well as the query set instances. How-\never, the efﬁcacy of the existing methods (Garcia and Bruna\n2018; Luo et al. 2020; Liu et al. 2019; Kim et al. 2019; Yang\net al. 2020) under the inductive setting is still lagging behind\nthe state-of-the-art (Zhang et al. 2020; Ye et al. 2020).\nWe believe that it is the joint meta-learning of the classi-\nﬁer and feature embedding that impedes the effectiveness of\nexisting GNN-based FSL methods under the inductive set-\nting. Speciﬁcally, there is an on-going debate (Raghu et al.\n2020; Oh et al. 2020; Tian et al. 2020) on what meta-learning\nis truly about: rapid learning or feature reuse, or both? There\nseems to be little doubt on the importance of learning a good\nfeature embedding, to the extent that it was argued recently\nthat a good embedding is all one needs (Tian et al. 2020).\nMeanwhile, the ability to rapid adaptation to the new task at\nhand can also be critical (Ye et al. 2020). Nevertheless, there\nis an emerging consensus that jointly meta-learning both the\nclassiﬁer which has to be adapted quickly to each new task,\nand the feature embedding network which is intrinsically\ntask-agnostic for feature reuse, is perhaps not a good idea\ndue to their contradictory nature (Raghu et al. 2020; Oh et al.\n2020).\nTo overcome this limitation, in this paper, a novel Hybrid\nGraph Neural Network (HGNN) based FSL framework is\nproposed. As shown in Figure 1(b), different from exist-\ning GNN-FSL methods (Garcia and Bruna 2018; Luo et al.\n2020; Liu et al. 2019; Kim et al. 2019; Yang et al. 2020)\nwhereby GNNs are used as classiﬁers via label propagation\nfrom support to query, our GNNs are used as feature embed-\nding task adaptation modules to address a speciﬁc problem\nthat is often ignored in FSL. That is, when only few support\nset instances are available to represent each class in a sup-\nport set, any classiﬁer built on them would be sensitive to\nbadly drawn samples. More speciﬁcally, as shown in Figure\n2(a), two issues can be caused by bad samples: outliers and\ninter-class overlapping. Outliers, caused by unusual pose/-\nlighting etc (Yu et al. 2019), are problematic for any learn-\ning tasks, but particular so in FSL – with few samples per\nclass, a single outlier could have an immense effect on the\nclass distribution. The issue of inter-class overlapping is also\ncommonplace when the training samples of different classes\nhave very similar background or object pose, or just being\nvisually similar. Through message passing across the whole\nsupports set containing all classes, our GNNs are learned to\nidentify these outliers, minimize their negative effects, and\nre-adjust the class distributions so that each class’ distribu-\ntion is compact and further apart from each other (see Figure\n2(b)).\nConcretely, our HGNN is integrated with a feature em-\nbedding network meta-learned using the popular prototypi-\ncal network (ProtoNet) (Snell, Swersky, and Zemel 2017).\nAs shown in Figure 3, it consists of two GNNs, namely an\nInstance Graph Neural Networks (IGNN) constructed with\nthe whole support set samples/instances plus a single query\nsample as nodes, and a Prototypical Graph Neural Network\n(PGNN) whose nodes correspond to class prototypes. They\nare designed to address the two bad sampling issues respec-\ntively. In particular, the IGNN focuses on outlier identiﬁca-\ntion and neutralization through instance-level message pass-\ning, while the PGNN operates at the class prototype level\nto make sure that different classes are well separable in the\nembedding space adapted by the GNN. These two objectives\nare clearly complementary and our HGNN exploits this us-\ning two graph-speciﬁc losses and an inter-graph consistency\nloss.\nOur contributions are as follows: (1) We propose a new\nframework for using GNNs for FSL which differs from ex-\nisting GNN-FSL methods in that it uses the GNNs for fea-\nture embedding adaptation for new tasks, rather than label\nprediction. (2) As an instantiation, we propose an instance\nGNN which is designed to address the outlying sample prob-\nlem. (3) We further propose a prototype GNN to deal with\noverlapping classes, which, as far as we know, has never\nbeen used before for FSL. (4) These two GNNs are inte-\ngrated into a hybrid graph model which produces new state-\nof-the-art on three widely used FSL datasets.\nRelated Work\nMeta-learning\nMost FSL methods are based on meta-learning, which aims\nto learn, through episodic training a model that can general-\nize to unseen new tasks. Depending on what is meta-learned,\nexisting methods can be divided broadly into two categories,\noptimization-based and metric-based.\nOptimization-based Methods target at learning a good op-\ntimization algorithm that can adapt a deep CNN to a new\ntask represented with few samples. Early works focused\non meta-learning an optimizer. These include the LSTM-\nbased meta-learner (Ravi and Larochelle 2017) and the\nexternal-memory assisted weight updating (Munkhdalai and\nYu 2017). Later, the focus shifted to meta-learning a model\ninitialization suitable for fast adapting the model to a new\ntask by ﬁne-tuning on few support samples with few itera-\ntions. The representative works are MAML (Finn, Abbeel,\nand Levine 2017) and its many variants (Nichol, Achiam,\nand Schulman 2018; Rajeswaran et al. 2019). These meth-\nods are faced with a difﬁcult bi-level optimization problem\n3180\n\n\ndue to the inter-dependency of the parameters updated in the\ninner and outer loops in each episode. To overcome this chal-\nlenge, (Sun et al. 2019) proposed to learn task-relevant scal-\ning and shifting functions to dynamically adjust the CNN\nweights.\nMetric-based Methods aim to learn a transferable feature\nembedding or distance metric. In the early works, Match-\ningNet (Vinyals et al. 2016) used an attention mechanism\nover the learned representations and applied nearest neigh-\nbor search for classiﬁcation. ProtoNet (Snell, Swersky, and\nZemel 2017) used the mean of each class’s support set in\nthe embedding space as a prototype without any classi-\nﬁer parameter and meta-learned the feature embedding net-\nwork in the outer loop using a query set. RelationNet (Sung\net al. 2018) learned a distance metric through a neural net-\nwork on top of a feature embedding network. Many recent\nworks, similar to our HGNN, were formulated with the Pro-\ntoNet (Snell, Swersky, and Zemel 2017) as basis, due to\nits simplicity and competitive performance. For example,\n(Allen et al. 2019) represented each class as a set of clus-\nters rather than a single cluster. (Li et al. 2019) replaced the\nglobal feature by a local descriptor. (Afrasiyabi, Lalonde,\nand Gagn´e 2020) introduced the idea of associative align-\nment for leveraging each informative part of the support\ndata. Most existing works used holistic image features. In\ncontrast, DeepEMD (Zhang et al. 2020) showed that image-\npatch features can be useful in addressing the spatial mis-\nalignment issue.\nRapid Learning vs Feature Reuse Despite their theoret-\nical attractiveness, optimization-based methods are in gen-\neral less competitive compared to metric-based methods.\nThis triggered discussions on the merits of rapid learn-\ning and feature reuse (Raghu et al. 2020; Oh et al. 2020;\nTian et al. 2020). The optimization-based methods obvi-\nously focus on rapid learning, while metric-based methods\nare mostly about feature reuse. Yet, it was discovered that\neven those optimization-based methods also heavily rely on\nlearning a good stable feature embedding that can be used\nfor any new task (Raghu et al. 2020; Oh et al. 2020). Based\non this understanding, it was suggested that the classiﬁer\nparameters, which must be adapted rapidly to new tasks,\nshould not be meta-learned jointly with the feature embed-\nding parameters, a principle adopted by our HGNN. Re-\ncently, (Tian et al. 2020) suggested that feature embedding\nlearning is all one needs - pre-training a feature embed-\nding network together with model distillation can bypass the\nepisodic training stage altogether. A similar ﬁnding was re-\nported in (Liu et al. 2020). However, it was argued that a task\nadaptation module (not a classiﬁer), jointly learned with a\nfeature embedding, is the best trade-off between rapid learn-\ning and feature reuse (Oreshkin, L´opez, and Lacoste 2018;\nYe et al. 2020). Our HGNN provides novel task adaptation\nmodules in the form of both instance and prototype GNNs\nand yields clearly better results than (Oreshkin, L´opez, and\nLacoste 2018; Ye et al. 2020).\nGraph Neural Networks in Few-Shot Learning\nGarcia et al.\n(Garcia and Bruna 2018) were the ﬁrst to\nuse GNNs to address few-shot classiﬁcation tasks. In their\nGNNs, each node corresponds to one instance (support or\nquery) and is represented as the concatenation of a feature\nembedding and a label embedding. The ﬁnal layer in their\nmodel is a linear classiﬁcation layer which directly outputs\nthe prediction scores for each query node. (Liu et al. 2019)\nproposed to learn to propagate labels from support nodes\nto query nodes under the transductive setting, by learning\na graph construction module that exploits the manifold of\ndata in a latent space. Similarly focusing on the transduc-\ntive setting but different from these node label prediction\nmodules, EGNN (Kim et al. 2019) learned to predict the\nedge-labels on the graph. Based on EGNN, (Luo et al. 2020)\njointly modeled the long-term inter-task correlations and\nshort-term intra-class adjacency with the derived continual\ngraph neural networks, which can retain and then access im-\nportant prior information associated with newly encountered\nepisodes. Recently, Yang et al. (Yang et al. 2020) proposed\nDPGN, a dual graph network consisting of a point graph and\na distribution graph, in which each instance node is used to\ncombine the distribution-level and instance-level relations.\nMost of these GNN based FSL methods focus on the\ntransductive setting, under which the full test query set can\nbe injected into the graph for label propagation to allevi-\nate the lack of training sample problem. However, their in-\nductive setting performance is lagging behind the state-of-\nthe-art. As mentioned early, we hypothesize that this is be-\ncause label propagation means that these GNNs are essen-\ntially classiﬁers, and jointly meta-learning a classiﬁer and a\nfeature embedding confuses the model on whether to em-\nphasize on rapid learning or feature reuse. In contrast, our\nHGNN removes the label propagation functionality and fo-\ncuses on feature embedding task adaptation. Further, differ-\nent from these instance GNN only methods, we additionally\nintroduce a prototype GNN. As a result, our HGNN pro-\nduces the new state-of-the-art under the inductive setting on\nseveral benchmarks.\nMethodology\nProblem Deﬁnition\nWe follow the standard FSL formulation (Vinyals et al.\n2016; Finn, Abbeel, and Levine 2017; Snell, Swersky, and\nZemel 2017). Concretely, a task is a N-way K-shot classiﬁ-\ncation problem sampled from a meta-test set DTST. Adopt-\ning an episodic-training based meta-learning strategy, N-\nway K-shot tasks are sampled from a meta-training dataset\nDTRN, in order to imitate the few-shot test setting. Note\nthat there is no overlapping between the class label spaces\nof the two sets, i.e., CTRN ∩CTST = ∅. In each episode,\nwe ﬁrst sample N classes CN from the training class space\nrandomly. Training instances are then sampled from these\nclasses to form a support set SN and a query set QN con-\nsisting of K and Q samples from each sampled class in\nCN respectively. The sampled training instances are denoted\nas SN = {(xi, yi) | yi ∈CN, i = 1, . . . , N × K} and\nQN = {(xi, yi) | yi ∈CN, i = 1, . . . , N × Q}, where\nSN ∩QN = ∅. During training, the model uses the support\nset SN to build a classiﬁer in an inner loop, and then the\nquery set QN is used in an outer loop to evaluate and update\nthe model parameters.\n3181\n\n\nCNN\nsupport\nquery\nPGNN\nIGNN\n𝐿!\n𝐿\"\n𝐿#\n𝑓$\n𝐺!\n𝐺\"\nprotos\nFigure 3: Overview of our HGNN model in a 3-way 2-shot case. Features extracted by a feature embedding CNN are fed into a\nPGNN and an IGNN for task adaptation. In the PGNN, each node represents a class prototype, which is initialized by averaging\nthe support set features for that class. The nodes of the IGNN, on the other hand, include all the instances in the support set\nas well as a single query instance. After instance feature adaptation using the IGNN, the updated instance features are used\nto produce another set of class prototypes. Note that in the IGNN, the edge/message passing from instances in the support set\nto the query instance is one-directional from support to query (blue), while the edges between instances in the support set are\nbidirectional (black). These two sets of GNN-updated prototypes are used to predict the class of the query instances and the\npredictions are evaluated using cross-entropy losses (L1 and L2), and a consistency loss (L3) is used to enforce the prediction\nconsistency between the two GNNs.\nLabel Propagation GNNs for FSL\nBefore introducing our framework, we ﬁrst brieﬂy describe\nthe formulation of existing GNN-based FSL methods in or-\nder to highlight the differences in our design. As mentioned\nearlier, all existing GNN-based FSL methods (Garcia and\nBruna 2018; Luo et al. 2020; Liu et al. 2019; Kim et al.\n2019; Yang et al. 2020) use a GNN as a label propagation\nmodule, i.e., a label classiﬁer. To explain this, we use the\nmodel in (Garcia and Bruna 2018) as an example. In this\nmodel, the GNN is a fully-connected graph composed of M\nnodes. Each node represents an instance from either a sup-\nport set or a query set. Under the inductive setting, only one\nquery instance is included in the graph. Therefore for a N-\nway K-shot task, we have M = N × K + 1. In contrast,\nall query instances are used to construct the graph under the\ntransductive setting; we thus have M = N × (K + Q). For\na graph G with L layers, the input with M entries from the\nlth layer is denoted as F l ∈RM×(df +dl), where df is the\ndimension of the instance feature obtained from a feature\nembedding network, and dl is the dimension of label embed-\nding. In other words, each node is represented as a concate-\nnation of a visual feature embedding and a label embedding\nindicating which class each support set instance belongs to.\nThe output of the (l + 1)th layer is given by\nF l+1 = Gl(F l) = ρ\n\u0000Alφl(F l)\n\u0001\n,\n(1)\nwhere ρ is an element-wise non-linear activation function,\nφl is a linear transformation layer , and Al ∈RM×M is an\nadjacency operator in layer l. Each entry in Al is computed\nas\nAl\nij = ψ(φl(Fi), φl(Fj)),\n(2)\nwhere ψ is a neural network. Al is normalized along each\nrow.\nThe ﬁnal class prediction score of a query node is com-\nputed as\npi = F L\ni w,\n(3)\nwhere w ∈R(df +dl)×N parameterizes a linear classiﬁcation\nlayer.\nIt is clear from the formulation of this GNN that, its ob-\njective is to take the support set labels as part of the node\nrepresentation at the input layer of the graph and perform\nlabel propagation on the graph. As a result, the query sam-\nple nodes at the output layer can be used directly for label\nprediction. The alternative designs in (Kim et al. 2019; Luo\net al. 2020; Yang et al. 2020) encode the support set labels on\nthe edges, instead of nodes of the graph, but the main func-\ntionality of the GNN as a label classiﬁer remains the same.\nIn contrast, our HGNN does not encode the label informa-\ntion anywhere in the graph and it serves as a feature embed-\nding task adaptation module as described in detail next.\nHybrid GNN for FSL\nThe proposed GNN based FSL framework is illustrated in\nFigure 3. It consists of a feature extraction backbone and a\nhybrid graph neural network (HGNN) that is further com-\nposed of an instance GNN (IGNN) and a prototype GNN\n(PGNN). In each training episode, we extract the features\nfor the images in the support set SN and the query set QN\nusing a feature embedding CNN. The extracted features are\nthen fed into the GNNs as node features for task adaptation.\nA key difference between the proposed GNNs and the prior\nones in (Garcia and Bruna 2018; Luo et al. 2020; Liu et al.\n3182\n\n\n2019; Kim et al. 2019; Yang et al. 2020) is that our GNNs\ncontain no label information.\nPrototypical Graph Neural Network (PGNN)\nAs illus-\ntrated in Figure 2(a), as each class in the support set SN is\nonly represented by K samples, a FSL model is challenged\nby two issues caused by badly sampled instances, namely\noutliers and class overlapping. Our PGNN is designed to ad-\ndress the class overlapping problem. More speciﬁcally, since\nour HGNN is integrated with a ProtoNet FSL model where\neach class in SN is represented by the class mean or proto-\ntypes, we feed the prototypes into the PGNN and use mes-\nsage passing between them to ensure that class overlapping\nis minimized.\nFormally, as shown in Figure 3, our PGNN GP receives\nthe prototypes’ features FP ∈RN×d of N classes in the sup-\nport set as nodes’ features, where FPn =\n1\nK\nPK\ni=1 fθ(xni),\nwhere fθ(·) is a feature embedding network producing fea-\ntures of d dimensions, and xni is the ith image from the nth\nclass in the support set. To stabilize the training, as per stan-\ndard, we adopt the residual connection (He et al. 2016) and\nthe layer norm (Ba, Kiros, and Hinton 2016) in our GNNs.\nThus the output of an one-layer PGNN is computed as\nˆFP = LayerNorm(FP + ϕP (GP (FP ))),\n(4)\nwhere ϕ is a linear transformation layer to improve the ex-\npressive power of the adapted features with the same dimen-\nsion d, and GP denotes the same operations in Equation 1.\nFinally, the reﬁned prototypes are used to classify the query\nsamples in QN . Concretely, the probability of the ith query\nbelonging to the class j is ﬁrst computed as\npP\ni,j =\nexp(−Ed(fθ(xi), ˆFPj))\nPN\nk=1 exp(−Ed(fθ(xi), ˆFPk)\n,\n(5)\nwhere Ed(·, ·) is the Euclidean distance. To maximize the\nprobability pP\ni,j, the learned PGNN has the incentive to re-\narrange the relative position of the N prototypes so that they\nbecome more separable. This way, it becomes easier to as-\nsign each query image to the correct class with high conﬁ-\ndence.\nInstance Graph Neural Network (IGNN)\nThe PGNN\nfocuses on the inter-class relationship with the class mean or\nprototypes as graph node. It thus has limited ability to deal\nwith the outliers that are best identiﬁed when intra-class in-\nstance relationships are examined. To that end, an IGNN is\nformulated.\nSince we focus on the inductive setting, that is, only one\nquery instance is available for inference at a time, our IGNN\nconsists of the whole support set instances and one query set\ninstance as nodes (see Figure 3). For a N-way K-shot task,\nthere are N × K + 1 nodes in the graph. This means that for\nQ query set samples in a training episode, Q graph needs\nto be constructed. Formally, the ith instance graph takes the\nith query raw feature together with all support set samples’\nfeature extracted by the feature embedding network fθ(·) as\nthe nodes FIi ∈R(N×K+1)×d. Similar to the PGNN, the\nfeatures of nodes in our IGNN are reﬁned by:\nˆFIi = LayerNorm(FIi + ϕI(GI(FIi))).\n(6)\nNote that the updated nodes include all support nodes and\na single query node. However, as shown in Figure 3, the mes-\nsage passing between support and query is one-directional\n(from support to query only) and the query has no effect on\nthe support set node updating. This means that though we\nhave Q graphs, the support set instances only need to be up-\ndated once and the additional computation is only on the\nquery set instances. As a result, once trained the inference\non our IGNN is very efﬁcient.\nWith the updated support set features, to adhere to the\nProtoNet pipeline, we compute another set of prototypes for\neach class by computing the updated support set instance\nclass means. Finally, the probability of the ith query node\nbelonging to class j is\npI\ni,j =\nexp(−Ed( ˆFIiq,\n˜\nFIiP j))\nPN\nk=1 exp(−Ed( ˆFIiq,\n˜\nFIiP k))\n,\n(7)\nwhere ˆFIiq is the updated query node feature in the ith\nIGNN, and\n˜\nFIiP j is the prototype for the jth class. To maxi-\nmize the probability pI\ni,j, the learned IGNN is encouraged to\nidentify the outlying support set samples and use the other\ninstances of the same class to ‘pull’ it closer in the updated\nembedding space. Together with PGNN, IGNN can create a\nmore friendly embedding space for classifying a query sam-\nple by comparing it with the support set samples. This is il-\nlustrated in Figure 2(b) and veriﬁed both quantitatively and\nqualitatively in our experiments.\nTraining Objectives\nDuring training, the two GNNs in our HGNN make predic-\ntions for each query using their respective prototypes, and\ntrained together with the shared feature embedding network\nfθ via cross entropy losses. All the parameters in fθ, PGNN\nGP , and IGNN GI are end-to-end trained.\nSpeciﬁcally, the classiﬁcation losses on PGNN and IGNN\nare\nL1 =\nQ\nX\ni\nN\nX\nj\n−I(yi == j) log(pP\ni,j),\n(8)\nL2 =\nQ\nX\ni\nN\nX\nj\n−I(yi == j) log(pI\ni,j),\n(9)\nwhere yi is the ground truth label of the ith query and I(x)\nis an indicator function: I(x) = 1 when x is true and 0 oth-\nerwise.\nIn addition, to make the prediction scores for each query\nfrom the GNNs to be consistent in our HGNN, a symmetric\nKullback–Leibler (KL) divergence loss is used:\nL3 =\nN\nX\nj\npI\ni,j log pI\ni,j\npP\ni,j\n+\nN\nX\nj\npP\ni,j log pP\ni,j\npI\ni,j\n.\n(10)\nThus, during training, the total loss is:\nL(θ, φI, φP ) = L1 + L2 + L3.\n(11)\n3183\n\n\nMethod\nBackbone\n1-shot\n5-shot\nProtoNet∗\nConv4\n52.78 ± 0.45\n71.26 ± 0.36\nMAML\nConv4\n48.70 ± 1.84\n63.10 ± 0.92\nCentroid\nConv4\n53.14 ± 1.06\n71.45 ± 0.72\nNeg-Cosine\nConv4\n52.84 ± 0.76\n70.41 ± 0.66\nFEAT\nConv4\n55.15 ± 0.20\n71.61 ± 0.16\nGNN∗\nConv4\n52.21 ± 0.20\n67.03 ± 0.17\nEGNN\nConv4\n51.65 ± 0.55\n66.85 ± 0.49\nEGNN∗\nConv4\n48.99 ± 0.59\n61.99 ± 0.43\nTPN †\nConv4\n53.75 ± n/a\n69.43 ± n/a\nBGNN∗\nConv4\n52.35 ± 0.42\n67.35 ± 0.35\nDPGN∗\nConv4\n53.22 ± 0.31\n65.34 ± 0.29\nHGNN\nConv4\n55.63 ± 0.20\n72.48 ± 0.16\nProtoNet∗\nResNet-12\n62.41 ± 0.44\n80.49 ± 0.29\nNeg-Cosine\nResNet-12\n63.85 ± 0.81\n81.57 ± 0.56\nDistill\nResNet-12\n64.82 ± 0.60\n82.14 ± 0.43\nDSN-MR\nResNet-12\n64.60 ± 0.72\n79.51 ± 0.50\nDeepEMD\nResNet-12\n65.91 ± 0.82\n82.41 ± 0.56\nFEAT\nResNet-12\n66.78 ± 0.20\n82.05 ± 0.14\nE3BM\nResNet-25\n64.3 ± n/a\n81.0 ± n/a\nMABAS\nResNet-12\n65.08 ±0.86\n82.70 ±0.54\nAPN\nCapsuleNet\n66.43 ±0.26\n82.13 ±0.23\nPSST\nWRN-28-10\n64.16 ±0.44\n80.64 ±0.32\nFRN\nResNet-12\n66.45 ±0.19\n82.83 ±0.13\nHGNN\nResNet-12\n67.02 ± 0.20\n83.00 ± 0.13\nTable 1: 5-way 1/5-shot classiﬁcation accuracy (%) and 95%\nconﬁdence interval on MiniImageNet. ∗indicates our re-\nproduced results with the same pre-trained backbone, and\n† means transductive setting.\nDuring meta-test, the class prediction of a query is given\nby the mean of two prediction scores from the two GNNs in\nour HGNN.\nExperiments\nDatasets and Settings\nDatasets\nThree\nwidely\nused\nFSL\nbenchmarks,\nMiniImageNet (Vinyals et al. 2016), TieredImageNet\n(Ren et al. 2018) and CUB-200-2011 (Wah et al. 2011) are\nused in our experiments. MiniImageNet contains a total\nof 100 classes and 600 images per class. We follow the\nstandard splits provided in (Vinyals et al. 2016), consisting\nof 64 classes for training, and 16 classes and 20 classes\nfor validation and testing respectively. TieredImageNet is\na larger subset of the ImageNet ILSVRC-12, comprising\n779,165 images from 608 classes. They are divided into\n351, 97, and 160 classes for training, validation and testing\nrespectively (Chen et al. 2018). Different from the other\ntwo datasets, CUB-200-2011 is a ﬁne-grained classiﬁcation\ndataset. It includes 11,778 images from 200 different bird\nclasses. The 200 classes are divided into 100, 50, 50 classes\nfor training, validation and testing respectively as in (Liu\net al. 2020; Afrasiyabi, Lalonde, and Gagn´e 2020). In\nall datasets, images are downsampled to 84 × 84 as per\nstandard.\nMethod\nBackbone\n1-shot\n5-shot\nProtoNet∗\nConv4\n50.89 ± 0.21\n69.26 ± 0.18\nMAML\nConv4\n51.67 ± 1.81\n70.30 ± 0.08\nE3BM\nConv4\n52.1 ± n/a\n70.2 ± n/a\nGNN∗\nConv4\n42.37 ± 0.20\n62.54 ± 0.19\nEGNN∗\nConv4\n47.40 ± 0.43\n62.66 ± 0.57\nBGNN∗\nConv4\n49.41 ± 0.43\n65.27 ± 0.35\nDPGN∗\nConv4\n53.99 ± 0.31\n69.86± 0.28\nHGNN\nConv4\n56.05 ± 0.21\n72.82 ± 0.18\nProtoNet∗\nResNet-12\n69.63 ± 0.53\n84.82 ± 0.36\nDistill\nResNet-12\n71.52 ± 0.69\n86.03 ± 0.49\nDSN-MR\nResNet-12\n67.39 ± 0.82\n82.85 ± 0.56\nDeepEMD\nResNet-12\n71.16 ± 0.87\n86.03 ± 0.58\nFEAT\nResNet-12\n70.80 ± 0.23\n84.79 ± 0.16\nE3BM\nResNet-12\n70.0 ± n/a\n85.0 ± n/a\nAPN\nResNet-12\n69.87 ±0.32\n86.35 ±0.41\nFRN\nResNet-12\n71.16 ±0.22\n86.01 ±0.15\nHGNN\nResNet-12\n72.05 ±0.23\n86.49 ± 0.15\nTable 2: Results on TieredImageNet\nMethods\nBackbone\n1-shot\n5-shot\nProtoNet∗\nConv4\n51.25 ± 0.21\n72.26 ± 0.18\nAdversarial\nConv4\n63.30 ± 0.94\n81.35 ± 0.67\nHGNN\nConv4\n69.02 ± 0.22\n83.20 ± 0.15\nProtoNet∗\nResNet-12\n68.11 ± 0.21\n87.33 ± 0.13\nDeepEMD\nResNet-12\n77.14 ± 0.29\n88.98 ± 0.49\nNeg-Cosine\nResNet-18\n72.66 ± 0.85\n89.40 ± 0.43\nCentroid\nResNet-18\n74.22 ± 1.09\n88.65 ± 0.55\nHGNN\nResNet-12\n78.58 ± 0.20\n90.02 ± 0.12\nTable 3: Results on CUB-200-2011\nFeature Embedding Network\nAs in many other CNN-\nbased visual recognition tasks, a feature embedding network\nis required in a FSL model and the choice of its backbone\nhas a major impact on its performance. For fair comparisons\nwith prior works, two widely used backbones are adopted\nin our experiments, namely Conv4 and ResNet-12. Follow-\ning (Snell, Swersky, and Zemel 2017), the Conv4 backbone\nhas four convolutional blocks and its ﬁnal output feature di-\nmension is 64. The ResNet-12 backbone is used in most\nof the state-of-the-art models (Zhang et al. 2020; Ye et al.\n2020; Liu et al. 2020). It consists of four residual blocks, and\nthe output feature dimension is 640. Following the common\npractice (Liu et al. 2020; Ye et al. 2020; Zhang et al. 2020),\nwe pre-train our feature embedding network with supervised\nlearning on the whole training set before the episodic meta-\nlearning stage.\nBaselines\nThree\ntypes\nof\nbaselines\nare\nchosen\nfor\ncomparisons: (1) representative FSL methods including\nMAML (Finn, Abbeel, and Levine 2017) (optimization-\nbased),\nProtoNet\n(Snell,\nSwersky,\nand\nZemel\n2017)\n(embedding-based),\nFEAT\n(Ye\net\nal.\n2020)\n(task-\nadaptation),\nand\nDistill\n(Tian\net\nal.\n2020)\n(without\nepisodic meta-learning), (2) GNN-based methods includ-\n3184\n\n\n(a) PGNN\n(b) IGNN\nFigure 4: Qualitative results on PGNN and IGNN alleviating the class overlapping and outlier issues respectively. The t-SNE\nprojection of instances and prototypes of 5 classes in CUB-200-2011 are shown here. In (a), we use vectors to indicate how class\nprototypes moves from before (CNN protos) to after PGNN updates (PGNN Protos). It can be seen that after the PGNN update,\nthe 5 class prototypes are clearly more separable. In (b), the distance between an outlying instance and its class prototypes\n(before and after IGNN updates) are highlighted using color-coded straight lines (same color means the same outlier). IGNN\nclearly neutralizes the negative impact of outliers by pulling those outliers closer to their prototypes, indicated by the shorter\nlines to IGNN protos.\ning GNN (Garcia and Bruna 2018), EGNN (Kim et al.\n2019), TPN (Liu et al. 2019), DPGN (Yang et al. 2020)\nand BGNN (Luo et al. 2020), and (3) the state-of-the-art\n(SOTA) methods published in 2020 and 2021, including\nCentroid (Afrasiyabi, Lalonde, and Gagn´e 2020), Neg-\nCosine (Liu et al. 2020), DSN-MR (Simon et al. 2020),\nDeepEMD (Zhang et al. 2020), E3BM (Liu, Schiele, and\nSun 2020), MABAS (Kim, Kim, and Kim 2020), APN (Lu,\nPang, and Zhang 2020), Adversarial (Afrasiyabi, Lalonde,\nand Gagn´e 2020), ArL (Zhang et al. 2021), PSST (Chen\net al. 2021), FRN (Wertheimer, Tang, and Hariharan 2021).\nMain Results\nThe comparative results on MiniImageNet, TieredImageNet\nand CUB-200-2011 are shown in Tables 1, 2 and 3 respec-\ntively. The following observations can be made. (1) Our\nHGNN achieves the new SOTA on all three datasets under\nboth the 1-shot and 5-shot settings and with both the Conv4\nand ResNet-12 backbones, validating its effectiveness. (2)\nOur GNN-FSL model signiﬁcantly outperforms all ﬁve ex-\nisting GNN-FSL models (Garcia and Bruna 2018; Luo et al.\n2020; Liu et al. 2019; Kim et al. 2019; Yang et al. 2020) un-\nder the inductive setting1. This veriﬁes our hypothesis that\njointly meta-learning a GNN-based label propagation/clas-\nsiﬁcation module with a feature embedding network con-\nfuses the objectives of rapid learning and feature reuse. This\nresults in inferior performance under the inductive setting\nwhere, without the access to the full query set, the usefulness\nof label propagation is limited. (3) Overall, the advantages\nof our HGNN over the SOTA alternatives under the more\nchallenging 1-shot setting and with the ﬁne-grained CUB-\n200-2011 dataset are more pronounced. This is expected:\nour IGNN and PGNN are designed to address the badly sam-\npled shots problems in the support set. With fewer shots and\n1For fair comparison, we use the same backbone pre-trained on\nthe base training dataset.\nmore inter-class overlapping in the ﬁne-grained cases, these\nproblems are more acute and hence the clearer advantages\nof our HGNN.\nAre PGNN and IGNN Doing Their jobs?\nOur PGNN and IGNN are designed to solve the inter-class\noverlapping and outlier issues caused by badly sampled in-\nstances in each few-shot classiﬁcation task. These two is-\nsues severely impact the performance of a few-shot learning\nmethod, but are rarely discussed before. Figure 4 visualizes\nhow the feature embedding distributions of the support set\ninstances in a task sampled from CUB-200-2011 under the\n5-way 5-shot setting. This qualitative result aims to show\nhow the 5 prototypes and outlying instances are distributed\nbefore and after the output of the feature embedding CNN\nis updated by the two GNNs. We can see clearly that these\ntwo GNNs are indeed addressing the two issues as we antic-\nipated: the PGNN pushes the class prototypes further away\nfrom each other to tackle the class overlapping problem,\nwhile the IGNN produces a more compact intra-class dis-\ntribution by shortening the distance between the outliers and\nprototypes, mitigating the outlier problem.\nConclusions\nWe have proposed a novel GNN-based FSL model. Dif-\nferent from the existing GNN-FSL methods which utilize\nGNN as a label propagation tool to be jointly meta-learned\nwith a feature embedding network, we argue that a GNN\nis best used in FSL as a feature embedding task adaptation\nmodule. In particular, it should address the outlying samples\nand class overlapping problems commonly existing in FSL\nthrough the task adaptation. To that end, an instance GNN\nand a prototype GNN are formulated and their complemen-\ntarity is exploited in a hybrid GNN framework. Extensive\nexperiments demonstrate that our HGNN is indeed effective\nin addressing the poor shot sampling problems, yielding new\nstate-of-the-art on three benchmarks.\n3185\n\n\nReferences\nAfrasiyabi, A.; Lalonde, J.-F.; and Gagn´e, C. 2020. Associa-\ntive Alignment for Few-shot Image Classiﬁcation. In ECCV.\nAllen, K. R.; Shelhamer, E.; Shin, H.; and Tenenbaum, J. B.\n2019. Inﬁnite mixture prototypes for few-shot learning. In\nICML.\nBa, J. L.; Kiros, J. R.; and Hinton, G. E. 2016. Layer nor-\nmalization. CoRR, 1607.06450.\nChen, L.-C.; Papandreou, G.; Kokkinos, I.; Murphy, K.; and\nYuille, A. L. 2017. Deeplab: Semantic image segmentation\nwith deep convolutional nets, atrous convolution, and fully\nconnected crfs. PAMI, 40(4): 834–848.\nChen, W.-Y.; Liu, Y.-C.; Kira, Z.; Wang, Y.-C. F.; and\nHuang, J.-B. 2018. A Closer Look at Few-shot Classiﬁca-\ntion. In ICLR.\nChen, Z.; Ge, J.; Zhan, H.; Huang, S.; and Wang, D. 2021.\nPareto Self-Supervised Training for Few-Shot Learning. In\nCVPR, 13663–13672.\nFinn, C.; Abbeel, P.; and Levine, S. 2017. Model-Agnostic\nMeta-Learning for Fast Adaptation of Deep Networks. In\nICML.\nGarcia, V.; and Bruna, J. 2018. Few-shot learning with graph\nneural networks. In ICLR.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual\nlearning for image recognition. In CVPR.\nHospedales, T.; Antoniou, A.; Micaelli, P.; and Storkey,\nA. 2020.\nMeta-Learning in Neural Networks: A Survey.\narXiv:2004.05439.\nKim, J.; Kim, H.; and Kim, G. 2020.\nModel-Agnostic\nBoundary-Adversarial Sampling for Test-Time Generaliza-\ntion in Few-Shot learning. In ECCV.\nKim, J.; Kim, T.; Kim, S.; and Yoo, C. D. 2019.\nEdge-\nlabeling graph neural network for few-shot learning.\nIn\nCVPR.\nKipf, T. N.; and Welling, M. 2017. Semi-supervised classi-\nﬁcation with graph convolutional networks. In ICLR.\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Im-\nagenet classiﬁcation with deep convolutional neural net-\nworks. In NeurIPS.\nLi, W.; Wang, L.; Xu, J.; Huo, J.; Gao, Y.; and Luo, J. 2019.\nRevisiting local descriptor based image-to-class measure for\nfew-shot learning. In CVPR.\nLiu, B.; Cao, Y.; Lin, Y.; Li, Q.; Zhang, Z.; Long, M.; and\nHu, H. 2020. Negative Margin Matters: Understanding Mar-\ngin in Few-shot Classiﬁcation. In ECCV.\nLiu, Y.; Lee, J.; Park, M.; Kim, S.; Yang, E.; Hwang, S. J.;\nand Yang, Y. 2019. Learning to Propagate Labels: Transduc-\ntive Propagation Network for Few-shot Learning. In ICLR.\nLiu, Y.; Schiele, B.; and Sun, Q. 2020.\nAn ensemble\nof epoch-wise empirical Bayes for few-shot learning.\nIn\nECCV.\nLu, W.; Pang, C.; and Zhang, B. 2020.\nAttentive Proto-\ntype Few-shot Learning with Capsule Network-based Em-\nbedding. In ECCV.\nLuo, Y.; Huang, Z.; Zhang, Z.; Wang, Z.; Baktashmotlagh,\nM.; and Yang, Y. 2020. Learning from the Past: Continual\nMeta-Learning via Bayesian Graph Modeling. In AAAI.\nMunkhdalai, T.; and Yu, H. 2017. Meta Networks. In ICML.\nNichol, A.; Achiam, J.; and Schulman, J. 2018. On First-\nOrder Meta-Learning Algorithms. CoRR, abs/1803.02999.\nOh, J.; Yoo, H.; Kim, C.; and Yun, S.-Y. 2020. Does MAML\nreally want feature reuse only? CoRR, abs/2008.08882.\nOreshkin, B.; L´opez, P. R.; and Lacoste, A. 2018. Tadam:\nTask dependent adaptive metric for improved few-shot\nlearning. In NeurIPS.\nRaghu, A.; Raghu, M.; Bengio, S.; and Vinyals, O. 2020.\nRapid learning or feature reuse? towards understanding the\neffectiveness of maml. In ICLR.\nRajeswaran, A.; Finn, C.; Kakade, S.; and Levine, S. 2019.\nMeta-Learning with Implicit Gradients. In NeurIPS.\nRavi, S.; and Larochelle, H. 2017. Optimization as a Model\nfor Few-Shot Learning. In ICLR.\nRen, M.; Triantaﬁllou, E.; Ravi, S.; Snell, J.; Swersky, K.;\nTenenbaum, J. B.; Larochelle, H.; and Zemel, R. S. 2018.\nMeta-learning for semi-supervised few-shot classiﬁcation.\nIn ICLR.\nRen, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster r-\ncnn: Towards real-time object detection with region proposal\nnetworks. In NeurIPS.\nSimon, C.; Koniusz, P.; Nock, R.; and Harandi, M. 2020.\nAdaptive Subspaces for Few-Shot Learning. In CVPR.\nSnell, J.; Swersky, K.; and Zemel, R. 2017.\nPrototypical\nnetworks for few-shot learning. In NeurIPS.\nSun, Q.; Liu, Y.; Chua, T.-S.; and Schiele, B. 2019. Meta-\ntransfer learning for few-shot learning. In CVPR.\nSung, F.; Yang, Y.; Zhang, L.; Xiang, T.; Torr, P. H.; and\nHospedales, T. M. 2018. Learning to compare: Relation net-\nwork for few-shot learning. In CVPR.\nTian, Y.; Wang, Y.; Krishnan, D.; Tenenbaum, J. B.; and\nIsola, P. 2020. Rethinking Few-Shot Image Classiﬁcation:\na Good Embedding Is All You Need? In ECCV.\nVinyals, O.; Blundell, C.; Lillicrap, T.; Wierstra, D.; et al.\n2016. Matching networks for one shot learning. In NeurIPS.\nWah, C.; Branson, S.; Welinder, P.; Perona, P.; and Belongie,\nS. 2011.\nThe Caltech-UCSD Birds-200-2011 Dataset.\nTechnical Report CNS-TR-2011-001, California Institute of\nTechnology.\nWertheimer, D.; Tang, L.; and Hariharan, B. 2021. Few-Shot\nClassiﬁcation With Feature Map Reconstruction Networks.\nIn CVPR, 8012–8021.\nXu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A.; Salakhudi-\nnov, R.; Zemel, R.; and Bengio, Y. 2015. Show, attend and\ntell: Neural image caption generation with visual attention.\nIn ICML.\nYang, L.; Li, L.; Zhang, Z.; Zhou, X.; Zhou, E.; and Liu, Y.\n2020. DPGN: Distribution Propagation Graph Network for\nFew-shot Learning. In CVPR.\n3186\n\n\nYe, H.-J.; Hu, H.; Zhan, D.-C.; and Sha, F. 2020. Few-shot\nlearning via embedding adaptation with set-to-set functions.\nIn CVPR.\nYu, T.; Li, D.; Yang, Y.; Hospedales, T. M.; and Xiang, T.\n2019. Robust person re-identiﬁcation by modelling feature\nuncertainty. In ICCV, 552–561.\nZhang, C.; Cai, Y.; Lin, G.; and Shen, C. 2020. DeepEMD:\nFew-Shot Image Classiﬁcation with Differentiable Earth\nMover’s Distance and Structured Classiﬁers. In CVPR.\nZhang, H.; Koniusz, P.; Jian, S.; Li, H.; and Torr, P. H.\n2021.\nRethinking Class Relations: Absolute-relative Su-\npervised and Unsupervised Few-shot Learning. In CVPR,\n9432–9441.\n3187\n"
}