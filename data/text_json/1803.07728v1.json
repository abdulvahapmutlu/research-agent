{
  "filename": "1803.07728v1.pdf",
  "num_pages": 16,
  "pages": [
    "Published as a conference paper at ICLR 2018\nUNSUPERVISED REPRESENTATION LEARNING BY PRE-\nDICTING IMAGE ROTATIONS\nSpyros Gidaris, Praveer Singh, Nikos Komodakis\nUniversity Paris-Est, LIGM\nEcole des Ponts ParisTech\n{spyros.gidaris,praveer.singh,nikos.komodakis}@enpc.fr\nABSTRACT\nOver the last years, deep convolutional neural networks (ConvNets) have trans-\nformed the ﬁeld of computer vision thanks to their unparalleled capacity to learn\nhigh level semantic image features. However, in order to successfully learn those\nfeatures, they usually require massive amounts of manually labeled data, which\nis both expensive and impractical to scale. Therefore, unsupervised semantic fea-\nture learning, i.e., learning without requiring manual annotation effort, is of crucial\nimportance in order to successfully harvest the vast amount of visual data that are\navailable today. In our work we propose to learn image features by training Con-\nvNets to recognize the 2d rotation that is applied to the image that it gets as input.\nWe demonstrate both qualitatively and quantitatively that this apparently simple\ntask actually provides a very powerful supervisory signal for semantic feature\nlearning. We exhaustively evaluate our method in various unsupervised feature\nlearning benchmarks and we exhibit in all of them state-of-the-art performance.\nSpeciﬁcally, our results on those benchmarks demonstrate dramatic improvements\nw.r.t. prior state-of-the-art approaches in unsupervised representation learning and\nthus signiﬁcantly close the gap with supervised feature learning. For instance, in\nPASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model\nachieves the state-of-the-art (among unsupervised methods) mAP of 54.4% that is\nonly 2.4 points lower from the supervised case. We get similarly striking results\nwhen we transfer our unsupervised learned features on various other tasks, such\nas ImageNet classiﬁcation, PASCAL classiﬁcation, PASCAL segmentation, and\nCIFAR-10 classiﬁcation. The code and models of our paper will be published on:\nhttps://github.com/gidariss/FeatureLearningRotNet.\n1\nINTRODUCTION\nIn recent years, the widespread adoption of deep convolutional neural networks (LeCun et al., 1998)\n(ConvNets) in computer vision, has lead to a tremendous progress in the ﬁeld. Speciﬁcally, by train-\ning ConvNets on the object recognition (Russakovsky et al., 2015) or the scene classiﬁcation (Zhou\net al., 2014) tasks with a massive amount of manually labeled data, they manage to learn power-\nful visual representations suitable for image understanding tasks. For instance, the image features\nlearned by ConvNets in this supervised manner have achieved excellent results when they are trans-\nferred to other vision tasks, such as object detection (Girshick, 2015), semantic segmentation (Long\net al., 2015), or image captioning (Karpathy & Fei-Fei, 2015). However, supervised feature learning\nhas the main limitation of requiring intensive manual labeling effort, which is both expensive and\ninfeasible to scale on the vast amount of visual data that are available today.\nDue to that, there is lately an increased interest to learn high level ConvNet based representations\nin an unsupervised manner that avoids manual annotation of visual data. Among them, a promi-\nnent paradigm is the so-called self-supervised learning that deﬁnes an annotation free pretext task,\nusing only the visual information present on the images or videos, in order to provide a surrogate\nsupervision signal for feature learning. For example, in order to learn features, Zhang et al. (2016a)\nand Larsson et al. (2016) train ConvNets to colorize gray scale images, Doersch et al. (2015) and\nNoroozi & Favaro (2016) predict the relative position of image patches, and Agrawal et al. (2015)\npredict the egomotion (i.e., self-motion) of a moving vehicle between two consecutive frames. The\n1\narXiv:1803.07728v1  [cs.CV]  21 Mar 2018\n",
    "Published as a conference paper at ICLR 2018\nrationale behind such self-supervised tasks is that solving them will force the ConvNet to learn se-\nmantic image features that can be useful for other vision tasks. In fact, image representations learned\nwith the above self-supervised tasks, although they have not managed to match the performance\nof supervised-learned representations, they have proved to be good alternatives for transferring on\nother vision tasks, such as object recognition, object detection, and semantic segmentation (Zhang\net al., 2016a; Larsson et al., 2016; Zhang et al., 2016b; Larsson et al., 2017; Doersch et al., 2015;\nNoroozi & Favaro, 2016; Noroozi et al., 2017; Pathak et al., 2016a; Doersch & Zisserman, 2017).\nOther successful cases of unsupervised feature learning are clustering based methods (Dosovitskiy\net al., 2014; Liao et al., 2016; Yang et al., 2016), reconstruction based methods (Bengio et al., 2007;\nHuang et al., 2007; Masci et al., 2011), and methods that involve learning generative probabilistic\nmodels Goodfellow et al. (2014); Donahue et al. (2016); Radford et al. (2015).\nOur work follows the self-supervised paradigm and proposes to learn image representations by train-\ning ConvNets to recognize the geometric transformation that is applied to the image that it gets as\ninput. More speciﬁcally, we ﬁrst deﬁne a small set of discrete geometric transformations, then each\nof those geometric transformations are applied to each image on the dataset and the produced trans-\nformed images are fed to the ConvNet model that is trained to recognize the transformation of each\nimage. In this formulation, it is the set of geometric transformations that actually deﬁnes the classiﬁ-\ncation pretext task that the ConvNet model has to learn. Therefore, in order to achieve unsupervised\nsemantic feature learning, it is of crucial importance to properly choose those geometric transfor-\nmations (we further discuss this aspect of our methodology in section 2.2). What we propose is to\ndeﬁne the geometric transformations as the image rotations by 0, 90, 180, and 270 degrees. Thus,\nthe ConvNet model is trained on the 4-way image classiﬁcation task of recognizing one of the four\nimage rotations (see Figure 2). We argue that in order a ConvNet model to be able recognize the\nrotation transformation that was applied to an image it will require to understand the concept of\nthe objects depicted in the image (see Figure 1), such as their location in the image, their type, and\ntheir pose. Throughout the paper we support that argument both qualitatively and quantitatively.\nFurthermore we demonstrate on the experimental section of the paper that despite the simplicity of\nour self-supervised approach, the task of predicting rotation transformations provides a powerful\nsurrogate supervision signal for feature learning and leads to dramatic improvements on the relevant\nbenchmarks.\nNote that our self-supervised task is different from the work of Dosovitskiy et al. (2014) and Agrawal\net al. (2015) that also involves geometric transformations. Dosovitskiy et al. (2014) train a ConvNet\nmodel to yield representations that are discriminative between images and at the same time invariant\non geometric and chromatic transformations. In contrast, we train a ConvNet model to recognize the\ngeometric transformation applied to an image. It is also fundamentally different from the egomotion\nmethod of Agrawal et al. (2015), which employs a ConvNet model with siamese like architecture\nthat takes as input two consecutive video frames and is trained to predict (through regression) their\ncamera transformation. Instead, in our approach, the ConvNet takes as input a single image to\nwhich we have applied a random geometric transformation (i.e., rotation) and is trained to recognize\n(through classiﬁcation) this geometric transformation without having access to the initial image.\nOur contributions are:\n• We propose a new self-supervised task that is very simple and at the same time, as we\ndemonstrate throughout the paper, offers a powerful supervisory signal for semantic feature\nlearning.\n• We exhaustively evaluate our self-supervised method under various settings (e.g. semi-\nsupervised or transfer learning settings) and in various vision tasks (i.e., CIFAR-10, Ima-\ngeNet, Places, and PASCAL classiﬁcation, detection, or segmentation tasks).\n• In all of them, our novel self-supervised formulation demonstrates state-of-the-art results\nwith dramatic improvements w.r.t. prior unsupervised approaches.\n• As a consequence we show that for several important vision tasks, our self-supervised\nlearning approach signiﬁcantly narrows the gap between unsupervised and supervised fea-\nture learning.\nIn the following sections, we describe our self-supervised methodology in §2, we provide experi-\nmental results in §3, and ﬁnally we conclude in §4.\n2\n",
    "Published as a conference paper at ICLR 2018\n90◦rotation\n270◦rotation\n180◦rotation\n0◦rotation\n270◦rotation\nFigure 1: Images rotated by random multiples of 90 degrees (e.g., 0, 90, 180, or 270 degrees). The\ncore intuition of our self-supervised feature learning approach is that if someone is not aware of the\nconcepts of the objects depicted in the images, he cannot recognize the rotation that was applied to\nthem.\n2\nMETHODOLOGY\n2.1\nOVERVIEW\nThe goal of our work is to learn ConvNet based semantic features in an unsupervised manner. To\nachieve that goal we propose to train a ConvNet model F(.) to estimate the geometric transformation\napplied to an image that is given to it as input. Speciﬁcally, we deﬁne a set of K discrete geometric\ntransformations G = {g(.|y)}K\ny=1, where g(.|y) is the operator that applies to image X the geometric\ntransformation with label y that yields the transformed image Xy = g(X|y). The ConvNet model\nF(.) gets as input an image Xy∗(where the label y∗is unknown to model F(.)) and yields as output\na probability distribution over all possible geometric transformations:\nF(Xy∗|θ) = {F y(Xy∗|θ)}K\ny=1,\n(1)\nwhere F y(Xy∗|θ) is the predicted probability for the geometric transformation with label y and θ\nare the learnable parameters of model F(.).\nTherefore, given a set of N training images D = {Xi}N\ni=0, the self-supervised training objective\nthat the ConvNet model must learn to solve is:\nmin\nθ\n1\nN\nN\nX\ni=1\nloss(Xi, θ),\n(2)\nwhere the loss function loss(.) is deﬁned as:\nloss(Xi, θ) = −1\nK\nK\nX\ny=1\nlog(F y(g(Xi|y)|θ)).\n(3)\nIn the following subsection we describe the type of geometric transformations that we propose in\nour work.\n2.2\nCHOOSING GEOMETRIC TRANSFORMATIONS: IMAGE ROTATIONS\nIn the above formulation, the geometric transformations G must deﬁne a classiﬁcation task that\nshould force the ConvNet model to learn semantic features useful for visual perception tasks (e.g.,\nobject detection or image classiﬁcation). In our work we propose to deﬁne the set of geometric\ntransformations G as all the image rotations by multiples of 90 degrees, i.e., 2d image rotations by\n0, 90, 180, and 270 degrees (see Figure 2). More formally, if Rot(X, φ) is an operator that rotates\nimage X by φ degrees, then our set of geometric transformations consists of the K = 4 image\nrotations G = {g(X|y)}4\ny=1, where g(X|y) = Rot(X, (y −1)90).\nForcing the learning of semantic features: The core intuition behind using these image rotations\nas the set of geometric transformations relates to the simple fact that it is essentially impossible for a\nConvNet model to effectively perform the above rotation recognition task unless it has ﬁrst learnt to\nrecognize and detect classes of objects as well as their semantic parts in images. More speciﬁcally,\n3\n",
    "Published as a conference paper at ICLR 2018\nRotated image: X\n0\nRotated image: X\n3\nRotated image: X\n2\nRotated image: X\n1\nConvNet \nmodel F(.)\nConvNet \nmodel F(.)\nConvNet \nmodel F(.)\nConvNet \nmodel F(.)\nImage X\nPredict 270 degrees rotation (y=3)\nRotate 270 degrees\ng( X , y=3)\nRotate 180 degrees\ng( X , y=2)\nRotate 90 degrees\ng( X , y=1)\nRotate 0 degrees\ng( X , y=0)\nMaximize prob.\nF\n3( X\n3)\nPredict 0 degrees rotation (y=0)\nMaximize prob.\nF\n2( X\n2)\nMaximize prob.\nF\n1( X\n1)\nMaximize prob.\nF\n0( X\n0)\nPredict 180 degrees rotation (y=2)\nPredict 90 degrees rotation (y=1)\nObjectives:\nFigure 2: Illustration of the self-supervised task that we propose for semantic feature learning.\nGiven four possible geometric transformations, the 0, 90, 180, and 270 degrees rotations, we train\na ConvNet model F(.) to recognize the rotation that is applied to the image that it gets as input.\nF y(Xy∗) is the probability of rotation transformation y predicted by model F(.) when it gets as\ninput an image that has been transformed by the rotation transformation y∗.\nto successfully predict the rotation of an image the ConvNet model must necessarily learn to localize\nsalient objects in the image, recognize their orientation and object type, and then relate the object\norientation with the dominant orientation that each type of object tends to be depicted within the\navailable images. In Figure 3b we visualize some attention maps generated by a model trained\non the rotation recognition task. These attention maps are computed based on the magnitude of\nactivations at each spatial cell of a convolutional layer and essentially reﬂect where the network\nputs most of its focus in order to classify an input image. We observe, indeed, that in order for the\nmodel to accomplish the rotation prediction task it learns to focus on high level object parts in the\nimage, such as eyes, nose, tails, and heads. By comparing them with the attention maps generated\nby a model trained on the object recognition task in a supervised way (see Figure 3a) we observe\nthat both models seem to focus on roughly the same image regions. Furthermore, in Figure 4 we\nvisualize the ﬁrst layer ﬁlters that were learnt by an AlexNet model trained on the proposed rotation\nrecognition task. As can be seen, they appear to have a big variety of edge ﬁlters on multiple\norientations and multiple frequencies. Remarkably, these ﬁlters seem to have a greater amount of\nvariety even than the ﬁlters learnt by the supervised object recognition task.\nAbsence of low-level visual artifacts: An additional important advantage of using image rotations\nby multiples of 90 degrees over other geometric transformations, is that they can be implemented by\nﬂip and transpose operations (as we will see below) that do not leave any easily detectable low-level\nvisual artifacts that will lead the ConvNet to learn trivial features with no practical value for the\nvision perception tasks. In contrast, had we decided to use as geometric transformations, e.g., scale\nand aspect ratio image transformations, in order to implement them we would need to use image\nresizing routines that leave easily detectable image artifacts.\nWell-posedness: Furthermore, human captured images tend to depict objects in an “up-standing”\nposition, thus making the rotation recognition task well deﬁned, i.e., given an image rotated by 0,\n90, 180, or 270 degrees, there is usually no ambiguity of what is the rotation transformation (with\nthe exception of images that only depict round objects). In contrast, that is not the case for the object\nscale that varies signiﬁcantly on human captured images.\nImplementing image rotations: In order to implement the image rotations by 90, 180, and 270\ndegrees (the 0 degrees case is the image itself), we use ﬂip and transpose operations. Speciﬁcally,\n4\n",
    "Published as a conference paper at ICLR 2018\nInput images on the models\nConv1 27 × 27\nConv3 13 × 13\nConv5 6 × 6\n(a) Attention maps of supervised model\nConv1 27 × 27\nConv3 13 × 13\nConv5 6 × 6\n(b) Attention maps of our self-supervised model\nFigure 3: Attention maps generated by an AlexNet model trained (a) to recognize objects (super-\nvised), and (b) to recognize image rotations (self-supervised). In order to generate the attention map\nof a conv. layer we ﬁrst compute the feature maps of this layer, then we raise each feature activation\non the power p, and ﬁnally we sum the activations at each location of the feature map. For the conv.\nlayers 1, 2, and 3 we used the powers p = 1, p = 2, and p = 4 respectively. For visualization of\nour self-supervised model’s attention maps for all the rotated versions of the images see Figure 6 in\nappendix A.\nfor 90 degrees rotation we ﬁrst transpose the image and then ﬂip it vertically (upside-down ﬂip),\nfor 180 degrees rotation we ﬂip the image ﬁrst vertically and then horizontally (left-right ﬂip), and\nﬁnally for 270 degrees rotation we ﬁrst ﬂip vertically the image and then we transpose it.\n2.3\nDISCUSSION\nThe simple formulation of our self-supervised task has several advantages. It has the same computa-\ntional cost as supervised learning, similar training convergence speed (that is signiﬁcantly faster than\nimage reconstruction based approaches; our AlexNet model trains in around 2 days using a single\nTitan X GPU), and can trivially adopt the efﬁcient parallelization schemes devised for supervised\nlearning (Goyal et al., 2017), making it an ideal candidate for unsupervised learning on internet-\nscale data (i.e., billions of images). Furthermore, our approach does not require any special image\npre-processing routine in order to avoid learning trivial features, as many other unsupervised or\nself-supervised approaches do. Despite the simplicity of our self-supervised formulation, as we will\nsee in the experimental section of the paper, the features learned by our approach achieve dramatic\nimprovements on the unsupervised feature learning benchmarks.\n3\nEXPERIMENTAL RESULTS\nIn this section we conduct an extensive evaluation of our approach on the most commonly used im-\nage datasets, such as CIFAR-10 (Krizhevsky & Hinton, 2009), ImageNet (Russakovsky et al., 2015),\n5\n",
    "Published as a conference paper at ICLR 2018\n(a) Supervised\n(b) Self-supervised to recognize rotations\nFigure 4: First layer ﬁlters learned by a AlexNet model trained on (a) the supervised object recogni-\ntion task and (b) the self-supervised task of recognizing rotated images. We observe that the ﬁlters\nlearned by the self-supervised task are mostly oriented edge ﬁlters on various frequencies and, re-\nmarkably, they seem to have more variety than those learned on the supervised task.\nTable 1: Evaluation of the unsupervised learned features by measuring the classiﬁcation accuracy\nthat they achieve when we train a non-linear object classiﬁer on top of them. The reported results\nare from CIFAR-10. The size of the ConvB1 feature maps is 96 × 16 × 16 and the size of the rest\nfeature maps is 192 × 8 × 8.\nModel\nConvB1\nConvB2\nConvB3\nConvB4\nConvB5\nRotNet with 3 conv. blocks\n85.45\n88.26\n62.09\n-\n-\nRotNet with 4 conv. blocks\n85.07\n89.06\n86.21\n61.73\n-\nRotNet with 5 conv. blocks\n85.04\n89.76\n86.82\n74.50\n50.37\nPASCAL (Everingham et al., 2010), and Places205 (Zhou et al., 2014), as well as on various vision\ntasks, such as object detection, object segmentation, and image classiﬁcation. We also consider sev-\neral learning scenarios, including transfer learning and semi-supervised learning. In all cases, we\ncompare our approach with corresponding state-of-the-art methods.\n3.1\nCIFAR EXPERIMENTS\nWe start by evaluating on the object recognition task of CIFAR-10 the ConvNet based features\nlearned by the proposed self-supervised task of rotation recognition. We will here after call a Con-\nvNet model that is trained on the self-supervised task of rotation recognition RotNet model.\nImplementation details: In our CIFAR-10 experiments we implement the RotNet models with\nNetwork-In-Network (NIN) architectures (Lin et al., 2013). In order to train them on the rotation\nprediction task, we use SGD with batch size 128, momentum 0.9, weight decay 5e −4 and lr of\n0.1. We drop the learning rates by a factor of 5 after epochs 30, 60, and 80. We train in total for 100\nepochs. In our preliminary experiments we found that we get signiﬁcant improvement when during\ntraining we train the network by feeding it all the four rotated copies of an image simultaneously\ninstead of each time randomly sampling a single rotation transformation. Therefore, at each training\nbatch the network sees 4 times more images than the batch size.\nEvaluation of the learned feature hierarchies: First, we explore how the quality of the learned\nfeatures depends from their depth (i.e., the depth of the layer that they come from) as well as from the\ntotal depth of the RotNet model. For that purpose, we ﬁrst train using the CIFAR-10 training images\nthree RotNet models which have 3, 4, and 5 convolutional blocks respectively (note that each conv.\nblock in the NIN architectures that implement our RotNet models have 3 conv. layers; therefore,\n6\n",
    "Published as a conference paper at ICLR 2018\nTable 2: Exploring the quality of the self-supervised learned features w.r.t. the number of recognized\nrotations. For all the entries we trained a non-linear classiﬁer with 3 fully connected layers (similar\nto Table 1) on top of the feature maps generated by the 2nd conv. block of a RotNet model with 4\nconv. blocks in total. The reported results are from CIFAR-10.\n# Rotations\nRotations\nCIFAR-10 Classiﬁcation Accuracy\n4\n0◦, 90◦, 180◦, 270◦\n89.06\n8\n0◦, 45◦, 90◦, 135◦, 180◦, 225◦, 270◦, 315◦\n88.51\n2\n0◦, 180◦\n87.46\n2\n90◦, 270◦\n85.52\nTable 3: Evaluation of unsupervised feature learning methods on CIFAR-10. The Supervised NIN\nand the (Ours) RotNet + conv entries have exactly the same architecture but the ﬁrst was trained fully\nsupervised while on the second the ﬁrst 2 conv. blocks were trained unsupervised with our rotation\nprediction task and the 3rd block only was trained in a supervised manner. In the Random Init. +\nconv entry a conv. classiﬁer (similar to that of (Ours) RotNet + conv) is trained on top of two NIN\nconv. blocks that are randomly initialized and stay frozen. Note that each of the prior approaches\nhas a different ConvNet architecture and thus the comparison with them is just indicative.\nMethod\nAccuracy\nSupervised NIN\n92.80\nRandom Init. + conv\n72.50\n(Ours) RotNet + non-linear\n89.06\n(Ours) RotNet + conv\n91.16\n(Ours) RotNet + non-linear (ﬁne-tuned)\n91.73\n(Ours) RotNet + conv (ﬁne-tuned)\n92.17\nRoto-Scat + SVM Oyallon & Mallat (2015)\n82.3\nExemplarCNN Dosovitskiy et al. (2014)\n84.3\nDCGAN Radford et al. (2015)\n82.8\nScattering Oyallon et al. (2017)\n84.7\nthe total number of conv. layers of the examined RotNet models is 9, 12, and 15 for 3, 4, and 5\nconv. blocks respectively). Afterwards, we learn classiﬁers on top of the feature maps generated\nby each conv. block of each RotNet model. Those classiﬁers are trained in a supervised way on\nthe object recognition task of CIFAR-10. They consist of 3 fully connected layers; the 2 hidden\nlayers have 200 feature channels each and are followed by batch-norm and relu units. We report\nthe accuracy results of CIFAR-10 test set in Table 1. We observe that in all cases the feature maps\ngenerated by the 2nd conv. block (that actually has depth 6 in terms of the total number of conv.\nlayer till that point) achieve the highest accuracy, i.e., between 88.26% and 89.06%. The features of\nthe conv. blocks that follow the 2nd one gradually degrade the object recognition accuracy, which\nwe assume is because they start becoming more and more speciﬁc on the self-supervised task of\nrotation prediction. Also, we observe that increasing the total depth of the RotNet models leads to\nincreased object recognition performance by the feature maps generated by earlier layers (and after\nthe 1st conv. block). We assume that this is because increasing the depth of the model and thus\nthe complexity of its head (i.e., top ConvNet layers) allows the features of earlier layers to be less\nspeciﬁc to the rotation prediction task.\nExploring the quality of the learned features w.r.t. the number of recognized rotations: In Ta-\nble 2 we explore how the quality of the self-supervised features depends on the number of discrete\nrotations used in the rotation prediction task. For that purpose we deﬁned three extra rotation recog-\nnition tasks: (a) one with 8 rotations that includes all the multiples of 45 degrees, (b) one with only\nthe 0◦and 180◦rotations, and (c) one with only the 90◦and 270◦rotations. In order to implement\nthe rotation transformation of the 45◦, 135◦, 225◦, 270◦, and 315◦rotations (in the 8 discrete rota-\ntions case) we used an image wrapping routine and then we took care to crop only the central square\n7\n",
    "Published as a conference paper at ICLR 2018\n(a)\n(b)\nFigure 5: (a) Plot with the rotation prediction accuracy and object recognition accuracy as a function\nof the training epochs used for solving the rotation prediction task. The red curve is the object\nrecognition accuracy of a fully supervised model (a NIN model), which is independent from the\ntraining epochs on the rotation prediction task. The yellow curve is the object recognition accuracy\nof an object classiﬁer trained on top of feature maps learned by a RotNet model at different snapshots\nof the training procedure. (b) Accuracy as a function of the number of training examples per category\nin CIFAR-10. Ours semi-supervised is a NIN model that the ﬁrst 2 conv. blocks are RotNet model\nthat was trained in a self-supervised way on the entire training set of CIFAR-10 and the 3rd conv.\nblock along with a prediction linear layer that was trained with the object recognition task only on\nthe available set of labeled images.\nimage regions that do not include any of the empty image areas introduced by the rotation transfor-\nmations (and which can easily indicate the image rotation). We observe that indeed for 4 discrete\nrotations (as we proposed) we achieve better object recognition performance than the 8 or 2 cases.\nWe believe that this is because the 2 orientations case offers too few classes for recognition (i.e., less\nsupervisory information is provided) while in the 8 orientations case the geometric transformations\nare not distinguishable enough and furthermore the 4 extra rotations introduced may lead to visual\nartifacts on the rotated images. Moreover, we observe that among the RotNet models trained with\n2 discrete rotations, the RotNet model trained with 90◦and 270◦rotations achieves worse object\nrecognition performance than the model trained with the 0◦and 180◦rotations, which is probably\ndue to the fact that the former model does not “see” during the unsupervised phase the 0◦rotation\nthat is typically used during the object recognition training phase.\nComparison against supervised and other unsupervised methods: In Table 3 we compare our\nunsupervised learned features against other unsupervised (or hand-crafted) features on CIFAR-10.\nFor our entries we use the feature maps generated by the 2nd conv. block of a RotNet model with\n4 conv. blocks in total. On top of those RotNet features we train 2 different classiﬁers: (a) a non-\nlinear classiﬁer with 3 fully connected layers as before (entry (Ours) RotNet + non-linear), and (b)\nthree conv. layers plus a linear prediction layer (entry (Ours) RotNet +conv.; note that this entry\nis basically a 3 blocks NIN model with the ﬁrst 2 blocks coming from a RotNet model and the\n3rd being randomly initialized and trained on the recognition task). We observe that we improve\nover the prior unsupervised approaches and we achieve state-of-the-art results in CIFAR-10 (note\nthat each of the prior approaches has a different ConvNet architecture thus the comparison with\nthem is just indicative). More notably, the accuracy gap between the RotNet based model and\nthe fully supervised NIN model is very small, only 1.64 percentage points (92.80% vs 91.16%).\nWe provide per class breakdown of the classiﬁcation accuracy of our unsupervised model as well\nas the supervised one in Table 9 (in appendix B). In Table 3 we also report the performance of the\nRotNet features when, instead of being kept frozen, they are ﬁne-tuned during the object recognition\ntraining phase. We observe that ﬁne-tuning the unsupervised learned features further improves the\nclassiﬁcation performance, thus reducing even more the gap with the supervised case.\nCorrelation between object classiﬁcation task and rotation prediction task: In Figure 5a, we\nplot the object classiﬁcation accuracy as a function of the training epochs used for solving the self-\nsupervised task of recognizing rotations, which learns the features used by the object classiﬁer.\n8\n",
    "Published as a conference paper at ICLR 2018\nTable 4: Task Generalization: ImageNet top-1 classiﬁcation with non-linear layers. We com-\npare our unsupervised feature learning approach with other unsupervised approaches by training\nnon-linear classiﬁers on top of the feature maps of each layer to perform the 1000-way ImageNet\nclassiﬁcation task, as proposed by Noroozi & Favaro (2016). For instance, for the conv5 feature\nmap we train the layers that follow the conv5 layer in the AlexNet architecture (i.e., fc6, fc7, and\nfc8). Similarly for the conv4 feature maps. We implemented those non-linear classiﬁers with batch\nnormalization units after each linear layer (fully connected or convolutional) and without employ-\ning drop out units. All approaches use AlexNet variants and were pre-trained on ImageNet without\nlabels except the ImageNet labels and Random entries. During testing we use a single crop and do\nnot perform ﬂipping augmentation. We report top-1 classiﬁcation accuracy.\nMethod\nConv4\nConv5\nImageNet labels from (Bojanowski & Joulin, 2017)\n59.7\n59.7\nRandom from (Noroozi & Favaro, 2016)\n27.1\n12.0\nTracking Wang & Gupta (2015)\n38.8\n29.8\nContext (Doersch et al., 2015)\n45.6\n30.4\nColorization (Zhang et al., 2016a)\n40.7\n35.2\nJigsaw Puzzles (Noroozi & Favaro, 2016)\n45.3\n34.6\nBIGAN (Donahue et al., 2016)\n41.9\n32.2\nNAT (Bojanowski & Joulin, 2017)\n-\n36.0\n(Ours) RotNet\n50.0\n43.8\nMore speciﬁcally, in order to create the object recognition accuracy curve, in each training snapshot\nof RotNet (i.e., every 20 epochs), we pause its training procedure and we train from scratch (until\nconvergence) a non-linear object classiﬁer on top of the so far learnt RotNet features. Therefore,\nthe object recognition accuracy curve depicts the accuracy of those non-linear object classiﬁers after\nthe end of their training while the rotation prediction accuracy curve depicts the accuracy of the\nRotNet at those snapshots. We observe that, as the ability of the RotNet features for solving the\nrotation prediction task improves (i.e., as the rotation prediction accuracy increases), their ability to\nhelp solving the object recognition task improves as well (i.e., the object recognition accuracy also\nincreases). Furthermore, we observe that the object recognition accuracy converges fast w.r.t. the\nnumber of training epochs used for solving the pretext task of rotation prediction.\nSemi-supervised setting: Motivated by the very high performance of our unsupervised feature\nlearning method, we also evaluate it on a semi-supervised setting. More speciﬁcally, we ﬁrst train\na 4 block RotNet model on the rotation prediction task using the entire image dataset of CIFAR-10\nand then we train on top of its feature maps object classiﬁers using only a subset of the available\nimages and their corresponding labels. As feature maps we use those generated by the 2nd conv.\nblock of the RotNet model. As a classiﬁer we use a set of convolutional layers that actually has\nthe same architecture as the 3rd conv. block of a NIN model plus a linear classiﬁer, all randomly\ninitialized. For training the object classiﬁer we use for each category 20, 100, 400, 1000, or 5000\nimage examples. Note that 5000 image examples is the extreme case of using the entire CIFAR-\n10 training dataset. Also, we compare our method with a supervised model that is trained only\non the available examples each time. In Figure 5b we plot the accuracy of the examined models\nas a function of the available training examples. We observe that our unsupervised trained model\nexceeds in this semi-supervised setting the supervised model when the number of examples per\ncategory drops below 1000. Furthermore, as the number of examples decreases, the performance\ngap in favor of our method is increased. This empirical evidence demonstrates the usefulness of our\nmethod on semi-supervised settings.\n3.2\nEVALUATION OF SELF-SUPERVISED FEATURES TRAINED IN IMAGENET\nHere we evaluate the performance of our self-supervised ConvNet models on the ImageNet, Places,\nand PASCAL VOC datasets. Speciﬁcally, we ﬁrst train a RotNet model on the training images of the\nImageNet dataset and then we evaluate the performance of the self-supervised features on the image\n9\n",
    "Published as a conference paper at ICLR 2018\nTable 5: Task Generalization: ImageNet top-1 classiﬁcation with linear layers. We compare\nour unsupervised feature learning approach with other unsupervised approaches by training logistic\nregression classiﬁers on top of the feature maps of each layer to perform the 1000-way ImageNet\nclassiﬁcation task, as proposed by Zhang et al. (2016a). All weights are frozen and feature maps are\nspatially resized (with adaptive max pooling) so as to have around 9000 elements. All approaches\nuse AlexNet variants and were pre-trained on ImageNet without labels except the ImageNet labels\nand Random entries.\nMethod\nConv1\nConv2\nConv3\nConv4\nConv5\nImageNet labels\n19.3\n36.3\n44.2\n48.3\n50.5\nRandom\n11.6\n17.1\n16.9\n16.3\n14.1\nRandom rescaled Kr¨ahenb¨uhl et al. (2015)\n17.5\n23.0\n24.5\n23.2\n20.6\nContext (Doersch et al., 2015)\n16.2\n23.3\n30.2\n31.7\n29.6\nContext Encoders (Pathak et al., 2016b)\n14.1\n20.7\n21.0\n19.8\n15.5\nColorization (Zhang et al., 2016a)\n12.5\n24.5\n30.4\n31.5\n30.3\nJigsaw Puzzles (Noroozi & Favaro, 2016)\n18.2\n28.8\n34.0\n33.9\n27.1\nBIGAN (Donahue et al., 2016)\n17.7\n24.5\n31.0\n29.9\n28.0\nSplit-Brain (Zhang et al., 2016b)\n17.7\n29.3\n35.4\n35.2\n32.8\nCounting (Noroozi et al., 2017)\n18.0\n30.6\n34.3\n32.5\n25.7\n(Ours) RotNet\n18.8\n31.7\n38.7\n38.2\n36.5\nTable 6: Task & Dataset Generalization: Places top-1 classiﬁcation with linear layers. We\ncompare our unsupervised feature learning approach with other unsupervised approaches by training\nlogistic regression classiﬁers on top of the feature maps of each layer to perform the 205-way Places\nclassiﬁcation task (Zhou et al., 2014). All unsupervised methods are pre-trained (in an unsupervised\nway) on ImageNet. All weights are frozen and feature maps are spatially resized (with adaptive max\npooling) so as to have around 9000 elements. All approaches use AlexNet variants and were pre-\ntrained on ImageNet without labels except the Place labels, ImageNet labels, and Random entries.\nMethod\nConv1\nConv2\nConv3\nConv4\nConv5\nPlaces labels Zhou et al. (2014)\n22.1\n35.1\n40.2\n43.3\n44.6\nImageNet labels\n22.7\n34.8\n38.4\n39.4\n38.7\nRandom\n15.7\n20.3\n19.8\n19.1\n17.5\nRandom rescaled Kr¨ahenb¨uhl et al. (2015)\n21.4\n26.2\n27.1\n26.1\n24.0\nContext (Doersch et al., 2015)\n19.7\n26.7\n31.9\n32.7\n30.9\nContext Encoders (Pathak et al., 2016b)\n18.2\n23.2\n23.4\n21.9\n18.4\nColorization (Zhang et al., 2016a)\n16.0\n25.7\n29.6\n30.3\n29.7\nJigsaw Puzzles (Noroozi & Favaro, 2016)\n23.0\n31.9\n35.0\n34.2\n29.3\nBIGAN (Donahue et al., 2016)\n22.0\n28.7\n31.8\n31.3\n29.7\nSplit-Brain (Zhang et al., 2016b)\n21.3\n30.7\n34.0\n34.1\n32.5\nCounting (Noroozi et al., 2017)\n23.3\n33.9\n36.3\n34.7\n29.6\n(Ours) RotNet\n21.5\n31.0\n35.1\n34.6\n33.7\nclassiﬁcation tasks of ImageNet, Places, and PASCAL VOC datasets and on the object detection and\nobject segmentation tasks of PASCAL VOC.\nImplementation details: For those experiments we implemented our RotNet model with an\nAlexNet architecture. Our implementation of the AlexNet model does not have local response\nnormalization units, dropout units, or groups in the colvolutional layers while it includes batch\nnormalization units after each linear layer (either convolutional or fully connected). In order to train\nthe AlexNet based RotNet model, we use SGD with batch size 192, momentum 0.9, weight decay\n5e −4 and lr of 0.01. We drop the learning rates by a factor of 10 after epochs 10, and 20 epochs.\nWe train in total for 30 epochs. As in the CIFAR experiments, during training we feed the RotNet\nmodel all four rotated copies of an image simultaneously (in the same mini-batch).\n10\n",
    "Published as a conference paper at ICLR 2018\nTable 7: Task & Dataset Generalization: PASCAL VOC 2007 classiﬁcation and detection re-\nsults, and PASCAL VOC 2012 segmentation results. We used the publicly available testing\nframeworks of Kr¨ahenb¨uhl et al. (2015) for classiﬁcation, of Girshick (2015) for detection, and\nof Long et al. (2015) for segmentation. For classiﬁcation, we either ﬁx the features before conv5\n(column fc6-8) or we ﬁne-tune the whole model (column all). For detection we use multi-scale\ntraining and single scale testing. All approaches use AlexNet variants and were pre-trained on Ima-\ngeNet without labels except the ImageNet labels and Random entries. After unsupervised training,\nwe absorb the batch normalization units on the linear layers and we use the weight rescaling tech-\nnique proposed by Kr¨ahenb¨uhl et al. (2015) (which is common among the unsupervised methods).\nAs customary, we report the mean average precision (mAP) on the classiﬁcation and detection tasks,\nand the mean intersection over union (mIoU) on the segmentation task.\nClassiﬁcation\nDetection\nSegmentation\n(%mAP)\n(%mAP)\n(%mIoU)\nTrained layers\nfc6-8\nall\nall\nall\nImageNet labels\n78.9\n79.9\n56.8\n48.0\nRandom\n53.3\n43.4\n19.8\nRandom rescaled Kr¨ahenb¨uhl et al. (2015)\n39.2\n56.6\n45.6\n32.6\nEgomotion (Agrawal et al., 2015)\n31.0\n54.2\n43.9\nContext Encoders (Pathak et al., 2016b)\n34.6\n56.5\n44.5\n29.7\nTracking (Wang & Gupta, 2015)\n55.6\n63.1\n47.4\nContext (Doersch et al., 2015)\n55.1\n65.3\n51.1\nColorization (Zhang et al., 2016a)\n61.5\n65.6\n46.9\n35.6\nBIGAN (Donahue et al., 2016)\n52.3\n60.1\n46.9\n34.9\nJigsaw Puzzles (Noroozi & Favaro, 2016)\n-\n67.6\n53.2\n37.6\nNAT (Bojanowski & Joulin, 2017)\n56.7\n65.3\n49.4\nSplit-Brain (Zhang et al., 2016b)\n63.0\n67.1\n46.7\n36.0\nColorProxy (Larsson et al., 2017)\n65.9\n38.4\nCounting (Noroozi et al., 2017)\n-\n67.7\n51.4\n36.6\n(Ours) RotNet\n70.87\n72.97\n54.4\n39.1\nImageNet classiﬁcation task: We evaluate the task generalization of our self-supervised learned\nfeatures by training on top of them non-linear object classiﬁers for the ImageNet classiﬁcation task\n(following the evaluation scheme of (Noroozi & Favaro, 2016)). In Table 4 we report the classiﬁ-\ncation performance of our self-supervised features and we compare it with the other unsupervised\napproaches. We observe that our approach surpasses all the other methods by a signiﬁcant margin.\nFor the feature maps generated by the Conv4 layer, our improvement is more than 4 percentage\npoints and for the feature maps generated by the Conv5 layer, our improvement is even bigger,\naround 8 percentage points. Furthermore, our approach signiﬁcantly narrows the performance gap\nbetween unsupervised features and supervised features. In Table 5 we report similar results but\nfor linear (logistic regression) classiﬁers (following the evaluation scheme of Zhang et al. (2016a)).\nAgain, our unsupervised method demonstrates signiﬁcant improvements over prior unsupervised\nmethods.\nTransfer learning evaluation on PASCAL VOC: In Table 7 we evaluate the task and dataset\ngeneralization of our unsupervised learned features by ﬁne-tuning them on the PASCAL VOC clas-\nsiﬁcation, detection, and segmentation tasks. As with the ImageNet classiﬁcation task, we outper-\nform by signiﬁcant margin all the competing unsupervised methods in all tested tasks, signiﬁcantly\nnarrowing the gap with the supervised case. Notably, the PASCAL VOC 2007 object detection per-\nformance that our self-supervised model achieves is 54.4% mAP, which is only 2.4 points lower\nthan the supervised case. We provide the per class detection performance of our method in Table 8\n(in appendix B).\nPlaces classiﬁcation task: In Table 6 we evaluate the task and dataset generalization of our approach\nby training linear (logistic regression) classiﬁers on top of the learned features in order to perform\nthe 205-way Places classiﬁcation task. Note that in this case the learnt features are evaluated w.r.t.\n11\n",
    "Published as a conference paper at ICLR 2018\ntheir generalization on classes that were “unseen” during the unsupervised training phase. As can\nbe seen, even in this case our method manages to either surpass or achieve comparable results w.r.t.\nprior state-of-the-art unsupervised learning approaches.\n4\nCONCLUSIONS\nIn our work we propose a novel formulation for self-supervised feature learning that trains a Con-\nvNet model to be able to recognize the image rotation that has been applied to its input images.\nDespite the simplicity of our self-supervised task, we demonstrate that it successfully forces the Con-\nvNet model trained on it to learn semantic features that are useful for a variety of visual perception\ntasks, such as object recognition, object detection, and object segmentation. We exhaustively eval-\nuate our method in various unsupervised and semi-supervised benchmarks and we achieve in all of\nthem state-of-the-art performance. Speciﬁcally, our self-supervised approach manages to drastically\nimprove the state-of-the-art results on unsupervised feature learning for ImageNet classiﬁcation,\nPASCAL classiﬁcation, PASCAL detection, PASCAL segmentation, and CIFAR-10 classiﬁcation,\nsurpassing prior approaches by a signiﬁcant margin and thus drastically reducing the gap between\nunsupervised and supervised feature learning.\n5\nACKNOWLEDGEMENTS\nThis work was supported by the ANR SEMAPOLIS project, an INTEL gift, and hardware donation\nby NVIDIA.\nREFERENCES\nPulkit Agrawal, Joao Carreira, and Jitendra Malik. Learning to see by moving. In Proceedings of\nthe IEEE International Conference on Computer Vision, pp. 37–45, 2015.\nYoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training\nof deep networks. In Advances in neural information processing systems, pp. 153–160, 2007.\nPiotr Bojanowski and Armand Joulin. Unsupervised learning by predicting noise. arXiv preprint\narXiv:1704.05310, 2017.\nCarl Doersch and Andrew Zisserman.\nMulti-task self-supervised visual learning.\nCoRR,\nabs/1708.07860, 2017.\nCarl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by\ncontext prediction. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n1422–1430, 2015.\nJeff Donahue, Philipp Kr¨ahenb¨uhl, and Trevor Darrell. Adversarial feature learning. arXiv preprint\narXiv:1605.09782, 2016.\nAlexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discrimina-\ntive unsupervised feature learning with convolutional neural networks. In Advances in Neural\nInformation Processing Systems, pp. 766–774, 2014.\nM. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object\nclasses (voc) challenge. International Journal of Computer Vision, 88(2):303–338, June 2010.\nRoss Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision,\npp. 1440–1448, 2015.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-\nmation processing systems, pp. 2672–2680, 2014.\nPriya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-\ndrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet\nin 1 hour. arXiv preprint arXiv:1706.02677, 2017.\n12\n",
    "Published as a conference paper at ICLR 2018\nFu Jie Huang, Y-Lan Boureau, Yann LeCun, et al. Unsupervised learning of invariant feature hierar-\nchies with applications to object recognition. In Computer Vision and Pattern Recognition, 2007.\nCVPR’07. IEEE Conference on, pp. 1–8. IEEE, 2007.\nAndrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descrip-\ntions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n3128–3137, 2015.\nPhilipp Kr¨ahenb¨uhl, Carl Doersch, Jeff Donahue, and Trevor Darrell. Data-dependent initializations\nof convolutional neural networks. arXiv preprint arXiv:1511.06856, 2015.\nAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.\nGustav Larsson, Michael Maire, and Gregory Shakhnarovich. Learning representations for auto-\nmatic colorization. In European Conference on Computer Vision, pp. 577–593. Springer, 2016.\nGustav Larsson, Michael Maire, and Gregory Shakhnarovich. Colorization as a proxy task for visual\nunderstanding. arXiv preprint arXiv:1703.04044, 2017.\nYann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\nRenjie Liao, Alex Schwing, Richard Zemel, and Raquel Urtasun.\nLearning deep parsimonious\nrepresentations. In Advances in Neural Information Processing Systems, pp. 5076–5084, 2016.\nMin Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400,\n2013.\nJonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic\nsegmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\nJune 2015.\nJonathan Masci, Ueli Meier, Dan Cires¸an, and J¨urgen Schmidhuber. Stacked convolutional auto-\nencoders for hierarchical feature extraction. Artiﬁcial Neural Networks and Machine Learning–\nICANN 2011, pp. 52–59, 2011.\nMehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw\npuzzles. In European Conference on Computer Vision, pp. 69–84. Springer, 2016.\nMehdi Noroozi, Hamed Pirsiavash, and Paolo Favaro. Representation learning by learning to count.\narXiv preprint arXiv:1708.06734, 2017.\nEdouard Oyallon and St´ephane Mallat. Deep roto-translation scattering for object classiﬁcation.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2865–\n2873, 2015.\nEdouard Oyallon, Eugene Belilovsky, and Sergey Zagoruyko. Scaling the scattering transform:\nDeep hybrid networks. arXiv preprint arXiv:1703.08961, 2017.\nDeepak Pathak, Ross Girshick, Piotr Doll´ar, Trevor Darrell, and Bharath Hariharan. Learning fea-\ntures by watching objects move. arXiv preprint arXiv:1612.06370, 2016a.\nDeepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context\nencoders: Feature learning by inpainting. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 2536–2544, 2016b.\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\nconvolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\nrecognition challenge. International Journal of Computer Vision, 115(3):211–252, 2015.\nXiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using videos.\nIn Proceedings of the IEEE International Conference on Computer Vision, pp. 2794–2802, 2015.\n13\n",
    "Published as a conference paper at ICLR 2018\nJianwei Yang, Devi Parikh, and Dhruv Batra. Joint unsupervised learning of deep representations\nand image clusters. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 5147–5156, 2016.\nRichard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In European Con-\nference on Computer Vision, pp. 649–666. Springer, 2016a.\nRichard Zhang, Phillip Isola, and Alexei A Efros. Split-brain autoencoders: Unsupervised learning\nby cross-channel prediction. arXiv preprint arXiv:1611.09842, 2016b.\nBolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep\nfeatures for scene recognition using places database. In Z. Ghahramani, M. Welling, C. Cortes,\nN. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Sys-\ntems 27, pp. 487–495. Curran Associates, Inc., 2014.\n14\n",
    "Published as a conference paper at ICLR 2018\nAPPENDIX A\nVISUALIZING ATTENTION MAPS OF ROTATED IMAGES\nHere we visualize the attention maps generated by an AlexNet model trained on the self-supervised\ntask of rotation recognition for all the rotated copies of a few images. We observe that the atten-\ntion maps of all the rotated copies of an image are roughly the same, i.e., the attention maps are\nequivariant w.r.t. the image rotations. This practically means that in order to accomplish the rotation\nprediction task the network focuses on the same object parts regardless of the image rotation.\nAttention maps of Conv3 feature maps (size: 13 × 13)\n0◦rotation\n90◦rotation\n180◦rotation\n270◦rotation\nAttention maps of Conv5 feature maps (size: 6 × 6)\n0◦rotation\n90◦rotation\n180◦rotation\n270◦rotation\nFigure 6: Attention maps of the Conv3 and Conv5 feature maps generated by an AlexNet model\ntrained on the self-supervised task of recognizing image rotations. Here we present the attention\nmaps generated for all the 4 rotated copies of an image.\n15\n",
    "Published as a conference paper at ICLR 2018\nAPPENDIX B\nPER CLASS BREAKDOWN OF DETECTION AND CLASSIFICATION\nPERFORMANCE\nIn Tables 8 and 9 we report the per class performance of our unsupervised learning method on the\nPASCAL detection and CIFAR-10 classiﬁcation tasks respectively.\nTable 8: Per class PASCAL VOC 2007 detection performance. As usual, we report the average\nprecision metric. The results of the supervised model (i.e., ImageNet labels entry) come from Doer-\nsch et al. (2015).\nClasses\naero\nbike\nbird\nboat\nbottle\nbus\ncar\ncat\nchair\ncow\ntable\ndog\nhorse\nmbike\nperson\nplant\nsheep\nsofa\ntrain\ntv\nImageNet labels\n64.0\n69.6\n53.2\n44.4\n24.9\n65.7\n69.6\n69.2\n28.9\n63.6\n62.8\n63.9\n73.3\n64.6\n55.8\n25.7\n50.5\n55.4\n69.3\n56.4\n(Ours) RotNet\n65.5\n65.3\n43.8\n39.8\n20.2\n65.4\n69.2\n63.9\n30.2\n56.3\n62.3\n56.8\n71.6\n67.2\n56.3\n22.7\n45.6\n59.5\n71.6\n55.3\nTable 9: Per class CIFAR-10 classiﬁcation accuracy.\nClasses\naero\ncar\nbird\ncat\ndeer\ndog\nfrog\nhorse\nship\ntruck\nSupervised\n93.7\n96.3\n89.4\n82.4\n93.6\n89.7\n95.0\n94.3\n95.7\n95.2\n(Ours) RotNet\n91.7\n95.8\n87.1\n83.5\n91.5\n85.3\n94.2\n91.9\n95.7\n94.2\n16\n"
  ],
  "full_text": "Published as a conference paper at ICLR 2018\nUNSUPERVISED REPRESENTATION LEARNING BY PRE-\nDICTING IMAGE ROTATIONS\nSpyros Gidaris, Praveer Singh, Nikos Komodakis\nUniversity Paris-Est, LIGM\nEcole des Ponts ParisTech\n{spyros.gidaris,praveer.singh,nikos.komodakis}@enpc.fr\nABSTRACT\nOver the last years, deep convolutional neural networks (ConvNets) have trans-\nformed the ﬁeld of computer vision thanks to their unparalleled capacity to learn\nhigh level semantic image features. However, in order to successfully learn those\nfeatures, they usually require massive amounts of manually labeled data, which\nis both expensive and impractical to scale. Therefore, unsupervised semantic fea-\nture learning, i.e., learning without requiring manual annotation effort, is of crucial\nimportance in order to successfully harvest the vast amount of visual data that are\navailable today. In our work we propose to learn image features by training Con-\nvNets to recognize the 2d rotation that is applied to the image that it gets as input.\nWe demonstrate both qualitatively and quantitatively that this apparently simple\ntask actually provides a very powerful supervisory signal for semantic feature\nlearning. We exhaustively evaluate our method in various unsupervised feature\nlearning benchmarks and we exhibit in all of them state-of-the-art performance.\nSpeciﬁcally, our results on those benchmarks demonstrate dramatic improvements\nw.r.t. prior state-of-the-art approaches in unsupervised representation learning and\nthus signiﬁcantly close the gap with supervised feature learning. For instance, in\nPASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model\nachieves the state-of-the-art (among unsupervised methods) mAP of 54.4% that is\nonly 2.4 points lower from the supervised case. We get similarly striking results\nwhen we transfer our unsupervised learned features on various other tasks, such\nas ImageNet classiﬁcation, PASCAL classiﬁcation, PASCAL segmentation, and\nCIFAR-10 classiﬁcation. The code and models of our paper will be published on:\nhttps://github.com/gidariss/FeatureLearningRotNet.\n1\nINTRODUCTION\nIn recent years, the widespread adoption of deep convolutional neural networks (LeCun et al., 1998)\n(ConvNets) in computer vision, has lead to a tremendous progress in the ﬁeld. Speciﬁcally, by train-\ning ConvNets on the object recognition (Russakovsky et al., 2015) or the scene classiﬁcation (Zhou\net al., 2014) tasks with a massive amount of manually labeled data, they manage to learn power-\nful visual representations suitable for image understanding tasks. For instance, the image features\nlearned by ConvNets in this supervised manner have achieved excellent results when they are trans-\nferred to other vision tasks, such as object detection (Girshick, 2015), semantic segmentation (Long\net al., 2015), or image captioning (Karpathy & Fei-Fei, 2015). However, supervised feature learning\nhas the main limitation of requiring intensive manual labeling effort, which is both expensive and\ninfeasible to scale on the vast amount of visual data that are available today.\nDue to that, there is lately an increased interest to learn high level ConvNet based representations\nin an unsupervised manner that avoids manual annotation of visual data. Among them, a promi-\nnent paradigm is the so-called self-supervised learning that deﬁnes an annotation free pretext task,\nusing only the visual information present on the images or videos, in order to provide a surrogate\nsupervision signal for feature learning. For example, in order to learn features, Zhang et al. (2016a)\nand Larsson et al. (2016) train ConvNets to colorize gray scale images, Doersch et al. (2015) and\nNoroozi & Favaro (2016) predict the relative position of image patches, and Agrawal et al. (2015)\npredict the egomotion (i.e., self-motion) of a moving vehicle between two consecutive frames. The\n1\narXiv:1803.07728v1  [cs.CV]  21 Mar 2018\n\n\nPublished as a conference paper at ICLR 2018\nrationale behind such self-supervised tasks is that solving them will force the ConvNet to learn se-\nmantic image features that can be useful for other vision tasks. In fact, image representations learned\nwith the above self-supervised tasks, although they have not managed to match the performance\nof supervised-learned representations, they have proved to be good alternatives for transferring on\nother vision tasks, such as object recognition, object detection, and semantic segmentation (Zhang\net al., 2016a; Larsson et al., 2016; Zhang et al., 2016b; Larsson et al., 2017; Doersch et al., 2015;\nNoroozi & Favaro, 2016; Noroozi et al., 2017; Pathak et al., 2016a; Doersch & Zisserman, 2017).\nOther successful cases of unsupervised feature learning are clustering based methods (Dosovitskiy\net al., 2014; Liao et al., 2016; Yang et al., 2016), reconstruction based methods (Bengio et al., 2007;\nHuang et al., 2007; Masci et al., 2011), and methods that involve learning generative probabilistic\nmodels Goodfellow et al. (2014); Donahue et al. (2016); Radford et al. (2015).\nOur work follows the self-supervised paradigm and proposes to learn image representations by train-\ning ConvNets to recognize the geometric transformation that is applied to the image that it gets as\ninput. More speciﬁcally, we ﬁrst deﬁne a small set of discrete geometric transformations, then each\nof those geometric transformations are applied to each image on the dataset and the produced trans-\nformed images are fed to the ConvNet model that is trained to recognize the transformation of each\nimage. In this formulation, it is the set of geometric transformations that actually deﬁnes the classiﬁ-\ncation pretext task that the ConvNet model has to learn. Therefore, in order to achieve unsupervised\nsemantic feature learning, it is of crucial importance to properly choose those geometric transfor-\nmations (we further discuss this aspect of our methodology in section 2.2). What we propose is to\ndeﬁne the geometric transformations as the image rotations by 0, 90, 180, and 270 degrees. Thus,\nthe ConvNet model is trained on the 4-way image classiﬁcation task of recognizing one of the four\nimage rotations (see Figure 2). We argue that in order a ConvNet model to be able recognize the\nrotation transformation that was applied to an image it will require to understand the concept of\nthe objects depicted in the image (see Figure 1), such as their location in the image, their type, and\ntheir pose. Throughout the paper we support that argument both qualitatively and quantitatively.\nFurthermore we demonstrate on the experimental section of the paper that despite the simplicity of\nour self-supervised approach, the task of predicting rotation transformations provides a powerful\nsurrogate supervision signal for feature learning and leads to dramatic improvements on the relevant\nbenchmarks.\nNote that our self-supervised task is different from the work of Dosovitskiy et al. (2014) and Agrawal\net al. (2015) that also involves geometric transformations. Dosovitskiy et al. (2014) train a ConvNet\nmodel to yield representations that are discriminative between images and at the same time invariant\non geometric and chromatic transformations. In contrast, we train a ConvNet model to recognize the\ngeometric transformation applied to an image. It is also fundamentally different from the egomotion\nmethod of Agrawal et al. (2015), which employs a ConvNet model with siamese like architecture\nthat takes as input two consecutive video frames and is trained to predict (through regression) their\ncamera transformation. Instead, in our approach, the ConvNet takes as input a single image to\nwhich we have applied a random geometric transformation (i.e., rotation) and is trained to recognize\n(through classiﬁcation) this geometric transformation without having access to the initial image.\nOur contributions are:\n• We propose a new self-supervised task that is very simple and at the same time, as we\ndemonstrate throughout the paper, offers a powerful supervisory signal for semantic feature\nlearning.\n• We exhaustively evaluate our self-supervised method under various settings (e.g. semi-\nsupervised or transfer learning settings) and in various vision tasks (i.e., CIFAR-10, Ima-\ngeNet, Places, and PASCAL classiﬁcation, detection, or segmentation tasks).\n• In all of them, our novel self-supervised formulation demonstrates state-of-the-art results\nwith dramatic improvements w.r.t. prior unsupervised approaches.\n• As a consequence we show that for several important vision tasks, our self-supervised\nlearning approach signiﬁcantly narrows the gap between unsupervised and supervised fea-\nture learning.\nIn the following sections, we describe our self-supervised methodology in §2, we provide experi-\nmental results in §3, and ﬁnally we conclude in §4.\n2\n\n\nPublished as a conference paper at ICLR 2018\n90◦rotation\n270◦rotation\n180◦rotation\n0◦rotation\n270◦rotation\nFigure 1: Images rotated by random multiples of 90 degrees (e.g., 0, 90, 180, or 270 degrees). The\ncore intuition of our self-supervised feature learning approach is that if someone is not aware of the\nconcepts of the objects depicted in the images, he cannot recognize the rotation that was applied to\nthem.\n2\nMETHODOLOGY\n2.1\nOVERVIEW\nThe goal of our work is to learn ConvNet based semantic features in an unsupervised manner. To\nachieve that goal we propose to train a ConvNet model F(.) to estimate the geometric transformation\napplied to an image that is given to it as input. Speciﬁcally, we deﬁne a set of K discrete geometric\ntransformations G = {g(.|y)}K\ny=1, where g(.|y) is the operator that applies to image X the geometric\ntransformation with label y that yields the transformed image Xy = g(X|y). The ConvNet model\nF(.) gets as input an image Xy∗(where the label y∗is unknown to model F(.)) and yields as output\na probability distribution over all possible geometric transformations:\nF(Xy∗|θ) = {F y(Xy∗|θ)}K\ny=1,\n(1)\nwhere F y(Xy∗|θ) is the predicted probability for the geometric transformation with label y and θ\nare the learnable parameters of model F(.).\nTherefore, given a set of N training images D = {Xi}N\ni=0, the self-supervised training objective\nthat the ConvNet model must learn to solve is:\nmin\nθ\n1\nN\nN\nX\ni=1\nloss(Xi, θ),\n(2)\nwhere the loss function loss(.) is deﬁned as:\nloss(Xi, θ) = −1\nK\nK\nX\ny=1\nlog(F y(g(Xi|y)|θ)).\n(3)\nIn the following subsection we describe the type of geometric transformations that we propose in\nour work.\n2.2\nCHOOSING GEOMETRIC TRANSFORMATIONS: IMAGE ROTATIONS\nIn the above formulation, the geometric transformations G must deﬁne a classiﬁcation task that\nshould force the ConvNet model to learn semantic features useful for visual perception tasks (e.g.,\nobject detection or image classiﬁcation). In our work we propose to deﬁne the set of geometric\ntransformations G as all the image rotations by multiples of 90 degrees, i.e., 2d image rotations by\n0, 90, 180, and 270 degrees (see Figure 2). More formally, if Rot(X, φ) is an operator that rotates\nimage X by φ degrees, then our set of geometric transformations consists of the K = 4 image\nrotations G = {g(X|y)}4\ny=1, where g(X|y) = Rot(X, (y −1)90).\nForcing the learning of semantic features: The core intuition behind using these image rotations\nas the set of geometric transformations relates to the simple fact that it is essentially impossible for a\nConvNet model to effectively perform the above rotation recognition task unless it has ﬁrst learnt to\nrecognize and detect classes of objects as well as their semantic parts in images. More speciﬁcally,\n3\n\n\nPublished as a conference paper at ICLR 2018\nRotated image: X\n0\nRotated image: X\n3\nRotated image: X\n2\nRotated image: X\n1\nConvNet \nmodel F(.)\nConvNet \nmodel F(.)\nConvNet \nmodel F(.)\nConvNet \nmodel F(.)\nImage X\nPredict 270 degrees rotation (y=3)\nRotate 270 degrees\ng( X , y=3)\nRotate 180 degrees\ng( X , y=2)\nRotate 90 degrees\ng( X , y=1)\nRotate 0 degrees\ng( X , y=0)\nMaximize prob.\nF\n3( X\n3)\nPredict 0 degrees rotation (y=0)\nMaximize prob.\nF\n2( X\n2)\nMaximize prob.\nF\n1( X\n1)\nMaximize prob.\nF\n0( X\n0)\nPredict 180 degrees rotation (y=2)\nPredict 90 degrees rotation (y=1)\nObjectives:\nFigure 2: Illustration of the self-supervised task that we propose for semantic feature learning.\nGiven four possible geometric transformations, the 0, 90, 180, and 270 degrees rotations, we train\na ConvNet model F(.) to recognize the rotation that is applied to the image that it gets as input.\nF y(Xy∗) is the probability of rotation transformation y predicted by model F(.) when it gets as\ninput an image that has been transformed by the rotation transformation y∗.\nto successfully predict the rotation of an image the ConvNet model must necessarily learn to localize\nsalient objects in the image, recognize their orientation and object type, and then relate the object\norientation with the dominant orientation that each type of object tends to be depicted within the\navailable images. In Figure 3b we visualize some attention maps generated by a model trained\non the rotation recognition task. These attention maps are computed based on the magnitude of\nactivations at each spatial cell of a convolutional layer and essentially reﬂect where the network\nputs most of its focus in order to classify an input image. We observe, indeed, that in order for the\nmodel to accomplish the rotation prediction task it learns to focus on high level object parts in the\nimage, such as eyes, nose, tails, and heads. By comparing them with the attention maps generated\nby a model trained on the object recognition task in a supervised way (see Figure 3a) we observe\nthat both models seem to focus on roughly the same image regions. Furthermore, in Figure 4 we\nvisualize the ﬁrst layer ﬁlters that were learnt by an AlexNet model trained on the proposed rotation\nrecognition task. As can be seen, they appear to have a big variety of edge ﬁlters on multiple\norientations and multiple frequencies. Remarkably, these ﬁlters seem to have a greater amount of\nvariety even than the ﬁlters learnt by the supervised object recognition task.\nAbsence of low-level visual artifacts: An additional important advantage of using image rotations\nby multiples of 90 degrees over other geometric transformations, is that they can be implemented by\nﬂip and transpose operations (as we will see below) that do not leave any easily detectable low-level\nvisual artifacts that will lead the ConvNet to learn trivial features with no practical value for the\nvision perception tasks. In contrast, had we decided to use as geometric transformations, e.g., scale\nand aspect ratio image transformations, in order to implement them we would need to use image\nresizing routines that leave easily detectable image artifacts.\nWell-posedness: Furthermore, human captured images tend to depict objects in an “up-standing”\nposition, thus making the rotation recognition task well deﬁned, i.e., given an image rotated by 0,\n90, 180, or 270 degrees, there is usually no ambiguity of what is the rotation transformation (with\nthe exception of images that only depict round objects). In contrast, that is not the case for the object\nscale that varies signiﬁcantly on human captured images.\nImplementing image rotations: In order to implement the image rotations by 90, 180, and 270\ndegrees (the 0 degrees case is the image itself), we use ﬂip and transpose operations. Speciﬁcally,\n4\n\n\nPublished as a conference paper at ICLR 2018\nInput images on the models\nConv1 27 × 27\nConv3 13 × 13\nConv5 6 × 6\n(a) Attention maps of supervised model\nConv1 27 × 27\nConv3 13 × 13\nConv5 6 × 6\n(b) Attention maps of our self-supervised model\nFigure 3: Attention maps generated by an AlexNet model trained (a) to recognize objects (super-\nvised), and (b) to recognize image rotations (self-supervised). In order to generate the attention map\nof a conv. layer we ﬁrst compute the feature maps of this layer, then we raise each feature activation\non the power p, and ﬁnally we sum the activations at each location of the feature map. For the conv.\nlayers 1, 2, and 3 we used the powers p = 1, p = 2, and p = 4 respectively. For visualization of\nour self-supervised model’s attention maps for all the rotated versions of the images see Figure 6 in\nappendix A.\nfor 90 degrees rotation we ﬁrst transpose the image and then ﬂip it vertically (upside-down ﬂip),\nfor 180 degrees rotation we ﬂip the image ﬁrst vertically and then horizontally (left-right ﬂip), and\nﬁnally for 270 degrees rotation we ﬁrst ﬂip vertically the image and then we transpose it.\n2.3\nDISCUSSION\nThe simple formulation of our self-supervised task has several advantages. It has the same computa-\ntional cost as supervised learning, similar training convergence speed (that is signiﬁcantly faster than\nimage reconstruction based approaches; our AlexNet model trains in around 2 days using a single\nTitan X GPU), and can trivially adopt the efﬁcient parallelization schemes devised for supervised\nlearning (Goyal et al., 2017), making it an ideal candidate for unsupervised learning on internet-\nscale data (i.e., billions of images). Furthermore, our approach does not require any special image\npre-processing routine in order to avoid learning trivial features, as many other unsupervised or\nself-supervised approaches do. Despite the simplicity of our self-supervised formulation, as we will\nsee in the experimental section of the paper, the features learned by our approach achieve dramatic\nimprovements on the unsupervised feature learning benchmarks.\n3\nEXPERIMENTAL RESULTS\nIn this section we conduct an extensive evaluation of our approach on the most commonly used im-\nage datasets, such as CIFAR-10 (Krizhevsky & Hinton, 2009), ImageNet (Russakovsky et al., 2015),\n5\n\n\nPublished as a conference paper at ICLR 2018\n(a) Supervised\n(b) Self-supervised to recognize rotations\nFigure 4: First layer ﬁlters learned by a AlexNet model trained on (a) the supervised object recogni-\ntion task and (b) the self-supervised task of recognizing rotated images. We observe that the ﬁlters\nlearned by the self-supervised task are mostly oriented edge ﬁlters on various frequencies and, re-\nmarkably, they seem to have more variety than those learned on the supervised task.\nTable 1: Evaluation of the unsupervised learned features by measuring the classiﬁcation accuracy\nthat they achieve when we train a non-linear object classiﬁer on top of them. The reported results\nare from CIFAR-10. The size of the ConvB1 feature maps is 96 × 16 × 16 and the size of the rest\nfeature maps is 192 × 8 × 8.\nModel\nConvB1\nConvB2\nConvB3\nConvB4\nConvB5\nRotNet with 3 conv. blocks\n85.45\n88.26\n62.09\n-\n-\nRotNet with 4 conv. blocks\n85.07\n89.06\n86.21\n61.73\n-\nRotNet with 5 conv. blocks\n85.04\n89.76\n86.82\n74.50\n50.37\nPASCAL (Everingham et al., 2010), and Places205 (Zhou et al., 2014), as well as on various vision\ntasks, such as object detection, object segmentation, and image classiﬁcation. We also consider sev-\neral learning scenarios, including transfer learning and semi-supervised learning. In all cases, we\ncompare our approach with corresponding state-of-the-art methods.\n3.1\nCIFAR EXPERIMENTS\nWe start by evaluating on the object recognition task of CIFAR-10 the ConvNet based features\nlearned by the proposed self-supervised task of rotation recognition. We will here after call a Con-\nvNet model that is trained on the self-supervised task of rotation recognition RotNet model.\nImplementation details: In our CIFAR-10 experiments we implement the RotNet models with\nNetwork-In-Network (NIN) architectures (Lin et al., 2013). In order to train them on the rotation\nprediction task, we use SGD with batch size 128, momentum 0.9, weight decay 5e −4 and lr of\n0.1. We drop the learning rates by a factor of 5 after epochs 30, 60, and 80. We train in total for 100\nepochs. In our preliminary experiments we found that we get signiﬁcant improvement when during\ntraining we train the network by feeding it all the four rotated copies of an image simultaneously\ninstead of each time randomly sampling a single rotation transformation. Therefore, at each training\nbatch the network sees 4 times more images than the batch size.\nEvaluation of the learned feature hierarchies: First, we explore how the quality of the learned\nfeatures depends from their depth (i.e., the depth of the layer that they come from) as well as from the\ntotal depth of the RotNet model. For that purpose, we ﬁrst train using the CIFAR-10 training images\nthree RotNet models which have 3, 4, and 5 convolutional blocks respectively (note that each conv.\nblock in the NIN architectures that implement our RotNet models have 3 conv. layers; therefore,\n6\n\n\nPublished as a conference paper at ICLR 2018\nTable 2: Exploring the quality of the self-supervised learned features w.r.t. the number of recognized\nrotations. For all the entries we trained a non-linear classiﬁer with 3 fully connected layers (similar\nto Table 1) on top of the feature maps generated by the 2nd conv. block of a RotNet model with 4\nconv. blocks in total. The reported results are from CIFAR-10.\n# Rotations\nRotations\nCIFAR-10 Classiﬁcation Accuracy\n4\n0◦, 90◦, 180◦, 270◦\n89.06\n8\n0◦, 45◦, 90◦, 135◦, 180◦, 225◦, 270◦, 315◦\n88.51\n2\n0◦, 180◦\n87.46\n2\n90◦, 270◦\n85.52\nTable 3: Evaluation of unsupervised feature learning methods on CIFAR-10. The Supervised NIN\nand the (Ours) RotNet + conv entries have exactly the same architecture but the ﬁrst was trained fully\nsupervised while on the second the ﬁrst 2 conv. blocks were trained unsupervised with our rotation\nprediction task and the 3rd block only was trained in a supervised manner. In the Random Init. +\nconv entry a conv. classiﬁer (similar to that of (Ours) RotNet + conv) is trained on top of two NIN\nconv. blocks that are randomly initialized and stay frozen. Note that each of the prior approaches\nhas a different ConvNet architecture and thus the comparison with them is just indicative.\nMethod\nAccuracy\nSupervised NIN\n92.80\nRandom Init. + conv\n72.50\n(Ours) RotNet + non-linear\n89.06\n(Ours) RotNet + conv\n91.16\n(Ours) RotNet + non-linear (ﬁne-tuned)\n91.73\n(Ours) RotNet + conv (ﬁne-tuned)\n92.17\nRoto-Scat + SVM Oyallon & Mallat (2015)\n82.3\nExemplarCNN Dosovitskiy et al. (2014)\n84.3\nDCGAN Radford et al. (2015)\n82.8\nScattering Oyallon et al. (2017)\n84.7\nthe total number of conv. layers of the examined RotNet models is 9, 12, and 15 for 3, 4, and 5\nconv. blocks respectively). Afterwards, we learn classiﬁers on top of the feature maps generated\nby each conv. block of each RotNet model. Those classiﬁers are trained in a supervised way on\nthe object recognition task of CIFAR-10. They consist of 3 fully connected layers; the 2 hidden\nlayers have 200 feature channels each and are followed by batch-norm and relu units. We report\nthe accuracy results of CIFAR-10 test set in Table 1. We observe that in all cases the feature maps\ngenerated by the 2nd conv. block (that actually has depth 6 in terms of the total number of conv.\nlayer till that point) achieve the highest accuracy, i.e., between 88.26% and 89.06%. The features of\nthe conv. blocks that follow the 2nd one gradually degrade the object recognition accuracy, which\nwe assume is because they start becoming more and more speciﬁc on the self-supervised task of\nrotation prediction. Also, we observe that increasing the total depth of the RotNet models leads to\nincreased object recognition performance by the feature maps generated by earlier layers (and after\nthe 1st conv. block). We assume that this is because increasing the depth of the model and thus\nthe complexity of its head (i.e., top ConvNet layers) allows the features of earlier layers to be less\nspeciﬁc to the rotation prediction task.\nExploring the quality of the learned features w.r.t. the number of recognized rotations: In Ta-\nble 2 we explore how the quality of the self-supervised features depends on the number of discrete\nrotations used in the rotation prediction task. For that purpose we deﬁned three extra rotation recog-\nnition tasks: (a) one with 8 rotations that includes all the multiples of 45 degrees, (b) one with only\nthe 0◦and 180◦rotations, and (c) one with only the 90◦and 270◦rotations. In order to implement\nthe rotation transformation of the 45◦, 135◦, 225◦, 270◦, and 315◦rotations (in the 8 discrete rota-\ntions case) we used an image wrapping routine and then we took care to crop only the central square\n7\n\n\nPublished as a conference paper at ICLR 2018\n(a)\n(b)\nFigure 5: (a) Plot with the rotation prediction accuracy and object recognition accuracy as a function\nof the training epochs used for solving the rotation prediction task. The red curve is the object\nrecognition accuracy of a fully supervised model (a NIN model), which is independent from the\ntraining epochs on the rotation prediction task. The yellow curve is the object recognition accuracy\nof an object classiﬁer trained on top of feature maps learned by a RotNet model at different snapshots\nof the training procedure. (b) Accuracy as a function of the number of training examples per category\nin CIFAR-10. Ours semi-supervised is a NIN model that the ﬁrst 2 conv. blocks are RotNet model\nthat was trained in a self-supervised way on the entire training set of CIFAR-10 and the 3rd conv.\nblock along with a prediction linear layer that was trained with the object recognition task only on\nthe available set of labeled images.\nimage regions that do not include any of the empty image areas introduced by the rotation transfor-\nmations (and which can easily indicate the image rotation). We observe that indeed for 4 discrete\nrotations (as we proposed) we achieve better object recognition performance than the 8 or 2 cases.\nWe believe that this is because the 2 orientations case offers too few classes for recognition (i.e., less\nsupervisory information is provided) while in the 8 orientations case the geometric transformations\nare not distinguishable enough and furthermore the 4 extra rotations introduced may lead to visual\nartifacts on the rotated images. Moreover, we observe that among the RotNet models trained with\n2 discrete rotations, the RotNet model trained with 90◦and 270◦rotations achieves worse object\nrecognition performance than the model trained with the 0◦and 180◦rotations, which is probably\ndue to the fact that the former model does not “see” during the unsupervised phase the 0◦rotation\nthat is typically used during the object recognition training phase.\nComparison against supervised and other unsupervised methods: In Table 3 we compare our\nunsupervised learned features against other unsupervised (or hand-crafted) features on CIFAR-10.\nFor our entries we use the feature maps generated by the 2nd conv. block of a RotNet model with\n4 conv. blocks in total. On top of those RotNet features we train 2 different classiﬁers: (a) a non-\nlinear classiﬁer with 3 fully connected layers as before (entry (Ours) RotNet + non-linear), and (b)\nthree conv. layers plus a linear prediction layer (entry (Ours) RotNet +conv.; note that this entry\nis basically a 3 blocks NIN model with the ﬁrst 2 blocks coming from a RotNet model and the\n3rd being randomly initialized and trained on the recognition task). We observe that we improve\nover the prior unsupervised approaches and we achieve state-of-the-art results in CIFAR-10 (note\nthat each of the prior approaches has a different ConvNet architecture thus the comparison with\nthem is just indicative). More notably, the accuracy gap between the RotNet based model and\nthe fully supervised NIN model is very small, only 1.64 percentage points (92.80% vs 91.16%).\nWe provide per class breakdown of the classiﬁcation accuracy of our unsupervised model as well\nas the supervised one in Table 9 (in appendix B). In Table 3 we also report the performance of the\nRotNet features when, instead of being kept frozen, they are ﬁne-tuned during the object recognition\ntraining phase. We observe that ﬁne-tuning the unsupervised learned features further improves the\nclassiﬁcation performance, thus reducing even more the gap with the supervised case.\nCorrelation between object classiﬁcation task and rotation prediction task: In Figure 5a, we\nplot the object classiﬁcation accuracy as a function of the training epochs used for solving the self-\nsupervised task of recognizing rotations, which learns the features used by the object classiﬁer.\n8\n\n\nPublished as a conference paper at ICLR 2018\nTable 4: Task Generalization: ImageNet top-1 classiﬁcation with non-linear layers. We com-\npare our unsupervised feature learning approach with other unsupervised approaches by training\nnon-linear classiﬁers on top of the feature maps of each layer to perform the 1000-way ImageNet\nclassiﬁcation task, as proposed by Noroozi & Favaro (2016). For instance, for the conv5 feature\nmap we train the layers that follow the conv5 layer in the AlexNet architecture (i.e., fc6, fc7, and\nfc8). Similarly for the conv4 feature maps. We implemented those non-linear classiﬁers with batch\nnormalization units after each linear layer (fully connected or convolutional) and without employ-\ning drop out units. All approaches use AlexNet variants and were pre-trained on ImageNet without\nlabels except the ImageNet labels and Random entries. During testing we use a single crop and do\nnot perform ﬂipping augmentation. We report top-1 classiﬁcation accuracy.\nMethod\nConv4\nConv5\nImageNet labels from (Bojanowski & Joulin, 2017)\n59.7\n59.7\nRandom from (Noroozi & Favaro, 2016)\n27.1\n12.0\nTracking Wang & Gupta (2015)\n38.8\n29.8\nContext (Doersch et al., 2015)\n45.6\n30.4\nColorization (Zhang et al., 2016a)\n40.7\n35.2\nJigsaw Puzzles (Noroozi & Favaro, 2016)\n45.3\n34.6\nBIGAN (Donahue et al., 2016)\n41.9\n32.2\nNAT (Bojanowski & Joulin, 2017)\n-\n36.0\n(Ours) RotNet\n50.0\n43.8\nMore speciﬁcally, in order to create the object recognition accuracy curve, in each training snapshot\nof RotNet (i.e., every 20 epochs), we pause its training procedure and we train from scratch (until\nconvergence) a non-linear object classiﬁer on top of the so far learnt RotNet features. Therefore,\nthe object recognition accuracy curve depicts the accuracy of those non-linear object classiﬁers after\nthe end of their training while the rotation prediction accuracy curve depicts the accuracy of the\nRotNet at those snapshots. We observe that, as the ability of the RotNet features for solving the\nrotation prediction task improves (i.e., as the rotation prediction accuracy increases), their ability to\nhelp solving the object recognition task improves as well (i.e., the object recognition accuracy also\nincreases). Furthermore, we observe that the object recognition accuracy converges fast w.r.t. the\nnumber of training epochs used for solving the pretext task of rotation prediction.\nSemi-supervised setting: Motivated by the very high performance of our unsupervised feature\nlearning method, we also evaluate it on a semi-supervised setting. More speciﬁcally, we ﬁrst train\na 4 block RotNet model on the rotation prediction task using the entire image dataset of CIFAR-10\nand then we train on top of its feature maps object classiﬁers using only a subset of the available\nimages and their corresponding labels. As feature maps we use those generated by the 2nd conv.\nblock of the RotNet model. As a classiﬁer we use a set of convolutional layers that actually has\nthe same architecture as the 3rd conv. block of a NIN model plus a linear classiﬁer, all randomly\ninitialized. For training the object classiﬁer we use for each category 20, 100, 400, 1000, or 5000\nimage examples. Note that 5000 image examples is the extreme case of using the entire CIFAR-\n10 training dataset. Also, we compare our method with a supervised model that is trained only\non the available examples each time. In Figure 5b we plot the accuracy of the examined models\nas a function of the available training examples. We observe that our unsupervised trained model\nexceeds in this semi-supervised setting the supervised model when the number of examples per\ncategory drops below 1000. Furthermore, as the number of examples decreases, the performance\ngap in favor of our method is increased. This empirical evidence demonstrates the usefulness of our\nmethod on semi-supervised settings.\n3.2\nEVALUATION OF SELF-SUPERVISED FEATURES TRAINED IN IMAGENET\nHere we evaluate the performance of our self-supervised ConvNet models on the ImageNet, Places,\nand PASCAL VOC datasets. Speciﬁcally, we ﬁrst train a RotNet model on the training images of the\nImageNet dataset and then we evaluate the performance of the self-supervised features on the image\n9\n\n\nPublished as a conference paper at ICLR 2018\nTable 5: Task Generalization: ImageNet top-1 classiﬁcation with linear layers. We compare\nour unsupervised feature learning approach with other unsupervised approaches by training logistic\nregression classiﬁers on top of the feature maps of each layer to perform the 1000-way ImageNet\nclassiﬁcation task, as proposed by Zhang et al. (2016a). All weights are frozen and feature maps are\nspatially resized (with adaptive max pooling) so as to have around 9000 elements. All approaches\nuse AlexNet variants and were pre-trained on ImageNet without labels except the ImageNet labels\nand Random entries.\nMethod\nConv1\nConv2\nConv3\nConv4\nConv5\nImageNet labels\n19.3\n36.3\n44.2\n48.3\n50.5\nRandom\n11.6\n17.1\n16.9\n16.3\n14.1\nRandom rescaled Kr¨ahenb¨uhl et al. (2015)\n17.5\n23.0\n24.5\n23.2\n20.6\nContext (Doersch et al., 2015)\n16.2\n23.3\n30.2\n31.7\n29.6\nContext Encoders (Pathak et al., 2016b)\n14.1\n20.7\n21.0\n19.8\n15.5\nColorization (Zhang et al., 2016a)\n12.5\n24.5\n30.4\n31.5\n30.3\nJigsaw Puzzles (Noroozi & Favaro, 2016)\n18.2\n28.8\n34.0\n33.9\n27.1\nBIGAN (Donahue et al., 2016)\n17.7\n24.5\n31.0\n29.9\n28.0\nSplit-Brain (Zhang et al., 2016b)\n17.7\n29.3\n35.4\n35.2\n32.8\nCounting (Noroozi et al., 2017)\n18.0\n30.6\n34.3\n32.5\n25.7\n(Ours) RotNet\n18.8\n31.7\n38.7\n38.2\n36.5\nTable 6: Task & Dataset Generalization: Places top-1 classiﬁcation with linear layers. We\ncompare our unsupervised feature learning approach with other unsupervised approaches by training\nlogistic regression classiﬁers on top of the feature maps of each layer to perform the 205-way Places\nclassiﬁcation task (Zhou et al., 2014). All unsupervised methods are pre-trained (in an unsupervised\nway) on ImageNet. All weights are frozen and feature maps are spatially resized (with adaptive max\npooling) so as to have around 9000 elements. All approaches use AlexNet variants and were pre-\ntrained on ImageNet without labels except the Place labels, ImageNet labels, and Random entries.\nMethod\nConv1\nConv2\nConv3\nConv4\nConv5\nPlaces labels Zhou et al. (2014)\n22.1\n35.1\n40.2\n43.3\n44.6\nImageNet labels\n22.7\n34.8\n38.4\n39.4\n38.7\nRandom\n15.7\n20.3\n19.8\n19.1\n17.5\nRandom rescaled Kr¨ahenb¨uhl et al. (2015)\n21.4\n26.2\n27.1\n26.1\n24.0\nContext (Doersch et al., 2015)\n19.7\n26.7\n31.9\n32.7\n30.9\nContext Encoders (Pathak et al., 2016b)\n18.2\n23.2\n23.4\n21.9\n18.4\nColorization (Zhang et al., 2016a)\n16.0\n25.7\n29.6\n30.3\n29.7\nJigsaw Puzzles (Noroozi & Favaro, 2016)\n23.0\n31.9\n35.0\n34.2\n29.3\nBIGAN (Donahue et al., 2016)\n22.0\n28.7\n31.8\n31.3\n29.7\nSplit-Brain (Zhang et al., 2016b)\n21.3\n30.7\n34.0\n34.1\n32.5\nCounting (Noroozi et al., 2017)\n23.3\n33.9\n36.3\n34.7\n29.6\n(Ours) RotNet\n21.5\n31.0\n35.1\n34.6\n33.7\nclassiﬁcation tasks of ImageNet, Places, and PASCAL VOC datasets and on the object detection and\nobject segmentation tasks of PASCAL VOC.\nImplementation details: For those experiments we implemented our RotNet model with an\nAlexNet architecture. Our implementation of the AlexNet model does not have local response\nnormalization units, dropout units, or groups in the colvolutional layers while it includes batch\nnormalization units after each linear layer (either convolutional or fully connected). In order to train\nthe AlexNet based RotNet model, we use SGD with batch size 192, momentum 0.9, weight decay\n5e −4 and lr of 0.01. We drop the learning rates by a factor of 10 after epochs 10, and 20 epochs.\nWe train in total for 30 epochs. As in the CIFAR experiments, during training we feed the RotNet\nmodel all four rotated copies of an image simultaneously (in the same mini-batch).\n10\n\n\nPublished as a conference paper at ICLR 2018\nTable 7: Task & Dataset Generalization: PASCAL VOC 2007 classiﬁcation and detection re-\nsults, and PASCAL VOC 2012 segmentation results. We used the publicly available testing\nframeworks of Kr¨ahenb¨uhl et al. (2015) for classiﬁcation, of Girshick (2015) for detection, and\nof Long et al. (2015) for segmentation. For classiﬁcation, we either ﬁx the features before conv5\n(column fc6-8) or we ﬁne-tune the whole model (column all). For detection we use multi-scale\ntraining and single scale testing. All approaches use AlexNet variants and were pre-trained on Ima-\ngeNet without labels except the ImageNet labels and Random entries. After unsupervised training,\nwe absorb the batch normalization units on the linear layers and we use the weight rescaling tech-\nnique proposed by Kr¨ahenb¨uhl et al. (2015) (which is common among the unsupervised methods).\nAs customary, we report the mean average precision (mAP) on the classiﬁcation and detection tasks,\nand the mean intersection over union (mIoU) on the segmentation task.\nClassiﬁcation\nDetection\nSegmentation\n(%mAP)\n(%mAP)\n(%mIoU)\nTrained layers\nfc6-8\nall\nall\nall\nImageNet labels\n78.9\n79.9\n56.8\n48.0\nRandom\n53.3\n43.4\n19.8\nRandom rescaled Kr¨ahenb¨uhl et al. (2015)\n39.2\n56.6\n45.6\n32.6\nEgomotion (Agrawal et al., 2015)\n31.0\n54.2\n43.9\nContext Encoders (Pathak et al., 2016b)\n34.6\n56.5\n44.5\n29.7\nTracking (Wang & Gupta, 2015)\n55.6\n63.1\n47.4\nContext (Doersch et al., 2015)\n55.1\n65.3\n51.1\nColorization (Zhang et al., 2016a)\n61.5\n65.6\n46.9\n35.6\nBIGAN (Donahue et al., 2016)\n52.3\n60.1\n46.9\n34.9\nJigsaw Puzzles (Noroozi & Favaro, 2016)\n-\n67.6\n53.2\n37.6\nNAT (Bojanowski & Joulin, 2017)\n56.7\n65.3\n49.4\nSplit-Brain (Zhang et al., 2016b)\n63.0\n67.1\n46.7\n36.0\nColorProxy (Larsson et al., 2017)\n65.9\n38.4\nCounting (Noroozi et al., 2017)\n-\n67.7\n51.4\n36.6\n(Ours) RotNet\n70.87\n72.97\n54.4\n39.1\nImageNet classiﬁcation task: We evaluate the task generalization of our self-supervised learned\nfeatures by training on top of them non-linear object classiﬁers for the ImageNet classiﬁcation task\n(following the evaluation scheme of (Noroozi & Favaro, 2016)). In Table 4 we report the classiﬁ-\ncation performance of our self-supervised features and we compare it with the other unsupervised\napproaches. We observe that our approach surpasses all the other methods by a signiﬁcant margin.\nFor the feature maps generated by the Conv4 layer, our improvement is more than 4 percentage\npoints and for the feature maps generated by the Conv5 layer, our improvement is even bigger,\naround 8 percentage points. Furthermore, our approach signiﬁcantly narrows the performance gap\nbetween unsupervised features and supervised features. In Table 5 we report similar results but\nfor linear (logistic regression) classiﬁers (following the evaluation scheme of Zhang et al. (2016a)).\nAgain, our unsupervised method demonstrates signiﬁcant improvements over prior unsupervised\nmethods.\nTransfer learning evaluation on PASCAL VOC: In Table 7 we evaluate the task and dataset\ngeneralization of our unsupervised learned features by ﬁne-tuning them on the PASCAL VOC clas-\nsiﬁcation, detection, and segmentation tasks. As with the ImageNet classiﬁcation task, we outper-\nform by signiﬁcant margin all the competing unsupervised methods in all tested tasks, signiﬁcantly\nnarrowing the gap with the supervised case. Notably, the PASCAL VOC 2007 object detection per-\nformance that our self-supervised model achieves is 54.4% mAP, which is only 2.4 points lower\nthan the supervised case. We provide the per class detection performance of our method in Table 8\n(in appendix B).\nPlaces classiﬁcation task: In Table 6 we evaluate the task and dataset generalization of our approach\nby training linear (logistic regression) classiﬁers on top of the learned features in order to perform\nthe 205-way Places classiﬁcation task. Note that in this case the learnt features are evaluated w.r.t.\n11\n\n\nPublished as a conference paper at ICLR 2018\ntheir generalization on classes that were “unseen” during the unsupervised training phase. As can\nbe seen, even in this case our method manages to either surpass or achieve comparable results w.r.t.\nprior state-of-the-art unsupervised learning approaches.\n4\nCONCLUSIONS\nIn our work we propose a novel formulation for self-supervised feature learning that trains a Con-\nvNet model to be able to recognize the image rotation that has been applied to its input images.\nDespite the simplicity of our self-supervised task, we demonstrate that it successfully forces the Con-\nvNet model trained on it to learn semantic features that are useful for a variety of visual perception\ntasks, such as object recognition, object detection, and object segmentation. We exhaustively eval-\nuate our method in various unsupervised and semi-supervised benchmarks and we achieve in all of\nthem state-of-the-art performance. Speciﬁcally, our self-supervised approach manages to drastically\nimprove the state-of-the-art results on unsupervised feature learning for ImageNet classiﬁcation,\nPASCAL classiﬁcation, PASCAL detection, PASCAL segmentation, and CIFAR-10 classiﬁcation,\nsurpassing prior approaches by a signiﬁcant margin and thus drastically reducing the gap between\nunsupervised and supervised feature learning.\n5\nACKNOWLEDGEMENTS\nThis work was supported by the ANR SEMAPOLIS project, an INTEL gift, and hardware donation\nby NVIDIA.\nREFERENCES\nPulkit Agrawal, Joao Carreira, and Jitendra Malik. Learning to see by moving. In Proceedings of\nthe IEEE International Conference on Computer Vision, pp. 37–45, 2015.\nYoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training\nof deep networks. In Advances in neural information processing systems, pp. 153–160, 2007.\nPiotr Bojanowski and Armand Joulin. Unsupervised learning by predicting noise. arXiv preprint\narXiv:1704.05310, 2017.\nCarl Doersch and Andrew Zisserman.\nMulti-task self-supervised visual learning.\nCoRR,\nabs/1708.07860, 2017.\nCarl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by\ncontext prediction. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n1422–1430, 2015.\nJeff Donahue, Philipp Kr¨ahenb¨uhl, and Trevor Darrell. Adversarial feature learning. arXiv preprint\narXiv:1605.09782, 2016.\nAlexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discrimina-\ntive unsupervised feature learning with convolutional neural networks. In Advances in Neural\nInformation Processing Systems, pp. 766–774, 2014.\nM. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object\nclasses (voc) challenge. International Journal of Computer Vision, 88(2):303–338, June 2010.\nRoss Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision,\npp. 1440–1448, 2015.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-\nmation processing systems, pp. 2672–2680, 2014.\nPriya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-\ndrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet\nin 1 hour. arXiv preprint arXiv:1706.02677, 2017.\n12\n\n\nPublished as a conference paper at ICLR 2018\nFu Jie Huang, Y-Lan Boureau, Yann LeCun, et al. Unsupervised learning of invariant feature hierar-\nchies with applications to object recognition. In Computer Vision and Pattern Recognition, 2007.\nCVPR’07. IEEE Conference on, pp. 1–8. IEEE, 2007.\nAndrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descrip-\ntions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n3128–3137, 2015.\nPhilipp Kr¨ahenb¨uhl, Carl Doersch, Jeff Donahue, and Trevor Darrell. Data-dependent initializations\nof convolutional neural networks. arXiv preprint arXiv:1511.06856, 2015.\nAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.\nGustav Larsson, Michael Maire, and Gregory Shakhnarovich. Learning representations for auto-\nmatic colorization. In European Conference on Computer Vision, pp. 577–593. Springer, 2016.\nGustav Larsson, Michael Maire, and Gregory Shakhnarovich. Colorization as a proxy task for visual\nunderstanding. arXiv preprint arXiv:1703.04044, 2017.\nYann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\nRenjie Liao, Alex Schwing, Richard Zemel, and Raquel Urtasun.\nLearning deep parsimonious\nrepresentations. In Advances in Neural Information Processing Systems, pp. 5076–5084, 2016.\nMin Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400,\n2013.\nJonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic\nsegmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\nJune 2015.\nJonathan Masci, Ueli Meier, Dan Cires¸an, and J¨urgen Schmidhuber. Stacked convolutional auto-\nencoders for hierarchical feature extraction. Artiﬁcial Neural Networks and Machine Learning–\nICANN 2011, pp. 52–59, 2011.\nMehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw\npuzzles. In European Conference on Computer Vision, pp. 69–84. Springer, 2016.\nMehdi Noroozi, Hamed Pirsiavash, and Paolo Favaro. Representation learning by learning to count.\narXiv preprint arXiv:1708.06734, 2017.\nEdouard Oyallon and St´ephane Mallat. Deep roto-translation scattering for object classiﬁcation.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2865–\n2873, 2015.\nEdouard Oyallon, Eugene Belilovsky, and Sergey Zagoruyko. Scaling the scattering transform:\nDeep hybrid networks. arXiv preprint arXiv:1703.08961, 2017.\nDeepak Pathak, Ross Girshick, Piotr Doll´ar, Trevor Darrell, and Bharath Hariharan. Learning fea-\ntures by watching objects move. arXiv preprint arXiv:1612.06370, 2016a.\nDeepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context\nencoders: Feature learning by inpainting. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 2536–2544, 2016b.\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\nconvolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\nrecognition challenge. International Journal of Computer Vision, 115(3):211–252, 2015.\nXiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using videos.\nIn Proceedings of the IEEE International Conference on Computer Vision, pp. 2794–2802, 2015.\n13\n\n\nPublished as a conference paper at ICLR 2018\nJianwei Yang, Devi Parikh, and Dhruv Batra. Joint unsupervised learning of deep representations\nand image clusters. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 5147–5156, 2016.\nRichard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In European Con-\nference on Computer Vision, pp. 649–666. Springer, 2016a.\nRichard Zhang, Phillip Isola, and Alexei A Efros. Split-brain autoencoders: Unsupervised learning\nby cross-channel prediction. arXiv preprint arXiv:1611.09842, 2016b.\nBolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep\nfeatures for scene recognition using places database. In Z. Ghahramani, M. Welling, C. Cortes,\nN. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Sys-\ntems 27, pp. 487–495. Curran Associates, Inc., 2014.\n14\n\n\nPublished as a conference paper at ICLR 2018\nAPPENDIX A\nVISUALIZING ATTENTION MAPS OF ROTATED IMAGES\nHere we visualize the attention maps generated by an AlexNet model trained on the self-supervised\ntask of rotation recognition for all the rotated copies of a few images. We observe that the atten-\ntion maps of all the rotated copies of an image are roughly the same, i.e., the attention maps are\nequivariant w.r.t. the image rotations. This practically means that in order to accomplish the rotation\nprediction task the network focuses on the same object parts regardless of the image rotation.\nAttention maps of Conv3 feature maps (size: 13 × 13)\n0◦rotation\n90◦rotation\n180◦rotation\n270◦rotation\nAttention maps of Conv5 feature maps (size: 6 × 6)\n0◦rotation\n90◦rotation\n180◦rotation\n270◦rotation\nFigure 6: Attention maps of the Conv3 and Conv5 feature maps generated by an AlexNet model\ntrained on the self-supervised task of recognizing image rotations. Here we present the attention\nmaps generated for all the 4 rotated copies of an image.\n15\n\n\nPublished as a conference paper at ICLR 2018\nAPPENDIX B\nPER CLASS BREAKDOWN OF DETECTION AND CLASSIFICATION\nPERFORMANCE\nIn Tables 8 and 9 we report the per class performance of our unsupervised learning method on the\nPASCAL detection and CIFAR-10 classiﬁcation tasks respectively.\nTable 8: Per class PASCAL VOC 2007 detection performance. As usual, we report the average\nprecision metric. The results of the supervised model (i.e., ImageNet labels entry) come from Doer-\nsch et al. (2015).\nClasses\naero\nbike\nbird\nboat\nbottle\nbus\ncar\ncat\nchair\ncow\ntable\ndog\nhorse\nmbike\nperson\nplant\nsheep\nsofa\ntrain\ntv\nImageNet labels\n64.0\n69.6\n53.2\n44.4\n24.9\n65.7\n69.6\n69.2\n28.9\n63.6\n62.8\n63.9\n73.3\n64.6\n55.8\n25.7\n50.5\n55.4\n69.3\n56.4\n(Ours) RotNet\n65.5\n65.3\n43.8\n39.8\n20.2\n65.4\n69.2\n63.9\n30.2\n56.3\n62.3\n56.8\n71.6\n67.2\n56.3\n22.7\n45.6\n59.5\n71.6\n55.3\nTable 9: Per class CIFAR-10 classiﬁcation accuracy.\nClasses\naero\ncar\nbird\ncat\ndeer\ndog\nfrog\nhorse\nship\ntruck\nSupervised\n93.7\n96.3\n89.4\n82.4\n93.6\n89.7\n95.0\n94.3\n95.7\n95.2\n(Ours) RotNet\n91.7\n95.8\n87.1\n83.5\n91.5\n85.3\n94.2\n91.9\n95.7\n94.2\n16\n"
}