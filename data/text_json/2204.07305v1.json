{
  "filename": "2204.07305v1.pdf",
  "num_pages": 18,
  "pages": [
    "Pushing the Limits of Simple Pipelines for Few-Shot Learning:\nExternal Data and Fine-Tuning Make a Difference\nShell Xu Hu1\nDa Li1*\nJan Stühmer1∗\nMinyoung Kim1∗\nTimothy M. Hospedales1,2\n1Samsung AI Center Cambridge\n2University of Edinburgh\n{shell.hu, da.li1, jan.stuhmer, k.minyoung, t.hospedales}@samsung.com\nAbstract\nFew-shot learning (FSL) is an important and topical prob-\nlem in computer vision that has motivated extensive research\ninto numerous methods spanning from sophisticated meta-\nlearning methods to simple transfer learning baselines. We\nseek to push the limits of a simple-but-effective pipeline for\nmore realistic and practical settings of few-shot image clas-\nsiﬁcation. To this end, we explore few-shot learning from\nthe perspective of neural network architecture, as well as a\nthree stage pipeline of network updates under different data\nsupplies, where unsupervised external data is considered for\npre-training, base categories are used to simulate few-shot\ntasks for meta-training, and the scarcely labelled data of\nan noval task is taken for ﬁne-tuning. We investigate ques-\ntions such as: 1 How pre-training on external data beneﬁts\nFSL? 2 How state-of-the-art transformer architectures can\nbe exploited? and\n3 How ﬁne-tuning mitigates domain\nshift? Ultimately, we show that a simple transformer-based\npipeline yields surprisingly good performance on standard\nbenchmarks such as Mini-ImageNet, CIFAR-FS, CDFSL\nand Meta-Dataset. Our code and demo are available at\nhttps://hushell.github.io/pmf.\n1. Introduction\nMainstream supervised deep learning achieves excellent\nresults in applications where huge annotated datasets are\navailable. However, this assumption is not met in many ap-\nplications where data (e.g., rare categories), or the cost of\nhuman annotation are prohibitive bottlenecks. This has moti-\nvated a large and growing set of research in few-shot learning\n(FSL), which aims to emulate the human ability to learn new\nconcepts from few training examples. The FSL challenge\nhas proven fertile ground for developing and testing a vast\narray of sophisticated research ideas spanning metric learn-\ning [59, 61], gradient-based meta-learning [29], program\ninduction [41], differentiable optimization layers [42], hy-\n*Equal contributions.\nCNN-4-64\nRN12\nRN18\nWRN-28-10 ViT-base\n50\n60\n70\n80\n90\n100\nminiImageNet 5-way-5-shot accuracy\nFigure 1. How does pre-training and architecture affect few-\nshot learning? Learning from a few shots can be achieved by a)\nmeta-learning [66,72] and b) transfer learning from self-supervised\nfoundation models pre-trained on large-scale external data [18,53].\nWhile the majority of FSL community focuses on the former, we\nshow that the latter can be more effective because it enables the use\nof stronger architectures such as vision transformer (ViT) [25] – and\ncan be combined with simple meta-learners such as ProtoNet. The\nﬁgure shows results aggregated from dozens of studies from the past\n5 years of FSL research and the result of ProtoNet + ViT backbone\n+ contrastive language-image pretraining (CLIP) [53] (yellow star).\nTo emphasize the importance of pre-training, ProtoNet + randomly\ninitialized ViT (blue square) is also compared.\npernetworks [9], neural optimizers [54], transductive label\npropagation [55], neural loss learning [4], Bayesian neural\npriors [72] and more [69]. But how much practical progress\nhave we made based on all these technical advances?\nA few studies [19, 20, 23, 51, 63, 68] have investigated\nwhether simpler baselines can offer comparable performance\nto sophisticated state of the art few-shot learners. While there\nis no conclusive answer, due to on-going developments in\nboth sophisticated learners [72] and simple baselines, there\nis a trend that simple approaches often perform surprisingly\narXiv:2204.07305v1  [cs.CV]  15 Apr 2022\n",
    "well compared to sophisticated counterparts. Their simplic-\nity and efﬁcacy leads these simple methods to be taken up\nin many practical applications of few-shot learning from\nmedical data analysis [11] to electronic engineering [40].\nWe follow this line of enquiry, but go further in inves-\ntigating previously under-studied factors that inﬂuence the\nperformance of simple few-shot pipelines. In particular we\nstart with a ProtoNet [59] few-shot learner, and investigate\nthree practically important design choices: pre-training data,\nneural network architecture, and meta-test time ﬁne-tuning.\nSource data\nWhile FSL addresses the small data regime,\nin reality FSL research is almost always about algorithms to\ntransfer knowledge from large scale source tasks (aka meta-\ntrain) to small scale target tasks (aka meta-test). Existing\nliterature almost always controls the source data, in order to\ncarefully compare the impact of different knowledge transfer\nmechanisms of interest from hyper-networks [9] to gradient-\nbased meta-learners [29]. While this is helpful to drive\nresearch on sophisticated algorithms, it does not answer the\nquestion of how choice of source data impacts performance?\nThis question has been studied in other areas of vision and\npattern recognition [10,31,60], but not for FSL. This is un-\nhelpful for consumers of computer vision FSL research, who\nwould be interested to know how much a simple change of\nsource data can improve their applications? Especially since\nfreely available large datasets already exist [21,62], and ex-\nploiting more external source data is easier in practice than\nimplementing sophisticated state-of-the-art meta-learners.\nTo this end we investigate the impact of unsupervised pre-\ntraining on external data – a workﬂow recently termed as\nexploiting a foundation model [10] – on FSL tasks. This\nsmall change has substantial impact compared to 5 years of\nFSL research (Figure 1). Although this may violate deﬁni-\ntions of the FSL problem that strictly prescribe the source\nset, the efﬁcacy of the approach may prompt reﬂection on\nwhether this is the best problem deﬁnition to focus on.\nNeural architecture\nSimilarly to the situation with\nsource data, FSL studies often control neural architecture to a\nhandful of small networks such as CNN-4-64 and ResNet-12.\nThis is partly to enable fair comparison of FSL algorithms,\nbut this particular suite of networks is also a consequence\nof the small size of the source datasets used for training in\ncommon benchmarks such as miniImageNet. Thus the archi-\ntectures commonly studied in FSL are somewhat out-of-date\nwith regard to state-of-the-art computer vision. We there-\nfore ask to what extent state-of-the-art architectures such as\nvision transformers [25] can beneﬁt few-shot performance,\nespecially in conjunction with larger pre-training datasets?\nFine-tuning\nThe many studies in the FSL literature are\nsomewhat divided in whether they advocate [29,54,65] some\nkind of ﬁne-tuning during model deployment (aka meta-test)\nfor individual tasks, or whether a ﬁxed feature representa-\ntion should be sufﬁcient [42, 59, 68]. We also investigate\nDomain A\nDomain B\nClass 1\nClass 2\nClass 3\nClass 4\nClass 5\nClass 6\nSupport set\nAugmented support set\nPre-trained backbone\nExternal data\nMeta-trained backbone\nTask-speciﬁcally ﬁne-tuned backbone\nFigure 2. Overview – A schematic of the simple-but-effective\npipeline that we consider: Pre-training →Meta-training →Fine-\ntuning (P>M>F). Following the red arrows, the pipeline turns a\nclass-agnostic feature backbone into a generic feature backbone\nand ultimately a task-speciﬁc feature backbone.\nthis issue, and suggest that ﬁne-tuning is necessary for de-\nploying foundation models to out-of-distribution tasks. We\nalso introduce an algorithmic improvement to ﬁne-tuning by\nautomating the learning rate selection via validation, which\nleads to a more performant pipeline for cross-domain FSL.\nIn summary, we advance few-shot learning by studying\ndesign choices of a simple pipeline [59] (Figure 2), rather\nthan developing new algorithms. We answer questions in-\ncluding: How does pre-training impact FSL? Can recent\ntransformer architectures be adapted to FSL? and How to\nbest exploit ﬁne-tuning? Based on this analysis we demon-\nstrate a new baseline for FSL that surpasses state-of-the-art\nperformance, while being simple and easy to implement.\n2. Related Work\nFew-shot learning\nFew-shot learning is now a deep and\nwidely studied area too large to review in detail here, and\nwe refer to relevant surveys for an overview [35, 69]. A\nkey point is that, despite the name, almost all FSL methods\nprovide algorithms for transferring knowledge from a large\nset of source data, to a set of sparsely annotated target cate-\ngories of interest. Much activity in the ﬁeld falls under the\numbrella of meta-learning [35], which aims to construct a\ndata-efﬁcient learner from the source (aka meta-train) dataset\nby simulating few-shot learning problems, and then deploy\nthe customized learner on the target (aka meta-test) set. The\nresulting learner may take the form of an initialization [29],\nlearned metric [59], Bayesian prior [72], or optimizer [54].\nSimple-but-effective baselines\nIn competition with the\nplethora of sophisticated few-shot learners [35,69] such as\nthose mentioned above, a number of recent studies have ad-\nvocated strong baselines that perform comparably well while\nbeing simpler. These are often based on a transfer learn-\ning [70] pipeline. They apply a conventional deep learner on\nthe source data, before adapting to the few-shot target data by\ntraining a simple linear [19,51,63] or centroid [68] classiﬁer\n",
    "on the ﬁxed representation, or ﬁne-tuning the feature back-\nbone as well [23]. These methods mostly use standardized\nFSL source datasets (such as miniImageNet) and architec-\ntures (such as ResNet-12 and WRN-10-28) to enable direct\ncomparisons of the advocated simple baselines to sophisti-\ncated learners. In contrast, we speciﬁcally aim to explore\nhow far practical FSL performance can be pushed by exploit-\ning other available pre-training datasets and architectures.\nA few studies have evaluated FSL on a larger scale using\ndatasets such as ImageNet1K [20] or ImageNet21K [23].\nHowever by changing both the source and target sets, this\ndoes not make it clear how choice/scale of source data im-\npacts a given target problem – the question that we answer\nhere. Others have explored the impact of conventional pre-\ntraining prior to meta-learning [20] or as a regularizer during\nmeta-learning [30] – but without exploiting extra data.\nBigger data and architectures\nThe impact of source\ndatasets is widely studied in standard supervised [60] and\nself-supervised [10, 31] learning in vision, and in pattern\nrecognition applications outside of vision [3,10,13,22]. How-\never, it is not widely evaluated in FSL, which is a surprising\nomission, since as we shall see it may well be the easiest\nway to improve practical FSL performance. Similarly, ex-\nisting FSL methods are almost exclusively based on a few\nless common architectures (e.g., Conv-4-64 and ResNet-12),\nwhich maybe due to the very ﬁrst experimental setup on\nsmall datasets like Omniglot [29, 66]. Transformers have\nseen limited use in FSL, mainly for metric learning [24],\nbut not for feature extraction. We explore how recent trans-\nformer feature extractors can be trained and applied to FSL,\nespecially when combined with a foundation model [10]\npre-trained on larger source datasets.\nSelf-supervised & few-shot\nOur pipeline extends the typ-\nical unsupervised pre-train →supervised ﬁne-tune workﬂow\nof the self-supervised research community [28,39], which\nhas recently demonstrated strong performance for low-shot\nsupervised learning [15, 18, 27]. However, there has been\nlimited direct comparison of self-supervised (SSL) and FSL\ncommunity methods for data efﬁcient learning due to dif-\nferent typical evaluation practices and benchmarks. For\nexample, many SSL evaluations perform unsupervised repre-\nsentation learning on ImageNet, before performing few-shot\nsupervised learning within ImageNet [15,18], which violates\nusual FSL community requirement of disjoint source and\ntarget data. One contribution of this paper is to provide a\ndegree of comparison between and combination of the SSL\nand FSL approaches. For example, our MetaDataset, CDFSL\nand teaser Figure 1 results, use disjoint source and target\ndata but beneﬁt from external self-supervised pre-training.\nCross-domain few-shot\nA FSL variant of particular prac-\ntical interest is cross-domain few-shot [33], where the\nsource/meta-train dataset is signiﬁcantly different to the\ntarget/meta-test dataset. This is more challenging than the\nstandard within-domain setting, but more practically relevant.\nThis is because in many scenarios where FSL is of interest\nsuch as medical or earth observation imaging [33], the target\ndata for FSL is signiﬁcantly different to available source data\n(such as (mini-)ImageNet [21]). Major benchmarks of this\ntype are CDFSL [33] and meta-dataset [65].\n3. A Simple Pipeline for FSL\nProblem formulation\nFew-shot learning (FSL) aims to\nlearn a model with only a few annotated examples. One\nwidely adopted formulation for FSL was introduced by\nVinyals et al. [66] from a meta-learning perspective, where\nthe assumption is that one should learn to solve new few-shot\ntasks based on previously seen experience of many similar\nfew-shot tasks. Therefore, the FSL problem is usually or-\nganized in two phases: meta-training a few-shot learner on\na distribution of training tasks and meta-testing the result-\ning learner by evaluating it on novel few-shot tasks. Within\neach phase, data arrives in an episodic fashion, where the\n“train-set” and “test-set” of each task are called support set\nand query set respectively to avoid terminology confusion.\nIn the case of classiﬁcation, the difﬁculty level of an episode\nis described as K-way-N-shot, which corresponds to learn-\ning a classiﬁer for K classes given N examples per class in\nthe support set. It is common to learn one model for each\ndifﬁculty level, but a more realistic setting [65] is to learn\na global model for various K’s and N’s. This is sometimes\ncalled various-way-various-shot, and we address this more\npractical setting here. This is also a reason to prefer simple\npipelines over sophisticated meta-learners that may not be\neasily extended to the various-way-various-shot setting.\nA different approach to small-data learning appears in\nthe transfer learning [12, 70] and self-supervision [10, 17]\nliterature. In this case one pre-trains a model using some\nlarge source data, and then re-purposes it for the sparse data\ntarget task of interest. The pre-training step aims to reduce\nthe sample complexity of learning the target problem in the\nadaptation step.\nAlthough typically studied separately, both families of\napproach provide mechanisms for knowledge transfer from\nsource data to the target few-shot problem of interest. To-\nwards the goal of high performance few-shot learning, we\ncombine both pre-training (typically on auxiliary unlabeled\ndata, which is freely and ubiquitously available) and meta-\nlearning (episodic training with labels) together in a simple\nsequential pipeline using a single feature extractor back-\nbone. Our pipeline consists of three phases: 1) pre-training\nthe feature backbone on unlabeled external data using self-\nsupervised loss, 2) meta-training the feature backbone on\nlabeled simulated few-shot tasks using ProtoNet [59] loss,\nand 3) deploying the feature backbone on novel few-shot\n",
    "tasks with optional ﬁne-tuning on the augmented support\nset of each task. A schematic of our pipeline is shown in Fig-\nure 2, which we call P>M>F (i.e., the pipeline Pre-training\n→Meta-training →Fine-tuning ). We next outline how the\nfeature backbone is updated in different stages.\n3.1. Pre-training of backbone\nWe consider the feature backbones of ResNet [34] or\nViT [25], to provide the foundation models in our pipeline.\nThere are then several well-established self-supervised learn-\ning algorithms for the pre-training step: DINO [15] uses\nImageNet1K and exploits the consistency in prediction be-\ntween a large crop and multiple local crops of the same\nimage, where a large crop is highly likely to overlap with a\nforeground object in the case of ImageNet images; BEiT [6]\namounts to solving a masked image reconstruction task on\nthe ImageNet-21K dataset in line with the original BERT\npre-training [22] for text data; and CLIP [53] leverages im-\nage captions in the YFCC100m dataset to align image and\ncaption representations in a common feature space. For\nmore ﬂexible architectures like ViT [25], pre-training on ex-\nternal data is important, as they are hard to train on common\nsmall-sized FSL benchmarks (Figure 1 and Table 1).\n3.2. Meta-training with ProtoNet\nAs the goal is to build a simple pipeline, we consider the\nprototypical network (ProtoNet) [59], which constructs class\ncentroids dynamically for each episode and then performs\nnearest centroid classiﬁcation. Speciﬁcally, ProtoNet only\nrequires a feature backbone f to map data points to a m-\ndimensional feature space: f : X →Rm, and the probability\nof a query image x belonging to class k is given by\np(y = k|x) =\nexp\n\u0000−d(f(x), ck)\n\u0001\nP\nk′ exp\n\u0000−d(f(x), ck′)\n\u0001,\n(1)\nwhere d is implemented by a cosine distance in our work\nas opposed to the commonly chosen Euclidean distance\nand ck is the prototype of class k, deﬁned as ck\n=\n1\nNk\nP\ni:yi=k f(xi) and Nk = P\ni:yi=k 1 on the support set.\nNote that the prototypes can be computed regardless of the\nvalue of k. This enables ProtoNet to be trained and deployed\nunder various-way-various-shot setting.\n3.3. Meta-testing with ﬁne-tuning\nTo be consistent with meta-training, by default, we de-\nploy the meta-trained ProtoNet directly on all novel tasks.\nHowever, if the a novel task is drawn from an unseen domain,\nthe learned feature representation may fail to generalize due\nto a substantial shift in the data distribution. To this end, we\npropose to ﬁne-tune the feature backbone by a few gradient\nsteps with the assistance of data augmentation. The details\nare summarized as PyTorch pseudo code in Algorithm 1.\nAlgorithm 1 PyTorch pseudo code for ﬁne-tuning\n# Inputs: a task including supp_x, supp_y, query_x\n# backbone_state: meta-trained backbone weights\n# optimizer: Adam optimizer\n# Outputs: logits\nbackbone = create_model_from_checkpoint(backbone_state)\ndef single_step(z):\nsupp_f = backbone(supp_x)\nproto = compute_prototypes(supp_f, supp_y)\nf = backbone(z)\nlogits = f.norm() @ proto.norm().T # cos similarity\nloss = cross_entropy_loss(logits, supp_y)\nreturn logits, loss\n# fine-tuning loop\nfor i in range(num_steps):\naug_supp_x = rand_data_augment(supp_x)\n_, loss = single_step(aug_supp_x)\nloss.backward() # back-prop\noptimizer.step() # gradient descent\nlogits, _ = single_step(query_x) # classification\nOur ﬁne-tuning algorithm is similar to that of [33, 43]\nwho ﬁne-tune the model weights using the support set since\nthis is the only accessible labeled data at meta-test time.\nWe exploit the support set slightly differently: we use data\naugmentation to create a pseudo query set derived from the\nsupport set; as such, we do not need to compute prototypes\nusing the support set and then again apply the prototypes on\nthe same support set using eq. (1). Besides, we simply up-\ndate the entire backbone rather than exploring partial model\nadaptation.\nLearning rate selection\nWe observe that the ﬁne-tuning\nperformance is relatively sensitive to the choice of learning\nrate (see supplemental material for more analysis). However,\nexisting few-shot learning problem formulation does not\noffer a validation set for each task to choose the best learning\nrate for ﬁne-tuning. Previous work [33,43] choose a learning\nrate a priori and ﬁx it for every task. This strategy requires\na good understanding of the backbone architecture but still\nleads to sub-optimal performance in general. Given a task\nwith very few labeled images (i.e. the support set), it is\nalmost unlikely to identify which learning rate yields good\ngeneralization for unlabeled images (i.e. the query set). The\ngood news is that we ﬁnd empirically the best learning rate\nis relatively stable across tasks within the same domain. To\nthis end we propose to sample N = 5 extra tasks from\neach domain and automate domain-wise learning rate search\nwithin a reasonable range (e.g., {0.01, 0.001, 0.0001, 0}).\nThe best learning rate is then used for every task within the\ndomain. This additional step amounts to preparing a few\nlabeled images per domain to create a validation set, which\nmakes sense in practice as we can easily organize tasks by\ndomains and identify domain for individual tasks to look up\nthe corresponding learning rate once searched.\n",
    "4. Experiments\nMeta-training datasets\nWe use standard benchmarks to\nevaluate our proposed pipeline. miniImageNet [66] con-\ntains 100 classes from ImageNet-1k, which is then split into\n64 training, 16 validation and 20 testing classes; each image\nis downsampled to 84×84. CIFAR-FS [8] is created by\ndividing the original CIFAR-100 into 64 training, 16 valida-\ntion and 20 testing classes. The images are of size 32×32.\nMeta-Dataset [65] subsumes 10 public image datasets of a\ndiverse range of domains: ImageNet-1k, Omniglot, FGVC-\nAircraft, CUB-200-2011, Describable Textures, QuickDraw,\nFGVCx Fungi, VGG Flower, Trafﬁc Signs and MSCOCO.\nEach dataset has train/val/test splits. We follow the two\ntraining protocols proposed by [65] and [24] respectively.\nFor the former, the train/val splits of the ﬁrst 8 datasets (in-\ndomain) are used for meta-training and validation, and the\ntest splits of all datasets are used for meta-testing. The latter\nconsiders only ImageNet-1k’s train-split for meta-training,\nand the other settings remain the same. For more details on\nMeta-Dataset we refer the readers to Appendix.3 of [65].\nEvaluation\nFor evaluating few-shot classiﬁcation perfor-\nmance, we simulate 600 episodes/tasks from the test-split\nfor each dataset of interest. The evaluation metric is the av-\nerage classiﬁcation accuracy over tasks. For miniImageNet\nand CIFAR-FS, the convention is to evaluate 5-way-1-shot\n(5w1s) and 5-way-5-shot episodes, and the size of the query\nset for each episode is ﬁxed to 15 × 5. For Meta-Dataset, the\nnumber of ways, shots and query images are sampled uni-\nformly at random with respect to the dataset speciﬁcations,\nexcept for ImageNet-1k and Omniglot (they have speciﬁc\nsampling strategies according to the hierarchy of classes). In\naddition, we evaluate the (5w5s) meta-trained model from\nminiImageNet for a cross-domain evaluation (CDFSL) [33],\nwhere 4 out-of-domain datasets are considered, and the re-\nsults are reported under 5-way-5/20/50-shot settings.\nTraining details\nTo avoid over-engineering training for\ndifferent datasets and architectures, we adopt a common\ntraining strategy for meta-training the backbone from pre-\ntrained model checkpoints (for both ResNet and ViT). This\nmay lead to sub-optimal results for some cases, but it sim-\npliﬁes comparison. Speciﬁcally, we train the backbone for\n100 epochs, where each epoch consists of 2000 episodes/-\ntasks. We use a warm-up plus cosine annealing learning rate\nschedule: the learning rate starts from 10−6, increases to\n5 × 10−5 in 5 epochs and then gradually decreases to 10−6\nwith a cosine annealing. We use the validation set to decide\nwhen to early stop, and turn off strong regularization and\ndata augmentation techniques for simplicity.\n4.1. Analysis\nWe now use the pipeline outlined in Sec 3 to answer a\nseries of questions about few-shot learner pipeline design.\nTraining Conﬁguration\nBenchmark Results\nID\nArch\nPre Train\nMetaTr\nMD\nminiIN\nCIFAR\n0\nViT-small\nDINO (IN1K)\n-\n67.4\n97.0\n79.8\n1\nViT-small\nDeiT (IN1K)\n-\n67.5\n98.8\n84.6\n2\nResNet50\nDINO (IN1K)\n-\n63.8\n91.5\n76.1\n3\nResNet50\nSup. (IN1K)\n-\n62.4\n96.4\n82.3\n4\nViT-small\nDINO (IN1K)\nPN\n78.4\n98.0\n92.5\n5\nViT-small\nDEIT (IN1K)\nPN\n79.3\n99.4\n93.6\n6\nViT-small\n-\nPN\n52.8\n49.1\n59.8\n7\nResNet50\nDINO (IN1K)\nPN\n72.4\n92.0\n84.0\n8\nResNet50\nSup. (IN1K)\nPN\n70.2\n97.4\n87.6\n9\nResNet50\n-\nPN\n62.9\n72.2\n68.4\n10\nResNet18\n-\nPN\n63.3\n73.7\n70.2\n11\nViT-base\nDINO (IN1K)\nPN\n79.2\n98.4\n92.2\n12\nViT-base\nCLIP (YFCC)\nPN\n80.0\n98.1\n93.2\n13\nViT-base\nSup (IN21K)\nPN\n81.4\n99.2\n96.7\n14\nViT-base\nBEIT (IN21K)\nPN\n82.8\n99.0\n97.5\n15\nResNet50\nCLIP (YFCC)\nPN\n75.0\n92.2\n82.6\nTable 1. The impact of architecture and pre-training algorithm\n(dataset) on downstream few-shot learning performance on Meta-\nDataset (MD), miniImageNet (miniIN) and CIFAR-FS. Meta-\nDataset results are averaged over all target datasets while minIN and\nCIFAR results are 5-way-5-shot. ProtoNet (PN) nearest-centroid\nclassiﬁer is used throughout for few-shot learning on the support set\nduring meta-test. MetaTr indicates the algorithm used for episodic\nlearning on the corresponding benchmark.\nNotably, 1 How does pre-training regime affect FSL? 2\nCan contemporary architectures such as ViT be adapted to\nFSL? 3 How to exploit ﬁne-tuning in meta-testing?\n4.1.1\nPre-training and architectures\nWe ﬁrst evaluate the impact of pre-training regime (includ-\ning algorithm and dataset), as well as neural architecture\non FSL benchmarks Meta-Dataset [65] (train on 8 datasets),\nminiImageNet [66], and CIFAR-FS [8]. To clearly con-\nvey the conﬁguration of each experiment, results in Table 1\nare organized by architecture, pre-training algorithm (and\ndataset) and meta-training algorithm. We assume ProtoNet\n(nearest-centroid) classiﬁer as the standard approach for\nmeta-testing throughout, and compare either episodically\ntrained ProtoNet or nothing as the meta-learning step be-\ntween pre-training and meta-testing (column MetaTr).\n1 How does pre-training regime affect FSL?\nFrom the\nresults in Table 1 we can draw the following conclusions: (i)\nPre-training on ImageNet1K generally provides a signiﬁcant\nimprovement across the board compared to the conventional\npipeline used by prior work which does not make use of pre-\ntraining (compare model M9 with M7 and M8, etc). (ii) We\nare primarily interested in unsupervised pre-training, with\nsupervised pre-training being included as an unfair upper\nbound. However, state of the art unsupervised pre-training\nwith DINO performs close to supervised pre-training (com-\npare M3 vs M2, etc). This is noteworthy, because while\nthere is some semantic overlap between some of the source\n",
    "(ImageNet1K) and target (Meta-Dataset, miniImageNet, CI-\nFAR) datasets considered here, good performance can be\nachieved without using source labels, where there is no train-\ntest label leakage1. (iii) Given a strong pre-training regime\nsuch as DINO, simple nearest centroid classiﬁcation based\non pre-trained features performs well (top block including\nM2, etc). In particular, off-the-shelf features from a founda-\ntion model without dataset-speciﬁc meta-learning perform\nfavorably compared to conventional dataset-speciﬁc training\nof ProtoNet-ResNet18 (M2 vs M10), which is arguably the\nclosest to industry standard in FSL. (iv) Nevertheless, dataset\nspeciﬁc meta-learning does improve further (M7 vs M2, etc).\nSimple linear readout of a frozen foundation model [18,27]\nis not competitive.\n2 Can state of the art architectures such as ViT be\nadapted to FSL?\nUsing the results in Table 1, we can\nalso answer this question. In particular, while ViT does not\ntrain well on the smaller meta-train benchmarks (miniIma-\ngeNet, CIFAR) compared to smaller architectures (see M6\nvs M9, M10), it generally performs excellently when bene-\nﬁting from large pre-training data (M6 vs M4). Overall ViT\noutperforms the industry standard ResNet18, as well as our\nResNet50 baseline, across the board when beneﬁtting from\npre-training. We remark that our ResNet50 baseline also per-\nforms comparitively poorly without pre-training, especially\non the smaller miniImageNet and CIFAR, suggesting that it\nis also too large to train well on the target datasets alone.\nOther foundation models\nOverall we can see that larger\npre-training data sources, and recent architectures make a\nhuge difference to downstream FSL performance on stan-\ndard benchmarks. We also compared a selection of other\nfoundation models [10] in M11-15. We can see that (i) All\nthe foundation models lead to substantial improvements on\nstandard within-dataset training (M10,M9), (ii) The largest\nfoundation models using, e.g., ViT-base and ImageNet21K\nor YFCC data source lead to strongest performance across\nthe board, but do not outperform hugely the more economic\nDINO+ImageNet1K-based ViT-small (M4). For efﬁciency\nof pre-training and deployment, we take this to be our default\nmodel in the following section.\n1 + 2 How does pre-training and architecture impact\nother Few-Shot Learners?\nOur main experiments built\nupon ProtoNet as a widely used industry standard. We next\n1In the case of miniImageNet and Meta-Dataset, parts of ImageNet1K\nare used in both meta-train and meta-test splits. EG: since Meta-Dataset’s\nImageNet uses a 712/288 source/target class split, this means that for one\nof Meta-Dataset’s 10 domains, there is some data (but not label) overlap\nbetween pre-train and meta-test for some foundation models. As discussed\nin Sec. 2, this overlap is ubiquitious in typical self-supervision evaluation\npipelines [15, 17]. It is less common in FSL evaluation pipelines, but\ncorresponds to making a semi-supervised or transductive assumption in\nterms of data access as per [38,45,49,55]. Nevertheless, we do not think\nthis is a signiﬁcant factor in the strong results, as CLIP’s YFCC does not\nhave this overlap and performs similarly to the ImageNet1K based models.\nTrain Conﬁg\nBenchmark\nID\nArch\nPre Train\nMetaTr\nminiIN\nCIFAR\n5/1\n5/5\n5/1\n5/5\n0\nViT-small\nDINO (IN1K)\n-\n88.8\n97.0\n59.1\n79.8\n1\nViT-small\nDINO (IN1K)\nProtoNet\n93.1\n98.0\n81.1\n92.5\n2\nResNet18\n-\nMetaQDA\n65.1\n81.0\n-\n-\n3\nViT-small\nDINO (IN1K)\nMetaQDA\n92.0\n97.0\n77.2\n90.1\n4\nResNet12\n-\nMetaOptNet 64.1\n80.0\n72.8\n85.0\n5\nViT-small\nDINO (IN1K)\nMetaOptNet 92.2\n97.8\n70.2\n84.1\nTable 2. Impact of architecture and pre-training on state-of-the-art\nfew-shot learners: MetaQDA [72], MetaOptNet [42].\nexplore how our pipeline impacts two few-shot learners that\nare more representative of recent state of the art, namely\nMetaOptNet [42] and MetaQDA [72]. From the results in\nTable 2, we can see that: (i) MetaQDA and MetaOptNet do\nimprove on direct feature transfer (M5 and M3 vs M0) and\non the simpler ResNet features they were initially evaluated\nwith (M5 vs M4, M3 vs M2). But (ii) With the stronger\nfeatures, they are outperformed by the simpler ProtoNet\nlearner (M3 and M5 vs M1). This suggests previous con-\nclusions about comparative meta-learner performance may\nneed re-evaluating in this new regime of stronger features.\nFew-shot learning v.s. self-supervised learning\nExist-\ning literature generally fails to directly compare algorithms\nfrom the few-shot learning community (such as ProtoNet,\n[59], MAML [29], MetaOptNet [42], etc), with those from\nthe self-supervised community (such as DINO [15], Sim-\nCLR [17,18], etc). This is partly because the popular evalua-\ntion protocol is different: For example 5-way-1-shot regime\nis popular the FSL community, vs 1% labels (≈1000-way-\n10-shot in the case of ImageNet) in the SSL community;\nnetwork architectures differ (≤ResNet18 vs ≥ResNet50 re-\nspectively); and image resolutions differ (84× vs full). Our\nresults provide a taster of such a direct comparison. Overall\nthey suggest that frozen self-supervised foundation models\n(using extra pre-training data) are competitive out of the box\ncompared to standard few-shot learners (using only meta-\ntraining data). However, more interestingly, combining these\ntwo paradigms as we have done, easily leads to state of the\nart performance on typical FSL metrics.\nClass overlap between pre-training and meta-testing\nAlthough unsupervised pre-training does not utilize labels, it\nis very likely that some classes used by pre-training also ap-\npear in meta-testing. Does this class overlap go against the\nvery deﬁnition of few-shot learning? From a meta-learning\npoint of view, the answer is yes. But we argue that class\noverlap is almost unavoidable unless a careful data split is\nsimulated. For example, in the case of Meta-Dataset, the\nCUB dataset [67], the Aircraft dataset [50] and the COCO\ndataset [47] have a class overlap with ImageNet [24,32] but\nthey are still used in meta-testing. As we consider more\npractical large-scale experiments, the class overlap issue be-\n",
    "M\nArch\nPreTr\nMetaTr\nMetaTe\nAvg\nOut-D\n1\nViT-small\nDINO\nPN (IN)\nPN\n68.38\n67.68\n2\nViT-small\nDINO\nPN (IN)\nPN+FT(lr=0.01)\n76.05\n76.54\n3\nViT-small\nDINO\nPN (IN)\nPN+FT(lr=0.001)\n74.47\n74.51\n4\nViT-small\nDINO\nPN (IN)\nPN+FT(Tuned)\n77.53\n77.85\n5\nViT-small\nDINO\nPN (MD)\nPN\n78.43\n55.71\n6\nViT-small\nDINO\nPN (MD)\nPN+FT(lr=0.01)\n76.09\n73.26\n7\nViT-small\nDINO\nPN (MD)\nPN+FT(lr=0.001)\n74.64\n69.97\n8\nViT-small\nDINO\nPN (MD)\nPN+FT(Tuned)\n83.13\n75.72\nTable 3. Fine-tuning (FT) during meta-test on Meta-Dataset. The\nmeta-train (MetaTr) setting indicates the source dataset as Ima-\ngeNet only (IN) or full MetaDataset (MD). Results are the averages\nacross all domains within meta-dataset (Avg), and just the out-of-\ndistribution subset (Out-D).\ncomes ubiquitous. We should worry about this issue if we\nwere benchmarking a meta-learning algorithm, but for the\nnature of few-shot learning, benchmarking the capability of\nquickly constructing a classiﬁer from very few labels is not\nhindered by class overlap. This is why self-supervised learn-\ning community is not bothered by this issue at all. It is worth\nmentioning that a similar setting called “few-shot few-shot\nlearning” has been proposed by [46,71], where they avoid\noverlap by either carefully picking up pre-training data from\na different domain or crawling pre-training data of base cate-\ngories from Internet. Alternatively, one may avoid overlap\nby using a different modality. We advocate meta-learning\nresearchers to consider this controlled setting as a testing bed\nfor incorporating powerful pre-trained feature backbones.\nINet\nOmglot\nAcraft \nCUB\nDTD \nQDraw \nFungi \nFlower \nSign \nCOCO\nAvg\n50\n55\n60\n65\n70\n75\n80\n85\n90\n95\nM1: DINO + PN (IN)\nM2: DINO + PN (IN) + FT\nM5: DINO + PN (MD) \nFigure 3. The impact of ﬁne-tuning during meta-test on Meta-\nDataset. Held out datasets such as Signs and COCO beneﬁt from\nﬁne-tuning; as do those very different from ImageNet such as\nomniglot and QuickDraw.\n4.1.2\nFine-tuning\nThe previous experiments used a ﬁxed feature extractor to-\ngether with ProtoNet for meta-testing. We next investigate\nuse of ﬁne-tuning during meta-testing to further improve per-\nformance. We focus on the DINO pre-trained ViT models,\nbased on their strong performance in Section 4.1.1.\n3\nHow to best exploit ﬁne-tuning for meta-testing?\nMethod (Backbone)\nExt.\nExt.\nCIFAR-FS\nMiniImageNet\ndat.\nlab.\n5w1s\n5w5s\n5w1s\n5w5s\nInductive\nProtoNet (CNN-4-64) [59]\n49.4\n68.2\n55.5\n72.0\nBaseline++ (CNN-4-64) [19]\n48.2\n66.4\nMetaOpt-SVM (ResNet12) [42]\n72.0\n84.3\n61.4\n77.9\nMeta-Baseline (ResNet12) [20]\n68.6\n83.7\nRS-FSL (ResNet12) [2]\n\u0013\n65.3\nTransductive\nFine-tuning (WRN-28-10) [23]\n76.6\n85.8\n65.7\n78.4\nSIB (WRN-28-10) [36]\n80.0\n85.3\n70.0\n79.2\nPT-MAP (WRN-28-10) [37]\n87.7\n90.7\n82.9\n88.8\nCNAPS + FETI (ResNet18) [7]\n\u0013\n\u0013\n79.9\n91.5\nSelf-supervised\nProtoNet (WRN-28-10) [30]\n73.6\n86.1\n62.9\n79.9\nProtoNet (AMDIM ResNet) [16]\n\u0013\n76.8\n91.0\nEPNet + SSL (WRN-28-10) [57]\n\u0013\n79.2\n88.1\nSemi-supervised\nLST (ResNet12) [45]\n\u0013\n70.1\n78.7\nPLCM (ResNet12) [38]\n\u0013\n77.6\n86.1\n70.1\n83.7\nP>M>F (IN1K, RN50)\n\u0013\n73.7\n84.0\n79.2\n92.0\nP>M>F (IN1K, ViT-Small)\n\u0013\n81.1\n92.5\n93.1\n98.0\nP>M>F (IN1K, ViT-base)\n\u0013\n84.3\n92.2\n95.3\n98.4\nTable 4. miniImageNet & CIFAR – Comparison with represen-\ntative SOTA FSL algorithms. Methods using external data and/or\nlabels are indicated.\nTo answer this question, we compare vanilla feature transfer\nas explored previously, with ProtoNet, and ProtoNet with\nepisode-wise ﬁne-tuning on the support set (ProtoNet+FT)\nas outlined in Section 3.3. We use Meta-Dataset including\nboth conditions of treating ImageNet alone as the source, and\njoint meta-training on all of Meta-Dataset. From the results\nin Figure 3 and Table 3 we can draw the following conclu-\nsions: (i) Meta-training on the full Meta-Dataset improves\non meta-training on ImageNet-training alone (M5 vs M1).\n(ii) Fine-tuning during meta-test improves substantially in\nthe out-of-distribution datasets, and especially in the case\nwhere meta-training is conducted on ImageNet, and then\ndeployed across-domain to all the other Meta-Dataset tasks:\nSee Out-D column and M2 vs M1 in Table 3; blue vs orange\nbars in Figure 3 for OmniGlot, QuickDraw, trafﬁc signs,\netc. However, for the condition where more Meta-Dataset\ndomains are used for training and testing, ﬁne-tuning has\ninconsistent impact across domains: While it is helpful for\nthe remaining OOD datasets, it is not helpful overall (M5 vs\nM6 for Avg and Out-D). Overall feature backbone updates\nby ﬁne-tuning are more helpful for domains unseen during\nmeta-training, concurring with [43, 65]. On analysing the\ninconsistent impact of ﬁne-tuning, we found this is due to\ndifﬁculty in choosing an appropriate learning rate. Using any\nsingle learning rate throughout, as we did above (lr=0.01) is\npoorly tuned for some datasets. We therefore also explore\nour learning rate selection heuristic proposed in Section 3.3,\nand we see this leads to the best performance (M4 vs M2).\n4.2. Results on standard benchmarks\nWe call our pipeline P>M>F, which can be instantiated\nwith any pre-training algorithm and backbone architectures,\n",
    "8 in-domain datasets\nIn-domain\nOut-of-domain\nINet\nOmglot\nAcraft\nCUB\nDTD\nQDraw\nFungi\nFlower\nSign\nCOCO\nAvg\nProtoNet [65] (RN18)\n67.01\n44.5\n79.56\n71.14\n67.01\n65.18\n64.88\n40.26\n86.85\n46.48\n63.29\nCNAPs [56] (RN18+Adapter)\n50.8\n91.7\n83.7\n73.6\n59.5\n74.7\n50.2\n88.9\n56.5\n39.4\n66.90\nSUR [26] (RN18+Adapter)\n57.2\n93.2\n90.1\n82.3\n73.5\n81.9\n67.9\n88.4\n67.4\n51.3\n75.32\nT-SCNAPs [7] (RN18+Adapter)\n58.8\n93.9\n84.1\n76.8\n69.0\n78.6\n48.8\n91.6\n76.1\n48.7\n72.64\nURT [48] (RN18+Adapter)\n55.7\n94.4\n85.8\n76.3\n71.8\n82.5\n63.5\n88.2\n69.4\n52.2\n73.98\nFLUTE [64] (RN18)\n51.8\n93.2\n87.2\n79.2\n68.8\n79.5\n58.1\n91.6\n58.4\n50.0\n71.78\nURL [44] (RN18+Adapter)\n57.51\n94.51\n88.59\n80.54\n76.17\n81.94\n68.75\n92.11\n63.34\n54.03\n75.75\nITA [43] (RN18+Adapter)\n57.35\n94.96\n89.33\n81.42\n76.74\n82.01\n67.4\n92.18\n83.55\n55.75\n78.07\nP>M>F (DINO/IN1K, RN50)\n67.51\n85.91\n80.3\n81.67\n87.08\n72.84\n60.03\n94.69\n87.17\n58.92\n77.61\nP>M>F (DINO/IN1K, ViT-small)\n74.59\n91.79\n88.33\n91.02\n86.61\n79.23\n74.2\n94.12\n88.85\n62.59\n83.13\nP>M>F (DINO/IN1K, ViT-base)\n77.02\n91.76\n89.73\n92.94\n86.94\n80.2\n78.28\n95.79\n89.86\n64.97\n84.75\nIn-domain = ImageNet\nIn-domain\nOut-of-domain\nINet\nOmglot\nAcraft\nCUB\nDTD\nQDraw\nFungi\nFlower\nSign\nCOCO\nAvg\nProtoNet [65] (RN18)\n50.5\n59.98\n53.1\n68.79\n66.56\n48.96\n39.71\n85.27\n47.12\n41\n56.10\nALFA+FP-MAML [5] (RN12)\n52.8\n61.87\n63.43\n69.75\n70.78\n59.17\n41.49\n85.96\n60.78\n48.11\n61.41\nBOHB [58] (RN18)\n51.92\n67.57\n54.12\n70.69\n68.34\n50.33\n41.38\n87.34\n51.8\n48.03\n59.15\nCTX [24] (RN34)\n62.76\n82.21\n79.49\n80.63\n75.57\n72.68\n51.58\n95.34\n82.65\n59.9\n74.28\nP>M>F (DINO/IN1K, RN50)\n67.08\n75.33\n75.39\n72.08\n86.42\n66.79\n50.53\n94.14\n86.54\n58.2\n73.25\nP>M>F (DINO/IN1K, ViT-small)\n74.69\n80.68\n76.78\n85.04\n86.63\n71.25\n54.78\n94.57\n88.33\n62.57\n77.53\nP>M>F (DINO/IN1K, ViT-base)\n76.69\n81.42\n80.33\n84.38\n86.87\n75.43\n55.93\n95.14\n89.68\n65.01\n79.09\nTable 5. Meta-Dataset – Comparison with SOTA FSL algorithms.\nChestX\nISIC\nEuroSAT\nCropDisease\n5w5s\n5w20s\n5w50s\n5w5s\n5w20s\n5w50s\n5w5s\n5w20s\n5w50s\n5w5s\n5w20s\n5w50s\nProtoNet [59] (RN10)\n24.05\n28.21\n29.32\n39.57\n49.50\n51.99\n73.29\n82.27\n80.48\n79.72\n88.15\n90.81\nRelationNet [61] (RN10)\n22.96\n26.63\n28.45\n39.41\n41.77\n49.32\n61.31\n74.43\n74.91\n68.99\n80.45\n85.08\nMetaOptNet [42] (RN10)\n22.53\n25.53\n29.35\n36.28\n49.42\n54.80\n64.44\n79.19\n83.62\n68.41\n82.89\n91.76\nFinetune [33] (RN10)\n25.97\n31.32\n35.49\n48.11\n59.31\n66.48\n79.08\n87.64\n90.89\n89.25\n95.51\n97.68\nCHEF [1] (RN10)\n24.72\n29.71\n31.25\n41.26\n54.30\n60.86\n74.15\n83.31\n86.55\n86.87\n94.78\n96.77\nSTARTUP [52] (RN10)\n26.94\n33.19\n36.91\n47.22\n58.63\n64.16\n82.29\n89.26\n91.99\n93.02\n97.51\n98.45\nDeepCluster2 [14,27] (IN1K, RN50)\n26.51\n31.51\n34.17\n40.73\n49.91\n53.65\n88.39\n92.02\n93.07\n93.63\n96.63\n97.04\nP>M>F (DINO/IN1K, ResNet50)\n27.13\n31.57\n34.17\n43.78\n54.06\n57.86\n89.18\n93.08\n96.06\n95.06\n97.25\n97.77\nP>M>F (DINO/IN1K, ViT-small)\n27.27\n35.33\n41.39\n50.12\n65.78\n73.50\n85.98\n91.32\n95.40\n92.96\n98.12\n99.24\nTable 6. Broader study of cross-domain few-shot learning – Comparison with SOTA FSL algorithms.\ne.g., DINO > ProtoNet (PN) > Fine-tuning (FT). We next\ncompare our pipeline with prior state of the art. We empha-\nsize that our results are not directly comparable to much\nprior SOTA in terms of architecture and use of external\ndata. We draw this comparison to see how simple changes\n(such as upgrading feature backbone to a modern network\narchitecture and exploiting publicly available data for a large-\nscale pre-training) compare against 5 years of intensive re-\nsearch on FSL algorithms. The results for the single-domain\ncases, i.e., mini-ImageNet and CIFAR-FS, are summarized\nin Table 4, while the results for the cross-domain datasets,\ni.e., Meta-Dataset and Broader Study CDFSL, are shown\nin Table 5 and 6 respectively. From the results we can see\nthat our framework outperforms much the state of the art\nin both within-domain and cross-domain conditions despite\nbeing signiﬁcantly simpler than some sophisticated competi-\ntors. We remark that for the single source benchmarks in\nTable 4, a few competitors also used external data or Im-\nageNet pre-training as indicated. Meanwhile our hybrid\npipeline outperforms SOTA pure external self-supervision\n[14, 27] for CDFSL in Table 6. Our code is available at\nhttps://github.com/hushell/pmf_cvpr22.\n4.3. Discussion\nTaken together, the results show that our simple pipeline\nof exploiting available pre-training data and a modern ar-\nchitecture often outperforms sophisticated state of the art in\nfew-shot learning. This margin is increased using our pro-\nposed adaptive ﬁne-tuning mechanism in the meta-test stage.\nBased on these observations we make recommendations both\nfor practitioners and few-shot learning researchers.\nPractitioners: Increasing pre-training data size or simply\nusing a foundation model [10,15] and upgrading to modern\narchitectures is likely to be more productive (and much easier\nto implement) than keeping up with and implementing state\nof the art few-shot learning algorithms. Fine-tuning is likely\nto be important if the target few-shot task of interest is less\nsimilar to the pre-training and meta-training data.\n",
    "FSL researchers: Our results show that using external data\nand modern architectures is an easy and effective way to\nachieve strong FSL performance, and also that some SOTA\nmeta-learners fail to provide expected improvements in this\nregime. While external data violates deﬁnitions of the FSL\nproblem that insist on a speciﬁc limited meta-train set, we\nshould take this setting seriously to maintain practical rele-\nvance in the face of advancing self-supervision [15,28,39,53].\nIn particular, we recommend a new evaluation setting for\nall the standard FSL benchmarks, where pre-train data and\narchitecture are freely chosen and clearly reported. Few-shot\nmeta-learning methods are then evaluated on their ability to\nimprove on linear readout, ﬁne-tuning, or our PMF baseline\nfor the given external dataset and architecture.\n5. Conclusions\nWe advanced few-shot learning from the perspective of\npushing the limits of a simple pre-train + ProtoNet pipeline\nin terms of dataset, architecture and ﬁne-tuning strategy.\nWe showed that source dataset, and neural architecture are\ndominant factors in FSL performance. When there is a\ndomain shift between training and testing, we showed that\nﬁne-tuning the feature backbone with data augmentation is\nalso important. We veriﬁed that our simple pipelines achieve\nvery competitive performance in four FSL benchmarks.\nLimitations and future work\nThere are several limita-\ntions of our empirical study. We only scratched the surface\nof the impact of external data and correspondingly larger\narchitectures on FSL. Our renewed focus on external data\nemphasizes the need for algorithms from the FSL commu-\nnity [29,42,59] to be directly compared against algorithms\nfrom the self-supervised community [10, 17], or possibly\nsynergistically combined, as we attempt here. The hybrid\npipeline that we propose is obviously restricted to modalities\nwhere large external datasets already exist, and would re-\nquire signiﬁcant up-front investment in compute and energy\ncost where pre-trained foundation models do not already ex-\nist. Possible bias within foundation models is also a potential\nrisk [10]. Finally, while effective, our adaptive ﬁne-tuning\nstrategy, is rather computationally expensive at meta-test\ntime, and may be unsupported on embedded platforms with-\nout backpropagation. Feed-forward representation adapta-\ntion methods [56] may be important for future work.\nAcknowledgement\nWe thank the anonymous reviewers and meta-reviewers of\nCVPR2022 for their careful reading and thorough discussion\nof our manuscript. We also thank our colleagues at SAIC-\nCambridge, especially Gabor Gyorkei, Taekwon Jang and\nBrais Martinez, for their help and support.\nReferences\n[1] Thomas Adler, Johannes Brandstetter, Michael Widrich, An-\ndreas Mayr, David Kreil, Michael Kopp, Günter Klambauer,\nand Sepp Hochreiter. Cross-domain few-shot learning by\nrepresentation fusion. In arXiv, 2021. 8\n[2] Mohamed Afham, Salman Khan, Muhammad Haris Khan,\nMuzammal Naseer, and Fahad Shahbaz Khan. Rich semantics\nimprove few-shot learning. In BMVC, 2021. 7\n[3] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and\nMichael Auli. wav2vec 2.0: A framework for self-supervised\nlearning of speech representations. NeurIPS, 2020. 3\n[4] Sungyong Baik, Janghoon Choi, Heewon Kim, Dohee Cho,\nJaesik Min, and Kyoung Mu Lee. Meta-learning with task-\nadaptive loss function for few-shot learning. In ICCV, 2021.\n1\n[5] Sungyong Baik, Myungsub Choi, Janghoon Choi, Heewon\nKim, and Kyoung Mu Lee. Meta-learning with adaptive\nhyperparameters. In NeurIPS, 2020. 8\n[6] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training\nof image transformers. In ICLR, 2022. 4\n[7] Peyman Bateni, Jarred Barber, Jan-Willem van de Meent, and\nFrank Wood. Enhancing few-shot image classiﬁcation with\nunlabelled examples. In WACV, 2022. 7, 8\n[8] Luca Bertinetto, João F. Henriques, Philip H.S. Torr, and\nAndrea Vedaldi. Meta-learning with differentiable closed-\nform solvers. In ICLR, 2019. 5\n[9] Luca Bertinetto, Joao F. Henriques, Jack Valmadre, Philip\nH. S. Torr, and Andrea Vedaldi. Learning feed-forward one-\nshot learners. In NIPS, 2016. 1, 2\n[10] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Alt-\nman, Simran Arora, Sydney von Arx, Michael S Bernstein,\nJeannette Bohg, Antoine Bosselut, Emma Brunskill, et al.\nOn the opportunities and risks of foundation models. arXiv\npreprint arXiv:2108.07258, 2021. 2, 3, 6, 8, 9\n[11] Myriam Bontonou, Nicolas Farrugia, and Vincent Gripon.\nFew-shot learning for decoding brain signals.\nCoRR,\nabs/2010.12500, 2020. 2\n[12] Stevo Bozinovski. Reminder of the ﬁrst paper on transfer\nlearning in neural networks, 1976. Informatica, 44(3), 2020.\n3\n[13] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. NeurIPS, 2020. 3\n[14] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal,\nPiotr Bojanowski, and Armand Joulin. Unsupervised learn-\ning of visual features by contrasting cluster assignments. In\nNeurIPS, 2020. 8\n[15] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In ICCV,\n2021. 3, 4, 6, 8, 9\n[16] Da Chen, Yuefeng Chen, Yuhong Li, Feng Mao, Yuan He,\nand Hui Xue. Self-supervised learning for few-shot image\nclassiﬁcation. In ICASSP, 2021. 7\n",
    "[17] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geof-\nfrey Hinton. A simple framework for contrastive learning of\nvisual representations. In ICML, 2020. 3, 6, 9\n[18] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad\nNorouzi, and Geoffrey Hinton. Big self-supervised models\nare strong semi-supervised learners. In NeurIPS, 2020. 1, 3,\n6\n[19] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank\nWang, and Jia-Bin Huang. A closer look at few-shot classiﬁ-\ncation. ICLR, 2019. 1, 2, 7\n[20] Yinbo Chen, Zhuang Liu, Huijuan Xu, Trevor Darrell, and Xi-\naolong Wang. Meta-baseline: Exploring simple meta-learning\nfor few-shot learning. In ICCV, 2021. 1, 3, 7\n[21] Jia Deng, Wei Dong, R. Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. Imagenet: A large-scale hierarchical image database.\nIn CVPR, 2009. 2, 3\n[22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: pre-training of deep bidirectional trans-\nformers for language understanding. In ACL, 2019. 3, 4\n[23] Guneet Singh Dhillon, Pratik Chaudhari, Avinash Ravichan-\ndran, and Stefano Soatto. A baseline for few-shot image\nclassiﬁcation. In ICLR, 2020. 1, 3, 7\n[24] Carl Doersch, Ankush Gupta, and Andrew Zisserman.\nCrosstransformers: spatially-aware few-shot transfer.\nIn\nNeurIPS, 2021. 3, 5, 6, 8\n[25] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In ICLR, 2021. 1, 2, 4\n[26] Nikita Dvornik, Cordelia Schmid, and Julien Mairal. Select-\ning relevant features from a multi-domain representation for\nfew-shot classiﬁcation. In ECCV, 2020. 8\n[27] Linus Ericsson, Henry Gouk, and Timothy M Hospedales.\nHow well do self-supervised models transfer?\nIn CVPR,\n2021. 3, 6, 8\n[28] Linus Ericsson, Henry Gouk, Chen Change Loy, and Timo-\nthy M Hospedales. Self-supervised representation learning:\nIntroduction, advances and challenges. IEEE Signal Process-\ning Magazine, 2022. 3, 9\n[29] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-\nagnostic meta-learning for fast adaptation of deep networks.\nIn ICML, 2017. 1, 2, 3, 6, 9\n[30] Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick\nPérez, and Matthieu Cord. Boosting few-shot visual learning\nwith self-supervision. In ICCV, 2019. 3, 7\n[31] Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan\nMisra. Scaling and benchmarking self-supervised visual rep-\nresentation learning. In ICCV, 2019. 2, 3\n[32] Pei Guo. Overlap between imagenet and cub. 6\n[33] Yunhui Guo, Noel C Codella, Leonid Karlinsky, James V\nCodella, John R Smith, Kate Saenko, Tajana Rosing, and\nRogerio Feris. A broader study of cross-domain few-shot\nlearning. In ECCV, 2020. 3, 4, 5, 8\n[34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR, 2016.\n4\n[35] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and\nAmos Storkey. Meta-learning in neural networks: A sur-\nvey. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 2021. 2\n[36] Shell Xu Hu, Pablo Moreno, Yang Xiao, Xi Shen, Guillaume\nObozinski, Neil Lawrence, and Andreas Damianou. Empiri-\ncal bayes transductive meta-learning with synthetic gradients.\nIn ICLR, 2020. 7\n[37] Yuqing Hu, Vincent Gripon, and Stéphane Pateux. Leveraging\nthe feature distribution in transfer-based few-shot learning. In\nICANN, 2021. 7\n[38] Kai Huang, Jie Geng, Wen Jiang, Xinyang Deng, and Zhe Xu.\nPseudo-loss conﬁdence metric for semi-supervised few-shot\nlearning. In ICCV, 2021. 6, 7\n[39] L. Jing and Y. Tian. Self-supervised visual feature learning\nwith deep neural networks: A survey. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 2021. 3, 9\n[40] Arman Kazemi, Shubham Sahay, Ayush Saxena, Moham-\nmad Mehdi Shariﬁ, Michael Niemier, and X. Sharon Hu. A\nﬂash-based multi-bit content-addressable memory with eu-\nclidean squared distance. In IEEE/ACM International Sympo-\nsium on Low Power Electronics and Design, 2021. 2\n[41] Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B.\nTenenbaum. Human-level concept learning through prob-\nabilistic program induction. Science, 2015. 1\n[42] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and\nStefano Soatto. Meta-learning with differentiable convex\noptimization. In CVPR, 2019. 1, 2, 6, 7, 8, 9\n[43] Wei-Hong Li, Xialei Liu, and Hakan Bilen. Improving task\nadaptation for cross-domain few-shot learning. arXiv preprint\narXiv:2107.00358, 2021. 4, 7, 8\n[44] Wei-Hong Li, Xialei Liu, and Hakan Bilen. Universal repre-\nsentation learning from multiple domains for few-shot classi-\nﬁcation. In ICCV, 2021. 8\n[45] Xinzhe Li, Qianru Sun, Yaoyao Liu, Qin Zhou, Shibao Zheng,\nTat-Seng Chua, and Bernt Schiele. Learning to self-train for\nsemi-supervised few-shot classiﬁcation. NeurIPS, 2019. 6, 7\n[46] Yann Lifchitz, Yannis Avrithis, and Sylvaine Picard. Few-\nshot few-shot learning and the role of spatial attention. In\n2020 25th International Conference on Pattern Recognition\n(ICPR), pages 2693–2700. IEEE, 2021. 7\n[47] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean conference on computer vision, pages 740–755.\nSpringer, 2014. 6\n[48] Lu Liu, William Hamilton, Guodong Long, Jing Jiang, and\nHugo Larochelle. A universal representation transformer\nlayer for few-shot image classiﬁcation. In ICLR, 2021. 8\n[49] Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho\nYang, Sung Ju Hwang, and Yi Yang. Learning to propagate la-\nbels: Transductive propagation network for few-shot learning.\nIn ICLR, 2019. 6\n[50] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew\nBlaschko, and Andrea Vedaldi.\nFine-grained visual clas-\nsiﬁcation of aircraft. arXiv preprint arXiv:1306.5151, 2013.\n6\n",
    "[51] Puneet Mangla, Nupur Kumari, Abhishek Sinha, Mayank\nSingh, Balaji Krishnamurthy, and Vineeth N Balasubrama-\nnian. Charting the right manifold: Manifold mixup for few-\nshot learning. In WACV, 2020. 1, 2\n[52] Cheng Perng Phoo and Bharath Hariharan. Self-training for\nfew-shot transfer across extreme task differences. In ICLR,\n2021. 8\n[53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. Learning transferable visual\nmodels from natural language supervision. In ICML, 2021. 1,\n4, 9\n[54] Sachin Ravi and Hugo Larochelle. Optimization as a model\nfor few-shot learning. In ICLR, 2017. 1, 2\n[55] Mengye Ren, Eleni Triantaﬁllou, Jake Snell Sachin Ravi,\nKevin Swersky, Joshua B. Tenenbaum, Hugo Larochelle, and\nRichard S. Zemel. Meta-learning for semi-supervised few-\nshot classiﬁcation. In ICLR, 2018. 1, 6\n[56] James Requeima, Jonathan Gordon, John Bronskill, Sebastian\nNowozin, and Richard E. Turner. Fast and ﬂexible multi-task\nclassiﬁcation using conditional neural adaptive processes. In\nNeurIPS, 2020. 8, 9\n[57] Pau Rodríguez, Issam Laradji, Alexandre Drouin, and Alexan-\ndre Lacoste. Embedding propagation: Smoother manifold for\nfew-shot classiﬁcation. In ECCV, 2020. 7\n[58] Tonmoy Saikia, Thomas Brox, and Cordelia Schmid. Op-\ntimized generic feature learning for few-shot classiﬁcation\nacross domains. In arXiv, 2020. 8\n[59] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical\nnetworks for few-shot learning. In NIPS, 2017. 1, 2, 3, 4, 6,\n7, 8, 9\n[60] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav\nGupta. Revisiting unreasonable effectiveness of data in deep\nlearning era. In ICCV, 2017. 2, 3\n[61] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H. S.\nTorr, and Timothy M. Hospedales. Learning to compare:\nRelation network for few-shot learning. In CVPR, 2018. 1, 8\n[62] Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin\nElizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-\nJia Li. Yfcc100m: The new data in multimedia research.\nCommun. ACM, 59(2):64–73, jan 2016. 2\n[63] Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B. Tenen-\nbaum, and Phillip Isola. Rethinking few-shot image classiﬁ-\ncation: a good embedding is all you need? In ECCV, 2020.\n1, 2\n[64] Eleni Triantaﬁllou, Hugo Larochelle, Richard Zemel, and\nVincent Dumoulin. Learning a universal template for few-\nshot dataset generalization. In ICML, 2021. 8\n[65] Eleni Triantaﬁllou, Tyler Zhu, Vincent Dumoulin, Pascal\nLamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles\nGelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo\nLarochelle. Meta-dataset: A dataset of datasets for learning\nto learn from few examples. In ICLR, 2020. 2, 3, 5, 7, 8\n[66] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray\nKavukcuoglu, and Daan Wierstra. Matching networks for one\nshot learning. In NeurIPS, 2016. 1, 3, 5\n[67] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona,\nand Serge Belongie. The caltech-ucsd birds-200-2011 dataset.\nTech. Report, 2011. 6\n[68] Yan Wang, Wei-Lun Chao, Kilian Q. Weinberger, and Laurens\nvan der Maaten. Simpleshot: Revisiting nearest-neighbor\nclassiﬁcation for few-shot learning, 2019. 1, 2\n[69] Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M\nNi. Generalizing from a few examples: A survey on few-\nshot learning. ACM Computing Surveys (CSUR), 53(3):1–34,\n2020. 1, 2\n[70] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.\nHow transferable are features in deep neural networks? In\nNIPS, 2014. 2, 3\n[71] Jianhong Zhang, Manli Zhang, Zhiwu Lu, and Tao Xiang.\nAdargcn: adaptive aggregation gcn for few-shot learning. In\nProceedings of the IEEE/CVF Winter Conference on Applica-\ntions of Computer Vision, pages 3482–3491, 2021. 7\n[72] X. Zhang, D. Meng, H. Gouk, and T. Hospedales. Shallow\nbayesian meta learning for real-world few-shot recognition.\nIn ICCV, 2021. 1, 2, 6\n",
    "Pushing the Limits of Simple Pipelines for Few-Shot Learning:\nSupplemental Material\nIn this supplemental material, we present:\n• In Section 1, we include additional results for Table 1 in the main paper.\n• In Section 2, we include additional results for Table 1 and Table 4 in the main paper.\n• In Section 3, we investigate the impact of the hyper-parameters for the ﬁne-tuning phase.\n• In Section 4, we show the T-SNE plots before and after ProtoNet meta-training.\n1. Additional results for Meta-Dataset\nIn this section, we show a complete view of the results presented in Table 1 in the main paper, including the outcomes\nof different pre-training methods (see Table 1), the outcomes of meta-training on ImageNet domain (see Table 2), and the\noutcomes of meta-training on eight pre-speciﬁed domains (see Table 3).\nAs indicated in the main paper, our pipeline is named in a form of “P > M > F (backbone)”, where “P”, “M” and “F” are\ntaken from the ﬁrst letters of pre-training, meta-training and ﬁne-tuning respectively. In this section, we only examine the\npre-training and backbone architecture parts with meta-training ﬁxed to ProtoNet. As an example, in Table 2, we use “DINO >\nPN (ViT-small)” to denote the pipeline that uses DINO pre-training, ProtoNet meta-training with backbone architecture being\nViT-small.\nTo clarify the shorten notations in Table 1, Table 2 and Table 3, we make a list here:\n• DINO: self-distillation pre-training on ImageNet-1k dataset by [2].\n• BEiT: BERT pre-training on ImageNet-21k dataset by [1].\n• CLIP: Contrastive language-image pre-training on YFCC100M dataset by [3].\n• Sup21k: Supervised pre-training on ImageNet-21k dataset.\n• Sup1k: Supervised pre-training on ImageNet-1k dataset.\n• BEiT + Sup21k: BERT unsupervised pre-training ﬁrst on ImageNet-21k dataset and then using the labels of ImageNet-\n21k to ﬁne-tune the model.\n2. Additional results for miniImageNet and CIFAR-FS\nWe also evaluate different pre-training methods and backbones on miniImageNet and CIFAR-FS, which is shown in Table 4.\nWe do not include some of the results to the main paper because supervised pre-training on ImageNet is only useful to check\nthe upper bound performance.\n3. Ablation study on ﬁne-tuning’s hyper-parameters\nThere are three hyper-parameters for the ﬁne-tuning stage: the learning rate, the number of gradient descent steps and the\nprobability of switching on data augmentation for the support set. We show in Figure 1 that the dominant hyper-parameter is\nthe learning rate. From the results, we also see that the higher the probability of switching on data augmentation the better,\nwhile 50 gradient steps give relatively good performance with the right learning rate. Therefore, we ﬁx the probability to 0.9\nand let the numbers of steps to be 50 in the ﬁne-tuning phase.\n1\narXiv:2204.07305v1  [cs.CV]  15 Apr 2022\n",
    "INet\nOmglot\nAcraft\nCUB\nDTD\nQDraw\nFungi\nFlower\nSign\nCOCO\nAvg\nDINO (ViT-small)\n73.48\n54.33\n62.17\n85.37\n83.67\n60.59\n56.26\n94.45\n53.7\n54.58\n67.86\nDINO (ViT-base)\n74.85\n59.44\n55.36\n80.08\n84\n59.61\n56.65\n94.84\n51.81\n57.1\n67.374\nBEiT (ViT-base)\n17.12\n23.96\n17.21\n18.59\n39.79\n23.89\n13.69\n45.81\n16.16\n16.36\n23.258\nCLIP (ViT-base)\n60.66\n62.12\n54.08\n80.26\n76.51\n62.90\n30.76\n68.43\n47.33\n41.95\n58.5\nDINO (ResNet50)\n64.13\n52.51\n57.02\n62.63\n84.5\n60.78\n50.41\n92.18\n58.27\n55.43\n63.786\nCLIP (ResNet50)\n51.67\n44.16\n44.18\n70.2\n70.64\n47.88\n34.13\n87.97\n39.59\n41.63\n53.205\nSup21k (ViT-base)\n67.00\n37.02\n47.72\n82.9\n79.77\n52.25\n41.98\n95.7\n46.22\n53.46\n60.402\nBEiT + Sup21k (ViT-base)\n33.85\n23.95\n33.92\n52.07\n63.79\n32.60\n28.19\n67.3\n27.18\n29.65\n39.25\nSup1k (ViT-base)\n89.1\n60.71\n55.36\n79.8\n79.75\n61.28\n47.45\n88.44\n56.3\n57.20\n67.539\nSup1k (ResNet50)\n76.22\n47.31\n55.75\n76.40\n80.40\n51.26\n43.42\n85.48\n50.46\n57.10\n62.38\nTable 1. Pre-training results on Meta-Dataset – Comparison of different pre-training methods and backbone architectures.\nIn-domain\nOut-of-domain\nINet\nOmglot\nAcraft\nCUB\nDTD\nQDraw\nFungi\nFlower\nSign\nCOCO\nAvg\nDINO > PN (ViT-small)\n74.69\n56.91\n60.5\n85.04\n84.21\n61.54\n54.78\n94.57\n54.21\n57.35\n68.38\nDINO > PN (ViT-base)\n76.69\n62.2\n54.76\n81.58\n84.48\n60.64\n55.93\n95.14\n56.81\n60.27\n68.85\nCLIP > PN (ViT-base)\n76.03\n59\n65.75\n90.2\n83.08\n65.45\n53.2\n96.35\n58.65\n61.2\n70.891\nDINO > PN (ResNet50)\n67.08\n49.21\n58.46\n72.08\n85.01\n59.2\n50.53\n89.91\n55.44\n53.94\n64.086\nCLIP > PN (ResNet50)\n69.41\n60.72\n57.53\n83.66\n80.03\n55.58\n50.07\n93.39\n48.56\n50.14\n64.909\nSup21k > PN (ViT-base)\n85.88\n39.72\n52.03\n94.54\n83.42\n54.58\n57.06\n99.01\n47.74\n69.02\n68.3\nBEiT+Sup21k > PN (ViT-base)\n84.39\n60.54\n74.04\n95.66\n86.14\n65.24\n64.25\n99.19\n63.02\n69.91\n76.238\nSup1k > PN (ViT-base)\n90.48\n62.96\n54.89\n78.88\n80.02\n61.81\n45.52\n88.56\n55.61\n59.12\n67.785\nTable 2. Meta-training results on Meta-Dataset (ImageNet only) – Comparison of different pre-training methods and backbone architec-\ntures.\nIn-domain\nOut-of-domain\nINet\nOmglot\nAcraft\nCUB\nDTD\nQDraw\nFungi\nFlower\nSign\nCOCO\nAvg\nDINO > PN (ViT-small)\n73.54\n91.79\n88.33\n91.02\n81.64\n79.23\n74.2\n94.12\n54.37\n57.04\n78.528\nDINO > PN (ViT-base)\n73.55\n91.54\n89.73\n92.94\n81.52\n80.2\n78.28\n94.53\n53.65\n59.13\n79.507\nCLIP > PN (ViT-base)\n74.76\n92.26\n91.42\n93.55\n80.97\n80.8\n79.13\n95.64\n54.52\n56.8\n79.985\nDINO > PN (ResNet50)\n63.7\n85.91\n80.3\n81.67\n82.69\n72.84\n60.03\n91.75\n54.26\n50.67\n72.382\nCLIP > PN (ResNet50)\n64.86\n92.09\n89.19\n89.17\n71.67\n78.71\n76.15\n91.25\n51.1\n45.88\n75.007\nSup21k > PN (ViT-base)\n84.86\n85.71\n83.77\n95.89\n85.1\n78.47\n74\n99.17\n59.86\n67.57\n81.44\nBEiT+Sup21k > PN (ViT-base)\n81.96\n94.19\n91.62\n93.76\n81.3\n83.48\n81.76\n98.84\n58.83\n61.81\n82.755\nSup1k > PN (ViT-small)\n83.87\n91.22\n87.9\n89.2\n78.11\n78.7\n70.33\n94\n56.24\n57.16\n78.673\nSup1k > PN (ViT-base)\n89.75\n93.48\n91.15\n92.48\n78.52\n80.65\n75.97\n95.78\n53.47\n55.89\n80.714\nSup1k > PN (ResNet50)\n68.04\n86.17\n80.72\n80.48\n71.65\n70.78\n59.58\n84.33\n50.06\n50.29\n70.21\nNone > PN (ViT-small)\n37.25\n74.14\n45.25\n49.66\n61.49\n70.24\n43.23\n72.03\n39.33\n35.43\n52.805\nNone > PN (ResNet50)\n40.74\n90.67\n80.67\n68.88\n62.4\n75.96\n55.72\n75.37\n43.11\n35.49\n62.901\nTable 3. Meta-training results on Meta-Dataset – Comparison of different pre-training methods and backbone architectures.\n4. T-SNE plots: before and after meta-training\nBy using T-SNE visualization, We identify that the feature representation of DINO pre-training is already of high quality in\nmultiple domains. Three examples are shown in Figure 2, Figure 3 and Figure 4. In general, many semantic clusters have\nalready emerged, even though these domains where the clusters are sitting are not necessarily similar to ImageNet. This gives\na very good initialization to ProtoNet so that it can reﬁne the clusters to be much tighter. While the situation would be quite\ndifferent if we were training the ProtoNet from scratch, which are conﬁrmed by the no-pre-training results in Table 3. This can\nbe explained in the sense of K-means clustering, where a good initialization is always desired.\n",
    "miniImageNet\nCIFAR-FS\n5w1s\n5w5s\n5w1s\n5w5s\nDINO > PN (ViT-small)\n93.1\n98.0\n81.1\n92.5\nDINO > PN (ViT-base)\n95.3\n98.4\n84.3\n92.2\nCLIP > PN (ViT-base)\n93.1\n98.1\n85.3\n93.2\nDINO > PN (ResNet50)\n79.2\n92.0\n73.7\n84.0\nCLIP > PN (ResNet50)\n78.9\n92.2\n71.4\n82.6\nSup21k > PN (ViT-base)\n97.2\n99.2\n92.3\n96.7\nBEiT+Sup21k > PN (ViT-base)\n96.6\n99\n93.8\n97.5\nSup1k > PN (ViT-small)\n97.7\n99.4\n86.2\n93.6\nSup1k > PN (ViT-base)\n99.2\n99.8\n88.2\n94.3\nSup1k > PN (ResNet50)\n91.7\n97.4\n77\n87.6\nNone > PN (ViT-small)\n36.5\n49.1\n45.9\n59.8\nNone > PN (ResNet50)\n46.1\n60.3\n54.1\n68.4\nTable 4. miniImageNet & CIFAR-FS – Comparison of different pre-training methods and backbone architectures.\nProbability of switching on data augmentation\nAccuracy\n75\n80\n85\n90\n0.5\n0.7\n0.9\nsteps=50\nsteps=100\nsteps=200\nTraffic sign, lr = 0.001\nProbability of switching on data augmentation\nAccuracy\n75\n80\n85\n90\n0.5\n0.7\n0.9\nsteps=50\nsteps=100\nsteps=200\nTraffic sign, lr = 0.01\nProbability of switching on data augmentation\nAccuracy\n50\n55\n60\n65\n0.5\n0.7\n0.9\nsteps=50\nsteps=100\nsteps=200\nMSCOCO, lr = 0.001\nProbability of switching on data augmentation\nAccuracy\n50\n55\n60\n65\n0.5\n0.7\n0.9\nsteps=50\nsteps=100\nsteps=200\nMSCOCO, lr = 0.01\nFigure 1. Ablation study of ﬁne-tuning’s hyper-parameters – The experiments are done in the validation set of the trafﬁc sign domain\nand the MSCOCO domain with learning rate ﬁxed to either 0.001 or 0.01.\n",
    "0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n11\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1 1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n11\n1\n1\n1\n11\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n22\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n3\n3\n3\n3\n3\n3\n3\n3\n3\n33\n3 3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n55\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5 5\n5\n5\n55\n5\n555\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n6\n6\n6\n6\n6\n6\n66\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n66\n6\n6\n6\n6\n6\n66\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n88\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n88\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9 9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10 10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n11\n11\n1111\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n1212\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n1212\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n1313\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n1313\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13 13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n1313\n13\n13\n13\n13\n13 13\n13\n13\n13\n13\n13\n13\n13\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\nPre-training\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n00\n0\n0\n0\n0 0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n11\n1\n1\n1\n1\n1\n1\n1\n1\n1 1\n1\n1\n1\n1\n1\n11\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1 1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2 2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n22\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n3\n3 3\n3\n3\n3\n3\n3\n3\n3\n3\n3 3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n33\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n4\n4\n4\n4\n4\n4\n4\n4 4\n44\n4\n4\n4\n4\n4\n4\n4\n4\n44\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n44\n44\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n44\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n55\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n55\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n55\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n55\n5\n5\n5\n5\n5\n5\n5\n5\n5\n55\n5\n5\n5\n5\n5\n5\n5\n5\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6 6\n6\n6\n6\n6\n6\n66\n6\n6\n6\n6\n6\n6\n6\n6\n66\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n66\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n66\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n7\n77\n7\n7\n7\n7 7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7 7\n7\n7\n7 7\n7\n7\n7\n7\n7\n7\n77\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8 8\n8\n8\n8\n8 8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8 8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n99\n99\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n99\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9 9\n9\n9\n9\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n1010\n1010\n1010\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n1010\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n1010\n10\n10\n11\n11\n11\n11\n1111\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n1111\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11 11\n11 11\n11\n11 11\n11\n11\n11\n11\n11\n11\n11\n1111\n11 11\n1111\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n1111\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n1111\n11\n11\n11\n11\n11\n11\n11\n11\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n1212\n1212\n12\n12\n12\n1212\n12 12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n1212\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n1313\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13 13\n13\n13\n13\n13\n1313\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n1313\n13\n13\n13\n13\n13\n13\n1313\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n1313\n13\n13\n13\n13\n13\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n1414\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n1414\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n1414\n14\n14\n14\n14\n14\n14\n14\n14 14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14 14\n14\n14\n14\n14\n14\n141414\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\nMeta-training\nFigure 2. Aircraft domain\n",
    "0\n0\n0\n0\n00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n00\n0\n0\n0\n0\n0\n0\n0\n000\n0\n0\n0 0\n0\n00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1 1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n11\n1\n1\n2\n22\n2\n2\n2\n2\n22\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n22\n2\n2\n2\n2\n2\n2\n22\n2\n2\n2\n2\n2\n2\n2\n2\n22\n2\n2\n2\n22\n2\n2\n2\n2\n2\n2\n3\n33\n3\n3\n33\n3\n33\n3\n3\n3\n3\n3\n3 3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n33\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3 3\n3 3\n3\n3\n4\n4\n4\n4\n44\n4\n4\n4\n4\n4\n4\n4\n4\n4\n44\n4\n4\n44\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n44\n44\n4\n4\n4\n4\n4\n4\n4\n4\n44\n4\n4\n4\n4\n4\n4\n4 4\n4\n4\n5\n5\n5\n5\n55\n55 5\n5\n5 5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5 5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n55\n5 5\n5\n5\n5\n5 5\n5\n5\n5\n5\n5\n5\n5\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6 6\n6\n6\n6\n6\n6 6\n66\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n66\n6\n6\n6\n6\n6\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n77\n7\n7\n7\n77\n7\n7\n7\n7\n7\n7 7\n7\n7\n7\n7\n7\n7\n7\n777\n7\n7\n7\n7\n7\n7\n7\n7\n77\n7\n7 7\n7\n7\n7\n7\n7\n7\n7\n7 7\n7\n7\n7\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n88\n8\n8\n8\n8\n8\n8 8\n8\n8\n8888\n8\n8\n8\n8\n8\n88\n8\n8\n8\n8\n8\n8\n8 8\n88\n8\n8\n8\n8\n8\n88\n88\n8\n88\n8\n8\n88\n8\n9 9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n99\n9\n9\n99\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n99\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n10\n10\n10\n10\n10\n10\n10 10\n10\n10\n1010\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n1010\n1010\n10\n10\n10\n1010\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n11\n11\n11 11\n11\n1111\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n1111\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11 11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12 12\n12\n12\n12\n12\n12\n12\n12\n1212\n12\n12\n12\n12\n12\n12\n12\n12\n12\n1212\n12\n12\n1212\n12\n12\n13\n13\n13\n13\n13\n13\n13\n13\n13\n131313\n13 13\n13\n13\n13\n13\n13\n13\n13\n131313\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13 13\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n1414\n14\n14\n14\n14\n14\n14\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15 15\n15\n15\n15\n15\n15\n15\n15\n1515\n15\n15\n15\n15\n15\n15\n15\n15\n15 15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15 15\n15\n15\n15\n1515\n15\n15\n15\n15\n15\n15\n15\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n1616\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n1616\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17 17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n18\n18\n18\n18\n18\n1818\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18 18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18 18\n18\n181818\n18\n18\n18\n18\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n20\n2020\n20\n20\n20\n20\n20\n20\n20\n2020\n20\n20\n20\n20\n2020\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n2020\n20\n20\n20\n20\n20\n2020\n20\n20\n20\n20\n20\n20 20\n20\n20\n20\n20\n20\n20 20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n2121\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21 21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n2222\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n2323\n2323\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n2424\n24\n2424\n24\n24\n24\n24\n25\n25\n25\n2525\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n2525\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n2525\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25 25\n25\n25\n25\n25\n25\n25\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n2626\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n27\n2727\n2727\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27 27\n27\n28\n28\n28\n28\n28\n28\n28\n2828\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28 28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n2828\n28\n28\n28\n28\n28\n28\n28\n28\n28\n29\n29\n29\n29 29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n2929\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n2929\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\nPre-training\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0 0\n0\n0\n0\n0\n0\n0\n0\n0\n00\n0\n0 00\n0\n0\n0\n000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n00\n0\n0\n00\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n11\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1 1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n11\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n11\n1\n1\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n22\n2\n22\n2\n22\n2\n22\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n22\n2\n2\n2\n2\n22\n22\n2\n22\n2\n2\n2\n2\n2\n3 3\n3\n3 3\n3\n3\n3\n33\n3\n33\n3\n3\n3 3\n3\n3\n3 3\n3\n3\n3\n33\n3\n33 3\n3\n3\n3\n3\n3\n3\n3\n3\n33\n3\n3\n3\n3\n3\n3 3\n3\n3\n3\n3\n3\n3\n3\n33\n3\n3\n3\n3\n4 4\n4\n44\n44\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n44\n4\n4\n4 4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n444\n4\n4\n4\n4 4\n4\n4\n4\n4\n4\n4\n44\n44\n4 4\n44\n5\n5\n5\n5\n5\n55\n5\n55\n5\n5\n5\n5\n5\n5\n55\n5\n5\n5\n5\n5\n5\n5\n5 5\n55\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n55\n5\n5\n5\n555\n5\n5\n6\n6\n6\n66\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n66\n6\n6\n6\n6\n6\n6\n6\n6\n6\n66\n66\n66\n6\n6\n6\n6\n66\n6\n6\n6\n6\n6\n6\n66\n6\n6\n66\n6\n6\n6\n6\n6\n6\n6\n66\n7\n7\n7\n7\n7\n7\n7\n77\n7\n7\n77\n7\n7\n7\n77\n77\n7\n7 7\n7\n77\n7\n7\n7\n7\n7\n7\n777\n7\n7\n7\n7\n7\n7\n7\n7\n77777\n7\n7\n7\n7\n7\n7\n7\n777\n7\n7\n88\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n88\n8\n8\n8 8\n8\n8\n8\n8\n8\n88\n8\n88\n88\n8\n888\n8\n8\n8\n88\n8\n8\n88\n8\n88\n8\n88\n88\n9\n99\n9\n9\n9\n9\n9\n9 9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n99\n9\n9\n99\n9\n9\n9\n99\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n10\n10\n10\n1010\n10\n1010\n10\n10\n10\n10\n10\n1010\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n1010\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n1010\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10 10\n10\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n1111\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n1111\n11\n11\n1111\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n1212\n12\n12\n12\n12\n12\n12\n13\n13\n13\n13\n13\n13\n13\n13\n13\n1313\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n1313\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n1313\n13\n13\n13\n13\n1313\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14 14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n1414\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14 14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n15\n15 15\n15\n15\n15\n1515\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n1515\n15\n15\n1515\n1515\n15\n15\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n1616\n16\n16\n16\n16\n16\n16\n1616\n16\n16\n16\n1616\n16\n16\n16\n16\n16 16\n16\n1616\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n1717\n17\n17\n17\n17\n171717\n17\n17\n17\n17\n17\n17\n17\n1717\n1717\n17\n17 17\n17\n17 17\n17\n17\n17\n17 17\n17\n17\n17\n17\n17\n17\n17\n1717\n17\n1717\n17\n17\n17\n17\n17\n18\n18\n18\n18\n1818\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n1818\n18\n18\n18\n18\n18\n18\n18\n18\n18\n1818\n18\n1818\n18\n18\n18\n18\n18\n18\n18\n1818\n1818\n18\n18\n18\n18\n1818\n18\n18\n18\n18\n18\n18\n18\n18\n18\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n1919\n19\n19\n1919\n19\n19\n19\n19\n19\n19\n19\n19\n19\n1919\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n2020\n20\n20\n20\n20\n2020\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n21\n21\n2121\n21\n21\n21\n21\n2121\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21 21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21 21\n21\n21\n21\n2121\n21\n21\n21\n21\n21 21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n2222\n22\n22\n2222\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n2222\n22\n22\n22\n22\n23\n23\n23\n23\n2323\n23\n23\n2323\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n2324\n24\n24\n24\n24\n24\n2424\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24 24\n24\n24\n24\n24\n24 24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n2424\n24\n24\n24\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n2525\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n2525\n2525\n25\n25\n25\n25\n25\n25\n25\n25\n252525\n25\n25\n25\n25\n25\n2525\n25\n25\n25\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n2626\n26\n26\n26\n26\n26\n26\n2626\n26\n26\n26\n26 26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n2727\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n2727\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27 27\n27\n27\n27\n27\n27\n27\n2727\n27\n27\n2727\n27\n27\n2727\n27\n2727\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n2727\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28 28\n28\n28\n28\n28\n28\n28\n28\n28\n2828\n28\n28\n28\n28\n28\n28\n28\n28\n2828\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28 28\n28\n2929\n29\n29\n29 29\n29\n29\n2929\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n2929\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n2929\n29\n29\nMeta-training\nFigure 3. CUB domain\n",
    "00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n00\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n11\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n2 2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2 2\n2\n2\n2\n2\n2\n3\n3\n3\n3\n3\n3\n33\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n55\n5\n5\n5\n5 5\n5\n55\n5\n5\n55\n5\n55\n5\n5\n5\n66\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6 6\n6\n6\n7 7\n7\n7\n7\n7 7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n8\n8\n8 8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n88\n8\n8\n8\n8\n8\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n99\n9\n10\n10\n10\n10\n10\n10\n1010\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n1111\n11\n11\n11\n11\n11\n11\n11\n11\n11\n1111\n11\n11\n11\n11\n11\n11 11\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n1414\n1414\n14\n14\n14\n14\n15\n15\n15\n15\n15\n1515\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n16\n16\n16\n16\n16\n16\n1616\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23 23\n23\n23\n24\n24\n2424\n24\n24\n24\n24\n2424\n24\n24\n24\n24\n24\n24\n2424\n24\n24\n25\n25\n25\n25\n25\n25\n25\n25\n25\n2525\n25\n25\n25\n25\n2525\n25\n25\n25\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n2727\n27\n2727\n27\n27\n27\n27\n27\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n2828\n28\n28\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n2929\n29\n2929\n29\n29\n29\n29\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n31\n31\n31\n31 31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n33\n33\n33\n33\n33\n33\n33\n33\n33\n33\n33\n33\n33\n33\n33\n33\n33\n33\n33\n33\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35 35\n35\n35\n35\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n37\n37\n37\n37\n37\n37\n37\n37\n37\n37\n37\n37\n37\n37\n37\n37\n37\n37\n37\n37\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n3838\n38\n38\n38\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n3939\n39\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n41\n41\n41\n41\n41\n41\n41\n41\n41\n41\n41\n41\n41\n4141\n41\n41\n41\n41\n41\n42\n42\n42\n42\n42 4242\n42\n42\n42\n42\n42\n42\n42\n42\n42\n42\n42\n4242\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n4343\n43\n43\n43\n43\n43\n44\n44\n44\n44\n44\n4444\n44\n44\n44\n44\n44\n44\n44\n44\n4444\n44\n44\n44\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n46\n46\n46\n46\n46\n46\n46\n46\n46\n46\n46\n46\n4646\n46\n46\n46\n46\n46\n46\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n4747\n47\n47\n47\n47\n47\n47\n47\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n50\n50\n50\n5050\n50\n50\n50\n50\n50\n50\n50\n50\n50\n50\n50\n50\n50\n50\n50\n51\n51\n51\n51\n51\n51\n51\n51\n51\n51\n51\n51\n51\n51\n51\n51\n51\n51\n51\n51\n52\n52\n52\n5252\n52\n5252\n5252\n52\n52\n5252\n52\n52\n52\n52\n52\n52\n53\n53\n53\n53 53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n56\n56\n56\n56\n56\n56\n56\n56\n56\n56\n56\n56\n56\n56\n56\n56\n56\n56\n56\n56\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n5757\n57\n57\n57\n57\n58\n58\n58\n58 58\n58\n58\n58\n58\n58\n58\n58\n58\n58\n58\n58\n58\n58\n58\n58\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n60\n60\n60\n60\n60\n60\n60\n60\n60\n60\n60\n6060\n60\n60\n60\n60\n60\n60\n60\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n62\n62 62\n62\n62\n62\n62\n62\n62\n62\n62\n62\n62\n62\n62\n62\n62\n62\n62\n62\n63\n63\n63\n63\n63\n63\n63\n63\n6363\n6363\n63\n63\n63\n63\n63\n63\n63\n63\n64\n64\n64\n64\n64\n64\n6464\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n65\n65\n65\n65\n65\n65\n65\n65\n65\n65\n65\n65\n65\n65\n65\n65\n65\n65\n65\n65\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n67\n67\n67\n67\n67\n67\n67\n67\n67\n67\n67\n67\n67\n67\n67\n67\n67\n67\n67\n67\n68\n68\n68\n68\n68\n68\n68\n68\n68\n68\n68\n68\n68\n68\n68\n68\n68\n68\n68\n68\n69\n69\n69\n69\n69\n69\n69\n69\n69\n69\n69\n69\n69\n69\n69\n69\n69\n69\n69\n69\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n71\n71\n71\n71\n71\n71\n71\n71\n71\n71\n71\n7171\n7171\n71\n71\n71\n71\n71\n72\n72\n72\n72\n72\n72\n72\n72\n72\n72\n7272\n72\n72\n72\n72\n72\n72\n72\n72\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n74\n74\n74\n74\n74\n74\n74\n74\n74\n74\n74\n74\n74\n74\n74\n74\n74\n74\n74\n74\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n7575\n75\n75\n75\n75\n75\n76\n76\n76\n76\n76\n76\n76\n76\n76\n76\n76\n76\n76\n76\n76\n76\n76\n76\n76\n76\n77\n77\n7777\n77\n77\n77\n77\n77\n7777\n7777\n77\n77\n77\n77\n7777\n77\n78\n78\n78\n78\n78\n78\n78\n78\n78\n78\n78\n78\n78\n78\n78\n78\n78\n78\n78\n78\n79\n79\n79\n79\n79\n79\n797979\n79\n79\n79\n79\n79\n79\n79\n7979\n79\n79\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\nPre-training\n0\n0\n0\n0\n00\n0\n0 0\n00 0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n11\n1\n1\n1 1\n1\n1\n1 1\n1 1\n1\n1\n1\n11\n22 2\n2\n2\n2\n2\n2\n22\n222\n22 2 2\n222\n3\n3\n3\n3\n333\n3\n3\n33\n3\n3\n3\n3\n3\n3\n3\n3\n3\n44\n444\n4\n44\n4\n4\n444\n444\n4\n4\n4\n4\n55\n55\n555\n5555\n5\n5\n55\n55\n55\n5\n6\n6\n6\n66\n666\n6\n6\n6\n6 6\n6\n6\n6\n6\n666\n7\n7\n7\n7\n7\n7\n77\n7\n7\n777\n77 7\n7\n77\n7\n8\n8\n8\n8 8\n8\n8\n8\n8\n88\n8\n88\n88\n8\n8\n88\n9\n99\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n99\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n1010\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n12\n12\n12\n12\n12\n12\n1212\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n1313\n13\n13\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n17\n17\n17\n17\n17\n17\n17\n17\n1717\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n18\n18\n18\n18\n18\n18\n18\n1818\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n20\n20\n20\n20\n20\n2020\n20\n20\n20\n20\n20\n20\n20\n20\n20\n202020\n20\n21\n21\n21\n21\n21\n21\n21\n21\n2121\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n22\n2222\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n2222\n22\n22\n22\n22\n22\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n2323\n23\n23\n23\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n2424\n24\n24\n24\n24\n24\n24\n24\n25\n25\n25\n2525\n25\n25\n25\n25 25\n25\n2525\n25\n25\n2525\n25\n25\n25\n26\n26\n26\n26\n26\n2626\n26\n26\n26\n26\n26\n26\n26\n26\n2626\n26\n26\n26\n27\n27\n27\n27\n27\n27\n27\n2727\n27\n27\n27\n27\n27\n27\n27\n27\n27\n2727\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n2929\n30\n30\n3030\n30\n30\n3030\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n3131\n31\n31\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n3232\n32\n32\n32\n32\n32\n33\n33\n33\n33\n33\n3333\n33\n33\n33\n33\n33\n33 33\n3333\n33\n33\n3333\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n3535\n3535\n35\n35\n35\n35\n35\n36\n36\n3636\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n37\n37\n37\n37\n37\n37\n3737\n3737\n37\n37\n37\n37\n3737\n37\n3737\n37\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n41\n41\n41\n41\n41\n41\n41\n4141\n41\n41\n41\n41\n41\n41\n41\n41\n4141\n41\n42\n42\n42\n4242\n42\n42\n42\n42\n42\n42\n42\n42\n42\n42\n4242\n42\n42\n42\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n44\n44\n44\n44\n44\n44\n44\n44\n44\n44\n44\n44\n4444\n44\n44\n44\n44\n44\n44\n45\n45\n4545\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n46\n46\n46\n46\n46\n46\n46\n46\n46\n46\n46\n46\n46\n46\n46\n46\n46\n46\n4646\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n48\n48\n48\n48\n48\n4848\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n49\n49\n49\n49\n4949\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n50\n50\n50\n50\n50\n50\n50\n50\n50\n50\n5050\n50\n50\n50\n50\n50\n50\n50\n50\n51\n51\n51\n5151\n51\n51\n5151\n51\n51\n51\n51\n51\n51\n51\n51\n51\n5151\n52\n52\n52\n52\n52\n52\n52\n52\n52\n52\n52\n52\n52\n52\n52\n52\n52\n52\n52\n52\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n54\n5454\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n55\n55\n55\n55\n5555\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n56\n56\n56\n5656\n56\n56\n5656\n56\n56\n56\n56\n5656\n56\n56\n56\n56\n56\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n58\n58\n58\n58\n58\n58\n58\n58\n58\n58\n58\n58\n58\n5858\n58\n58\n5858\n58\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n5959\n59\n59\n59\n60\n60\n60\n60\n60\n60\n6060\n60\n60\n60\n60\n60\n60\n60\n60\n6060\n6060\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n6161\n61\n61\n61\n62\n62\n62\n62\n62\n62\n62\n62\n62\n62\n62\n62\n62\n6262\n62\n62\n62\n62\n62\n63\n63\n63\n6363\n63\n63\n63\n6363\n63\n63\n63\n63\n63\n63\n63\n63\n63\n63\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n65\n65\n65\n65\n65\n65\n65\n65\n65\n6565\n65\n65\n65\n65\n65\n65\n65\n65\n65\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n67\n67\n67\n67\n67\n67\n6767\n67\n67\n67\n6767\n6767\n67\n67 67\n67\n67\n68\n68\n68\n68\n68\n68\n6868\n68\n68\n68\n68\n6868\n68\n68\n68\n68\n68\n68\n69\n69\n69\n6969\n69\n69\n6969\n69\n69\n69\n69\n69\n69\n69\n69\n69\n69\n69\n7070\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n7070\n70\n71\n71\n71\n71\n71\n71\n71\n71\n71\n71\n71\n7171\n71\n71\n7171\n71\n71\n71\n72\n72\n72\n72\n72\n72\n72\n72\n7272\n72\n72\n72\n72\n72\n72\n72\n72\n72\n72\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n74\n74\n74\n74\n74\n74\n74\n74\n7474\n74\n74\n74\n74\n74\n74\n74\n74\n74\n74\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n76\n76\n76\n76\n76\n76\n76\n76\n76\n7676\n7676\n76\n76\n7676\n76\n76\n76\n77\n77\n77\n77\n77\n77\n77\n77\n77\n77\n77\n77\n77\n77\n77\n77\n77\n77\n77\n77\n78\n78\n78\n78\n78\n78\n78\n78\n78\n78\n7878\n78\n78\n7878\n78\n78\n78\n78\n79\n79\n79\n79\n79\n79\n79\n79\n79\n79\n7979\n79\n7979\n79\n79\n79\n79\n79\n80\n80\n80\n80\n80\n80\n8080\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\nMeta-training\nFigure 4. Omniglot domain\n",
    "References\n[1] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. In ICLR, 2022. 1\n[2] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in\nself-supervised vision transformers. In ICCV, 2021. 1\n[3] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela\nMishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In\nICML, 2021. 1\n"
  ],
  "full_text": "Pushing the Limits of Simple Pipelines for Few-Shot Learning:\nExternal Data and Fine-Tuning Make a Difference\nShell Xu Hu1\nDa Li1*\nJan Stühmer1∗\nMinyoung Kim1∗\nTimothy M. Hospedales1,2\n1Samsung AI Center Cambridge\n2University of Edinburgh\n{shell.hu, da.li1, jan.stuhmer, k.minyoung, t.hospedales}@samsung.com\nAbstract\nFew-shot learning (FSL) is an important and topical prob-\nlem in computer vision that has motivated extensive research\ninto numerous methods spanning from sophisticated meta-\nlearning methods to simple transfer learning baselines. We\nseek to push the limits of a simple-but-effective pipeline for\nmore realistic and practical settings of few-shot image clas-\nsiﬁcation. To this end, we explore few-shot learning from\nthe perspective of neural network architecture, as well as a\nthree stage pipeline of network updates under different data\nsupplies, where unsupervised external data is considered for\npre-training, base categories are used to simulate few-shot\ntasks for meta-training, and the scarcely labelled data of\nan noval task is taken for ﬁne-tuning. We investigate ques-\ntions such as: 1 How pre-training on external data beneﬁts\nFSL? 2 How state-of-the-art transformer architectures can\nbe exploited? and\n3 How ﬁne-tuning mitigates domain\nshift? Ultimately, we show that a simple transformer-based\npipeline yields surprisingly good performance on standard\nbenchmarks such as Mini-ImageNet, CIFAR-FS, CDFSL\nand Meta-Dataset. Our code and demo are available at\nhttps://hushell.github.io/pmf.\n1. Introduction\nMainstream supervised deep learning achieves excellent\nresults in applications where huge annotated datasets are\navailable. However, this assumption is not met in many ap-\nplications where data (e.g., rare categories), or the cost of\nhuman annotation are prohibitive bottlenecks. This has moti-\nvated a large and growing set of research in few-shot learning\n(FSL), which aims to emulate the human ability to learn new\nconcepts from few training examples. The FSL challenge\nhas proven fertile ground for developing and testing a vast\narray of sophisticated research ideas spanning metric learn-\ning [59, 61], gradient-based meta-learning [29], program\ninduction [41], differentiable optimization layers [42], hy-\n*Equal contributions.\nCNN-4-64\nRN12\nRN18\nWRN-28-10 ViT-base\n50\n60\n70\n80\n90\n100\nminiImageNet 5-way-5-shot accuracy\nFigure 1. How does pre-training and architecture affect few-\nshot learning? Learning from a few shots can be achieved by a)\nmeta-learning [66,72] and b) transfer learning from self-supervised\nfoundation models pre-trained on large-scale external data [18,53].\nWhile the majority of FSL community focuses on the former, we\nshow that the latter can be more effective because it enables the use\nof stronger architectures such as vision transformer (ViT) [25] – and\ncan be combined with simple meta-learners such as ProtoNet. The\nﬁgure shows results aggregated from dozens of studies from the past\n5 years of FSL research and the result of ProtoNet + ViT backbone\n+ contrastive language-image pretraining (CLIP) [53] (yellow star).\nTo emphasize the importance of pre-training, ProtoNet + randomly\ninitialized ViT (blue square) is also compared.\npernetworks [9], neural optimizers [54], transductive label\npropagation [55], neural loss learning [4], Bayesian neural\npriors [72] and more [69]. But how much practical progress\nhave we made based on all these technical advances?\nA few studies [19, 20, 23, 51, 63, 68] have investigated\nwhether simpler baselines can offer comparable performance\nto sophisticated state of the art few-shot learners. While there\nis no conclusive answer, due to on-going developments in\nboth sophisticated learners [72] and simple baselines, there\nis a trend that simple approaches often perform surprisingly\narXiv:2204.07305v1  [cs.CV]  15 Apr 2022\n\n\nwell compared to sophisticated counterparts. Their simplic-\nity and efﬁcacy leads these simple methods to be taken up\nin many practical applications of few-shot learning from\nmedical data analysis [11] to electronic engineering [40].\nWe follow this line of enquiry, but go further in inves-\ntigating previously under-studied factors that inﬂuence the\nperformance of simple few-shot pipelines. In particular we\nstart with a ProtoNet [59] few-shot learner, and investigate\nthree practically important design choices: pre-training data,\nneural network architecture, and meta-test time ﬁne-tuning.\nSource data\nWhile FSL addresses the small data regime,\nin reality FSL research is almost always about algorithms to\ntransfer knowledge from large scale source tasks (aka meta-\ntrain) to small scale target tasks (aka meta-test). Existing\nliterature almost always controls the source data, in order to\ncarefully compare the impact of different knowledge transfer\nmechanisms of interest from hyper-networks [9] to gradient-\nbased meta-learners [29]. While this is helpful to drive\nresearch on sophisticated algorithms, it does not answer the\nquestion of how choice of source data impacts performance?\nThis question has been studied in other areas of vision and\npattern recognition [10,31,60], but not for FSL. This is un-\nhelpful for consumers of computer vision FSL research, who\nwould be interested to know how much a simple change of\nsource data can improve their applications? Especially since\nfreely available large datasets already exist [21,62], and ex-\nploiting more external source data is easier in practice than\nimplementing sophisticated state-of-the-art meta-learners.\nTo this end we investigate the impact of unsupervised pre-\ntraining on external data – a workﬂow recently termed as\nexploiting a foundation model [10] – on FSL tasks. This\nsmall change has substantial impact compared to 5 years of\nFSL research (Figure 1). Although this may violate deﬁni-\ntions of the FSL problem that strictly prescribe the source\nset, the efﬁcacy of the approach may prompt reﬂection on\nwhether this is the best problem deﬁnition to focus on.\nNeural architecture\nSimilarly to the situation with\nsource data, FSL studies often control neural architecture to a\nhandful of small networks such as CNN-4-64 and ResNet-12.\nThis is partly to enable fair comparison of FSL algorithms,\nbut this particular suite of networks is also a consequence\nof the small size of the source datasets used for training in\ncommon benchmarks such as miniImageNet. Thus the archi-\ntectures commonly studied in FSL are somewhat out-of-date\nwith regard to state-of-the-art computer vision. We there-\nfore ask to what extent state-of-the-art architectures such as\nvision transformers [25] can beneﬁt few-shot performance,\nespecially in conjunction with larger pre-training datasets?\nFine-tuning\nThe many studies in the FSL literature are\nsomewhat divided in whether they advocate [29,54,65] some\nkind of ﬁne-tuning during model deployment (aka meta-test)\nfor individual tasks, or whether a ﬁxed feature representa-\ntion should be sufﬁcient [42, 59, 68]. We also investigate\nDomain A\nDomain B\nClass 1\nClass 2\nClass 3\nClass 4\nClass 5\nClass 6\nSupport set\nAugmented support set\nPre-trained backbone\nExternal data\nMeta-trained backbone\nTask-speciﬁcally ﬁne-tuned backbone\nFigure 2. Overview – A schematic of the simple-but-effective\npipeline that we consider: Pre-training →Meta-training →Fine-\ntuning (P>M>F). Following the red arrows, the pipeline turns a\nclass-agnostic feature backbone into a generic feature backbone\nand ultimately a task-speciﬁc feature backbone.\nthis issue, and suggest that ﬁne-tuning is necessary for de-\nploying foundation models to out-of-distribution tasks. We\nalso introduce an algorithmic improvement to ﬁne-tuning by\nautomating the learning rate selection via validation, which\nleads to a more performant pipeline for cross-domain FSL.\nIn summary, we advance few-shot learning by studying\ndesign choices of a simple pipeline [59] (Figure 2), rather\nthan developing new algorithms. We answer questions in-\ncluding: How does pre-training impact FSL? Can recent\ntransformer architectures be adapted to FSL? and How to\nbest exploit ﬁne-tuning? Based on this analysis we demon-\nstrate a new baseline for FSL that surpasses state-of-the-art\nperformance, while being simple and easy to implement.\n2. Related Work\nFew-shot learning\nFew-shot learning is now a deep and\nwidely studied area too large to review in detail here, and\nwe refer to relevant surveys for an overview [35, 69]. A\nkey point is that, despite the name, almost all FSL methods\nprovide algorithms for transferring knowledge from a large\nset of source data, to a set of sparsely annotated target cate-\ngories of interest. Much activity in the ﬁeld falls under the\numbrella of meta-learning [35], which aims to construct a\ndata-efﬁcient learner from the source (aka meta-train) dataset\nby simulating few-shot learning problems, and then deploy\nthe customized learner on the target (aka meta-test) set. The\nresulting learner may take the form of an initialization [29],\nlearned metric [59], Bayesian prior [72], or optimizer [54].\nSimple-but-effective baselines\nIn competition with the\nplethora of sophisticated few-shot learners [35,69] such as\nthose mentioned above, a number of recent studies have ad-\nvocated strong baselines that perform comparably well while\nbeing simpler. These are often based on a transfer learn-\ning [70] pipeline. They apply a conventional deep learner on\nthe source data, before adapting to the few-shot target data by\ntraining a simple linear [19,51,63] or centroid [68] classiﬁer\n\n\non the ﬁxed representation, or ﬁne-tuning the feature back-\nbone as well [23]. These methods mostly use standardized\nFSL source datasets (such as miniImageNet) and architec-\ntures (such as ResNet-12 and WRN-10-28) to enable direct\ncomparisons of the advocated simple baselines to sophisti-\ncated learners. In contrast, we speciﬁcally aim to explore\nhow far practical FSL performance can be pushed by exploit-\ning other available pre-training datasets and architectures.\nA few studies have evaluated FSL on a larger scale using\ndatasets such as ImageNet1K [20] or ImageNet21K [23].\nHowever by changing both the source and target sets, this\ndoes not make it clear how choice/scale of source data im-\npacts a given target problem – the question that we answer\nhere. Others have explored the impact of conventional pre-\ntraining prior to meta-learning [20] or as a regularizer during\nmeta-learning [30] – but without exploiting extra data.\nBigger data and architectures\nThe impact of source\ndatasets is widely studied in standard supervised [60] and\nself-supervised [10, 31] learning in vision, and in pattern\nrecognition applications outside of vision [3,10,13,22]. How-\never, it is not widely evaluated in FSL, which is a surprising\nomission, since as we shall see it may well be the easiest\nway to improve practical FSL performance. Similarly, ex-\nisting FSL methods are almost exclusively based on a few\nless common architectures (e.g., Conv-4-64 and ResNet-12),\nwhich maybe due to the very ﬁrst experimental setup on\nsmall datasets like Omniglot [29, 66]. Transformers have\nseen limited use in FSL, mainly for metric learning [24],\nbut not for feature extraction. We explore how recent trans-\nformer feature extractors can be trained and applied to FSL,\nespecially when combined with a foundation model [10]\npre-trained on larger source datasets.\nSelf-supervised & few-shot\nOur pipeline extends the typ-\nical unsupervised pre-train →supervised ﬁne-tune workﬂow\nof the self-supervised research community [28,39], which\nhas recently demonstrated strong performance for low-shot\nsupervised learning [15, 18, 27]. However, there has been\nlimited direct comparison of self-supervised (SSL) and FSL\ncommunity methods for data efﬁcient learning due to dif-\nferent typical evaluation practices and benchmarks. For\nexample, many SSL evaluations perform unsupervised repre-\nsentation learning on ImageNet, before performing few-shot\nsupervised learning within ImageNet [15,18], which violates\nusual FSL community requirement of disjoint source and\ntarget data. One contribution of this paper is to provide a\ndegree of comparison between and combination of the SSL\nand FSL approaches. For example, our MetaDataset, CDFSL\nand teaser Figure 1 results, use disjoint source and target\ndata but beneﬁt from external self-supervised pre-training.\nCross-domain few-shot\nA FSL variant of particular prac-\ntical interest is cross-domain few-shot [33], where the\nsource/meta-train dataset is signiﬁcantly different to the\ntarget/meta-test dataset. This is more challenging than the\nstandard within-domain setting, but more practically relevant.\nThis is because in many scenarios where FSL is of interest\nsuch as medical or earth observation imaging [33], the target\ndata for FSL is signiﬁcantly different to available source data\n(such as (mini-)ImageNet [21]). Major benchmarks of this\ntype are CDFSL [33] and meta-dataset [65].\n3. A Simple Pipeline for FSL\nProblem formulation\nFew-shot learning (FSL) aims to\nlearn a model with only a few annotated examples. One\nwidely adopted formulation for FSL was introduced by\nVinyals et al. [66] from a meta-learning perspective, where\nthe assumption is that one should learn to solve new few-shot\ntasks based on previously seen experience of many similar\nfew-shot tasks. Therefore, the FSL problem is usually or-\nganized in two phases: meta-training a few-shot learner on\na distribution of training tasks and meta-testing the result-\ning learner by evaluating it on novel few-shot tasks. Within\neach phase, data arrives in an episodic fashion, where the\n“train-set” and “test-set” of each task are called support set\nand query set respectively to avoid terminology confusion.\nIn the case of classiﬁcation, the difﬁculty level of an episode\nis described as K-way-N-shot, which corresponds to learn-\ning a classiﬁer for K classes given N examples per class in\nthe support set. It is common to learn one model for each\ndifﬁculty level, but a more realistic setting [65] is to learn\na global model for various K’s and N’s. This is sometimes\ncalled various-way-various-shot, and we address this more\npractical setting here. This is also a reason to prefer simple\npipelines over sophisticated meta-learners that may not be\neasily extended to the various-way-various-shot setting.\nA different approach to small-data learning appears in\nthe transfer learning [12, 70] and self-supervision [10, 17]\nliterature. In this case one pre-trains a model using some\nlarge source data, and then re-purposes it for the sparse data\ntarget task of interest. The pre-training step aims to reduce\nthe sample complexity of learning the target problem in the\nadaptation step.\nAlthough typically studied separately, both families of\napproach provide mechanisms for knowledge transfer from\nsource data to the target few-shot problem of interest. To-\nwards the goal of high performance few-shot learning, we\ncombine both pre-training (typically on auxiliary unlabeled\ndata, which is freely and ubiquitously available) and meta-\nlearning (episodic training with labels) together in a simple\nsequential pipeline using a single feature extractor back-\nbone. Our pipeline consists of three phases: 1) pre-training\nthe feature backbone on unlabeled external data using self-\nsupervised loss, 2) meta-training the feature backbone on\nlabeled simulated few-shot tasks using ProtoNet [59] loss,\nand 3) deploying the feature backbone on novel few-shot\n\n\ntasks with optional ﬁne-tuning on the augmented support\nset of each task. A schematic of our pipeline is shown in Fig-\nure 2, which we call P>M>F (i.e., the pipeline Pre-training\n→Meta-training →Fine-tuning ). We next outline how the\nfeature backbone is updated in different stages.\n3.1. Pre-training of backbone\nWe consider the feature backbones of ResNet [34] or\nViT [25], to provide the foundation models in our pipeline.\nThere are then several well-established self-supervised learn-\ning algorithms for the pre-training step: DINO [15] uses\nImageNet1K and exploits the consistency in prediction be-\ntween a large crop and multiple local crops of the same\nimage, where a large crop is highly likely to overlap with a\nforeground object in the case of ImageNet images; BEiT [6]\namounts to solving a masked image reconstruction task on\nthe ImageNet-21K dataset in line with the original BERT\npre-training [22] for text data; and CLIP [53] leverages im-\nage captions in the YFCC100m dataset to align image and\ncaption representations in a common feature space. For\nmore ﬂexible architectures like ViT [25], pre-training on ex-\nternal data is important, as they are hard to train on common\nsmall-sized FSL benchmarks (Figure 1 and Table 1).\n3.2. Meta-training with ProtoNet\nAs the goal is to build a simple pipeline, we consider the\nprototypical network (ProtoNet) [59], which constructs class\ncentroids dynamically for each episode and then performs\nnearest centroid classiﬁcation. Speciﬁcally, ProtoNet only\nrequires a feature backbone f to map data points to a m-\ndimensional feature space: f : X →Rm, and the probability\nof a query image x belonging to class k is given by\np(y = k|x) =\nexp\n\u0000−d(f(x), ck)\n\u0001\nP\nk′ exp\n\u0000−d(f(x), ck′)\n\u0001,\n(1)\nwhere d is implemented by a cosine distance in our work\nas opposed to the commonly chosen Euclidean distance\nand ck is the prototype of class k, deﬁned as ck\n=\n1\nNk\nP\ni:yi=k f(xi) and Nk = P\ni:yi=k 1 on the support set.\nNote that the prototypes can be computed regardless of the\nvalue of k. This enables ProtoNet to be trained and deployed\nunder various-way-various-shot setting.\n3.3. Meta-testing with ﬁne-tuning\nTo be consistent with meta-training, by default, we de-\nploy the meta-trained ProtoNet directly on all novel tasks.\nHowever, if the a novel task is drawn from an unseen domain,\nthe learned feature representation may fail to generalize due\nto a substantial shift in the data distribution. To this end, we\npropose to ﬁne-tune the feature backbone by a few gradient\nsteps with the assistance of data augmentation. The details\nare summarized as PyTorch pseudo code in Algorithm 1.\nAlgorithm 1 PyTorch pseudo code for ﬁne-tuning\n# Inputs: a task including supp_x, supp_y, query_x\n# backbone_state: meta-trained backbone weights\n# optimizer: Adam optimizer\n# Outputs: logits\nbackbone = create_model_from_checkpoint(backbone_state)\ndef single_step(z):\nsupp_f = backbone(supp_x)\nproto = compute_prototypes(supp_f, supp_y)\nf = backbone(z)\nlogits = f.norm() @ proto.norm().T # cos similarity\nloss = cross_entropy_loss(logits, supp_y)\nreturn logits, loss\n# fine-tuning loop\nfor i in range(num_steps):\naug_supp_x = rand_data_augment(supp_x)\n_, loss = single_step(aug_supp_x)\nloss.backward() # back-prop\noptimizer.step() # gradient descent\nlogits, _ = single_step(query_x) # classification\nOur ﬁne-tuning algorithm is similar to that of [33, 43]\nwho ﬁne-tune the model weights using the support set since\nthis is the only accessible labeled data at meta-test time.\nWe exploit the support set slightly differently: we use data\naugmentation to create a pseudo query set derived from the\nsupport set; as such, we do not need to compute prototypes\nusing the support set and then again apply the prototypes on\nthe same support set using eq. (1). Besides, we simply up-\ndate the entire backbone rather than exploring partial model\nadaptation.\nLearning rate selection\nWe observe that the ﬁne-tuning\nperformance is relatively sensitive to the choice of learning\nrate (see supplemental material for more analysis). However,\nexisting few-shot learning problem formulation does not\noffer a validation set for each task to choose the best learning\nrate for ﬁne-tuning. Previous work [33,43] choose a learning\nrate a priori and ﬁx it for every task. This strategy requires\na good understanding of the backbone architecture but still\nleads to sub-optimal performance in general. Given a task\nwith very few labeled images (i.e. the support set), it is\nalmost unlikely to identify which learning rate yields good\ngeneralization for unlabeled images (i.e. the query set). The\ngood news is that we ﬁnd empirically the best learning rate\nis relatively stable across tasks within the same domain. To\nthis end we propose to sample N = 5 extra tasks from\neach domain and automate domain-wise learning rate search\nwithin a reasonable range (e.g., {0.01, 0.001, 0.0001, 0}).\nThe best learning rate is then used for every task within the\ndomain. This additional step amounts to preparing a few\nlabeled images per domain to create a validation set, which\nmakes sense in practice as we can easily organize tasks by\ndomains and identify domain for individual tasks to look up\nthe corresponding learning rate once searched.\n\n\n4. Experiments\nMeta-training datasets\nWe use standard benchmarks to\nevaluate our proposed pipeline. miniImageNet [66] con-\ntains 100 classes from ImageNet-1k, which is then split into\n64 training, 16 validation and 20 testing classes; each image\nis downsampled to 84×84. CIFAR-FS [8] is created by\ndividing the original CIFAR-100 into 64 training, 16 valida-\ntion and 20 testing classes. The images are of size 32×32.\nMeta-Dataset [65] subsumes 10 public image datasets of a\ndiverse range of domains: ImageNet-1k, Omniglot, FGVC-\nAircraft, CUB-200-2011, Describable Textures, QuickDraw,\nFGVCx Fungi, VGG Flower, Trafﬁc Signs and MSCOCO.\nEach dataset has train/val/test splits. We follow the two\ntraining protocols proposed by [65] and [24] respectively.\nFor the former, the train/val splits of the ﬁrst 8 datasets (in-\ndomain) are used for meta-training and validation, and the\ntest splits of all datasets are used for meta-testing. The latter\nconsiders only ImageNet-1k’s train-split for meta-training,\nand the other settings remain the same. For more details on\nMeta-Dataset we refer the readers to Appendix.3 of [65].\nEvaluation\nFor evaluating few-shot classiﬁcation perfor-\nmance, we simulate 600 episodes/tasks from the test-split\nfor each dataset of interest. The evaluation metric is the av-\nerage classiﬁcation accuracy over tasks. For miniImageNet\nand CIFAR-FS, the convention is to evaluate 5-way-1-shot\n(5w1s) and 5-way-5-shot episodes, and the size of the query\nset for each episode is ﬁxed to 15 × 5. For Meta-Dataset, the\nnumber of ways, shots and query images are sampled uni-\nformly at random with respect to the dataset speciﬁcations,\nexcept for ImageNet-1k and Omniglot (they have speciﬁc\nsampling strategies according to the hierarchy of classes). In\naddition, we evaluate the (5w5s) meta-trained model from\nminiImageNet for a cross-domain evaluation (CDFSL) [33],\nwhere 4 out-of-domain datasets are considered, and the re-\nsults are reported under 5-way-5/20/50-shot settings.\nTraining details\nTo avoid over-engineering training for\ndifferent datasets and architectures, we adopt a common\ntraining strategy for meta-training the backbone from pre-\ntrained model checkpoints (for both ResNet and ViT). This\nmay lead to sub-optimal results for some cases, but it sim-\npliﬁes comparison. Speciﬁcally, we train the backbone for\n100 epochs, where each epoch consists of 2000 episodes/-\ntasks. We use a warm-up plus cosine annealing learning rate\nschedule: the learning rate starts from 10−6, increases to\n5 × 10−5 in 5 epochs and then gradually decreases to 10−6\nwith a cosine annealing. We use the validation set to decide\nwhen to early stop, and turn off strong regularization and\ndata augmentation techniques for simplicity.\n4.1. Analysis\nWe now use the pipeline outlined in Sec 3 to answer a\nseries of questions about few-shot learner pipeline design.\nTraining Conﬁguration\nBenchmark Results\nID\nArch\nPre Train\nMetaTr\nMD\nminiIN\nCIFAR\n0\nViT-small\nDINO (IN1K)\n-\n67.4\n97.0\n79.8\n1\nViT-small\nDeiT (IN1K)\n-\n67.5\n98.8\n84.6\n2\nResNet50\nDINO (IN1K)\n-\n63.8\n91.5\n76.1\n3\nResNet50\nSup. (IN1K)\n-\n62.4\n96.4\n82.3\n4\nViT-small\nDINO (IN1K)\nPN\n78.4\n98.0\n92.5\n5\nViT-small\nDEIT (IN1K)\nPN\n79.3\n99.4\n93.6\n6\nViT-small\n-\nPN\n52.8\n49.1\n59.8\n7\nResNet50\nDINO (IN1K)\nPN\n72.4\n92.0\n84.0\n8\nResNet50\nSup. (IN1K)\nPN\n70.2\n97.4\n87.6\n9\nResNet50\n-\nPN\n62.9\n72.2\n68.4\n10\nResNet18\n-\nPN\n63.3\n73.7\n70.2\n11\nViT-base\nDINO (IN1K)\nPN\n79.2\n98.4\n92.2\n12\nViT-base\nCLIP (YFCC)\nPN\n80.0\n98.1\n93.2\n13\nViT-base\nSup (IN21K)\nPN\n81.4\n99.2\n96.7\n14\nViT-base\nBEIT (IN21K)\nPN\n82.8\n99.0\n97.5\n15\nResNet50\nCLIP (YFCC)\nPN\n75.0\n92.2\n82.6\nTable 1. The impact of architecture and pre-training algorithm\n(dataset) on downstream few-shot learning performance on Meta-\nDataset (MD), miniImageNet (miniIN) and CIFAR-FS. Meta-\nDataset results are averaged over all target datasets while minIN and\nCIFAR results are 5-way-5-shot. ProtoNet (PN) nearest-centroid\nclassiﬁer is used throughout for few-shot learning on the support set\nduring meta-test. MetaTr indicates the algorithm used for episodic\nlearning on the corresponding benchmark.\nNotably, 1 How does pre-training regime affect FSL? 2\nCan contemporary architectures such as ViT be adapted to\nFSL? 3 How to exploit ﬁne-tuning in meta-testing?\n4.1.1\nPre-training and architectures\nWe ﬁrst evaluate the impact of pre-training regime (includ-\ning algorithm and dataset), as well as neural architecture\non FSL benchmarks Meta-Dataset [65] (train on 8 datasets),\nminiImageNet [66], and CIFAR-FS [8]. To clearly con-\nvey the conﬁguration of each experiment, results in Table 1\nare organized by architecture, pre-training algorithm (and\ndataset) and meta-training algorithm. We assume ProtoNet\n(nearest-centroid) classiﬁer as the standard approach for\nmeta-testing throughout, and compare either episodically\ntrained ProtoNet or nothing as the meta-learning step be-\ntween pre-training and meta-testing (column MetaTr).\n1 How does pre-training regime affect FSL?\nFrom the\nresults in Table 1 we can draw the following conclusions: (i)\nPre-training on ImageNet1K generally provides a signiﬁcant\nimprovement across the board compared to the conventional\npipeline used by prior work which does not make use of pre-\ntraining (compare model M9 with M7 and M8, etc). (ii) We\nare primarily interested in unsupervised pre-training, with\nsupervised pre-training being included as an unfair upper\nbound. However, state of the art unsupervised pre-training\nwith DINO performs close to supervised pre-training (com-\npare M3 vs M2, etc). This is noteworthy, because while\nthere is some semantic overlap between some of the source\n\n\n(ImageNet1K) and target (Meta-Dataset, miniImageNet, CI-\nFAR) datasets considered here, good performance can be\nachieved without using source labels, where there is no train-\ntest label leakage1. (iii) Given a strong pre-training regime\nsuch as DINO, simple nearest centroid classiﬁcation based\non pre-trained features performs well (top block including\nM2, etc). In particular, off-the-shelf features from a founda-\ntion model without dataset-speciﬁc meta-learning perform\nfavorably compared to conventional dataset-speciﬁc training\nof ProtoNet-ResNet18 (M2 vs M10), which is arguably the\nclosest to industry standard in FSL. (iv) Nevertheless, dataset\nspeciﬁc meta-learning does improve further (M7 vs M2, etc).\nSimple linear readout of a frozen foundation model [18,27]\nis not competitive.\n2 Can state of the art architectures such as ViT be\nadapted to FSL?\nUsing the results in Table 1, we can\nalso answer this question. In particular, while ViT does not\ntrain well on the smaller meta-train benchmarks (miniIma-\ngeNet, CIFAR) compared to smaller architectures (see M6\nvs M9, M10), it generally performs excellently when bene-\nﬁting from large pre-training data (M6 vs M4). Overall ViT\noutperforms the industry standard ResNet18, as well as our\nResNet50 baseline, across the board when beneﬁtting from\npre-training. We remark that our ResNet50 baseline also per-\nforms comparitively poorly without pre-training, especially\non the smaller miniImageNet and CIFAR, suggesting that it\nis also too large to train well on the target datasets alone.\nOther foundation models\nOverall we can see that larger\npre-training data sources, and recent architectures make a\nhuge difference to downstream FSL performance on stan-\ndard benchmarks. We also compared a selection of other\nfoundation models [10] in M11-15. We can see that (i) All\nthe foundation models lead to substantial improvements on\nstandard within-dataset training (M10,M9), (ii) The largest\nfoundation models using, e.g., ViT-base and ImageNet21K\nor YFCC data source lead to strongest performance across\nthe board, but do not outperform hugely the more economic\nDINO+ImageNet1K-based ViT-small (M4). For efﬁciency\nof pre-training and deployment, we take this to be our default\nmodel in the following section.\n1 + 2 How does pre-training and architecture impact\nother Few-Shot Learners?\nOur main experiments built\nupon ProtoNet as a widely used industry standard. We next\n1In the case of miniImageNet and Meta-Dataset, parts of ImageNet1K\nare used in both meta-train and meta-test splits. EG: since Meta-Dataset’s\nImageNet uses a 712/288 source/target class split, this means that for one\nof Meta-Dataset’s 10 domains, there is some data (but not label) overlap\nbetween pre-train and meta-test for some foundation models. As discussed\nin Sec. 2, this overlap is ubiquitious in typical self-supervision evaluation\npipelines [15, 17]. It is less common in FSL evaluation pipelines, but\ncorresponds to making a semi-supervised or transductive assumption in\nterms of data access as per [38,45,49,55]. Nevertheless, we do not think\nthis is a signiﬁcant factor in the strong results, as CLIP’s YFCC does not\nhave this overlap and performs similarly to the ImageNet1K based models.\nTrain Conﬁg\nBenchmark\nID\nArch\nPre Train\nMetaTr\nminiIN\nCIFAR\n5/1\n5/5\n5/1\n5/5\n0\nViT-small\nDINO (IN1K)\n-\n88.8\n97.0\n59.1\n79.8\n1\nViT-small\nDINO (IN1K)\nProtoNet\n93.1\n98.0\n81.1\n92.5\n2\nResNet18\n-\nMetaQDA\n65.1\n81.0\n-\n-\n3\nViT-small\nDINO (IN1K)\nMetaQDA\n92.0\n97.0\n77.2\n90.1\n4\nResNet12\n-\nMetaOptNet 64.1\n80.0\n72.8\n85.0\n5\nViT-small\nDINO (IN1K)\nMetaOptNet 92.2\n97.8\n70.2\n84.1\nTable 2. Impact of architecture and pre-training on state-of-the-art\nfew-shot learners: MetaQDA [72], MetaOptNet [42].\nexplore how our pipeline impacts two few-shot learners that\nare more representative of recent state of the art, namely\nMetaOptNet [42] and MetaQDA [72]. From the results in\nTable 2, we can see that: (i) MetaQDA and MetaOptNet do\nimprove on direct feature transfer (M5 and M3 vs M0) and\non the simpler ResNet features they were initially evaluated\nwith (M5 vs M4, M3 vs M2). But (ii) With the stronger\nfeatures, they are outperformed by the simpler ProtoNet\nlearner (M3 and M5 vs M1). This suggests previous con-\nclusions about comparative meta-learner performance may\nneed re-evaluating in this new regime of stronger features.\nFew-shot learning v.s. self-supervised learning\nExist-\ning literature generally fails to directly compare algorithms\nfrom the few-shot learning community (such as ProtoNet,\n[59], MAML [29], MetaOptNet [42], etc), with those from\nthe self-supervised community (such as DINO [15], Sim-\nCLR [17,18], etc). This is partly because the popular evalua-\ntion protocol is different: For example 5-way-1-shot regime\nis popular the FSL community, vs 1% labels (≈1000-way-\n10-shot in the case of ImageNet) in the SSL community;\nnetwork architectures differ (≤ResNet18 vs ≥ResNet50 re-\nspectively); and image resolutions differ (84× vs full). Our\nresults provide a taster of such a direct comparison. Overall\nthey suggest that frozen self-supervised foundation models\n(using extra pre-training data) are competitive out of the box\ncompared to standard few-shot learners (using only meta-\ntraining data). However, more interestingly, combining these\ntwo paradigms as we have done, easily leads to state of the\nart performance on typical FSL metrics.\nClass overlap between pre-training and meta-testing\nAlthough unsupervised pre-training does not utilize labels, it\nis very likely that some classes used by pre-training also ap-\npear in meta-testing. Does this class overlap go against the\nvery deﬁnition of few-shot learning? From a meta-learning\npoint of view, the answer is yes. But we argue that class\noverlap is almost unavoidable unless a careful data split is\nsimulated. For example, in the case of Meta-Dataset, the\nCUB dataset [67], the Aircraft dataset [50] and the COCO\ndataset [47] have a class overlap with ImageNet [24,32] but\nthey are still used in meta-testing. As we consider more\npractical large-scale experiments, the class overlap issue be-\n\n\nM\nArch\nPreTr\nMetaTr\nMetaTe\nAvg\nOut-D\n1\nViT-small\nDINO\nPN (IN)\nPN\n68.38\n67.68\n2\nViT-small\nDINO\nPN (IN)\nPN+FT(lr=0.01)\n76.05\n76.54\n3\nViT-small\nDINO\nPN (IN)\nPN+FT(lr=0.001)\n74.47\n74.51\n4\nViT-small\nDINO\nPN (IN)\nPN+FT(Tuned)\n77.53\n77.85\n5\nViT-small\nDINO\nPN (MD)\nPN\n78.43\n55.71\n6\nViT-small\nDINO\nPN (MD)\nPN+FT(lr=0.01)\n76.09\n73.26\n7\nViT-small\nDINO\nPN (MD)\nPN+FT(lr=0.001)\n74.64\n69.97\n8\nViT-small\nDINO\nPN (MD)\nPN+FT(Tuned)\n83.13\n75.72\nTable 3. Fine-tuning (FT) during meta-test on Meta-Dataset. The\nmeta-train (MetaTr) setting indicates the source dataset as Ima-\ngeNet only (IN) or full MetaDataset (MD). Results are the averages\nacross all domains within meta-dataset (Avg), and just the out-of-\ndistribution subset (Out-D).\ncomes ubiquitous. We should worry about this issue if we\nwere benchmarking a meta-learning algorithm, but for the\nnature of few-shot learning, benchmarking the capability of\nquickly constructing a classiﬁer from very few labels is not\nhindered by class overlap. This is why self-supervised learn-\ning community is not bothered by this issue at all. It is worth\nmentioning that a similar setting called “few-shot few-shot\nlearning” has been proposed by [46,71], where they avoid\noverlap by either carefully picking up pre-training data from\na different domain or crawling pre-training data of base cate-\ngories from Internet. Alternatively, one may avoid overlap\nby using a different modality. We advocate meta-learning\nresearchers to consider this controlled setting as a testing bed\nfor incorporating powerful pre-trained feature backbones.\nINet\nOmglot\nAcraft \nCUB\nDTD \nQDraw \nFungi \nFlower \nSign \nCOCO\nAvg\n50\n55\n60\n65\n70\n75\n80\n85\n90\n95\nM1: DINO + PN (IN)\nM2: DINO + PN (IN) + FT\nM5: DINO + PN (MD) \nFigure 3. The impact of ﬁne-tuning during meta-test on Meta-\nDataset. Held out datasets such as Signs and COCO beneﬁt from\nﬁne-tuning; as do those very different from ImageNet such as\nomniglot and QuickDraw.\n4.1.2\nFine-tuning\nThe previous experiments used a ﬁxed feature extractor to-\ngether with ProtoNet for meta-testing. We next investigate\nuse of ﬁne-tuning during meta-testing to further improve per-\nformance. We focus on the DINO pre-trained ViT models,\nbased on their strong performance in Section 4.1.1.\n3\nHow to best exploit ﬁne-tuning for meta-testing?\nMethod (Backbone)\nExt.\nExt.\nCIFAR-FS\nMiniImageNet\ndat.\nlab.\n5w1s\n5w5s\n5w1s\n5w5s\nInductive\nProtoNet (CNN-4-64) [59]\n49.4\n68.2\n55.5\n72.0\nBaseline++ (CNN-4-64) [19]\n48.2\n66.4\nMetaOpt-SVM (ResNet12) [42]\n72.0\n84.3\n61.4\n77.9\nMeta-Baseline (ResNet12) [20]\n68.6\n83.7\nRS-FSL (ResNet12) [2]\n\u0013\n65.3\nTransductive\nFine-tuning (WRN-28-10) [23]\n76.6\n85.8\n65.7\n78.4\nSIB (WRN-28-10) [36]\n80.0\n85.3\n70.0\n79.2\nPT-MAP (WRN-28-10) [37]\n87.7\n90.7\n82.9\n88.8\nCNAPS + FETI (ResNet18) [7]\n\u0013\n\u0013\n79.9\n91.5\nSelf-supervised\nProtoNet (WRN-28-10) [30]\n73.6\n86.1\n62.9\n79.9\nProtoNet (AMDIM ResNet) [16]\n\u0013\n76.8\n91.0\nEPNet + SSL (WRN-28-10) [57]\n\u0013\n79.2\n88.1\nSemi-supervised\nLST (ResNet12) [45]\n\u0013\n70.1\n78.7\nPLCM (ResNet12) [38]\n\u0013\n77.6\n86.1\n70.1\n83.7\nP>M>F (IN1K, RN50)\n\u0013\n73.7\n84.0\n79.2\n92.0\nP>M>F (IN1K, ViT-Small)\n\u0013\n81.1\n92.5\n93.1\n98.0\nP>M>F (IN1K, ViT-base)\n\u0013\n84.3\n92.2\n95.3\n98.4\nTable 4. miniImageNet & CIFAR – Comparison with represen-\ntative SOTA FSL algorithms. Methods using external data and/or\nlabels are indicated.\nTo answer this question, we compare vanilla feature transfer\nas explored previously, with ProtoNet, and ProtoNet with\nepisode-wise ﬁne-tuning on the support set (ProtoNet+FT)\nas outlined in Section 3.3. We use Meta-Dataset including\nboth conditions of treating ImageNet alone as the source, and\njoint meta-training on all of Meta-Dataset. From the results\nin Figure 3 and Table 3 we can draw the following conclu-\nsions: (i) Meta-training on the full Meta-Dataset improves\non meta-training on ImageNet-training alone (M5 vs M1).\n(ii) Fine-tuning during meta-test improves substantially in\nthe out-of-distribution datasets, and especially in the case\nwhere meta-training is conducted on ImageNet, and then\ndeployed across-domain to all the other Meta-Dataset tasks:\nSee Out-D column and M2 vs M1 in Table 3; blue vs orange\nbars in Figure 3 for OmniGlot, QuickDraw, trafﬁc signs,\netc. However, for the condition where more Meta-Dataset\ndomains are used for training and testing, ﬁne-tuning has\ninconsistent impact across domains: While it is helpful for\nthe remaining OOD datasets, it is not helpful overall (M5 vs\nM6 for Avg and Out-D). Overall feature backbone updates\nby ﬁne-tuning are more helpful for domains unseen during\nmeta-training, concurring with [43, 65]. On analysing the\ninconsistent impact of ﬁne-tuning, we found this is due to\ndifﬁculty in choosing an appropriate learning rate. Using any\nsingle learning rate throughout, as we did above (lr=0.01) is\npoorly tuned for some datasets. We therefore also explore\nour learning rate selection heuristic proposed in Section 3.3,\nand we see this leads to the best performance (M4 vs M2).\n4.2. Results on standard benchmarks\nWe call our pipeline P>M>F, which can be instantiated\nwith any pre-training algorithm and backbone architectures,\n\n\n8 in-domain datasets\nIn-domain\nOut-of-domain\nINet\nOmglot\nAcraft\nCUB\nDTD\nQDraw\nFungi\nFlower\nSign\nCOCO\nAvg\nProtoNet [65] (RN18)\n67.01\n44.5\n79.56\n71.14\n67.01\n65.18\n64.88\n40.26\n86.85\n46.48\n63.29\nCNAPs [56] (RN18+Adapter)\n50.8\n91.7\n83.7\n73.6\n59.5\n74.7\n50.2\n88.9\n56.5\n39.4\n66.90\nSUR [26] (RN18+Adapter)\n57.2\n93.2\n90.1\n82.3\n73.5\n81.9\n67.9\n88.4\n67.4\n51.3\n75.32\nT-SCNAPs [7] (RN18+Adapter)\n58.8\n93.9\n84.1\n76.8\n69.0\n78.6\n48.8\n91.6\n76.1\n48.7\n72.64\nURT [48] (RN18+Adapter)\n55.7\n94.4\n85.8\n76.3\n71.8\n82.5\n63.5\n88.2\n69.4\n52.2\n73.98\nFLUTE [64] (RN18)\n51.8\n93.2\n87.2\n79.2\n68.8\n79.5\n58.1\n91.6\n58.4\n50.0\n71.78\nURL [44] (RN18+Adapter)\n57.51\n94.51\n88.59\n80.54\n76.17\n81.94\n68.75\n92.11\n63.34\n54.03\n75.75\nITA [43] (RN18+Adapter)\n57.35\n94.96\n89.33\n81.42\n76.74\n82.01\n67.4\n92.18\n83.55\n55.75\n78.07\nP>M>F (DINO/IN1K, RN50)\n67.51\n85.91\n80.3\n81.67\n87.08\n72.84\n60.03\n94.69\n87.17\n58.92\n77.61\nP>M>F (DINO/IN1K, ViT-small)\n74.59\n91.79\n88.33\n91.02\n86.61\n79.23\n74.2\n94.12\n88.85\n62.59\n83.13\nP>M>F (DINO/IN1K, ViT-base)\n77.02\n91.76\n89.73\n92.94\n86.94\n80.2\n78.28\n95.79\n89.86\n64.97\n84.75\nIn-domain = ImageNet\nIn-domain\nOut-of-domain\nINet\nOmglot\nAcraft\nCUB\nDTD\nQDraw\nFungi\nFlower\nSign\nCOCO\nAvg\nProtoNet [65] (RN18)\n50.5\n59.98\n53.1\n68.79\n66.56\n48.96\n39.71\n85.27\n47.12\n41\n56.10\nALFA+FP-MAML [5] (RN12)\n52.8\n61.87\n63.43\n69.75\n70.78\n59.17\n41.49\n85.96\n60.78\n48.11\n61.41\nBOHB [58] (RN18)\n51.92\n67.57\n54.12\n70.69\n68.34\n50.33\n41.38\n87.34\n51.8\n48.03\n59.15\nCTX [24] (RN34)\n62.76\n82.21\n79.49\n80.63\n75.57\n72.68\n51.58\n95.34\n82.65\n59.9\n74.28\nP>M>F (DINO/IN1K, RN50)\n67.08\n75.33\n75.39\n72.08\n86.42\n66.79\n50.53\n94.14\n86.54\n58.2\n73.25\nP>M>F (DINO/IN1K, ViT-small)\n74.69\n80.68\n76.78\n85.04\n86.63\n71.25\n54.78\n94.57\n88.33\n62.57\n77.53\nP>M>F (DINO/IN1K, ViT-base)\n76.69\n81.42\n80.33\n84.38\n86.87\n75.43\n55.93\n95.14\n89.68\n65.01\n79.09\nTable 5. Meta-Dataset – Comparison with SOTA FSL algorithms.\nChestX\nISIC\nEuroSAT\nCropDisease\n5w5s\n5w20s\n5w50s\n5w5s\n5w20s\n5w50s\n5w5s\n5w20s\n5w50s\n5w5s\n5w20s\n5w50s\nProtoNet [59] (RN10)\n24.05\n28.21\n29.32\n39.57\n49.50\n51.99\n73.29\n82.27\n80.48\n79.72\n88.15\n90.81\nRelationNet [61] (RN10)\n22.96\n26.63\n28.45\n39.41\n41.77\n49.32\n61.31\n74.43\n74.91\n68.99\n80.45\n85.08\nMetaOptNet [42] (RN10)\n22.53\n25.53\n29.35\n36.28\n49.42\n54.80\n64.44\n79.19\n83.62\n68.41\n82.89\n91.76\nFinetune [33] (RN10)\n25.97\n31.32\n35.49\n48.11\n59.31\n66.48\n79.08\n87.64\n90.89\n89.25\n95.51\n97.68\nCHEF [1] (RN10)\n24.72\n29.71\n31.25\n41.26\n54.30\n60.86\n74.15\n83.31\n86.55\n86.87\n94.78\n96.77\nSTARTUP [52] (RN10)\n26.94\n33.19\n36.91\n47.22\n58.63\n64.16\n82.29\n89.26\n91.99\n93.02\n97.51\n98.45\nDeepCluster2 [14,27] (IN1K, RN50)\n26.51\n31.51\n34.17\n40.73\n49.91\n53.65\n88.39\n92.02\n93.07\n93.63\n96.63\n97.04\nP>M>F (DINO/IN1K, ResNet50)\n27.13\n31.57\n34.17\n43.78\n54.06\n57.86\n89.18\n93.08\n96.06\n95.06\n97.25\n97.77\nP>M>F (DINO/IN1K, ViT-small)\n27.27\n35.33\n41.39\n50.12\n65.78\n73.50\n85.98\n91.32\n95.40\n92.96\n98.12\n99.24\nTable 6. Broader study of cross-domain few-shot learning – Comparison with SOTA FSL algorithms.\ne.g., DINO > ProtoNet (PN) > Fine-tuning (FT). We next\ncompare our pipeline with prior state of the art. We empha-\nsize that our results are not directly comparable to much\nprior SOTA in terms of architecture and use of external\ndata. We draw this comparison to see how simple changes\n(such as upgrading feature backbone to a modern network\narchitecture and exploiting publicly available data for a large-\nscale pre-training) compare against 5 years of intensive re-\nsearch on FSL algorithms. The results for the single-domain\ncases, i.e., mini-ImageNet and CIFAR-FS, are summarized\nin Table 4, while the results for the cross-domain datasets,\ni.e., Meta-Dataset and Broader Study CDFSL, are shown\nin Table 5 and 6 respectively. From the results we can see\nthat our framework outperforms much the state of the art\nin both within-domain and cross-domain conditions despite\nbeing signiﬁcantly simpler than some sophisticated competi-\ntors. We remark that for the single source benchmarks in\nTable 4, a few competitors also used external data or Im-\nageNet pre-training as indicated. Meanwhile our hybrid\npipeline outperforms SOTA pure external self-supervision\n[14, 27] for CDFSL in Table 6. Our code is available at\nhttps://github.com/hushell/pmf_cvpr22.\n4.3. Discussion\nTaken together, the results show that our simple pipeline\nof exploiting available pre-training data and a modern ar-\nchitecture often outperforms sophisticated state of the art in\nfew-shot learning. This margin is increased using our pro-\nposed adaptive ﬁne-tuning mechanism in the meta-test stage.\nBased on these observations we make recommendations both\nfor practitioners and few-shot learning researchers.\nPractitioners: Increasing pre-training data size or simply\nusing a foundation model [10,15] and upgrading to modern\narchitectures is likely to be more productive (and much easier\nto implement) than keeping up with and implementing state\nof the art few-shot learning algorithms. Fine-tuning is likely\nto be important if the target few-shot task of interest is less\nsimilar to the pre-training and meta-training data.\n\n\nFSL researchers: Our results show that using external data\nand modern architectures is an easy and effective way to\nachieve strong FSL performance, and also that some SOTA\nmeta-learners fail to provide expected improvements in this\nregime. While external data violates deﬁnitions of the FSL\nproblem that insist on a speciﬁc limited meta-train set, we\nshould take this setting seriously to maintain practical rele-\nvance in the face of advancing self-supervision [15,28,39,53].\nIn particular, we recommend a new evaluation setting for\nall the standard FSL benchmarks, where pre-train data and\narchitecture are freely chosen and clearly reported. Few-shot\nmeta-learning methods are then evaluated on their ability to\nimprove on linear readout, ﬁne-tuning, or our PMF baseline\nfor the given external dataset and architecture.\n5. Conclusions\nWe advanced few-shot learning from the perspective of\npushing the limits of a simple pre-train + ProtoNet pipeline\nin terms of dataset, architecture and ﬁne-tuning strategy.\nWe showed that source dataset, and neural architecture are\ndominant factors in FSL performance. When there is a\ndomain shift between training and testing, we showed that\nﬁne-tuning the feature backbone with data augmentation is\nalso important. We veriﬁed that our simple pipelines achieve\nvery competitive performance in four FSL benchmarks.\nLimitations and future work\nThere are several limita-\ntions of our empirical study. We only scratched the surface\nof the impact of external data and correspondingly larger\narchitectures on FSL. Our renewed focus on external data\nemphasizes the need for algorithms from the FSL commu-\nnity [29,42,59] to be directly compared against algorithms\nfrom the self-supervised community [10, 17], or possibly\nsynergistically combined, as we attempt here. The hybrid\npipeline that we propose is obviously restricted to modalities\nwhere large external datasets already exist, and would re-\nquire signiﬁcant up-front investment in compute and energy\ncost where pre-trained foundation models do not already ex-\nist. Possible bias within foundation models is also a potential\nrisk [10]. Finally, while effective, our adaptive ﬁne-tuning\nstrategy, is rather computationally expensive at meta-test\ntime, and may be unsupported on embedded platforms with-\nout backpropagation. Feed-forward representation adapta-\ntion methods [56] may be important for future work.\nAcknowledgement\nWe thank the anonymous reviewers and meta-reviewers of\nCVPR2022 for their careful reading and thorough discussion\nof our manuscript. We also thank our colleagues at SAIC-\nCambridge, especially Gabor Gyorkei, Taekwon Jang and\nBrais Martinez, for their help and support.\nReferences\n[1] Thomas Adler, Johannes Brandstetter, Michael Widrich, An-\ndreas Mayr, David Kreil, Michael Kopp, Günter Klambauer,\nand Sepp Hochreiter. Cross-domain few-shot learning by\nrepresentation fusion. In arXiv, 2021. 8\n[2] Mohamed Afham, Salman Khan, Muhammad Haris Khan,\nMuzammal Naseer, and Fahad Shahbaz Khan. Rich semantics\nimprove few-shot learning. In BMVC, 2021. 7\n[3] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and\nMichael Auli. wav2vec 2.0: A framework for self-supervised\nlearning of speech representations. NeurIPS, 2020. 3\n[4] Sungyong Baik, Janghoon Choi, Heewon Kim, Dohee Cho,\nJaesik Min, and Kyoung Mu Lee. Meta-learning with task-\nadaptive loss function for few-shot learning. In ICCV, 2021.\n1\n[5] Sungyong Baik, Myungsub Choi, Janghoon Choi, Heewon\nKim, and Kyoung Mu Lee. Meta-learning with adaptive\nhyperparameters. In NeurIPS, 2020. 8\n[6] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training\nof image transformers. In ICLR, 2022. 4\n[7] Peyman Bateni, Jarred Barber, Jan-Willem van de Meent, and\nFrank Wood. Enhancing few-shot image classiﬁcation with\nunlabelled examples. In WACV, 2022. 7, 8\n[8] Luca Bertinetto, João F. Henriques, Philip H.S. Torr, and\nAndrea Vedaldi. Meta-learning with differentiable closed-\nform solvers. In ICLR, 2019. 5\n[9] Luca Bertinetto, Joao F. Henriques, Jack Valmadre, Philip\nH. S. Torr, and Andrea Vedaldi. Learning feed-forward one-\nshot learners. In NIPS, 2016. 1, 2\n[10] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Alt-\nman, Simran Arora, Sydney von Arx, Michael S Bernstein,\nJeannette Bohg, Antoine Bosselut, Emma Brunskill, et al.\nOn the opportunities and risks of foundation models. arXiv\npreprint arXiv:2108.07258, 2021. 2, 3, 6, 8, 9\n[11] Myriam Bontonou, Nicolas Farrugia, and Vincent Gripon.\nFew-shot learning for decoding brain signals.\nCoRR,\nabs/2010.12500, 2020. 2\n[12] Stevo Bozinovski. Reminder of the ﬁrst paper on transfer\nlearning in neural networks, 1976. Informatica, 44(3), 2020.\n3\n[13] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. NeurIPS, 2020. 3\n[14] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal,\nPiotr Bojanowski, and Armand Joulin. Unsupervised learn-\ning of visual features by contrasting cluster assignments. In\nNeurIPS, 2020. 8\n[15] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In ICCV,\n2021. 3, 4, 6, 8, 9\n[16] Da Chen, Yuefeng Chen, Yuhong Li, Feng Mao, Yuan He,\nand Hui Xue. Self-supervised learning for few-shot image\nclassiﬁcation. In ICASSP, 2021. 7\n\n\n[17] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geof-\nfrey Hinton. A simple framework for contrastive learning of\nvisual representations. In ICML, 2020. 3, 6, 9\n[18] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad\nNorouzi, and Geoffrey Hinton. Big self-supervised models\nare strong semi-supervised learners. In NeurIPS, 2020. 1, 3,\n6\n[19] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank\nWang, and Jia-Bin Huang. A closer look at few-shot classiﬁ-\ncation. ICLR, 2019. 1, 2, 7\n[20] Yinbo Chen, Zhuang Liu, Huijuan Xu, Trevor Darrell, and Xi-\naolong Wang. Meta-baseline: Exploring simple meta-learning\nfor few-shot learning. In ICCV, 2021. 1, 3, 7\n[21] Jia Deng, Wei Dong, R. Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. Imagenet: A large-scale hierarchical image database.\nIn CVPR, 2009. 2, 3\n[22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: pre-training of deep bidirectional trans-\nformers for language understanding. In ACL, 2019. 3, 4\n[23] Guneet Singh Dhillon, Pratik Chaudhari, Avinash Ravichan-\ndran, and Stefano Soatto. A baseline for few-shot image\nclassiﬁcation. In ICLR, 2020. 1, 3, 7\n[24] Carl Doersch, Ankush Gupta, and Andrew Zisserman.\nCrosstransformers: spatially-aware few-shot transfer.\nIn\nNeurIPS, 2021. 3, 5, 6, 8\n[25] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In ICLR, 2021. 1, 2, 4\n[26] Nikita Dvornik, Cordelia Schmid, and Julien Mairal. Select-\ning relevant features from a multi-domain representation for\nfew-shot classiﬁcation. In ECCV, 2020. 8\n[27] Linus Ericsson, Henry Gouk, and Timothy M Hospedales.\nHow well do self-supervised models transfer?\nIn CVPR,\n2021. 3, 6, 8\n[28] Linus Ericsson, Henry Gouk, Chen Change Loy, and Timo-\nthy M Hospedales. Self-supervised representation learning:\nIntroduction, advances and challenges. IEEE Signal Process-\ning Magazine, 2022. 3, 9\n[29] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-\nagnostic meta-learning for fast adaptation of deep networks.\nIn ICML, 2017. 1, 2, 3, 6, 9\n[30] Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick\nPérez, and Matthieu Cord. Boosting few-shot visual learning\nwith self-supervision. In ICCV, 2019. 3, 7\n[31] Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan\nMisra. Scaling and benchmarking self-supervised visual rep-\nresentation learning. In ICCV, 2019. 2, 3\n[32] Pei Guo. Overlap between imagenet and cub. 6\n[33] Yunhui Guo, Noel C Codella, Leonid Karlinsky, James V\nCodella, John R Smith, Kate Saenko, Tajana Rosing, and\nRogerio Feris. A broader study of cross-domain few-shot\nlearning. In ECCV, 2020. 3, 4, 5, 8\n[34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR, 2016.\n4\n[35] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and\nAmos Storkey. Meta-learning in neural networks: A sur-\nvey. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 2021. 2\n[36] Shell Xu Hu, Pablo Moreno, Yang Xiao, Xi Shen, Guillaume\nObozinski, Neil Lawrence, and Andreas Damianou. Empiri-\ncal bayes transductive meta-learning with synthetic gradients.\nIn ICLR, 2020. 7\n[37] Yuqing Hu, Vincent Gripon, and Stéphane Pateux. Leveraging\nthe feature distribution in transfer-based few-shot learning. In\nICANN, 2021. 7\n[38] Kai Huang, Jie Geng, Wen Jiang, Xinyang Deng, and Zhe Xu.\nPseudo-loss conﬁdence metric for semi-supervised few-shot\nlearning. In ICCV, 2021. 6, 7\n[39] L. Jing and Y. Tian. Self-supervised visual feature learning\nwith deep neural networks: A survey. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 2021. 3, 9\n[40] Arman Kazemi, Shubham Sahay, Ayush Saxena, Moham-\nmad Mehdi Shariﬁ, Michael Niemier, and X. Sharon Hu. A\nﬂash-based multi-bit content-addressable memory with eu-\nclidean squared distance. In IEEE/ACM International Sympo-\nsium on Low Power Electronics and Design, 2021. 2\n[41] Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B.\nTenenbaum. Human-level concept learning through prob-\nabilistic program induction. Science, 2015. 1\n[42] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and\nStefano Soatto. Meta-learning with differentiable convex\noptimization. In CVPR, 2019. 1, 2, 6, 7, 8, 9\n[43] Wei-Hong Li, Xialei Liu, and Hakan Bilen. Improving task\nadaptation for cross-domain few-shot learning. arXiv preprint\narXiv:2107.00358, 2021. 4, 7, 8\n[44] Wei-Hong Li, Xialei Liu, and Hakan Bilen. Universal repre-\nsentation learning from multiple domains for few-shot classi-\nﬁcation. In ICCV, 2021. 8\n[45] Xinzhe Li, Qianru Sun, Yaoyao Liu, Qin Zhou, Shibao Zheng,\nTat-Seng Chua, and Bernt Schiele. Learning to self-train for\nsemi-supervised few-shot classiﬁcation. NeurIPS, 2019. 6, 7\n[46] Yann Lifchitz, Yannis Avrithis, and Sylvaine Picard. Few-\nshot few-shot learning and the role of spatial attention. In\n2020 25th International Conference on Pattern Recognition\n(ICPR), pages 2693–2700. IEEE, 2021. 7\n[47] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean conference on computer vision, pages 740–755.\nSpringer, 2014. 6\n[48] Lu Liu, William Hamilton, Guodong Long, Jing Jiang, and\nHugo Larochelle. A universal representation transformer\nlayer for few-shot image classiﬁcation. In ICLR, 2021. 8\n[49] Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho\nYang, Sung Ju Hwang, and Yi Yang. Learning to propagate la-\nbels: Transductive propagation network for few-shot learning.\nIn ICLR, 2019. 6\n[50] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew\nBlaschko, and Andrea Vedaldi.\nFine-grained visual clas-\nsiﬁcation of aircraft. arXiv preprint arXiv:1306.5151, 2013.\n6\n\n\n[51] Puneet Mangla, Nupur Kumari, Abhishek Sinha, Mayank\nSingh, Balaji Krishnamurthy, and Vineeth N Balasubrama-\nnian. Charting the right manifold: Manifold mixup for few-\nshot learning. In WACV, 2020. 1, 2\n[52] Cheng Perng Phoo and Bharath Hariharan. Self-training for\nfew-shot transfer across extreme task differences. In ICLR,\n2021. 8\n[53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. Learning transferable visual\nmodels from natural language supervision. In ICML, 2021. 1,\n4, 9\n[54] Sachin Ravi and Hugo Larochelle. Optimization as a model\nfor few-shot learning. In ICLR, 2017. 1, 2\n[55] Mengye Ren, Eleni Triantaﬁllou, Jake Snell Sachin Ravi,\nKevin Swersky, Joshua B. Tenenbaum, Hugo Larochelle, and\nRichard S. Zemel. Meta-learning for semi-supervised few-\nshot classiﬁcation. In ICLR, 2018. 1, 6\n[56] James Requeima, Jonathan Gordon, John Bronskill, Sebastian\nNowozin, and Richard E. Turner. Fast and ﬂexible multi-task\nclassiﬁcation using conditional neural adaptive processes. In\nNeurIPS, 2020. 8, 9\n[57] Pau Rodríguez, Issam Laradji, Alexandre Drouin, and Alexan-\ndre Lacoste. Embedding propagation: Smoother manifold for\nfew-shot classiﬁcation. In ECCV, 2020. 7\n[58] Tonmoy Saikia, Thomas Brox, and Cordelia Schmid. Op-\ntimized generic feature learning for few-shot classiﬁcation\nacross domains. In arXiv, 2020. 8\n[59] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical\nnetworks for few-shot learning. In NIPS, 2017. 1, 2, 3, 4, 6,\n7, 8, 9\n[60] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav\nGupta. Revisiting unreasonable effectiveness of data in deep\nlearning era. In ICCV, 2017. 2, 3\n[61] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H. S.\nTorr, and Timothy M. Hospedales. Learning to compare:\nRelation network for few-shot learning. In CVPR, 2018. 1, 8\n[62] Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin\nElizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-\nJia Li. Yfcc100m: The new data in multimedia research.\nCommun. ACM, 59(2):64–73, jan 2016. 2\n[63] Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B. Tenen-\nbaum, and Phillip Isola. Rethinking few-shot image classiﬁ-\ncation: a good embedding is all you need? In ECCV, 2020.\n1, 2\n[64] Eleni Triantaﬁllou, Hugo Larochelle, Richard Zemel, and\nVincent Dumoulin. Learning a universal template for few-\nshot dataset generalization. In ICML, 2021. 8\n[65] Eleni Triantaﬁllou, Tyler Zhu, Vincent Dumoulin, Pascal\nLamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles\nGelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo\nLarochelle. Meta-dataset: A dataset of datasets for learning\nto learn from few examples. In ICLR, 2020. 2, 3, 5, 7, 8\n[66] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray\nKavukcuoglu, and Daan Wierstra. Matching networks for one\nshot learning. In NeurIPS, 2016. 1, 3, 5\n[67] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona,\nand Serge Belongie. The caltech-ucsd birds-200-2011 dataset.\nTech. Report, 2011. 6\n[68] Yan Wang, Wei-Lun Chao, Kilian Q. Weinberger, and Laurens\nvan der Maaten. Simpleshot: Revisiting nearest-neighbor\nclassiﬁcation for few-shot learning, 2019. 1, 2\n[69] Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M\nNi. Generalizing from a few examples: A survey on few-\nshot learning. ACM Computing Surveys (CSUR), 53(3):1–34,\n2020. 1, 2\n[70] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.\nHow transferable are features in deep neural networks? In\nNIPS, 2014. 2, 3\n[71] Jianhong Zhang, Manli Zhang, Zhiwu Lu, and Tao Xiang.\nAdargcn: adaptive aggregation gcn for few-shot learning. In\nProceedings of the IEEE/CVF Winter Conference on Applica-\ntions of Computer Vision, pages 3482–3491, 2021. 7\n[72] X. Zhang, D. Meng, H. Gouk, and T. Hospedales. Shallow\nbayesian meta learning for real-world few-shot recognition.\nIn ICCV, 2021. 1, 2, 6\n\n\nPushing the Limits of Simple Pipelines for Few-Shot Learning:\nSupplemental Material\nIn this supplemental material, we present:\n• In Section 1, we include additional results for Table 1 in the main paper.\n• In Section 2, we include additional results for Table 1 and Table 4 in the main paper.\n• In Section 3, we investigate the impact of the hyper-parameters for the ﬁne-tuning phase.\n• In Section 4, we show the T-SNE plots before and after ProtoNet meta-training.\n1. Additional results for Meta-Dataset\nIn this section, we show a complete view of the results presented in Table 1 in the main paper, including the outcomes\nof different pre-training methods (see Table 1), the outcomes of meta-training on ImageNet domain (see Table 2), and the\noutcomes of meta-training on eight pre-speciﬁed domains (see Table 3).\nAs indicated in the main paper, our pipeline is named in a form of “P > M > F (backbone)”, where “P”, “M” and “F” are\ntaken from the ﬁrst letters of pre-training, meta-training and ﬁne-tuning respectively. In this section, we only examine the\npre-training and backbone architecture parts with meta-training ﬁxed to ProtoNet. As an example, in Table 2, we use “DINO >\nPN (ViT-small)” to denote the pipeline that uses DINO pre-training, ProtoNet meta-training with backbone architecture being\nViT-small.\nTo clarify the shorten notations in Table 1, Table 2 and Table 3, we make a list here:\n• DINO: self-distillation pre-training on ImageNet-1k dataset by [2].\n• BEiT: BERT pre-training on ImageNet-21k dataset by [1].\n• CLIP: Contrastive language-image pre-training on YFCC100M dataset by [3].\n• Sup21k: Supervised pre-training on ImageNet-21k dataset.\n• Sup1k: Supervised pre-training on ImageNet-1k dataset.\n• BEiT + Sup21k: BERT unsupervised pre-training ﬁrst on ImageNet-21k dataset and then using the labels of ImageNet-\n21k to ﬁne-tune the model.\n2. Additional results for miniImageNet and CIFAR-FS\nWe also evaluate different pre-training methods and backbones on miniImageNet and CIFAR-FS, which is shown in Table 4.\nWe do not include some of the results to the main paper because supervised pre-training on ImageNet is only useful to check\nthe upper bound performance.\n3. Ablation study on ﬁne-tuning’s hyper-parameters\nThere are three hyper-parameters for the ﬁne-tuning stage: the learning rate, the number of gradient descent steps and the\nprobability of switching on data augmentation for the support set. We show in Figure 1 that the dominant hyper-parameter is\nthe learning rate. From the results, we also see that the higher the probability of switching on data augmentation the better,\nwhile 50 gradient steps give relatively good performance with the right learning rate. Therefore, we ﬁx the probability to 0.9\nand let the numbers of steps to be 50 in the ﬁne-tuning phase.\n1\narXiv:2204.07305v1  [cs.CV]  15 Apr 2022\n\n\nINet\nOmglot\nAcraft\nCUB\nDTD\nQDraw\nFungi\nFlower\nSign\nCOCO\nAvg\nDINO (ViT-small)\n73.48\n54.33\n62.17\n85.37\n83.67\n60.59\n56.26\n94.45\n53.7\n54.58\n67.86\nDINO (ViT-base)\n74.85\n59.44\n55.36\n80.08\n84\n59.61\n56.65\n94.84\n51.81\n57.1\n67.374\nBEiT (ViT-base)\n17.12\n23.96\n17.21\n18.59\n39.79\n23.89\n13.69\n45.81\n16.16\n16.36\n23.258\nCLIP (ViT-base)\n60.66\n62.12\n54.08\n80.26\n76.51\n62.90\n30.76\n68.43\n47.33\n41.95\n58.5\nDINO (ResNet50)\n64.13\n52.51\n57.02\n62.63\n84.5\n60.78\n50.41\n92.18\n58.27\n55.43\n63.786\nCLIP (ResNet50)\n51.67\n44.16\n44.18\n70.2\n70.64\n47.88\n34.13\n87.97\n39.59\n41.63\n53.205\nSup21k (ViT-base)\n67.00\n37.02\n47.72\n82.9\n79.77\n52.25\n41.98\n95.7\n46.22\n53.46\n60.402\nBEiT + Sup21k (ViT-base)\n33.85\n23.95\n33.92\n52.07\n63.79\n32.60\n28.19\n67.3\n27.18\n29.65\n39.25\nSup1k (ViT-base)\n89.1\n60.71\n55.36\n79.8\n79.75\n61.28\n47.45\n88.44\n56.3\n57.20\n67.539\nSup1k (ResNet50)\n76.22\n47.31\n55.75\n76.40\n80.40\n51.26\n43.42\n85.48\n50.46\n57.10\n62.38\nTable 1. Pre-training results on Meta-Dataset – Comparison of different pre-training methods and backbone architectures.\nIn-domain\nOut-of-domain\nINet\nOmglot\nAcraft\nCUB\nDTD\nQDraw\nFungi\nFlower\nSign\nCOCO\nAvg\nDINO > PN (ViT-small)\n74.69\n56.91\n60.5\n85.04\n84.21\n61.54\n54.78\n94.57\n54.21\n57.35\n68.38\nDINO > PN (ViT-base)\n76.69\n62.2\n54.76\n81.58\n84.48\n60.64\n55.93\n95.14\n56.81\n60.27\n68.85\nCLIP > PN (ViT-base)\n76.03\n59\n65.75\n90.2\n83.08\n65.45\n53.2\n96.35\n58.65\n61.2\n70.891\nDINO > PN (ResNet50)\n67.08\n49.21\n58.46\n72.08\n85.01\n59.2\n50.53\n89.91\n55.44\n53.94\n64.086\nCLIP > PN (ResNet50)\n69.41\n60.72\n57.53\n83.66\n80.03\n55.58\n50.07\n93.39\n48.56\n50.14\n64.909\nSup21k > PN (ViT-base)\n85.88\n39.72\n52.03\n94.54\n83.42\n54.58\n57.06\n99.01\n47.74\n69.02\n68.3\nBEiT+Sup21k > PN (ViT-base)\n84.39\n60.54\n74.04\n95.66\n86.14\n65.24\n64.25\n99.19\n63.02\n69.91\n76.238\nSup1k > PN (ViT-base)\n90.48\n62.96\n54.89\n78.88\n80.02\n61.81\n45.52\n88.56\n55.61\n59.12\n67.785\nTable 2. Meta-training results on Meta-Dataset (ImageNet only) – Comparison of different pre-training methods and backbone architec-\ntures.\nIn-domain\nOut-of-domain\nINet\nOmglot\nAcraft\nCUB\nDTD\nQDraw\nFungi\nFlower\nSign\nCOCO\nAvg\nDINO > PN (ViT-small)\n73.54\n91.79\n88.33\n91.02\n81.64\n79.23\n74.2\n94.12\n54.37\n57.04\n78.528\nDINO > PN (ViT-base)\n73.55\n91.54\n89.73\n92.94\n81.52\n80.2\n78.28\n94.53\n53.65\n59.13\n79.507\nCLIP > PN (ViT-base)\n74.76\n92.26\n91.42\n93.55\n80.97\n80.8\n79.13\n95.64\n54.52\n56.8\n79.985\nDINO > PN (ResNet50)\n63.7\n85.91\n80.3\n81.67\n82.69\n72.84\n60.03\n91.75\n54.26\n50.67\n72.382\nCLIP > PN (ResNet50)\n64.86\n92.09\n89.19\n89.17\n71.67\n78.71\n76.15\n91.25\n51.1\n45.88\n75.007\nSup21k > PN (ViT-base)\n84.86\n85.71\n83.77\n95.89\n85.1\n78.47\n74\n99.17\n59.86\n67.57\n81.44\nBEiT+Sup21k > PN (ViT-base)\n81.96\n94.19\n91.62\n93.76\n81.3\n83.48\n81.76\n98.84\n58.83\n61.81\n82.755\nSup1k > PN (ViT-small)\n83.87\n91.22\n87.9\n89.2\n78.11\n78.7\n70.33\n94\n56.24\n57.16\n78.673\nSup1k > PN (ViT-base)\n89.75\n93.48\n91.15\n92.48\n78.52\n80.65\n75.97\n95.78\n53.47\n55.89\n80.714\nSup1k > PN (ResNet50)\n68.04\n86.17\n80.72\n80.48\n71.65\n70.78\n59.58\n84.33\n50.06\n50.29\n70.21\nNone > PN (ViT-small)\n37.25\n74.14\n45.25\n49.66\n61.49\n70.24\n43.23\n72.03\n39.33\n35.43\n52.805\nNone > PN (ResNet50)\n40.74\n90.67\n80.67\n68.88\n62.4\n75.96\n55.72\n75.37\n43.11\n35.49\n62.901\nTable 3. Meta-training results on Meta-Dataset – Comparison of different pre-training methods and backbone architectures.\n4. T-SNE plots: before and after meta-training\nBy using T-SNE visualization, We identify that the feature representation of DINO pre-training is already of high quality in\nmultiple domains. Three examples are shown in Figure 2, Figure 3 and Figure 4. In general, many semantic clusters have\nalready emerged, even though these domains where the clusters are sitting are not necessarily similar to ImageNet. This gives\na very good initialization to ProtoNet so that it can reﬁne the clusters to be much tighter. While the situation would be quite\ndifferent if we were training the ProtoNet from scratch, which are conﬁrmed by the no-pre-training results in Table 3. This can\nbe explained in the sense of K-means clustering, where a good initialization is always desired.\n\n\nminiImageNet\nCIFAR-FS\n5w1s\n5w5s\n5w1s\n5w5s\nDINO > PN (ViT-small)\n93.1\n98.0\n81.1\n92.5\nDINO > PN (ViT-base)\n95.3\n98.4\n84.3\n92.2\nCLIP > PN (ViT-base)\n93.1\n98.1\n85.3\n93.2\nDINO > PN (ResNet50)\n79.2\n92.0\n73.7\n84.0\nCLIP > PN (ResNet50)\n78.9\n92.2\n71.4\n82.6\nSup21k > PN (ViT-base)\n97.2\n99.2\n92.3\n96.7\nBEiT+Sup21k > PN (ViT-base)\n96.6\n99\n93.8\n97.5\nSup1k > PN (ViT-small)\n97.7\n99.4\n86.2\n93.6\nSup1k > PN (ViT-base)\n99.2\n99.8\n88.2\n94.3\nSup1k > PN (ResNet50)\n91.7\n97.4\n77\n87.6\nNone > PN (ViT-small)\n36.5\n49.1\n45.9\n59.8\nNone > PN (ResNet50)\n46.1\n60.3\n54.1\n68.4\nTable 4. miniImageNet & CIFAR-FS – Comparison of different pre-training methods and backbone architectures.\nProbability of switching on data augmentation\nAccuracy\n75\n80\n85\n90\n0.5\n0.7\n0.9\nsteps=50\nsteps=100\nsteps=200\nTraffic sign, lr = 0.001\nProbability of switching on data augmentation\nAccuracy\n75\n80\n85\n90\n0.5\n0.7\n0.9\nsteps=50\nsteps=100\nsteps=200\nTraffic sign, lr = 0.01\nProbability of switching on data augmentation\nAccuracy\n50\n55\n60\n65\n0.5\n0.7\n0.9\nsteps=50\nsteps=100\nsteps=200\nMSCOCO, lr = 0.001\nProbability of switching on data augmentation\nAccuracy\n50\n55\n60\n65\n0.5\n0.7\n0.9\nsteps=50\nsteps=100\nsteps=200\nMSCOCO, lr = 0.01\nFigure 1. Ablation study of ﬁne-tuning’s hyper-parameters – The experiments are done in the validation set of the trafﬁc sign domain\nand the MSCOCO domain with learning rate ﬁxed to either 0.001 or 0.01.\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n11\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1 1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n11\n1\n1\n1\n11\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n22\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n3\n3\n3\n3\n3\n3\n3\n3\n3\n33\n3 3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n55\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5 5\n5\n5\n55\n5\n555\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n6\n6\n6\n6\n6\n6\n66\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n66\n6\n6\n6\n6\n6\n66\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n88\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n88\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9 9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10 10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n11\n11\n1111\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n1212\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n1212\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n1313\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n1313\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13 13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n1313\n13\n13\n13\n13\n13 13\n13\n13\n13\n13\n13\n13\n13\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\nPre-training\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n00\n0\n0\n0\n0 0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n11\n1\n1\n1\n1\n1\n1\n1\n1\n1 1\n1\n1\n1\n1\n1\n11\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1 1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2 2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n22\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n3\n3 3\n3\n3\n3\n3\n3\n3\n3\n3\n3 3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n33\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n4\n4\n4\n4\n4\n4\n4\n4 4\n44\n4\n4\n4\n4\n4\n4\n4\n4\n44\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n44\n44\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n44\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n55\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n55\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n55\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n55\n5\n5\n5\n5\n5\n5\n5\n5\n5\n55\n5\n5\n5\n5\n5\n5\n5\n5\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6 6\n6\n6\n6\n6\n6\n66\n6\n6\n6\n6\n6\n6\n6\n6\n66\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n66\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n66\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n7\n77\n7\n7\n7\n7 7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7 7\n7\n7\n7 7\n7\n7\n7\n7\n7\n7\n77\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8 8\n8\n8\n8\n8 8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8 8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n99\n99\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n99\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9 9\n9\n9\n9\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n1010\n1010\n1010\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n1010\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n1010\n10\n10\n11\n11\n11\n11\n1111\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n1111\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11 11\n11 11\n11\n11 11\n11\n11\n11\n11\n11\n11\n11\n1111\n11 11\n1111\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n1111\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n1111\n11\n11\n11\n11\n11\n11\n11\n11\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n1212\n1212\n12\n12\n12\n1212\n12 12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n1212\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n1313\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13 13\n13\n13\n13\n13\n1313\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n1313\n13\n13\n13\n13\n13\n13\n1313\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n1313\n13\n13\n13\n13\n13\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n1414\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n1414\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n1414\n14\n14\n14\n14\n14\n14\n14\n14 14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14 14\n14\n14\n14\n14\n14\n141414\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\nMeta-training\nFigure 2. Aircraft domain\n\n\n0\n0\n0\n0\n00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n00\n0\n0\n0\n0\n0\n0\n0\n000\n0\n0\n0 0\n0\n00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1 1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n11\n1\n1\n2\n22\n2\n2\n2\n2\n22\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n22\n2\n2\n2\n2\n2\n2\n22\n2\n2\n2\n2\n2\n2\n2\n2\n22\n2\n2\n2\n22\n2\n2\n2\n2\n2\n2\n3\n33\n3\n3\n33\n3\n33\n3\n3\n3\n3\n3\n3 3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n33\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3 3\n3 3\n3\n3\n4\n4\n4\n4\n44\n4\n4\n4\n4\n4\n4\n4\n4\n4\n44\n4\n4\n44\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n44\n44\n4\n4\n4\n4\n4\n4\n4\n4\n44\n4\n4\n4\n4\n4\n4\n4 4\n4\n4\n5\n5\n5\n5\n55\n55 5\n5\n5 5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5 5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n55\n5 5\n5\n5\n5\n5 5\n5\n5\n5\n5\n5\n5\n5\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6 6\n6\n6\n6\n6\n6 6\n66\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n66\n6\n6\n6\n6\n6\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n77\n7\n7\n7\n77\n7\n7\n7\n7\n7\n7 7\n7\n7\n7\n7\n7\n7\n7\n777\n7\n7\n7\n7\n7\n7\n7\n7\n77\n7\n7 7\n7\n7\n7\n7\n7\n7\n7\n7 7\n7\n7\n7\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n88\n8\n8\n8\n8\n8\n8 8\n8\n8\n8888\n8\n8\n8\n8\n8\n88\n8\n8\n8\n8\n8\n8\n8 8\n88\n8\n8\n8\n8\n8\n88\n88\n8\n88\n8\n8\n88\n8\n9 9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n99\n9\n9\n99\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n99\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n10\n10\n10\n10\n10\n10\n10 10\n10\n10\n1010\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n1010\n1010\n10\n10\n10\n1010\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n11\n11\n11 11\n11\n1111\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n1111\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11 11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12 12\n12\n12\n12\n12\n12\n12\n12\n1212\n12\n12\n12\n12\n12\n12\n12\n12\n12\n1212\n12\n12\n1212\n12\n12\n13\n13\n13\n13\n13\n13\n13\n13\n13\n131313\n13 13\n13\n13\n13\n13\n13\n13\n13\n131313\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13 13\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n1414\n14\n14\n14\n14\n14\n14\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15 15\n15\n15\n15\n15\n15\n15\n15\n1515\n15\n15\n15\n15\n15\n15\n15\n15\n15 15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15 15\n15\n15\n15\n1515\n15\n15\n15\n15\n15\n15\n15\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n1616\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n1616\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17 17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n18\n18\n18\n18\n18\n1818\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18 18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18 18\n18\n181818\n18\n18\n18\n18\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n20\n2020\n20\n20\n20\n20\n20\n20\n20\n2020\n20\n20\n20\n20\n2020\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n2020\n20\n20\n20\n20\n20\n2020\n20\n20\n20\n20\n20\n20 20\n20\n20\n20\n20\n20\n20 20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n2121\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21 21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n2222\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n2323\n2323\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n2424\n24\n2424\n24\n24\n24\n24\n25\n25\n25\n2525\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n2525\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n2525\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25 25\n25\n25\n25\n25\n25\n25\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n2626\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n27\n2727\n2727\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27 27\n27\n28\n28\n28\n28\n28\n28\n28\n2828\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28 28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n2828\n28\n28\n28\n28\n28\n28\n28\n28\n28\n29\n29\n29\n29 29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n2929\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n2929\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\nPre-training\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0 0\n0\n0\n0\n0\n0\n0\n0\n0\n00\n0\n0 00\n0\n0\n0\n000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n00\n0\n0\n00\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n11\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1 1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n11\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n11\n1\n1\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n22\n2\n22\n2\n22\n2\n22\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n22\n2\n2\n2\n2\n22\n22\n2\n22\n2\n2\n2\n2\n2\n3 3\n3\n3 3\n3\n3\n3\n33\n3\n33\n3\n3\n3 3\n3\n3\n3 3\n3\n3\n3\n33\n3\n33 3\n3\n3\n3\n3\n3\n3\n3\n3\n33\n3\n3\n3\n3\n3\n3 3\n3\n3\n3\n3\n3\n3\n3\n33\n3\n3\n3\n3\n4 4\n4\n44\n44\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n44\n4\n4\n4 4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n444\n4\n4\n4\n4 4\n4\n4\n4\n4\n4\n4\n44\n44\n4 4\n44\n5\n5\n5\n5\n5\n55\n5\n55\n5\n5\n5\n5\n5\n5\n55\n5\n5\n5\n5\n5\n5\n5\n5 5\n55\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n55\n5\n5\n5\n555\n5\n5\n6\n6\n6\n66\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n66\n6\n6\n6\n6\n6\n6\n6\n6\n6\n66\n66\n66\n6\n6\n6\n6\n66\n6\n6\n6\n6\n6\n6\n66\n6\n6\n66\n6\n6\n6\n6\n6\n6\n6\n66\n7\n7\n7\n7\n7\n7\n7\n77\n7\n7\n77\n7\n7\n7\n77\n77\n7\n7 7\n7\n77\n7\n7\n7\n7\n7\n7\n777\n7\n7\n7\n7\n7\n7\n7\n7\n77777\n7\n7\n7\n7\n7\n7\n7\n777\n7\n7\n88\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n88\n8\n8\n8 8\n8\n8\n8\n8\n8\n88\n8\n88\n88\n8\n888\n8\n8\n8\n88\n8\n8\n88\n8\n88\n8\n88\n88\n9\n99\n9\n9\n9\n9\n9\n9 9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n99\n9\n9\n99\n9\n9\n9\n99\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n10\n10\n10\n1010\n10\n1010\n10\n10\n10\n10\n10\n1010\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n1010\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n1010\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10 10\n10\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n1111\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n1111\n11\n11\n1111\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n1212\n12\n12\n12\n12\n12\n12\n13\n13\n13\n13\n13\n13\n13\n13\n13\n1313\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n1313\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n1313\n13\n13\n13\n13\n1313\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14 14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n1414\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14 14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n15\n15 15\n15\n15\n15\n1515\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n1515\n15\n15\n1515\n1515\n15\n15\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n1616\n16\n16\n16\n16\n16\n16\n1616\n16\n16\n16\n1616\n16\n16\n16\n16\n16 16\n16\n1616\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n1717\n17\n17\n17\n17\n171717\n17\n17\n17\n17\n17\n17\n17\n1717\n1717\n17\n17 17\n17\n17 17\n17\n17\n17\n17 17\n17\n17\n17\n17\n17\n17\n17\n1717\n17\n1717\n17\n17\n17\n17\n17\n18\n18\n18\n18\n1818\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n1818\n18\n18\n18\n18\n18\n18\n18\n18\n18\n1818\n18\n1818\n18\n18\n18\n18\n18\n18\n18\n1818\n1818\n18\n18\n18\n18\n1818\n18\n18\n18\n18\n18\n18\n18\n18\n18\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n1919\n19\n19\n1919\n19\n19\n19\n19\n19\n19\n19\n19\n19\n1919\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n2020\n20\n20\n20\n20\n2020\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n21\n21\n2121\n21\n21\n21\n21\n2121\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21 21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21 21\n21\n21\n21\n2121\n21\n21\n21\n21\n21 21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n2222\n22\n22\n2222\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n2222\n22\n22\n22\n22\n23\n23\n23\n23\n2323\n23\n23\n2323\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n2324\n24\n24\n24\n24\n24\n2424\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24 24\n24\n24\n24\n24\n24 24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n2424\n24\n24\n24\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n2525\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n2525\n2525\n25\n25\n25\n25\n25\n25\n25\n25\n252525\n25\n25\n25\n25\n25\n2525\n25\n25\n25\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n2626\n26\n26\n26\n26\n26\n26\n2626\n26\n26\n26\n26 26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n2727\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n2727\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27 27\n27\n27\n27\n27\n27\n27\n2727\n27\n27\n2727\n27\n27\n2727\n27\n2727\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n2727\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28 28\n28\n28\n28\n28\n28\n28\n28\n28\n2828\n28\n28\n28\n28\n28\n28\n28\n28\n2828\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28 28\n28\n2929\n29\n29\n29 29\n29\n29\n2929\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n2929\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n2929\n29\n29\nMeta-training\nFigure 3. CUB domain\n\n\n00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n00\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n11\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n2 2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2 2\n2\n2\n2\n2\n2\n3\n3\n3\n3\n3\n3\n33\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n55\n5\n5\n5\n5 5\n5\n55\n5\n5\n55\n5\n55\n5\n5\n5\n66\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6 6\n6\n6\n7 7\n7\n7\n7\n7 7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n8\n8\n8 8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n88\n8\n8\n8\n8\n8\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n99\n9\n10\n10\n10\n10\n10\n10\n1010\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n1111\n11\n11\n11\n11\n11\n11\n11\n11\n11\n1111\n11\n11\n11\n11\n11\n11 11\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n1414\n1414\n14\n14\n14\n14\n15\n15\n15\n15\n15\n1515\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n16\n16\n16\n16\n16\n16\n1616\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23 23\n23\n23\n24\n24\n2424\n24\n24\n24\n24\n2424\n24\n24\n24\n24\n24\n24\n2424\n24\n24\n25\n25\n25\n25\n25\n25\n25\n25\n25\n2525\n25\n25\n25\n25\n2525\n25\n25\n25\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n2727\n27\n2727\n27\n27\n27\n27\n27\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n2828\n28\n28\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n2929\n29\n2929\n29\n29\n29\n29\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n31\n31\n31\n31 31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n33\n33\n33\n33\n33\n33\n33\n33\n33\n33\n33\n33\n33\n33\n33\n33\n33\n33\n33\n33\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35 35\n35\n35\n35\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n37\n37\n37\n37\n37\n37\n37\n37\n37\n37\n37\n37\n37\n37\n37\n37\n37\n37\n37\n37\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n3838\n38\n38\n38\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n3939\n39\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n41\n41\n41\n41\n41\n41\n41\n41\n41\n41\n41\n41\n41\n4141\n41\n41\n41\n41\n41\n42\n42\n42\n42\n42 4242\n42\n42\n42\n42\n42\n42\n42\n42\n42\n42\n42\n4242\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n4343\n43\n43\n43\n43\n43\n44\n44\n44\n44\n44\n4444\n44\n44\n44\n44\n44\n44\n44\n44\n4444\n44\n44\n44\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n46\n46\n46\n46\n46\n46\n46\n46\n46\n46\n46\n46\n4646\n46\n46\n46\n46\n46\n46\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n4747\n47\n47\n47\n47\n47\n47\n47\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n50\n50\n50\n5050\n50\n50\n50\n50\n50\n50\n50\n50\n50\n50\n50\n50\n50\n50\n50\n51\n51\n51\n51\n51\n51\n51\n51\n51\n51\n51\n51\n51\n51\n51\n51\n51\n51\n51\n51\n52\n52\n52\n5252\n52\n5252\n5252\n52\n52\n5252\n52\n52\n52\n52\n52\n52\n53\n53\n53\n53 53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n56\n56\n56\n56\n56\n56\n56\n56\n56\n56\n56\n56\n56\n56\n56\n56\n56\n56\n56\n56\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n5757\n57\n57\n57\n57\n58\n58\n58\n58 58\n58\n58\n58\n58\n58\n58\n58\n58\n58\n58\n58\n58\n58\n58\n58\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n60\n60\n60\n60\n60\n60\n60\n60\n60\n60\n60\n6060\n60\n60\n60\n60\n60\n60\n60\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n62\n62 62\n62\n62\n62\n62\n62\n62\n62\n62\n62\n62\n62\n62\n62\n62\n62\n62\n62\n63\n63\n63\n63\n63\n63\n63\n63\n6363\n6363\n63\n63\n63\n63\n63\n63\n63\n63\n64\n64\n64\n64\n64\n64\n6464\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n65\n65\n65\n65\n65\n65\n65\n65\n65\n65\n65\n65\n65\n65\n65\n65\n65\n65\n65\n65\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n67\n67\n67\n67\n67\n67\n67\n67\n67\n67\n67\n67\n67\n67\n67\n67\n67\n67\n67\n67\n68\n68\n68\n68\n68\n68\n68\n68\n68\n68\n68\n68\n68\n68\n68\n68\n68\n68\n68\n68\n69\n69\n69\n69\n69\n69\n69\n69\n69\n69\n69\n69\n69\n69\n69\n69\n69\n69\n69\n69\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n71\n71\n71\n71\n71\n71\n71\n71\n71\n71\n71\n7171\n7171\n71\n71\n71\n71\n71\n72\n72\n72\n72\n72\n72\n72\n72\n72\n72\n7272\n72\n72\n72\n72\n72\n72\n72\n72\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n74\n74\n74\n74\n74\n74\n74\n74\n74\n74\n74\n74\n74\n74\n74\n74\n74\n74\n74\n74\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n7575\n75\n75\n75\n75\n75\n76\n76\n76\n76\n76\n76\n76\n76\n76\n76\n76\n76\n76\n76\n76\n76\n76\n76\n76\n76\n77\n77\n7777\n77\n77\n77\n77\n77\n7777\n7777\n77\n77\n77\n77\n7777\n77\n78\n78\n78\n78\n78\n78\n78\n78\n78\n78\n78\n78\n78\n78\n78\n78\n78\n78\n78\n78\n79\n79\n79\n79\n79\n79\n797979\n79\n79\n79\n79\n79\n79\n79\n7979\n79\n79\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\nPre-training\n0\n0\n0\n0\n00\n0\n0 0\n00 0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n11\n1\n1\n1 1\n1\n1\n1 1\n1 1\n1\n1\n1\n11\n22 2\n2\n2\n2\n2\n2\n22\n222\n22 2 2\n222\n3\n3\n3\n3\n333\n3\n3\n33\n3\n3\n3\n3\n3\n3\n3\n3\n3\n44\n444\n4\n44\n4\n4\n444\n444\n4\n4\n4\n4\n55\n55\n555\n5555\n5\n5\n55\n55\n55\n5\n6\n6\n6\n66\n666\n6\n6\n6\n6 6\n6\n6\n6\n6\n666\n7\n7\n7\n7\n7\n7\n77\n7\n7\n777\n77 7\n7\n77\n7\n8\n8\n8\n8 8\n8\n8\n8\n8\n88\n8\n88\n88\n8\n8\n88\n9\n99\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n9\n99\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n1010\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n12\n12\n12\n12\n12\n12\n1212\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n13\n1313\n13\n13\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n16\n17\n17\n17\n17\n17\n17\n17\n17\n1717\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\n18\n18\n18\n18\n18\n18\n18\n1818\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n19\n20\n20\n20\n20\n20\n2020\n20\n20\n20\n20\n20\n20\n20\n20\n20\n202020\n20\n21\n21\n21\n21\n21\n21\n21\n21\n2121\n21\n21\n21\n21\n21\n21\n21\n21\n21\n21\n22\n2222\n22\n22\n22\n22\n22\n22\n22\n22\n22\n22\n2222\n22\n22\n22\n22\n22\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n23\n2323\n23\n23\n23\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n24\n2424\n24\n24\n24\n24\n24\n24\n24\n25\n25\n25\n2525\n25\n25\n25\n25 25\n25\n2525\n25\n25\n2525\n25\n25\n25\n26\n26\n26\n26\n26\n2626\n26\n26\n26\n26\n26\n26\n26\n26\n2626\n26\n26\n26\n27\n27\n27\n27\n27\n27\n27\n2727\n27\n27\n27\n27\n27\n27\n27\n27\n27\n2727\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n28\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n29\n2929\n30\n30\n3030\n30\n30\n3030\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n3131\n31\n31\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n3232\n32\n32\n32\n32\n32\n33\n33\n33\n33\n33\n3333\n33\n33\n33\n33\n33\n33 33\n3333\n33\n33\n3333\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n3535\n3535\n35\n35\n35\n35\n35\n36\n36\n3636\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n36\n37\n37\n37\n37\n37\n37\n3737\n3737\n37\n37\n37\n37\n3737\n37\n3737\n37\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n38\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n39\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n41\n41\n41\n41\n41\n41\n41\n4141\n41\n41\n41\n41\n41\n41\n41\n41\n4141\n41\n42\n42\n42\n4242\n42\n42\n42\n42\n42\n42\n42\n42\n42\n42\n4242\n42\n42\n42\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n43\n44\n44\n44\n44\n44\n44\n44\n44\n44\n44\n44\n44\n4444\n44\n44\n44\n44\n44\n44\n45\n45\n4545\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n46\n46\n46\n46\n46\n46\n46\n46\n46\n46\n46\n46\n46\n46\n46\n46\n46\n46\n4646\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n48\n48\n48\n48\n48\n4848\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n49\n49\n49\n49\n4949\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n50\n50\n50\n50\n50\n50\n50\n50\n50\n50\n5050\n50\n50\n50\n50\n50\n50\n50\n50\n51\n51\n51\n5151\n51\n51\n5151\n51\n51\n51\n51\n51\n51\n51\n51\n51\n5151\n52\n52\n52\n52\n52\n52\n52\n52\n52\n52\n52\n52\n52\n52\n52\n52\n52\n52\n52\n52\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n53\n54\n5454\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n54\n55\n55\n55\n55\n5555\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n55\n56\n56\n56\n5656\n56\n56\n5656\n56\n56\n56\n56\n5656\n56\n56\n56\n56\n56\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n58\n58\n58\n58\n58\n58\n58\n58\n58\n58\n58\n58\n58\n5858\n58\n58\n5858\n58\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n59\n5959\n59\n59\n59\n60\n60\n60\n60\n60\n60\n6060\n60\n60\n60\n60\n60\n60\n60\n60\n6060\n6060\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n61\n6161\n61\n61\n61\n62\n62\n62\n62\n62\n62\n62\n62\n62\n62\n62\n62\n62\n6262\n62\n62\n62\n62\n62\n63\n63\n63\n6363\n63\n63\n63\n6363\n63\n63\n63\n63\n63\n63\n63\n63\n63\n63\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n65\n65\n65\n65\n65\n65\n65\n65\n65\n6565\n65\n65\n65\n65\n65\n65\n65\n65\n65\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n66\n67\n67\n67\n67\n67\n67\n6767\n67\n67\n67\n6767\n6767\n67\n67 67\n67\n67\n68\n68\n68\n68\n68\n68\n6868\n68\n68\n68\n68\n6868\n68\n68\n68\n68\n68\n68\n69\n69\n69\n6969\n69\n69\n6969\n69\n69\n69\n69\n69\n69\n69\n69\n69\n69\n69\n7070\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n70\n7070\n70\n71\n71\n71\n71\n71\n71\n71\n71\n71\n71\n71\n7171\n71\n71\n7171\n71\n71\n71\n72\n72\n72\n72\n72\n72\n72\n72\n7272\n72\n72\n72\n72\n72\n72\n72\n72\n72\n72\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n73\n74\n74\n74\n74\n74\n74\n74\n74\n7474\n74\n74\n74\n74\n74\n74\n74\n74\n74\n74\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n76\n76\n76\n76\n76\n76\n76\n76\n76\n7676\n7676\n76\n76\n7676\n76\n76\n76\n77\n77\n77\n77\n77\n77\n77\n77\n77\n77\n77\n77\n77\n77\n77\n77\n77\n77\n77\n77\n78\n78\n78\n78\n78\n78\n78\n78\n78\n78\n7878\n78\n78\n7878\n78\n78\n78\n78\n79\n79\n79\n79\n79\n79\n79\n79\n79\n79\n7979\n79\n7979\n79\n79\n79\n79\n79\n80\n80\n80\n80\n80\n80\n8080\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\n80\nMeta-training\nFigure 4. Omniglot domain\n\n\nReferences\n[1] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. In ICLR, 2022. 1\n[2] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in\nself-supervised vision transformers. In ICCV, 2021. 1\n[3] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela\nMishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In\nICML, 2021. 1\n"
}