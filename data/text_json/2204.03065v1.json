{
  "filename": "2204.03065v1.pdf",
  "num_pages": 21,
  "pages": [
    "The Self-Optimal-Transport Feature Transform\nDaniel Shalam and Simon Korman\nUniversity of Haifa, Israel\nAbstract. The Self-Optimal-Transport (SOT) feature transform is de-\nsigned to upgrade the set of features of a data instance to facilitate\ndownstream matching or grouping related tasks. The transformed set\nencodes a rich representation of high order relations between the in-\nstance features. Distances between transformed features capture their\ndirect original similarity and their third party ‘agreement’ regarding sim-\nilarity to other features in the set. A particular min-cost-max-flow frac-\ntional matching problem, whose entropy regularized version can be ap-\nproximated by an optimal transport (OT) optimization, results in our\ntransductive transform which is efficient, differentiable, equivariant, pa-\nrameterless and probabilistically interpretable. Empirically, the trans-\nform is highly effective and flexible in its use, consistently improving\nnetworks it is inserted into, in a variety of tasks and training schemes.\nWe demonstrate its merits through the problem of unsupervised cluster-\ning and its efficiency and wide applicability for few-shot-classification,\nwith state-of-the-art results, and large-scale person re-identification.\n1\nIntroduction\nIn this work, we reassess the design and functionality of features for instance-\nspecific problems. In such problems, typically, features computed at test time\nare mainly compared relative to one another, and less so to the features seen\nat training time. For such problems the standard practice of learning a generic\nfeature extractor during training and applying it at test time might be sub-\noptimal.\nWe aim at finding training and inference schemes that take into account\nthese considerations, being able to exploit large corpuses of training data to\nlearn features that can easily adapt, or be relevant, to the test time task. Our\napproach to doing so will be in the form of a feature transform that jointly re-\nembeds the set of features of an instance in a way that resembles how recently\npopular self-attention mechanisms and Transformers [29,22,26,16] re-embed sets\nof features.\nBeing at the low-to-mid-level of most relevant architectures, advances in such\nfeature re-embeddings have a direct impact and wide applicability in instance-\nspecific problems such as few-shot classification [30], clustering [37], patch match-\ning [19] and person re-identification [43], to name but a few.\nThe general idea of the Self-Optimal-Transport (SOT) feature transform that\nwe propose is depicted and explained in Fig. 1, as part of the general design of\nnetworks that work on sets which we illustrate in Fig. 2.\n",
    "2\nDaniel Shalam and Simon Korman\nFig. 1:\nThe SOT transform: Its input is a set of n d-dimensional features (each shown\nas a horizontal gray rectangle, and as a colored point in the input embedding space where\ncolor depicts class label or equivalent). Processing is as follows: The unit length (normalized)\nfeatures are arranged in an n × d matrix for computing a pairwise n × n cosine similarity\nmatrix S. Then, the transport-plan matrix W (given a specific OT instance that depends\non S) is computed using several Sinkhorn [7] iterations. Finally, the transformed output\nfeatures are basically the rows of the matrix W. As we claim and observe in real results, the\nfeatures are re-embedded in a way that is consistently superior for downstream grouping\nand matching tasks (observed the better formation of the embedded points, e.g. towards\napplying a linear classifier or an off-the-shelf clustering procedure).\n1.1\nOverview\nWe are given an instance of some inference problem, in the form of a set of n\nitems {xi}n\ni=1, represented as vectors in RD, for a fixed dimension D. A generic\nneural-network (Fig. 2 Left) typically uses a feature embedding (extractor) F :\nRD →Rd (with d ≪D), which is applied independently on each input item,\nto obtain a set of features V = {vi}n\ni=1 = {F(xi)}n\ni=1. The features V might be\nof high quality (concise, unique, descriptive), but are limited in representation\nsince they are extracted based on knowledge acquired for similar examples at\ntrain time, with no context of the test time instance that they are part of.\nWe adapt a rather simple framework (Fig. 2 Right) in which some transform\nacts on the entire set of instance features. The idea is to jointly process the set\nof features to output an updated set (one for each input feature), that re-embeds\neach feature in light of the joint statistics of the entire instance. The proposed\nfeatures transform can be seen as a special case of an attention mechanism\n[29] specialized to features of instance-specific tasks, with required adaptations.\nTechniques developed here borrow from and might lend to those used in set-to-\nset [44,42,25], self-attention [29,26] and transformer [22,16] architectures.\n1.2\nContributions\nWe propose a parameter-less transform T, which can be used as a drop-in addi-\ntion that can convert a conventional network to an instance-aware one (e.g. from\nFig. 2 Left to Right). We propose an optimal-transport based feature transform\nwhich is shown to have the following attractive set of qualities. (i) efficiency:\nhaving real-time inference; (ii) differentiability: allowing end-to-end training of\nthe entire ‘embedding-transform-inference’ pipeline of Fig. 2 Right; (iii) equiv-\nariance: ensuring that the embedding works coherently under any order of the\n",
    "The Self-Optimal-Transport Feature Transform\n3\nFig. 2: Generic designs of networks that act on sets of inputs. These cover relevant\narchitectures, e.g. for few-shot-classification and clustering. Left: A generic network for pro-\ncessing a set of input items typically follows the depicted structure: (i) Each item separately\ngoes through a common feature extractor F. (ii) The set of extracted features is the input\nto a downstream task processing module G. ; Right: A more general structure in which the\nextracted features undergo a joint processing by a transform T. Our SOT transform (as well\nas other attention mechanisms) is of this type and its high-level design (within the ‘green’\nmodule) is detailed in Fig. 1.\ninput items; (iv) capturing relative similarity: The comparison of embedded vec-\ntors will include both direct and indirect (third-party) similarity information be-\ntween the input features; (v) probabilistic interpretation: each embedded feature\nwill encode its distribution of similarities to all other features, by conforming to\na doubly-stochastic constraint; (vi) instance-aware dimensionality: embedding\ndimension (capacity) is adaptive to input size (complexity).\nWe provide a detailed analysis of our method and show its flexibility and ease\nof application to a wide variety of tasks, by incorporating it in leading methods of\neach kind. A controlled experiment on unsupervised clustering is used to verify its\nperformance, with a detailed analysis. For few-shot-classification we perform an\nextensive comparison to existing work on several benchmarks, showing that SOT\nachieves new state-of-art results. Finally, we show that SOT is easily applicable\nto large-scale benchmarks by using the person re-identification task, for which\nit consistently improves state-of-art networks that it is incorporated into.\n2\nRelated Work\n2.1\nRelated Techniques\nSet-to-set or set-to-feature functions Our method can clearly be categorized\nalong with recent techniques that act jointly on a set of items (typically fea-\ntures) to output an updated set (or a single feature), which are typically used\nfor downstream inference tasks on the items individually, or as a set. The pio-\nneering Deep-Sets [44] formalized fundamental requirements from architectures\nthat process sets. Point-Net [27] presented an influential design that learns lo-\ncal and global features on 3D point-clouds, while Maron et.al. [25] study layer\n",
    "4\nDaniel Shalam and Simon Korman\ndesigns that approximate equivariant and invariant functions. Unlike the pro-\nposed SOT transform, the joint processing in these methods is very limited,\namounting to (Siamese) weight-sharing between separate processes and simple\njoint aggregations like average pooling.\nSelf-Attention The introduction of Relational Networks [32] and transform-\ners [38] and their initial applications in vision models [29] have lead to a surge of\nfollowing successful works [16], many of which are dedicated to few-shot-learning,\nsuch as ReNet [15], DeepEMD [45] and FEAT[42]. Different from these methods,\nSOT is parameterless, and hence can work at test-time on any pre-trained net-\nwork. In addition, SOT is the only method that provides an explicit probabilistic\nglobal interpretation of the instance data.\nOptimal Transport Optimal transport (OT) problems are tightly related to mea-\nsuring and calculating distances between distributions or sets of features. In [7]\nCuturi popularized the Sinkhorn algorithm which is a simple, differentiable and\nfast approximation of entropy-regularized OT problems. The Set transformer [22]\nuses an OT-based clustering algorithm, SuperGlue [33] uses OT in an end-to-\nend manner for feature-point matching, and many state-of-the-art methods in\nfew-shot learning, which we review next, have adopted the Sinkhorn algorithm\nto model relations between features and class representations. The differentia-\nbility and efficiency of regularized OT solvers has recently been shown useful in\nrelated domains, to derive a differentiable ‘top-k’ operator [41] or for style trans-\nfer applications, by viewing styles as a distributions between which distances\nare approximated [18]. In this work we focus on self applications of OT, which\nenables concise modelings of the relative similarities within a set of items.\n2.2\nFew-Shot-Classification (FSC)\nFew-Shot-Classification [39] is a branch of few-shot-learning in which a classifier\nneeds to learn to recognize classes unseen given a limited number of labeled\nexamples. A FSC task is a self-contained instance that includes both support\n(labeled) and query (unlabeled) items, hence is a clear instance-specific setup\nwhich SOT can handle.\nSome leading FSC approaches follow the meta-learning (or “learn-to-learn”)\nprinciple in which the training data is split into tasks (or episodes) mimicking\nthe test time tasks to which the learner is required to generalize. The celebrated\nMAML [10] “learns to fine-tune” by learning a network initialization from which\nit can adapt to a novel set of classes with very few gradient update steps on the\nlabeled examples. In ProtoNet [34], a learner is meta-trained to predict query\nfeature classes, based on distances from support (labeled) class-prototypes in the\nembedding space. The trainable version of SOT is a meta-learning algorithm,\nbut unlike the above, it is transductive (see ahead) and exploits the task items as\na set, while directly assessing the relative similarity relations between its items.\nSubsequent works [5,9] have questioned the benefits of meta-learning, advo-\ncating the standard transfer learning procedure of fine-tuning pre-trained net-\nworks. In particular, they demonstrate the advantages of using larger and more\n",
    "The Self-Optimal-Transport Feature Transform\n5\npowerful feature-encoding architectures, as well as the employment of trans-\nductive inference, which fully exploits the data of the inference task, including\nunlabeled images. As mentioned, SOT is a purely transductive method, but it is\nsignificantly more flexible in its assumptions, since the transform is based on a\ngeneral probabilistic grouping action. It does not make any assumptions on (nor\ndoes it need to know) the number of classes and the number of items per class\nin an instance.\nMore recently, attention mechanisms were shown to be effective for FSC. We\nhave reviewed some relevant works of this line in the previous section.\nFinally, a large number of works have adopted the Sinkhorn Algorithm [7] as\na parameterless unsupervised classifier that computes fractional matchings be-\ntween query embeddings and class centers. Many leading FSC works use this ap-\nproach, including Laplacian-Shot [50], CentroidNet [13] and PT-MAP [12]. The\ncurrent state-of-the-art is set by the recent Sill-Net [46], which augments training\nsamples with illumination features that are separated from the images in fea-\nture space and by PT-MAP-sf [6], who propose a DCT-based feature embedding\nnetwork, encoding detailed frequency-domain information that complements the\nstandard spatial domain features. Both methods are based on PT-MAP [12].\nSOT uses Sinkhorn to solve an entirely different OT problem - that of matching\nthe set of features to itself, rather than against class representations. Never-\ntheless, SOT can be incorporated into these methods, immediately after their\nfeature extraction stage.\n2.3\nUnsupervised Clustering and Person Re-Identification (Re-ID)\nThese domains are not at the focus of this work therefore we only briefly give\nsome useful pointers for the sake of brevity.\nUnsupervised image clustering is an active area of research, with standard-\nised evaluation protocols (from Cifar-10 [20] to different subsets of ImageNet [8]).\nProminent works in this area include Deep Adaptive Clustering (DAC) [4], In-\nvariant Information Clustering (IIC) [14] and SCAN [37]. Clustering has recently\ngained popularity as a means for self-supervision in feature learning, showing\nexcellent results on unsupervised image classification. See for example Deep-\nCluster [2] and SWAV [3]. Clustering is a clear case instance specific problem,\nsince most information is relative and unrelated directly to other training data.\nOur transform can hence be used to upgrade the feature representation quality.\nWe chose the Re-ID application as another instance-specific problem, which\nfrom our point of view differs from the others considered in two main aspects\nwhich we find attractive: (i) The tasks are of larger scale - querying thousands\nof identities against a target set of (tens of) thousands. (ii) The data is much\nmore real-world compared to the carefully curated classification and clustering\ntasks. See [43] for an excellent recent and comprehensive survey on the topic.\n",
    "6\nDaniel Shalam and Simon Korman\n3\nMethod\nAssume we are given a task which consists of an inference problem over a set\nof n items {xi}n\ni=1, where each of the items belongs to a space of input items\nΩ⊆RD. The inference task can be modeled as fθ({xi}n\ni=1), using a learned\nfunction fθ, which acts on the set of input items and is parameterized by a set\nof parameters θ.\nTypically, such functions combine an initial feature extraction stage that is\napplied independently to each input item, with a subsequent stage of (separate\nor joint) processing of the feature vectors (see Fig. 2 Left or Right, respectively).\nThat is, the function fθ takes the form fθ({xi}n\ni=1) = Gψ({Fϕ(xi)}n\ni=1), where\nFϕ is the feature extractor (or embedding network) and Gψ is the task inference\nfunction, parameterized by ϕ and ψ respectively, where θ = ϕ ∪ψ.\nThe feature embedding F : RD →Rd, usually in the form of a neural-network\n(with d ≪D), could be either pre-trained, or trained in the context of the task\nfunction f, along with the inference function G.\nFor an input {xi}n\ni=1, let us define the set of features {vi}n\ni=1 = {F(xi)}n\ni=1. In\nthe following, we consider these sets of input vectors and features as real-valued\nrow-stacked matrices X ∈Rn×D and V ∈Rn×d.\nWe suggest a novel re-embedding of the feature set V, using a transform that\nwe denote by T, in order to obtain a new set of features W = T(V), where W ∈\nRn×n. The new feature set W has an explicit probabilistic interpretation, which\nis specifically suited for tasks related to classification, matching or grouping of\nitems in the input set X. In particular, W will be a symmetric, doubly-stochastic\nmatrix, where the entry wij (for i ̸= j) gives the probability that items xi and\nxj belong to the same class or cluster.\nThe proposed transform T : Rn×d →Rn×n (see Fig. 1) acts on the original\nfeature set V as follows. It begins by computing the squared Euclidean pairwise\ndistances matrix D, namely, dij = ||vi −vj||2, which can be computed efficiently\nas dij = 2(1 −cos(vi, vj)) = 2(1 −vi · vT\nj ), assuming that the rows of V are unit\nnormalized. Or in a compact form, D = 2(1 −S), where 1 is the all ones n × n\nmatrix and S = V · VT is the cosine similarity matrix of V.\nW will be computed as the optimal transport (OT) plan matrix between the\nn-dimensional all-ones vector 1n and itself, under the cost matrix D∞, which is\nthe distance matrix D with a very (infinitely) large scalar replacing each of the\nentries on its diagonal (which were all zero). Explicitly, let D∞= D + αI, where\nα is a very (infinitely) large constant and I is an n × n identity matrix.\nW is defined to be the doubly-stochastic matrix1 that is the minimizer of the\nfunctional\n  \\la bel\n {eq\n.fra ctional_matching} \\mathcal {W}=\\argminA _{\\mathcal {W}\\in B_n}\\:\\langle \\mathcal {D}_\\infty ,\\mathcal {W}\\rangle \n(1)\nwhere Bn is the set (known as the Birkhoff polytope) of n × n doubly-stochastic\nmatrices and ⟨·, ·⟩stands for the Frobenius (standard) dot-product.\n1 a square (n×n) matrix of non-negative real values, each of whose rows and columns\nsums to 1\n",
    "The Self-Optimal-Transport Feature Transform\n7\nThis objective can be minimized using simplex or interior point methods with\ncomplexity Θ(n3 log n). In practice, we use the highly efficient Sinkhorn-Knopp\nmethod [7], which is an iterative scheme that optimizes an entropy-regularized\nversion of the problem, where each iteration takes Θ(n2). Namely:\n  \\la bel\n {eq\n.ent ro p y\n_min} \\mathcal {W}=\\argminA _{\\mathcal {W}\\in B_n}\\:\\langle \\mathcal {D}_\\infty ,\\mathcal {W}\\rangle -\\frac {1}{\\lambda }h(\\mathcal {W}) \n(2)\nwhere h(W) = −P\ni,j wij log(wij) is the Shannon entropy of W and λ is the\nentropy regularization parameter.\nThe transport-plan matrix W that is the minimizer of Eq. (2) is the result of\nour transform, i.e. W = T(V) and each of its rows is the re-embedding of each of\nthe corresponding features (rows) in V. Recall that W is doubly-stochastic and\nnote that it is symmetric2. We next explain its probabilistic interpretation.\nThe optimization problem in Eq. (1) can be written more explicitly as follows:\n  \\\nb egin  {\nalig\nn e d}  \\m i n _ {\\mathcal {W}} \\; \\langle \\mathcal {D}_\\infty ,\\mathcal {W}\\rangle \\quad \\quad \\textrm {s.t.} \\quad \\quad & \\mathcal {W}\\cdot \\textbf {1}_n=\\mathcal {W}^T\\cdot \\textbf {1}_n= \\textbf {1}_n \\end {aligned} \\label {eq.opt_D_inf} \n(3)\nwhich can be seen to be the same as:\n  \\\nb egi n \n{ali\ng n ed }  \\ m in  _{\n\\ma t h\ncal\n { W} }  \\ ; \\langle \\mathcal {D},\\mathcal {W}\\rangle \\quad \\quad \\textrm {s.t.} \\quad \\quad & \\mathcal {W}\\cdot \\textbf {1}_n=\\mathcal {W}^T\\cdot \\textbf {1}_n= \\textbf {1}_n \\\\ & w_{ii}=0 \\quad \\text {for}\\quad i = 1,\\dots n \\end {aligned} \\label {eq.opt_fractional_matching} \n(4)\nsince the use of the infinite weights on the diagonal of D∞is equivalent to using\nthe original D with a constraint of zeros along the diagonal of W.\nThe optimization problem in Eq. (4) is in fact a fractional matching instance\nbetween the set of n original features and itself. It can be posed as a bipartite-\ngraph min-cost max-flow instance. The graph has n nodes on each side, repre-\nsenting the original features {vi}n\ni=1 (the rows of V). Across the two sides, the\ncost of the edge (vi, vj) is the distance dij and the edges of the type (vi, vi) have\na cost of infinity (or can simply be removed). Each ‘left’ node is connected to a\n’source’ node by an edge of cost 0 and similarly each ’right’ node is connected to\na ‘target’ (sink) node by an edge of cost 0. All edges in the graph have a capac-\nity of 1 and the goal is to find an optimal fractional self matching, by finding a\nmin-cost max-flow from source to sink. Note that the maximum flow can easily\nbe seen to be n, but a min-cost flow is sought among the max-flows.\nIn this set-to-itself matching view, each vector vi is fractionally matched to\nthe set of all other vectors V −{vi} based on the pairwise distances, but im-\nportantly taking into account the fractional matches of the rest of the vectors\nin order to satisfy the double-stochasticity constraint3. Therefore, the ith trans-\nformed (re-embedded) feature wi (ith row of W) is a distribution (non-negative\nentries, summing to 1), where wii = 0 and wij is the relative belief that features\ni and j belong to the same ‘class’.\n2 The symmetry of W is as a result of the symmetry of D and the double-stochasticity\nof W.\n3 The construction constrains the maximum flow to exactly have a total outgoing flow\nof 1 from each ‘left’ node and a total incoming flow of 1 from each ‘right’ node.\n",
    "8\nDaniel Shalam and Simon Korman\n(a) orig.\n(b) S\n(c) W\n(d) D\n(e) DW\n(f) SOT\n(orig. embedding) (cos-similarity) (SOT features)\n(orig. dists)\n(SOT dists)\n(SOT embedding)\nFig. 3:\nA close look at the SOT transform as it operates on a 10-way 20-shot\nsupervised clustering task: The input is a set of 200 33-dimensional unit-length feature\nvectors that are visualized on the plane in (a) using a t-SNE dimension reduction [36],\nwhere colors refer to the 10 classes. In (b) is the pairwise cosine similarity matrix S, which\nis linearly related to the Euclidean pairwise distances D shown in (d). Next, in (c) we show\nthe SOT matrix W whose rows (or columns, symmetrically) consist of our new embedding\nof the features. These 200-dimensional features are shown again on the plane in (f). Notice\nthe visually apparent improvement in point gathering by class, from (a) to (f), which can\nbe explained by comparing the matrices D and DW, which are the self-pairwise distances of\nthe original and SOT embedding respectively. Notice the greater contrast in DW between\ninter- and intra- cluster points. Note, that like in the visualizations of Fig. 1, we show the\nmatrices with row/col order based on the true classes, purely for ease of visualization.\nOur final set of features W is obtained by replacing the diagonal entries from\n0s to 1s, namely W = W + I, where I is the n × n identity matrix. Please refer\nto Fig. 3 for a close look at the application of SOT to a toy clustering prob-\nlem, where we demonstrate visually the improved embedding obtained through\nexamining the pairwise distances before and after the transform. We can now\npoint out some important properties of this new embedding W:\nDirect and Indirect similarity encoding: Each embedded feature encodes\nits distribution of similarities to all other features. An important property of our\nembedding is that the comparison of the embedded vectors wi and wj includes\nboth direct and indirect information about the similarity between the features.\nPlease refer to Fig. 4 for a detailed explanation of this property. If we look\nat the different coordinates k of the absolute difference vector a = |wi −wj|,\nSOT captures (i) direct similarity: For k which is either i or j, it holds that\nak = 1 −wij = 1 −wji 4. This amount measures how high (i.e.close to 1) is\nthe mutual belief of features i and j about one another. (ii) indirect (3rd-party)\nsimilarity: For k /∈{i, j}, we have ak = |wik −wjk|, which is a comparison of\nthe beliefs of features i and j regarding the (third-party) feature k.\nParameterless-ness: Our proposed transform is parameterless, giving it the\nflexibility to be used in other pipelines, directly over different kinds of embed-\ndings, without the harsh requirement of retraining the entire pipeline5.\n4 Note: (i) wii = wjj = 1 ; (ii) wij = wji from the symmetry of W ; (iii) all elements\nof W are ≤1 and hence the | · | can be dropped ;\n5 Retraining is certainly possible, and beneficial in many situations, but not manda-\ntory, as our experiments work quite well without it.\n",
    "The Self-Optimal-Transport Feature Transform\n9\nFig. 4:\nThe (symmetric) embedding matrix\nW and the absolute difference between its ith\nand jth rows: We examine the vector |wi −wj|:\n(i) Its ith and jth coordinates equal |1 −wij| =\n|1 −wji|, giving the direct similarity between the\noriginal features, since this amount (in green) is\ngreater when wij and wji (the mutual beliefs) are\nhigh (closer to 1). ; (ii) Its kth coordinate (for any\nk /∈{i, j}) gives |wik −wjk| which is an indirect\n(third-party) comparison between the original fea-\ntures through the kth feature. Similarity (in yellow)\nis stronger when features i and j have a similar be-\nlief regarding feature k, i.e. wik and wjk are close.\nDifferentiability: Due to the differentiability of Cuturi’s [7] version of Sinkhorn,\nback-propagating through the SOT can be done naturally, hence it is possible\nto (re-)train the hosting network to adapt to the SOT, if desired.\nEquivariance: The embedding works coherently with respect to any change\nof order of the input items (features). This can be shown by construction, since\nmin-cost max-flow solvers as well as the Sinkhorn OT solver are equivariant with\nrespect to permutations of their inputs.\nExplainability: The non-parametric nature gives SOT an advantage over other\nset-to-set methods such as transformers in that its output is interpretable (e.g.\nby visually inspecting the transport-plan matrix W), with a clear probabilistic\ncharacterization of the relations it had found.\nTask-Aware Dimensionality: SOT has the unique property that the dimen-\nsion of the embedded feature depends on (equals) the number of features. On\nthe one hand, this is a desired property, since it is only natural that the feature\ndimensionality (capacity) depends on the complexity of the task, which typically\ngrows with the number of features (think of the inter-relations which are more\ncomplex to model). On the other hand, it might impose a problem in situations\nin which the downstream calculation that follows the feature embedding expects\na fixed input size, for example a pre-trained non-convolutional layer. Neverthe-\nless, in many situations the downstream computation has the flexibility to work\nwith varying input dimensions. Also, in most benchmarks the instance set sizes\nare fixed, allowing for a single setting of sizes to work throughout.\n4\nImplementation details\nDatasets: We consider three different applications to evaluate the performance\nof our method. For unsupervised clustering we designed a specialized synthetic\ndata set with the goal of enabling controlled experiments over a wide range of\ndifficulties, which are determined by data dimensionality and in-cluster spread.\n",
    "10\nDaniel Shalam and Simon Korman\nFor few-shot classification we use the standard benchmarks in the literature.\nThe MiniImagenet [39] dataset is a subset of Imagenet [31] that contains 100\nclasses and 600 images of size 84x84 per class. We follow the standard setup\nof using 64 classes for training and 16 and 20 novel classes for validation and\ntesting. The CIFAR-FS [1] dataset includes 100 classes with 600 images of size\n32 × 32 per-class. We used the same splits as in MiniImagenet for this dataset.\nThe CUB [40] dataset includes 200 classes of bird species and has 11,788 images\nof size 84 × 84 pixels in total. We followed the split suggested in [11] into 100\nbase classes, 50 validation classes and 50 novel classes.\nFor person re-identification (ReID) we use two common large-scale datasets.\nThe Market-1501 [47] and CUHK03 [23] dataset consists of 1,501 and 1,467\nidentities and a total of 32,668 and 14,097 images taken from 6 cameras. We use\nthe validation and test sets according to the splits in [49].\nPre-training: We pre-trained ProtoNet [34] with a 4-layer Convolution network\nadapting the procedures of [34] for training both with and without SOT, training\non a 5-way (5/1)-shot 15-query task, using ADAM [17] with learning rate 0.01\nand step size of 20 over 100 episodes (tasks) per epoch.\nFine-tuning: We perform fine-tuning on two types of backbone residual net-\nworks - a resnet-12 as used in [42] and a WRN-28-10 as used in [24]. For Pro-\ntoNet [34] and ProtoNet-SOT, we fine-tune the base network with parameters\ntaken from [42]. For PTMAP-SOT, we use meta-training with batches of a single\n10-way 5-shot 15-query task per batch. We use ADAM with learning rate 5e −5\nthat decreases with step size 10 for 25 epochs. We train the WRN-28-10 and the\nresnet-12 backbones for 800 and 100 episodes respectively per epoch.\nHyper-parameters: SOT has two hyper-parameters which were chosen through\ncross-validation and were kept fixed for each of the applications over all datasets.\n(i) The number of Sinkhorn iterations for computing the optimal transport plan\nwas fixed to 10. (ii) The entropy regularization parameter λ (Eq. (3)) was set\nto 0.1 for clustering and few-shot-learning experiments and to 1.0 for the ReID\nexperiments. We further ablate these in the supplementaries.\n5\nResults\n5.1\nClustering on the Sphere\nWe first demonstrate the effectiveness of SOT using a controlled synthetically\ngenerated clustering experiment, with k = 10 cluster centers that are distributed\nuniformly at random on a d-dimensional unit-sphere, and 20 points per cluster\n(200 in total) that are perturbed around the cluster centers by Gaussian noise\nof increasing standard deviation, of up to 0.75, followed by a re-projection back\nto the sphere by dividing each vector by its L2 magnitude. We also apply di-\nmensionality reduction with PCA to d = 50, for dimensions above 50.\nWe performed the experiment over a logarithmic 2D grid of combinations\nof data dimensionalities d in the range [10, 1234] and Gaussian in-cluster noise\nSTD in the range [0.1, 0.75]. Refer to Fig. 9 (i) for a visualization of the data\ngeneration process.\n",
    "The Self-Optimal-Transport Feature Transform\n11\nSTD=0.01\nSTD=0.12\nSTD=0.20\nSTD=0.31\nSTD=0.48\nSTD=0.75\n(i) 10 Random cluster centers\non the unit sphere, perturbed\nwith increasing noise STD σ.\n(ii) Clustering accuracy across dimensions d (left) and noise levels\nσ (right). For each configuration, k-means accuracy is reported\nwhen applied with original (solid) and SOT (dashed) features.\nFig. 5:\nClustering on the d-dimensional sphere. Left (i): the data generation pro-\ncess (illustrated for the 3D case). Right (ii): detailed k-means accuracy results. The SOT\n(dashed) features give superior results throughout a majority of the space of settings.\nEach point is represented by its d-dimensional euclidean coordinates vector,\nwhere the baseline clustering is obtained by running k-means on these location\nfeatures. In addition, we run k-means on the set of features that has undergone\nSOT. Hence, the benefits of the transform (embedding) are measured indirectly\nthrough the accuracy6 achieved by running k-means on the embedded vs. original\nvectors. Evaluation results are reported in Fig. 9 (ii) as averages over 10 runs, by\nplotting accuracy vs. dimensionality (for different noise STDs) and accuracy vs\nnoise STDs (for different dimensionalities). The results show (i) general accuracy\ngains and robustness to wide ranges of data dimensionality (ii) the ability of\nSOT to find meaningful representations that enable clustering quality to degrade\ngracefully with the increase in cluster noise level. Note that the levels of noise\nare rather high, as they are relative to a unit radius sphere (a 3-dimensional\nexample is shown at the top of the figure). We provide further details on this\nexperiment in the supplementaries.\n5.2\nFew-Shot Classification (FSC)\nOur main experiment is a comprehensive evaluation on the standard few-shot\nclassification benchmarks MiniImagenet [39], CIFAR-FS [1], and CUB [40], with\ndetailed results in Tables 1 and 2. For MiniImagenet (Table 1) we report on both\nversions “SOTp” and “SOTt” over a range of backbone architectures, while for\nthe smaller datasets CIFAR-FS and CUB (Table 2) we focus on the ‘drop-in’\nversion “SOTp” and only the strongest wrn-28-10 architecture.\nOne goal here is to show that we can achieve new state-of-the-art FSC results,\nwhen we build on current state-of-the-art. But more importantly, we demonstrate\nthe flexibility and simplicity of applying SOT in this setup, with improvements\nin the entire range of testing, including: (i) when building on different ‘host-\ning’ methods; (ii) when working above different feature embeddings of different\n6 Accuracy is measured by comparison with the optimal permutation of the predicted\nlabels, found by the Hungarian Algorithm [21].\n",
    "12\nDaniel Shalam and Simon Korman\nmethod\ntransductive\nbackbone\n5way-1shot\n5way-5shot\nMAML(*) [10]\n✗\nconv-4\n46.47\n62.71\nRelationNet(*) [35]\n✗\nconv-4\n49.31\n66.60\nProtoNet(#) [34]\n✗\nconv-4\n49.10\n66.79\nFEAT($) [42]\n✗\nconv-4\n55.15\n71.61\nProtoNet-SOTp\n✓\nconv-4\n54.01 (+10.2%)\n69.39 (+3.9%)\nProtoNet-SOTt\n✓\nconv-4\n53.70 (+9.3%)\n70.40 (+5.4%)\nProtoNet(#) [34]\n✗\nresnet-12\n62.39\n80.33\nDeepEMD($) [45]\n✗\nresnet-12\n65.91\n82.41\nFEAT($) [42]\n✗\nresnet-12\n66.78\n82.05\nRENet($) [15]\n✗\nresnet-12\n67.60\n82.58\nPTMAP(#) [12]\n✓\nresnet-12\n76.90\n85.20\nProtoNet-SOTp\n✓\nresnet-12\n67.34 (+7.9%)\n81.84 (+1.6%)\nProtoNet-SOTt\n✓\nresnet-12\n67.90 (+8.8%)\n83.09 (+3.2%)\nPTMAP-SOTp\n✓\nresnet-12\n78.35 (+1.9%)\n86.01 (+1.0%)\nPTMAP-SOTt\n✓\nresnet-12\n77.30 (+0.5%)\n85.49 (+0.3%)\nProtoNet(&) [34]\n✗\nwrn-28-10\n62.60\n79.97\nPTMAP($) [12]\n✓\nwrn-28-10\n82.92\n88.80\nSill-Net($) [46]\n✓\nwrn-28-10\n82.99\n89.14\nPTMAP-SF($) [6]\n✓\nwrn-28-10\n84.81\n90.62\nPTMAP-COSINE\n✓\nwrn-28-10\n74.60 (-10.0%)\n84.68 (-4.6%)\nPTMAP-SOFTMAX\n✓\nwrn-28-10\n80.08 (-3.4%)\n83.83 (-5.6%)\nPTMAP-SOTp\n✓\nwrn-28-10\n83.19 (+0.3%)\n89.56 (+0.9%)\nPTMAP-SOTt\n✓\nwrn-28-10\n84.18 (+1.5%)\n90.51 (+1.9%)\nSill-Net-SOTp\n✓\nwrn-28-10\n83.35 (+0.4%)\n89.65 (+0.6%)\nPTMAP-SF-SOTp\n✓\nwrn-28-10\n85.59 (+0.9%)\n91.34 (+0.8%)\nTable 1:\nFew-Shot Classification (FSC) accuracy on MiniImagenet [39]. The im-\nprovements introduced by the variants of SOT (percentages in brackets) are in comparison\nwith each respective baseline hosting method. Bold and underline notations highlight best\nand second best results per backbone. (*) = from [5] ; (&) = from [50] ; ($) = from the\nmethod’s paper itself ; (#) = our implementation ;\ncomplexity backbones; and (iii) whether retraining the hosting network or just\ndropping-in SOT and performing standard inference.\nTo evaluate the performance of the proposed SOT, we applied it to previous\nFSC methods including the very recent state-of-the-art (PT-MAP [12], Sill-NET\n[46] and PT-MAP-SF [6]) as well as a to more conventional methods like the\npopular ProtoNet [34]. The detailed results are presented in Tables 1 and 2)\nfor the different datasets. Note that SOT is by nature a transductive method7,\nhence we marked its results as so, regardless of whether the hosting network is\ntransductive or not. In the following, we discuss the two modes in which our\ntransform can be used in existing FSC methods.\n7 SOT is transductive in the sense that it needs to jointly process the data, but im-\nportantly, unlike other methods it does not gain its benefit in being so from making\nlimiting assumptions about the structure of the instance, like knowing the number\nof classes, or the number of items per class.\n",
    "The Self-Optimal-Transport Feature Transform\n13\nFSC benchmark\nCIFAR-FS [1]\nCUB [40]\nmethod\n5way-1shot\n5way-5shot\n5way-1shot\n5way-5shot\nPTMAP($) [12]\n87.69\n90.68\n91.55\n93.99\nSill-Net($) [46]\n87.73\n91.09\n94.73\n96.28\nPTMAP-SF($) [6]\n89.39\n92.08\n95.45\n96.70\nPTMAP-SOTp\n87.37 (-0.4%)\n91.12 (+0.5%)\n91.90 (+0.4%)\n94.63 (+0.7%)\nSill-Net-SOTp\n87.30 (-0.5%)\n91.40 (+0.3%)\n94.86 (+0.1%)\n96.61 (+0.3%)\nPTMAP-SF-SOTp 89.94 (+0.6%) 92.83 (+0.8%) 95.80 (+0.4%) 97.12 (+0.4%)\nTable 2: Few-Shot Classification (FSC) accuracy on CIFAR-FS [1] and CUB [40].\nSOT insertion without network retraining (notated by SOTp in Tables\n1 and 2). Recall that the proposed transform is non-parametric. As such, we\ncan simply apply it to a trained network at inference, without the need to\nre-train. This basic ‘drop-in’ use of SOT consistently, and in many cases also\nsignificantly, improved the performance of the tested methods, including state-\nof-the-art, across all benchmarks and backbones. SOTp gave improvements of\naround 3.5% and 1.5% on 1 and 5 shot MiniImagenet tasks. This improvement\nwithout re-training the embedding backbone network shows SOT’s effectiveness\nin capturing meaningful relationships between features in a very general sense.\nSOT insertion with network retraining (notated by SOTt in Table 1). Due\nto its differentiability property, the proposed method can be applied while train-\ning and hence we expect an adaptation of the hosting network’s parameters to\nthe presence of the transform with a potential for improvement. To evaluate this\nmode, we focused on the MiniImagenet benchmark [39], specifically on the same\nconfigurations that we used without re-training, to enable a direct comparison.\nThe results in Table 1 show additional improvements in almost every method.\nSOTt gave improvements of around 5% and 3% on 1 and 5 shot MiniImagenet\ntasks, further improving on the pre-trained counterpart. This result indicates\nthe effectiveness of training with SOT in an end-to-end fashion.\nAblations Within the context of few-shot learning on MiniImagenet, we per-\nformed several ablation studies. In Table 1, the networks ‘PTMAP-COSINE’\nand ‘PTMAP-SOFTMAX’ stand for the obvious baseline attempts (found to be\nunsuccessful) that work in the line of our approach, without the specialized OT-\nbased transform. In the former, we take the output features to be the rows of\nthe (un-normalized) matrix S (rather than those of W) and in the latter we also\nnormalize its rows using soft-max. In the supplementaries we ablate on SOT’s\ntwo parameters - the number of Sinkhorn iterations and the entropy term λ.\n5.3\nPerson re-Identification (Re-ID)\nIn this section, we explore the possibility of using SOT on large-scale datasets by\nconsidering the Person re-Identification task. Given a set of query images and a\nlarge set of gallery images, the task is to rank the similarities of each single query\nagainst the gallery. This is done by computing specialized image features among\nwhich similarities are based on Euclidean distances. SOT is applied to such pre-\n",
    "14\nDaniel Shalam and Simon Korman\nReID benchmark\nCUHK03-detected [23]\nMarket-1501 [47]\nnetwork\nmAP\nRank-1\nmAP\nRank-1\nTopDBNet [28]\n72.9\n75.7\n85.7\n94.3\nTopDBNet-rerank [28]\n87.1\n87.1\n94.0\n95.3\nTopDBNet-SOTp\n77.9 (+6.9%) 80.4 (+6.2%) 88.1 (+2.8%) 94.4 (+0.1%)\nTopDBNet-rerank-SOTp 87.9 (+0.9%) 88.0 (+1.0%) 94.0 (0.0%)\n95.0 (-0.3%)\nTable 3: Re-ID results on CUHK03 [23] and Market-1501 [47]\ncomputed image features, refining them with the strong relative information that\nit is able to capture by applying it on the union of all query and gallery features.\nWe adapted a pre-trained standard resnet-50 architecture [49] and the popular\nTopDBNet [28], which we tested on the large-scale ReID benchmarks CUHK03\n[23] (on the ’detected’ version and similar results on the ‘labeled’ version in the\nsupplementaries) and Market-1501 [47], with and without the re-ranking [48]\nprocedure. For evaluation, we followed their conventions and compare results\nusing the mAP (mean Average Precision) and Rank-1 metrics.\nThe results in Table 3 show a consistent benefit in using SOT within the\ndifferent networks. For CUHK03, the results improved by a large margin of\n+6.8% in mAP for the best configuration. These results demonstrate that the\nproposed SOT scales well to large-scale problems (with number of features in the\nthousands) and is attractive for a variety of applications. ReID is not the main\nfocus of this work, hence, we did not re-train the hosting networks with SOT\nincluded. Further research is required to measure the possible effects of doing so.\n6\nConclusions, Limitations and Future Work\nIn this paper, we explored the idea of utilizing global information of features,\nfor instance-specific problems such as clustering, few-shot learning, and per-\nson re-identification. We proposed a novel module: the Self-Optimal-Transport\n(SOT) - a features transform that is non-parametric, differentiable and which\ncan capture high-level relationships between data points in problems of this\nnature. The proposed method outperforms state-of-the-art networks on popular\nfew-shot classification benchmarks and shows consistent improvements on tested\nReID benchmarks. Based on these promising results, we believe that exploring\nits full potential can lead to improvements in a variety of fields and open new\npossibilities.\nIn future work, we plan to address some current limitations. (i) Regarding\nthe output dimensionality of the embedding, which is dictated by the input set\nsize. We will aim at being able to obtain an arbitrary dimension, for increased\nusage flexibility; (ii) We plan to investigate the usage of SOT in unsupervised\nsettings, which would be possible by utilizing its informative representation for\nself-supervision; (iii) It would likely be beneficial to have a variant of SOT in\nwhich the transform is enriched with learnable parameters, similar to transform-\ners, to extend its modeling capacity even further; (iv) SOT is purely transduc-\ntive. We plan to explore non-transductive variants, possibly by comparing each\nsample separately to the support or gallery sets.\n",
    "The Self-Optimal-Transport Feature Transform\n15\nReferences\n1. Luca Bertinetto, Joao F. Henriques, Philip Torr, and Andrea Vedaldi.\nMeta-\nlearning with differentiable closed-form solvers. In International Conference on\nLearning Representations (ICLR), 2019. 10, 11, 13\n2. Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep\nclustering for unsupervised learning of visual features. In Proceedings of the Euro-\npean Conference on Computer Vision (ECCV), 2018. 5\n3. Mathilde Caron, Ishan Misra, J. Mairal, Priya Goyal, Piotr Bojanowski, and Ar-\nmand Joulin. Unsupervised learning of visual features by contrasting cluster as-\nsignments. ArXiv, abs/2006.09882, 2020. 5\n4. Jianlong Chang, Lingfeng Wang, Gaofeng Meng, Shiming Xiang, and Chunhong\nPan. Deep adaptive image clustering. In Proceedings of the IEEE International\nConference on Computer Vision (ICCV), 2017. 5\n5. Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin\nHuang. A closer look at few-shot classification. In International Conference on\nLearning Representations (ICLR), 2018. 4, 12\n6. Xiangyu Chen and Guanghui Wang. Few-shot learning by integrating spatial and\nfrequency representation. arXiv preprint arXiv:2105.05348, 2021. 5, 12, 13\n7. Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport.\nIn Advances in Neural Information Processing Systems (NeurIPS), 2013. 2, 4, 5,\n7, 9, 18\n8. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet:\nA large-scale hierarchical image database. In 2009 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR). IEEE, 2009. 5\n9. Guneet S Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A\nbaseline for few-shot image classification. In International Conference on Learning\nRepresentations (ICLR), 2020. 4\n10. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning\nfor fast adaptation of deep networks.\nIn International Conference on Machine\nLearning (ICML), 2017. 4, 12\n11. Yuqing Hu, Vincent Gripon, and St´ephane Pateux. Exploiting unsupervised inputs\nfor accurate few-shot classification. ArXiv, abs/2001.09849, 2020. 10\n12. Yuqing Hu, Vincent Gripon, and St´ephane Pateux. Leveraging the feature distri-\nbution in transfer-based few-shot learning. In arXiv preprint arXiv:2006.03806,\n2020. 5, 12, 13\n13. Gabriel Huang, Hugo Larochelle, and Simon Lacoste-Julien. Are few-shot learning\nbenchmarks too simple? solving them without task supervision at test-time. arXiv\npreprint arXiv:1902.08605, 2019. 5\n14. Xu Ji, Joao F Henriques, and Andrea Vedaldi.\nInvariant information cluster-\ning for unsupervised image classification and segmentation. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision (ICCV), 2019. 5\n15. Dahyun Kang, Heeseung Kwon, Juhong Min, and Minsu Cho. Relational embed-\nding for few-shot classification.\nIn Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), 2021. 4, 12\n16. Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fa-\nhad Shahbaz Khan, and Mubarak Shah. Transformers in vision: A survey. arXiv\npreprint arXiv:2101.01169, 2021. 1, 2, 4\n17. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.\narXiv preprint arXiv:1412.6980, 2014. 10\n",
    "16\nDaniel Shalam and Simon Korman\n18. Nicholas Kolkin, Jason Salavon, and Gregory Shakhnarovich.\nStyle transfer by\nrelaxed optimal transport and self-similarity.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), 2019. 4\n19. Simon Korman and Shai Avidan. Coherency sensitive hashing. IEEE Transactions\non Pattern Analysis and Machine Intelligence (PAMI), 2015. 1\n20. Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from\ntiny images. 2009. 5\n21. Harold W Kuhn.\nThe hungarian method for the assignment problem.\nNaval\nResearch Logistics Quarterly, 2, 1955. 11\n22. Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and\nYee Whye Teh. Set transformer: A framework for attention-based permutation-\ninvariant neural networks.\nIn International Conference on Machine Learning\n(ICML), 2019. 1, 2, 4\n23. Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. Deepreid: Deep filter pairing\nneural network for person re-identification. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2014. 10, 14\n24. Puneet Mangla, Nupur Kumari, Abhishek Sinha, Mayank Singh, Balaji Krishna-\nmurthy, and Vineeth N Balasubramanian. Charting the right manifold: Manifold\nmixup for few-shot learning. In Proceedings of the IEEE/CVF Winter Conference\non Applications of Computer Vision (WACV), 2020. 10\n25. Haggai Maron, Or Litany, Gal Chechik, and Ethan Fetaya. On learning sets of\nsymmetric elements. In International Conference on Machine Learning (ICML),\n2020. 2, 3\n26. Gr´egoire Mialon, Dexiong Chen, Alexandre d’Aspremont, and Julien Mairal. A\ntrainable optimal transport embedding for feature aggregation and its relationship\nto attention. In International Conference on Learning Representations (ICLR),\n2021. 1, 2\n27. Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\nPointnet: Deep\nlearning on point sets for 3d classification and segmentation. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 3\n28. Rodolfo Quispe and Helio Pedrini. Top-db-net: Top dropblock for activation en-\nhancement in person re-identification. 25th International Conference on Pattern\nRecognition (ICPR), 2020. 14\n29. Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Lev-\nskaya, and Jon Shlens. Stand-alone self-attention in vision models. Advances in\nNeural Information Processing Systems (NeurIPS), 2019. 1, 2, 4\n30. Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning.\nIn International Conference on Learning Representations (ICLR), 2017. 1\n31. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean\nMa, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexan-\nder C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge.\nInternational Journal of Computer Vision (IJCV), 2015. 10\n32. Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan\nPascanu, Peter Battaglia, and Timothy Lillicrap. A simple neural network mod-\nule for relational reasoning. Advances in Neural Information Processing Systems\n(NeurIPS), 2017. 4\n33. Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-\nnovich.\nSuperglue: Learning feature matching with graph neural networks.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2020. 4\n",
    "The Self-Optimal-Transport Feature Transform\n17\n34. Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot\nlearning. In Advances in Neural Information Processing Systems (NeurIPS), 2017.\n4, 10, 12, 18, 20\n35. Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M\nHospedales.\nLearning to compare: Relation network for few-shot learning.\nIn\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2018. 12\n36. Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Jour-\nnal of Machine Learning Research (JMLR), 9(11), 2008. 8\n37. Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proes-\nmans, and Luc Van Gool. Scan: Learning to classify images without labels. In\nEuropean Conference on Computer Vision (ECCV). Springer, 2020. 1, 5\n38. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez,  Lukasz Kaiser, and Illia Polosukhin. Attention is all you need.\nIn Advances in Neural Information Processing Systems (NeurIPS), 2017. 4\n39. Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan\nWierstra. Matching networks for one shot learning. In Proceedings of the 30th\nInternational Conference on Neural Information Processing Systems (NeurIPS),\n2016. 4, 10, 11, 12, 13, 18\n40. Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge J. Be-\nlongie. The caltech-ucsd birds-200-2011 dataset. 2011. 10, 11, 13\n41. Yujia Xie, Hanjun Dai, Minshuo Chen, Bo Dai, Tuo Zhao, Hongyuan Zha, Wei\nWei, and Tomas Pfister. Differentiable top-k with optimal transport. Advances in\nNeural Information Processing Systems (NeurIPS), 2020. 4\n42. Han-Jia Ye, Hexiang Hu, De-Chuan Zhan, and Fei Sha.\nFew-shot learning via\nembedding adaptation with set-to-set functions. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition (CVPR), 2020.\n2, 4, 10,\n12\n43. Mang Ye, Jianbing Shen, Gaojie Lin, Tao Xiang, Ling Shao, and Steven CH Hoi.\nDeep learning for person re-identification: A survey and outlook. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence (PAMI), 2021. 1, 5\n44. Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R\nSalakhutdinov, and Alexander J Smola. Deep sets. In Advances in Neural Infor-\nmation Processing Systems (NeurIPS), 2017. 2, 3\n45. Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen. Deepemd: Few-shot\nimage classification with differentiable earth mover’s distance and structured clas-\nsifiers. In IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), June 2020. 4, 12\n46. Haipeng Zhang, Zhong Cao, Ziang Yan, and Changshui Zhang.\nSill-net: Fea-\nture augmentation with separated illumination representation.\narXiv preprint\narXiv:2102.03539, 2021. 5, 12, 13\n47. Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian.\nScalable person re-identification: A benchmark. In 2015 IEEE International Con-\nference on Computer Vision (ICCV), 2015. 10, 14, 18\n48. Zhun Zhong, Liang Zheng, Donglin Cao, and Shaozi Li. Re-ranking person re-\nidentification with k-reciprocal encoding. 2017. 14\n49. Kaiyang Zhou and Tao Xiang. Torchreid: A library for deep learning person re-\nidentification in pytorch. arXiv preprint arXiv:1910.10093, 2019. 10, 14\n50. Imtiaz Masud Ziko, Jose Dolz, Eric Granger, and Ismail Ben Ayed.\nLaplacian\nregularized few-shot learning. In International Conference on Machine Learning\n(ICML), 2020. 5, 12\n",
    "18\nDaniel Shalam and Simon Korman\nAppendix\nA\nablation studies\nA.1\nSinkhorn iterations\nIn Table 4 we ablate the number of normalization iterations in the Sinkhorn-\nKnopp (SK) [7] algorithm at test-time. We measured accuracy on the validation\nset of MiniImagenet [39], using ProtoNet-SOTp (which is the non-fine-tuned\ndrop-in version of SOT within ProtoNet [34]). As was reported in prior works fol-\nlowing [7], we empirically observe that a very small number of iterations (around\n5) provide rapid convergence. We observed similar behavior for other hosting\nmethods, and therefore chose to use a fixed number of 10 iterations throughout\nthe experiments.\nmethod\niterations\n5way-1shot\n5way-5shot\nProtoNet-SOTp\n1\n70.71\n83.79\nProtoNet-SOTp\n2\n71.10\n84.01\nProtoNet-SOTp\n4\n71.18\n84.08\nProtoNet-SOTp\n8\n71.20\n84.10\nProtoNet-SOTp\n16\n71.20\n84.10\nTable 4: Sinkhorn iterations ablation study: See text for details.\nA.2\nOT entropy regularization parameter λ\nWe measured the impact of using different values of the optimal-transport en-\ntropy regularization parameter λ (the main parameter of the Sinkhorn algo-\nrithm) on a variety of configurations (ways and shots) in Few-Shot-Classification\n(FSC) on MiniImagenet [39] in Fig. 6 as well as on the Person-Re-Identification\n(RE-ID) experiment on Market-1501 [47] in Fig. 7. In both cases, the ablation\nwas executed on the validation set.\nFor FSC, in Fig. 6, the left plot shows that the effect of the choice of λ is\nsimilar across tasks with a varying number of ways. The right plot shows the\nbehavior as a function of λ across multiple shot-values, where the optimal value\nof λ can be seen to have a certain dependence on the number of shots. Recall that\nwe chose to use a fixed value of λ = 0.1, which gives an overall good accuracy\ntrade-off. Note that a further improvement could be achieved by picking the best\nvalues for the particular cases. Notice also the log-scale of the x-axes to see that\nperformance is rather stable around the chosen value.\nFor Re-ID, in Fig. 7, we experiment with a range of λ values on the validation\nset of the Market-1501 dataset. The results (shown both for mAP and rank-1\nmeasures) reveal a strong resemblance to those of the FSC experiment in Fig.\n6, however, the optimal choices for λ are slightly higher, which is consistent\nwith the dependence on the shots number, since the re-ID tasks are typically\n",
    "The Self-Optimal-Transport Feature Transform\n19\nFig. 6:\nAblation study on λ in Few-Shot-Classification (FSC): Considering\ndifferent ‘ways’ (left), and different ‘shots’ (right). See text for details.\nlarge ones. In this re-ID ablation, we found that a value of λ = 0.25 gives good\nresults across both datasets. We ask to note that in the paper we mistakenly\nreported that we used λ = 1.0, while in practice all our results were obtained\nusing λ = 0.25.\nFig. 7: Ablation study on λ in\nPerson-Re-Identification (Re-\nID): Using the validation set of the\nMarket-1501 dataset and consider-\ning both mAP and Rank-1 mea-\nsures. See text for details.\nB\nUnsupervised Clustering - further details\nIn this section we provide further details (due to lack of space in main paper)\non the experiment on unsupervised clustering on the unit sphere (Exp. 5.1).\nB.1\nSeparation between inter- and intra-class features\nFig. 8 depicts the average percentile of the in-class and out-class distances com-\nputed by the original and the SOT points. Each panel presents the distributions\nof both types of distances, for instances of a different level of noise. We compute\nthe mean (and plus-minus half-std) percentiles, with respect to the entire set of\npair-wise distances, for a fixed level of in-class noise (increasing from top-left to\nbottom-right panels), for a range of data dimensionality (x-axis). Naturally, the\n",
    "20\nDaniel Shalam and Simon Korman\nFig. 8:\nintra (in) vs. inter (out) class distances before and after SOT. A\nstrong indicative property of an embedding that works on class (cluster) objects\nis its ability to reduce embedded intra-class (pink shaded) pairwise feature dis-\ntances compared to inter-class (green shaded) ones. SOT (red lines) consistently\nimproves this separation compared to the baseline (brown lines) - leading to\nbetter downstream clustering and classification. x-axis represents data dimen-\nsionality; y-axis represents percentiles of pair-wise distances; The four panels\npresent results for the noise standard deviations levels in {0.15, 0.19, 0.23, 0.29}\noverlap between in-class and between-class distances increases both with dimen-\nsionality and with in-class noise. Nevertheless, across almost all sampled points,\nthe situation is far better after SOT application (in red), compared to prior to\nSOT application (in brown). This can explain, in part, the effectiveness of using\nSOT in Euclidean-based downstream methods, like k-means and ProtoNet [34].\nB.2\nEvaluation on an extended set of measures\nIn Fig. 9 we evaluate the performance on additional popular clustering metrics,\nNMI and ARI (in addition to the accuracy measure we reported on in Figure 5\nof the paper). The results shows the same trend as with accuracy, perhaps even\nstronger for NMI, where SOT significantly improves the clustering performance.\n",
    "The Self-Optimal-Transport Feature Transform\n21\nFig. 9:\nA controlled clustering experiment on the d-dimensional sphere -\nExtension of results from Figure 5 of the paper, with 2 additional measures:\nIt can be seen that the SOT (dashed - - -) shows superior results in all aspects (see text\nfor explanations and interpretation). Clustering accuracy across different noise levels\nσ and dimensions d.\nNote: For each configuration, SOT is shown by a dashed line\nwhile the baseline features are shown by a solid line. For all 3 measures - the higher\nthe better.\n"
  ],
  "full_text": "The Self-Optimal-Transport Feature Transform\nDaniel Shalam and Simon Korman\nUniversity of Haifa, Israel\nAbstract. The Self-Optimal-Transport (SOT) feature transform is de-\nsigned to upgrade the set of features of a data instance to facilitate\ndownstream matching or grouping related tasks. The transformed set\nencodes a rich representation of high order relations between the in-\nstance features. Distances between transformed features capture their\ndirect original similarity and their third party ‘agreement’ regarding sim-\nilarity to other features in the set. A particular min-cost-max-flow frac-\ntional matching problem, whose entropy regularized version can be ap-\nproximated by an optimal transport (OT) optimization, results in our\ntransductive transform which is efficient, differentiable, equivariant, pa-\nrameterless and probabilistically interpretable. Empirically, the trans-\nform is highly effective and flexible in its use, consistently improving\nnetworks it is inserted into, in a variety of tasks and training schemes.\nWe demonstrate its merits through the problem of unsupervised cluster-\ning and its efficiency and wide applicability for few-shot-classification,\nwith state-of-the-art results, and large-scale person re-identification.\n1\nIntroduction\nIn this work, we reassess the design and functionality of features for instance-\nspecific problems. In such problems, typically, features computed at test time\nare mainly compared relative to one another, and less so to the features seen\nat training time. For such problems the standard practice of learning a generic\nfeature extractor during training and applying it at test time might be sub-\noptimal.\nWe aim at finding training and inference schemes that take into account\nthese considerations, being able to exploit large corpuses of training data to\nlearn features that can easily adapt, or be relevant, to the test time task. Our\napproach to doing so will be in the form of a feature transform that jointly re-\nembeds the set of features of an instance in a way that resembles how recently\npopular self-attention mechanisms and Transformers [29,22,26,16] re-embed sets\nof features.\nBeing at the low-to-mid-level of most relevant architectures, advances in such\nfeature re-embeddings have a direct impact and wide applicability in instance-\nspecific problems such as few-shot classification [30], clustering [37], patch match-\ning [19] and person re-identification [43], to name but a few.\nThe general idea of the Self-Optimal-Transport (SOT) feature transform that\nwe propose is depicted and explained in Fig. 1, as part of the general design of\nnetworks that work on sets which we illustrate in Fig. 2.\n\n\n2\nDaniel Shalam and Simon Korman\nFig. 1:\nThe SOT transform: Its input is a set of n d-dimensional features (each shown\nas a horizontal gray rectangle, and as a colored point in the input embedding space where\ncolor depicts class label or equivalent). Processing is as follows: The unit length (normalized)\nfeatures are arranged in an n × d matrix for computing a pairwise n × n cosine similarity\nmatrix S. Then, the transport-plan matrix W (given a specific OT instance that depends\non S) is computed using several Sinkhorn [7] iterations. Finally, the transformed output\nfeatures are basically the rows of the matrix W. As we claim and observe in real results, the\nfeatures are re-embedded in a way that is consistently superior for downstream grouping\nand matching tasks (observed the better formation of the embedded points, e.g. towards\napplying a linear classifier or an off-the-shelf clustering procedure).\n1.1\nOverview\nWe are given an instance of some inference problem, in the form of a set of n\nitems {xi}n\ni=1, represented as vectors in RD, for a fixed dimension D. A generic\nneural-network (Fig. 2 Left) typically uses a feature embedding (extractor) F :\nRD →Rd (with d ≪D), which is applied independently on each input item,\nto obtain a set of features V = {vi}n\ni=1 = {F(xi)}n\ni=1. The features V might be\nof high quality (concise, unique, descriptive), but are limited in representation\nsince they are extracted based on knowledge acquired for similar examples at\ntrain time, with no context of the test time instance that they are part of.\nWe adapt a rather simple framework (Fig. 2 Right) in which some transform\nacts on the entire set of instance features. The idea is to jointly process the set\nof features to output an updated set (one for each input feature), that re-embeds\neach feature in light of the joint statistics of the entire instance. The proposed\nfeatures transform can be seen as a special case of an attention mechanism\n[29] specialized to features of instance-specific tasks, with required adaptations.\nTechniques developed here borrow from and might lend to those used in set-to-\nset [44,42,25], self-attention [29,26] and transformer [22,16] architectures.\n1.2\nContributions\nWe propose a parameter-less transform T, which can be used as a drop-in addi-\ntion that can convert a conventional network to an instance-aware one (e.g. from\nFig. 2 Left to Right). We propose an optimal-transport based feature transform\nwhich is shown to have the following attractive set of qualities. (i) efficiency:\nhaving real-time inference; (ii) differentiability: allowing end-to-end training of\nthe entire ‘embedding-transform-inference’ pipeline of Fig. 2 Right; (iii) equiv-\nariance: ensuring that the embedding works coherently under any order of the\n\n\nThe Self-Optimal-Transport Feature Transform\n3\nFig. 2: Generic designs of networks that act on sets of inputs. These cover relevant\narchitectures, e.g. for few-shot-classification and clustering. Left: A generic network for pro-\ncessing a set of input items typically follows the depicted structure: (i) Each item separately\ngoes through a common feature extractor F. (ii) The set of extracted features is the input\nto a downstream task processing module G. ; Right: A more general structure in which the\nextracted features undergo a joint processing by a transform T. Our SOT transform (as well\nas other attention mechanisms) is of this type and its high-level design (within the ‘green’\nmodule) is detailed in Fig. 1.\ninput items; (iv) capturing relative similarity: The comparison of embedded vec-\ntors will include both direct and indirect (third-party) similarity information be-\ntween the input features; (v) probabilistic interpretation: each embedded feature\nwill encode its distribution of similarities to all other features, by conforming to\na doubly-stochastic constraint; (vi) instance-aware dimensionality: embedding\ndimension (capacity) is adaptive to input size (complexity).\nWe provide a detailed analysis of our method and show its flexibility and ease\nof application to a wide variety of tasks, by incorporating it in leading methods of\neach kind. A controlled experiment on unsupervised clustering is used to verify its\nperformance, with a detailed analysis. For few-shot-classification we perform an\nextensive comparison to existing work on several benchmarks, showing that SOT\nachieves new state-of-art results. Finally, we show that SOT is easily applicable\nto large-scale benchmarks by using the person re-identification task, for which\nit consistently improves state-of-art networks that it is incorporated into.\n2\nRelated Work\n2.1\nRelated Techniques\nSet-to-set or set-to-feature functions Our method can clearly be categorized\nalong with recent techniques that act jointly on a set of items (typically fea-\ntures) to output an updated set (or a single feature), which are typically used\nfor downstream inference tasks on the items individually, or as a set. The pio-\nneering Deep-Sets [44] formalized fundamental requirements from architectures\nthat process sets. Point-Net [27] presented an influential design that learns lo-\ncal and global features on 3D point-clouds, while Maron et.al. [25] study layer\n\n\n4\nDaniel Shalam and Simon Korman\ndesigns that approximate equivariant and invariant functions. Unlike the pro-\nposed SOT transform, the joint processing in these methods is very limited,\namounting to (Siamese) weight-sharing between separate processes and simple\njoint aggregations like average pooling.\nSelf-Attention The introduction of Relational Networks [32] and transform-\ners [38] and their initial applications in vision models [29] have lead to a surge of\nfollowing successful works [16], many of which are dedicated to few-shot-learning,\nsuch as ReNet [15], DeepEMD [45] and FEAT[42]. Different from these methods,\nSOT is parameterless, and hence can work at test-time on any pre-trained net-\nwork. In addition, SOT is the only method that provides an explicit probabilistic\nglobal interpretation of the instance data.\nOptimal Transport Optimal transport (OT) problems are tightly related to mea-\nsuring and calculating distances between distributions or sets of features. In [7]\nCuturi popularized the Sinkhorn algorithm which is a simple, differentiable and\nfast approximation of entropy-regularized OT problems. The Set transformer [22]\nuses an OT-based clustering algorithm, SuperGlue [33] uses OT in an end-to-\nend manner for feature-point matching, and many state-of-the-art methods in\nfew-shot learning, which we review next, have adopted the Sinkhorn algorithm\nto model relations between features and class representations. The differentia-\nbility and efficiency of regularized OT solvers has recently been shown useful in\nrelated domains, to derive a differentiable ‘top-k’ operator [41] or for style trans-\nfer applications, by viewing styles as a distributions between which distances\nare approximated [18]. In this work we focus on self applications of OT, which\nenables concise modelings of the relative similarities within a set of items.\n2.2\nFew-Shot-Classification (FSC)\nFew-Shot-Classification [39] is a branch of few-shot-learning in which a classifier\nneeds to learn to recognize classes unseen given a limited number of labeled\nexamples. A FSC task is a self-contained instance that includes both support\n(labeled) and query (unlabeled) items, hence is a clear instance-specific setup\nwhich SOT can handle.\nSome leading FSC approaches follow the meta-learning (or “learn-to-learn”)\nprinciple in which the training data is split into tasks (or episodes) mimicking\nthe test time tasks to which the learner is required to generalize. The celebrated\nMAML [10] “learns to fine-tune” by learning a network initialization from which\nit can adapt to a novel set of classes with very few gradient update steps on the\nlabeled examples. In ProtoNet [34], a learner is meta-trained to predict query\nfeature classes, based on distances from support (labeled) class-prototypes in the\nembedding space. The trainable version of SOT is a meta-learning algorithm,\nbut unlike the above, it is transductive (see ahead) and exploits the task items as\na set, while directly assessing the relative similarity relations between its items.\nSubsequent works [5,9] have questioned the benefits of meta-learning, advo-\ncating the standard transfer learning procedure of fine-tuning pre-trained net-\nworks. In particular, they demonstrate the advantages of using larger and more\n\n\nThe Self-Optimal-Transport Feature Transform\n5\npowerful feature-encoding architectures, as well as the employment of trans-\nductive inference, which fully exploits the data of the inference task, including\nunlabeled images. As mentioned, SOT is a purely transductive method, but it is\nsignificantly more flexible in its assumptions, since the transform is based on a\ngeneral probabilistic grouping action. It does not make any assumptions on (nor\ndoes it need to know) the number of classes and the number of items per class\nin an instance.\nMore recently, attention mechanisms were shown to be effective for FSC. We\nhave reviewed some relevant works of this line in the previous section.\nFinally, a large number of works have adopted the Sinkhorn Algorithm [7] as\na parameterless unsupervised classifier that computes fractional matchings be-\ntween query embeddings and class centers. Many leading FSC works use this ap-\nproach, including Laplacian-Shot [50], CentroidNet [13] and PT-MAP [12]. The\ncurrent state-of-the-art is set by the recent Sill-Net [46], which augments training\nsamples with illumination features that are separated from the images in fea-\nture space and by PT-MAP-sf [6], who propose a DCT-based feature embedding\nnetwork, encoding detailed frequency-domain information that complements the\nstandard spatial domain features. Both methods are based on PT-MAP [12].\nSOT uses Sinkhorn to solve an entirely different OT problem - that of matching\nthe set of features to itself, rather than against class representations. Never-\ntheless, SOT can be incorporated into these methods, immediately after their\nfeature extraction stage.\n2.3\nUnsupervised Clustering and Person Re-Identification (Re-ID)\nThese domains are not at the focus of this work therefore we only briefly give\nsome useful pointers for the sake of brevity.\nUnsupervised image clustering is an active area of research, with standard-\nised evaluation protocols (from Cifar-10 [20] to different subsets of ImageNet [8]).\nProminent works in this area include Deep Adaptive Clustering (DAC) [4], In-\nvariant Information Clustering (IIC) [14] and SCAN [37]. Clustering has recently\ngained popularity as a means for self-supervision in feature learning, showing\nexcellent results on unsupervised image classification. See for example Deep-\nCluster [2] and SWAV [3]. Clustering is a clear case instance specific problem,\nsince most information is relative and unrelated directly to other training data.\nOur transform can hence be used to upgrade the feature representation quality.\nWe chose the Re-ID application as another instance-specific problem, which\nfrom our point of view differs from the others considered in two main aspects\nwhich we find attractive: (i) The tasks are of larger scale - querying thousands\nof identities against a target set of (tens of) thousands. (ii) The data is much\nmore real-world compared to the carefully curated classification and clustering\ntasks. See [43] for an excellent recent and comprehensive survey on the topic.\n\n\n6\nDaniel Shalam and Simon Korman\n3\nMethod\nAssume we are given a task which consists of an inference problem over a set\nof n items {xi}n\ni=1, where each of the items belongs to a space of input items\nΩ⊆RD. The inference task can be modeled as fθ({xi}n\ni=1), using a learned\nfunction fθ, which acts on the set of input items and is parameterized by a set\nof parameters θ.\nTypically, such functions combine an initial feature extraction stage that is\napplied independently to each input item, with a subsequent stage of (separate\nor joint) processing of the feature vectors (see Fig. 2 Left or Right, respectively).\nThat is, the function fθ takes the form fθ({xi}n\ni=1) = Gψ({Fϕ(xi)}n\ni=1), where\nFϕ is the feature extractor (or embedding network) and Gψ is the task inference\nfunction, parameterized by ϕ and ψ respectively, where θ = ϕ ∪ψ.\nThe feature embedding F : RD →Rd, usually in the form of a neural-network\n(with d ≪D), could be either pre-trained, or trained in the context of the task\nfunction f, along with the inference function G.\nFor an input {xi}n\ni=1, let us define the set of features {vi}n\ni=1 = {F(xi)}n\ni=1. In\nthe following, we consider these sets of input vectors and features as real-valued\nrow-stacked matrices X ∈Rn×D and V ∈Rn×d.\nWe suggest a novel re-embedding of the feature set V, using a transform that\nwe denote by T, in order to obtain a new set of features W = T(V), where W ∈\nRn×n. The new feature set W has an explicit probabilistic interpretation, which\nis specifically suited for tasks related to classification, matching or grouping of\nitems in the input set X. In particular, W will be a symmetric, doubly-stochastic\nmatrix, where the entry wij (for i ̸= j) gives the probability that items xi and\nxj belong to the same class or cluster.\nThe proposed transform T : Rn×d →Rn×n (see Fig. 1) acts on the original\nfeature set V as follows. It begins by computing the squared Euclidean pairwise\ndistances matrix D, namely, dij = ||vi −vj||2, which can be computed efficiently\nas dij = 2(1 −cos(vi, vj)) = 2(1 −vi · vT\nj ), assuming that the rows of V are unit\nnormalized. Or in a compact form, D = 2(1 −S), where 1 is the all ones n × n\nmatrix and S = V · VT is the cosine similarity matrix of V.\nW will be computed as the optimal transport (OT) plan matrix between the\nn-dimensional all-ones vector 1n and itself, under the cost matrix D∞, which is\nthe distance matrix D with a very (infinitely) large scalar replacing each of the\nentries on its diagonal (which were all zero). Explicitly, let D∞= D + αI, where\nα is a very (infinitely) large constant and I is an n × n identity matrix.\nW is defined to be the doubly-stochastic matrix1 that is the minimizer of the\nfunctional\n  \\la bel\n {eq\n.fra ctional_matching} \\mathcal {W}=\\argminA _{\\mathcal {W}\\in B_n}\\:\\langle \\mathcal {D}_\\infty ,\\mathcal {W}\\rangle \n(1)\nwhere Bn is the set (known as the Birkhoff polytope) of n × n doubly-stochastic\nmatrices and ⟨·, ·⟩stands for the Frobenius (standard) dot-product.\n1 a square (n×n) matrix of non-negative real values, each of whose rows and columns\nsums to 1\n\n\nThe Self-Optimal-Transport Feature Transform\n7\nThis objective can be minimized using simplex or interior point methods with\ncomplexity Θ(n3 log n). In practice, we use the highly efficient Sinkhorn-Knopp\nmethod [7], which is an iterative scheme that optimizes an entropy-regularized\nversion of the problem, where each iteration takes Θ(n2). Namely:\n  \\la bel\n {eq\n.ent ro p y\n_min} \\mathcal {W}=\\argminA _{\\mathcal {W}\\in B_n}\\:\\langle \\mathcal {D}_\\infty ,\\mathcal {W}\\rangle -\\frac {1}{\\lambda }h(\\mathcal {W}) \n(2)\nwhere h(W) = −P\ni,j wij log(wij) is the Shannon entropy of W and λ is the\nentropy regularization parameter.\nThe transport-plan matrix W that is the minimizer of Eq. (2) is the result of\nour transform, i.e. W = T(V) and each of its rows is the re-embedding of each of\nthe corresponding features (rows) in V. Recall that W is doubly-stochastic and\nnote that it is symmetric2. We next explain its probabilistic interpretation.\nThe optimization problem in Eq. (1) can be written more explicitly as follows:\n  \\\nb egin  {\nalig\nn e d}  \\m i n _ {\\mathcal {W}} \\; \\langle \\mathcal {D}_\\infty ,\\mathcal {W}\\rangle \\quad \\quad \\textrm {s.t.} \\quad \\quad & \\mathcal {W}\\cdot \\textbf {1}_n=\\mathcal {W}^T\\cdot \\textbf {1}_n= \\textbf {1}_n \\end {aligned} \\label {eq.opt_D_inf} \n(3)\nwhich can be seen to be the same as:\n  \\\nb egi n \n{ali\ng n ed }  \\ m in  _{\n\\ma t h\ncal\n { W} }  \\ ; \\langle \\mathcal {D},\\mathcal {W}\\rangle \\quad \\quad \\textrm {s.t.} \\quad \\quad & \\mathcal {W}\\cdot \\textbf {1}_n=\\mathcal {W}^T\\cdot \\textbf {1}_n= \\textbf {1}_n \\\\ & w_{ii}=0 \\quad \\text {for}\\quad i = 1,\\dots n \\end {aligned} \\label {eq.opt_fractional_matching} \n(4)\nsince the use of the infinite weights on the diagonal of D∞is equivalent to using\nthe original D with a constraint of zeros along the diagonal of W.\nThe optimization problem in Eq. (4) is in fact a fractional matching instance\nbetween the set of n original features and itself. It can be posed as a bipartite-\ngraph min-cost max-flow instance. The graph has n nodes on each side, repre-\nsenting the original features {vi}n\ni=1 (the rows of V). Across the two sides, the\ncost of the edge (vi, vj) is the distance dij and the edges of the type (vi, vi) have\na cost of infinity (or can simply be removed). Each ‘left’ node is connected to a\n’source’ node by an edge of cost 0 and similarly each ’right’ node is connected to\na ‘target’ (sink) node by an edge of cost 0. All edges in the graph have a capac-\nity of 1 and the goal is to find an optimal fractional self matching, by finding a\nmin-cost max-flow from source to sink. Note that the maximum flow can easily\nbe seen to be n, but a min-cost flow is sought among the max-flows.\nIn this set-to-itself matching view, each vector vi is fractionally matched to\nthe set of all other vectors V −{vi} based on the pairwise distances, but im-\nportantly taking into account the fractional matches of the rest of the vectors\nin order to satisfy the double-stochasticity constraint3. Therefore, the ith trans-\nformed (re-embedded) feature wi (ith row of W) is a distribution (non-negative\nentries, summing to 1), where wii = 0 and wij is the relative belief that features\ni and j belong to the same ‘class’.\n2 The symmetry of W is as a result of the symmetry of D and the double-stochasticity\nof W.\n3 The construction constrains the maximum flow to exactly have a total outgoing flow\nof 1 from each ‘left’ node and a total incoming flow of 1 from each ‘right’ node.\n\n\n8\nDaniel Shalam and Simon Korman\n(a) orig.\n(b) S\n(c) W\n(d) D\n(e) DW\n(f) SOT\n(orig. embedding) (cos-similarity) (SOT features)\n(orig. dists)\n(SOT dists)\n(SOT embedding)\nFig. 3:\nA close look at the SOT transform as it operates on a 10-way 20-shot\nsupervised clustering task: The input is a set of 200 33-dimensional unit-length feature\nvectors that are visualized on the plane in (a) using a t-SNE dimension reduction [36],\nwhere colors refer to the 10 classes. In (b) is the pairwise cosine similarity matrix S, which\nis linearly related to the Euclidean pairwise distances D shown in (d). Next, in (c) we show\nthe SOT matrix W whose rows (or columns, symmetrically) consist of our new embedding\nof the features. These 200-dimensional features are shown again on the plane in (f). Notice\nthe visually apparent improvement in point gathering by class, from (a) to (f), which can\nbe explained by comparing the matrices D and DW, which are the self-pairwise distances of\nthe original and SOT embedding respectively. Notice the greater contrast in DW between\ninter- and intra- cluster points. Note, that like in the visualizations of Fig. 1, we show the\nmatrices with row/col order based on the true classes, purely for ease of visualization.\nOur final set of features W is obtained by replacing the diagonal entries from\n0s to 1s, namely W = W + I, where I is the n × n identity matrix. Please refer\nto Fig. 3 for a close look at the application of SOT to a toy clustering prob-\nlem, where we demonstrate visually the improved embedding obtained through\nexamining the pairwise distances before and after the transform. We can now\npoint out some important properties of this new embedding W:\nDirect and Indirect similarity encoding: Each embedded feature encodes\nits distribution of similarities to all other features. An important property of our\nembedding is that the comparison of the embedded vectors wi and wj includes\nboth direct and indirect information about the similarity between the features.\nPlease refer to Fig. 4 for a detailed explanation of this property. If we look\nat the different coordinates k of the absolute difference vector a = |wi −wj|,\nSOT captures (i) direct similarity: For k which is either i or j, it holds that\nak = 1 −wij = 1 −wji 4. This amount measures how high (i.e.close to 1) is\nthe mutual belief of features i and j about one another. (ii) indirect (3rd-party)\nsimilarity: For k /∈{i, j}, we have ak = |wik −wjk|, which is a comparison of\nthe beliefs of features i and j regarding the (third-party) feature k.\nParameterless-ness: Our proposed transform is parameterless, giving it the\nflexibility to be used in other pipelines, directly over different kinds of embed-\ndings, without the harsh requirement of retraining the entire pipeline5.\n4 Note: (i) wii = wjj = 1 ; (ii) wij = wji from the symmetry of W ; (iii) all elements\nof W are ≤1 and hence the | · | can be dropped ;\n5 Retraining is certainly possible, and beneficial in many situations, but not manda-\ntory, as our experiments work quite well without it.\n\n\nThe Self-Optimal-Transport Feature Transform\n9\nFig. 4:\nThe (symmetric) embedding matrix\nW and the absolute difference between its ith\nand jth rows: We examine the vector |wi −wj|:\n(i) Its ith and jth coordinates equal |1 −wij| =\n|1 −wji|, giving the direct similarity between the\noriginal features, since this amount (in green) is\ngreater when wij and wji (the mutual beliefs) are\nhigh (closer to 1). ; (ii) Its kth coordinate (for any\nk /∈{i, j}) gives |wik −wjk| which is an indirect\n(third-party) comparison between the original fea-\ntures through the kth feature. Similarity (in yellow)\nis stronger when features i and j have a similar be-\nlief regarding feature k, i.e. wik and wjk are close.\nDifferentiability: Due to the differentiability of Cuturi’s [7] version of Sinkhorn,\nback-propagating through the SOT can be done naturally, hence it is possible\nto (re-)train the hosting network to adapt to the SOT, if desired.\nEquivariance: The embedding works coherently with respect to any change\nof order of the input items (features). This can be shown by construction, since\nmin-cost max-flow solvers as well as the Sinkhorn OT solver are equivariant with\nrespect to permutations of their inputs.\nExplainability: The non-parametric nature gives SOT an advantage over other\nset-to-set methods such as transformers in that its output is interpretable (e.g.\nby visually inspecting the transport-plan matrix W), with a clear probabilistic\ncharacterization of the relations it had found.\nTask-Aware Dimensionality: SOT has the unique property that the dimen-\nsion of the embedded feature depends on (equals) the number of features. On\nthe one hand, this is a desired property, since it is only natural that the feature\ndimensionality (capacity) depends on the complexity of the task, which typically\ngrows with the number of features (think of the inter-relations which are more\ncomplex to model). On the other hand, it might impose a problem in situations\nin which the downstream calculation that follows the feature embedding expects\na fixed input size, for example a pre-trained non-convolutional layer. Neverthe-\nless, in many situations the downstream computation has the flexibility to work\nwith varying input dimensions. Also, in most benchmarks the instance set sizes\nare fixed, allowing for a single setting of sizes to work throughout.\n4\nImplementation details\nDatasets: We consider three different applications to evaluate the performance\nof our method. For unsupervised clustering we designed a specialized synthetic\ndata set with the goal of enabling controlled experiments over a wide range of\ndifficulties, which are determined by data dimensionality and in-cluster spread.\n\n\n10\nDaniel Shalam and Simon Korman\nFor few-shot classification we use the standard benchmarks in the literature.\nThe MiniImagenet [39] dataset is a subset of Imagenet [31] that contains 100\nclasses and 600 images of size 84x84 per class. We follow the standard setup\nof using 64 classes for training and 16 and 20 novel classes for validation and\ntesting. The CIFAR-FS [1] dataset includes 100 classes with 600 images of size\n32 × 32 per-class. We used the same splits as in MiniImagenet for this dataset.\nThe CUB [40] dataset includes 200 classes of bird species and has 11,788 images\nof size 84 × 84 pixels in total. We followed the split suggested in [11] into 100\nbase classes, 50 validation classes and 50 novel classes.\nFor person re-identification (ReID) we use two common large-scale datasets.\nThe Market-1501 [47] and CUHK03 [23] dataset consists of 1,501 and 1,467\nidentities and a total of 32,668 and 14,097 images taken from 6 cameras. We use\nthe validation and test sets according to the splits in [49].\nPre-training: We pre-trained ProtoNet [34] with a 4-layer Convolution network\nadapting the procedures of [34] for training both with and without SOT, training\non a 5-way (5/1)-shot 15-query task, using ADAM [17] with learning rate 0.01\nand step size of 20 over 100 episodes (tasks) per epoch.\nFine-tuning: We perform fine-tuning on two types of backbone residual net-\nworks - a resnet-12 as used in [42] and a WRN-28-10 as used in [24]. For Pro-\ntoNet [34] and ProtoNet-SOT, we fine-tune the base network with parameters\ntaken from [42]. For PTMAP-SOT, we use meta-training with batches of a single\n10-way 5-shot 15-query task per batch. We use ADAM with learning rate 5e −5\nthat decreases with step size 10 for 25 epochs. We train the WRN-28-10 and the\nresnet-12 backbones for 800 and 100 episodes respectively per epoch.\nHyper-parameters: SOT has two hyper-parameters which were chosen through\ncross-validation and were kept fixed for each of the applications over all datasets.\n(i) The number of Sinkhorn iterations for computing the optimal transport plan\nwas fixed to 10. (ii) The entropy regularization parameter λ (Eq. (3)) was set\nto 0.1 for clustering and few-shot-learning experiments and to 1.0 for the ReID\nexperiments. We further ablate these in the supplementaries.\n5\nResults\n5.1\nClustering on the Sphere\nWe first demonstrate the effectiveness of SOT using a controlled synthetically\ngenerated clustering experiment, with k = 10 cluster centers that are distributed\nuniformly at random on a d-dimensional unit-sphere, and 20 points per cluster\n(200 in total) that are perturbed around the cluster centers by Gaussian noise\nof increasing standard deviation, of up to 0.75, followed by a re-projection back\nto the sphere by dividing each vector by its L2 magnitude. We also apply di-\nmensionality reduction with PCA to d = 50, for dimensions above 50.\nWe performed the experiment over a logarithmic 2D grid of combinations\nof data dimensionalities d in the range [10, 1234] and Gaussian in-cluster noise\nSTD in the range [0.1, 0.75]. Refer to Fig. 9 (i) for a visualization of the data\ngeneration process.\n\n\nThe Self-Optimal-Transport Feature Transform\n11\nSTD=0.01\nSTD=0.12\nSTD=0.20\nSTD=0.31\nSTD=0.48\nSTD=0.75\n(i) 10 Random cluster centers\non the unit sphere, perturbed\nwith increasing noise STD σ.\n(ii) Clustering accuracy across dimensions d (left) and noise levels\nσ (right). For each configuration, k-means accuracy is reported\nwhen applied with original (solid) and SOT (dashed) features.\nFig. 5:\nClustering on the d-dimensional sphere. Left (i): the data generation pro-\ncess (illustrated for the 3D case). Right (ii): detailed k-means accuracy results. The SOT\n(dashed) features give superior results throughout a majority of the space of settings.\nEach point is represented by its d-dimensional euclidean coordinates vector,\nwhere the baseline clustering is obtained by running k-means on these location\nfeatures. In addition, we run k-means on the set of features that has undergone\nSOT. Hence, the benefits of the transform (embedding) are measured indirectly\nthrough the accuracy6 achieved by running k-means on the embedded vs. original\nvectors. Evaluation results are reported in Fig. 9 (ii) as averages over 10 runs, by\nplotting accuracy vs. dimensionality (for different noise STDs) and accuracy vs\nnoise STDs (for different dimensionalities). The results show (i) general accuracy\ngains and robustness to wide ranges of data dimensionality (ii) the ability of\nSOT to find meaningful representations that enable clustering quality to degrade\ngracefully with the increase in cluster noise level. Note that the levels of noise\nare rather high, as they are relative to a unit radius sphere (a 3-dimensional\nexample is shown at the top of the figure). We provide further details on this\nexperiment in the supplementaries.\n5.2\nFew-Shot Classification (FSC)\nOur main experiment is a comprehensive evaluation on the standard few-shot\nclassification benchmarks MiniImagenet [39], CIFAR-FS [1], and CUB [40], with\ndetailed results in Tables 1 and 2. For MiniImagenet (Table 1) we report on both\nversions “SOTp” and “SOTt” over a range of backbone architectures, while for\nthe smaller datasets CIFAR-FS and CUB (Table 2) we focus on the ‘drop-in’\nversion “SOTp” and only the strongest wrn-28-10 architecture.\nOne goal here is to show that we can achieve new state-of-the-art FSC results,\nwhen we build on current state-of-the-art. But more importantly, we demonstrate\nthe flexibility and simplicity of applying SOT in this setup, with improvements\nin the entire range of testing, including: (i) when building on different ‘host-\ning’ methods; (ii) when working above different feature embeddings of different\n6 Accuracy is measured by comparison with the optimal permutation of the predicted\nlabels, found by the Hungarian Algorithm [21].\n\n\n12\nDaniel Shalam and Simon Korman\nmethod\ntransductive\nbackbone\n5way-1shot\n5way-5shot\nMAML(*) [10]\n✗\nconv-4\n46.47\n62.71\nRelationNet(*) [35]\n✗\nconv-4\n49.31\n66.60\nProtoNet(#) [34]\n✗\nconv-4\n49.10\n66.79\nFEAT($) [42]\n✗\nconv-4\n55.15\n71.61\nProtoNet-SOTp\n✓\nconv-4\n54.01 (+10.2%)\n69.39 (+3.9%)\nProtoNet-SOTt\n✓\nconv-4\n53.70 (+9.3%)\n70.40 (+5.4%)\nProtoNet(#) [34]\n✗\nresnet-12\n62.39\n80.33\nDeepEMD($) [45]\n✗\nresnet-12\n65.91\n82.41\nFEAT($) [42]\n✗\nresnet-12\n66.78\n82.05\nRENet($) [15]\n✗\nresnet-12\n67.60\n82.58\nPTMAP(#) [12]\n✓\nresnet-12\n76.90\n85.20\nProtoNet-SOTp\n✓\nresnet-12\n67.34 (+7.9%)\n81.84 (+1.6%)\nProtoNet-SOTt\n✓\nresnet-12\n67.90 (+8.8%)\n83.09 (+3.2%)\nPTMAP-SOTp\n✓\nresnet-12\n78.35 (+1.9%)\n86.01 (+1.0%)\nPTMAP-SOTt\n✓\nresnet-12\n77.30 (+0.5%)\n85.49 (+0.3%)\nProtoNet(&) [34]\n✗\nwrn-28-10\n62.60\n79.97\nPTMAP($) [12]\n✓\nwrn-28-10\n82.92\n88.80\nSill-Net($) [46]\n✓\nwrn-28-10\n82.99\n89.14\nPTMAP-SF($) [6]\n✓\nwrn-28-10\n84.81\n90.62\nPTMAP-COSINE\n✓\nwrn-28-10\n74.60 (-10.0%)\n84.68 (-4.6%)\nPTMAP-SOFTMAX\n✓\nwrn-28-10\n80.08 (-3.4%)\n83.83 (-5.6%)\nPTMAP-SOTp\n✓\nwrn-28-10\n83.19 (+0.3%)\n89.56 (+0.9%)\nPTMAP-SOTt\n✓\nwrn-28-10\n84.18 (+1.5%)\n90.51 (+1.9%)\nSill-Net-SOTp\n✓\nwrn-28-10\n83.35 (+0.4%)\n89.65 (+0.6%)\nPTMAP-SF-SOTp\n✓\nwrn-28-10\n85.59 (+0.9%)\n91.34 (+0.8%)\nTable 1:\nFew-Shot Classification (FSC) accuracy on MiniImagenet [39]. The im-\nprovements introduced by the variants of SOT (percentages in brackets) are in comparison\nwith each respective baseline hosting method. Bold and underline notations highlight best\nand second best results per backbone. (*) = from [5] ; (&) = from [50] ; ($) = from the\nmethod’s paper itself ; (#) = our implementation ;\ncomplexity backbones; and (iii) whether retraining the hosting network or just\ndropping-in SOT and performing standard inference.\nTo evaluate the performance of the proposed SOT, we applied it to previous\nFSC methods including the very recent state-of-the-art (PT-MAP [12], Sill-NET\n[46] and PT-MAP-SF [6]) as well as a to more conventional methods like the\npopular ProtoNet [34]. The detailed results are presented in Tables 1 and 2)\nfor the different datasets. Note that SOT is by nature a transductive method7,\nhence we marked its results as so, regardless of whether the hosting network is\ntransductive or not. In the following, we discuss the two modes in which our\ntransform can be used in existing FSC methods.\n7 SOT is transductive in the sense that it needs to jointly process the data, but im-\nportantly, unlike other methods it does not gain its benefit in being so from making\nlimiting assumptions about the structure of the instance, like knowing the number\nof classes, or the number of items per class.\n\n\nThe Self-Optimal-Transport Feature Transform\n13\nFSC benchmark\nCIFAR-FS [1]\nCUB [40]\nmethod\n5way-1shot\n5way-5shot\n5way-1shot\n5way-5shot\nPTMAP($) [12]\n87.69\n90.68\n91.55\n93.99\nSill-Net($) [46]\n87.73\n91.09\n94.73\n96.28\nPTMAP-SF($) [6]\n89.39\n92.08\n95.45\n96.70\nPTMAP-SOTp\n87.37 (-0.4%)\n91.12 (+0.5%)\n91.90 (+0.4%)\n94.63 (+0.7%)\nSill-Net-SOTp\n87.30 (-0.5%)\n91.40 (+0.3%)\n94.86 (+0.1%)\n96.61 (+0.3%)\nPTMAP-SF-SOTp 89.94 (+0.6%) 92.83 (+0.8%) 95.80 (+0.4%) 97.12 (+0.4%)\nTable 2: Few-Shot Classification (FSC) accuracy on CIFAR-FS [1] and CUB [40].\nSOT insertion without network retraining (notated by SOTp in Tables\n1 and 2). Recall that the proposed transform is non-parametric. As such, we\ncan simply apply it to a trained network at inference, without the need to\nre-train. This basic ‘drop-in’ use of SOT consistently, and in many cases also\nsignificantly, improved the performance of the tested methods, including state-\nof-the-art, across all benchmarks and backbones. SOTp gave improvements of\naround 3.5% and 1.5% on 1 and 5 shot MiniImagenet tasks. This improvement\nwithout re-training the embedding backbone network shows SOT’s effectiveness\nin capturing meaningful relationships between features in a very general sense.\nSOT insertion with network retraining (notated by SOTt in Table 1). Due\nto its differentiability property, the proposed method can be applied while train-\ning and hence we expect an adaptation of the hosting network’s parameters to\nthe presence of the transform with a potential for improvement. To evaluate this\nmode, we focused on the MiniImagenet benchmark [39], specifically on the same\nconfigurations that we used without re-training, to enable a direct comparison.\nThe results in Table 1 show additional improvements in almost every method.\nSOTt gave improvements of around 5% and 3% on 1 and 5 shot MiniImagenet\ntasks, further improving on the pre-trained counterpart. This result indicates\nthe effectiveness of training with SOT in an end-to-end fashion.\nAblations Within the context of few-shot learning on MiniImagenet, we per-\nformed several ablation studies. In Table 1, the networks ‘PTMAP-COSINE’\nand ‘PTMAP-SOFTMAX’ stand for the obvious baseline attempts (found to be\nunsuccessful) that work in the line of our approach, without the specialized OT-\nbased transform. In the former, we take the output features to be the rows of\nthe (un-normalized) matrix S (rather than those of W) and in the latter we also\nnormalize its rows using soft-max. In the supplementaries we ablate on SOT’s\ntwo parameters - the number of Sinkhorn iterations and the entropy term λ.\n5.3\nPerson re-Identification (Re-ID)\nIn this section, we explore the possibility of using SOT on large-scale datasets by\nconsidering the Person re-Identification task. Given a set of query images and a\nlarge set of gallery images, the task is to rank the similarities of each single query\nagainst the gallery. This is done by computing specialized image features among\nwhich similarities are based on Euclidean distances. SOT is applied to such pre-\n\n\n14\nDaniel Shalam and Simon Korman\nReID benchmark\nCUHK03-detected [23]\nMarket-1501 [47]\nnetwork\nmAP\nRank-1\nmAP\nRank-1\nTopDBNet [28]\n72.9\n75.7\n85.7\n94.3\nTopDBNet-rerank [28]\n87.1\n87.1\n94.0\n95.3\nTopDBNet-SOTp\n77.9 (+6.9%) 80.4 (+6.2%) 88.1 (+2.8%) 94.4 (+0.1%)\nTopDBNet-rerank-SOTp 87.9 (+0.9%) 88.0 (+1.0%) 94.0 (0.0%)\n95.0 (-0.3%)\nTable 3: Re-ID results on CUHK03 [23] and Market-1501 [47]\ncomputed image features, refining them with the strong relative information that\nit is able to capture by applying it on the union of all query and gallery features.\nWe adapted a pre-trained standard resnet-50 architecture [49] and the popular\nTopDBNet [28], which we tested on the large-scale ReID benchmarks CUHK03\n[23] (on the ’detected’ version and similar results on the ‘labeled’ version in the\nsupplementaries) and Market-1501 [47], with and without the re-ranking [48]\nprocedure. For evaluation, we followed their conventions and compare results\nusing the mAP (mean Average Precision) and Rank-1 metrics.\nThe results in Table 3 show a consistent benefit in using SOT within the\ndifferent networks. For CUHK03, the results improved by a large margin of\n+6.8% in mAP for the best configuration. These results demonstrate that the\nproposed SOT scales well to large-scale problems (with number of features in the\nthousands) and is attractive for a variety of applications. ReID is not the main\nfocus of this work, hence, we did not re-train the hosting networks with SOT\nincluded. Further research is required to measure the possible effects of doing so.\n6\nConclusions, Limitations and Future Work\nIn this paper, we explored the idea of utilizing global information of features,\nfor instance-specific problems such as clustering, few-shot learning, and per-\nson re-identification. We proposed a novel module: the Self-Optimal-Transport\n(SOT) - a features transform that is non-parametric, differentiable and which\ncan capture high-level relationships between data points in problems of this\nnature. The proposed method outperforms state-of-the-art networks on popular\nfew-shot classification benchmarks and shows consistent improvements on tested\nReID benchmarks. Based on these promising results, we believe that exploring\nits full potential can lead to improvements in a variety of fields and open new\npossibilities.\nIn future work, we plan to address some current limitations. (i) Regarding\nthe output dimensionality of the embedding, which is dictated by the input set\nsize. We will aim at being able to obtain an arbitrary dimension, for increased\nusage flexibility; (ii) We plan to investigate the usage of SOT in unsupervised\nsettings, which would be possible by utilizing its informative representation for\nself-supervision; (iii) It would likely be beneficial to have a variant of SOT in\nwhich the transform is enriched with learnable parameters, similar to transform-\ners, to extend its modeling capacity even further; (iv) SOT is purely transduc-\ntive. We plan to explore non-transductive variants, possibly by comparing each\nsample separately to the support or gallery sets.\n\n\nThe Self-Optimal-Transport Feature Transform\n15\nReferences\n1. Luca Bertinetto, Joao F. Henriques, Philip Torr, and Andrea Vedaldi.\nMeta-\nlearning with differentiable closed-form solvers. In International Conference on\nLearning Representations (ICLR), 2019. 10, 11, 13\n2. Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep\nclustering for unsupervised learning of visual features. In Proceedings of the Euro-\npean Conference on Computer Vision (ECCV), 2018. 5\n3. Mathilde Caron, Ishan Misra, J. Mairal, Priya Goyal, Piotr Bojanowski, and Ar-\nmand Joulin. Unsupervised learning of visual features by contrasting cluster as-\nsignments. ArXiv, abs/2006.09882, 2020. 5\n4. Jianlong Chang, Lingfeng Wang, Gaofeng Meng, Shiming Xiang, and Chunhong\nPan. Deep adaptive image clustering. In Proceedings of the IEEE International\nConference on Computer Vision (ICCV), 2017. 5\n5. Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin\nHuang. A closer look at few-shot classification. In International Conference on\nLearning Representations (ICLR), 2018. 4, 12\n6. Xiangyu Chen and Guanghui Wang. Few-shot learning by integrating spatial and\nfrequency representation. arXiv preprint arXiv:2105.05348, 2021. 5, 12, 13\n7. Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport.\nIn Advances in Neural Information Processing Systems (NeurIPS), 2013. 2, 4, 5,\n7, 9, 18\n8. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet:\nA large-scale hierarchical image database. In 2009 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR). IEEE, 2009. 5\n9. Guneet S Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A\nbaseline for few-shot image classification. In International Conference on Learning\nRepresentations (ICLR), 2020. 4\n10. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning\nfor fast adaptation of deep networks.\nIn International Conference on Machine\nLearning (ICML), 2017. 4, 12\n11. Yuqing Hu, Vincent Gripon, and St´ephane Pateux. Exploiting unsupervised inputs\nfor accurate few-shot classification. ArXiv, abs/2001.09849, 2020. 10\n12. Yuqing Hu, Vincent Gripon, and St´ephane Pateux. Leveraging the feature distri-\nbution in transfer-based few-shot learning. In arXiv preprint arXiv:2006.03806,\n2020. 5, 12, 13\n13. Gabriel Huang, Hugo Larochelle, and Simon Lacoste-Julien. Are few-shot learning\nbenchmarks too simple? solving them without task supervision at test-time. arXiv\npreprint arXiv:1902.08605, 2019. 5\n14. Xu Ji, Joao F Henriques, and Andrea Vedaldi.\nInvariant information cluster-\ning for unsupervised image classification and segmentation. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision (ICCV), 2019. 5\n15. Dahyun Kang, Heeseung Kwon, Juhong Min, and Minsu Cho. Relational embed-\nding for few-shot classification.\nIn Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), 2021. 4, 12\n16. Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fa-\nhad Shahbaz Khan, and Mubarak Shah. Transformers in vision: A survey. arXiv\npreprint arXiv:2101.01169, 2021. 1, 2, 4\n17. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.\narXiv preprint arXiv:1412.6980, 2014. 10\n\n\n16\nDaniel Shalam and Simon Korman\n18. Nicholas Kolkin, Jason Salavon, and Gregory Shakhnarovich.\nStyle transfer by\nrelaxed optimal transport and self-similarity.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), 2019. 4\n19. Simon Korman and Shai Avidan. Coherency sensitive hashing. IEEE Transactions\non Pattern Analysis and Machine Intelligence (PAMI), 2015. 1\n20. Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from\ntiny images. 2009. 5\n21. Harold W Kuhn.\nThe hungarian method for the assignment problem.\nNaval\nResearch Logistics Quarterly, 2, 1955. 11\n22. Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and\nYee Whye Teh. Set transformer: A framework for attention-based permutation-\ninvariant neural networks.\nIn International Conference on Machine Learning\n(ICML), 2019. 1, 2, 4\n23. Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. Deepreid: Deep filter pairing\nneural network for person re-identification. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2014. 10, 14\n24. Puneet Mangla, Nupur Kumari, Abhishek Sinha, Mayank Singh, Balaji Krishna-\nmurthy, and Vineeth N Balasubramanian. Charting the right manifold: Manifold\nmixup for few-shot learning. In Proceedings of the IEEE/CVF Winter Conference\non Applications of Computer Vision (WACV), 2020. 10\n25. Haggai Maron, Or Litany, Gal Chechik, and Ethan Fetaya. On learning sets of\nsymmetric elements. In International Conference on Machine Learning (ICML),\n2020. 2, 3\n26. Gr´egoire Mialon, Dexiong Chen, Alexandre d’Aspremont, and Julien Mairal. A\ntrainable optimal transport embedding for feature aggregation and its relationship\nto attention. In International Conference on Learning Representations (ICLR),\n2021. 1, 2\n27. Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\nPointnet: Deep\nlearning on point sets for 3d classification and segmentation. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 3\n28. Rodolfo Quispe and Helio Pedrini. Top-db-net: Top dropblock for activation en-\nhancement in person re-identification. 25th International Conference on Pattern\nRecognition (ICPR), 2020. 14\n29. Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Lev-\nskaya, and Jon Shlens. Stand-alone self-attention in vision models. Advances in\nNeural Information Processing Systems (NeurIPS), 2019. 1, 2, 4\n30. Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning.\nIn International Conference on Learning Representations (ICLR), 2017. 1\n31. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean\nMa, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexan-\nder C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge.\nInternational Journal of Computer Vision (IJCV), 2015. 10\n32. Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan\nPascanu, Peter Battaglia, and Timothy Lillicrap. A simple neural network mod-\nule for relational reasoning. Advances in Neural Information Processing Systems\n(NeurIPS), 2017. 4\n33. Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-\nnovich.\nSuperglue: Learning feature matching with graph neural networks.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2020. 4\n\n\nThe Self-Optimal-Transport Feature Transform\n17\n34. Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot\nlearning. In Advances in Neural Information Processing Systems (NeurIPS), 2017.\n4, 10, 12, 18, 20\n35. Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M\nHospedales.\nLearning to compare: Relation network for few-shot learning.\nIn\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2018. 12\n36. Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Jour-\nnal of Machine Learning Research (JMLR), 9(11), 2008. 8\n37. Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proes-\nmans, and Luc Van Gool. Scan: Learning to classify images without labels. In\nEuropean Conference on Computer Vision (ECCV). Springer, 2020. 1, 5\n38. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez,  Lukasz Kaiser, and Illia Polosukhin. Attention is all you need.\nIn Advances in Neural Information Processing Systems (NeurIPS), 2017. 4\n39. Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan\nWierstra. Matching networks for one shot learning. In Proceedings of the 30th\nInternational Conference on Neural Information Processing Systems (NeurIPS),\n2016. 4, 10, 11, 12, 13, 18\n40. Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge J. Be-\nlongie. The caltech-ucsd birds-200-2011 dataset. 2011. 10, 11, 13\n41. Yujia Xie, Hanjun Dai, Minshuo Chen, Bo Dai, Tuo Zhao, Hongyuan Zha, Wei\nWei, and Tomas Pfister. Differentiable top-k with optimal transport. Advances in\nNeural Information Processing Systems (NeurIPS), 2020. 4\n42. Han-Jia Ye, Hexiang Hu, De-Chuan Zhan, and Fei Sha.\nFew-shot learning via\nembedding adaptation with set-to-set functions. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition (CVPR), 2020.\n2, 4, 10,\n12\n43. Mang Ye, Jianbing Shen, Gaojie Lin, Tao Xiang, Ling Shao, and Steven CH Hoi.\nDeep learning for person re-identification: A survey and outlook. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence (PAMI), 2021. 1, 5\n44. Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R\nSalakhutdinov, and Alexander J Smola. Deep sets. In Advances in Neural Infor-\nmation Processing Systems (NeurIPS), 2017. 2, 3\n45. Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen. Deepemd: Few-shot\nimage classification with differentiable earth mover’s distance and structured clas-\nsifiers. In IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), June 2020. 4, 12\n46. Haipeng Zhang, Zhong Cao, Ziang Yan, and Changshui Zhang.\nSill-net: Fea-\nture augmentation with separated illumination representation.\narXiv preprint\narXiv:2102.03539, 2021. 5, 12, 13\n47. Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian.\nScalable person re-identification: A benchmark. In 2015 IEEE International Con-\nference on Computer Vision (ICCV), 2015. 10, 14, 18\n48. Zhun Zhong, Liang Zheng, Donglin Cao, and Shaozi Li. Re-ranking person re-\nidentification with k-reciprocal encoding. 2017. 14\n49. Kaiyang Zhou and Tao Xiang. Torchreid: A library for deep learning person re-\nidentification in pytorch. arXiv preprint arXiv:1910.10093, 2019. 10, 14\n50. Imtiaz Masud Ziko, Jose Dolz, Eric Granger, and Ismail Ben Ayed.\nLaplacian\nregularized few-shot learning. In International Conference on Machine Learning\n(ICML), 2020. 5, 12\n\n\n18\nDaniel Shalam and Simon Korman\nAppendix\nA\nablation studies\nA.1\nSinkhorn iterations\nIn Table 4 we ablate the number of normalization iterations in the Sinkhorn-\nKnopp (SK) [7] algorithm at test-time. We measured accuracy on the validation\nset of MiniImagenet [39], using ProtoNet-SOTp (which is the non-fine-tuned\ndrop-in version of SOT within ProtoNet [34]). As was reported in prior works fol-\nlowing [7], we empirically observe that a very small number of iterations (around\n5) provide rapid convergence. We observed similar behavior for other hosting\nmethods, and therefore chose to use a fixed number of 10 iterations throughout\nthe experiments.\nmethod\niterations\n5way-1shot\n5way-5shot\nProtoNet-SOTp\n1\n70.71\n83.79\nProtoNet-SOTp\n2\n71.10\n84.01\nProtoNet-SOTp\n4\n71.18\n84.08\nProtoNet-SOTp\n8\n71.20\n84.10\nProtoNet-SOTp\n16\n71.20\n84.10\nTable 4: Sinkhorn iterations ablation study: See text for details.\nA.2\nOT entropy regularization parameter λ\nWe measured the impact of using different values of the optimal-transport en-\ntropy regularization parameter λ (the main parameter of the Sinkhorn algo-\nrithm) on a variety of configurations (ways and shots) in Few-Shot-Classification\n(FSC) on MiniImagenet [39] in Fig. 6 as well as on the Person-Re-Identification\n(RE-ID) experiment on Market-1501 [47] in Fig. 7. In both cases, the ablation\nwas executed on the validation set.\nFor FSC, in Fig. 6, the left plot shows that the effect of the choice of λ is\nsimilar across tasks with a varying number of ways. The right plot shows the\nbehavior as a function of λ across multiple shot-values, where the optimal value\nof λ can be seen to have a certain dependence on the number of shots. Recall that\nwe chose to use a fixed value of λ = 0.1, which gives an overall good accuracy\ntrade-off. Note that a further improvement could be achieved by picking the best\nvalues for the particular cases. Notice also the log-scale of the x-axes to see that\nperformance is rather stable around the chosen value.\nFor Re-ID, in Fig. 7, we experiment with a range of λ values on the validation\nset of the Market-1501 dataset. The results (shown both for mAP and rank-1\nmeasures) reveal a strong resemblance to those of the FSC experiment in Fig.\n6, however, the optimal choices for λ are slightly higher, which is consistent\nwith the dependence on the shots number, since the re-ID tasks are typically\n\n\nThe Self-Optimal-Transport Feature Transform\n19\nFig. 6:\nAblation study on λ in Few-Shot-Classification (FSC): Considering\ndifferent ‘ways’ (left), and different ‘shots’ (right). See text for details.\nlarge ones. In this re-ID ablation, we found that a value of λ = 0.25 gives good\nresults across both datasets. We ask to note that in the paper we mistakenly\nreported that we used λ = 1.0, while in practice all our results were obtained\nusing λ = 0.25.\nFig. 7: Ablation study on λ in\nPerson-Re-Identification (Re-\nID): Using the validation set of the\nMarket-1501 dataset and consider-\ning both mAP and Rank-1 mea-\nsures. See text for details.\nB\nUnsupervised Clustering - further details\nIn this section we provide further details (due to lack of space in main paper)\non the experiment on unsupervised clustering on the unit sphere (Exp. 5.1).\nB.1\nSeparation between inter- and intra-class features\nFig. 8 depicts the average percentile of the in-class and out-class distances com-\nputed by the original and the SOT points. Each panel presents the distributions\nof both types of distances, for instances of a different level of noise. We compute\nthe mean (and plus-minus half-std) percentiles, with respect to the entire set of\npair-wise distances, for a fixed level of in-class noise (increasing from top-left to\nbottom-right panels), for a range of data dimensionality (x-axis). Naturally, the\n\n\n20\nDaniel Shalam and Simon Korman\nFig. 8:\nintra (in) vs. inter (out) class distances before and after SOT. A\nstrong indicative property of an embedding that works on class (cluster) objects\nis its ability to reduce embedded intra-class (pink shaded) pairwise feature dis-\ntances compared to inter-class (green shaded) ones. SOT (red lines) consistently\nimproves this separation compared to the baseline (brown lines) - leading to\nbetter downstream clustering and classification. x-axis represents data dimen-\nsionality; y-axis represents percentiles of pair-wise distances; The four panels\npresent results for the noise standard deviations levels in {0.15, 0.19, 0.23, 0.29}\noverlap between in-class and between-class distances increases both with dimen-\nsionality and with in-class noise. Nevertheless, across almost all sampled points,\nthe situation is far better after SOT application (in red), compared to prior to\nSOT application (in brown). This can explain, in part, the effectiveness of using\nSOT in Euclidean-based downstream methods, like k-means and ProtoNet [34].\nB.2\nEvaluation on an extended set of measures\nIn Fig. 9 we evaluate the performance on additional popular clustering metrics,\nNMI and ARI (in addition to the accuracy measure we reported on in Figure 5\nof the paper). The results shows the same trend as with accuracy, perhaps even\nstronger for NMI, where SOT significantly improves the clustering performance.\n\n\nThe Self-Optimal-Transport Feature Transform\n21\nFig. 9:\nA controlled clustering experiment on the d-dimensional sphere -\nExtension of results from Figure 5 of the paper, with 2 additional measures:\nIt can be seen that the SOT (dashed - - -) shows superior results in all aspects (see text\nfor explanations and interpretation). Clustering accuracy across different noise levels\nσ and dimensions d.\nNote: For each configuration, SOT is shown by a dashed line\nwhile the baseline features are shown by a solid line. For all 3 measures - the higher\nthe better.\n"
}