{
  "filename": "910_few_shot_learning_with_graph_n.pdf",
  "num_pages": 13,
  "pages": [
    "Published as a conference paper at ICLR 2018\nFEW-SHOT LEARNING WITH GRAPH NEURAL NET-\nWORKS\nVictor Garcia∗\nAmsterdam Machine Learning Lab\nUniversity of Amsterdam\nAmsterdam, 1098 XH, NL\nv.garciasatorras@uva.nl\nJoan Bruna\nCourant Institute of Mathematical Sciences\nNew York University\nNew York City, NY, 10010, USA\nbruna@cims.nyu.edu\nABSTRACT\nWe propose to study the problem of few-shot learning with the prism of infer-\nence on a partially observed graphical model, constructed from a collection of\ninput images whose label can be either observed or not. By assimilating generic\nmessage-passing inference algorithms with their neural-network counterparts, we\ndeﬁne a graph neural network architecture that generalizes several of the recently\nproposed few-shot learning models. Besides providing improved numerical per-\nformance, our framework is easily extended to variants of few-shot learning, such\nas semi-supervised or active learning, demonstrating the ability of graph-based\nmodels to operate well on ‘relational’ tasks.\n1\nINTRODUCTION\nSupervised end-to-end learning has been extremely successful in computer vision, speech, or ma-\nchine translation tasks, thanks to improvements in optimization technology, larger datasets and\nstreamlined designs of deep convolutional or recurrent architectures. Despite these successes, this\nlearning setup does not cover many aspects where learning is nonetheless possible and desirable.\nOne such instance is the ability to learn from few examples, in the so-called few-shot learning tasks.\nRather than relying on regularization to compensate for the lack of data, researchers have explored\nways to leverage a distribution of similar tasks, inspired by human learning Lake et al. (2015). This\ndeﬁnes a new supervised learning setup (also called ‘meta-learning’) in which the input-output pairs\nare no longer given by iid samples of images and their associated labels, but by iid samples of\ncollections of images and their associated label similarity.\nA recent and highly-successful research program has exploited this meta-learning paradigm on the\nfew-shot image classiﬁcation task Lake et al. (2015); Koch et al. (2015); Vinyals et al. (2016); Mishra\net al. (2017); Snell et al. (2017). In essence, these works learn a contextual, task-speciﬁc similarity\nmeasure, that ﬁrst embeds input images using a CNN, and then learns how to combine the embedded\nimages in the collection to propagate the label information towards the target image.\nIn particular, Vinyals et al. (2016) cast the few-shot learning problem as a supervised classiﬁcation\ntask mapping a support set of images into the desired label, and developed an end-to-end architec-\nture accepting those support sets as input via attention mechanisms. In this work, we build upon\nthis line of work, and argue that this task is naturally expressed as a supervised interpolation prob-\nlem on a graph, where nodes are associated with the images in the collection, and edges are given\nby a trainable similarity kernels. Leveraging recent progress on representation learning for graph-\nstructured data Bronstein et al. (2017); Gilmer et al. (2017), we thus propose a simple graph-based\nfew-shot learning model that implements a task-driven message passing algorithm. The resulting\narchitecture is trained end-to-end, captures the invariances of the task, such as permutations within\nthe input collections, and offers a good tradeoff between simplicity, generality, performance and\nsample complexity.\nBesides few-shot learning, a related task is the ability to learn from a mixture of labeled and unla-\nbeled examples — semi-supervised learning, as well as active learning, in which the learner has the\n∗Work done while Victor Garcia was a visiting scholar at New York University\n1\n",
    "Published as a conference paper at ICLR 2018\noption to request those missing labels that will be most helpful for the prediction task. Our graph-\nbased architecture is naturally extended to these setups with minimal changes in the training design.\nWe validate experimentally the model on few-shot image classiﬁcation, matching state-of-the-art\nperformance with considerably fewer parameters, and demonstrate applications to semi-supervised\nand active learning setups.\nOur contributions are summarized as follows:\n• We cast few-shot learning as a supervised message passing task which is trained end-to-end\nusing graph neural networks.\n• We match state-of-the-art performance on Omniglot and Mini-Imagenet tasks with fewer\nparameters.\n• We extend the model in the semi-supervised and active learning regimes.\nThe rest of the paper is structured as follows. Section 2 describes related work, Sections 3, 4 and 5\npresent the problem setup, our graph neural network model and the training, and Section 6 reports\nnumerical experiments.\n2\nRELATED WORK\nOne-shot learning was ﬁrst introduced by Fei-Fei et al. (2006), they assumed that currently learned\nclasses can help to make predictions on new ones when just one or few labels are available. More\nrecently, Lake et al. (2015) presented a Hierarchical Bayesian model that reached human level error\non few-shot learning alphabet recongition tasks.\nSince then, great progress has been done in one-shot learning. Koch et al. (2015) presented a deep-\nlearning model based on computing the pair-wise distance between samples using Siamese Net-\nworks, then, this learned distance can be used to solve one-shot problems by k-nearest neighbors\nclassiﬁcation. Vinyals et al. (2016) Presented an end-to-end trainable k-nearest neighbors using\nthe cosine distance, they also introduced a contextual mechanism using an attention LSTM model\nthat takes into account all the samples of the subset T when computing the pair-wise distance be-\ntween samples. Snell et al. (2017) extended the work from Vinyals et al. (2016), by using euclidean\ndistance instead of cosine which provided signiﬁcant improvements, they also build a prototype rep-\nresentation of each class for the few-shot learning scenario. Mehrotra & Dukkipati (2017) trained\na deep residual network together with a generative model to approximate the pair-wise distance\nbetween samples.\nA new line of meta-learners for one-shot learning is rising lately: Ravi & Larochelle (2016) in-\ntroduced a meta-learning method where an LSTM updates the weights of a classiﬁer for a given\nepisode. Munkhdalai & Yu (2017) also presented a meta-learning architecture that learns meta-level\nknowledge across tasks, and it changes its inductive bias via fast parametrization. Finn et al. (2017)\nis using a model agnostic meta-learner based on gradient descent, the goal is to train a classiﬁcation\nmodel such that given a new task, a small amount of gradient steps with few data will be enough\nto generalize. Lately, Mishra et al. (2017) used Temporal Convolutions which are deep recurrent\nnetworks based on dilated convolutions, this method also exploits contextual information from the\nsubset T providing very good results.\nAnother related area of research concerns deep learning architectures on graph-structured data. The\nGNN was ﬁrst proposed in Gori et al. (2005); Scarselli et al. (2009), as a trainable recurrent message-\npassing whose ﬁxed points could be adjusted discriminatively. Subsequent works Li et al. (2015);\nSukhbaatar et al. (2016) have relaxed the model by untying the recurrent layer weights and proposed\nseveral nonlinear updates through gating mechanisms. Graph neural networks are in fact natural\ngeneralizations of convolutional networks to non-Euclidean graphs. Bruna et al. (2013); Henaff\net al. (2015) proposed to learn smooth spectral multipliers of the graph Laplacian, albeit with high\ncomputational cost, and Defferrard et al. (2016); Kipf & Welling (2016) resolved the computational\nbottleneck by learning polynomials of the graph Laplacian, thus avoiding the computation of eigen-\nvectors and completing the connection with GNNs. In particular, Kipf & Welling (2016) was the\nﬁrst to propose the use of GNNs on semi-supervised classiﬁcation problems. We refer the reader\nto Bronstein et al. (2017) for an exhaustive literature review on the topic. GNNs and the analogous\nNeural Message Passing Models are ﬁnding application in many different domains. Battaglia et al.\n2\n",
    "Published as a conference paper at ICLR 2018\n(2016); Chang et al. (2016) develop graph interaction networks that learn pairwise particle interac-\ntions and apply them to discrete particle physical dynamics. Duvenaud et al. (2015); Kearnes et al.\n(2016) study molecular ﬁngerprints using variants of the GNN architecture, and Gilmer et al. (2017)\nfurther develop the model by combining it with set representations Vinyals et al. (2015), showing\nstate-of-the-art results on molecular prediction.\n3\nPROBLEM SET-UP\nWe describe ﬁrst the general setup and notations, and then particularize it to the case of few-shot\nlearning, semi-supervised learning and active learning.\nWe consider input-output pairs (Ti, Yi)i drawn iid from a distribution P of partially-labeled image\ncollections\nT\n=\n\b\n{(x1, l1), . . . (xs, ls)}, {˜x1, . . . , ˜xr}, {¯x1, . . . , ¯xt} ; li ∈{1, K}, xi, ˜xj, ¯xj ∼Pl(RN)\n\t\n,\nand Y\n=\n(y1, . . . , yt) ∈{1, K}t ,\n(1)\nfor arbitrary values of s, r, t and K. Where s is the number of labeled samples, r is the number of\nunlabeled samples (r > 0 for the semi-supervised and active learning scenarios) and t is the number\nof samples to classify. K is the number of classes. We will focus in the case t = 1 where we just\nclassify one sample per task T . Pl(RN) denotes a class-speciﬁc image distribution over RN. In our\ncontext, the targets Yi are associated with image categories of designated images ¯x1, . . . , ¯xt ∈Ti\nwith no observed label. Given a training set {(Ti, Yi)i}i≤L, we consider the standard supervised\nlearning objective\nmin\nΘ\n1\nL\nX\ni≤L\nℓ(Φ(Ti; Θ), Yi) + R(Θ) ,\nusing the model Φ(T ; Θ) = p(Y | T ) speciﬁed in Section 4 and R is a standard regularization\nobjective.\nFew-Shot Learning\nWhen r = 0, t = 1 and s = qK, there is a single image in the collection\nwith unknown label. If moreover each label appears exactly q times, this setting is referred as the\nq-shot, K-way learning.\nSemi-Supervised Learning\nWhen r > 0 and t = 1, the input collection contains auxiliary images\n˜x1, . . . , ˜xr that the model can use to improve the prediction accuracy, by leveraging the fact that\nthese samples are drawn from common distributions as those determining the output.\nActive Learning\nIn the active learning setting, the learner has the ability to request labels from\nthe sub-collection {˜x1, . . . , ˜xr}. We are interested in studying to what extent this active learning\ncan improve the performance with respect to the previous semi-supervised setup, and match the\nperformance of the one-shot learning setting with s0 known labels when s + r = s0, s ≪s0.\n4\nMODEL\nThis section presents our approach, based on a simple end-to-end graph neural network architecture.\nWe ﬁrst explain how the input context is mapped into a graphical representation, then detail the\narchitecture, and next show how this model generalizes a number of previously published few-shot\nlearning architectures.\n4.1\nSET AND GRAPH INPUT REPRESENTATIONS\nThe input T contains a collection of images, both labeled and unlabeled. The goal of few-shot\nlearning is to propagate label information from labeled samples towards the unlabeled query image.\nThis propagation of information can be formalized as a posterior inference over a graphical model\ndetermined by the input images and labels.\nFollowing several recent works that cast posterior inference using message passing with neural net-\nworks deﬁned over graphs Scarselli et al. (2009); Duvenaud et al. (2015); Gilmer et al. (2017), we\n3\n",
    "Published as a conference paper at ICLR 2018\nFigure 1: Visual representation of One-Shot Learning setting.\nassociate T with a fully-connected graph GT = (V, E) where nodes va ∈V correspond to the\nimages present in T (both labeled and unlabeled). In this context, the setup does not specify a ﬁxed\nsimilarity ea,a′ between images xa and xa′, suggesting an approach where this similarity measure\nis learnt in a discriminative fashion with a parametric model similarly as in Gilmer et al. (2017),\nsuch as a siamese neural architecture. This framework is closely related to the set representation\nfrom Vinyals et al. (2016), but extends the inference mechanism using the graph neural network\nformalism that we detail next.\n4.2\nGRAPH NEURAL NETWORKS\nGraph Neural Networks, introduced in Gori et al. (2005); Scarselli et al. (2009) and further simpliﬁed\nin Li et al. (2015); Duvenaud et al. (2015); Sukhbaatar et al. (2016) are neural networks based on\nlocal operators of a graph G = (V, E), offering a powerful balance between expressivity and sample\ncomplexity; see Bronstein et al. (2017) for a recent survey on models and applications of deep\nlearning on graphs.\nIn its simplest incarnation, given an input signal F ∈RV ×d on the vertices of a weighted graph G,\nwe consider a family A of graph intrinsic linear operators that act locally on this signal. The simplest\nis the adjacency operator A : F 7→A(F) where (AF)i := P\nj∼i wi,jFj , with i ∼j iff (i, j) ∈E\nand wi,j its associated weight. A GNN layer Gc(·) receives as input a signal x(k) ∈RV ×dk and\nproduces x(k+1) ∈RV ×dk+1 as\nx(k+1)\nl\n= Gc(x(k)) = ρ\n X\nB∈A\nBx(k)θ(k)\nB,l\n!\n, l = d1 . . . dk+1 ,\n(2)\nwhere Θ = {θ(k)\n1 , . . . , θ(k)\n|A|}k, θ(k)\nB\n∈Rdk×dk+1, are trainable parameters and ρ(·) is a point-wise\nnon-linearity, chosen in this work to be a ‘leaky’ ReLU Xu et al. (2015).\nAuthors have explored several modeling variants from this basic formulation, by replacing the point-\nwise nonlinearity with gating operations Duvenaud et al. (2015), or by generalizing the generator\nfamily to Laplacian polynomials Defferrard et al. (2016); Kipf & Welling (2016); Bruna et al. (2013),\nor including 2J-th powers of A to A, AJ = min(1, A2J) to encode 2J-hop neighborhoods of each\nnode Bruna & Li (2017). Cascaded operations in the form (2) are able to approximate a wide\nrange of graph inference tasks. In particular, inspired by message-passing algorithms, Kearnes et al.\n(2016); Gilmer et al. (2017) generalized the GNN to also learn edge features ˜A(k) from the current\nnode hidden representation:\n˜A(k)\ni,j = ϕ˜θ(x(k)\ni\n, x(k)\nj ) ,\n(3)\n4\n",
    "Published as a conference paper at ICLR 2018\nFigure 2: Graph Neural Network ilustration. The Adjacency matrix is computed before every Con-\nvolutional Layer.\nwhere ϕ is a symmetric function parametrized with e.g. a neural network. In this work, we consider\na Multilayer Perceptron stacked after the absolute difference between two vector nodes. See eq. 4:\nϕ˜θ(x(k)\ni\n, x(k)\nj ) = MLP˜θ(abs(x(k)\ni\n−x(k)\nj ))\n(4)\nThen ϕ is a metric, which is learned by doing a non-linear combination of the absolute difference\nbetween the individual features of two nodes. Using this architecture the distance property Symmetry\nϕ˜θ(a, b) = ϕ˜θ(b, a) is fulﬁlled by construction and the distance property Identity ϕ˜θ(a, a) = 0 is\neasily learned.\nThe trainable adjacency is then normalized to a stochastic kernel by using a softmax along each row.\nThe resulting update rules for node features are obtained by adding the edge feature kernel ˜A(k) into\nthe generator family A = { ˜A(k), 1} and applying (2). Adjacency learning is particularly important\nin applications where the input set is believed to have some geometric structure, but the metric is not\nknown a priori, such as is our case.\nIn general graphs, the network depth is chosen to be of the order of the graph diameter, so that all\nnodes obtain information from the entire graph. In our context, however, since the graph is densely\nconnected, the depth is interpreted simply as giving the model more expressive power.\nConstruction of Initial Node Features\nThe input collection T is mapped into node features as\nfollows. For images xi ∈T with known label li, the one-hot encoding of the label is concatenated\nwith the embedding features of the image at the input of the GNN.\nx(0)\ni\n= (φ(xi), h(li)) ,\n(5)\nwhere φ is a Convolutional neural network and h(l) ∈RK\n+ is a one-hot encoding of the label.\nArchitectural details for φ are detailed in Section 6.1.1 and 6.1.2. For images ˜xj, ¯xj′ with unknown\nlabel li, we modify the previous construction to account for full uncertainty about the label variable\nby replacing h(l) with the uniform distribution over the K-simplex: Vj = (φ(˜xj), K−11K), and\nanalogously for ¯x.\n4.3\nRELATIONSHIP WITH EXISTING MODELS\nThe graph neural network formulation of few-shot learning generalizes a number of recent models\nproposed in the literature.\nSiamese Networks\nSiamese Networks Koch et al. (2015) can be interpreted as a single layer\nmessage-passing iteration of our model, and using the same initial node embedding (5) x(0)\ni\n=\n(φ(xi), hi) , using a non-trainable edge feature\nϕ(xi, xj) = ∥φ(xi) −φ(xj)∥, ˜A(0) = softmax(−ϕ) ,\n5\n",
    "Published as a conference paper at ICLR 2018\nand resulting label estimation\nˆY∗=\nX\nj\n˜A(0)\n∗,j⟨x(0)\nj , u⟩,\nwith u selecting the label ﬁeld from x. In this model, the learning is reduced to learning image\nembeddings φ(xi) whose euclidean metric is consistent with the label similarities.\nPrototypical Networks\nPrototypical networks Snell et al. (2017) evolve Siamese networks by\naggregating information within each cluster determined by nodes with the same label. This operation\ncan also be accomplished with a gnn as follows. we consider\n˜A(0)\ni,j =\n\u001a\nq−1\nif li = lj\n0\notherwise.\nwhere q is the number of examples per class, and\nx(1)\ni\n=\nX\nj\n˜A(0)\ni,j x(0)\nj\n,\nwhere x(0) is deﬁned as in the Siamese Networks. We ﬁnally apply the previous kernel ˜A(1) =\nsoftmax(ϕ) applied to x(1) to yield class prototypes:\nˆY∗=\nX\nj\n˜A(1)\n∗,j⟨x(1)\nj , u⟩.\nMatching Networks\nMatching networks Vinyals et al. (2016) use a set representation for the\nensemble of images in T , similarly as our proposed graph neural network model, but with two\nimportant differences. First, the attention mechanism considered in this set representation is akin\nto the edge feature learning, with the difference that the mechanism attends always to the same\nnode embeddings, as opposed to our stacked adjacency learning, which is closer to Vaswani et al.\n(2017). In other words, instead of the attention kernel in (3), matching networks consider attention\nmechanisms of the form ˜A(k)\n∗,j = ϕ(x(k)\n∗, x(T )\nj\n), where x(T )\nj\nis the encoding function for the elements\nof the support set, obtained with bidirectional LSTMs. In that case, the support set encoding is\nthus computed independently of the target image. Second, the label and image ﬁelds are treated\nseparately throughout the model, with a ﬁnal step that aggregates linearly the labels using a trained\nkernel. This may prevent the model to leverage complex dependencies between labels and images\nat intermediate stages.\n5\nTRAINING\nWe describe next how to train the parameters of the GNN in the different setups we consider: few-\nshot learning, semi-supervised learning and active learning.\n5.1\nFEW-SHOT AND SEMI-SUPERVISED LEARNING\nIn this setup, the model is asked only to predict the label Y corresponding to the image to classify\n¯x ∈T , associated with node ∗in the graph. The ﬁnal layer of the GNN is thus a softmax mapping\nthe node features to the K-simplex. We then consider the Cross-entropy loss evaluated at node ∗:\nℓ(Φ(T ; Θ), Y ) = −\nX\nk\nyk log P(Y∗= yk | T ) .\nThe semi-supervised setting is trained identically — the only difference is that the initial label ﬁelds\nof the node will be ﬁlled with the uniform distribution on nodes corresponding to ˜xj.\n5.2\nACTIVE LEARNING\nIn the Active Learning setup, the model has the intrinsic ability to query for one of the labels from\n{˜x1, . . . , ˜xr}. The network will learn to ask for the most informative label in order to classify the\n6\n",
    "Published as a conference paper at ICLR 2018\nsample ¯x ∈T . The querying is done after the ﬁrst layer of the GNN by using a Softmax attention\nover the unlabeled nodes of the graph. For this we apply a function g(x(1)\ni ) ∈R1 that maps each\nunlabeled vector node to a scalar value. Function g is parametrized by a two layers neural network.\nA Softmax is applied over the {1, . . . , r} scalar values obtained after applying g:\nAttention = Softmax(g(x(1)\n{1,...,r}))\nIn order to query only one sample, we set all elements from the Attention ∈Rr vector to 0 except\nfor one. At test time we keep the maximum value, at train time we randomly sample one value based\non its multinomial probability. Then we multiply this sampled attention by the label vectors:\nw · h(li∗) = ⟨Attention′, h(l{1,...,r})⟩\nThe label of the queried vector h(li∗) is obtained, scaled by the weight w ∈(0, 1). This value is then\nsummed to the current representation x(1)\ni∗, since we are using dense connections in our GNN model\nwe can sum this w · h(li∗) value directly to where the uniform label distribution was concatenated\nx(1)\ni∗= [Gc(x(0)\ni∗), x(0)\ni∗] = [Gc(x(0)\ni∗), (φ(xi∗), h(li∗))]\nAfter the label has been summed to the current node, the information is forward propagated. This\nattention part is trained end-to-end with the rest of the network by backpropagating the loss from\nthe output of the GNN.\n6\nEXPERIMENTS\nFor the few-shot, semi-supervised and active learning experiments we used the Omniglot dataset\npresented by Lake et al. (2015) and Mini-Imagenet dataset introduced by Vinyals et al. (2016) which\nis a small version of ILSVRC-12 Krizhevsky et al. (2012). All experiments are based on the q-shot,\nK-way setting. For all experiments we used the same values q-shot and K-way for both training\nand testing.\nCode available at: https://github.com/vgsatorras/few-shot-gnn\n6.1\nDATASETS AND IMPLEMENTATION\n6.1.1\nOMNIGLOT\nDataset:\nOmniglot is a dataset of 1623 characters from 50 different alphabets, each character/class\nhas been drawn by 20 different people. Following Vinyals et al. (2016) implementation we split the\ndataset into 1200 classes for training and the remaining 423 for testing. We augmented the dataset\nby multiples of 90 degrees as proposed by Santoro et al. (2016).\nArchitectures:\nInspired by the embedding architecture from Vinyals et al. (2016), following\nMishra et al. (2017), a CNN was used as an embedding φ function consisting of four stacked blocks\nof {3×3-convolutional layer with 64 ﬁlters, batch-normalization, 2×2 max-pooling, leaky-relu} the\noutput is passed through a fully connected layer resulting in a 64-dimensional embedding. For the\nGNN we used 3 blocks each of them composed by 1) a module that computes the adjacency matrix\nand 2) a graph convolutional layer. A more detailed description of each block can be found at Figure\n3.\n6.1.2\nMINI-IMAGENET\nDataset:\nMini-Imagenet is a more challenging dataset for one-shot learning proposed by Vinyals\net al. (2016) derived from the original ILSVRC-12 dataset Krizhevsky et al. (2012). It consists of\n84×84 RGB images from 100 different classes with 600 samples per class. It was created with the\npurpose of increasing the complexity for one-shot tasks while keeping the simplicity of a light size\ndataset, that makes it suitable for fast prototyping. We used the splits proposed by Ravi & Larochelle\n(2016) of 64 classes for training, 16 for validation and 20 for testing. Using 64 classes for training,\nand the 16 validation classes only for early stopping and parameter tuning.\n7\n",
    "Published as a conference paper at ICLR 2018\nArchitecture:\nThe embedding architecture used for Mini-Imagenet is formed by 4 convolutional\nlayers followed by a fully-connected layer resulting in a 128 dimensional embedding. This light\narchitecture is useful for fast prototyping:\n1×{3×3-conv. layer (64 ﬁlters), batch normalization, max pool(2, 2), leaky relu},\n1×{3×3-conv. layer (96 ﬁlters), batch normalization, max pool(2, 2), leaky relu},\n1×{3×3-conv. layer (128 ﬁlters), batch normalization, max pool(2, 2), leaky relu, dropout(0.5)},\n1×{3×3-conv. layer (256 ﬁlters), batch normalization, max pool(2, 2), leaky relu, dropout(0.5)},\n1×{ fc-layer (128 ﬁlters), batch normalization}.\nThe two dropout layers are useful to avoid overﬁtting the GNN in Mini-Imagenet dataset. The GNN\narchitecture is similar than for Omniglot, it is formed by 3 blocks, each block is described at Figure\n3.\n6.2\nFEW-SHOT\nFew-shot learning experiments for Omniglot and Mini-Imagenet are presented at Table 1 and Table\n2 respectively.\nWe evaluate our model by performing different q-shot, K-way experiments on both datasets. For\nevery few-shot task T , we sample K random classes from the dataset, and from each class we\nsample q random samples. An extra sample to classify is chosen from one of that K classes.\nOmniglot: The GNN method is providing competitive results while still remaining simpler than\nother methods. State of the art results are reached in the 5-Way and 20-way 1-shot experiments. In\nthe 20-Way 1-shot setting the GNN is providing slightly better results than Munkhdalai & Yu (2017)\nwhile still being a more simple approach. The TCML approach from Mishra et al. (2017) is in the\nsame conﬁdence interval for 3 out of 4 experiments, but it is slightly better for the 20-Way 5-shot,\nalthough the number of parameters is reduced from ∼5M (TCML) to ∼300K (3 layers GNN).\nAt Mini-Imagenet table we are also presenting a baseline ”Our metric learning + KNN” where no\ninformation has been aggregated among nodes, it is a K-nearest neighbors applied on top of the\npair-wise learnable metric ϕθ(x(0)\ni , x(0)\nj ) and trained end-to-end, this learnable metric is competi-\ntive by itself compared to other state of the art methods. Even so, a signiﬁcant improvement (from\n64.02% to 66.41%) can be seen for the 5-shot 5-Way Mini-Imagenet setting when aggregating in-\nformation among nodes by using the full GNN architecture. A variety of embedding functions φ are\nused among the different papers for Mini-Imagenet experiments, in our case we are using a simple\nnetwork of 4 conv. layers followed by a fully connected layer (Section 6.1.2) which served us to\ncompare between Our GNN and Our metric learning + KNN and it is useful for fast prototyping.\nMore complex embeddings have proven to produce better results, at Mishra et al. (2017) a deep\nresidual network is used as embedding network φ increasing the accuracy considerably. Regarding\nthe TCML architecture in Mini-Imagenet, the number of parameters is reduced from ∼11M (TCML)\nto ∼400K (3 layers GNN).\n5-Way\n20-Way\nModel\n1-shot\n5-shot\n1-shot\n5-shot\nPixels Vinyals et al. (2016)\n41.7%\n63.2%\n26.7%\n42.6%\nSiamese Net Koch et al. (2015)\n97.3%\n98.4%\n88.2%\n97.0%\nMatching Networks Vinyals et al. (2016)\n98.1%\n98.9%\n93.8%\n98.5%\nN. Statistician Edwards & Storkey (2016)\n98.1%\n99.5%\n93.2%\n98.1%\nRes. Pair-Wise Mehrotra & Dukkipati (2017)\n-\n-\n94.8%\n-\nPrototypical Networks Snell et al. (2017)\n97.4%\n99.3%\n95.4%\n98.8%\nConvNet with Memory Kaiser et al. (2017)\n98.4%\n99.6%\n95.0%\n98.6%\nAgnostic Meta-learner Finn et al. (2017)\n98.7 ±0.4%\n99.9 ±0.3%\n95.8 ±0.3%\n98.9 ±0.2%\nMeta Networks Munkhdalai & Yu (2017)\n98.9%\n-\n97.0%\n-\nTCML Mishra et al. (2017)\n98.96% ±0.20% 99.75% ±0.11% 97.64% ±0.30% 99.36% ±0.18%\nOur GNN\n99.2%\n99.7%\n97.4%\n99.0%\nTable 1: Few-Shot Learning — Omniglot accuracies. Siamese Net results are extracted from Vinyals\net al. (2016) reimplementation.\n8\n",
    "Published as a conference paper at ICLR 2018\n5-Way\nModel\n1-shot\n5-shot\nMatching Networks Vinyals et al. (2016)\n43.6%\n55.3%\nPrototypical Networks Snell et al. (2017)\n46.61% ±0.78%\n65.77% ±0.70%\nModel Agnostic Meta-learner Finn et al. (2017)\n48.70% ±1.84%\n63.1% ±0.92%\nMeta Networks Munkhdalai & Yu (2017)\n49.21% ±0.96\n-\nRavi & Larochelle Ravi & Larochelle (2016)\n43.4% ±0.77%\n60.2% ±0.71%\nTCML Mishra et al. (2017)\n55.71% ±0.99%\n68.88% ±0.92%\nOur metric learning + KNN\n49.44% ±0.28%\n64.02% ±0.51%\nOur GNN\n50.33% ±0.36%\n66.41% ±0.63%\nTable 2: Few-shot learning — Mini-Imagenet average accuracies with 95% conﬁdence intervals.\n6.3\nSEMI-SUPERVISED\nSemi-supervised experiments are performed on the 5-way 5-shot setting. Different results are pre-\nsented when 20% and 40% of the samples are labeled. The labeled samples are balanced among\nclasses in all experiments, in other words, all the classes have the same amount of labeled and\nunlabeled samples.\nTwo strategies can be seen at Tables 3 and 4. ”GNN - Trained only with labeled” is equivalent to\nthe supervised few-shot setting, for example, in the 5-Way 5-shot 20%-labeled setting, this method\nis equivalent to the 5-way 1-shot learning setting since it is ignoring the unlabeled samples. ”GNN\n- Semi supervised” is the actual semi-supervised method, for example, in the 5-Way 5-shot 20%-\nlabeled setting, the GNN receives as input 1 labeled sample per class and 4 unlabeled samples per\nclass.\nOmniglot results are presented at Table 3, for this scenario we observe that the accuracy improve-\nment is similar when adding images than when adding labels. The GNN is able to extract infor-\nmation from the input distribution of unlabeled samples such that only using 20% of the labels in a\n5-shot semi-supervised environment we get same results as in the 40% supervised setting.\nIn Mini-Imagenet experiments, Table 4, we also notice an improvement when using semi-supervised\ndata although it is not as signiﬁcant as in Omniglot. The distribution of Mini-Imagenet images is\nmore complex than for Omniglot. In spite of it, the GNN manages to improve by ∼2% in the 20%\nand 40% settings.\n5-Way 5-shot\nModel\n20%-labeled\n40%-labeled\n100%-labeled\nGNN - Trained only with labeled\n99.18%\n99.59%\n99.71%\nGNN - Semi supervised\n99.59%\n99.63%\n99.71%\nTable 3: Semi-Supervised Learning — Omniglot accuracies.\n5-Way 5-shot\nModel\n20%-labeled\n40%-labeled\n100%-labeled\nGNN - Trained only with labeled\n50.33% ±0.36%\n56.91% ±0.42%\n66.41% ±0.63%\nGNN - Semi supervised\n52.45% ±0.88%\n58.76% ±0.86%\n66.41% ±0.63%\nTable 4: Semi-Supervised Learning — Mini-Imagenet average accuracies with 95% conﬁdence\nintervals.\n6.4\nACTIVE LEARNING\nWe performed Active Learning experiments on the 5-Way 5-shot set-up when 20% of the samples\nare labeled. In this scenario our network will query for the label of one sample from the unlabeled\nones. The results are compared with the Random baseline where the network chooses a random\nsample to be labeled instead of one that maximally reduces the loss of the classiﬁcation task T .\n9\n",
    "Published as a conference paper at ICLR 2018\nResults are shown at Table 5. The results of the GNN-Random criterion are close to the Semi-\nsupervised results for 20%-labeled samples from Tables 3 and 4. It means that selecting one random\nlabel practically does not improve the accuracy at all. When using the GNN-AL learned criterion, we\nnotice an improvement of ∼3.4% for Mini-Imagenet, it means that the GNN manages to correctly\nchoose a more informative sample than a random one. In Omniglot the improvement is smaller since\nthe accuracy is almost saturated and the improving margin is less.\nMethod\n5-Way 5-shot 20%-labeled\nGNN - AL\n99.62%\nGNN - Random\n99.59%\nMethod\n5-Way 5-shot 20%-labeled\nGNN - AL\n55.99% ±1.35%\nGNN - Random\n52.56% ±1.18%\nTable 5: Omniglot (left) and Mini-Imagneet (right), average accuracies are shown at both tables, the\nGNN-AL is the learned criterion that performs Active Learning by selecting the sample that will\nmaximally reduce the loss of the current classiﬁcation. The GNN - Random is also selecting one\nsample, but in this case a random one. Mini-Imagenet results are presented with 95% conﬁdence\nintervals.\n7\nCONCLUSIONS\nThis paper explored graph neural representations for few-shot, semi-supervised and active learn-\ning. From the meta-learning perspective, these tasks become supervised learning problems where\nthe input is given by a collection or set of elements, whose relational structure can be leveraged\nwith neural message passing models. In particular, stacked node and edge features generalize the\ncontextual similarity learning underpinning previous few-shot learning models.\nThe graph formulation is helpful to unify several training setups (few-shot, active, semi-supervised)\nunder the same framework, a necessary step towards the goal of having a single learner which is\nable to operate simultaneously in different regimes (stream of labels with few examples per class, or\nstream of examples with few labels). This general goal requires scaling up graph models to millions\nof nodes, motivating graph hierarchical and coarsening approaches Defferrard et al. (2016).\nAnother future direction is to generalize the scope of Active Learning, to include e.g. the ability to\nask questions Rothe et al. (2017), or in reinforcement learning setups, where few-shot learning is\ncritical to adapt to non-stationary environments.\nACKNOWLEDGMENTS\nThis work was partly supported by Samsung Electronics (Improving Deep Learning using Latent\nStructure).\nREFERENCES\nPeter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks\nfor learning about objects, relations and physics. In Advances in Neural Information Processing\nSystems, pp. 4502–4510, 2016.\nMichael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geomet-\nric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18–42,\n2017.\nJoan Bruna and Xiang Li.\nCommunity detection with graph neural networks.\narXiv preprint\narXiv:1705.08415, 2017.\nJoan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally\nconnected networks on graphs. Proc. ICLR, 2013.\nMichael B. Chang, Tomer Ullman, Antonio Torralba, and Joshua B. Tenenbaum. A compositional\nobject-based approach to learning physical dynamics. ICLR, 2016.\n10\n",
    "Published as a conference paper at ICLR 2018\nMicha¨el Defferrard, Xavier Bresson, and Pierre Vandergheynst.\nConvolutional neural networks\non graphs with fast localized spectral ﬁltering. In Advances in Neural Information Processing\nSystems, pp. 3837–3845, 2016.\nDavid Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael G´omez-Bombarelli, Tim-\nothy Hirzel, Al´an Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for\nlearning molecular ﬁngerprints. In Neural Information Processing Systems, 2015.\nHarrison Edwards and Amos Storkey.\nTowards a neural statistician.\narXiv preprint\narXiv:1606.02185, 2016.\nLi Fei-Fei, Rob Fergus, and Pietro Perona. One-shot learning of object categories. IEEE transactions\non pattern analysis and machine intelligence, 28(4):594–611, 2006.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation\nof deep networks. arXiv preprint arXiv:1703.03400, 2017.\nJustin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural\nmessage passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.\nM. Gori, G. Monfardini, and F. Scarselli. A new model for learning in graph domains. In Proc.\nIJCNN, 2005.\nM. Henaff, J. Bruna, and Y. LeCun.\nDeep convolutional networks on graph-structured data.\narXiv:1506.05163, 2015.\nŁukasz Kaiser, Oﬁr Nachum, Aurko Roy, and Samy Bengio. Learning to remember rare events.\narXiv preprint arXiv:1703.03129, 2017.\nSteven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph\nconvolutions: moving beyond ﬁngerprints. Journal of computer-aided molecular design, 30(8):\n595–608, 2016.\nThomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional net-\nworks. arXiv preprint arXiv:1609.02907, 2016.\nGregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot\nimage recognition. In ICML Deep Learning Workshop, volume 2, 2015.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convo-\nlutional neural networks. In Advances in neural information processing systems, pp. 1097–1105,\n2012.\nBrenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning\nthrough probabilistic program induction. Science, 350(6266):1332–1338, 2015.\nYujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural\nnetworks. arXiv preprint arXiv:1511.05493, 2015.\nAkshay Mehrotra and Ambedkar Dukkipati. Generative adversarial residual pairwise networks for\none shot learning. arXiv preprint arXiv:1703.08033, 2017.\nNikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. Meta-learning with temporal\nconvolutions. arXiv preprint arXiv:1707.03141, 2017.\nTsendsuren Munkhdalai and Hong Yu. Meta networks. arXiv preprint arXiv:1703.00837, 2017.\nSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. ICLR, 2016.\nAnselm Rothe, Brenden Lake, and Todd Gureckis. Question asking as program generation. NIPS,\n2017.\nAdam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-\nlearning with memory-augmented neural networks. In International conference on machine learn-\ning, pp. 1842–1850, 2016.\n11\n",
    "Published as a conference paper at ICLR 2018\nFranco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.\nThe graph neural network model. IEEE Transactions on Neural Networks, 20(1):61–80, 2009.\nJake Snell, Kevin Swersky, and Richard S Zemel. Prototypical networks for few-shot learning. arXiv\npreprint arXiv:1703.05175, 2017.\nSainbayar Sukhbaatar, Rob Fergus, et al. Learning multiagent communication with backpropaga-\ntion. In Advances in Neural Information Processing Systems, pp. 2244–2252, 2016.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762,\n2017.\nOriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets.\narXiv preprint arXiv:1511.06391, 2015.\nOriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one\nshot learning. In Advances in Neural Information Processing Systems, pp. 3630–3638, 2016.\nBing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectiﬁed activations in\nconvolutional network. arXiv preprint arXiv:1505.00853, 2015.\n12\n",
    "Published as a conference paper at ICLR 2018\nAPPENDIX\nFigure 3: GNN model. Three blue blocks are used for Omniglot and Mini-Imagenet. (nf=96).\n13\n"
  ],
  "full_text": "Published as a conference paper at ICLR 2018\nFEW-SHOT LEARNING WITH GRAPH NEURAL NET-\nWORKS\nVictor Garcia∗\nAmsterdam Machine Learning Lab\nUniversity of Amsterdam\nAmsterdam, 1098 XH, NL\nv.garciasatorras@uva.nl\nJoan Bruna\nCourant Institute of Mathematical Sciences\nNew York University\nNew York City, NY, 10010, USA\nbruna@cims.nyu.edu\nABSTRACT\nWe propose to study the problem of few-shot learning with the prism of infer-\nence on a partially observed graphical model, constructed from a collection of\ninput images whose label can be either observed or not. By assimilating generic\nmessage-passing inference algorithms with their neural-network counterparts, we\ndeﬁne a graph neural network architecture that generalizes several of the recently\nproposed few-shot learning models. Besides providing improved numerical per-\nformance, our framework is easily extended to variants of few-shot learning, such\nas semi-supervised or active learning, demonstrating the ability of graph-based\nmodels to operate well on ‘relational’ tasks.\n1\nINTRODUCTION\nSupervised end-to-end learning has been extremely successful in computer vision, speech, or ma-\nchine translation tasks, thanks to improvements in optimization technology, larger datasets and\nstreamlined designs of deep convolutional or recurrent architectures. Despite these successes, this\nlearning setup does not cover many aspects where learning is nonetheless possible and desirable.\nOne such instance is the ability to learn from few examples, in the so-called few-shot learning tasks.\nRather than relying on regularization to compensate for the lack of data, researchers have explored\nways to leverage a distribution of similar tasks, inspired by human learning Lake et al. (2015). This\ndeﬁnes a new supervised learning setup (also called ‘meta-learning’) in which the input-output pairs\nare no longer given by iid samples of images and their associated labels, but by iid samples of\ncollections of images and their associated label similarity.\nA recent and highly-successful research program has exploited this meta-learning paradigm on the\nfew-shot image classiﬁcation task Lake et al. (2015); Koch et al. (2015); Vinyals et al. (2016); Mishra\net al. (2017); Snell et al. (2017). In essence, these works learn a contextual, task-speciﬁc similarity\nmeasure, that ﬁrst embeds input images using a CNN, and then learns how to combine the embedded\nimages in the collection to propagate the label information towards the target image.\nIn particular, Vinyals et al. (2016) cast the few-shot learning problem as a supervised classiﬁcation\ntask mapping a support set of images into the desired label, and developed an end-to-end architec-\nture accepting those support sets as input via attention mechanisms. In this work, we build upon\nthis line of work, and argue that this task is naturally expressed as a supervised interpolation prob-\nlem on a graph, where nodes are associated with the images in the collection, and edges are given\nby a trainable similarity kernels. Leveraging recent progress on representation learning for graph-\nstructured data Bronstein et al. (2017); Gilmer et al. (2017), we thus propose a simple graph-based\nfew-shot learning model that implements a task-driven message passing algorithm. The resulting\narchitecture is trained end-to-end, captures the invariances of the task, such as permutations within\nthe input collections, and offers a good tradeoff between simplicity, generality, performance and\nsample complexity.\nBesides few-shot learning, a related task is the ability to learn from a mixture of labeled and unla-\nbeled examples — semi-supervised learning, as well as active learning, in which the learner has the\n∗Work done while Victor Garcia was a visiting scholar at New York University\n1\n\n\nPublished as a conference paper at ICLR 2018\noption to request those missing labels that will be most helpful for the prediction task. Our graph-\nbased architecture is naturally extended to these setups with minimal changes in the training design.\nWe validate experimentally the model on few-shot image classiﬁcation, matching state-of-the-art\nperformance with considerably fewer parameters, and demonstrate applications to semi-supervised\nand active learning setups.\nOur contributions are summarized as follows:\n• We cast few-shot learning as a supervised message passing task which is trained end-to-end\nusing graph neural networks.\n• We match state-of-the-art performance on Omniglot and Mini-Imagenet tasks with fewer\nparameters.\n• We extend the model in the semi-supervised and active learning regimes.\nThe rest of the paper is structured as follows. Section 2 describes related work, Sections 3, 4 and 5\npresent the problem setup, our graph neural network model and the training, and Section 6 reports\nnumerical experiments.\n2\nRELATED WORK\nOne-shot learning was ﬁrst introduced by Fei-Fei et al. (2006), they assumed that currently learned\nclasses can help to make predictions on new ones when just one or few labels are available. More\nrecently, Lake et al. (2015) presented a Hierarchical Bayesian model that reached human level error\non few-shot learning alphabet recongition tasks.\nSince then, great progress has been done in one-shot learning. Koch et al. (2015) presented a deep-\nlearning model based on computing the pair-wise distance between samples using Siamese Net-\nworks, then, this learned distance can be used to solve one-shot problems by k-nearest neighbors\nclassiﬁcation. Vinyals et al. (2016) Presented an end-to-end trainable k-nearest neighbors using\nthe cosine distance, they also introduced a contextual mechanism using an attention LSTM model\nthat takes into account all the samples of the subset T when computing the pair-wise distance be-\ntween samples. Snell et al. (2017) extended the work from Vinyals et al. (2016), by using euclidean\ndistance instead of cosine which provided signiﬁcant improvements, they also build a prototype rep-\nresentation of each class for the few-shot learning scenario. Mehrotra & Dukkipati (2017) trained\na deep residual network together with a generative model to approximate the pair-wise distance\nbetween samples.\nA new line of meta-learners for one-shot learning is rising lately: Ravi & Larochelle (2016) in-\ntroduced a meta-learning method where an LSTM updates the weights of a classiﬁer for a given\nepisode. Munkhdalai & Yu (2017) also presented a meta-learning architecture that learns meta-level\nknowledge across tasks, and it changes its inductive bias via fast parametrization. Finn et al. (2017)\nis using a model agnostic meta-learner based on gradient descent, the goal is to train a classiﬁcation\nmodel such that given a new task, a small amount of gradient steps with few data will be enough\nto generalize. Lately, Mishra et al. (2017) used Temporal Convolutions which are deep recurrent\nnetworks based on dilated convolutions, this method also exploits contextual information from the\nsubset T providing very good results.\nAnother related area of research concerns deep learning architectures on graph-structured data. The\nGNN was ﬁrst proposed in Gori et al. (2005); Scarselli et al. (2009), as a trainable recurrent message-\npassing whose ﬁxed points could be adjusted discriminatively. Subsequent works Li et al. (2015);\nSukhbaatar et al. (2016) have relaxed the model by untying the recurrent layer weights and proposed\nseveral nonlinear updates through gating mechanisms. Graph neural networks are in fact natural\ngeneralizations of convolutional networks to non-Euclidean graphs. Bruna et al. (2013); Henaff\net al. (2015) proposed to learn smooth spectral multipliers of the graph Laplacian, albeit with high\ncomputational cost, and Defferrard et al. (2016); Kipf & Welling (2016) resolved the computational\nbottleneck by learning polynomials of the graph Laplacian, thus avoiding the computation of eigen-\nvectors and completing the connection with GNNs. In particular, Kipf & Welling (2016) was the\nﬁrst to propose the use of GNNs on semi-supervised classiﬁcation problems. We refer the reader\nto Bronstein et al. (2017) for an exhaustive literature review on the topic. GNNs and the analogous\nNeural Message Passing Models are ﬁnding application in many different domains. Battaglia et al.\n2\n\n\nPublished as a conference paper at ICLR 2018\n(2016); Chang et al. (2016) develop graph interaction networks that learn pairwise particle interac-\ntions and apply them to discrete particle physical dynamics. Duvenaud et al. (2015); Kearnes et al.\n(2016) study molecular ﬁngerprints using variants of the GNN architecture, and Gilmer et al. (2017)\nfurther develop the model by combining it with set representations Vinyals et al. (2015), showing\nstate-of-the-art results on molecular prediction.\n3\nPROBLEM SET-UP\nWe describe ﬁrst the general setup and notations, and then particularize it to the case of few-shot\nlearning, semi-supervised learning and active learning.\nWe consider input-output pairs (Ti, Yi)i drawn iid from a distribution P of partially-labeled image\ncollections\nT\n=\n\b\n{(x1, l1), . . . (xs, ls)}, {˜x1, . . . , ˜xr}, {¯x1, . . . , ¯xt} ; li ∈{1, K}, xi, ˜xj, ¯xj ∼Pl(RN)\n\t\n,\nand Y\n=\n(y1, . . . , yt) ∈{1, K}t ,\n(1)\nfor arbitrary values of s, r, t and K. Where s is the number of labeled samples, r is the number of\nunlabeled samples (r > 0 for the semi-supervised and active learning scenarios) and t is the number\nof samples to classify. K is the number of classes. We will focus in the case t = 1 where we just\nclassify one sample per task T . Pl(RN) denotes a class-speciﬁc image distribution over RN. In our\ncontext, the targets Yi are associated with image categories of designated images ¯x1, . . . , ¯xt ∈Ti\nwith no observed label. Given a training set {(Ti, Yi)i}i≤L, we consider the standard supervised\nlearning objective\nmin\nΘ\n1\nL\nX\ni≤L\nℓ(Φ(Ti; Θ), Yi) + R(Θ) ,\nusing the model Φ(T ; Θ) = p(Y | T ) speciﬁed in Section 4 and R is a standard regularization\nobjective.\nFew-Shot Learning\nWhen r = 0, t = 1 and s = qK, there is a single image in the collection\nwith unknown label. If moreover each label appears exactly q times, this setting is referred as the\nq-shot, K-way learning.\nSemi-Supervised Learning\nWhen r > 0 and t = 1, the input collection contains auxiliary images\n˜x1, . . . , ˜xr that the model can use to improve the prediction accuracy, by leveraging the fact that\nthese samples are drawn from common distributions as those determining the output.\nActive Learning\nIn the active learning setting, the learner has the ability to request labels from\nthe sub-collection {˜x1, . . . , ˜xr}. We are interested in studying to what extent this active learning\ncan improve the performance with respect to the previous semi-supervised setup, and match the\nperformance of the one-shot learning setting with s0 known labels when s + r = s0, s ≪s0.\n4\nMODEL\nThis section presents our approach, based on a simple end-to-end graph neural network architecture.\nWe ﬁrst explain how the input context is mapped into a graphical representation, then detail the\narchitecture, and next show how this model generalizes a number of previously published few-shot\nlearning architectures.\n4.1\nSET AND GRAPH INPUT REPRESENTATIONS\nThe input T contains a collection of images, both labeled and unlabeled. The goal of few-shot\nlearning is to propagate label information from labeled samples towards the unlabeled query image.\nThis propagation of information can be formalized as a posterior inference over a graphical model\ndetermined by the input images and labels.\nFollowing several recent works that cast posterior inference using message passing with neural net-\nworks deﬁned over graphs Scarselli et al. (2009); Duvenaud et al. (2015); Gilmer et al. (2017), we\n3\n\n\nPublished as a conference paper at ICLR 2018\nFigure 1: Visual representation of One-Shot Learning setting.\nassociate T with a fully-connected graph GT = (V, E) where nodes va ∈V correspond to the\nimages present in T (both labeled and unlabeled). In this context, the setup does not specify a ﬁxed\nsimilarity ea,a′ between images xa and xa′, suggesting an approach where this similarity measure\nis learnt in a discriminative fashion with a parametric model similarly as in Gilmer et al. (2017),\nsuch as a siamese neural architecture. This framework is closely related to the set representation\nfrom Vinyals et al. (2016), but extends the inference mechanism using the graph neural network\nformalism that we detail next.\n4.2\nGRAPH NEURAL NETWORKS\nGraph Neural Networks, introduced in Gori et al. (2005); Scarselli et al. (2009) and further simpliﬁed\nin Li et al. (2015); Duvenaud et al. (2015); Sukhbaatar et al. (2016) are neural networks based on\nlocal operators of a graph G = (V, E), offering a powerful balance between expressivity and sample\ncomplexity; see Bronstein et al. (2017) for a recent survey on models and applications of deep\nlearning on graphs.\nIn its simplest incarnation, given an input signal F ∈RV ×d on the vertices of a weighted graph G,\nwe consider a family A of graph intrinsic linear operators that act locally on this signal. The simplest\nis the adjacency operator A : F 7→A(F) where (AF)i := P\nj∼i wi,jFj , with i ∼j iff (i, j) ∈E\nand wi,j its associated weight. A GNN layer Gc(·) receives as input a signal x(k) ∈RV ×dk and\nproduces x(k+1) ∈RV ×dk+1 as\nx(k+1)\nl\n= Gc(x(k)) = ρ\n X\nB∈A\nBx(k)θ(k)\nB,l\n!\n, l = d1 . . . dk+1 ,\n(2)\nwhere Θ = {θ(k)\n1 , . . . , θ(k)\n|A|}k, θ(k)\nB\n∈Rdk×dk+1, are trainable parameters and ρ(·) is a point-wise\nnon-linearity, chosen in this work to be a ‘leaky’ ReLU Xu et al. (2015).\nAuthors have explored several modeling variants from this basic formulation, by replacing the point-\nwise nonlinearity with gating operations Duvenaud et al. (2015), or by generalizing the generator\nfamily to Laplacian polynomials Defferrard et al. (2016); Kipf & Welling (2016); Bruna et al. (2013),\nor including 2J-th powers of A to A, AJ = min(1, A2J) to encode 2J-hop neighborhoods of each\nnode Bruna & Li (2017). Cascaded operations in the form (2) are able to approximate a wide\nrange of graph inference tasks. In particular, inspired by message-passing algorithms, Kearnes et al.\n(2016); Gilmer et al. (2017) generalized the GNN to also learn edge features ˜A(k) from the current\nnode hidden representation:\n˜A(k)\ni,j = ϕ˜θ(x(k)\ni\n, x(k)\nj ) ,\n(3)\n4\n\n\nPublished as a conference paper at ICLR 2018\nFigure 2: Graph Neural Network ilustration. The Adjacency matrix is computed before every Con-\nvolutional Layer.\nwhere ϕ is a symmetric function parametrized with e.g. a neural network. In this work, we consider\na Multilayer Perceptron stacked after the absolute difference between two vector nodes. See eq. 4:\nϕ˜θ(x(k)\ni\n, x(k)\nj ) = MLP˜θ(abs(x(k)\ni\n−x(k)\nj ))\n(4)\nThen ϕ is a metric, which is learned by doing a non-linear combination of the absolute difference\nbetween the individual features of two nodes. Using this architecture the distance property Symmetry\nϕ˜θ(a, b) = ϕ˜θ(b, a) is fulﬁlled by construction and the distance property Identity ϕ˜θ(a, a) = 0 is\neasily learned.\nThe trainable adjacency is then normalized to a stochastic kernel by using a softmax along each row.\nThe resulting update rules for node features are obtained by adding the edge feature kernel ˜A(k) into\nthe generator family A = { ˜A(k), 1} and applying (2). Adjacency learning is particularly important\nin applications where the input set is believed to have some geometric structure, but the metric is not\nknown a priori, such as is our case.\nIn general graphs, the network depth is chosen to be of the order of the graph diameter, so that all\nnodes obtain information from the entire graph. In our context, however, since the graph is densely\nconnected, the depth is interpreted simply as giving the model more expressive power.\nConstruction of Initial Node Features\nThe input collection T is mapped into node features as\nfollows. For images xi ∈T with known label li, the one-hot encoding of the label is concatenated\nwith the embedding features of the image at the input of the GNN.\nx(0)\ni\n= (φ(xi), h(li)) ,\n(5)\nwhere φ is a Convolutional neural network and h(l) ∈RK\n+ is a one-hot encoding of the label.\nArchitectural details for φ are detailed in Section 6.1.1 and 6.1.2. For images ˜xj, ¯xj′ with unknown\nlabel li, we modify the previous construction to account for full uncertainty about the label variable\nby replacing h(l) with the uniform distribution over the K-simplex: Vj = (φ(˜xj), K−11K), and\nanalogously for ¯x.\n4.3\nRELATIONSHIP WITH EXISTING MODELS\nThe graph neural network formulation of few-shot learning generalizes a number of recent models\nproposed in the literature.\nSiamese Networks\nSiamese Networks Koch et al. (2015) can be interpreted as a single layer\nmessage-passing iteration of our model, and using the same initial node embedding (5) x(0)\ni\n=\n(φ(xi), hi) , using a non-trainable edge feature\nϕ(xi, xj) = ∥φ(xi) −φ(xj)∥, ˜A(0) = softmax(−ϕ) ,\n5\n\n\nPublished as a conference paper at ICLR 2018\nand resulting label estimation\nˆY∗=\nX\nj\n˜A(0)\n∗,j⟨x(0)\nj , u⟩,\nwith u selecting the label ﬁeld from x. In this model, the learning is reduced to learning image\nembeddings φ(xi) whose euclidean metric is consistent with the label similarities.\nPrototypical Networks\nPrototypical networks Snell et al. (2017) evolve Siamese networks by\naggregating information within each cluster determined by nodes with the same label. This operation\ncan also be accomplished with a gnn as follows. we consider\n˜A(0)\ni,j =\n\u001a\nq−1\nif li = lj\n0\notherwise.\nwhere q is the number of examples per class, and\nx(1)\ni\n=\nX\nj\n˜A(0)\ni,j x(0)\nj\n,\nwhere x(0) is deﬁned as in the Siamese Networks. We ﬁnally apply the previous kernel ˜A(1) =\nsoftmax(ϕ) applied to x(1) to yield class prototypes:\nˆY∗=\nX\nj\n˜A(1)\n∗,j⟨x(1)\nj , u⟩.\nMatching Networks\nMatching networks Vinyals et al. (2016) use a set representation for the\nensemble of images in T , similarly as our proposed graph neural network model, but with two\nimportant differences. First, the attention mechanism considered in this set representation is akin\nto the edge feature learning, with the difference that the mechanism attends always to the same\nnode embeddings, as opposed to our stacked adjacency learning, which is closer to Vaswani et al.\n(2017). In other words, instead of the attention kernel in (3), matching networks consider attention\nmechanisms of the form ˜A(k)\n∗,j = ϕ(x(k)\n∗, x(T )\nj\n), where x(T )\nj\nis the encoding function for the elements\nof the support set, obtained with bidirectional LSTMs. In that case, the support set encoding is\nthus computed independently of the target image. Second, the label and image ﬁelds are treated\nseparately throughout the model, with a ﬁnal step that aggregates linearly the labels using a trained\nkernel. This may prevent the model to leverage complex dependencies between labels and images\nat intermediate stages.\n5\nTRAINING\nWe describe next how to train the parameters of the GNN in the different setups we consider: few-\nshot learning, semi-supervised learning and active learning.\n5.1\nFEW-SHOT AND SEMI-SUPERVISED LEARNING\nIn this setup, the model is asked only to predict the label Y corresponding to the image to classify\n¯x ∈T , associated with node ∗in the graph. The ﬁnal layer of the GNN is thus a softmax mapping\nthe node features to the K-simplex. We then consider the Cross-entropy loss evaluated at node ∗:\nℓ(Φ(T ; Θ), Y ) = −\nX\nk\nyk log P(Y∗= yk | T ) .\nThe semi-supervised setting is trained identically — the only difference is that the initial label ﬁelds\nof the node will be ﬁlled with the uniform distribution on nodes corresponding to ˜xj.\n5.2\nACTIVE LEARNING\nIn the Active Learning setup, the model has the intrinsic ability to query for one of the labels from\n{˜x1, . . . , ˜xr}. The network will learn to ask for the most informative label in order to classify the\n6\n\n\nPublished as a conference paper at ICLR 2018\nsample ¯x ∈T . The querying is done after the ﬁrst layer of the GNN by using a Softmax attention\nover the unlabeled nodes of the graph. For this we apply a function g(x(1)\ni ) ∈R1 that maps each\nunlabeled vector node to a scalar value. Function g is parametrized by a two layers neural network.\nA Softmax is applied over the {1, . . . , r} scalar values obtained after applying g:\nAttention = Softmax(g(x(1)\n{1,...,r}))\nIn order to query only one sample, we set all elements from the Attention ∈Rr vector to 0 except\nfor one. At test time we keep the maximum value, at train time we randomly sample one value based\non its multinomial probability. Then we multiply this sampled attention by the label vectors:\nw · h(li∗) = ⟨Attention′, h(l{1,...,r})⟩\nThe label of the queried vector h(li∗) is obtained, scaled by the weight w ∈(0, 1). This value is then\nsummed to the current representation x(1)\ni∗, since we are using dense connections in our GNN model\nwe can sum this w · h(li∗) value directly to where the uniform label distribution was concatenated\nx(1)\ni∗= [Gc(x(0)\ni∗), x(0)\ni∗] = [Gc(x(0)\ni∗), (φ(xi∗), h(li∗))]\nAfter the label has been summed to the current node, the information is forward propagated. This\nattention part is trained end-to-end with the rest of the network by backpropagating the loss from\nthe output of the GNN.\n6\nEXPERIMENTS\nFor the few-shot, semi-supervised and active learning experiments we used the Omniglot dataset\npresented by Lake et al. (2015) and Mini-Imagenet dataset introduced by Vinyals et al. (2016) which\nis a small version of ILSVRC-12 Krizhevsky et al. (2012). All experiments are based on the q-shot,\nK-way setting. For all experiments we used the same values q-shot and K-way for both training\nand testing.\nCode available at: https://github.com/vgsatorras/few-shot-gnn\n6.1\nDATASETS AND IMPLEMENTATION\n6.1.1\nOMNIGLOT\nDataset:\nOmniglot is a dataset of 1623 characters from 50 different alphabets, each character/class\nhas been drawn by 20 different people. Following Vinyals et al. (2016) implementation we split the\ndataset into 1200 classes for training and the remaining 423 for testing. We augmented the dataset\nby multiples of 90 degrees as proposed by Santoro et al. (2016).\nArchitectures:\nInspired by the embedding architecture from Vinyals et al. (2016), following\nMishra et al. (2017), a CNN was used as an embedding φ function consisting of four stacked blocks\nof {3×3-convolutional layer with 64 ﬁlters, batch-normalization, 2×2 max-pooling, leaky-relu} the\noutput is passed through a fully connected layer resulting in a 64-dimensional embedding. For the\nGNN we used 3 blocks each of them composed by 1) a module that computes the adjacency matrix\nand 2) a graph convolutional layer. A more detailed description of each block can be found at Figure\n3.\n6.1.2\nMINI-IMAGENET\nDataset:\nMini-Imagenet is a more challenging dataset for one-shot learning proposed by Vinyals\net al. (2016) derived from the original ILSVRC-12 dataset Krizhevsky et al. (2012). It consists of\n84×84 RGB images from 100 different classes with 600 samples per class. It was created with the\npurpose of increasing the complexity for one-shot tasks while keeping the simplicity of a light size\ndataset, that makes it suitable for fast prototyping. We used the splits proposed by Ravi & Larochelle\n(2016) of 64 classes for training, 16 for validation and 20 for testing. Using 64 classes for training,\nand the 16 validation classes only for early stopping and parameter tuning.\n7\n\n\nPublished as a conference paper at ICLR 2018\nArchitecture:\nThe embedding architecture used for Mini-Imagenet is formed by 4 convolutional\nlayers followed by a fully-connected layer resulting in a 128 dimensional embedding. This light\narchitecture is useful for fast prototyping:\n1×{3×3-conv. layer (64 ﬁlters), batch normalization, max pool(2, 2), leaky relu},\n1×{3×3-conv. layer (96 ﬁlters), batch normalization, max pool(2, 2), leaky relu},\n1×{3×3-conv. layer (128 ﬁlters), batch normalization, max pool(2, 2), leaky relu, dropout(0.5)},\n1×{3×3-conv. layer (256 ﬁlters), batch normalization, max pool(2, 2), leaky relu, dropout(0.5)},\n1×{ fc-layer (128 ﬁlters), batch normalization}.\nThe two dropout layers are useful to avoid overﬁtting the GNN in Mini-Imagenet dataset. The GNN\narchitecture is similar than for Omniglot, it is formed by 3 blocks, each block is described at Figure\n3.\n6.2\nFEW-SHOT\nFew-shot learning experiments for Omniglot and Mini-Imagenet are presented at Table 1 and Table\n2 respectively.\nWe evaluate our model by performing different q-shot, K-way experiments on both datasets. For\nevery few-shot task T , we sample K random classes from the dataset, and from each class we\nsample q random samples. An extra sample to classify is chosen from one of that K classes.\nOmniglot: The GNN method is providing competitive results while still remaining simpler than\nother methods. State of the art results are reached in the 5-Way and 20-way 1-shot experiments. In\nthe 20-Way 1-shot setting the GNN is providing slightly better results than Munkhdalai & Yu (2017)\nwhile still being a more simple approach. The TCML approach from Mishra et al. (2017) is in the\nsame conﬁdence interval for 3 out of 4 experiments, but it is slightly better for the 20-Way 5-shot,\nalthough the number of parameters is reduced from ∼5M (TCML) to ∼300K (3 layers GNN).\nAt Mini-Imagenet table we are also presenting a baseline ”Our metric learning + KNN” where no\ninformation has been aggregated among nodes, it is a K-nearest neighbors applied on top of the\npair-wise learnable metric ϕθ(x(0)\ni , x(0)\nj ) and trained end-to-end, this learnable metric is competi-\ntive by itself compared to other state of the art methods. Even so, a signiﬁcant improvement (from\n64.02% to 66.41%) can be seen for the 5-shot 5-Way Mini-Imagenet setting when aggregating in-\nformation among nodes by using the full GNN architecture. A variety of embedding functions φ are\nused among the different papers for Mini-Imagenet experiments, in our case we are using a simple\nnetwork of 4 conv. layers followed by a fully connected layer (Section 6.1.2) which served us to\ncompare between Our GNN and Our metric learning + KNN and it is useful for fast prototyping.\nMore complex embeddings have proven to produce better results, at Mishra et al. (2017) a deep\nresidual network is used as embedding network φ increasing the accuracy considerably. Regarding\nthe TCML architecture in Mini-Imagenet, the number of parameters is reduced from ∼11M (TCML)\nto ∼400K (3 layers GNN).\n5-Way\n20-Way\nModel\n1-shot\n5-shot\n1-shot\n5-shot\nPixels Vinyals et al. (2016)\n41.7%\n63.2%\n26.7%\n42.6%\nSiamese Net Koch et al. (2015)\n97.3%\n98.4%\n88.2%\n97.0%\nMatching Networks Vinyals et al. (2016)\n98.1%\n98.9%\n93.8%\n98.5%\nN. Statistician Edwards & Storkey (2016)\n98.1%\n99.5%\n93.2%\n98.1%\nRes. Pair-Wise Mehrotra & Dukkipati (2017)\n-\n-\n94.8%\n-\nPrototypical Networks Snell et al. (2017)\n97.4%\n99.3%\n95.4%\n98.8%\nConvNet with Memory Kaiser et al. (2017)\n98.4%\n99.6%\n95.0%\n98.6%\nAgnostic Meta-learner Finn et al. (2017)\n98.7 ±0.4%\n99.9 ±0.3%\n95.8 ±0.3%\n98.9 ±0.2%\nMeta Networks Munkhdalai & Yu (2017)\n98.9%\n-\n97.0%\n-\nTCML Mishra et al. (2017)\n98.96% ±0.20% 99.75% ±0.11% 97.64% ±0.30% 99.36% ±0.18%\nOur GNN\n99.2%\n99.7%\n97.4%\n99.0%\nTable 1: Few-Shot Learning — Omniglot accuracies. Siamese Net results are extracted from Vinyals\net al. (2016) reimplementation.\n8\n\n\nPublished as a conference paper at ICLR 2018\n5-Way\nModel\n1-shot\n5-shot\nMatching Networks Vinyals et al. (2016)\n43.6%\n55.3%\nPrototypical Networks Snell et al. (2017)\n46.61% ±0.78%\n65.77% ±0.70%\nModel Agnostic Meta-learner Finn et al. (2017)\n48.70% ±1.84%\n63.1% ±0.92%\nMeta Networks Munkhdalai & Yu (2017)\n49.21% ±0.96\n-\nRavi & Larochelle Ravi & Larochelle (2016)\n43.4% ±0.77%\n60.2% ±0.71%\nTCML Mishra et al. (2017)\n55.71% ±0.99%\n68.88% ±0.92%\nOur metric learning + KNN\n49.44% ±0.28%\n64.02% ±0.51%\nOur GNN\n50.33% ±0.36%\n66.41% ±0.63%\nTable 2: Few-shot learning — Mini-Imagenet average accuracies with 95% conﬁdence intervals.\n6.3\nSEMI-SUPERVISED\nSemi-supervised experiments are performed on the 5-way 5-shot setting. Different results are pre-\nsented when 20% and 40% of the samples are labeled. The labeled samples are balanced among\nclasses in all experiments, in other words, all the classes have the same amount of labeled and\nunlabeled samples.\nTwo strategies can be seen at Tables 3 and 4. ”GNN - Trained only with labeled” is equivalent to\nthe supervised few-shot setting, for example, in the 5-Way 5-shot 20%-labeled setting, this method\nis equivalent to the 5-way 1-shot learning setting since it is ignoring the unlabeled samples. ”GNN\n- Semi supervised” is the actual semi-supervised method, for example, in the 5-Way 5-shot 20%-\nlabeled setting, the GNN receives as input 1 labeled sample per class and 4 unlabeled samples per\nclass.\nOmniglot results are presented at Table 3, for this scenario we observe that the accuracy improve-\nment is similar when adding images than when adding labels. The GNN is able to extract infor-\nmation from the input distribution of unlabeled samples such that only using 20% of the labels in a\n5-shot semi-supervised environment we get same results as in the 40% supervised setting.\nIn Mini-Imagenet experiments, Table 4, we also notice an improvement when using semi-supervised\ndata although it is not as signiﬁcant as in Omniglot. The distribution of Mini-Imagenet images is\nmore complex than for Omniglot. In spite of it, the GNN manages to improve by ∼2% in the 20%\nand 40% settings.\n5-Way 5-shot\nModel\n20%-labeled\n40%-labeled\n100%-labeled\nGNN - Trained only with labeled\n99.18%\n99.59%\n99.71%\nGNN - Semi supervised\n99.59%\n99.63%\n99.71%\nTable 3: Semi-Supervised Learning — Omniglot accuracies.\n5-Way 5-shot\nModel\n20%-labeled\n40%-labeled\n100%-labeled\nGNN - Trained only with labeled\n50.33% ±0.36%\n56.91% ±0.42%\n66.41% ±0.63%\nGNN - Semi supervised\n52.45% ±0.88%\n58.76% ±0.86%\n66.41% ±0.63%\nTable 4: Semi-Supervised Learning — Mini-Imagenet average accuracies with 95% conﬁdence\nintervals.\n6.4\nACTIVE LEARNING\nWe performed Active Learning experiments on the 5-Way 5-shot set-up when 20% of the samples\nare labeled. In this scenario our network will query for the label of one sample from the unlabeled\nones. The results are compared with the Random baseline where the network chooses a random\nsample to be labeled instead of one that maximally reduces the loss of the classiﬁcation task T .\n9\n\n\nPublished as a conference paper at ICLR 2018\nResults are shown at Table 5. The results of the GNN-Random criterion are close to the Semi-\nsupervised results for 20%-labeled samples from Tables 3 and 4. It means that selecting one random\nlabel practically does not improve the accuracy at all. When using the GNN-AL learned criterion, we\nnotice an improvement of ∼3.4% for Mini-Imagenet, it means that the GNN manages to correctly\nchoose a more informative sample than a random one. In Omniglot the improvement is smaller since\nthe accuracy is almost saturated and the improving margin is less.\nMethod\n5-Way 5-shot 20%-labeled\nGNN - AL\n99.62%\nGNN - Random\n99.59%\nMethod\n5-Way 5-shot 20%-labeled\nGNN - AL\n55.99% ±1.35%\nGNN - Random\n52.56% ±1.18%\nTable 5: Omniglot (left) and Mini-Imagneet (right), average accuracies are shown at both tables, the\nGNN-AL is the learned criterion that performs Active Learning by selecting the sample that will\nmaximally reduce the loss of the current classiﬁcation. The GNN - Random is also selecting one\nsample, but in this case a random one. Mini-Imagenet results are presented with 95% conﬁdence\nintervals.\n7\nCONCLUSIONS\nThis paper explored graph neural representations for few-shot, semi-supervised and active learn-\ning. From the meta-learning perspective, these tasks become supervised learning problems where\nthe input is given by a collection or set of elements, whose relational structure can be leveraged\nwith neural message passing models. In particular, stacked node and edge features generalize the\ncontextual similarity learning underpinning previous few-shot learning models.\nThe graph formulation is helpful to unify several training setups (few-shot, active, semi-supervised)\nunder the same framework, a necessary step towards the goal of having a single learner which is\nable to operate simultaneously in different regimes (stream of labels with few examples per class, or\nstream of examples with few labels). This general goal requires scaling up graph models to millions\nof nodes, motivating graph hierarchical and coarsening approaches Defferrard et al. (2016).\nAnother future direction is to generalize the scope of Active Learning, to include e.g. the ability to\nask questions Rothe et al. (2017), or in reinforcement learning setups, where few-shot learning is\ncritical to adapt to non-stationary environments.\nACKNOWLEDGMENTS\nThis work was partly supported by Samsung Electronics (Improving Deep Learning using Latent\nStructure).\nREFERENCES\nPeter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks\nfor learning about objects, relations and physics. In Advances in Neural Information Processing\nSystems, pp. 4502–4510, 2016.\nMichael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geomet-\nric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18–42,\n2017.\nJoan Bruna and Xiang Li.\nCommunity detection with graph neural networks.\narXiv preprint\narXiv:1705.08415, 2017.\nJoan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally\nconnected networks on graphs. Proc. ICLR, 2013.\nMichael B. Chang, Tomer Ullman, Antonio Torralba, and Joshua B. Tenenbaum. A compositional\nobject-based approach to learning physical dynamics. ICLR, 2016.\n10\n\n\nPublished as a conference paper at ICLR 2018\nMicha¨el Defferrard, Xavier Bresson, and Pierre Vandergheynst.\nConvolutional neural networks\non graphs with fast localized spectral ﬁltering. In Advances in Neural Information Processing\nSystems, pp. 3837–3845, 2016.\nDavid Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael G´omez-Bombarelli, Tim-\nothy Hirzel, Al´an Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for\nlearning molecular ﬁngerprints. In Neural Information Processing Systems, 2015.\nHarrison Edwards and Amos Storkey.\nTowards a neural statistician.\narXiv preprint\narXiv:1606.02185, 2016.\nLi Fei-Fei, Rob Fergus, and Pietro Perona. One-shot learning of object categories. IEEE transactions\non pattern analysis and machine intelligence, 28(4):594–611, 2006.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation\nof deep networks. arXiv preprint arXiv:1703.03400, 2017.\nJustin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural\nmessage passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.\nM. Gori, G. Monfardini, and F. Scarselli. A new model for learning in graph domains. In Proc.\nIJCNN, 2005.\nM. Henaff, J. Bruna, and Y. LeCun.\nDeep convolutional networks on graph-structured data.\narXiv:1506.05163, 2015.\nŁukasz Kaiser, Oﬁr Nachum, Aurko Roy, and Samy Bengio. Learning to remember rare events.\narXiv preprint arXiv:1703.03129, 2017.\nSteven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph\nconvolutions: moving beyond ﬁngerprints. Journal of computer-aided molecular design, 30(8):\n595–608, 2016.\nThomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional net-\nworks. arXiv preprint arXiv:1609.02907, 2016.\nGregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot\nimage recognition. In ICML Deep Learning Workshop, volume 2, 2015.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convo-\nlutional neural networks. In Advances in neural information processing systems, pp. 1097–1105,\n2012.\nBrenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning\nthrough probabilistic program induction. Science, 350(6266):1332–1338, 2015.\nYujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural\nnetworks. arXiv preprint arXiv:1511.05493, 2015.\nAkshay Mehrotra and Ambedkar Dukkipati. Generative adversarial residual pairwise networks for\none shot learning. arXiv preprint arXiv:1703.08033, 2017.\nNikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. Meta-learning with temporal\nconvolutions. arXiv preprint arXiv:1707.03141, 2017.\nTsendsuren Munkhdalai and Hong Yu. Meta networks. arXiv preprint arXiv:1703.00837, 2017.\nSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. ICLR, 2016.\nAnselm Rothe, Brenden Lake, and Todd Gureckis. Question asking as program generation. NIPS,\n2017.\nAdam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-\nlearning with memory-augmented neural networks. In International conference on machine learn-\ning, pp. 1842–1850, 2016.\n11\n\n\nPublished as a conference paper at ICLR 2018\nFranco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.\nThe graph neural network model. IEEE Transactions on Neural Networks, 20(1):61–80, 2009.\nJake Snell, Kevin Swersky, and Richard S Zemel. Prototypical networks for few-shot learning. arXiv\npreprint arXiv:1703.05175, 2017.\nSainbayar Sukhbaatar, Rob Fergus, et al. Learning multiagent communication with backpropaga-\ntion. In Advances in Neural Information Processing Systems, pp. 2244–2252, 2016.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762,\n2017.\nOriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets.\narXiv preprint arXiv:1511.06391, 2015.\nOriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one\nshot learning. In Advances in Neural Information Processing Systems, pp. 3630–3638, 2016.\nBing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectiﬁed activations in\nconvolutional network. arXiv preprint arXiv:1505.00853, 2015.\n12\n\n\nPublished as a conference paper at ICLR 2018\nAPPENDIX\nFigure 3: GNN model. Three blue blocks are used for Omniglot and Mini-Imagenet. (nf=96).\n13\n"
}