{
  "filename": "2106.13871v3.pdf",
  "num_pages": 13,
  "pages": [
    "Transflower: probabilistic autoregressive dance generation with\nmultimodal attention\nGUILLERMO VALLE-PÃ‰REZ, Inria, Ensta ParisTech, University of Bordeaux, France\nGUSTAV EJE HENTER, KTH Royal Institute of Technology, Sweden\nJONAS BESKOW, KTH Royal Institute of Technology, Sweden\nANDRE HOLZAPFEL, KTH Royal Institute of Technology, Sweden\nPIERRE-YVES OUDEYER, Inria, Ensta ParisTech, University of Bordeaux, France\nSIMON ALEXANDERSON, KTH Royal Institute of Technology, Sweden\nFig. 1. We have aggregated the largest dataset of 3D dance motion, and used it to train Transflower, a new probabilistic autoregressive model of motion. As a\nresult we obtained a model that can generate dance to any piece of music, which ranks well in terms of appropriateness, naturalness, and diversity.\nDance requires skillful composition of complex movements that follow\nrhythmic, tonal and timbral features of music. Formally, generating dance\nconditioned on a piece of music can be expressed as a problem of modelling\na high-dimensional continuous motion signal, conditioned on an audio\nsignal. In this work we make two contributions to tackle this problem. First,\nwe present a novel probabilistic autoregressive architecture that models\nthe distribution over future poses with a normalizing flow conditioned on\nprevious poses as well as music context, using a multimodal transformer\nencoder. Second, we introduce the currently largest 3D dance-motion dataset,\nobtained with a variety of motion-capture technologies, and including both\nprofessional and casual dancers. Using this dataset, we compare our new\nmodel against two baselines, via objective metrics and a user study, and\nshow that both the ability to model a probability distribution, as well as\nbeing able to attend over a large motion and music context are necessary to\nproduce interesting, diverse, and realistic dance that matches the music.\nAuthorsâ€™ addresses: Guillermo Valle-PÃ©rez, guillermo-jorge.valle-perez@inria.fr, In-\nria, Ensta ParisTech, University of Bordeaux, Bordeaux, France; Gustav Eje Henter,\nghe@kth.se, KTH Royal Institute of Technology, Stockholm, Sweden; Jonas Beskow,\nbeskow@kth.se, KTH Royal Institute of Technology, Stockholm, Sweden; Andre Holzap-\nfel, holzap@kth.se, KTH Royal Institute of Technology, Stockholm, Sweden; Pierre-Yves\nOudeyer, pierre-yves.oudeyer@inria.fr, Inria, Ensta ParisTech, University of Bordeaux,\nBordeaux, France; Simon Alexanderson, simonal@kth.se, KTH Royal Institute of Tech-\nnology, Stockholm, Sweden.\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nÂ© 2021 Copyright held by the owner/author(s).\n0730-0301/2021/12-ART1\nhttps://doi.org/10.1145/3478513.3480570\nCCS Concepts: â€¢ Computing methodologies â†’Animation; Neural net-\nworks; Motion capture.\nAdditional Key Words and Phrases: Generative models, machine learning,\nnormalising flows, Glow, transformers, dance\nACM Reference Format:\nGuillermo Valle-PÃ©rez, Gustav Eje Henter, Jonas Beskow, Andre Holzapfel,\nPierre-Yves Oudeyer, and Simon Alexanderson. 2021. Transflower: probabil-\nistic autoregressive dance generation with multimodal attention. ACM Trans.\nGraph. 40, 6, Article 1 (December 2021), 13 pages. https://doi.org/10.1145/\n3478513.3480570\n1\nINTRODUCTION\nDancing â€“ body motions performed together with music â€“ is a\ndeeply human activity that transcends cultural barriers, and we\nhave been called â€œthe dancing speciesâ€ [LaMothe 2019]. Today, con-\ntent involving dance is some of the most watched on digital video\nplatforms such as YouTube and TikTok. The recent pandemic led\ndance â€“ as other performing arts â€“ to become an increasingly virtual\npractice, and hence an increasingly digitized cultural expression.\nHowever, good dancing, whether analog or digital, is challenging\nto create. Professional dancing requires physical prowess and ex-\ntensive practise, and capturing or recreating a similar experience\nthrough digital means is labour intensive, whether done through\nmotion capture or hand animation. Consequently, the problem of\nautomatic, data-driven dance generation has gathered interest in\nrecent years [Li et al. 2021b, 2020, 2021a; Zhuang et al. 2020]. Access\nto generative models of dance could help creators and animators, by\nspeeding up their workflow, by offering inspiration, and by opening\nACM Trans. Graph., Vol. 40, No. 6, Article 1. Publication date: December 2021.\narXiv:2106.13871v3  [cs.SD]  11 Jun 2022\n",
    "1:2\nâ€¢\nGuillermo Valle-PÃ©rez, Gustav Eje Henter, Jonas Beskow, Andre Holzapfel, Pierre-Yves Oudeyer, and Simon Alexanderson\nup novel possibilities such as creating interactive characters that\nreact to the userâ€™s choice of music in real time. The same models\ncan also give insight into how humans connect music and move-\nment, both of which have been identified as capturing important\nand inter-related aspects of our cognition [BlÃ¤sing et al. 2012].\nThe kinematic processes embodied in dance are highly complex\nand nonlinear, even when compared to other human movement\nsuch as locomotion. Dance is furthermore multimodal, and the con-\nnection between music and dance motion is extremely multifaceted\nand far from deterministic. Generative modelling with deep neural\nnetworks is becoming one of the most promising approaches to\nlearn representations of such complex domains. This general ap-\nproach has already made significant progress in the domains of\nimages [Brock et al. 2018; Karras et al. 2019; Park et al. 2019], music\n[Dhariwal et al. 2020; Huang et al. 2018], motion [Henter et al. 2020;\nLing et al. 2020], speech [Prenger et al. 2019; Shen et al. 2018], and\nnatural language [Brown et al. 2020; Raffel et al. 2019]. Recently, mul-\ntimodal models are being developed, that learn to capture the even\nmore complex interactions between standard data domains, such\nas between language and images [Ramesh et al. 2021], or between\nlanguage and video [Wu et al. 2021]. Similarly, dance synthesis sits\nat the intersection between movement modelling and music un-\nderstanding, and is an exciting problem that combines compelling\nmachine-learning challenges with a distinct sociocultural impact.\nIn this work, we tackle the problem of music-conditioned 3D\ndance motion generation through deep learning. In particular, we\nexplore two important factors that affect model performance on this\ndifficult task: 1) the ability to capture patterns that are extended over\nlonger periods of time, and 2) the ability to express complex probab-\nility distributions over the predicted outputs. We argue that previous\nworks are lacking in one of these two properties, and present a new\nautoregressive neural architecture which combines a transformer\n[Vaswani et al. 2017] to encode the multimodal context (previous\nmotion, and both previous and future music), and a normalizing flow\n[Papamakarios et al. 2021] head to faithfully model the future distri-\nbution over the predicted modality, which for dance synthesis is the\nfuture motion. We call this new architecture Transflower and show,\nthrough objective metrics and human evaluation studies, that both\nof these factors are important to model the complex distribution\nof movements in dance as well as their dependence on the music\nmodality. Human evaluations are the gold standard to evaluate the\nperceptual quality of generative models, and are complementary to\nthe objective metrics. Furthermore, they allow us to evaluate the\nmodel on arbitrary â€œin-the-wildâ€ songs downloaded from YouTube,\nfor which no ground truth dance motion is available.\nOne of the biggest challenges in learning-based motion synthesis\nis the availability of large-scale datasets for 3D movement. Existing\ndatasets are mainly gathered in two ways: in a motion capture\nstudio [CMU Graphics Lab 2003; Ferstl and McDonnell 2018; Lee\net al. 2019a; Mahmood et al. 2019; Mandery et al. 2015; Troje 2002],\nwhich provides the highest quality motion, but requires expensive\nequipment and is difficult to scale to larger dataset sizes, or via\nmonocular 3D pose estimation from video [Habibie et al. 2021; Peng\net al. 2018b], which trades off quality for a much larger availability\nof videos from the Internet.\nIn this paper we present the largest dataset of 3D dance motion,\ncombining different sources and motion capture technologies. We\nintroduce a new approach to obtain large-scale motion datasets,\ncomplementary to the two mostly used in previous works. Specific-\nally, we make use of the growing popularity and user base of virtual\nreality (VR) technologies, and of VR dancing in particular [Lang\n2021], to find participants interested in contributing dance data for\nour study. We argue that, while consumer-grade VR motion capture\ndoes not produce as high quality as professional motion capture, it\nis significantly better and more robust than current monocular 3D\npose estimation from video. Furthermore, it is poised to improve\nboth in quality and availability as the VR market grows [Statista\n2020], offering potential new avenues for participatory research.\nWe also collect the largest dataset of dance motion using profes-\nsional motion capture equipment, from both casual dancers and a\nprofessional dancer. Finally, to train our models, we combine our\nnew data, with two existing 3D dance motion datasets, GrooveNet\n[Alemi et al. 2017] and AIST++ [Li et al. 2021a], which we standard-\nize to a common skeleton. In total, we have over 20 h of dance data\nin a wide variety of dance styles, including freestyle, casual dancing,\nand street dance styles, as well as a variety of music genres, includ-\ning pop, hip hop, trap, K-pop, and street dance music. Furthermore,\nalthough all of our data sources offer higher quality motion capture\nthan 3D motion data estimated from monocular video, the different\nsources offer different levels of quality, and different capture arti-\nfacts. We find that this diversity in data sources, on top of the large\ndiversity in dance styles and skill levels, makes deterministic models\nunable to converge to model the data faithfully, while probabilistic\nmodels are able to adapt to such heterogeneous material.\nOur contributions are as follows:\nâ€¢ We present a novel architecture for autoregressive probabil-\nistic modelling of high-dimensional continuous signals, which\nwe demonstrate achieves state of the art performance on the\ntask of music-conditioned dance generation. Our architecture\nis, to the best of our knowledge, the first to combine the bene-\nfits of transformers for sequence modelling, with normalizing\nflows for probabilistic modelling.\nâ€¢ We introduce the largest dataset of 3D dance motion gener-\nated with a variety of motion capture systems. This dataset\nalso serves to showcase the potential of VR for participatory\nresearch and democratizing mocap.\nâ€¢ We evaluate our new model objectively and in a user study\nagainst two baselines, showing that both the probabilistic and\nmultimodal attention components are important to produce\nnatural and diverse dance matching the music.\nâ€¢ Finally, we explore the use of fine-tuning and â€œmotion prompt-\ningâ€ to attain control over the quality and style of the dance.\nOur paper website at metagen.ai/transflower provides data, code,\npre-trained models, videos, supplementary material, and a demo for\ntesting the models on any song with a selection of starting motion.\n2\nBACKGROUND AND PRIOR WORK\n2.1\nLearning-based motion synthesis\nThe task of generating 3D motion has been tackled in a variety of\nways. The traditional approaches to motion synthesis were based\nACM Trans. Graph., Vol. 40, No. 6, Article 1. Publication date: December 2021.\n",
    "Transflower: probabilistic autoregressive dance generation with multimodal attention\nâ€¢\n1:3\non retrieval from motion databases and motion graphs [Arikan and\nForsyth 2002; Chao et al. 2004; Kovar and Gleicher 2004; Kovar et al.\n2002; Lee et al. 2002; Safonova and Hodgins 2007; Takano et al. 2010].\nRecently, there has been more interest in statistical and learning-\nbased approaches, which can be more flexible and scale to larger\ndatasets and more complex tasks. Holden et al. [2020] explored\na continuum between the traditional retrieval-based approaches\nto motion synthesis and the more scalable deep learning-based\napproaches, showing that combining ideas from both may be fruitful.\nAmong the learning-based techniques, most works follow an\nautoregressive approach, where either the next pose or the next key\npose in the sequence is predicted based on previous poses in the\nsequence. For dance, the prediction is also conditioned on music\nfeatures, typically spanning a window of time around the time of\nprediction. We refer to both the previous poses and this window of\nmusic features together as the context. We can categorize the autore-\ngressive methods along the factors proposed in the introduction,\ni.e., the way the autoregressive model handles its inputs (context),\nand the way it handles its outputs (predicted motion). Later in this\nsection, we also compare approaches according to the amount of\nassumptions they make, and the type of learning algorithm.\nContext-dependence. We have seen an evolution towards mod-\nels that more effectively retain and utilize information from wider\ncontext windows. The first works applying neural networks to mo-\ntion prediction relied on recurrent neural networks like LSTMs,\nwhich were applied to unconditional motion [Fragkiadaki et al. 2015;\nZhou et al. 2018] and dance synthesis [Crnkovic-Friis and Crnkovic-\nFriis 2016; Tang et al. 2018]. LSTMs represent the recent context\nin a latent state. However, this latent state can act as a bottleneck\nhindering information flow from the past context, thus limiting the\nextent of the temporal correlations the network can learn. Different\narchitectures have been used to tackle this problem. BÃ¼tepage et al.\n[2017]; Holden et al. [2017]; Starke et al. [2020] directly feed the\nrecent history of poses through a feedforward network to predict\nthe next pose, while Zhuang et al. [2020] use a WaveNet-style ar-\nchitecture to extend the context even further for dance synthesis.\nRecently, Li et al. [2021a] introduce a cross-modal transformer ar-\nchitecture (extending [Vaswani et al. 2017]) that learns to attend to\nthe relevant features over the last 2 seconds of motion, as well as\nthe neighbouring 4 seconds of music, for dance generation.\nOutput modelling. Most works in motion synthesis have treated\nthe next-pose prediction as a deterministic function of the context.\nThis includes the earlier work using LSTMS [Fragkiadaki et al. 2015;\nTang et al. 2018; Zhou et al. 2018], and some of the most recent work\non dance generation [Li et al. 2021b,a]. However, in many situations,\nthe output motion is highly unconstrained by the input context. For\nexample, there are many plausible dance moves that can accompany\na piece of music, or many different gestures that fit well with an\nutterance [Alexanderson et al. 2020]. Earlier approaches to model\na probabilistic distribution over motions include Gaussian mixture\nmodels [Crnkovic-Friis and Crnkovic-Friis 2016] and Gaussian pro-\ncesses latent-variable models [Grochow et al. 2004; Levine et al. 2012;\nWang et al. 2008]. VAEs weaken the assumption of Gaussianity, and\nhave been applied to motion synthesis [Habibie et al. 2017; Ling et al.\n2020]. Recently, Petrovich et al. [2021] used VAEs combined with a\ntransformer for non-autoregressive motion synthesis â€“ they predict\nthe whole motion â€œat onceâ€, as the output of a full-attention trans-\nformer. Their architecture allows for learning complex multimodal\nmotion distributions, but their non-autoregressive approach limits\nthe length of the generated sequences. MoGlow [Henter et al. 2020]\nmodels the motion with an autoregressive normalizing flow, allow-\ning for fitting flexible probability distributions, with exact likelihood\nmaximization, producing state of the art motion synthesis results. In\nLi et al. [2020], they discretize the joint angle space. However, their\nmulti-softmax output distribution assumes independence of each\njoint, which is unusual in many cases. Lee et al. [2019b] develop a\nmusic-conditioned GAN to predict a distribution over latent rep-\nresentations of the motion. However, in order to stabilize training,\nthey apply an MSE regularization loss on the latents which may\nlimit its ability to model complex distributions. Li et al. [2021b] also\nintroduce an adversarially-trained model which, however, does not\nhave a noise input â€“ no source of randomness â€“ and thus cannot be\nsaid to model a music-conditioned probability distribution.1\nDomain-specific assumptions. A third dimension in which we\ncan compare the different approaches to motion synthesis is the\namount of domain-specific assumptions they make. To disambigu-\nate the predictions from a deterministic model, or to increase mo-\ntion realism, different works have added extra inputs to the model,\ntailored at the desired types of animations, including foot contact\n[Holden et al. 2016], pace [Pavllo et al. 2018], and phase information\n[Holden et al. 2017; Starke et al. 2020]. For dance synthesis, Lee\net al. [2019b] and Li et al. [2021b] use the observation that dance\ncan often be fruitfully decomposed into short movement segments,\nwhose transitions lie at music beats. Furthermore Li et al. [2021b]\ndevelop an architecture that includes inductive biases specific to\nkinematic skeletons, as well as a bias towards learning local tem-\nporal correlations. On the other end of the spectrum, Henter et al.\n[2020] presents a general sequence prediction model that makes few\nassumptions about the data. This allows it to be applied to varied\ntasks like humanoid or quadruped motion synthesis, or gesture gen-\neration [Alexanderson et al. 2020] without fundamentally changing\nthe model, but it has not yet been applied to dance. For dance syn-\nthesis, Li et al. [2021a] demonstrate that a similarly general-purpose\nmodel can produce impressive results.\nLearning algorithm. An alternative approach to learn to gener-\nate realistic movements from motion capture data is to use a set of\ntechniques known as imitation learning (IL). Merel et al. [2017] use\ngenerative adversarial imitation learning (GAIL), a technique closely\nrelated to GANs, to learn a controller for a physics-based humanoid\nto imitate different gait motions. Peng et al. [2018a] and Peng et al.\n[2021] extend this work to characters that learn a diversity of skills,\nwith a variety of morphologies. This approach can learn to imitate\nmocap data in a physics-based environment, so that the character\nmovement is automatically physically realistic, which is necessary\nbut not sufficient for natural motion. Ling et al. [2020] also found\ncombining IL approaches with previously described supervised and\nself-supervised learning approaches to be a promising direction to\nget the benefits of both.\n1However, it is possible that the music input itself could serve as a kind of noise source,\nallowing the model to effectively learn a probability distribution. This deserves further\ninvestigation.\nACM Trans. Graph., Vol. 40, No. 6, Article 1. Publication date: December 2021.\n",
    "1:4\nâ€¢\nGuillermo Valle-PÃ©rez, Gustav Eje Henter, Jonas Beskow, Andre Holzapfel, Pierre-Yves Oudeyer, and Simon Alexanderson\nOverall, we expect that learning-based motion synthesis will\nfollow a similar trend as in other areas where machine learning\nis applied to complex data distributions: models that can flexibly\nattend over a large context, like transformers, while being able\nto model complex distributions over its outputs, produce the best\ngenerative results, when enough data is available [Brown et al. 2020;\nDosovitskiy et al. 2020; Henighan et al. 2020; Kaplan et al. 2020;\nRamesh et al. 2021]. Here we present what we believe is the first\nmodel for autoregressive motion synthesis combining both of these\ndesirable properties, which we expect to become crucial as motion\ncapture datasets grow both in size and diversity. Furthermore, we use\nthe model for music-conditioned dance generation, demonstrating\nthat it is able to be used for tasks involving multiple modalities\n(music and movement in our case).\n2.2\nOther data-driven dance motion synthesis\nAlthough the focus of this work is on purely learning-based ap-\nproaches to dance synthesis, these are not the only data-driven\nmethods to for generating dance. As we discussed in section 2.1,\nthere is in fact a continuum between completely non-learning based\nand purely learning-based approaches. In general, learning-based\ntechniques typically trade off a larger compute cost for training,\nfor a reduced cost at inference [Holden et al. 2020], which often\namortizes the training cost. There are also several dimensions along\nwhich machine learning techniques can be introduced in a dance\nsynthesis pipeline. Here we focus on the motion synthesis part.\nSeveral works have approached the problem of dance generation\nusing motion graphs, where transitions between pre-made motion\nclips are used to create a choreography. In effect, these methods\ngenerate motion by selection, whereas deep learning can be seen as\nmore akin to interpolation. Fan et al. [2011] and Fukayama and Goto\n[2015] used statistical methods for traversing the graph, but recently,\ndeep learning techniques have been developed for traversing the\nmotion graph [Kang et al. 2021; Ye et al. 2020]. These approaches\ntend to produce reliable and high quality dances, that are easier to\nedit. However, a lot of motion-graph-based dance synthesis works\nrely on datasets of key-framed animations, which may cause the\ngenerated dances to look less natural. The large mocap dance data-\nbase we are introducing may therefore help enable more natural\nmotion also for this class of techniques.\n2.3\nDance datasets\nPrevious research on dance generation has been conducted with\ndata obtained from a variety of different techniques. Alemi et al.\n[2017] and Zhuang et al. [2020] recorded mocap datasets of dances\nsynchronized to music, totalling 23 min and 58 min, respectively. Li\net al. [2021a] use the AIST dance dataset [Tsuchida et al. 2019] to\nobtain 3D dance motion using multi-view 3D pose estimation. Lee\net al. [2019b] and Li et al. [2020] use monocular 3D pose estimation\nfrom YouTube videos to produce a dataset of 3D dance motion of 71\nh and 50 h in total, respectively. Monocular 3D pose estimation is\nan area of active research [Bogo et al. 2016; Mathis et al. 2020; Rong\net al. 2021], but current methods suffer from inaccurate root motion\nestimation [Li et al. 2021a], so these works tend to focus on the joint\nmovements. Finally, Li et al. [2021b] introduce a 5 h animated dance\ndataset, created by professional animators. Overall, we find that\nthere is a trade-off between data quality and data availability. In this\nwork, we present a new way of collecting motion data, from remote\nVR user participants, which pushes the currently available Pareto\nfrontier, with data quality approaching that of mocap equipment,\nand increasing availability as the number of VR users grows. We\nthink that the democratization of motion capture with VR, brings\nexciting possibilities for researchers and VR users alike. In particular,\nthe ability to crowdsource data at scale should make it possible to\nharness the scaling phenomena seen in many other generative-\nmodelling tasks [Henighan et al. 2020], and also study the effects of\nscaling on model performance. Compared to datasets like AIST++ [Li\net al. 2021a], we believe that our dataset captures a larger diversity\nof skill levels, as well as dance styles not present in AIST++.\n3\nDATASET\nIn this paper we introduce two new large scale datasets of 3D dance\nmotion synchronized to music: the PMSD dance dataset, and the\nShaderMotion VR dance dataset. We combine these datasets with\ntwo previous existing dance datasets, AIST++ [Li et al. 2021a] and\nGrooveNet [Alemi et al. 2017], to create our combined dataset on\nwhich we train our models. We standardize all the datasets into a\ncommon skeleton (the one used for the PMSD dataset), and will\npublicly release the data. We here describe the two new datasets,\nand compare all the sources, including existing ones, in table 1.\nPopular music and street dance (PMSD) dataset. The PMSD\ndance dataset consists of synchronized audio and motion capture\nrecordings of various dancers and dance styles. The data was recor-\nded with an Optitrack Prime41 motion capture system (17 cameras\nat 120 Hz) and is divided in two parts. The first part (PMSDCasual)\ncontains 142 minutes of casual dancing to popular music by 7 non-\nprofessional dancers. The 37 markers where solved to a skeleton\nwith 21 joints. The second part (PMSDStreet) contains 44 minutes of\nstreet dance performed by one professional dancer. The dances are\ndivided in three distinct styles: Hip-Hop, Popping and Krumping.\nThe music was selected by the dancer to be appropriate for each\nstyle. In this setup we used 65 markers (45 on the body and 2 on each\nfinger), and solved the data to a skeleton with 51 joints, including\nfingers and hinged toes. Compared to the casual dances, the street\ndances have considerably more complex choreographies with more\ntempo-shifts and less repetitive patterns.\nShaderMotion VR dance dataset. The data was recorded by\nparticipants who dance in the social VR platform VRChat2, using a\ntool called ShaderMotion that encodes their avatarâ€™s motion into a\nvideo [lox9973 2021]. Their avatar follows their movement using\ninverse kinematics, anchored to their real body movement via a\n6-point tracking system, where the head, hands, hips, and feet are\nbeing tracked, using a VR headset, and HTC Vive trackers. The\nvideos with encoded motion can then be converted back to motion\non a skeleton rig via the MotionPlayer script provided in the Sha-\nderMotion repository. The data includes finger tracking, with an\naccuracy dependent on the VR equipment used. We further divide\nthe data into four components, which have different dancers and\nstyles (see table 1). We only included part of the ShaderMotion for\n2https://hello.vrchat.com/\nACM Trans. Graph., Vol. 40, No. 6, Article 1. Publication date: December 2021.\n",
    "Transflower: probabilistic autoregressive dance generation with multimodal attention\nâ€¢\n1:5\nSource\nMinutes\n#dancers Styles\nAIST++*\n312.1\n30\nBreak, Pop, Lock,\nHip Hop, House,\nWaack,\nKrump,\nStreet Jazz, Ballet\nJazz\nGrooveNet*\n25.0\n1\nGrooveNet\nPMSDCasual*\n142.1\n1\nCasual\nPMSDStreet*\n88.1\n1\nKrump, Hip Hop,\nPop\nSM1*\n229.9\n2\nFreestyle\nSMVibe*\n40.2\n6\nFreestyle, Ballet\nSMJustDance\n188.3\n1\nJustDance\nSMDavid\n15.9\n1\nBreak, Krump, Hip\nHop, Pop\nSMKonata\n138.9\n1\nFreestyle\nSyrtos\n49.7\n6\nSyrtos\nTotal\n1240.2\n49\n-\nTable 1. Sources of dance data in our dataset. PMSD refers to the com-\nponents of the PMSD dance dataset, and SM refers to the components of the\nShaderMotion VR dance dataset. We mark with * the components which we\nused to train the models in this paper (we got the rest of the data recently,\nand are working on scaling up the models to the full dataset). Note that\nthere is one dancer in common between SMKonata and SMVibe. Note that\nwe classify GrooveNet and JustDance as their own styles as their dances\nmostly consist on repeating certain motifs. GrooveNet consists mostly of\nsimple rhythmic motifs, while JustDance has a larger diversity of motifs\nthat appear in the game JustDance.\ntraining (the ShaderMotion1 and ShaderMotionVibe components),\nbecause we only obtained the rest of the data recently. We plan to\nrelease models trained on the complete data soon. Some examples\nof how the VR dancing in our dataset looks, for street dance style,\ncan be seen in this URL https://www.youtube.com/playlist?list=\nPLmwqDOin_Zt4WCMWqoK6SdHlg0C_WeCP6\nSyrtos dataset. The Syrtos dataset consists of synchronized au-\ndio and motion capture recordings of a specific Greek dance style â€“\nthe Cretan Syrtos â€“ from six dancers. Eleven performances are con-\ntained in the data, with all but one dancer performing twice, giving a\ntotal duration of approximately 50 min. The data was recorded with\nthe dancers individually during ethnographic fieldwork in Crete\nin 2019 [Holzapfel et al. 2020], using a Xsens3 inertia system with\n17 sensors operating at 240 Hz, and post-processed through Xsens\nsoftware to a skeleton with 22 joints. The audio data â€“ performed\nlive by musicians during the recordings â€“ consists of single tracks\ncontaining the individual instruments. Note that we did not use the\nSyrtos data in this study, but release it for future research.\n4\nMETHOD\nWe introduce a new autoregressive probabilistic model with atten-\ntion which we call Transflower. The architecture combines ideas\nfor autoregressive cross-modal attention using transformers from\nLi et al. [2021a] with ideas for autoregressive probabilistic models\n3https://www.xsens.com/\nusing normalizing flows from Henter et al. [2020]. The model is\ndesigned to be applicable to any autoregressive probabilistic mod-\nelling with multiple modalities as inputs and outputs, although in\nthis paper we focus on the application to music-conditioned motion\ngeneration. The code and trained models will be made available.\nWe model dance-conditioned motion autoregressively. To do this,\nwe represent motion as a sequence of poses x = {ğ‘¥ğ‘–}ğ‘–=ğ‘\nğ‘–=1 âˆˆRğ‘Ã—ğ‘‘ğ‘¥\nsampled at ğ‘times ğ‘¡ğ‘–at a fixed sampling rate, and music as a\nsequence of audio features {ğ‘šğ‘–}ğ‘–=ğ‘\nğ‘–=1 âˆˆRğ‘Ã—ğ‘‘ğ‘šextracted from win-\ndows centered around the same times ğ‘¡ğ‘–. ğ‘‘ğ‘¥and ğ‘‘ğ‘šare the number\nof features for the pose and music samples. For our experiments we\nsample motion poses and music features at 20 Hz. The autoregress-\nive task is to predict the ğ‘–th pose given all previous poses, and some\nmusic context. For simplicity, we restrict the prediction to depend\nonly on the previous ğ‘˜ğ‘¥poses and the previous ğ‘˜ğ‘šand future ğ‘™ğ‘š\nmusic features. The probability distribution over the entire motion\ncan be written as a product, using the chain rule of probability:\nğ‘(x) =\nğ‘\nÃ–\nğ‘–=1\nğ‘(ğ‘¥ğ‘–|ğ‘¥ğ‘–âˆ’ğ‘˜ğ‘¥, ...,ğ‘¥ğ‘–âˆ’1;ğ‘šğ‘–âˆ’ğ‘˜ğ‘š, ...,ğ‘šğ‘–+ğ‘™ğ‘š)\n(1)\nwhere ğ‘¥orğ‘šwith indices smaller than 0 are either padded, or repres-\nent the â€œcontext seedâ€ (the initial ğ‘˜ğ‘¥poses and initial ğ‘˜ğ‘š+ğ‘™ğ‘šmusic\nfeatures) that is fed to the model. We experimented with different\nmotion seeds for the autoregressive generation in section 5.3.\nTransflower is composed of two components, a transformer en-\ncoder, to encode the motion and music context, and a normalizing\nflow head, that predicts a probability distribution over future poses,\ngiven the context. We expressğ‘(ğ‘¥ğ‘–|ğ‘¥ğ‘–âˆ’ğ‘˜ğ‘¥, ...,ğ‘¥ğ‘–âˆ’1;ğ‘šğ‘–âˆ’ğ‘˜ğ‘š, ...,ğ‘šğ‘–+ğ‘™ğ‘š) =\nğ‘(ğ‘¥ğ‘–|h) with a normalizing flow conditioned on a latent vector h.\nThis vector encodes the context via a transformer encoder h =\nğ‘“(ğ‘¥ğ‘–âˆ’ğ‘˜ğ‘¥, ...,ğ‘¥ğ‘–âˆ’1;ğ‘šğ‘–âˆ’ğ‘˜ğ‘š, ...,ğ‘šğ‘–+ğ‘™ğ‘š). For the encoder, we use the design\nproposed by Li et al. [2021a], with two transformers that encode\nthe motion part of the context hğ‘¥= ğ‘“ğ‘¥(ğ‘¥ğ‘–âˆ’ğ‘˜ğ‘¥, ...,ğ‘¥ğ‘–âˆ’1) âˆˆRğ‘˜ğ‘¥Ã—ğ‘‘ğ‘š\nand the music part hğ‘¥= ğ‘“ğ‘š(ğ‘šğ‘–âˆ’ğ‘˜ğ‘š, ...,ğ‘šğ‘–+ğ‘™ğ‘š) âˆˆR(ğ‘™ğ‘š+ğ‘˜ğ‘š)Ã—ğ‘‘ğ‘šsep-\narately, where the output dimension ğ‘‘ğ‘šis the same for both pose\nand music encoders. The outputs of these two transformers are then\nconcatenated into a cross-modal transformer Ëœh = ğ‘“ğ‘ğ‘š(hğ‘¥, hğ‘¥) âˆˆ\nR(ğ‘™ğ‘š+ğ‘˜ğ‘š+ğ‘˜ğ‘¥)Ã—ğ‘‘â„. The latent h âˆˆRğ¾Ã—ğ‘‘â„corresponds to a prefix of\nthis output Ëœh (the way ğ¾is chosen will be explained later). We use\nthe standard transformer encoder implementation in PyTorch, and\nuse T5-style relative positional embeddings [Raffel et al. 2019] in\nall transformers to obtain translation invariance across time [Wen-\nnberg and Henter 2021]. While Li et al. [2021a] use the outputs of\nthe cross-modal transformer as the deterministic prediction of the\nmodel, we interpret the first ğ¾outputs of the encoder as a latent\nvector h on which we condition the normalizing flow output head.\nWe use a normalizing flow model based on 1x1 invertible convo-\nlutions and affine coupling layers [Henter et al. 2020; Kingma and\nDhariwal 2018]. Like Ho et al. [2019], we use attention for the affine\ncoupling layers, but unlike them, we remove the convolutional lay-\ners, and use a pure-attention affine coupling layer. The inputs to the\nnormalizing flow correspond to ğ‘predicted poses of dimension ğ‘‘ğ‘¥.\nThe affine coupling layer splits the inputs ğ‘§ğ‘–âˆˆRğ‘Ã—ğ‘‘ğ‘¥channel-wise\ninto ğ‘§â€²\nğ‘–,ğ‘§â€²â€²\nğ‘–\nâˆˆRğ‘Ã—ğ‘‘ğ‘¥/2 and applies an affine transformation to ğ‘§â€²â€²\nğ‘–\nwith parameters depending on ğ‘§â€²\nğ‘–, i.e. ğ‘§ğ‘–+1 = A(ğ‘§â€²\nğ‘–, h) âŠ™ğ‘§â€²â€²\nğ‘–+B(ğ‘§â€²\nğ‘–, h).\nACM Trans. Graph., Vol. 40, No. 6, Article 1. Publication date: December 2021.\n",
    "1:6\nâ€¢\nGuillermo Valle-PÃ©rez, Gustav Eje Henter, Jonas Beskow, Andre Holzapfel, Pierre-Yves Oudeyer, and Simon Alexanderson\nFig. 2. The Transflower architecture. Green blocks represent neural network modules which take input from below and feed their output to the module\nabove. In the affine coupling layer, split and concatenation are done channel-wise, and the â€˜affine couplingâ€™ is an element wise linear scaling with shift and\nscale parameters determined by the output of the coupling transformer, as in Henter et al. [2020]; Kingma and Dhariwal [2018]. The normalizing flow is\ncomposed of several blocks, each of containing a batch normalization, an invertible 1x1 convolution, and an affine coupling layer. The motion, audio, and\ncross-modal transformers are standard full-attention transformer encoders, like in Li et al. [2021a], except that we use T5-style relative positional encodings.\nThese affine parameters are the output of the coupling transformer,\n(A, B) = ğ‘“ğ‘ğ‘¡( Ëœğ‘¥) âˆˆRğ‘Ã—2ğ‘‘ğ‘¥, where Ëœğ‘¥= (ğ‘§â€²\nğ‘–, h) âˆˆRğ‘Ã—(ğ‘‘ğ‘¥+ğ‘‘â„). Like in\nMoGlow [Henter et al. 2020], we concatenate the latent vector h\nto the inputs of ğ‘“ğ‘ğ‘¡along the channel dimension, to condition the\nnormalizing flow on the context. The main architectural difference\nis that MoGlow primarily relies on LSTMs for propagating inform-\nation over time, whereas the proposed model uses Transformers\nand attention mechanisms. We believe this should make it easier\nfor the model to discern and focus on specific features in a long\ncontext window, which we think may be important for learning\nchoreography, executing consistent dance moves, and also for being\nsensitive to the music. Kingma and Dhariwal [2018] used ActNorm\nlayers motivated by the inaccuracy of batch normalization when\nusing very small batch sizes (they used a batch size of 1). As we use\nlarger batch sizes, we found that batch normalization sometimes\nproduced moderately faster convergence, so we use it in our net-\nworks instead of ActNorm, unless specified otherwise. Our model\nuses 16 of these normalizing flow blocks.\nWe also found, like in Li et al. [2021a], that training to predict the\nnext ğ‘poses improves model performance. We therefore model the\nnormalizing flow output as a ğ‘Ã— ğ‘‘tensor (ğ‘‘being the dimension\nof the pose vector and ğ‘the sequence dimension). With this setup,\nthe transformers in the coupling layers A act along this sequence\ndimension, and the 1x1 convolutions act independently on each\nelement in the sequence. The transformer encoder latent â€œvectorâ€\nh is then interpreted as a ğ¾Ã— ğ‘‘â„tensor where ğ‘‘â„is the output\ndimension of the transformer encoder. By making ğ¾and ğ‘equal we\ncan concatenate h with the input to A along the channel dimension.\nWe show a diagram of the whole architecture in fig. 2.\nMotion features. We retarget all the motion data to the same\nskeleton with 21 joints (including the root) using Autodesk Motion\nBuilder. We represent the root (hip) motion as (Î”ğ‘¥, Î”ğ‘§,ğ‘¦,ğœƒ1,ğœƒ2,ğœƒ3, Î”ğ‘Ÿğ‘¦)\nwhere Î”ğ‘¥and Î”ğ‘§are the position changes relative to the rootâ€™s\nground-projected coordinate frame, i.e. the coordinate frame ob-\ntained by removing the roots rotation around the ğ‘¦(vertical) axis, so\nthat Î”ğ‘¥represents sideways movement and Î”ğ‘§represents forward\nmovement. ğ‘¦is the vertical position of the root in the base coordin-\nate system, that is the height from the floor. ğœƒis a exponential map\nrepresentation of 3D rotation of the root joint with respect to the\nrootâ€™s ground-projected frame and Î”ğ‘Ÿğ‘¦is the change of 2D facing\nangle. For all the joints we use exponential map parametrization\n[Grassia 1998] of the rotation, resulting in 3 features per (non-root)\njoint, and a total 67 motion features.\nAudio features. To represent the music, we combine spectro-\ngram features with beat-related features, by concatenating:\nâ€¢ 80 dimensional mel-frequency logarithmic magnitude spec-\ntrum with a hop size equal to 50 ms.\nâ€¢ One dimensional spectral flux onset strength envelope feature\nas provided by Librosa4.\nâ€¢ Two-dimensional output activations of the RNNDownBeat-\nProcessor model in the Madmom toolbox5.\nâ€¢ Two-dimensional beat features extracted from the two prin-\ncipal components of the last layer of the beat detection neural\nnetwork in stage 1 of DeepSaber [Labs 2019], which is the\nsame beat-detection architecture in Dance Dance Convolu-\ntion [Donahue et al. 2017], but trained on Beat Saber levels.\nAll the above features were used with settings to obtain the same\nframe rate as for the mel-frequency spectrum (20 Hz). We included\nboth Madmom and DeepSaber features because we observed in\npreliminary investigations that while the Madmom beat detector\nworked well for detecting the regular beats of a song, the DeepSaber\nfeatures sometimes worked better as general onset detectors.\nAfter processing the data into the above features, we individually\nstandardize them to have a mean of 0 and standard deviation of 1\nover the whole dataset used for training.\n4https://github.com/librosa/librosa\n5https://github.com/CPJKU/madmom\nACM Trans. Graph., Vol. 40, No. 6, Article 1. Publication date: December 2021.\n",
    "Transflower: probabilistic autoregressive dance generation with multimodal attention\nâ€¢\n1:7\nTraining. We train both Transflower and the MoGlow baseline\n[Henter et al. 2020] on 4 V100 Nvidia GPUs with a batch size of 84\nper GPU, for 600k steps, which took 7 days. We used a learning rate\nof 7 Ã— 10âˆ’5, which we decay by a factor of 0.1 after 200k iterations,\nand again after 400k iterations. The AI Choreographer baseline [Li\net al. 2021a] was trained for 600k iterations on a single TPUv3 with\na batch size of 128 (per TPU core) and a learning rate of 1 Ã— 10âˆ’4\ndecayed to 1 Ã— 10âˆ’5 and 1 Ã— 10âˆ’6 after 100k and 200k iterations\nrespectively. During training, we use â€œteacher forcingâ€, that is the\ninputs to the model come from the ground truth data, rather than\nautoregressively from model outputs. The architecture hyperpara-\nmeters for the different models are given in table 5 in appendix A,\nwhere we also explain how these hyperparameters were chosen.\nSynthesis. Transflower and MoGlow both run at over 20 Hz on\nan Nvidia V100 GPU, while the transformer from AI Choreographer\nruns at 100 Hz on an Nvidia V100.\nFine-tuning. We investigate the effect of fine-tuning Transflower\non the PMSD motion dataset, the portion of our data with highest\nquality motion tracking. We train the model for an extra 50k itera-\ntions only on this dataset. We found that training for longer reduced\nthe diversity, and 50k iterations produced a good trade-off between\ndiversity and improved quality of produced motions, as we find in\nsection 5. Note that the baselines were not fine-tuned in this manner,\nand should not be compared directly to this fine-tuned system.\nMotion â€œpromptingâ€. We also explored the role of the motion\nseed in autoregressive motion generation. We seed the different\nmodels with 5 different seeds (6 seconds long, chosen to be repres-\nentative of the different styles in our dataset), and compare how the\nseed affects the style of dancing. We find that the seed can indeed\nbe used to control the style of dancing, serving as a weak form of\nâ€œpromptingâ€ similar to the current trend in language models [Brown\net al. 2020; Reynolds and McDonell 2021]. We show more detailed\nresults in section 5.3.\n5\nEXPERIMENTS AND EVALUATION\nWe compare Transflower with the deterministic transformer model\nfrom AI Choreographer [Li et al. 2021a] and with the probabilistic\nmotion generation model MoGlow [Henter et al. 2020], which does\nnot use attention and has not been applied to dance motion syn-\nthesis before. In our experiments, we train all the models with the\nsame amount of past motion context, and past and future music\ncontext on the same dataset (the marked components in table 1).\nComparing MoGlow with Transflower, we can find out the effect of\nusing attention for learning the dependence on the context (Trans-\nflower), versus using a combination of recent frame concatenation\nand LSTMs (MoGlow). Comparing AI Choreographer with Trans-\nflower, we can discern the effect of having a probabilistic (Trans-\nflower) versus a deterministic (AI Choreographer) model.\n5.1\nObjective metrics\nWe look at two objective metrics that capture how realistic the\nmotion is, and how well it matches the music. For the evaluation of\nthe objective metrics we use a test set consisting of 33 songs and\nmotion seeds that were not found in the training set (i.e., excluding\nAIST++, which had song overlap with the training data), and 27\nModel\nFPD\nFMD\nAI Choreographer\n963.9\n2977.4\nMoGlow\n600.2\n1847.6\nTransflower (fine-tuned)\n549.0\n1711.9\nTransflower\n511.6\n1610.5\nTable 2. Realism metrics. FrÃ©chet pose distance (FPD) and FrÃ©chet move-\nment distance (FMD) for the three models we compare, as well as Trans-\nflower fine-tuned on the dance dataset.\nTFF\nTF\nMG\nAIC\nMatch\nMismatch\nMean\n0.25\n0.27\n0.34\n0.44\n0.20\n0.22\nSD\n0.33\n0.31\n0.52\n0.54\n0.24\n0.67\nTable 3. Beat alignment. Mean and standard deviations of the time offset\nbetween musical and kinematic beats(s).\nshorter songs (from AIST++) that were found on the training set\nbut with a different motion seed. Of the 33 non-AIST++ songs, 18\nwere ones randomly held out from the training set, while the other\n15 were added manually. We generated samples from each of the\nmodels, for 5 different motion seeds, and evaluated the metrics on\nthe full set of 300 generated sequences.\nRealism metric. For assessing realism, we use the FrÃ©chet dis-\ntance between the distribution of poses ğ‘ğ‘–and the distribution of\nconcatenations of three consecutive poses (ğ‘ğ‘–âˆ’1, ğ‘ğ‘–, ğ‘ğ‘–+1), which\ncaptures information about pose, joint velocity, and joint accelera-\ntion. We call these measures the FrÃ©chet pose distance (FPD) and\nthe FrÃ©chet movement distance (FMD), respectively. The measures\nwere computed on the â€œrawâ€ pose features, without mean and vari-\nance normalization. The results are shown in table 2. We can see\nthat AI Choreographer struggles to faithfully capture the variety\n(of both styles and tracking methods) in our dataset. We observe\nit often produces chaotic movement or freezes into a mean pose.\nMoGlow does better, while Transflower (both fine-tuned and non-\nfine-tuned) capture the distribution of real movements best (with\nthe non-fine-tuned slightly better, as expected).\nMusic-matching metric. In addition to the realism measure\nabove, we objectively assess rhythmical aspects of the music and\ndance based on metrics calculated from audio and motion beats.\nThese are relatively easy to track, but are by far are not the only\naspect that matters in good dancing. To extract the audio beats, we\nemployed the beat-tracking algorithms of BÃ¶ck and Schedl [2011];\nKrebs et al. [2015], included in the Madmom library. To detect dance\nbeats, we rely on a large body of studies on the relation between\nmusical meter and periodicity of dance movements [Burger et al.\n2013; Haugen 2014; Misgeld et al. 2019; Naveda and Leman 2011;\nToiviainen et al. 2010]. In specific, we draw on observations from\nToiviainen et al. [2010], showing that audio beat locations closely\nmatch the points of maximal downward velocity of the body (which\nis straightforward to measure at the hip), a finding consistent with\nobservations from Haugen [2014]; Misgeld et al. [2019] that the\ncenter of gravity of the body provides stable beat information. Hence,\nwe extracted the kinematic beat locations as the local minima of the\nğ‘¦-velocity of the Hips-joint. A visualization of the relation between\nACM Trans. Graph., Vol. 40, No. 6, Article 1. Publication date: December 2021.\n",
    "1:8\nâ€¢\nGuillermo Valle-PÃ©rez, Gustav Eje Henter, Jonas Beskow, Andre Holzapfel, Pierre-Yves Oudeyer, and Simon Alexanderson\n0:00\n0:50\n1:40\n2:30\n3:20\nTime\n16\n32\n64\n128\n256\nBPM\n(a) Music\n0:00\n0:50\n1:40\n2:30\n3:20\nTime\n16\n32\n64\n128\n256\nBPM\n(b) TFF\n0:00\n0:50\n1:40\n2:30\n3:20\nTime\n16\n32\n64\n128\n256\nBPM\n(c) TF\n0:00\n0:50\n1:40\n2:30\n3:20\nTime\n16\n32\n64\n128\n256\nBPM\n(d) MG\n0:00\n0:50\n1:40\n2:30\n3:20\nTime\n16\n32\n64\n128\n256\nBPM\n(e) AIC\nFig. 3. Music and kinematic tempograms for one complete dance. From left to right: Music, Transflower fine-tuned (TFF), Transflower (TF), MoGlow (MG) and\nAI Choreographer (AIC). The vertical axis corresponds to frequency, measured in beats per minute (bpm).\nthe tempo processes emerging from dance beats of the various\nsystems and the audio beats is shown in Figure 3, where we depict\naudio and visual tempograms [Davis and Agrawala 2018] for one\ncomplete dance. For the visual histograms, we used the local Hips\nvelocity at the kinematic beat locations as magnitude information.\nThe visual tempograms in Figure 3 show that the music has a very\nstrong and regular rhythmic structure, visible as horizontal bands.\nAmong the different models, it is apparent that the dance generated\nby Transflower (especially after fine-tuning) is characterized by the\nstrongest relation to the audio beat, with a pronounced periodicity at\nhalf the estimated audio beat (i.e., around 120 bpm). This periodicity\nbecomes more and more washed out for the other models as we move\nto the right through the subfigures, indicating less rhythmically\nconsistent motion. To quantify the alignment between the audio\nand kinematic beats, we calculate the absolute time offset between\neach music beat and its closest kinematic beat over the complete set\nof generated seeds and motions. The mean and standard deviation of\nthe offsets for the evaluated systems are reported in table 3 together\nwith the corresponding values for the complete (matched) training\ndata as well as randomly paired (mismatched) music and motion. It\nis apparent that the proposed model leads to an improved alignment\nas compared to the baseline systems.\n5.2\nUser study\nQualitative user studies are important to evaluate generative models,\nas perceived quality of different metrics tends to be the most relevant\nmetric for many downstream applications. We thus performed a\nuser study to evaluate our model and the baselines along three\naxis: naturalness of the motion, appropriateness to the music, and\ndiversity of the dance movements.\nTo perform the user study we used 17 songs that were not present\nin the training set. Among these, we include 10 songs obtained\nâ€œin-the-wildâ€ from YouTube, that may not necessarily match the\ngenres found in our datasets, and for which no ground truth dance\nis available. We use a fixed motion seed from the PMSDCasual\ndataset, as it consists of a generic T-pose. However, for the AI Cho-\nreographer baseline, this seed often caused the model to degenerate\ninto a frozen pose, probably because the PMSDCasual T-pose has\nparticularly high uncertainty over which motion will follow, making\nthe deterministic model more likely to regress into a mean pose. To\narrive to the 17 songs used for evaluation, we thus used a seed from\nShaderMotion for AI Choreographer to alleviate this problem, and\nremoved from the evaluation the remaining sequences where AI\nChoreographer still regressed to a mean pose.\nWe rendered 15-second animation clips with a stylized skinned\ncharacter for each of the 17 songs and the four models, yielding\na total of 68 stimuli. We performed three separate perception ex-\nperiments, detailed below. All experiments were carried out online\nand participants were recruited via a crowd-sourcing platform (Pro-\nlific). The presentation order of the stimuli was randomized between\nthe participants. In each of the experiments, 25 participants rated\neach video clip on a five point Likert-type scale. Participants were\nbetween 22 and 66 years of age (median 34), 39% male, 61% female.\nâ€¢ Naturalness: We asked participants to rate videos of dances\ngenerated by the different models according to the question\nOn a scale from 1 to 5: how natural is the dancing motion? I.e.\nto what extent the movements look like they could be carried\nout by a dancing human being? where 1 is very unnatural and\n5 is very natural. We removed the audio so that participants\nwould only be able to judge the naturalness of the motion.\nâ€¢ Appropriateness: We asked participants to rate videos of\ndances generated by the different models according to the\nquestion On a scale from 1 to 5: To what extent is the character\ndancing to the music? I.e. how well do the movements match\nthe audio? where 1 is not at all and 5 is very well.\nâ€¢ Within-dance diversity: Models can exhibit many types of\ndiversity. They can show diverse motions when changing the\nmotion seed, the song, or at different points within the same\nsong. Probabilistic models can furthermore generate different\nmotions even for the same seed and song. For this study, we\ndecided to focus on the diversity of motions within the same\nsong, for a fixed seed, as this scenario is representative of how\ndance generation models may be used in practice. We thus\nselected two disjoint pieces of generated dance from within\nthe same song, for each model, and presented the videos side\nby side. We asked participants to rate the generated motions\n(without audio) according to the question On a scale from 1\nto 5: How different are the two dances from each other? where\n1 is very similar and 5 is very different.\nResults. The fine-tuned Transflower model was rated highest\nboth in terms of naturalness and appropriateness, followed by the\nstandard Transflower model. Figure 4 shows the mean ratings for all\nfour models across the three experiments. A one-way ANOVA and\na post-hoc Tukey multiple comparison test was performed, in order\nACM Trans. Graph., Vol. 40, No. 6, Article 1. Publication date: December 2021.\n",
    "Transflower: probabilistic autoregressive dance generation with multimodal attention\nâ€¢\n1:9\nTFF\nTF\nMG\nAIC\n0\n1\n2\n3\n4\n5\nRating\nNaturalness\nTFF\nTF\nMG\nAIC\nAppropriateness\nTFF\nTF\nMG\nAIC\nWithin-dance diversity\nFig. 4. Results of user study, mean ratings with standard deviation bars. From left to right: naturalness, appropriateness and diversity, for the four models in\nthe study: Transflower fine-tuned (TFF), Transflower (TF), MoGlow (MG) and AI Choreographer (AIC). 95% confidence intervals for the mean ratings are not\nshown in the figure, but equate to Â±0.13 or less for all models in the three experiments, which is much narrower than the plotted standard deviations.\nto identify significant differences. For naturalness, all differences\nbetween models were significant (ğ‘< 0.001). For appropriateness,\nall differences except between MoGlow and AI Choreographer were\nsignificant (ğ‘< 0.001). For diversity, fine-tuned Transflower was\nrated less diverse than the other models (ğ‘< 0.001), and Transflower\nwas more diverse than MoGlow ğ‘= 0.02). However, we again em-\nphasize that only the non-fine-tuned Transflower model is directly\ncomparable to the two baselines, since the latter were not fine-tuned\non the higher-quality components of the data.\n5.3\nMotion prompting\nAutoregressive models require an initial context seed to initiate\nthe generation. We experimented with feeding the model different\nmotion seeds, and observed that the seed has a significant effect on\nthe style of the generated dance. To make this observation more\nquantitative, we measured the FMD between the distribution of\nmotions that Transflower produces when seeded with a motion seed\nof different styles (and over the different songs in the test set) and\nthe ground truth data for those styles. Results are shown in table 4,\nwhere we see that the seed changes the FMD distribution (and thus\nthe style of the dance), and tends to make the style closer to the style\nrepresented by the seed, in the sense that the smallest FMD is found\non the diagonal (matched seed and style) in three of five cases. This\nappears to be a weak version of the effect of â€œpromptingâ€ observed\nin language models [Brown et al. 2020; Reynolds and McDonell\n2021]. We conjecture that this prompting effect will allow more\ncontrolability of motion and dance generation models, as we make\nthe models more powerful, by increasing dataset and model sizes.\nWe also observe in table 4 how the FMD is a lot higher for some\nstyles than others. Freestyle (with data from SM1) seems like the\nmost challenging for the model to capture, seeing that the FMD is\nvery high for all seeds. This is expected as this dataset is both highly\ndiverse and has a lower motion-tracking quality. If we discount this\nstyle, the smallest FMD is on the diagonal in three out of four cases.\n6\nDISCUSSION\nIn this work, we have described the first model for dance synthesis\ncombining powerful probabilistic modelling based on normalizing\nflows, and an attention-based encoder for encoding the motion and\nMotion seed\nFS\nCA\nHH\nBR\nGN\nGround trutth\nFS\n5615.7\n5649.1\n5492.8\n5495.6\n6054.4\nCA\n352.3\n7.4\n192.9\n155.2\n242.9\nHH\n92.1\n712.0\n187.9\n238.4\n1619.4\nBR\n487.7\n109.7\n286.1\n238.0\n254.0\nGN\n1326.3\n340.1\n979.8\n881.2\n51.3\nTable 4. Effect of different motion seeds on dance style. We compare\nthe FMD between the distribution of motions for Transflower seeded with\na motion seed of different styles (and different songs), and the ground truth\ndata for those styles. FS: freestyle, CA: casual, HH: hip hop, BR: break dance,\nGN: GrooveNet.\nmusic context. We have shown that both of these properties are\nimportant, by comparing our model with two previously proposed\nmodels: MoGlow, a general motion synthesis model not previously\napplied to dance generation, which is based on autoregressive nor-\nmalizing flows and an LSTM context encoder [Henter et al. 2020],\nand AI Choregographer, a deterministic dance generation model\nbased on a cross-modal attention encoder [Li et al. 2021a].\nIn section 5, we found that Transflower matches the ground truth\ndistribution of poses and movements better than MoGlow and AI\nChoreographer (table 2), and is also ranked higher in naturalness and\nappropriateness to the music by human subjects (section 5.2). We\nobserve the same trend in the kinematic tempograms in fig. 3 which\ngive a more objective view on how well the motion matches the\nmusic. Comparing the two baselines, we further see that MoGlow\n(which is probabilistic, but lacks transformers) achieved a substan-\ntially better naturalness rating than the deterministic, transformer-\nbased AI Choreographer. We take this as evidence that a probabilistic\napproach was particularly important for good results in the present\nevaluation. In preliminary experiments, we found that AI Choreo-\ngrapher produced more natural motion when trained on the AIST++\ndata only than when trained on our full training set. In previous\nworks where AI Choreographer tended to reach relatively high\nscores in naturalness, the model was only trained on a single data\nsource at a time [Li et al. 2021b,a]. Taken together, this suggests\nthat the high diversity and heterogeneity of our dataset signific-\nantly degrades the performance of a deterministic model, while the\nACM Trans. Graph., Vol. 40, No. 6, Article 1. Publication date: December 2021.\n",
    "1:10\nâ€¢\nGuillermo Valle-PÃ©rez, Gustav Eje Henter, Jonas Beskow, Andre Holzapfel, Pierre-Yves Oudeyer, and Simon Alexanderson\nprobabilistic models are better able to adapt to the heterogeneity.\nThat said, random sampling from Transflower trained on on AIST++\nalone also exhibited improved motion quality. Unlike results from\nnatural language [Henighan et al. 2020], we thus did not see any\nevidence of favourable scaling behaviour when trading increased\ndataset size for greater data diversity and potentially reduced quality,\nexcept perhaps in the diversity of generated dances. However, part\nof this might be due to a difference in output-generation methods,\nwhere our strongest language models [Brown et al. 2020] bene-\nfit from sampling only among the most probable outcomes of the\nlearned distribution [Holtzman et al. 2020] (often called â€œreducing\nthe temperatureâ€), whereas our experiments sampled directly from\nthe distribution learned by the flow without temperature reduction.\nWe also evaluated the models in terms of the diversity of move-\nments found at different points in time for a single dance. In fig. 4\nwe see that although Transflower scored higher than MoGlow, the\ndifference with AI Choreographer was not significant. However,\nconsidering the low score of AI Choreographer for naturalness, the\nhigh diversity of the movements of AI Choreographer may not cor-\nrespond to meaningful diversity in terms of dance. We observed AI\nChoreographer tended to produce much more chaotic movements,\nand also was more likely to regress to a frozen pose (the latter stim-\nuli were not included in the user study). Therefore we argue that\nTransflower achieved more meaningful diversity than the baselines.\nTo explore the effect of fine-tuning the model on higher quality\ndata, we trained Transflower only on the PMSD dance data, for an\nextra 50k iterations. We found that resulted in significantly more\nnatural motion, as well as motion that was judged more appropriate\nto the music by the participants in our user study (fig. 4), compared\nto without fine-tuning. However, this improvement came at the\nexpense of reduced (within-dance) diversity in the dance, which\nprobably explains the increased FrÃ©chet distribution distances for\nthe fine-tuned model (table 2).\nIn section 5.3, we studied the effect of the autoregressive motion\nseed on the generated dance. We found that the seed had a big effect\non the style of the dance, with a tendency to make the dance more\nsimilar to the style from which the 6s motion seed was taken. We call\nthis effect â€œmotion promptingâ€ in analogy to the effect of prompting\nin language models [Reynolds and McDonell 2021]. Our results\nsuggest that this may be a new way to achieve stylistic control over\nautoregressive dance and motion models.\nIn order to drive research into better learning-based dance syn-\nthesis, that approach more human-like dance, an important piece is\nthe availability of large dance datasets. In this work, we introduce\nthe largest dataset of 3D dance movements obtained with motion\ncapture technologies. This can benefit not only learning-based ap-\nproaches, but data-driven dance synthesis in general (see section 2.2).\nWe think that the growing use of VR could offer novel opportun-\nities for research, like the one we explored here. Finally, we think\nthat investigating the most effective ways to scale models to bigger\ndatasets is an interesting direction for future work.\n7\nLIMITATIONS\nLearning-based methods for dance synthesis have certain limita-\ntions. As discussed above, they require large amounts of data, and\nmay produce less reliable and controllable results than approaches\nthat impose more structure. On the upside, they are more flexible\n(require fewer changes to apply to other tasks), and tend to produce\nmore natural results when enough data is available.\nMore specific to our model, normalizing flows show certain ad-\nvantages and limitations relative to other generative modelling meth-\nods [Bond-Taylor et al. 2021]. They allow exact likelihood maximiz-\nation, which leads to stable training, large diversity in samples, and\ngood quality results for large enough data and expressive enough\nmodels. However, they are less parameter efficient and slower to\ntrain than other approaches such as generative adversarial networks\nor variational autoencoders, at least for images [Bond-Taylor et al.\n2021]. Furthermore, the full-attention transformer encoder which\nwe use is slower to train than purely decoder-based models like\nGPT, due to being less parallelizable [Vaswani et al. 2017]. We think\nthat exploring architectures that overcome these limitations while\npreserving performance, is an important area for future work.\nFinally, we think that further work is needed in evaluation. This\nincludes comparing state-of-the-art learning-based methods like\nTransflower with current motion-graph-based methods like Choreo-\nMaster [Kang et al. 2021], in terms of naturalness, appropriateness,\nand diversity of the generated dance, as well as how these depend\non the data available. Models that include physics constraints, like\nthose discussed in section 2.1, also have shown promise in terms of\nnaturalness of the motion, and we think should be compared with\nless constrained models like Transflower in future work. We note\nthat the Transflower model we propose could be used to parametrize\nthe policy for physics-based IL algorithms, as it allows for exact\ncomputation of the log probability over the output. Even more so,\nhowever, it encompasses work towards understanding what are the\nbest metrics by which to evaluate dance synthesis models, in terms\nof reliability and relevance for downstream tasks. In particular, al-\nthough we looked at beat alignment as one of our objective metrics,\nthis is primarily due to a lack of other well-developed objective\nevaluation measures. Good dancing is not a beat-matching task, and\nprevious dance synthesis works have found that the quality of the\ndance is a big factor in determining how humans judge how well the\ndance matches the music. For example, in Li et al. [2021a], ground\ntruth dances paired with a random piece of music from the data set\nwas ranked significantly higher than any of the models, with regards\nto how well it matched the music. This may be linked to a previously\nobserved effect where people tend to find good dance moves with a\ndiversity of nested rhythms to match almost any music piece they\nare played with [Avirgan 2013; Miller et al. 2013]. This diversity\nof nested rhythms may also go some ways toward explaining the\nrelatively strong results of mismatched motion in table 3.\nThe phenomenon of rhythms at multiple levels extends to many\naspects of human and animal communication [Pouw et al. 2021].\nA similar effect as for mismatched dance has been observed in the\nrelated field of speech-driven gesture generation, where a recent\ninternational challenge [Kucherenko et al. 2021] found that mis-\nmatched ground-truth motion clips, unrelated to the speech, were\nrated as more appropriate for the speech than the best-rated syn-\nthetic system. For gestures, the disparity between matched and\nmismatched motion becomes more obvious to raters if the data con-\ntains periods of both speech and silence (during which no gesture\nACM Trans. Graph., Vol. 40, No. 6, Article 1. Publication date: December 2021.\n",
    "Transflower: probabilistic autoregressive dance generation with multimodal attention\nâ€¢\n1:11\nactivity is expected), as observed in Yoon et al. [2019]. Bringing\nthis back to dance, this might correspond to several songs with\nsilence in between, or music that exhibits extended dramatic pauses.\nMore broadly, these findings point to the quality of dance move-\nments being a more important factor for downstream applications,\nbut also suggest a need for further research on how to evaluate\ndance and dance synthesis and disentangling aspects of motion\nquality from rhythmic and stylistic appropriateness. For instance,\none could evaluate on songs with extensive pausing, to make the\ndifference between appropriate and less appropriate dancing more\npronounced. Our current models, never having seen data on silence\nor standing still during training, generate dance motion even for si-\nlent audio input, mirroring results in locomotion generation, where\nmany models find it difficult to stand still [Henter et al. 2020].\n8\nCONCLUSION\nIn conclusion, we introduced a new model for probabilistic autore-\ngressive modelling of high-dimensional continuous signals, condi-\ntioned on a multimodal context, which we applied to the problem\nof music-conditioned dance generation. We created the currently\nlargest 3D dance motion dataset, and used it to evaluate our model\nversus two representative baselines. Our results show that our model\nimproves on previous state of the art along several benchmarks, and\nthe two main features of our model, the probabilistic modelling of\nthe output, and the attention-based encoding of the inputs, are both\nnecessary to produce realistic and diverse dance that is appropriate\nfor the music.\nACKNOWLEDGMENTS\nWe thank the dancers: Mario Perez Amigo, Konata, Max and others.\nWe thank Esther Ericsson for dance and motion capture, and lox9973\nfor the VR dance recording. We thank the Syrtos dance collaborators\nStella Pashalidou, Michael Hagleitner, and Rainer and Ronja Polak.\nWe are also grateful to the reviewers for their thoughtful feedback.\nThis work benefited from access to the HPC resources of IDRIS\nunder the allocation 2020-[A0091011996] made by GENCI, using\nthe Jean Zay supercomputer. This research was partially supported\nby the Google TRC program, the Swedish Research Council pro-\njects 2018-05409 and 2019-03694, the Wallenberg AI, Autonomous\nSystems and Software Program (WASP) funded by the Knut and\nAlice Wallenberg Foundation, and Marianne and Marcus Wallen-\nberg Foundation MMW 2020.0102. Guillermo Valle-PÃ©rez benefited\nfrom funding of project â€œDeepCuriosityâ€ from RÃ©gion Aquitaine\nand Inria, France.\nREFERENCES\nOmid Alemi, Jules FranÃ§oise, and Philippe Pasquier. 2017. Groovenet: Real-time music-\ndriven dance movement generation using artificial neural networks. networks 8, 17\n(2017), 26.\nSimon Alexanderson, Gustav Eje Henter, Taras Kucherenko, and Jonas Beskow. 2020.\nStyle-controllable speech-driven gesture synthesis using normalising flows. Comput.\nGraph. Forum 39, 2 (2020), 487â€“496. https://doi.org/10.1111/cgf.13946\nOkan Arikan and David A. Forsyth. 2002. Interactive motion generation from examples.\nACM Trans. Graph. 21, 3 (2002), 483â€“490. https://doi.org/10.1145/566570.566606\nJody Avirgan. 2013. Why Spiderman is Such a Good Dancer. https://www.wnycstudios.\norg/podcasts/radiolab/articles/299399-why-spiderman-such-good-dancer\nBettina BlÃ¤sing, Beatriz Calvo-Merino, Emily S Cross, Corinne Jola, Juliane Honisch,\nand Catherine J Stevens. 2012. Neurocognitive control in dance perception and\nperformance. Acta psychologica 139, 2 (2012), 300â€“308.\nSebastian BÃ¶ck and Markus Schedl. 2011. Enhanced beat tracking with context-aware\nneural networks. In Proc. Int. Conf. Digital Audio Effects. 135â€“139.\nFederica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and\nMichael J. Black. 2016. Keep it SMPL: Automatic estimation of 3D human pose and\nshape from a single image. In European Conference on Computer Vision. Springer,\n561â€“578.\nSam Bond-Taylor, Adam Leach, Yang Long, and Chris G Willcocks. 2021. Deep Gen-\nerative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows,\nEnergy-Based and Autoregressive Models. arXiv preprint arXiv:2103.04922 (2021).\nAndrew Brock, Jeff Donahue, and Karen Simonyan. 2018. Large Scale GAN Training\nfor High Fidelity Natural Image Synthesis. In International Conference on Learning\nRepresentations.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\nSandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Re-\nwon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris\nHesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. 2020.\nLanguage Models are Few-Shot Learners. In Proc.\nNeurIPS, Vol. 33. 1877â€“1901.\nhttps://proceedings.neurips.cc/paper/2020/file/\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\nBirgitta Burger, Marc R Thompson, Geoff Luck, Suvi Saarikallio, and Petri Toiviainen.\n2013. Influences of rhythm-and timbre-related musical features on characteristics\nof music-induced movement. Frontiers in Psychology 4, Article 183 (2013), 10 pages.\nhttps://doi.org/10.3389/fpsyg.2013.00183\nJudith BÃ¼tepage, Michael J. Black, Danica Kragic, and Hedvig KjellstrÃ¶m. 2017. Deep\nrepresentation learning for human motion prediction and classification. In Pro-\nceedings of the IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPRâ€™17). IEEE Computer Society, Los Alamitos, CA, USA, 1591â€“1599.\nhttps://doi.org/10.1109/CVPR.2017.173\nShih-Pin Chao, Chih-Yi Chiu, Jui-Hsiang Chao, Shi-Nine Yang, and T-K Lin. 2004. Mo-\ntion retrieval and its application to motion synthesis. In 24th International Conference\non Distributed Computing Systems Workshops. IEEE, 254â€“259.\nCMU Graphics Lab. 2003. Carnegie Mellon University motion capture database. http:\n//mocap.cs.cmu.edu/\nLuka Crnkovic-Friis and Louise Crnkovic-Friis. 2016. Generative choreography using\ndeep learning. arXiv preprint arXiv:1605.06921 (2016).\nAbe Davis and Maneesh Agrawala. 2018. Visual rhythm and beat. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition Workshops. 2532â€“2535.\nPrafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and\nIlya Sutskever. 2020.\nJukebox: A generative model for music.\narXiv preprint\narXiv:2005.00341 (2020).\nChris Donahue, Zachary C Lipton, and Julian McAuley. 2017. Dance dance convolution.\nIn International conference on machine learning. PMLR, 1039â€“1048.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua\nZhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image\nrecognition at scale. arXiv preprint arXiv:2010.11929 (2020).\nRukun Fan, Songhua Xu, and Weidong Geng. 2011. Example-based automatic music-\ndriven conventional dance motion synthesis. IEEE transactions on visualization and\ncomputer graphics 18, 3 (2011), 501â€“515.\nYlva Ferstl and Rachel McDonnell. 2018. IVA: Investigating the use of recurrent motion\nmodelling for speech gesture generation. In IVA â€™18 Proceedings of the 18th Interna-\ntional Conference on Intelligent Virtual Agents. https://trinityspeechgesture.scss.tcd.\nie\nKaterina Fragkiadaki, Sergey Levine, Panna Felsen, and Jitendra Malik. 2015. Recurrent\nnetwork models for human dynamics. In Proceedings of the IEEE International Con-\nference on Computer Vision (ICCVâ€™15). IEEE Computer Society, Los Alamitos, CA,\nUSA, 4346â€“4354. https://doi.org/10.1109/ICCV.2015.494\nSatoru Fukayama and Masataka Goto. 2015. Music content driven automated choreo-\ngraphy with beat-wise motion connectivity constraints. Proceedings of SMC (2015),\n177â€“183.\nF. Sebastian Grassia. 1998. Practical parameterization of rotations using the exponential\nmap. J. Graph. Tools 3, 3 (1998), 29â€“48.\nhttps://doi.org/10.1080/10867651.1998.\n10487493\nKeith Grochow, Steven L. Martin, Aaron Hertzmann, and Zoran PopoviÄ‡. 2004. Style-\nbased inverse kinematics. ACM Trans. Graph. 23, 3 (2004), 522â€“531. https://doi.org/\n10.1145/1015706.1015755\nIkhansul Habibie, Daniel Holden, Jonathan Schwarz, Joe Yearsley, and Taku Komura.\n2017. A recurrent variational autoencoder for human motion synthesis. In Proceed-\nings of the British Machine Vision Conference (BMVCâ€™17). BMVA Press, Durham, UK,\nArticle 119, 12 pages. https://doi.org/10.5244/C.31.119\nIkhsanul Habibie, Weipeng Xu, Dushyant Mehta, Lingjie Liu, Hans-Peter Seidel, Gerard\nPons-Moll, Mohamed Elgharib, and Christian Theobalt. 2021. Learning Speech-\ndriven 3D Conversational Gestures from Video. arXiv preprint arXiv:2102.06837\n(2021).\nACM Trans. Graph., Vol. 40, No. 6, Article 1. Publication date: December 2021.\n",
    "1:12\nâ€¢\nGuillermo Valle-PÃ©rez, Gustav Eje Henter, Jonas Beskow, Andre Holzapfel, Pierre-Yves Oudeyer, and Simon Alexanderson\nMari Romarheim Haugen. 2014. Studying rhythmical structures in Norwegian folk\nmusic and dance using motion capture technology: A case study of Norwegian\ntelespringar. Musikk og Tradisjon 28 (2014), 27â€“52.\nTom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson,\nHeewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. 2020. Scaling laws\nfor autoregressive generative modeling. arXiv preprint arXiv:2010.14701 (2020).\nGustav Eje Henter, Simon Alexanderson, and Jonas Beskow. 2020. Moglow: Probabilistic\nand controllable motion synthesis using normalising flows. ACM Transactions on\nGraphics (TOG) 39, 6 (2020), 1â€“14.\nJonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. 2019. Flow++:\nImproving flow-based generative models with variational dequantization and archi-\ntecture design. In International Conference on Machine Learning. PMLR, 2722â€“2730.\nDaniel Holden, Oussama Kanoun, Maksym Perepichka, and Tiberiu Popa. 2020. Learned\nmotion matching. ACM Trans. Graph. 39, 4 (2020), 53â€“1.\nDaniel Holden, Taku Komura, and Jun Saito. 2017. Phase-functioned neural networks\nfor character control. ACM Trans. Graph. 36, 4, Article 42 (2017), 13 pages. https:\n//doi.org/10.1145/3072959.3073663\nDaniel Holden, Jun Saito, and Taku Komura. 2016. A deep learning framework for\ncharacter motion synthesis and editing. ACM Trans. Graph. 35, 4, Article 138 (2016),\n11 pages. https://doi.org/10.1145/2897824.2925975\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The Curious Case\nof Neural Text Degeneration. In International Conference on Learning Representations.\nAndre Holzapfel, Michael Hagleitner, and Stella Pashalidou. 2020. Diversity of Tradi-\ntional Dance Expression in Crete: Data Collection, Research Questions, and Method\nDevelopment. In Proceedings of the ICTM Study Group on Sound, Movement, and the\nSciences Symposium.\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis\nHawthorne, Noam Shazeer, Andrew M Dai, Matthew D Hoffman, Monica Dinculescu,\nand Douglas Eck. 2018. Music Transformer: Generating Music with Long-Term\nStructure. In International Conference on Learning Representations.\nChen Kang, Zhipeng Tan, Jin Lei, Song-Hai Zhang, Yuan-Chen Guo, Weidong Zhang,\nand Shi-Min Hu. 2021. ChoreoMaster: Choreography-oriented Music-driven Dance\nSynthesis. (2021). https://www.youtube.com/watch?v=V8MlYa_yhF0 accepted for\npublication at SIGGRAPH 2021.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon\nChild, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws\nfor neural language models. arXiv preprint arXiv:2001.08361 (2020).\nTero Karras, Samuli Laine, and Timo Aila. 2019. A style-based generator architecture\nfor generative adversarial networks. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. 4401â€“4410.\nDiederik P. Kingma and Prafulla Dhariwal. 2018. Glow: Generative flow with invertible\n1x1 convolutions. In Advances in Neural Information Processing Systems (NeurIPSâ€™18).\nCurran Associates, Inc., Red Hook, NY, USA, 10236â€“10245. http://papers.nips.cc/\npaper/8224-glow-generative-flow-with-invertible-1x1-con\nLucas Kovar and Michael Gleicher. 2004. Automated extraction and parameterization\nof motions in large data sets. ACM Trans. Graph. 23, 3 (2004), 559â€“568.\nhttps:\n//doi.org/10.1145/1015706.1015760\nLucas Kovar, Michael Gleicher, and FrÃ©dÃ©ric Pighin. 2002. Motion graphs. ACM Trans.\nGraph. 21, 3 (2002), 473â€“482. https://doi.org/10.1145/566654.566605\nFlorian Krebs, Sebastian BÃ¶ck, and Gerhard Widmer. 2015. An Efficient State-Space\nModel for Joint Tempo and Meter Tracking.. In ISMIR. 72â€“78.\nTaras Kucherenko, Patrik Jonell, Youngwoo Yoon, Pieter Wolfert, and Gustav Eje\nHenter. 2021. A Large, Crowdsourced Evaluation of Gesture Generation Systems\non Common Data: The GENEA Challenge 2020. In 26th International Conference on\nIntelligent User Interfaces (College Station, TX, USA) (IUI â€™21). ACM, New York, NY,\nUSA, 11â€“21. https://doi.org/10.1145/3397481.3450692\nOxAI Labs. 2019. DeepSaber. https://github.com/oxai/deepsaber/.\nKimerer LaMothe. 2019. The dancing species: how moving together in time helps\nmake us human. Aeon (June 2019). https://aeon.co/ideas/the-dancing-species-how-\nmoving-together-in-time-helps-make-us-human\nBen Lang. 2021. The Future is Now: Live Breakdance Battles in VR Are Connecting\nPeople Across the Globe.\nhttps://www.roadtovr.com/vr-dance-battle-vrchat-\nbreakdance/\nGilwoo Lee, Zhiwei Deng, Shugao Ma, Takaaki Shiratori, Siddhartha S Srinivasa, and\nYaser Sheikh. 2019a. Talking with hands 16.2 m: A large-scale dataset of synchronized\nbody-finger motion and audio for conversational motion analysis and synthesis. In\nProceedings of the IEEE/CVF International Conference on Computer Vision. 763â€“772.\nHsin-Ying Lee, Xiaodong Yang, Ming-Yu Liu, Ting-Chun Wang, Yu-Ding Lu, Ming-\nHsuan Yang, and Jan Kautz. 2019b. Dancing to music. arXiv preprint arXiv:1911.02001\n(2019).\nJehee Lee, Jinxiang Chai, Paul S. A. Reitsma, Jessica K. Hodgins, and Nancy S. Pollard.\n2002. Interactive control of avatars animated with human motion data. ACM Trans.\nGraph. 21, 3 (2002), 491â€“500. https://doi.org/10.1145/566654.566607\nSergey Levine, Jack M. Wang, Alexis Haraux, Zoran PopoviÄ‡, and Vladlen Koltun. 2012.\nContinuous character control with low-dimensional embeddings. ACM Trans. Graph.\n31, 4, Article 28 (2012), 10 pages. https://doi.org/10.1145/2185520.2185524\nBuyu Li, Yongchi Zhao, and Lu Sheng. 2021b. DanceNet3D: Music Based Dance Gener-\nation with Parametric Motion Transformer. arXiv preprint arXiv:2103.10206 (2021).\nJiaman Li, Yihang Yin, Hang Chu, Yi Zhou, Tingwu Wang, Sanja Fidler, and Hao Li.\n2020. Learning to Generate Diverse Dance Motions with Transformer. arXiv preprint\narXiv:2008.08171 (2020).\nRuilong Li, Shan Yang, David A Ross, and Angjoo Kanazawa. 2021a. Learn to Dance with\nAIST++: Music Conditioned 3D Dance Generation. arXiv preprint arXiv:2101.08779\n(2021).\nHung Yu Ling, Fabio Zinno, George Cheng, and Michiel van de Panne. 2020. Character\ncontrollers using motion VAEs. ACM Trans. Graph. 39, 4, Article 40 (2020), 12 pages.\nhttps://doi.org/10.1145/3386569.3392422\nlox9973. 2021. ShaderMotion. https://gitlab.com/lox9973/ShaderMotion.\nNaureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll, and Michael J.\nBlack. 2019. AMASS: Archive of Motion Capture as Surface Shapes. In International\nConference on Computer Vision. 5442â€“5451.\nChristian Mandery, Ã–mer Terlemez, Martin Do, Nikolaus Vahrenkamp, and Tamim\nAsfour. 2015. The KIT whole-body human motion database. In 2015 International\nConference on Advanced Robotics (ICAR). IEEE, 329â€“336.\nAlexander Mathis, Steffen Schneider, Jessy Lauer, and Mackenzie Weygandt Mathis.\n2020. A primer on motion capture with deep learning: principles, pitfalls, and\nperspectives. Neuron 108, 1 (2020), 44â€“65.\nJosh Merel, Yuval Tassa, Sriram Srinivasan, Jay Lemmon, Ziyu Wang, Greg Wayne, and\nNicolas Heess. 2017. Learning human behaviors from motion capture by adversarial\nimitation. arXiv preprint arXiv:1707.02201 (2017).\nJared E Miller, Laura A Carlson, and J Devin McAuley. 2013. When what you hear\ninfluences when you see: listening to an auditory rhythm influences the temporal\nallocation of visual attention. Psychological science 24, 1 (2013), 11â€“18.\nOlof Misgeld, Andre Holzapfel, and Sven AhlbÃ¤ck. 2019. Dancing Dots â€“ Investigating\nthe Link between Dancer and Musician in Swedish Folk Dance. In Sound & Music\nComputing Conference.\nLuiz Naveda and Marc Leman. 2011. Hypotheses on the choreographic roots of the\nmusical meter: a case study on Afro-Brazilian dance and music. Debates actuales en\nevoluciÃ³n, desarrollo y cogniciÃ³n e implicancias socio-culturales (2011), 477â€“495.\nGeorge Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and\nBalaji Lakshminarayanan. 2021. Normalizing Flows for Probabilistic Modeling\nand Inference. Journal of Machine Learning Research 22, 57 (2021), 1â€“64.\nhttp:\n//jmlr.org/papers/v22/19-1028.html\nTaesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. 2019. Semantic image\nsynthesis with spatially-adaptive normalization. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 2337â€“2346.\nDario Pavllo, David Grangier, and Michael Auli. 2018. QuaterNet: A quaternion-based\nrecurrent model for human motion. In Proceedings of the British Machine Vision\nConference (BMVCâ€™18). BMVA Press, Durham, UK, 14 pages. http://www.bmva.org/\nbmvc/2018/contents/papers/0675.pdf\nXue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. 2018a. Deep-\nMimic: Example-guided deep reinforcement learning of physics-based character\nskills. ACM Trans. Graph. 37, 4 (2018), 1â€“14.\nXue Bin Peng, Angjoo Kanazawa, Jitendra Malik, Pieter Abbeel, and Sergey Levine.\n2018b. Sfv: Reinforcement learning of physical skills from videos. ACM Transactions\nOn Graphics (TOG) 37, 6 (2018), 1â€“14.\nXue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa. 2021. AMP:\nAdversarial Motion Priors for Stylized Physics-Based Character Control. ACM Trans.\nGraph. 40, 4, Article 1 (July 2021), 15 pages. https://doi.org/10.1145/3450626.3459670\nMathis Petrovich, Michael J Black, and GÃ¼l Varol. 2021. Action-Conditioned 3D Human\nMotion Synthesis with Transformer VAE. arXiv preprint arXiv:2104.05670 (2021).\nWim Pouw, Shannon Proksch, Linda Drijvers, Marco Gamba, Judith Holler, Christopher\nKello, Rebecca S. Schaefer, and Geraint A. Wiggins. 2021. Multilevel rhythms in\nmultimodal communication. Philosophical Transactions of the Royal Society B 376,\n1835 (2021), 20200334. https://doi.org/10.1098/rstb.2020.0334\nRyan Prenger, Rafael Valle, and Bryan Catanzaro. 2019. WaveGlow: A flow-based\ngenerative network for speech synthesis. In Proceedings of the IEEE International\nConference on Acoustics, Speech, and Signal Processing (ICASSPâ€™19). IEEE Signal\nProcessing Society, Piscataway, NJ, USA, 3617â€“3621. https://doi.org/10.1109/ICASSP.\n2019.8683143\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer\nlearning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683\n(2019).\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford,\nMark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation. arXiv\npreprint arXiv:2102.12092 (2021).\nLaria Reynolds and Kyle McDonell. 2021. Prompt programming for large language\nmodels: Beyond the few-shot paradigm. arXiv preprint arXiv:2102.07350 (2021).\nYu Rong, Takaaki Shiratori, and Hanbyul Joo. 2021. FrankMocap: A Monocular 3D\nWhole-Body Pose Estimation System via Regression and Integration. In IEEE Inter-\nnational Conference on Computer Vision Workshops.\nACM Trans. Graph., Vol. 40, No. 6, Article 1. Publication date: December 2021.\n",
    "Transflower: probabilistic autoregressive dance generation with multimodal attention\nâ€¢\n1:13\nAlla Safonova and Jessica K. Hodgins. 2007. Construction and Optimal Search of\nInterpolated Motion Graphs. ACM Trans. Graph. 26, 3 (July 2007), 106â€“es. https:\n//doi.org/10.1145/1276377.1276510\nJonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng\nYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, Yannis\nAgiomyrgiannakis, and Yonghui Wu. 2018. Natural TTS synthesis by conditioning\nWaveNet on mel spectrogram predictions. In Proceedings of the IEEE International\nConference on Acoustics, Speech, and Signal Processing (ICASSPâ€™18). IEEE Signal\nProcessing Society, Piscataway, NJ, USA, 4799â€“4783. https://doi.org/10.1109/ICASSP.\n2018.8461368\nSebastian Starke, Yiwei Zhao, Taku Komura, and Kazi Zaman. 2020. Local motion\nphases for learning multi-contact character movements. ACM Trans. Graph. 39, 4,\nArticle 54 (2020), 14 pages. https://doi.org/10.1145/3386569.3392450\nStatista. 2020. Augmented reality (AR) and virtual reality (VR) headset shipments world-\nwide from 2020 to 2025.\nhttps://www.statista.com/statistics/653390/worldwide-\nvirtual-and-augmented-reality-headset-shipments/\nWataru Takano, Katsu Yamane, and Yoshihiro Nakamura. 2010. Retrieval and Generation\nof Human Motions Based on Associative Model between Motion Symbols and\nMotion Labels. Proceedings of Journal of the Robotics Society of Japan 28, 6 (2010),\n723â€“734.\nTaoran Tang, Jia Jia, and Hanyang Mao. 2018.\nDance with melody: An LSTM-\nautoencoder approach to music-oriented dance synthesis. In Proceedings of the\n26th ACM International Conference on Multimedia. 1598â€“1606.\nPetri Toiviainen, Geoff Luck, and Marc R Thompson. 2010. Embodied meter: hierarchical\neigenmodes in music-induced movement. Music Perception 28, 1 (2010), 59â€“70.\nNikolaus F Troje. 2002. Decomposing biological motion: A framework for analysis and\nsynthesis of human gait patterns. Journal of vision 2, 5 (2002), 2â€“2.\nShuhei Tsuchida, Satoru Fukayama, Masahiro Hamasaki, and Masataka Goto. 2019.\nAIST Dance Video Database: Multi-genre, Multi-dancer, and Multi-camera Database\nfor Dance Information Processing. In Proceedings of the 20th International Society\nfor Music Information Retrieval Conference, ISMIR 2019. Delft, Netherlands, 501â€“510.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\nGomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In\nAdvances in Neural Information Processing Systems (NIPSâ€™17). Curran Associates,\nInc., Red Hook, NY, USA, 5998â€“6008. https://papers.nips.cc/paper/7181-attention-\nis-all-you-need\nJack M. Wang, David J. Fleet, and Aaron Hertzmann. 2008. Gaussian process dynamical\nmodels for human motion. IEEE T. Pattern Anal. 30, 2 (2008), 283â€“298.\nhttps:\n//doi.org/10.1109/TPAMI.2007.1167\nUlme Wennberg and Gustav Eje Henter. 2021. The Case for Translation-Invariant Self-\nAttention in Transformer-Based Language Models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing (Volume 2: Short Papers) (ACL â€™21).\nACL, 130â€“140. https://doi.org/10.18653/v1/2021.acl-short.18\nChenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro,\nand Nan Duan. 2021. GODIVA: Generating Open-DomaIn Videos from nAtural\nDescriptions. arXiv preprint arXiv:2104.14806 (2021).\nZijie Ye, Haozhe Wu, Jia Jia, Yaohua Bu, Wei Chen, Fanbo Meng, and Yanfeng Wang.\n2020. ChoreoNet: Towards Music to Dance Synthesis with Choreographic Action\nUnit. In Proceedings of the 28th ACM International Conference on Multimedia. 744â€“\n752.\nYoungwoo Yoon, Woo-Ri Ko, Minsu Jang, Jaeyeon Lee, Jaehong Kim, and Geehyuk Lee.\n2019. Robots learn social skills: End-to-end learning of co-speech gesture generation\nfor humanoid robots. In Proceedings of the IEEE International Conference on Robotics\nand Automation (ICRAâ€™19). IEEE Robotics and Automation Society, Piscataway, NJ,\nUSA, 4303â€“4309. https://doi.org/10.1109/ICRA.2019.8793720\nYi Zhou, Zimo Li, Shuangjiu Xiao, Chong He, Zeng Huang, and Hao Li. 2018. Auto-\nconditioned recurrent networks for extended complex human motion synthesis. In\nProceedings of the International Conference on Learning Representations (ICLRâ€™18).\n13 pages. https://openreview.net/forum?id=r11Q2SlRW\nWenlin Zhuang, Congyi Wang, Siyu Xia, Jinxiang Chai, and Yangang Wang. 2020.\nMusic2Dance: DanceNet for Music-driven Dance Generation. arXiv e-prints (2020),\narXivâ€“2002.\nA\nMODEL DETAILS\nWe give the main architecture hyperparameter details in table 5.\nThe hyperparameters for the AI Choreographer architecture were\nchosen to match those in the original AI Choreographer [Li et al.\n2021a], with the only difference that we used T5-style relative posi-\ntional embeddings, and that we used positional embeddings in the\ncross-modal transformer, as we found these gave slightly better con-\nvergence. The hyperparameters for the transformer encoders and\ncross-modal transformer in Transflower are identical to those of AI\nChoreographer, while the normalizing flow parameters are the same\nas in MoGlow, except for the affine coupling layers for which we\nuse 2-layer transformers. The hyperparameters for MoGlow were\nchosen to be similar to the original implementation in Henter et al.\n[2020], except that we increased the concatenated context length\nfed at each time step, from 10 frames in the original MoGlow, to\n40 frames of motion and 50 frames of music (40 in the past, 10 in\nthe future). This makes the concatenated input to the LSTM have\na dimension of 85 Ã— 50 + 67 Ã— 40 = 6930 which accounts for the\nsignificant parameter increase of the model. We did this change\nbecause we found that an increased concatenated context length\nfor MoGlow was necessary for it to produce good results.\nModel\n#Params\nğ¿ğ‘šğ‘œ\nğ¿ğ‘šğ‘¢\nğ¿ğ‘ğ‘š\nğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™\nğ¾\nğ¿ğ‘ğ‘\nğ¿ğ‘™ğ‘ ğ‘¡ğ‘š\nğ‘˜ğ‘¥\nğ‘˜ğ‘š\nğ‘™ğ‘š\nAI Choreographer\n64M\n2\n2\n12\n800\nN/A\nN/A\nN/A\n120\n120\n20\nMoGlow\n281M\nN/A\nN/A\nN/A\nN/A\n16\nN/A\n2\n120\n120\n20\nTransflower\n122M\n2\n2\n12\n800\n16\n2\nN/A\n120\n120\n20\nTable 5. Basic architecture hyperparameters for the different models. ğ¿ğ‘šğ‘œ, ğ¿ğ‘šğ‘¢, ğ¿ğ‘ğ‘š, ğ¿ğ‘ğ‘, ğ¿ğ‘™ğ‘ ğ‘¡ğ‘šare the number of layers in the motion encoder\ntransformer, the music encoder transformer, the cross-modal transformer, the affine coupling layer, and the LSTM respectively. ğ¾is the number of blocks in\nthe normalizing flow, and ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™is the latent dimension of the encoder transformers.\nACM Trans. Graph., Vol. 40, No. 6, Article 1. Publication date: December 2021.\n"
  ],
  "full_text": "Transflower: probabilistic autoregressive dance generation with\nmultimodal attention\nGUILLERMO VALLE-PÃ‰REZ, Inria, Ensta ParisTech, University of Bordeaux, France\nGUSTAV EJE HENTER, KTH Royal Institute of Technology, Sweden\nJONAS BESKOW, KTH Royal Institute of Technology, Sweden\nANDRE HOLZAPFEL, KTH Royal Institute of Technology, Sweden\nPIERRE-YVES OUDEYER, Inria, Ensta ParisTech, University of Bordeaux, France\nSIMON ALEXANDERSON, KTH Royal Institute of Technology, Sweden\nFig. 1. We have aggregated the largest dataset of 3D dance motion, and used it to train Transflower, a new probabilistic autoregressive model of motion. As a\nresult we obtained a model that can generate dance to any piece of music, which ranks well in terms of appropriateness, naturalness, and diversity.\nDance requires skillful composition of complex movements that follow\nrhythmic, tonal and timbral features of music. Formally, generating dance\nconditioned on a piece of music can be expressed as a problem of modelling\na high-dimensional continuous motion signal, conditioned on an audio\nsignal. In this work we make two contributions to tackle this problem. First,\nwe present a novel probabilistic autoregressive architecture that models\nthe distribution over future poses with a normalizing flow conditioned on\nprevious poses as well as music context, using a multimodal transformer\nencoder. Second, we introduce the currently largest 3D dance-motion dataset,\nobtained with a variety of motion-capture technologies, and including both\nprofessional and casual dancers. Using this dataset, we compare our new\nmodel against two baselines, via objective metrics and a user study, and\nshow that both the ability to model a probability distribution, as well as\nbeing able to attend over a large motion and music context are necessary to\nproduce interesting, diverse, and realistic dance that matches the music.\nAuthorsâ€™ addresses: Guillermo Valle-PÃ©rez, guillermo-jorge.valle-perez@inria.fr, In-\nria, Ensta ParisTech, University of Bordeaux, Bordeaux, France; Gustav Eje Henter,\nghe@kth.se, KTH Royal Institute of Technology, Stockholm, Sweden; Jonas Beskow,\nbeskow@kth.se, KTH Royal Institute of Technology, Stockholm, Sweden; Andre Holzap-\nfel, holzap@kth.se, KTH Royal Institute of Technology, Stockholm, Sweden; Pierre-Yves\nOudeyer, pierre-yves.oudeyer@inria.fr, Inria, Ensta ParisTech, University of Bordeaux,\nBordeaux, France; Simon Alexanderson, simonal@kth.se, KTH Royal Institute of Tech-\nnology, Stockholm, Sweden.\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nÂ© 2021 Copyright held by the owner/author(s).\n0730-0301/2021/12-ART1\nhttps://doi.org/10.1145/3478513.3480570\nCCS Concepts: â€¢ Computing methodologies â†’Animation; Neural net-\nworks; Motion capture.\nAdditional Key Words and Phrases: Generative models, machine learning,\nnormalising flows, Glow, transformers, dance\nACM Reference Format:\nGuillermo Valle-PÃ©rez, Gustav Eje Henter, Jonas Beskow, Andre Holzapfel,\nPierre-Yves Oudeyer, and Simon Alexanderson. 2021. Transflower: probabil-\nistic autoregressive dance generation with multimodal attention. ACM Trans.\nGraph. 40, 6, Article 1 (December 2021), 13 pages. https://doi.org/10.1145/\n3478513.3480570\n1\nINTRODUCTION\nDancing â€“ body motions performed together with music â€“ is a\ndeeply human activity that transcends cultural barriers, and we\nhave been called â€œthe dancing speciesâ€ [LaMothe 2019]. Today, con-\ntent involving dance is some of the most watched on digital video\nplatforms such as YouTube and TikTok. The recent pandemic led\ndance â€“ as other performing arts â€“ to become an increasingly virtual\npractice, and hence an increasingly digitized cultural expression.\nHowever, good dancing, whether analog or digital, is challenging\nto create. Professional dancing requires physical prowess and ex-\ntensive practise, and capturing or recreating a similar experience\nthrough digital means is labour intensive, whether done through\nmotion capture or hand animation. Consequently, the problem of\nautomatic, data-driven dance generation has gathered interest in\nrecent years [Li et al. 2021b, 2020, 2021a; Zhuang et al. 2020]. Access\nto generative models of dance could help creators and animators, by\nspeeding up their workflow, by offering inspiration, and by opening\nACM Trans. Graph., Vol. 40, No. 6, Article 1. Publication date: December 2021.\narXiv:2106.13871v3  [cs.SD]  11 Jun 2022\n\n\n1:2\nâ€¢\nGuillermo Valle-PÃ©rez, Gustav Eje Henter, Jonas Beskow, Andre Holzapfel, Pierre-Yves Oudeyer, and Simon Alexanderson\nup novel possibilities such as creating interactive characters that\nreact to the userâ€™s choice of music in real time. The same models\ncan also give insight into how humans connect music and move-\nment, both of which have been identified as capturing important\nand inter-related aspects of our cognition [BlÃ¤sing et al. 2012].\nThe kinematic processes embodied in dance are highly complex\nand nonlinear, even when compared to other human movement\nsuch as locomotion. Dance is furthermore multimodal, and the con-\nnection between music and dance motion is extremely multifaceted\nand far from deterministic. Generative modelling with deep neural\nnetworks is becoming one of the most promising approaches to\nlearn representations of such complex domains. This general ap-\nproach has already made significant progress in the domains of\nimages [Brock et al. 2018; Karras et al. 2019; Park et al. 2019], music\n[Dhariwal et al. 2020; Huang et al. 2018], motion [Henter et al. 2020;\nLing et al. 2020], speech [Prenger et al. 2019; Shen et al. 2018], and\nnatural language [Brown et al. 2020; Raffel et al. 2019]. Recently, mul-\ntimodal models are being developed, that learn to capture the even\nmore complex interactions between standard data domains, such\nas between language and images [Ramesh et al. 2021], or between\nlanguage and video [Wu et al. 2021]. Similarly, dance synthesis sits\nat the intersection between movement modelling and music un-\nderstanding, and is an exciting problem that combines compelling\nmachine-learning challenges with a distinct sociocultural impact.\nIn this work, we tackle the problem of music-conditioned 3D\ndance motion generation through deep learning. In particular, we\nexplore two important factors that affect model performance on this\ndifficult task: 1) the ability to capture patterns that are extended over\nlonger periods of time, and 2) the ability to express complex probab-\nility distributions over the predicted outputs. We argue that previous\nworks are lacking in one of these two properties, and present a new\nautoregressive neural architecture which combines a transformer\n[Vaswani et al. 2017] to encode the multimodal context (previous\nmotion, and both previous and future music), and a normalizing flow\n[Papamakarios et al. 2021] head to faithfully model the future distri-\nbution over the predicted modality, which for dance synthesis is the\nfuture motion. We call this new architecture Transflower and show,\nthrough objective metrics and human evaluation studies, that both\nof these factors are important to model the complex distribution\nof movements in dance as well as their dependence on the music\nmodality. Human evaluations are the gold standard to evaluate the\nperceptual quality of generative models, and are complementary to\nthe objective metrics. Furthermore, they allow us to evaluate the\nmodel on arbitrary â€œin-the-wildâ€ songs downloaded from YouTube,\nfor which no ground truth dance motion is available.\nOne of the biggest challenges in learning-based motion synthesis\nis the availability of large-scale datasets for 3D movement. Existing\ndatasets are mainly gathered in two ways: in a motion capture\nstudio [CMU Graphics Lab 2003; Ferstl and McDonnell 2018; Lee\net al. 2019a; Mahmood et al. 2019; Mandery et al. 2015; Troje 2002],\nwhich provides the highest quality motion, but requires expensive\nequipment and is difficult to scale to larger dataset sizes, or via\nmonocular 3D pose estimation from video [Habibie et al. 2021; Peng\net al. 2018b], which trades off quality for a much larger availability\nof videos from the Internet.\nIn this paper we present the largest dataset of 3D dance motion,\ncombining different sources and motion capture technologies. We\nintroduce a new approach to obtain large-scale motion datasets,\ncomplementary to the two mostly used in previous works. Specific-\nally, we make use of the growing popularity and user base of virtual\nreality (VR) technologies, and of VR dancing in particular [Lang\n2021], to find participants interested in contributing dance data for\nour study. We argue that, while consumer-grade VR motion capture\ndoes not produce as high quality as professional motion capture, it\nis significantly better and more robust than current monocular 3D\npose estimation from video. Furthermore, it is poised to improve\nboth in quality and availability as the VR market grows [Statista\n2020], offering potential new avenues for participatory research.\nWe also collect the largest dataset of dance motion using profes-\nsional motion capture equipment, from both casual dancers and a\nprofessional dancer. Finally, to train our models, we combine our\nnew data, with two existing 3D dance motion datasets, GrooveNet\n[Alemi et al. 2017] and AIST++ [Li et al. 2021a], which we standard-\nize to a common skeleton. In total, we have over 20 h of dance data\nin a wide variety of dance styles, including freestyle, casual dancing,\nand street dance styles, as well as a variety of music genres, includ-\ning pop, hip hop, trap, K-pop, and street dance music. Furthermore,\nalthough all of our data sources offer higher quality motion capture\nthan 3D motion data estimated from monocular video, the different\nsources offer different levels of quality, and different capture arti-\nfacts. We find that this diversity in data sources, on top of the large\ndiversity in dance styles and skill levels, makes deterministic models\nunable to converge to model the data faithfully, while probabilistic\nmodels are able to adapt to such heterogeneous material.\nOur contributions are as follows:\nâ€¢ We present a novel architecture for autoregressive probabil-\nistic modelling of high-dimensional continuous signals, which\nwe demonstrate achieves state of the art performance on the\ntask of music-conditioned dance generation. Our architecture\nis, to the best of our knowledge, the first to combine the bene-\nfits of transformers for sequence modelling, with normalizing\nflows for probabilistic modelling.\nâ€¢ We introduce the largest dataset of 3D dance motion gener-\nated with a variety of motion capture systems. This dataset\nalso serves to showcase the potential of VR for participatory\nresearch and democratizing mocap.\nâ€¢ We evaluate our new model objectively and in a user study\nagainst two baselines, showing that both the probabilistic and\nmultimodal attention components are important to produce\nnatural and diverse dance matching the music.\nâ€¢ Finally, we explore the use of fine-tuning and â€œmotion prompt-\ningâ€ to attain control over the quality and style of the dance.\nOur paper website at metagen.ai/transflower provides data, code,\npre-trained models, videos, supplementary material, and a demo for\ntesting the models on any song with a selection of starting motion.\n2\nBACKGROUND AND PRIOR WORK\n2.1\nLearning-based motion synthesis\nThe task of generating 3D motion has been tackled in a variety of\nways. The traditional approaches to motion synthesis were based\nACM Trans. Graph., Vol. 40, No. 6, Article 1. Publication date: December 2021.\n\n\nTransflower: probabilistic autoregressive dance generation with multimodal attention\nâ€¢\n1:3\non retrieval from motion databases and motion graphs [Arikan and\nForsyth 2002; Chao et al. 2004; Kovar and Gleicher 2004; Kovar et al.\n2002; Lee et al. 2002; Safonova and Hodgins 2007; Takano et al. 2010].\nRecently, there has been more interest in statistical and learning-\nbased approaches, which can be more flexible and scale to larger\ndatasets and more complex tasks. Holden et al. [2020] explored\na continuum between the traditional retrieval-based approaches\nto motion synthesis and the more scalable deep learning-based\napproaches, showing that combining ideas from both may be fruitful.\nAmong the learning-based techniques, most works follow an\nautoregressive approach, where either the next pose or the next key\npose in the sequence is predicted based on previous poses in the\nsequence. For dance, the prediction is also conditioned on music\nfeatures, typically spanning a window of time around the time of\nprediction. We refer to both the previous poses and this window of\nmusic features together as the context. We can categorize the autore-\ngressive methods along the factors proposed in the introduction,\ni.e., the way the autoregressive model handles its inputs (context),\nand the way it handles its outputs (predicted motion). Later in this\nsection, we also compare approaches according to the amount of\nassumptions they make, and the type of learning algorithm.\nContext-dependence. We have seen an evolution towards mod-\nels that more effectively retain and utilize information from wider\ncontext windows. The first works applying neural networks to mo-\ntion prediction relied on recurrent neural networks like LSTMs,\nwhich were applied to unconditional motion [Fragkiadaki et al. 2015;\nZhou et al. 2018] and dance synthesis [Crnkovic-Friis and Crnkovic-\nFriis 2016; Tang et al. 2018]. LSTMs represent the recent context\nin a latent state. However, this latent state can act as a bottleneck\nhindering information flow from the past context, thus limiting the\nextent of the temporal correlations the network can learn. Different\narchitectures have been used to tackle this problem. BÃ¼tepage et al.\n[2017]; Holden et al. [2017]; Starke et al. [2020] directly feed the\nrecent history of poses through a feedforward network to predict\nthe next pose, while Zhuang et al. [2020] use a WaveNet-style ar-\nchitecture to extend the context even further for dance synthesis.\nRecently, Li et al. [2021a] introduce a cross-modal transformer ar-\nchitecture (extending [Vaswani et al. 2017]) that learns to attend to\nthe relevant features over the last 2 seconds of motion, as well as\nthe neighbouring 4 seconds of music, for dance generation.\nOutput modelling. Most works in motion synthesis have treated\nthe next-pose prediction as a deterministic function of the context.\nThis includes the earlier work using LSTMS [Fragkiadaki et al. 2015;\nTang et al. 2018; Zhou et al. 2018], and some of the most recent work\non dance generation [Li et al. 2021b,a]. However, in many situations,\nthe output motion is highly unconstrained by the input context. For\nexample, there are many plausible dance moves that can accompany\na piece of music, or many different gestures that fit well with an\nutterance [Alexanderson et al. 2020]. Earlier approaches to model\na probabilistic distribution over motions include Gaussian mixture\nmodels [Crnkovic-Friis and Crnkovic-Friis 2016] and Gaussian pro-\ncesses latent-variable models [Grochow et al. 2004; Levine et al. 2012;\nWang et al. 2008]. VAEs weaken the assumption of Gaussianity, and\nhave been applied to motion synthesis [Habibie et al. 2017; Ling et al.\n2020]. Recently, Petrovich et al. [2021] used VAEs combined with a\ntransformer for non-autoregressive motion synthesis â€“ they predict\nthe whole motion â€œat onceâ€, as the output of a full-attention trans-\nformer. Their architecture allows for learning complex multimodal\nmotion distributions, but their non-autoregressive approach limits\nthe length of the generated sequences. MoGlow [Henter et al. 2020]\nmodels the motion with an autoregressive normalizing flow, allow-\ning for fitting flexible probability distributions, with exact likelihood\nmaximization, producing state of the art motion synthesis results. In\nLi et al. [2020], they discretize the joint angle space. However, their\nmulti-softmax output distribution assumes independence of each\njoint, which is unusual in many cases. Lee et al. [2019b] develop a\nmusic-conditioned GAN to predict a distribution over latent rep-\nresentations of the motion. However, in order to stabilize training,\nthey apply an MSE regularization loss on the latents which may\nlimit its ability to model complex distributions. Li et al. [2021b] also\nintroduce an adversarially-trained model which, however, does not\nhave a noise input â€“ no source of randomness â€“ and thus cannot be\nsaid to model a music-conditioned probability distribution.1\nDomain-specific assumptions. A third dimension in which we\ncan compare the different approaches to motion synthesis is the\namount of domain-specific assumptions they make. To disambigu-\nate the predictions from a deterministic model, or to increase mo-\ntion realism, different works have added extra inputs to the model,\ntailored at the desired types of animations, including foot contact\n[Holden et al. 2016], pace [Pavllo et al. 2018], and phase information\n[Holden et al. 2017; Starke et al. 2020]. For dance synthesis, Lee\net al. [2019b] and Li et al. [2021b] use the observation that dance\ncan often be fruitfully decomposed into short movement segments,\nwhose transitions lie at music beats. Furthermore Li et al. [2021b]\ndevelop an architecture that includes inductive biases specific to\nkinematic skeletons, as well as a bias towards learning local tem-\nporal correlations. On the other end of the spectrum, Henter et al.\n[2020] presents a general sequence prediction model that makes few\nassumptions about the data. This allows it to be applied to varied\ntasks like humanoid or quadruped motion synthesis, or gesture gen-\neration [Alexanderson et al. 2020] without fundamentally changing\nthe model, but it has not yet been applied to dance. For dance syn-\nthesis, Li et al. [2021a] demonstrate that a similarly general-purpose\nmodel can produce impressive results.\nLearning algorithm. An alternative approach to learn to gener-\nate realistic movements from motion capture data is to use a set of\ntechniques known as imitation learning (IL). Merel et al. [2017] use\ngenerative adversarial imitation learning (GAIL), a technique closely\nrelated to GANs, to learn a controller for a physics-based humanoid\nto imitate different gait motions. Peng et al. [2018a] and Peng et al.\n[2021] extend this work to characters that learn a diversity of skills,\nwith a variety of morphologies. This approach can learn to imitate\nmocap data in a physics-based environment, so that the character\nmovement is automatically physically realistic, which is necessary\nbut not sufficient for natural motion. Ling et al. [2020] also found\ncombining IL approaches with previously described supervised and\nself-supervised learning approaches to be a promising direction to\nget the benefits of both.\n1However, it is possible that the music input itself could serve as a kind of noise source,\nallowing the model to effectively learn a probability distribution. This deserves further\ninvestigation.\nACM Trans. Graph., Vol. 40, No. 6, Article 1. Publication date: December 2021.\n\n\n1:4\nâ€¢\nGuillermo Valle-PÃ©rez, Gustav Eje Henter, Jonas Beskow, Andre Holzapfel, Pierre-Yves Oudeyer, and Simon Alexanderson\nOverall, we expect that learning-based motion synthesis will\nfollow a similar trend as in other areas where machine learning\nis applied to complex data distributions: models that can flexibly\nattend over a large context, like transformers, while being able\nto model complex distributions over its outputs, produce the best\ngenerative results, when enough data is available [Brown et al. 2020;\nDosovitskiy et al. 2020; Henighan et al. 2020; Kaplan et al. 2020;\nRamesh et al. 2021]. Here we present what we believe is the first\nmodel for autoregressive motion synthesis combining both of these\ndesirable properties, which we expect to become crucial as motion\ncapture datasets grow both in size and diversity. Furthermore, we use\nthe model for music-conditioned dance generation, demonstrating\nthat it is able to be used for tasks involving multiple modalities\n(music and movement in our case).\n2.2\nOther data-driven dance motion synthesis\nAlthough the focus of this work is on purely learning-based ap-\nproaches to dance synthesis, these are not the only data-driven\nmethods to for generating dance. As we discussed in section 2.1,\nthere is in fact a continuum between completely non-learning based\nand purely learning-based approaches. In general, learning-based\ntechniques typically trade off a larger compute cost for training,\nfor a reduced cost at inference [Holden et al. 2020], which often\namortizes the training cost. There are also several dimensions along\nwhich machine learning techniques can be introduced in a dance\nsynthesis pipeline. Here we focus on the motion synthesis part.\nSeveral works have approached the problem of dance generation\nusing motion graphs, where transitions between pre-made motion\nclips are used to create a choreography. In effect, these methods\ngenerate motion by selection, whereas deep learning can be seen as\nmore akin to interpolation. Fan et al. [2011] and Fukayama and Goto\n[2015] used statistical methods for traversing the graph, but recently,\ndeep learning techniques have been developed for traversing the\nmotion graph [Kang et al. 2021; Ye et al. 2020]. These approaches\ntend to produce reliable and high quality dances, that are easier to\nedit. However, a lot of motion-graph-based dance synthesis works\nrely on datasets of key-framed animations, which may cause the\ngenerated dances to look less natural. The large mocap dance data-\nbase we are introducing may therefore help enable more natural\nmotion also for this class of techniques.\n2.3\nDance datasets\nPrevious research on dance generation has been conducted with\ndata obtained from a variety of different techniques. Alemi et al.\n[2017] and Zhuang et al. [2020] recorded mocap datasets of dances\nsynchronized to music, totalling 23 min and 58 min, respectively. Li\net al. [2021a] use the AIST dance dataset [Tsuchida et al. 2019] to\nobtain 3D dance motion using multi-view 3D pose estimation. Lee\net al. [2019b] and Li et al. [2020] use monocular 3D pose estimation\nfrom YouTube videos to produce a dataset of 3D dance motion of 71\nh and 50 h in total, respectively. Monocular 3D pose estimation is\nan area of active research [Bogo et al. 2016; Mathis et al. 2020; Rong\net al. 2021], but current methods suffer from inaccurate root motion\nestimation [Li et al. 2021a], so these works tend to focus on the joint\nmovements. Finally, Li et al. [2021b] introduce a 5 h animated dance\ndataset, created by professional animators. Overall, we find that\nthere is a trade-off between data quality and data availability. In this\nwork, we present a new way of collecting motion data, from remote\nVR user participants, which pushes the currently available Pareto\nfrontier, with data quality approaching that of mocap equipment,\nand increasing availability as the number of VR users grows. We\nthink that the democratization of motion capture with VR, brings\nexciting possibilities for researchers and VR users alike. In particular,\nthe ability to crowdsource data at scale should make it possible to\nharness the scaling phenomena seen in many other generative-\nmodelling tasks [Henighan et al. 2020], and also study the effects of\nscaling on model performance. Compared to datasets like AIST++ [Li\net al. 2021a], we believe that our dataset captures a larger diversity\nof skill levels, as well as dance styles not present in AIST++.\n3\nDATASET\nIn this paper we introduce two new large scale datasets of 3D dance\nmotion synchronized to music: the PMSD dance dataset, and the\nShaderMotion VR dance dataset. We combine these datasets with\ntwo previous existing dance datasets, AIST++ [Li et al. 2021a] and\nGrooveNet [Alemi et al. 2017], to create our combined dataset on\nwhich we train our models. We standardize all the datasets into a\ncommon skeleton (the one used for the PMSD dataset), and will\npublicly release the data. We here describe the two new datasets,\nand compare all the sources, including existing ones, in table 1.\nPopular music and street dance (PMSD) dataset. The PMSD\ndance dataset consists of synchronized audio and motion capture\nrecordings of various dancers and dance styles. The data was recor-\nded with an Optitrack Prime41 motion capture system (17 cameras\nat 120 Hz) and is divided in two parts. The first part (PMSDCasual)\ncontains 142 minutes of casual dancing to popular music by 7 non-\nprofessional dancers. The 37 markers where solved to a skeleton\nwith 21 joints. The second part (PMSDStreet) contains 44 minutes of\nstreet dance performed by one professional dancer. The dances are\ndivided in three distinct styles: Hip-Hop, Popping and Krumping.\nThe music was selected by the dancer to be appropriate for each\nstyle. In this setup we used 65 markers (45 on the body and 2 on each\nfinger), and solved the data to a skeleton with 51 joints, including\nfingers and hinged toes. Compared to the casual dances, the street\ndances have considerably more complex choreographies with more\ntempo-shifts and less repetitive patterns.\nShaderMotion VR dance dataset. The data was recorded by\nparticipants who dance in the social VR platform VRChat2, using a\ntool called ShaderMotion that encodes their avatarâ€™s motion into a\nvideo [lox9973 2021]. Their avatar follows their movement using\ninverse kinematics, anchored to their real body movement via a\n6-point tracking system, where the head, hands, hips, and feet are\nbeing tracked, using a VR headset, and HTC Vive trackers. The\nvideos with encoded motion can then be converted back to motion\non a skeleton rig via the MotionPlayer script provided in the Sha-\nderMotion repository. The data includes finger tracking, with an\naccuracy dependent on the VR equipment used. We further divide\nthe data into four components, which have different dancers and\nstyles (see table 1). We only included part of the ShaderMotion for\n2https://hello.vrchat.com/\nACM Trans. Graph., Vol. 40, No. 6, Article 1. Publication date: December 2021.\n\n\nTransflower: probabilistic autoregressive dance generation with multimodal attention\nâ€¢\n1:5\nSource\nMinutes\n#dancers Styles\nAIST++*\n312.1\n30\nBreak, Pop, Lock,\nHip Hop, House,\nWaack,\nKrump,\nStreet Jazz, Ballet\nJazz\nGrooveNet*\n25.0\n1\nGrooveNet\nPMSDCasual*\n142.1\n1\nCasual\nPMSDStreet*\n88.1\n1\nKrump, Hip Hop,\nPop\nSM1*\n229.9\n2\nFreestyle\nSMVibe*\n40.2\n6\nFreestyle, Ballet\nSMJustDance\n188.3\n1\nJustDance\nSMDavid\n15.9\n1\nBreak, Krump, Hip\nHop, Pop\nSMKonata\n138.9\n1\nFreestyle\nSyrtos\n49.7\n6\nSyrtos\nTotal\n1240.2\n49\n-\nTable 1. Sources of dance data in our dataset. PMSD refers to the com-\nponents of the PMSD dance dataset, and SM refers to the components of the\nShaderMotion VR dance dataset. We mark with * the components which we\nused to train the models in this paper (we got the rest of the data recently,\nand are working on scaling up the models to the full dataset). Note that\nthere is one dancer in common between SMKonata and SMVibe. Note that\nwe classify GrooveNet and JustDance as their own styles as their dances\nmostly consist on repeating certain motifs. GrooveNet consists mostly of\nsimple rhythmic motifs, while JustDance has a larger diversity of motifs\nthat appear in the game JustDance.\ntraining (the ShaderMotion1 and ShaderMotionVibe components),\nbecause we only obtained the rest of the data recently. We plan to\nrelease models trained on the complete data soon. Some examples\nof how the VR dancing in our dataset looks, for street dance style,\ncan be seen in this URL https://www.youtube.com/playlist?list=\nPLmwqDOin_Zt4WCMWqoK6SdHlg0C_WeCP6\nSyrtos dataset. The Syrtos dataset consists of synchronized au-\ndio and motion capture recordings of a specific Greek dance style â€“\nthe Cretan Syrtos â€“ from six dancers. Eleven performances are con-\ntained in the data, with all but one dancer performing twice, giving a\ntotal duration of approximately 50 min. The data was recorded with\nthe dancers individually during ethnographic fieldwork in Crete\nin 2019 [Holzapfel et al. 2020], using a Xsens3 inertia system with\n17 sensors operating at 240 Hz, and post-processed through Xsens\nsoftware to a skeleton with 22 joints. The audio data â€“ performed\nlive by musicians during the recordings â€“ consists of single tracks\ncontaining the individual instruments. Note that we did not use the\nSyrtos data in this study, but release it for future research.\n4\nMETHOD\nWe introduce a new autoregressive probabilistic model with atten-\ntion which we call Transflower. The architecture combines ideas\nfor autoregressive cross-modal attention using transformers from\nLi et al. [2021a] with ideas for autoregressive probabilistic models\n3https://www.xsens.com/\nusing normalizing flows from Henter et al. [2020]. The model is\ndesigned to be applicable to any autoregressive probabilistic mod-\nelling with multiple modalities as inputs and outputs, although in\nthis paper we focus on the application to music-conditioned motion\ngeneration. The code and trained models will be made available.\nWe model dance-conditioned motion autoregressively. To do this,\nwe represent motion as a sequence of poses x = {ğ‘¥ğ‘–}ğ‘–=ğ‘\nğ‘–=1 âˆˆRğ‘Ã—ğ‘‘ğ‘¥\nsampled at ğ‘times ğ‘¡ğ‘–at a fixed sampling rate, and music as a\nsequence of audio features {ğ‘šğ‘–}ğ‘–=ğ‘\nğ‘–=1 âˆˆRğ‘Ã—ğ‘‘ğ‘šextracted from win-\ndows centered around the same times ğ‘¡ğ‘–. ğ‘‘ğ‘¥and ğ‘‘ğ‘šare the number\nof features for the pose and music samples. For our experiments we\nsample motion poses and music features at 20 Hz. The autoregress-\nive task is to predict the ğ‘–th pose given all previous poses, and some\nmusic context. For simplicity, we restrict the prediction to depend\nonly on the previous ğ‘˜ğ‘¥poses and the previous ğ‘˜ğ‘šand future ğ‘™ğ‘š\nmusic features. The probability distribution over the entire motion\ncan be written as a product, using the chain rule of probability:\nğ‘(x) =\nğ‘\nÃ–\nğ‘–=1\nğ‘(ğ‘¥ğ‘–|ğ‘¥ğ‘–âˆ’ğ‘˜ğ‘¥, ...,ğ‘¥ğ‘–âˆ’1;ğ‘šğ‘–âˆ’ğ‘˜ğ‘š, ...,ğ‘šğ‘–+ğ‘™ğ‘š)\n(1)\nwhere ğ‘¥orğ‘šwith indices smaller than 0 are either padded, or repres-\nent the â€œcontext seedâ€ (the initial ğ‘˜ğ‘¥poses and initial ğ‘˜ğ‘š+ğ‘™ğ‘šmusic\nfeatures) that is fed to the model. We experimented with different\nmotion seeds for the autoregressive generation in section 5.3.\nTransflower is composed of two components, a transformer en-\ncoder, to encode the motion and music context, and a normalizing\nflow head, that predicts a probability distribution over future poses,\ngiven the context. We expressğ‘(ğ‘¥ğ‘–|ğ‘¥ğ‘–âˆ’ğ‘˜ğ‘¥, ...,ğ‘¥ğ‘–âˆ’1;ğ‘šğ‘–âˆ’ğ‘˜ğ‘š, ...,ğ‘šğ‘–+ğ‘™ğ‘š) =\nğ‘(ğ‘¥ğ‘–|h) with a normalizing flow conditioned on a latent vector h.\nThis vector encodes the context via a transformer encoder h =\nğ‘“(ğ‘¥ğ‘–âˆ’ğ‘˜ğ‘¥, ...,ğ‘¥ğ‘–âˆ’1;ğ‘šğ‘–âˆ’ğ‘˜ğ‘š, ...,ğ‘šğ‘–+ğ‘™ğ‘š). For the encoder, we use the design\nproposed by Li et al. [2021a], with two transformers that encode\nthe motion part of the context hğ‘¥= ğ‘“ğ‘¥(ğ‘¥ğ‘–âˆ’ğ‘˜ğ‘¥, ...,ğ‘¥ğ‘–âˆ’1) âˆˆRğ‘˜ğ‘¥Ã—ğ‘‘ğ‘š\nand the music part hğ‘¥= ğ‘“ğ‘š(ğ‘šğ‘–âˆ’ğ‘˜ğ‘š, ...,ğ‘šğ‘–+ğ‘™ğ‘š) âˆˆR(ğ‘™ğ‘š+ğ‘˜ğ‘š)Ã—ğ‘‘ğ‘šsep-\narately, where the output dimension ğ‘‘ğ‘šis the same for both pose\nand music encoders. The outputs of these two transformers are then\nconcatenated into a cross-modal transformer Ëœh = ğ‘“ğ‘ğ‘š(hğ‘¥, hğ‘¥) âˆˆ\nR(ğ‘™ğ‘š+ğ‘˜ğ‘š+ğ‘˜ğ‘¥)Ã—ğ‘‘â„. The latent h âˆˆRğ¾Ã—ğ‘‘â„corresponds to a prefix of\nthis output Ëœh (the way ğ¾is chosen will be explained later). We use\nthe standard transformer encoder implementation in PyTorch, and\nuse T5-style relative positional embeddings [Raffel et al. 2019] in\nall transformers to obtain translation invariance across time [Wen-\nnberg and Henter 2021]. While Li et al. [2021a] use the outputs of\nthe cross-modal transformer as the deterministic prediction of the\nmodel, we interpret the first ğ¾outputs of the encoder as a latent\nvector h on which we condition the normalizing flow output head.\nWe use a normalizing flow model based on 1x1 invertible convo-\nlutions and affine coupling layers [Henter et al. 2020; Kingma and\nDhariwal 2018]. Like Ho et al. [2019], we use attention for the affine\ncoupling layers, but unlike them, we remove the convolutional lay-\ners, and use a pure-attention affine coupling layer. The inputs to the\nnormalizing flow correspond to ğ‘predicted poses of dimension ğ‘‘ğ‘¥.\nThe affine coupling layer splits the inputs ğ‘§ğ‘–âˆˆRğ‘Ã—ğ‘‘ğ‘¥channel-wise\ninto ğ‘§â€²\nğ‘–,ğ‘§â€²â€²\nğ‘–\nâˆˆRğ‘Ã—ğ‘‘ğ‘¥/2 and applies an affine transformation to ğ‘§â€²â€²\nğ‘–\nwith parameters depending on ğ‘§â€²\nğ‘–, i.e. ğ‘§ğ‘–+1 = A(ğ‘§â€²\nğ‘–, h) âŠ™ğ‘§â€²â€²\nğ‘–+B(ğ‘§â€²\nğ‘–, h).\nACM Trans. Graph., Vol. 40, No. 6, Article 1. Publication date: December 2021.\n\n\n1:6\nâ€¢\nGuillermo Valle-PÃ©rez, Gustav Eje Henter, Jonas Beskow, Andre Holzapfel, Pierre-Yves Oudeyer, and Simon Alexanderson\nFig. 2. The Transflower architecture. Green blocks represent neural network modules which take input from below and feed their output to the module\nabove. In the affine coupling layer, split and concatenation are done channel-wise, and the â€˜affine couplingâ€™ is an element wise linear scaling with shift and\nscale parameters determined by the output of the coupling transformer, as in Henter et al. [2020]; Kingma and Dhariwal [2018]. The normalizing flow is\ncomposed of several blocks, each of containing a batch normalization, an invertible 1x1 convolution, and an affine coupling layer. The motion, audio, and\ncross-modal transformers are standard full-attention transformer encoders, like in Li et al. [2021a], except that we use T5-style relative positional encodings.\nThese affine parameters are the output of the coupling transformer,\n(A, B) = ğ‘“ğ‘ğ‘¡( Ëœğ‘¥) âˆˆRğ‘Ã—2ğ‘‘ğ‘¥, where Ëœğ‘¥= (ğ‘§â€²\nğ‘–, h) âˆˆRğ‘Ã—(ğ‘‘ğ‘¥+ğ‘‘â„). Like in\nMoGlow [Henter et al. 2020], we concatenate the latent vector h\nto the inputs of ğ‘“ğ‘ğ‘¡along the channel dimension, to condition the\nnormalizing flow on the context. The main architectural difference\nis that MoGlow primarily relies on LSTMs for propagating inform-\nation over time, whereas the proposed model uses Transformers\nand attention mechanisms. We believe this should make it easier\nfor the model to discern and focus on specific features in a long\ncontext window, which we think may be important for learning\nchoreography, executing consistent dance moves, and also for being\nsensitive to the music. Kingma and Dhariwal [2018] used ActNorm\nlayers motivated by the inaccuracy of batch normalization when\nusing very small batch sizes (they used a batch size of 1). As we use\nlarger batch sizes, we found that batch normalization sometimes\nproduced moderately faster convergence, so we use it in our net-\nworks instead of ActNorm, unless specified otherwise. Our model\nuses 16 of these normalizing flow blocks.\nWe also found, like in Li et al. [2021a], that training to predict the\nnext ğ‘poses improves model performance. We therefore model the\nnormalizing flow output as a ğ‘Ã— ğ‘‘tensor (ğ‘‘being the dimension\nof the pose vector and ğ‘the sequence dimension). With this setup,\nthe transformers in the coupling layers A act along this sequence\ndimension, and the 1x1 convolutions act independently on each\nelement in the sequence. The transformer encoder latent â€œvectorâ€\nh is then interpreted as a ğ¾Ã— ğ‘‘â„tensor where ğ‘‘â„is the output\ndimension of the transformer encoder. By making ğ¾and ğ‘equal we\ncan concatenate h with the input to A along the channel dimension.\nWe show a diagram of the whole architecture in fig. 2.\nMotion features. We retarget all the motion data to the same\nskeleton with 21 joints (including the root) using Autodesk Motion\nBuilder. We represent the root (hip) motion as (Î”ğ‘¥, Î”ğ‘§,ğ‘¦,ğœƒ1,ğœƒ2,ğœƒ3, Î”ğ‘Ÿğ‘¦)\nwhere Î”ğ‘¥and Î”ğ‘§are the position changes relative to the rootâ€™s\nground-projected coordinate frame, i.e. the coordinate frame ob-\ntained by removing the roots rotation around the ğ‘¦(vertical) axis, so\nthat Î”ğ‘¥represents sideways movement and Î”ğ‘§represents forward\nmovement. ğ‘¦is the vertical position of the root in the base coordin-\nate system, that is the height from the floor. ğœƒis a exponential map\nrepresentation of 3D rotation of the root joint with respect to the\nrootâ€™s ground-projected frame and Î”ğ‘Ÿğ‘¦is the change of 2D facing\nangle. For all the joints we use exponential map parametrization\n[Grassia 1998] of the rotation, resulting in 3 features per (non-root)\njoint, and a total 67 motion features.\nAudio features. To represent the music, we combine spectro-\ngram features with beat-related features, by concatenating:\nâ€¢ 80 dimensional mel-frequency logarithmic magnitude spec-\ntrum with a hop size equal to 50 ms.\nâ€¢ One dimensional spectral flux onset strength envelope feature\nas provided by Librosa4.\nâ€¢ Two-dimensional output activations of the RNNDownBeat-\nProcessor model in the Madmom toolbox5.\nâ€¢ Two-dimensional beat features extracted from the two prin-\ncipal components of the last layer of the beat detection neural\nnetwork in stage 1 of DeepSaber [Labs 2019], which is the\nsame beat-detection architecture in Dance Dance Convolu-\ntion [Donahue et al. 2017], but trained on Beat Saber levels.\nAll the above features were used with settings to obtain the same\nframe rate as for the mel-frequency spectrum (20 Hz). We included\nboth Madmom and DeepSaber features because we observed in\npreliminary investigations that while the Madmom beat detector\nworked well for detecting the regular beats of a song, the DeepSaber\nfeatures sometimes worked better as general onset detectors.\nAfter processing the data into the above features, we individually\nstandardize them to have a mean of 0 and standard deviation of 1\nover the whole dataset used for training.\n4https://github.com/librosa/librosa\n5https://github.com/CPJKU/madmom\nACM Trans. Graph., Vol. 40, No. 6, Article 1. Publication date: December 2021.\n\n\nTransflower: probabilistic autoregressive dance generation with multimodal attention\nâ€¢\n1:7\nTraining. We train both Transflower and the MoGlow baseline\n[Henter et al. 2020] on 4 V100 Nvidia GPUs with a batch size of 84\nper GPU, for 600k steps, which took 7 days. We used a learning rate\nof 7 Ã— 10âˆ’5, which we decay by a factor of 0.1 after 200k iterations,\nand again after 400k iterations. The AI Choreographer baseline [Li\net al. 2021a] was trained for 600k iterations on a single TPUv3 with\na batch size of 128 (per TPU core) and a learning rate of 1 Ã— 10âˆ’4\ndecayed to 1 Ã— 10âˆ’5 and 1 Ã— 10âˆ’6 after 100k and 200k iterations\nrespectively. During training, we use â€œteacher forcingâ€, that is the\ninputs to the model come from the ground truth data, rather than\nautoregressively from model outputs. The architecture hyperpara-\nmeters for the different models are given in table 5 in appendix A,\nwhere we also explain how these hyperparameters were chosen.\nSynthesis. Transflower and MoGlow both run at over 20 Hz on\nan Nvidia V100 GPU, while the transformer from AI Choreographer\nruns at 100 Hz on an Nvidia V100.\nFine-tuning. We investigate the effect of fine-tuning Transflower\non the PMSD motion dataset, the portion of our data with highest\nquality motion tracking. We train the model for an extra 50k itera-\ntions only on this dataset. We found that training for longer reduced\nthe diversity, and 50k iterations produced a good trade-off between\ndiversity and improved quality of produced motions, as we find in\nsection 5. Note that the baselines were not fine-tuned in this manner,\nand should not be compared directly to this fine-tuned system.\nMotion â€œpromptingâ€. We also explored the role of the motion\nseed in autoregressive motion generation. We seed the different\nmodels with 5 different seeds (6 seconds long, chosen to be repres-\nentative of the different styles in our dataset), and compare how the\nseed affects the style of dancing. We find that the seed can indeed\nbe used to control the style of dancing, serving as a weak form of\nâ€œpromptingâ€ similar to the current trend in language models [Brown\net al. 2020; Reynolds and McDonell 2021]. We show more detailed\nresults in section 5.3.\n5\nEXPERIMENTS AND EVALUATION\nWe compare Transflower with the deterministic transformer model\nfrom AI Choreographer [Li et al. 2021a] and with the probabilistic\nmotion generation model MoGlow [Henter et al. 2020], which does\nnot use attention and has not been applied to dance motion syn-\nthesis before. In our experiments, we train all the models with the\nsame amount of past motion context, and past and future music\ncontext on the same dataset (the marked components in table 1).\nComparing MoGlow with Transflower, we can find out the effect of\nusing attention for learning the dependence on the context (Trans-\nflower), versus using a combination of recent frame concatenation\nand LSTMs (MoGlow). Comparing AI Choreographer with Trans-\nflower, we can discern the effect of having a probabilistic (Trans-\nflower) versus a deterministic (AI Choreographer) model.\n5.1\nObjective metrics\nWe look at two objective metrics that capture how realistic the\nmotion is, and how well it matches the music. For the evaluation of\nthe objective metrics we use a test set consisting of 33 songs and\nmotion seeds that were not found in the training set (i.e., excluding\nAIST++, which had song overlap with the training data), and 27\nModel\nFPD\nFMD\nAI Choreographer\n963.9\n2977.4\nMoGlow\n600.2\n1847.6\nTransflower (fine-tuned)\n549.0\n1711.9\nTransflower\n511.6\n1610.5\nTable 2. Realism metrics. FrÃ©chet pose distance (FPD) and FrÃ©chet move-\nment distance (FMD) for the three models we compare, as well as Trans-\nflower fine-tuned on the dance dataset.\nTFF\nTF\nMG\nAIC\nMatch\nMismatch\nMean\n0.25\n0.27\n0.34\n0.44\n0.20\n0.22\nSD\n0.33\n0.31\n0.52\n0.54\n0.24\n0.67\nTable 3. Beat alignment. Mean and standard deviations of the time offset\nbetween musical and kinematic beats(s).\nshorter songs (from AIST++) that were found on the training set\nbut with a different motion seed. Of the 33 non-AIST++ songs, 18\nwere ones randomly held out from the training set, while the other\n15 were added manually. We generated samples from each of the\nmodels, for 5 different motion seeds, and evaluated the metrics on\nthe full set of 300 generated sequences.\nRealism metric. For assessing realism, we use the FrÃ©chet dis-\ntance between the distribution of poses ğ‘ğ‘–and the distribution of\nconcatenations of three consecutive poses (ğ‘ğ‘–âˆ’1, ğ‘ğ‘–, ğ‘ğ‘–+1), which\ncaptures information about pose, joint velocity, and joint accelera-\ntion. We call these measures the FrÃ©chet pose distance (FPD) and\nthe FrÃ©chet movement distance (FMD), respectively. The measures\nwere computed on the â€œrawâ€ pose features, without mean and vari-\nance normalization. The results are shown in table 2. We can see\nthat AI Choreographer struggles to faithfully capture the variety\n(of both styles and tracking methods) in our dataset. We observe\nit often produces chaotic movement or freezes into a mean pose.\nMoGlow does better, while Transflower (both fine-tuned and non-\nfine-tuned) capture the distribution of real movements best (with\nthe non-fine-tuned slightly better, as expected).\nMusic-matching metric. In addition to the realism measure\nabove, we objectively assess rhythmical aspects of the music and\ndance based on metrics calculated from audio and motion beats.\nThese are relatively easy to track, but are by far are not the only\naspect that matters in good dancing. To extract the audio beats, we\nemployed the beat-tracking algorithms of BÃ¶ck and Schedl [2011];\nKrebs et al. [2015], included in the Madmom library. To detect dance\nbeats, we rely on a large body of studies on the relation between\nmusical meter and periodicity of dance movements [Burger et al.\n2013; Haugen 2014; Misgeld et al. 2019; Naveda and Leman 2011;\nToiviainen et al. 2010]. In specific, we draw on observations from\nToiviainen et al. [2010], showing that audio beat locations closely\nmatch the points of maximal downward velocity of the body (which\nis straightforward to measure at the hip), a finding consistent with\nobservations from Haugen [2014]; Misgeld et al. [2019] that the\ncenter of gravity of the body provides stable beat information. Hence,\nwe extracted the kinematic beat locations as the local minima of the\nğ‘¦-velocity of the Hips-joint. A visualization of the relation between\nACM Trans. Graph., Vol. 40, No. 6, Article 1. Publication date: December 2021.\n\n\n1:8\nâ€¢\nGuillermo Valle-PÃ©rez, Gustav Eje Henter, Jonas Beskow, Andre Holzapfel, Pierre-Yves Oudeyer, and Simon Alexanderson\n0:00\n0:50\n1:40\n2:30\n3:20\nTime\n16\n32\n64\n128\n256\nBPM\n(a) Music\n0:00\n0:50\n1:40\n2:30\n3:20\nTime\n16\n32\n64\n128\n256\nBPM\n(b) TFF\n0:00\n0:50\n1:40\n2:30\n3:20\nTime\n16\n32\n64\n128\n256\nBPM\n(c) TF\n0:00\n0:50\n1:40\n2:30\n3:20\nTime\n16\n32\n64\n128\n256\nBPM\n(d) MG\n0:00\n0:50\n1:40\n2:30\n3:20\nTime\n16\n32\n64\n128\n256\nBPM\n(e) AIC\nFig. 3. Music and kinematic tempograms for one complete dance. From left to right: Music, Transflower fine-tuned (TFF), Transflower (TF), MoGlow (MG) and\nAI Choreographer (AIC). The vertical axis corresponds to frequency, measured in beats per minute (bpm).\nthe tempo processes emerging from dance beats of the various\nsystems and the audio beats is shown in Figure 3, where we depict\naudio and visual tempograms [Davis and Agrawala 2018] for one\ncomplete dance. For the visual histograms, we used the local Hips\nvelocity at the kinematic beat locations as magnitude information.\nThe visual tempograms in Figure 3 show that the music has a very\nstrong and regular rhythmic structure, visible as horizontal bands.\nAmong the different models, it is apparent that the dance generated\nby Transflower (especially after fine-tuning) is characterized by the\nstrongest relation to the audio beat, with a pronounced periodicity at\nhalf the estimated audio beat (i.e., around 120 bpm). This periodicity\nbecomes more and more washed out for the other models as we move\nto the right through the subfigures, indicating less rhythmically\nconsistent motion. To quantify the alignment between the audio\nand kinematic beats, we calculate the absolute time offset between\neach music beat and its closest kinematic beat over the complete set\nof generated seeds and motions. The mean and standard deviation of\nthe offsets for the evaluated systems are reported in table 3 together\nwith the corresponding values for the complete (matched) training\ndata as well as randomly paired (mismatched) music and motion. It\nis apparent that the proposed model leads to an improved alignment\nas compared to the baseline systems.\n5.2\nUser study\nQualitative user studies are important to evaluate generative models,\nas perceived quality of different metrics tends to be the most relevant\nmetric for many downstream applications. We thus performed a\nuser study to evaluate our model and the baselines along three\naxis: naturalness of the motion, appropriateness to the music, and\ndiversity of the dance movements.\nTo perform the user study we used 17 songs that were not present\nin the training set. Among these, we include 10 songs obtained\nâ€œin-the-wildâ€ from YouTube, that may not necessarily match the\ngenres found in our datasets, and for which no ground truth dance\nis available. We use a fixed motion seed from the PMSDCasual\ndataset, as it consists of a generic T-pose. However, for the AI Cho-\nreographer baseline, this seed often caused the model to degenerate\ninto a frozen pose, probably because the PMSDCasual T-pose has\nparticularly high uncertainty over which motion will follow, making\nthe deterministic model more likely to regress into a mean pose. To\narrive to the 17 songs used for evaluation, we thus used a seed from\nShaderMotion for AI Choreographer to alleviate this problem, and\nremoved from the evaluation the remaining sequences where AI\nChoreographer still regressed to a mean pose.\nWe rendered 15-second animation clips with a stylized skinned\ncharacter for each of the 17 songs and the four models, yielding\na total of 68 stimuli. We performed three separate perception ex-\nperiments, detailed below. All experiments were carried out online\nand participants were recruited via a crowd-sourcing platform (Pro-\nlific). The presentation order of the stimuli was randomized between\nthe participants. In each of the experiments, 25 participants rated\neach video clip on a five point Likert-type scale. Participants were\nbetween 22 and 66 years of age (median 34), 39% male, 61% female.\nâ€¢ Naturalness: We asked participants to rate videos of dances\ngenerated by the different models according to the question\nOn a scale from 1 to 5: how natural is the dancing motion? I.e.\nto what extent the movements look like they could be carried\nout by a dancing human being? where 1 is very unnatural and\n5 is very natural. We removed the audio so that participants\nwould only be able to judge the naturalness of the motion.\nâ€¢ Appropriateness: We asked participants to rate videos of\ndances generated by the different models according to the\nquestion On a scale from 1 to 5: To what extent is the character\ndancing to the music? I.e. how well do the movements match\nthe audio? where 1 is not at all and 5 is very well.\nâ€¢ Within-dance diversity: Models can exhibit many types of\ndiversity. They can show diverse motions when changing the\nmotion seed, the song, or at different points within the same\nsong. Probabilistic models can furthermore generate different\nmotions even for the same seed and song. For this study, we\ndecided to focus on the diversity of motions within the same\nsong, for a fixed seed, as this scenario is representative of how\ndance generation models may be used in practice. We thus\nselected two disjoint pieces of generated dance from within\nthe same song, for each model, and presented the videos side\nby side. We asked participants to rate the generated motions\n(without audio) according to the question On a scale from 1\nto 5: How different are the two dances from each other? where\n1 is very similar and 5 is very different.\nResults. The fine-tuned Transflower model was rated highest\nboth in terms of naturalness and appropriateness, followed by the\nstandard Transflower model. Figure 4 shows the mean ratings for all\nfour models across the three experiments. A one-way ANOVA and\na post-hoc Tukey multiple comparison test was performed, in order\nACM Trans. Graph., Vol. 40, No. 6, Article 1. Publication date: December 2021.\n\n\nTransflower: probabilistic autoregressive dance generation with multimodal attention\nâ€¢\n1:9\nTFF\nTF\nMG\nAIC\n0\n1\n2\n3\n4\n5\nRating\nNaturalness\nTFF\nTF\nMG\nAIC\nAppropriateness\nTFF\nTF\nMG\nAIC\nWithin-dance diversity\nFig. 4. Results of user study, mean ratings with standard deviation bars. From left to right: naturalness, appropriateness and diversity, for the four models in\nthe study: Transflower fine-tuned (TFF), Transflower (TF), MoGlow (MG) and AI Choreographer (AIC). 95% confidence intervals for the mean ratings are not\nshown in the figure, but equate to Â±0.13 or less for all models in the three experiments, which is much narrower than the plotted standard deviations.\nto identify significant differences. For naturalness, all differences\nbetween models were significant (ğ‘< 0.001). For appropriateness,\nall differences except between MoGlow and AI Choreographer were\nsignificant (ğ‘< 0.001). For diversity, fine-tuned Transflower was\nrated less diverse than the other models (ğ‘< 0.001), and Transflower\nwas more diverse than MoGlow ğ‘= 0.02). However, we again em-\nphasize that only the non-fine-tuned Transflower model is directly\ncomparable to the two baselines, since the latter were not fine-tuned\non the higher-quality components of the data.\n5.3\nMotion prompting\nAutoregressive models require an initial context seed to initiate\nthe generation. We experimented with feeding the model different\nmotion seeds, and observed that the seed has a significant effect on\nthe style of the generated dance. To make this observation more\nquantitative, we measured the FMD between the distribution of\nmotions that Transflower produces when seeded with a motion seed\nof different styles (and over the different songs in the test set) and\nthe ground truth data for those styles. Results are shown in table 4,\nwhere we see that the seed changes the FMD distribution (and thus\nthe style of the dance), and tends to make the style closer to the style\nrepresented by the seed, in the sense that the smallest FMD is found\non the diagonal (matched seed and style) in three of five cases. This\nappears to be a weak version of the effect of â€œpromptingâ€ observed\nin language models [Brown et al. 2020; Reynolds and McDonell\n2021]. We conjecture that this prompting effect will allow more\ncontrolability of motion and dance generation models, as we make\nthe models more powerful, by increasing dataset and model sizes.\nWe also observe in table 4 how the FMD is a lot higher for some\nstyles than others. Freestyle (with data from SM1) seems like the\nmost challenging for the model to capture, seeing that the FMD is\nvery high for all seeds. This is expected as this dataset is both highly\ndiverse and has a lower motion-tracking quality. If we discount this\nstyle, the smallest FMD is on the diagonal in three out of four cases.\n6\nDISCUSSION\nIn this work, we have described the first model for dance synthesis\ncombining powerful probabilistic modelling based on normalizing\nflows, and an attention-based encoder for encoding the motion and\nMotion seed\nFS\nCA\nHH\nBR\nGN\nGround trutth\nFS\n5615.7\n5649.1\n5492.8\n5495.6\n6054.4\nCA\n352.3\n7.4\n192.9\n155.2\n242.9\nHH\n92.1\n712.0\n187.9\n238.4\n1619.4\nBR\n487.7\n109.7\n286.1\n238.0\n254.0\nGN\n1326.3\n340.1\n979.8\n881.2\n51.3\nTable 4. Effect of different motion seeds on dance style. We compare\nthe FMD between the distribution of motions for Transflower seeded with\na motion seed of different styles (and different songs), and the ground truth\ndata for those styles. FS: freestyle, CA: casual, HH: hip hop, BR: break dance,\nGN: GrooveNet.\nmusic context. We have shown that both of these properties are\nimportant, by comparing our model with two previously proposed\nmodels: MoGlow, a general motion synthesis model not previously\napplied to dance generation, which is based on autoregressive nor-\nmalizing flows and an LSTM context encoder [Henter et al. 2020],\nand AI Choregographer, a deterministic dance generation model\nbased on a cross-modal attention encoder [Li et al. 2021a].\nIn section 5, we found that Transflower matches the ground truth\ndistribution of poses and movements better than MoGlow and AI\nChoreographer (table 2), and is also ranked higher in naturalness and\nappropriateness to the music by human subjects (section 5.2). We\nobserve the same trend in the kinematic tempograms in fig. 3 which\ngive a more objective view on how well the motion matches the\nmusic. Comparing the two baselines, we further see that MoGlow\n(which is probabilistic, but lacks transformers) achieved a substan-\ntially better naturalness rating than the deterministic, transformer-\nbased AI Choreographer. We take this as evidence that a probabilistic\napproach was particularly important for good results in the present\nevaluation. In preliminary experiments, we found that AI Choreo-\ngrapher produced more natural motion when trained on the AIST++\ndata only than when trained on our full training set. In previous\nworks where AI Choreographer tended to reach relatively high\nscores in naturalness, the model was only trained on a single data\nsource at a time [Li et al. 2021b,a]. Taken together, this suggests\nthat the high diversity and heterogeneity of our dataset signific-\nantly degrades the performance of a deterministic model, while the\nACM Trans. Graph., Vol. 40, No. 6, Article 1. Publication date: December 2021.\n\n\n1:10\nâ€¢\nGuillermo Valle-PÃ©rez, Gustav Eje Henter, Jonas Beskow, Andre Holzapfel, Pierre-Yves Oudeyer, and Simon Alexanderson\nprobabilistic models are better able to adapt to the heterogeneity.\nThat said, random sampling from Transflower trained on on AIST++\nalone also exhibited improved motion quality. Unlike results from\nnatural language [Henighan et al. 2020], we thus did not see any\nevidence of favourable scaling behaviour when trading increased\ndataset size for greater data diversity and potentially reduced quality,\nexcept perhaps in the diversity of generated dances. However, part\nof this might be due to a difference in output-generation methods,\nwhere our strongest language models [Brown et al. 2020] bene-\nfit from sampling only among the most probable outcomes of the\nlearned distribution [Holtzman et al. 2020] (often called â€œreducing\nthe temperatureâ€), whereas our experiments sampled directly from\nthe distribution learned by the flow without temperature reduction.\nWe also evaluated the models in terms of the diversity of move-\nments found at different points in time for a single dance. In fig. 4\nwe see that although Transflower scored higher than MoGlow, the\ndifference with AI Choreographer was not significant. However,\nconsidering the low score of AI Choreographer for naturalness, the\nhigh diversity of the movements of AI Choreographer may not cor-\nrespond to meaningful diversity in terms of dance. We observed AI\nChoreographer tended to produce much more chaotic movements,\nand also was more likely to regress to a frozen pose (the latter stim-\nuli were not included in the user study). Therefore we argue that\nTransflower achieved more meaningful diversity than the baselines.\nTo explore the effect of fine-tuning the model on higher quality\ndata, we trained Transflower only on the PMSD dance data, for an\nextra 50k iterations. We found that resulted in significantly more\nnatural motion, as well as motion that was judged more appropriate\nto the music by the participants in our user study (fig. 4), compared\nto without fine-tuning. However, this improvement came at the\nexpense of reduced (within-dance) diversity in the dance, which\nprobably explains the increased FrÃ©chet distribution distances for\nthe fine-tuned model (table 2).\nIn section 5.3, we studied the effect of the autoregressive motion\nseed on the generated dance. We found that the seed had a big effect\non the style of the dance, with a tendency to make the dance more\nsimilar to the style from which the 6s motion seed was taken. We call\nthis effect â€œmotion promptingâ€ in analogy to the effect of prompting\nin language models [Reynolds and McDonell 2021]. Our results\nsuggest that this may be a new way to achieve stylistic control over\nautoregressive dance and motion models.\nIn order to drive research into better learning-based dance syn-\nthesis, that approach more human-like dance, an important piece is\nthe availability of large dance datasets. In this work, we introduce\nthe largest dataset of 3D dance movements obtained with motion\ncapture technologies. This can benefit not only learning-based ap-\nproaches, but data-driven dance synthesis in general (see section 2.2).\nWe think that the growing use of VR could offer novel opportun-\nities for research, like the one we explored here. Finally, we think\nthat investigating the most effective ways to scale models to bigger\ndatasets is an interesting direction for future work.\n7\nLIMITATIONS\nLearning-based methods for dance synthesis have certain limita-\ntions. As discussed above, they require large amounts of data, and\nmay produce less reliable and controllable results than approaches\nthat impose more structure. On the upside, they are more flexible\n(require fewer changes to apply to other tasks), and tend to produce\nmore natural results when enough data is available.\nMore specific to our model, normalizing flows show certain ad-\nvantages and limitations relative to other generative modelling meth-\nods [Bond-Taylor et al. 2021]. They allow exact likelihood maximiz-\nation, which leads to stable training, large diversity in samples, and\ngood quality results for large enough data and expressive enough\nmodels. However, they are less parameter efficient and slower to\ntrain than other approaches such as generative adversarial networks\nor variational autoencoders, at least for images [Bond-Taylor et al.\n2021]. Furthermore, the full-attention transformer encoder which\nwe use is slower to train than purely decoder-based models like\nGPT, due to being less parallelizable [Vaswani et al. 2017]. We think\nthat exploring architectures that overcome these limitations while\npreserving performance, is an important area for future work.\nFinally, we think that further work is needed in evaluation. This\nincludes comparing state-of-the-art learning-based methods like\nTransflower with current motion-graph-based methods like Choreo-\nMaster [Kang et al. 2021], in terms of naturalness, appropriateness,\nand diversity of the generated dance, as well as how these depend\non the data available. Models that include physics constraints, like\nthose discussed in section 2.1, also have shown promise in terms of\nnaturalness of the motion, and we think should be compared with\nless constrained models like Transflower in future work. We note\nthat the Transflower model we propose could be used to parametrize\nthe policy for physics-based IL algorithms, as it allows for exact\ncomputation of the log probability over the output. Even more so,\nhowever, it encompasses work towards understanding what are the\nbest metrics by which to evaluate dance synthesis models, in terms\nof reliability and relevance for downstream tasks. In particular, al-\nthough we looked at beat alignment as one of our objective metrics,\nthis is primarily due to a lack of other well-developed objective\nevaluation measures. Good dancing is not a beat-matching task, and\nprevious dance synthesis works have found that the quality of the\ndance is a big factor in determining how humans judge how well the\ndance matches the music. For example, in Li et al. [2021a], ground\ntruth dances paired with a random piece of music from the data set\nwas ranked significantly higher than any of the models, with regards\nto how well it matched the music. This may be linked to a previously\nobserved effect where people tend to find good dance moves with a\ndiversity of nested rhythms to match almost any music piece they\nare played with [Avirgan 2013; Miller et al. 2013]. This diversity\nof nested rhythms may also go some ways toward explaining the\nrelatively strong results of mismatched motion in table 3.\nThe phenomenon of rhythms at multiple levels extends to many\naspects of human and animal communication [Pouw et al. 2021].\nA similar effect as for mismatched dance has been observed in the\nrelated field of speech-driven gesture generation, where a recent\ninternational challenge [Kucherenko et al. 2021] found that mis-\nmatched ground-truth motion clips, unrelated to the speech, were\nrated as more appropriate for the speech than the best-rated syn-\nthetic system. For gestures, the disparity between matched and\nmismatched motion becomes more obvious to raters if the data con-\ntains periods of both speech and silence (during which no gesture\nACM Trans. Graph., Vol. 40, No. 6, Article 1. Publication date: December 2021.\n\n\nTransflower: probabilistic autoregressive dance generation with multimodal attention\nâ€¢\n1:11\nactivity is expected), as observed in Yoon et al. [2019]. Bringing\nthis back to dance, this might correspond to several songs with\nsilence in between, or music that exhibits extended dramatic pauses.\nMore broadly, these findings point to the quality of dance move-\nments being a more important factor for downstream applications,\nbut also suggest a need for further research on how to evaluate\ndance and dance synthesis and disentangling aspects of motion\nquality from rhythmic and stylistic appropriateness. For instance,\none could evaluate on songs with extensive pausing, to make the\ndifference between appropriate and less appropriate dancing more\npronounced. Our current models, never having seen data on silence\nor standing still during training, generate dance motion even for si-\nlent audio input, mirroring results in locomotion generation, where\nmany models find it difficult to stand still [Henter et al. 2020].\n8\nCONCLUSION\nIn conclusion, we introduced a new model for probabilistic autore-\ngressive modelling of high-dimensional continuous signals, condi-\ntioned on a multimodal context, which we applied to the problem\nof music-conditioned dance generation. We created the currently\nlargest 3D dance motion dataset, and used it to evaluate our model\nversus two representative baselines. Our results show that our model\nimproves on previous state of the art along several benchmarks, and\nthe two main features of our model, the probabilistic modelling of\nthe output, and the attention-based encoding of the inputs, are both\nnecessary to produce realistic and diverse dance that is appropriate\nfor the music.\nACKNOWLEDGMENTS\nWe thank the dancers: Mario Perez Amigo, Konata, Max and others.\nWe thank Esther Ericsson for dance and motion capture, and lox9973\nfor the VR dance recording. We thank the Syrtos dance collaborators\nStella Pashalidou, Michael Hagleitner, and Rainer and Ronja Polak.\nWe are also grateful to the reviewers for their thoughtful feedback.\nThis work benefited from access to the HPC resources of IDRIS\nunder the allocation 2020-[A0091011996] made by GENCI, using\nthe Jean Zay supercomputer. This research was partially supported\nby the Google TRC program, the Swedish Research Council pro-\njects 2018-05409 and 2019-03694, the Wallenberg AI, Autonomous\nSystems and Software Program (WASP) funded by the Knut and\nAlice Wallenberg Foundation, and Marianne and Marcus Wallen-\nberg Foundation MMW 2020.0102. Guillermo Valle-PÃ©rez benefited\nfrom funding of project â€œDeepCuriosityâ€ from RÃ©gion Aquitaine\nand Inria, France.\nREFERENCES\nOmid Alemi, Jules FranÃ§oise, and Philippe Pasquier. 2017. Groovenet: Real-time music-\ndriven dance movement generation using artificial neural networks. networks 8, 17\n(2017), 26.\nSimon Alexanderson, Gustav Eje Henter, Taras Kucherenko, and Jonas Beskow. 2020.\nStyle-controllable speech-driven gesture synthesis using normalising flows. Comput.\nGraph. Forum 39, 2 (2020), 487â€“496. https://doi.org/10.1111/cgf.13946\nOkan Arikan and David A. Forsyth. 2002. Interactive motion generation from examples.\nACM Trans. Graph. 21, 3 (2002), 483â€“490. https://doi.org/10.1145/566570.566606\nJody Avirgan. 2013. Why Spiderman is Such a Good Dancer. https://www.wnycstudios.\norg/podcasts/radiolab/articles/299399-why-spiderman-such-good-dancer\nBettina BlÃ¤sing, Beatriz Calvo-Merino, Emily S Cross, Corinne Jola, Juliane Honisch,\nand Catherine J Stevens. 2012. Neurocognitive control in dance perception and\nperformance. Acta psychologica 139, 2 (2012), 300â€“308.\nSebastian BÃ¶ck and Markus Schedl. 2011. Enhanced beat tracking with context-aware\nneural networks. In Proc. Int. Conf. Digital Audio Effects. 135â€“139.\nFederica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and\nMichael J. Black. 2016. Keep it SMPL: Automatic estimation of 3D human pose and\nshape from a single image. In European Conference on Computer Vision. Springer,\n561â€“578.\nSam Bond-Taylor, Adam Leach, Yang Long, and Chris G Willcocks. 2021. Deep Gen-\nerative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows,\nEnergy-Based and Autoregressive Models. arXiv preprint arXiv:2103.04922 (2021).\nAndrew Brock, Jeff Donahue, and Karen Simonyan. 2018. Large Scale GAN Training\nfor High Fidelity Natural Image Synthesis. In International Conference on Learning\nRepresentations.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\nSandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Re-\nwon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris\nHesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. 2020.\nLanguage Models are Few-Shot Learners. In Proc.\nNeurIPS, Vol. 33. 1877â€“1901.\nhttps://proceedings.neurips.cc/paper/2020/file/\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\nBirgitta Burger, Marc R Thompson, Geoff Luck, Suvi Saarikallio, and Petri Toiviainen.\n2013. Influences of rhythm-and timbre-related musical features on characteristics\nof music-induced movement. Frontiers in Psychology 4, Article 183 (2013), 10 pages.\nhttps://doi.org/10.3389/fpsyg.2013.00183\nJudith BÃ¼tepage, Michael J. Black, Danica Kragic, and Hedvig KjellstrÃ¶m. 2017. Deep\nrepresentation learning for human motion prediction and classification. In Pro-\nceedings of the IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPRâ€™17). IEEE Computer Society, Los Alamitos, CA, USA, 1591â€“1599.\nhttps://doi.org/10.1109/CVPR.2017.173\nShih-Pin Chao, Chih-Yi Chiu, Jui-Hsiang Chao, Shi-Nine Yang, and T-K Lin. 2004. Mo-\ntion retrieval and its application to motion synthesis. In 24th International Conference\non Distributed Computing Systems Workshops. IEEE, 254â€“259.\nCMU Graphics Lab. 2003. Carnegie Mellon University motion capture database. http:\n//mocap.cs.cmu.edu/\nLuka Crnkovic-Friis and Louise Crnkovic-Friis. 2016. Generative choreography using\ndeep learning. arXiv preprint arXiv:1605.06921 (2016).\nAbe Davis and Maneesh Agrawala. 2018. Visual rhythm and beat. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition Workshops. 2532â€“2535.\nPrafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and\nIlya Sutskever. 2020.\nJukebox: A generative model for music.\narXiv preprint\narXiv:2005.00341 (2020).\nChris Donahue, Zachary C Lipton, and Julian McAuley. 2017. Dance dance convolution.\nIn International conference on machine learning. PMLR, 1039â€“1048.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua\nZhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image\nrecognition at scale. arXiv preprint arXiv:2010.11929 (2020).\nRukun Fan, Songhua Xu, and Weidong Geng. 2011. Example-based automatic music-\ndriven conventional dance motion synthesis. IEEE transactions on visualization and\ncomputer graphics 18, 3 (2011), 501â€“515.\nYlva Ferstl and Rachel McDonnell. 2018. IVA: Investigating the use of recurrent motion\nmodelling for speech gesture generation. In IVA â€™18 Proceedings of the 18th Interna-\ntional Conference on Intelligent Virtual Agents. https://trinityspeechgesture.scss.tcd.\nie\nKaterina Fragkiadaki, Sergey Levine, Panna Felsen, and Jitendra Malik. 2015. Recurrent\nnetwork models for human dynamics. In Proceedings of the IEEE International Con-\nference on Computer Vision (ICCVâ€™15). IEEE Computer Society, Los Alamitos, CA,\nUSA, 4346â€“4354. https://doi.org/10.1109/ICCV.2015.494\nSatoru Fukayama and Masataka Goto. 2015. Music content driven automated choreo-\ngraphy with beat-wise motion connectivity constraints. Proceedings of SMC (2015),\n177â€“183.\nF. Sebastian Grassia. 1998. Practical parameterization of rotations using the exponential\nmap. J. Graph. Tools 3, 3 (1998), 29â€“48.\nhttps://doi.org/10.1080/10867651.1998.\n10487493\nKeith Grochow, Steven L. Martin, Aaron Hertzmann, and Zoran PopoviÄ‡. 2004. Style-\nbased inverse kinematics. ACM Trans. Graph. 23, 3 (2004), 522â€“531. https://doi.org/\n10.1145/1015706.1015755\nIkhansul Habibie, Daniel Holden, Jonathan Schwarz, Joe Yearsley, and Taku Komura.\n2017. A recurrent variational autoencoder for human motion synthesis. In Proceed-\nings of the British Machine Vision Conference (BMVCâ€™17). BMVA Press, Durham, UK,\nArticle 119, 12 pages. https://doi.org/10.5244/C.31.119\nIkhsanul Habibie, Weipeng Xu, Dushyant Mehta, Lingjie Liu, Hans-Peter Seidel, Gerard\nPons-Moll, Mohamed Elgharib, and Christian Theobalt. 2021. Learning Speech-\ndriven 3D Conversational Gestures from Video. arXiv preprint arXiv:2102.06837\n(2021).\nACM Trans. Graph., Vol. 40, No. 6, Article 1. Publication date: December 2021.\n\n\n1:12\nâ€¢\nGuillermo Valle-PÃ©rez, Gustav Eje Henter, Jonas Beskow, Andre Holzapfel, Pierre-Yves Oudeyer, and Simon Alexanderson\nMari Romarheim Haugen. 2014. Studying rhythmical structures in Norwegian folk\nmusic and dance using motion capture technology: A case study of Norwegian\ntelespringar. Musikk og Tradisjon 28 (2014), 27â€“52.\nTom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson,\nHeewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. 2020. Scaling laws\nfor autoregressive generative modeling. arXiv preprint arXiv:2010.14701 (2020).\nGustav Eje Henter, Simon Alexanderson, and Jonas Beskow. 2020. Moglow: Probabilistic\nand controllable motion synthesis using normalising flows. ACM Transactions on\nGraphics (TOG) 39, 6 (2020), 1â€“14.\nJonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. 2019. Flow++:\nImproving flow-based generative models with variational dequantization and archi-\ntecture design. In International Conference on Machine Learning. PMLR, 2722â€“2730.\nDaniel Holden, Oussama Kanoun, Maksym Perepichka, and Tiberiu Popa. 2020. Learned\nmotion matching. ACM Trans. Graph. 39, 4 (2020), 53â€“1.\nDaniel Holden, Taku Komura, and Jun Saito. 2017. Phase-functioned neural networks\nfor character control. ACM Trans. Graph. 36, 4, Article 42 (2017), 13 pages. https:\n//doi.org/10.1145/3072959.3073663\nDaniel Holden, Jun Saito, and Taku Komura. 2016. A deep learning framework for\ncharacter motion synthesis and editing. ACM Trans. Graph. 35, 4, Article 138 (2016),\n11 pages. https://doi.org/10.1145/2897824.2925975\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The Curious Case\nof Neural Text Degeneration. In International Conference on Learning Representations.\nAndre Holzapfel, Michael Hagleitner, and Stella Pashalidou. 2020. Diversity of Tradi-\ntional Dance Expression in Crete: Data Collection, Research Questions, and Method\nDevelopment. In Proceedings of the ICTM Study Group on Sound, Movement, and the\nSciences Symposium.\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis\nHawthorne, Noam Shazeer, Andrew M Dai, Matthew D Hoffman, Monica Dinculescu,\nand Douglas Eck. 2018. Music Transformer: Generating Music with Long-Term\nStructure. In International Conference on Learning Representations.\nChen Kang, Zhipeng Tan, Jin Lei, Song-Hai Zhang, Yuan-Chen Guo, Weidong Zhang,\nand Shi-Min Hu. 2021. ChoreoMaster: Choreography-oriented Music-driven Dance\nSynthesis. (2021). https://www.youtube.com/watch?v=V8MlYa_yhF0 accepted for\npublication at SIGGRAPH 2021.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon\nChild, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws\nfor neural language models. arXiv preprint arXiv:2001.08361 (2020).\nTero Karras, Samuli Laine, and Timo Aila. 2019. A style-based generator architecture\nfor generative adversarial networks. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. 4401â€“4410.\nDiederik P. Kingma and Prafulla Dhariwal. 2018. Glow: Generative flow with invertible\n1x1 convolutions. In Advances in Neural Information Processing Systems (NeurIPSâ€™18).\nCurran Associates, Inc., Red Hook, NY, USA, 10236â€“10245. http://papers.nips.cc/\npaper/8224-glow-generative-flow-with-invertible-1x1-con\nLucas Kovar and Michael Gleicher. 2004. Automated extraction and parameterization\nof motions in large data sets. ACM Trans. Graph. 23, 3 (2004), 559â€“568.\nhttps:\n//doi.org/10.1145/1015706.1015760\nLucas Kovar, Michael Gleicher, and FrÃ©dÃ©ric Pighin. 2002. Motion graphs. ACM Trans.\nGraph. 21, 3 (2002), 473â€“482. https://doi.org/10.1145/566654.566605\nFlorian Krebs, Sebastian BÃ¶ck, and Gerhard Widmer. 2015. An Efficient State-Space\nModel for Joint Tempo and Meter Tracking.. In ISMIR. 72â€“78.\nTaras Kucherenko, Patrik Jonell, Youngwoo Yoon, Pieter Wolfert, and Gustav Eje\nHenter. 2021. A Large, Crowdsourced Evaluation of Gesture Generation Systems\non Common Data: The GENEA Challenge 2020. In 26th International Conference on\nIntelligent User Interfaces (College Station, TX, USA) (IUI â€™21). ACM, New York, NY,\nUSA, 11â€“21. https://doi.org/10.1145/3397481.3450692\nOxAI Labs. 2019. DeepSaber. https://github.com/oxai/deepsaber/.\nKimerer LaMothe. 2019. The dancing species: how moving together in time helps\nmake us human. Aeon (June 2019). https://aeon.co/ideas/the-dancing-species-how-\nmoving-together-in-time-helps-make-us-human\nBen Lang. 2021. The Future is Now: Live Breakdance Battles in VR Are Connecting\nPeople Across the Globe.\nhttps://www.roadtovr.com/vr-dance-battle-vrchat-\nbreakdance/\nGilwoo Lee, Zhiwei Deng, Shugao Ma, Takaaki Shiratori, Siddhartha S Srinivasa, and\nYaser Sheikh. 2019a. Talking with hands 16.2 m: A large-scale dataset of synchronized\nbody-finger motion and audio for conversational motion analysis and synthesis. In\nProceedings of the IEEE/CVF International Conference on Computer Vision. 763â€“772.\nHsin-Ying Lee, Xiaodong Yang, Ming-Yu Liu, Ting-Chun Wang, Yu-Ding Lu, Ming-\nHsuan Yang, and Jan Kautz. 2019b. Dancing to music. arXiv preprint arXiv:1911.02001\n(2019).\nJehee Lee, Jinxiang Chai, Paul S. A. Reitsma, Jessica K. Hodgins, and Nancy S. Pollard.\n2002. Interactive control of avatars animated with human motion data. ACM Trans.\nGraph. 21, 3 (2002), 491â€“500. https://doi.org/10.1145/566654.566607\nSergey Levine, Jack M. Wang, Alexis Haraux, Zoran PopoviÄ‡, and Vladlen Koltun. 2012.\nContinuous character control with low-dimensional embeddings. ACM Trans. Graph.\n31, 4, Article 28 (2012), 10 pages. https://doi.org/10.1145/2185520.2185524\nBuyu Li, Yongchi Zhao, and Lu Sheng. 2021b. DanceNet3D: Music Based Dance Gener-\nation with Parametric Motion Transformer. arXiv preprint arXiv:2103.10206 (2021).\nJiaman Li, Yihang Yin, Hang Chu, Yi Zhou, Tingwu Wang, Sanja Fidler, and Hao Li.\n2020. Learning to Generate Diverse Dance Motions with Transformer. arXiv preprint\narXiv:2008.08171 (2020).\nRuilong Li, Shan Yang, David A Ross, and Angjoo Kanazawa. 2021a. Learn to Dance with\nAIST++: Music Conditioned 3D Dance Generation. arXiv preprint arXiv:2101.08779\n(2021).\nHung Yu Ling, Fabio Zinno, George Cheng, and Michiel van de Panne. 2020. Character\ncontrollers using motion VAEs. ACM Trans. Graph. 39, 4, Article 40 (2020), 12 pages.\nhttps://doi.org/10.1145/3386569.3392422\nlox9973. 2021. ShaderMotion. https://gitlab.com/lox9973/ShaderMotion.\nNaureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll, and Michael J.\nBlack. 2019. AMASS: Archive of Motion Capture as Surface Shapes. In International\nConference on Computer Vision. 5442â€“5451.\nChristian Mandery, Ã–mer Terlemez, Martin Do, Nikolaus Vahrenkamp, and Tamim\nAsfour. 2015. The KIT whole-body human motion database. In 2015 International\nConference on Advanced Robotics (ICAR). IEEE, 329â€“336.\nAlexander Mathis, Steffen Schneider, Jessy Lauer, and Mackenzie Weygandt Mathis.\n2020. A primer on motion capture with deep learning: principles, pitfalls, and\nperspectives. Neuron 108, 1 (2020), 44â€“65.\nJosh Merel, Yuval Tassa, Sriram Srinivasan, Jay Lemmon, Ziyu Wang, Greg Wayne, and\nNicolas Heess. 2017. Learning human behaviors from motion capture by adversarial\nimitation. arXiv preprint arXiv:1707.02201 (2017).\nJared E Miller, Laura A Carlson, and J Devin McAuley. 2013. When what you hear\ninfluences when you see: listening to an auditory rhythm influences the temporal\nallocation of visual attention. Psychological science 24, 1 (2013), 11â€“18.\nOlof Misgeld, Andre Holzapfel, and Sven AhlbÃ¤ck. 2019. Dancing Dots â€“ Investigating\nthe Link between Dancer and Musician in Swedish Folk Dance. In Sound & Music\nComputing Conference.\nLuiz Naveda and Marc Leman. 2011. Hypotheses on the choreographic roots of the\nmusical meter: a case study on Afro-Brazilian dance and music. Debates actuales en\nevoluciÃ³n, desarrollo y cogniciÃ³n e implicancias socio-culturales (2011), 477â€“495.\nGeorge Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and\nBalaji Lakshminarayanan. 2021. Normalizing Flows for Probabilistic Modeling\nand Inference. Journal of Machine Learning Research 22, 57 (2021), 1â€“64.\nhttp:\n//jmlr.org/papers/v22/19-1028.html\nTaesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. 2019. Semantic image\nsynthesis with spatially-adaptive normalization. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 2337â€“2346.\nDario Pavllo, David Grangier, and Michael Auli. 2018. QuaterNet: A quaternion-based\nrecurrent model for human motion. In Proceedings of the British Machine Vision\nConference (BMVCâ€™18). BMVA Press, Durham, UK, 14 pages. http://www.bmva.org/\nbmvc/2018/contents/papers/0675.pdf\nXue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. 2018a. Deep-\nMimic: Example-guided deep reinforcement learning of physics-based character\nskills. ACM Trans. Graph. 37, 4 (2018), 1â€“14.\nXue Bin Peng, Angjoo Kanazawa, Jitendra Malik, Pieter Abbeel, and Sergey Levine.\n2018b. Sfv: Reinforcement learning of physical skills from videos. ACM Transactions\nOn Graphics (TOG) 37, 6 (2018), 1â€“14.\nXue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa. 2021. AMP:\nAdversarial Motion Priors for Stylized Physics-Based Character Control. ACM Trans.\nGraph. 40, 4, Article 1 (July 2021), 15 pages. https://doi.org/10.1145/3450626.3459670\nMathis Petrovich, Michael J Black, and GÃ¼l Varol. 2021. Action-Conditioned 3D Human\nMotion Synthesis with Transformer VAE. arXiv preprint arXiv:2104.05670 (2021).\nWim Pouw, Shannon Proksch, Linda Drijvers, Marco Gamba, Judith Holler, Christopher\nKello, Rebecca S. Schaefer, and Geraint A. Wiggins. 2021. Multilevel rhythms in\nmultimodal communication. Philosophical Transactions of the Royal Society B 376,\n1835 (2021), 20200334. https://doi.org/10.1098/rstb.2020.0334\nRyan Prenger, Rafael Valle, and Bryan Catanzaro. 2019. WaveGlow: A flow-based\ngenerative network for speech synthesis. In Proceedings of the IEEE International\nConference on Acoustics, Speech, and Signal Processing (ICASSPâ€™19). IEEE Signal\nProcessing Society, Piscataway, NJ, USA, 3617â€“3621. https://doi.org/10.1109/ICASSP.\n2019.8683143\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer\nlearning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683\n(2019).\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford,\nMark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation. arXiv\npreprint arXiv:2102.12092 (2021).\nLaria Reynolds and Kyle McDonell. 2021. Prompt programming for large language\nmodels: Beyond the few-shot paradigm. arXiv preprint arXiv:2102.07350 (2021).\nYu Rong, Takaaki Shiratori, and Hanbyul Joo. 2021. FrankMocap: A Monocular 3D\nWhole-Body Pose Estimation System via Regression and Integration. In IEEE Inter-\nnational Conference on Computer Vision Workshops.\nACM Trans. Graph., Vol. 40, No. 6, Article 1. Publication date: December 2021.\n\n\nTransflower: probabilistic autoregressive dance generation with multimodal attention\nâ€¢\n1:13\nAlla Safonova and Jessica K. Hodgins. 2007. Construction and Optimal Search of\nInterpolated Motion Graphs. ACM Trans. Graph. 26, 3 (July 2007), 106â€“es. https:\n//doi.org/10.1145/1276377.1276510\nJonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng\nYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, Yannis\nAgiomyrgiannakis, and Yonghui Wu. 2018. Natural TTS synthesis by conditioning\nWaveNet on mel spectrogram predictions. In Proceedings of the IEEE International\nConference on Acoustics, Speech, and Signal Processing (ICASSPâ€™18). IEEE Signal\nProcessing Society, Piscataway, NJ, USA, 4799â€“4783. https://doi.org/10.1109/ICASSP.\n2018.8461368\nSebastian Starke, Yiwei Zhao, Taku Komura, and Kazi Zaman. 2020. Local motion\nphases for learning multi-contact character movements. ACM Trans. Graph. 39, 4,\nArticle 54 (2020), 14 pages. https://doi.org/10.1145/3386569.3392450\nStatista. 2020. Augmented reality (AR) and virtual reality (VR) headset shipments world-\nwide from 2020 to 2025.\nhttps://www.statista.com/statistics/653390/worldwide-\nvirtual-and-augmented-reality-headset-shipments/\nWataru Takano, Katsu Yamane, and Yoshihiro Nakamura. 2010. Retrieval and Generation\nof Human Motions Based on Associative Model between Motion Symbols and\nMotion Labels. Proceedings of Journal of the Robotics Society of Japan 28, 6 (2010),\n723â€“734.\nTaoran Tang, Jia Jia, and Hanyang Mao. 2018.\nDance with melody: An LSTM-\nautoencoder approach to music-oriented dance synthesis. In Proceedings of the\n26th ACM International Conference on Multimedia. 1598â€“1606.\nPetri Toiviainen, Geoff Luck, and Marc R Thompson. 2010. Embodied meter: hierarchical\neigenmodes in music-induced movement. Music Perception 28, 1 (2010), 59â€“70.\nNikolaus F Troje. 2002. Decomposing biological motion: A framework for analysis and\nsynthesis of human gait patterns. Journal of vision 2, 5 (2002), 2â€“2.\nShuhei Tsuchida, Satoru Fukayama, Masahiro Hamasaki, and Masataka Goto. 2019.\nAIST Dance Video Database: Multi-genre, Multi-dancer, and Multi-camera Database\nfor Dance Information Processing. In Proceedings of the 20th International Society\nfor Music Information Retrieval Conference, ISMIR 2019. Delft, Netherlands, 501â€“510.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\nGomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In\nAdvances in Neural Information Processing Systems (NIPSâ€™17). Curran Associates,\nInc., Red Hook, NY, USA, 5998â€“6008. https://papers.nips.cc/paper/7181-attention-\nis-all-you-need\nJack M. Wang, David J. Fleet, and Aaron Hertzmann. 2008. Gaussian process dynamical\nmodels for human motion. IEEE T. Pattern Anal. 30, 2 (2008), 283â€“298.\nhttps:\n//doi.org/10.1109/TPAMI.2007.1167\nUlme Wennberg and Gustav Eje Henter. 2021. The Case for Translation-Invariant Self-\nAttention in Transformer-Based Language Models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing (Volume 2: Short Papers) (ACL â€™21).\nACL, 130â€“140. https://doi.org/10.18653/v1/2021.acl-short.18\nChenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro,\nand Nan Duan. 2021. GODIVA: Generating Open-DomaIn Videos from nAtural\nDescriptions. arXiv preprint arXiv:2104.14806 (2021).\nZijie Ye, Haozhe Wu, Jia Jia, Yaohua Bu, Wei Chen, Fanbo Meng, and Yanfeng Wang.\n2020. ChoreoNet: Towards Music to Dance Synthesis with Choreographic Action\nUnit. In Proceedings of the 28th ACM International Conference on Multimedia. 744â€“\n752.\nYoungwoo Yoon, Woo-Ri Ko, Minsu Jang, Jaeyeon Lee, Jaehong Kim, and Geehyuk Lee.\n2019. Robots learn social skills: End-to-end learning of co-speech gesture generation\nfor humanoid robots. In Proceedings of the IEEE International Conference on Robotics\nand Automation (ICRAâ€™19). IEEE Robotics and Automation Society, Piscataway, NJ,\nUSA, 4303â€“4309. https://doi.org/10.1109/ICRA.2019.8793720\nYi Zhou, Zimo Li, Shuangjiu Xiao, Chong He, Zeng Huang, and Hao Li. 2018. Auto-\nconditioned recurrent networks for extended complex human motion synthesis. In\nProceedings of the International Conference on Learning Representations (ICLRâ€™18).\n13 pages. https://openreview.net/forum?id=r11Q2SlRW\nWenlin Zhuang, Congyi Wang, Siyu Xia, Jinxiang Chai, and Yangang Wang. 2020.\nMusic2Dance: DanceNet for Music-driven Dance Generation. arXiv e-prints (2020),\narXivâ€“2002.\nA\nMODEL DETAILS\nWe give the main architecture hyperparameter details in table 5.\nThe hyperparameters for the AI Choreographer architecture were\nchosen to match those in the original AI Choreographer [Li et al.\n2021a], with the only difference that we used T5-style relative posi-\ntional embeddings, and that we used positional embeddings in the\ncross-modal transformer, as we found these gave slightly better con-\nvergence. The hyperparameters for the transformer encoders and\ncross-modal transformer in Transflower are identical to those of AI\nChoreographer, while the normalizing flow parameters are the same\nas in MoGlow, except for the affine coupling layers for which we\nuse 2-layer transformers. The hyperparameters for MoGlow were\nchosen to be similar to the original implementation in Henter et al.\n[2020], except that we increased the concatenated context length\nfed at each time step, from 10 frames in the original MoGlow, to\n40 frames of motion and 50 frames of music (40 in the past, 10 in\nthe future). This makes the concatenated input to the LSTM have\na dimension of 85 Ã— 50 + 67 Ã— 40 = 6930 which accounts for the\nsignificant parameter increase of the model. We did this change\nbecause we found that an increased concatenated context length\nfor MoGlow was necessary for it to produce good results.\nModel\n#Params\nğ¿ğ‘šğ‘œ\nğ¿ğ‘šğ‘¢\nğ¿ğ‘ğ‘š\nğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™\nğ¾\nğ¿ğ‘ğ‘\nğ¿ğ‘™ğ‘ ğ‘¡ğ‘š\nğ‘˜ğ‘¥\nğ‘˜ğ‘š\nğ‘™ğ‘š\nAI Choreographer\n64M\n2\n2\n12\n800\nN/A\nN/A\nN/A\n120\n120\n20\nMoGlow\n281M\nN/A\nN/A\nN/A\nN/A\n16\nN/A\n2\n120\n120\n20\nTransflower\n122M\n2\n2\n12\n800\n16\n2\nN/A\n120\n120\n20\nTable 5. Basic architecture hyperparameters for the different models. ğ¿ğ‘šğ‘œ, ğ¿ğ‘šğ‘¢, ğ¿ğ‘ğ‘š, ğ¿ğ‘ğ‘, ğ¿ğ‘™ğ‘ ğ‘¡ğ‘šare the number of layers in the motion encoder\ntransformer, the music encoder transformer, the cross-modal transformer, the affine coupling layer, and the LSTM respectively. ğ¾is the number of blocks in\nthe normalizing flow, and ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™is the latent dimension of the encoder transformers.\nACM Trans. Graph., Vol. 40, No. 6, Article 1. Publication date: December 2021.\n"
}