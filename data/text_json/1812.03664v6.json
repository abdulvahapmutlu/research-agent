{
  "filename": "1812.03664v6.pdf",
  "num_pages": 18,
  "pages": [
    "Few-Shot Learning via Embedding Adaptation with Set-to-Set Functions\nHan-Jia Ye*\nNanjing University\nyehj@lamda.nju.edu.cn\nHexiang Hu\nUSC\nhexiangh@usc.edu\nDe-Chuan Zhan\nNanjing University\nzhandc@lamda.nju.edu.cn\nFei Sha†\nUSC & Google\nfsha@google.com\nAbstract\nLearning with limited data is a key challenge for vi-\nsual recognition. Many few-shot learning methods address\nthis challenge by learning an instance embedding function\nfrom seen classes and apply the function to instances from\nunseen classes with limited labels. This style of transfer\nlearning is task-agnostic: the embedding function is not\nlearned optimally discriminative with respect to the unseen\nclasses, where discerning among them leads to the tar-\nget task. In this paper, we propose a novel approach to\nadapt the instance embeddings to the target classiﬁcation\ntask with a set-to-set function, yielding embeddings that are\ntask-speciﬁc and are discriminative. We empirically investi-\ngated various instantiations of such set-to-set functions and\nobserved the Transformer is most effective — as it naturally\nsatisﬁes key properties of our desired model. We denote this\nmodel as FEAT (few-shot embedding adaptation w/ Trans-\nformer) and validate it on both the standard few-shot classi-\nﬁcation benchmark and four extended few-shot learning set-\ntings with essential use cases, i.e., cross-domain, transduc-\ntive, generalized few-shot learning, and low-shot learning.\nIt archived consistent improvements over baseline models\nas well as previous methods, and established the new state-\nof-the-art results on two benchmarks.\n1. Introduction\nFew-shot visual recognition [10, 23, 24, 27, 49] emerged\nas a promising direction in tackling the challenge of learn-\ning new visual concepts with limited annotations.\nCon-\ncretely, it distinguishes two sets of visual concepts: SEEN\nand UNSEEN ones. The target task is to construct visual\nclassiﬁers to identify classes from the UNSEEN where each\nclass has a very small number of exemplars (“few-shot”).\nThe main idea is to discover transferable visual knowl-\nedge in the SEEN classes, which have ample labeled in-\nstances, and leverage it to construct the desired classiﬁer.\nFor example, state-of-the-art approaches for few-shot learn-\n*Work mostly done when the author was a visiting scholar at USC.\n†On leave from USC\ning [40, 43, 46, 49] usually learn a discriminative instance\nembedding model on the SEEN categories, and apply it to\nvisual data in UNSEEN categories. In this common embed-\nding space, non-parametric classiﬁers (e.g., nearest neigh-\nbors) are then used to avoid learning complicated recogni-\ntion models from a small number of examples.\nSuch approaches suffer from one important limitation.\nAssuming a common embedding space implies that the dis-\ncovered knowledge – discriminative visual features – on\nthe SEEN classes are equally effective for any classiﬁcation\ntasks constructed for an arbitrary set of UNSEEN classes. In\nconcrete words, suppose we have two different target tasks:\ndiscerning “cat” versus “dog” and discerning “cat” versus\n“tiger”. Intuitively, each task uses a different set of discrim-\ninative features. Thus, the most desired embedding model\nﬁrst needs to be able to extract discerning features for either\ntask at the same time. This could be a challenging aspect in\nits own right as the current approaches are agnostic to what\nthose “downstream” target tasks are and could accidentally\nde-emphasize selecting features for future use. Secondly,\neven if both sets of discriminative features are extracted,\nthey do not necessarily lead to the optimal performance for\na speciﬁc target task. The most useful features for discern-\ning “cat” versus “tiger” could be irrelevant and noise to the\ntask of discerning “cat” versus “dog”!\nWhat is missing from the current few-shot learning ap-\nproaches is an adaptation strategy that tailors the visual\nknowledge extracted from the SEEN classes to the UNSEEN\nones in a target task. In other words, we desire separate em-\nbedding spaces where each one of them is customized such\nthat the visual features are most discriminative for a given\ntask. Towards this, we propose a few-shot model-based em-\nbedding adaptation method that adjusts the instance embed-\nding models derived from the SEEN classes. Such model-\nbased embedding adaptation requires a set-to-set function: a\nfunction mapping that takes all instances from the few-shot\nsupport set and outputs the set of adapted support instance\nembeddings, with elements in the set co-adapting with each\nother. Such output embeddings are then assembled as the\nprototypes for each visual category and serve as the near-\nest neighbor classiﬁers.\nFigure 1 qualitatively illustrates\n1\narXiv:1812.03664v6  [cs.LG]  13 Jun 2021\n",
    "Malamute\nAnt\nSchool bus\nGolden retriever\nTheater curtain\nAdaptation\nLion\nSchool bus\nHourglass\nVase\nTrifle\nAdaptation\nTrifle\nScoreboard\nGolden retriever\nDalmatian\nVase\nAdaptation\nGolden retriever\nNematode\nLion\nDalmatian\nMalamute\nAdaptation\n(a) Acc↑: 40.33% →55.33%\n(b) Acc↑: 48.00% →69.60%\n(c) Acc↑: 43.60% →63.33%\n(d) Acc↓: 56.33% →47.13%\nFigure 1: Qualitative visualization of model-based embedding adaptation procedure (implemented using FEAT) on test tasks (refer to\n§ 5.2.2 for more details). Each ﬁgure shows the locations of PCA projected support embeddings (class prototypes) before and after the\nadaptation of FEAT. Values below are the 1-shot 5-way classiﬁcation accuracy before and after the the adaptation. Interestingly, the\nembedding adaptation step of FEAT pushes the support embeddings apart from the clutter and toward their own clusters, such that they can\nbetter ﬁts the test data of its categories. (Best view in colors!)\nthe embedding adaptation procedure (as results of our best\nmodel). These class prototypes spread out in the embedding\nspace toward the samples cluster of each category, indicat-\ning the effectiveness of embedding adaptation.\nIn this paper, we implement the set-to-set transformation\nusing a variety of function approximators, including bidi-\nrectional LSTM [16] (Bi-LSTM), deep sets [56], graph con-\nvolutional network (GCN) [21], and Transformer [29, 47].\nOur experimental results (refer to § 5.2.1) suggest that\nTransformer is the most parameter efﬁcient choice that at\nthe same time best implements the key properties of the de-\nsired set-to-set transformation, including contextualization,\npermutation invariance, interpolation and extrapolation ca-\npabilities (see § 4.1). As a consequence, we choose the\nset-to-set function instantiated with Transformer to be our\nﬁnal model and denote it as FEAT (Few-shot Embedding\nAdaptation with Transformer). We further conduct compre-\nhensive analysis on FEAT and evaluate it on many extended\ntasks, including few-shot domain generalization, transduc-\ntive few-shot learning, and generalized few-shot learning.\nOur overall contribution is three-fold.\n• We formulate the few-shot learning as a model-based em-\nbedding adaptation to make instance embeddings task-\nspeciﬁc, via using a set-to-set transformation.\n• We instantiate such set-to-set transformation with various\nfunction approximators, validating and analyzing their\nfew-shot learning ability, task interpolation ability, and\nextrapolation ability, etc. It concludes our model (FEAT)\nthat uses the Transformer as the set-to-set function.\n• We evaluate our FEAT model on a variety of extended\nfew-shot learning tasks, where it achieves superior per-\nformances compared with strong baseline approaches.\n2. Related Work\nMethods speciﬁcally designed for few-shot learning fall\nbroadly into two categories. The ﬁrst is to control how a\nclassiﬁer for the target task should be constructed.\nOne\nfruitful idea is the meta-learning framework where the\nclassiﬁers are optimized in anticipation that a future up-\ndate due to data from a new task performs well on that\ntask [2, 3, 10, 13, 26, 32, 36, 40], or the classiﬁer itself is\ndirectly meta-predicted by the new task data [35, 53].\nAnother line of approach has focused on learning gener-\nalizable instance embeddings [1, 5, 6, 17, 22, 31, 42, 46, 49]\nand uses those embeddings on simple classiﬁers such as\nnearest neighbor rules. The key assumption is that the em-\nbeddings capture all necessarily discriminative representa-\ntions of data such that simple classiﬁers are sufﬁced, hence\navoiding the danger of overﬁtting on a small number of la-\nbeled instances. Early work such as [22] ﬁrst validated the\nimportance of embedding in one-shot learning, whilst [49]\nproposes to learn the embedding with a soft nearest neigh-\nbor objective, following a meta-learning routine. Recent\nadvances have leveraged different objective functions for\nlearning such embedding models, e.g., considering the class\nprototypes [43], decision ranking [46], and similarity com-\nparison [45]. Most recently, [41] utilizes the graph convo-\nlution network [21] to unify the embedding learning.\nOur work follows the second school of thoughts. The\nmain difference is that we do not assume the embed-\ndings learned on SEEN classes, being agnostic to the tar-\nget tasks, are necessarily discriminative for those tasks.\nIn contrast, we propose to adapt those embeddings for\neach target task with a set-to-set function so that the trans-\nformed embeddings are better aligned with the discrimina-\ntion needed in those tasks. We show empirically that such\ntask-speciﬁc embeddings perform better than task-agnostic\nones.\nMetaOptNet [25] and CTM [28] follow the same\nspirit of learning task-speciﬁc embedding (or classiﬁers) via\neither explicitly optimization of target task or using concen-\ntrator and projector to make distance metric task-speciﬁc.\n2\n",
    "Classification \nScores\nCNN\nCNN\nCNN\nCNN\nSoft Nearest \nNeighbor\n(a) Instance Embedding\nClassification \nScores\nEmbedding \nAdaptation\nCNN\nCNN\nCNN\nCNN\nSoft Nearest \nNeighbor\nTrain Instance\nTest Instance\nTask Agnostic\nEmbedding\nTask Specific \nEmbedding\n(b) Embedding Adaptation\nSet-to-Set Function\nFigure 2:\nIllustration of the proposed Few-Shot Embedding\nAdaptation Transformer (FEAT). Existing methods usually use the\nsame embedding function E for all tasks. We propose to adapt the\nembeddings to each target few-shot learning task with a set-to-set\nfunction such as Transformer, BiLSTM, DeepSets, and GCN.\n3. Learning Embedding for Task-agnostic FSL\nIn the standard formulation of few-shot learning\n(FSL) [10, 49], a task is represented as a M-shot N-way\nclassiﬁcation problem with N classes sampled from a set\nof visual concepts U and M (training/support) examples\nper class.\nWe denote the training set (also referred as\nsupport sets in the literature) as Dtrain = {xi, yi}NM\ni=1 ,\nwith the instance xi ∈RD and the one-hot labeling vec-\ntor yi ∈{0, 1}N. We will use “support set” and “train-\ning set” interchangeably in the paper. In FSL, M is of-\nten small (e.g., M = 1 or M = 5).\nThe goal is to\nﬁnd a function f that classiﬁes a test instance xtest by\nˆytest = f(xtest; Dtrain) ∈{0, 1}N.\nGiven a small number of training instances, it is chal-\nlenging to construct complex classiﬁers f(·). To this end,\nthe learning algorithm is also supplied with additional data\nconsisting of ample labeled instances. These additional data\nare drawn from visual classes S, which does not overlap\nwith U. We refer to the original task as the target task which\ndiscerns N UNSEEN classes U. To avoid confusion, we de-\nnote the data from the SEEN classes S as DS.\nTo learn f(·) using DS, we synthesize many M-shot N-\nway FSL tasks by sampling the data in the meta-learning\nmanner [10, 49]. Each sampling gives rise to a task to clas-\nsify a test set instance xS\ntest into one of the N SEEN classes\nby f(·), where the test instances set DS\ntest is composed of\nthe labeled instances with the same distribution as DS\ntrain.\nFormally, the function f(·) is learnt to minimize the aver-\naged error over those sampled tasks\nf ∗= arg min\nf\nX\n(xS\ntest,yS\ntest)∈DS\ntest\nℓ(f(xS\ntest; DS\ntrain), yS\ntest)\n(1)\nwhere the loss ℓ(·) measures the discrepancy between the\nprediction and the true label. For simplicity, we have as-\nsumed we only synthesize one task with test set DS\ntest. The\noptimal f ∗is then applied to the original target task.\nWe consider the approach based on learning embeddings\nAlgorithm 1 Training strategy of embedding adaptation\nRequire: Seen class set S\n1: for all iteration = 1,...,MaxIteration do\n2:\nSample N-way M-shot (DS\ntrain, DS\ntest) from S\n3:\nCompute φx = E(x), for x ∈X S\ntrain ∪X S\ntest\n4:\nfor all (xS\ntest, yS\ntest) ∈DS\ntest do\n5:\nCompute {ψx ; ∀x ∈X S\ntrain} with T via Eq. 3\n6:\nPredict ˆyS\ntest with {ψx} as Eq. 4\n7:\nCompute ℓ(ˆyS\ntest, yS\ntest) with Eq. 1\n8:\nend for\n9:\nCompute ∇E,T\nP\n(xS\ntest,yS\ntest)∈DS\ntest ℓ(ˆyS\ntest, yS\ntest)\n10:\nUpdate E and T with ∇E,T use SGD\n11: end for\n12: return Embedding function E and set function T.\nfor FSL [43, 49] (see Figure 2 (a) for an overview). In par-\nticular, the classiﬁer f(·) is composed of two elements. The\nﬁrst is an embedding function φx = E(x) ∈Rd that maps\nan instance x to a representation space. The second compo-\nnent applies the nearest neighbor classiﬁers in this space:\nˆytest = f(φxtest; {φx, ∀(x, y) ∈Dtrain})\n(2)\n∝exp\n\u0000sim(φxtest, φx)\n\u0001\n· y, ∀(x, y) ∈Dtrain\nNote that only the embedding function is learned by opti-\nmizing the loss in Eq. 1. For reasons to be made clear in\nbelow, we refer this embedding function as task-agnostic.\n4. Adapting Embedding for Task-speciﬁc FSL\nIn what follows, we describe our approach for few-shot\nlearning (FSL). We start by describing the main idea (§ 4.1,\nalso illustrated in Figure 2), then introduce the set-to-set\nadaptation function (§ 4.2). Last are learning (§ 4.3) and\nimplementations details (§ 4.4).\n4.1. Adapting to Task-Speciﬁc Embeddings\nThe key difference between our approach and traditional\nones is to learn task-speciﬁc embeddings. We argue that the\nembedding φx is not ideal. In particular, the embeddings do\nnot necessarily highlight the most discriminative represen-\ntation for a speciﬁc target task. To this end, we introduce an\nadaption step where the embedding function φx (more pre-\ncisely, its values on instances) is transformed. This trans-\nformation is a set-to-set function that contextualizes over\nthe image instances of a set, to enable strong co-adaptation\nof each item.\nInstance functions fails to have such co-\nadaptation property.\nFurthermore, the set-to-set-function\nreceives instances as bags, or sets without orders, requiring\nthe function to output the set of reﬁned instance embeddings\n3\n",
    "while being permutation-invariant. Concretely,\n{ψx ; ∀x ∈Xtrain} = T ({φx ; ∀x ∈Xtrain})\n(3)\n= T (π {φx ; ∀x ∈Xtrain}))\nwhere Xtrain is a set of all the instances in the training set\nDtrain for the target task. π(·) is a permutation operator\nover a set. Thus the set of adapted embedding will not\nchange if we apply a permutation over the input embedding\nset. With adapted embedding ψx, the test instance xtest can\nbe classiﬁed by computing nearest neighbors w.r.t. Dtrain:\nˆytest = f(φxtest; {ψx, ∀(x, y) ∈Dtrain})\n(4)\nOur approach is generally applicable to different types of\ntask-agnostic embedding function E and similarity measure\nsim(·, ·), e.g., the (normalized) cosine similarity [49] or the\nnegative distance [43]. Both the embedding function E and\nthe set transformation function T are optimized over syn-\nthesized FSL tasks sampled from DS, sketched in Alg. 1.\nIts key difference from conventional FSL is in the line 4 to\nline 8 where the embeddings are transformed.\n4.2. Embedding Adaptation via Set-to-set Functions\nNext, we explain various choices as the instantiations of\nthe set-to-set embedding adaptation function.\nBidirectional LSTM (BILSTM) [16, 49] is one of the\ncommon choice to instantiate the set-to-set transformation,\nwhere the addition between the input and the hidden layer\noutputs of each BILSTM cell leads to the adapted embed-\nding. It is notable that the output of the BILSTM suppose to\ndepend on the order of the input set. Note that using BIL-\nSTM as embedding adaptation model is similar but different\nfrom the fully conditional embedding [49], where the later\none contextualizes both training and test instance embed-\nding altogether, which results in a transductive setting.\nDeepSets [56] is inherently a permutation-invariant trans-\nformation function. It is worth noting that DEEPSETS aggre-\ngates the instances in a set into a holistic set vector. We con-\nsider two components to implement such DeepSets transfor-\nmation, an instance centric set vector combined with a set\ncontext vector. For x ∈Xtrain, we deﬁne its complemen-\ntary set as x∁. Then we implement the DEEPSETS by:\nψx = φx + g([φx;\nX\nxi′∈x∁\nh(φxi′)])\n(5)\nIn Eq. 5, g and h are two-layer multi-layer perception\n(MLP) with ReLU activation which map the embedding\ninto another space and increase the representation ability\nof the embedding. For each instance, embeddings in its\ncomplementary set is ﬁrst combined into a set vector as the\ncontext, and then this vector is concatenated with the input\nembedding to obtain the residual component of adapted em-\nbedding. This conditioned embedding takes other instances\nin the set into consideration, and keeps the “set (permutation\ninvariant)” property. In practice, we ﬁnd using the maxi-\nmum operator in Eq. 5 works better than the sum operator\nsuggested in [56].\nGraph Convolutional Networks (GCN) [21, 41] propa-\ngate the relationship between instances in the set. We ﬁrst\nconstruct the degree matrix A to represent the similarity be-\ntween instances in a set. If two instances come from the\nsame class, then we set the corresponding element in A to\n1, otherwise to 0. Based on A, we build the “normalized”\nadjacency matrix S for a given set with added self-loops\nS = D−1\n2 (A + I)D−1\n2 . I is the identity matrix, and D is\nthe diagonal matrix whose elements are equal to the sum of\nelements in the corresponding row of A + I.\nLet Φ0 = {φx ; ∀x ∈Xtrain}, the relationship between\ninstances could be propagated based on S, i.e.,\nΦt+1 = ReLU(SΦtW) , t = 0, 1, . . . , T −1\n(6)\nW is a projection matrix for feature transformation.\nIn\nGCN, the embedding in the set is transformed based on\nEq. 6 multiple times, and the ﬁnal ΦT gives rise to the {ψx}.\nTransformer. [47] We use the Transformer architec-\nture [47] to implement T. In particular, we employ self-\nattention mechanism [29, 47] to transform each instance\nembedding with consideration to its contextual instances.\nNote that it naturally satisﬁes the desired properties of T\nbecause it outputs reﬁned instance embeddings and is per-\nmutation invariant. We denote it as Few-Shot Embedding\nAdaptation with Transformer (FEAT).\nTransformer is a store of triplets in the form of (query\nQ, key K, and value V). To compute proximity and re-\nturn values, those points are ﬁrst linearly mapped into some\nspace K = W ⊤\nK\n\u0002\nφxk; ∀xk ∈K\n\u0003\n∈Rd×|K|, which\nis also the same for Q and V with WQ and WV respec-\ntively. Transformer computes what is the right value for a\nquery point — the query xq ∈Q is ﬁrst matched against\na list of keys K where each key has a value V . The ﬁ-\nnal value is then returned as the sum of all the values\nweighted by the proximity of the key to the query point,\ni.e. ψxq = φxq + P\nk αqkV:,k, where\nαqk ∝exp\n \nφ⊤\nxqWQ · K\n√\nd\n!\nand V:,k is the k-th column of V . In the standard FSL setup,\nwe have Q = K = V = Xtrain.\n4.3. Contrastive Learning of Set-to-Set Functions\nTo facilitate the learning of embedding adaptation, we\napply a contrastive objective in addition to the general one.\n4\n",
    "It is designed to make sure that instances embeddings af-\nter adaptation is similar to the same class neighbors and\ndissimilar to those from different classes. Speciﬁcally, the\nembedding adaptation function T is applied to instances of\neach n of the N class in DS\ntrain ∪DS\ntest, which gives rise to\nthe transformed embedding ψ′\nx and class centers {cn}N\nn=1.\nThen we apply the contrastive objective to make sure train-\ning instances are close to its own class center than other\ncenters. The total objective function (together with Eq. 1) is\nshown as following:\nL(ˆytest, ytest) = ℓ(ˆytest, ytest)\n(7)\n+λ · ℓ\n\u0000softmax\n\u0000sim(ψ′\nxtest, cn)\n\u0001\n, ytest\n\u0001\nThis contrastive learning makes the set transformation ex-\ntract common characteristic for instances of the same cate-\ngory, so as to preserve the category-wise similarity.\n4.4. Implementation details\nWe consider three different types of convolutional net-\nworks as the backbone for instance embedding function E:\n1) A 4-layer convolution network (ConvNet) [43, 46, 49]\nand 2) the 12-layer residual network (ResNet) used in [25],\nand 3) the Wide Residual Network (WideResNet) [40, 55].\nWe apply an additional pre-training stage for the backbones\nover the SEEN classes, based on which our re-implemented\nmethods are further optimized. To achieve more precise em-\nbedding, we average the same-class instances in the train-\ning set before the embedding adaptation with the set-to-\nset transformation. Adam [20] and SGD are used to op-\ntimize ConvNet and ResNet variants respectively. More-\nover, we follow the most standard implementations for the\nfour set-to-set functions — BiLSTM [16], DeepSets [56],\nGraph Convolutional Networks (GCN) [21] and Trans-\nformer (FEAT) [47].\nWe refer readers to supplementary\nmaterial (SM) for complete details and ablation studies of\neach set-to-set functions. Our implementation is available\nat https://github.com/Sha-Lab/FEAT.\n5. Experiments\nIn this section, we ﬁrst evaluate a variety of models for\nembedding adaptation in § 5.2 with standard FSL. It con-\ncludes that FEAT (with Transformer) is the most effective\napproach among different instantiations. Next, we perform\nablation studies in § 5.2.2 to analyze FEAT in details. Even-\ntually, we evaluate FEAT on many extended few-shot learn-\ning tasks to study its general applicability (§ 5.3).\nThis\nstudy includes few-shot domain generalization, transduc-\ntive few-shot learning, generalized few-shot learning, and\nlarge-scale low-shot learning (refer to SM).\n5.1. Experimental Setups\nDatasets.\nMiniImageNet [49] and TieredImageNet [38]\ndatasets are subsets of the ImageNet [39]. MiniImageNet\nincludes a total number of 100 classes and 600 examples\nper class. We follow the setup provided by [36], and use\n64 classes as SEEN categories, 16 and 20 as two sets of\nUNSEEN categories for model validation and evaluation re-\nspectively.\nTieredImageNet is a large-scale dataset with\nmore categories, which contains 351, 97, and 160 categories\nfor model training, validation, and evaluation, respectively.\nIn addition to these, we investigate the OfﬁceHome [48]\ndataset to validate the generalization ability of FEAT across\ndomains. There are four domains in OfﬁceHome, and two\nof them (“Clipart” and “Real World”) are selected, which\ncontains 8722 images. After randomly splitting all classes,\n25 classes serve as the seen classes to train the model, and\nthe remaining 15 and 25 classes are used as two UNSEEN\nfor evaluation. Please refer to SM for more details.\nEvaluation protocols. Previous approaches [10, 43, 46]\nusually follow the original setting of [49] and evaluate the\nmodels on 600 sampled target tasks (15 test instances per\nclass).\nIn a later study [40], it was suggested that such\nan evaluation process could potentially introduce high vari-\nances. Therefore, we follow the new and more trustworthy\nevaluation setting to evaluate both baseline models and our\napproach on 10,000 sampled tasks. We report the mean ac-\ncuracy (in %) as well as the 95% conﬁdence interval.\nBaseline and embedding adaptation methods.\nWe re-\nimplement the prototypical network (ProtoNet) [43] as a\ntask-agnostic embedding baseline model. This is known\nas a very strong approach [8] when the backbone archi-\ntecture is deep, i.e., residual networks [15]. As suggested\nby [33], we tune the scalar temperature carefully to scale\nthe logits of both approaches in our re-implementation. As\nmentioned, we implement the embedding adaptation model\nwith four different function approximators, and denote them\nas BILSTM, DEEPSETS, GCN, and FEAT (i.e. Transformer).\nThe concrete details of each model are included in the SM.\nBackbone pre-training.\nInstead of optimizing from\nscratch, we apply an additional pre-training strategy as\nsuggested in [35, 40]. The backbone network, appended\nwith a softmax layer, is trained to classify all SEEN\nclasses with the cross-entropy loss (e.g., 64 classes in the\nMiniImageNet).\nThe classiﬁcation performance over the\npenultimate layer embeddings of sampled 1-shot tasks from\nthe model validation split is evaluated to select the best pre-\ntrained model, whose weights are then used to initialize the\nembedding function E in the few-shot learning.\n5.2. Standard Few-Shot Image Classiﬁcation\nWe compare our proposed FEAT method with the in-\nstance embedding baselines as well as previous methods on\n5\n",
    "Table 1: Few-shot classiﬁcation accuracy on MiniImageNet. ⋆\nCTM [28] and SimpleShot [51] utilize the ResNet-18. (see SM\nfor the full table with conﬁdence intervals and WRN results.).\nSetups →\n1-Shot 5-Way\n5-Shot 5-Way\nBackbone →\nConvNet\nResNet\nConvNet\nResNet\nMatchNet [49]\n43.40\n-\n51.09\n-\nMAML [10]\n48.70\n-\n63.11\n-\nProtoNet [43]\n49.42\n-\n68.20\n-\nRelationNet [45]\n51.38\n-\n67.07\n-\nPFA [35]\n54.53\n59.60\n67.87\n73.74\nTADAM [33]\n-\n58.50\n-\n76.70\nMetaOptNet [25]\n-\n62.64\n-\n78.63\nCTM [28]\n-\n64.12\n-\n80.51\nSimpleShot [51]\n49.69\n62.85\n66.92\n80.02\nInstance embedding\nProtoNet\n52.61\n62.39\n71.33\n80.53\nEmbedding adaptation\nBILSTM\n52.13\n63.90\n69.15\n80.62\nDEEPSETS\n54.41\n64.14\n70.96\n80.93\nGCN\n53.25\n64.50\n70.59\n81.65\nFEAT\n55.15\n66.78\n71.61\n82.05\nthe standard MiniImageNet [49] and TieredImageNet [38]\nbenchmarks, and then perform detailed analysis on the ab-\nlated models. We include additional results with CUB [50]\ndataset in SM, which shares a similar observation.\n5.2.1. Main Results\nComparison to previous State-of-the-arts. Table 1 and\nTable 2 show the results of our method and others on the\nMiniImageNet and TieredImageNet. First, we observe that\nthe best embedding adaptation method (FEAT) outperforms\nthe instance embedding baseline on both datasets, indi-\ncating the effectiveness of learning task-speciﬁc embed-\nding space. Meanwhile, the FEAT model performs signif-\nicantly better than the current state-of-the-art methods on\nMiniImageNet dataset. On the TieredImageNet, we observe\nthat the ProtoNet baseline is already better than some pre-\nvious state-of-the-arts based on the 12-layer ResNet back-\nbone.\nThis might due to the effectiveness of the pre-\ntraining stage on the TieredImageNet as it is larger than\nMiniImageNet and a fully converged model can be itself\nvery effective. Based on this, all embedding adaptation ap-\nproaches further improves over ProtoNet almost in all cases,\nwith FEAT achieving the best performances among all ap-\nproaches. Note that here our pre-training strategy is most\nsimilar to the one used in PFA [35], while we further ﬁne-\ntune the backbone. Temperature scaling of the logits inﬂu-\nences the performance a lot when ﬁne-tuning over the pre-\ntrained weights. Additionally, we list some recent methods\n(SimpleShot [51], and CTM [28]) using different backbone\nTable 2: Few-shot classiﬁcation accuracy and 95% conﬁdence in-\nterval on TieredImageNet with the ResNet backbone.\nSetups →\n1-Shot 5-Way\n5-Shot 5-Way\nProtoNet [43]\n53.31 ± 0.89\n72.69 ± 0.74\nRelationNet [45]\n54.48 ± 0.93\n71.32 ± 0.78\nMetaOptNet [25]\n65.99 ± 0.72\n81.56 ± 0.63\nCTM [28]\n68.41 ± 0.39\n84.28 ± 1.73\nSimpleShot [51]\n69.09 ± 0.22\n84.58 ± 0.16\nInstance embedding\nProtoNet\n68.23 ± 0.23\n84.03 ± 0.16\nEmbedding adaptation\nBILSTM\n68.14 ± 0.23\n84.23 ± 0.16\nDEEPSETS\n68.59 ± 0.24\n84.36 ± 0.16\nGCN\n68.20 ± 0.23\n84.64 ± 0.16\nFEAT\n70.80 ± 0.23\n84.79 ± 0.16\nTable 3: Number of parameters introduced by each set-to-set\nfunction in additional to the backbone’s parameters.\nBILSTM\nDEEPSETS\nGCN\nFEAT\nConvNet\n25K\n82K\n33K\n16K\nResNet\n2.5M\n8.2M\n3.3M\n1.6M\narchitectures such as ResNet-18 for reference.\nComparison among the embedding adaptation models.\nAmong the four embedding adaptation methods, BILSTM in\nmost cases achieves the worst performances and sometimes\neven performs worse than ProtoNet. This is partially due\nto the fact that BILSTM can not easily implement the re-\nquired permutation invariant property (also shown in [56]),\nwhich confuses the learning process of embedding adapta-\ntion. Secondly, we ﬁnd that DEEPSETS and GCN have the\nability to adapt discriminative task-speciﬁc embeddings but\ndo not achieve consistent performance improvement over\nthe baseline ProtoNet especially on MiniImageNet with the\nConvNet backbone. A potential explanation is that, such\nmodels when jointly learned with the backbone model, can\nmake the optimization process more difﬁcult, which leads\nto the varying ﬁnal performances. In contrast, we observe\nthat FEAT can consistently improve ProtoNet and other em-\nbedding adaptation approaches in all cases, without addi-\ntional bells and whistles. It shows that the Transformer as a\nset-to-set function can implement rich interactions between\ninstances, which provides its high expressiveness to model\nthe embedding adaptation process.\nInterpolation and extrapolation of classiﬁcation ways.\nNext, we study different set-to-set functions on their capa-\nbility of interpolating and extrapolating across the number\nof classiﬁcation ways. To do so, we train each variant of em-\n6\n",
    "5\n10\n15\n20\nNumber of categories per task\n0\n10\n20\n30\n40\n50\n60\n70\nMean accuracy (in %)\n52.5\n36.8\n29.3\n24.6\n55.0\n38.6\n30.6\n25.8\n53.2\n37.1\n29.5\n24.9\n55.1\n39.1\n31.3\n26.4\nMethods\nRandom\nBILSTM\nDeepSets\nGCN\nFEAT\n5\n10\n15\n20\nNumber of categories per task\n0\n10\n20\n30\n40\n50\n60\n70\n52.1\n35.5\n27.5\n22.9\n54.4\n36.9\n27.3\n20.6\n54.1\n37.9\n30.1\n25.3\n55.1\n39.1\n31.1\n26.2\nMethods\nRandom\nBILSTM\nDeepSets\nGCN\nFEAT\n(a) Way Interpolation\n(b) Way Extrapolation\nFigure 3: Interpolation and Extrapolation of few-shot tasks\nfrom the “way” perspective. First, We train various embedding\nadaptation models on 1-shot 20-way (a) or 5-way (b) classiﬁcation\ntasks and evaluate models on unseen tasks with different number\nof classes (N={5, 10, 15, 20}). It shows that FEAT is superior in\nterms of way interpolation and extrapolation ability.\nbedding adaptation functions with both 1-shot 20-way and\n1-shot 5-way tasks, and measure the performance change as\na function to the number of categories in the test time. We\nreport the mean accuracies evaluated on few-shot classiﬁ-\ncation with N = {5, 10, 15, 20} classes, and show results\nin Figure 3. Surprisingly, we observe that FEAT achieves\nalmost the same numerical performances in both extrapo-\nlation and interpolation scenarios, which further displays\nits strong capability of learning the set-to-set transforma-\ntion. Meanwhile, we observe that DEEPSETS works well\nwith interpolation but fails with extrapolation as its perfor-\nmance drops signiﬁcantly with the larger N. In contrast,\nGCN achieves strong extrapolation performances but does\nnot work as effectively in interpolation. BILSTM performs\nthe worst in both cases, as it is by design not permutation\ninvariant and may have ﬁtted an arbitrary dependency be-\ntween instances.\nParameter efﬁciency. Table 3 shows the number of ad-\nditional parameters each set-to-set function has introduced.\nFrom this, we observe that with both ConvNet and ResNet\nbackbones, FEAT has the smallest number of parameters\ncompared with all other approaches while achieving best\nperformances from various aspects (as results discussed\nabove), which highlights its high parameter efﬁciency.\nAll above, we conclude that: 1) learning embedding\nadaptation with a set-to-set model is very effective in mod-\neling task-speciﬁc embeddings for few-shot learning 2)\nFEAT is the most parameter-efﬁcient function approximater\nthat achieves the best empirical performances, together with\nnice permutation invariant property and strong interpola-\ntion/extrapolation capability over the classiﬁcation way.\n5.2.2. Ablation Studies\nWe analyze\nFEAT and its ablated variants on the\nMiniImageNet dataset with ConvNet backbone.\nHow does the embedding adaptation looks like qualita-\ntively? We sample four few-shot learning tasks and learn\na principal component analysis (PCA) model (that projects\nembeddings into 2-D space) using the instance embeddings\nof the test data. We then apply this learned PCA projection\nto both the support set’s pre-adapted and post-adapted em-\nbeddings. The results are shown in Figure 1 (the beginning\nof the paper). In three out of four examples, post-adaptation\nembeddings of FEAT improve over the pre-adaption embed-\ndings. Interestingly, we found that the embedding adap-\ntation step of FEAT has the tendency of pushing the sup-\nport embeddings apart from the clutter, such that they can\nbetter ﬁt the test data of its categories. In the negative ex-\nample where post-adaptation degenerates the performances,\nwe observe that the embedding adaptation step has pushed\ntwo support embeddings “Golden Retriever” and “Lion” too\nclose to each other. It has qualitatively shown that the adap-\ntation is crucial to obtain superior performances and helps\nto contrast against task-agnostic embeddings.\n5.3. Extended Few-Shot Learning Tasks\nIn this section, we evaluate FEAT on 3 different few-shot\nlearning tasks. Speciﬁcally, cross-domain FSL, transductive\nFSL [30, 38], and generalized FSL [7]. We overview the\nsetups brieﬂy and please refer to SM for details.\nFS Domain Generalization assumes that examples in UN-\nSEEN support and test set can come from the different do-\nmains, e.g., sampled from different distributions [9, 19].\nThe example of this task can be found in Figure 4. It re-\nquires a model to recognize the intrinsic property than tex-\nture of objects, and is de facto analogical recognition.\nTransductive FSL. The key difference between standard\nand transductive FSL is whether test instances arrive one\nat a time or all simultaneously. The latter setup allows the\nstructure of unlabeled test instances to be utilized. There-\nfore, the prediction would depend on both the training (sup-\nport) instances and all the available test instances in the tar-\nget task from UNSEEN categories.\nGeneralized FSL. Prior works assumed the test instances\ncoming from unseen classes only. Different from them, the\ngeneralized FSL setting considers test instances from both\nSEEN and UNSEEN classes [37]. In other words, during the\nmodel evaluation, while support instances all come from U,\nthe test instances come from S ∪U, and the classiﬁer is\nrequired to predict on both SEEN and UNSEEN categories. .\n5.3.1. Few-Shot Domain Generalization\nWe show that FEAT learns to adapt the intrinsic structure\nof tasks, and generalizes across domains, i.e., predicting\ntest instances even when the visual appearance is changed.\nSetups. We train the FSL model in the standard domain and\nevaluate with cross-domain tasks, where the N-categories\nare aligned but domains are different. In detail, a model is\n7\n",
    "C →C\nC →R\nSupervised\n34.38±0.16\n29.49±0.16\nProtoNet\n35.51±0.16\n29.47±0.16\nFEAT\n36.83±0.17\n30.89±0.17\n1-Shot\n5-Shot\nTPN [30]\n55.51\n69.86\nTEAM [34]\n56.57\n72.04\nFEAT\n57.04 ± 0.20\n72.89 ± 0.16\nSEEN\nUNSEEN\nCOMBINED\nRandom\n1.56 ±0.00 20.00±0.00\n1.45±0.00\nProtoNet 41.73±0.03 48.64±0.20\n35.69±0.03\nFEAT\n43.94±0.03 49.72±0.20\n40.50±0.03\n(a) Few-shot domain generalization\n(b) Transductive few-shot learning\n(c) Generalized few-shot learning\nTable 4: We evaluate our model on three additional few-shot learning tasks: (a) Few-shot domain generalization, (b) Transductive few-shot\nlearning, and (c) Generalized few-shot learning. We observe that FEAT consistently outperform all previous methods or baselines.\nDrill\nBed\nTV\nFlower\nScrewdriver\n𝒟𝒟𝐭𝐭𝐭𝐭𝐭𝐭𝐭𝐭𝐭𝐭from \n“Clipart”\n𝒟𝒟𝐭𝐭𝐭𝐭𝐭𝐭𝐭𝐭from \n“Real World”\nClassify\nTest Set\nTrain Set\nTrain Set\nTest Set\n𝒟𝒟𝐭𝐭𝐭𝐭𝐭𝐭𝐭𝐭𝐭𝐭from \n“Clipart”\n𝒟𝒟𝐭𝐭𝐭𝐭𝐭𝐭𝐭𝐭from \n“Real World”\nBed\nCurtains\nRefrigerator\nSneakers\nDrill\nClassify\nFigure 4: Qualitative results of few-shot domain-generalization\nfor FEAT. Correctly classiﬁed examples are shown in red boxes\nand incorrectly ones are shown in blue boxes. We visualize one\ntask that FEAT succeeds (top) and one that fails (bottom).\ntrained on tasks from the “Clipart” domain of OfﬁceHome\ndataset [48], then the model is required to generalize to both\n“Clipart (C)” and“Real World (R)” test instances. In other\nwords, we need to classify complex real images by seeing\nonly a few sketches (Figure 4 gives an overview of data).\nResults. Table 4 (a) gives the quantitative results and Fig-\nure 4 qualitatively examines it. Here, the “supervised” de-\nnotes a model trained with the standard classiﬁcation strat-\negy and then its penultimate layer’s output feature is used\nas the nearest neighbor classiﬁer. We observe that ProtoNet\ncan outperform this baseline on tasks when evaluating in-\nstances from “Clipart” but not ones from “real world”.\nHowever, FEAT improves over “real world” few-shot classi-\nﬁcation even only seeing the support data from “Clipart”.\n5.3.2. Transductive Few-Shot Learning\nWe show that without additional efforts in modeling,\nFEAT outperforms existing methods in transductive FSL.\nSetups. We further study this semi-supervised learning set-\nting to see how well FEAT can incorporate test instances into\njoint embedding adaptation. Speciﬁcally, we use the unla-\nbeled test instances to augment the key and value sets of\nTransformer (refer to SM for details), so that the embedding\nadaptation takes relationship of all test instances into con-\nsideration. We evaluate this setting on the transductive pro-\ntocol of MiniImageNet [38]. With the adapted embedding,\nFEAT makes predictions based on Semi-ProtoNet [38].\nResults.\nWe compare with two previous approaches,\nTPN [30] and TEAM [34]. The results are shown in Ta-\nble 4 (b). We observe that FEAT improves its standard FSL\nperformance (refer to Table 1) and also outperforms previ-\nous semi-supervised approaches by a margin.\n5.3.3. Generalized Few-Shot Learning\nWe show that FEAT performs well on generalized few-\nshot classiﬁcation of both SEEN and UNSEEN classes.\nSetups. In this scenario, we evaluate not only on classi-\nfying test instances from a N-way M-shot task from UN-\nSEEN set U, but also on all available SEEN classes from S.\nTo do so, we hold out 150 instances from each of the 64\nseen classes in MiniImageNet for validation and evaluation.\nNext, given a 1-shot 5-way training set Dtrain, we consider\nthree evaluation protocols based on different class sets [7]:\nUNSEEN measures the mean accuracy on test instances only\nfrom U (5-Way few-shot classiﬁcation); SEEN measures the\nmean accuracy on test instances only from S (64-Way clas-\nsiﬁcation); COMBINED measures the mean accuracy on test\ninstances from S ∪U (69-Way mixed classiﬁcation).\nResults.\nThe results can be found in Table 4 (c).\nWe\nobserve that again FEAT outperforms baseline ProtoNet.\nTo calibrate the prediction score on SEEN and UNSEEN\nclasses [7, 52], we select a constant seen/unseen class prob-\nability over the validation set, and subtract this calibration\nfactor from seen classes’ prediction score. Then we take the\nprediction with maximum score value after calibration.\n6. Discussion\nA common embedding space fails to tailor discriminative\nvisual knowledge for a target task especially when there are\na few labeled training data. We propose to do embedding\nadaptation with a set-to-set function and instantiate it with\ntransformer (FEAT), which customizes task-speciﬁc embed-\nding spaces via a self-attention architecture. The adapted\nembedding space leverages the relationship between target\n8\n",
    "task training instances, which leads to discriminative in-\nstance representations.\nFEAT achieves the state-of-the-art\nperformance on benchmarks, and its superiority can gener-\nalize to tasks like cross-domain, transductive, and general-\nized few-shot classiﬁcations.\nAcknowledgments.This work is partially supported by The\nNational Key R&D Program of China (2018YFB1004300),\nDARPA# FA8750-18-2-0117, NSF IIS-1065243, 1451412,\n1513966/ 1632803/1833137, 1208500, CCF-1139148, a\nGoogle Research Award, an Alfred P. Sloan Research Fel-\nlowship, ARO# W911NF-12-1-0241 and W911NF-15-1-\n0484, China Scholarship Council (CSC), NSFC (61773198,\n61773198, 61632004), and NSFC-NRF joint research\nproject 61861146001.\nReferences\n[1] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label-\nembedding for attribute-based classiﬁcation. In CVPR, pages\n819–826, 2013. 2\n[2] M. Andrychowicz, M. Denil, S. G. Colmenarejo, M. W.\nHoffman, D. Pfau, T. Schaul, and N. de Freitas. Learning\nto learn by gradient descent by gradient descent. In NIPS,\npages 3981–3989. 2016. 2\n[3] A. Antoniou, H. Edwards, and A. J. Storkey. How to train\nyour MAML. In ICLR, 2019. 2\n[4] L. J. Ba, R. Kiros, and G. E. Hinton. Layer normalization.\nCoRR, abs/1607.06450, 2016. 13\n[5] S. Changpinyo, W.-L. Chao, B. Gong, and F. Sha.\nSyn-\nthesized classiﬁers for zero-shot learning. In CVPR, pages\n5327–5336, 2016. 2\n[6] S. Changpinyo, W.-L. Chao, and F. Sha. Predicting visual\nexemplars of unseen classes for zero-shot learning. In ICCV,\npages 3496–3505, 2017. 2\n[7] W.-L. Chao, S. Changpinyo, B. Gong, and F. Sha. An empir-\nical study and analysis of generalized zero-shot learning for\nobject recognition in the wild. In ECCV, pages 52–68, 2016.\n7, 8\n[8] W.-Y. Chen, Y.-C. Liu, Z. Kira, Y.-C. F. Wang, and J.-B.\nHuang. A closer look at few-shot classiﬁcation. In ICLR,\n2019. 5\n[9] N. Dong and E. P. Xing. Domain adaption in one-shot learn-\ning. In ECML PKDD, pages 573–588, 2018. 7\n[10] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-\nlearning for fast adaptation of deep networks.\nIn ICML,\npages 1126–1135, 2017. 1, 2, 3, 5, 6, 14, 15\n[11] G. Ghiasi, T.-Y. Lin, and Q. V. Le. Dropblock: A regulariza-\ntion method for convolutional networks. In NeurIPS, pages\n10750–10760. 2018. 13\n[12] S. Gidaris and N. Komodakis.\nDynamic few-shot visual\nlearning without forgetting.\nIn CVPR, pages 4367–4375,\n2018. 18\n[13] L.-Y. Gui, Y.-X. Wang, D. Ramanan, and J. M. F. Moura.\nFew-shot human motion prediction via meta-learning.\nIn\nECCV, pages 441–459, 2018. 2\n[14] B. Hariharan and R. B. Girshick. Low-shot visual recogni-\ntion by shrinking and hallucinating features. In ICCV, pages\n3037–3046, 2017. 18\n[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In CVPR, pages 770–778, 2016. 5,\n18\n[16] S. Hochreiter and J. Schmidhuber. Long short-term memory.\nNeural Computation, 9(8):1735–1780, 1997. 2, 4, 5, 11\n[17] K. Hsu, S. Levine, and C. Finn. Unsupervised learning via\nmeta-learning. In ICLR, 2019. 2\n[18] S. Ioffe and C. Szegedy. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift. In\nICML, pages 448–456, 2015. 13\n[19] B. Kang and J. Feng. Transferable meta learning across do-\nmains. In UAI, pages 177–187, 2018. 7\n[20] D. P. Kingma and J. Ba. Adam: A method for stochastic\noptimization. In ICLR, 2015. 5, 14\n[21] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation\nwith graph convolutional networks. In ICLR, 2017. 2, 4, 5,\n12\n[22] G. Koch, R. Zemel, and R. Salakhutdinov.\nSiamese neu-\nral networks for one-shot image recognition. In ICML Deep\nLearning Workshop, volume 2, 2015. 2\n[23] B. M. Lake, R. Salakhutdinov, J. Gross, and J. B. Tenen-\nbaum.\nOne shot learning of simple visual concepts.\nIn\nCogSci, 2011. 1\n[24] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-\nlevel concept learning through probabilistic program induc-\ntion. Science, 350(6266):1332–1338, 2015. 1\n[25] K. Lee, S. Maji, A. Ravichandran, and S. Soatto.\nMeta-\nlearning with differentiable convex optimization. In CVPR,\npages 10657–10665, 2019. 2, 5, 6, 13, 15\n[26] Y. Lee and S. Choi.\nGradient-based meta-learning with\nlearned layerwise metric and subspace.\nIn ICML, pages\n2933–2942, 2018. 2\n[27] F.-F. Li, R. Fergus, and P. Perona. One-shot learning of ob-\nject categories. TPAMI, 28(4):594–611, 2006. 1\n[28] H. Li, D. Eigen, S. Dodge, M. Zeiler, and X. Wang. Find-\ning task-relevant features for few-shot learning by category\ntraversal. In CVPR, pages 1–10, 2019. 2, 6\n[29] Z. Lin, M. Feng, C. N. dos Santos, M. Yu, B. Xiang, B. Zhou,\nand Y. Bengio. A structured self-attentive sentence embed-\nding. In ICLR, 2017. 2, 4\n[30] Y. Liu, J. Lee, M. Park, S. Kim, E. Yang, S. J. Hwang, and\nY. Yang. Learning to propagate labels: Transductive propa-\ngation network for few-shot learning. In ICLR, 2019. 7, 8,\n17, 18\n[31] L. Metz, N. Maheswaranathan, B. Cheung, and J. Sohl-\nDickstein.\nLearning unsupervised learning rules.\nCoRR,\nabs/1804.00222, 2018. 2\n[32] A. Nichol, J. Achiam, and J. Schulman. On ﬁrst-order meta-\nlearning algorithms. CoRR, abs/1803.02999, 2018. 2\n[33] B. N. Oreshkin, P. R. L´opez, and A. Lacoste. TADAM: task\ndependent adaptive metric for improved few-shot learning.\nIn NeurIPS, pages 719–729. 2018. 5, 6, 11, 15\n[34] L. Qiao, Y. Shi, J. Li, Y. Wang, T. Huang, and Y. Tian. Trans-\nductive episodic-wise adaptive metric for few-shot learning.\nIn ICCV, pages 3603–3612, 2019. 8, 17, 18\n[35] S. Qiao, C. Liu, W. Shen, and A. L. Yuille. Few-shot image\nrecognition by predicting parameters from activations.\nIn\nCVPR, pages 7229–7238, 2018. 2, 5, 6, 13, 14, 15\n[36] S. Ravi and H. Larochelle. Optimization as a model for few-\nshot learning. In ICLR, 2017. 2, 5\n9\n",
    "[37] M. Ren, R. Liao, E. Fetaya, and R. S. Zemel. Incremen-\ntal few-shot learning with attention attractor networks. In\nNeurIPS, pages 5276–5286, 2019. 7\n[38] M. Ren, E. Triantaﬁllou, S. Ravi, J. Snell, K. Swersky, J. B.\nTenenbaum, H. Larochelle, and R. S. Zemel. Meta-learning\nfor semi-supervised few-shot classiﬁcation. In ICLR, 2018.\n5, 6, 7, 8, 14, 17, 18\n[39] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein,\nA. C. Berg, and F.-F. Li. Imagenet large scale visual recog-\nnition challenge. IJCV, 115(3):211–252, 2015. 5, 18\n[40] A. A. Rusu, D. Rao, J. Sygnowski, O. Vinyals, R. Pascanu,\nS. Osindero, and R. Hadsell. Meta-learning with latent em-\nbedding optimization. In ICLR, 2019. 1, 2, 5, 13, 14, 15\n[41] V. G. Satorras and J. B. Estrach.\nFew-shot learning with\ngraph neural networks. In ICLR, 2018. 2, 4, 12\n[42] T. R. Scott, K. Ridgeway, and M. C. Mozer. Adapted deep\nembeddings: A synthesis of methods for k-shot inductive\ntransfer learning. In NeurIPS, pages 76–85. 2018. 2\n[43] J. Snell, K. Swersky, and R. S. Zemel. Prototypical networks\nfor few-shot learning. In NIPS, pages 4080–4090. 2017. 1,\n2, 3, 4, 5, 6, 11, 13, 15, 18\n[44] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov. Dropout: a simple way to prevent neural\nnetworks from overﬁtting. JMLR, 15(1):1929–1958, 2014.\n13\n[45] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. S. Torr, and\nT. M. Hospedales. Learning to compare: Relation network\nfor few-shot learning. In CVPR, pages 1199–1208, 2018. 2,\n6, 15\n[46] E. Triantaﬁllou, R. S. Zemel, and R. Urtasun.\nFew-shot\nlearning through an information retrieval lens.\nIn NIPS,\npages 2252–2262. 2017. 1, 2, 5, 13, 14\n[47] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all\nyou need. In NIPS, pages 6000–6010. 2017. 2, 4, 5, 12, 13,\n14, 16\n[48] H. Venkateswara, J. Eusebio, S. Chakraborty, and S. Pan-\nchanathan. Deep hashing network for unsupervised domain\nadaptation. In CVPR, pages 5385–5394, 2017. 5, 8, 14, 17\n[49] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and\nD. Wierstra. Matching networks for one shot learning. In\nNIPS, pages 3630–3638. 2016. 1, 2, 3, 4, 5, 6, 11, 12, 13,\n14, 15\n[50] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.\nThe Caltech-UCSD Birds-200-2011 Dataset. Technical Re-\nport CNS-TR-2011-001, California Institute of Technology,\n2011. 6, 14\n[51] Y. Wang, W.-L. Chao, K. Q. Weinberger, and L. van der\nMaaten.\nSimpleshot: Revisiting nearest-neighbor classiﬁ-\ncation for few-shot learning. CoRR, abs/1911.04623, 2019.\n6, 14, 15\n[52] Y.-X. Wang, R. B. Girshick, M. Hebert, and B. Hariharan.\nLow-shot learning from imaginary data.\nIn CVPR, pages\n7278–7286, 2018. 8, 18\n[53] X.-S. Wei, P. Wang, L. Liu, C. Shen, and J. Wu.\nPiece-\nwise classiﬁer mappings: Learning ﬁne-grained learners for\nnovel categories with few examples. TIP, 28(12):6116–6125,\n2019. 2\n[54] H.-J. Ye, H. Hu, D.-C. Zhan, and F. Sha.\nLearn-\ning embedding adaptation for few-shot learning.\nCoRR,\nabs/1812.03664, 2018. 13\n[55] S. Zagoruyko and N. Komodakis. Wide residual networks.\nIn BMVC, 2016. 5, 13\n[56] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. P´oczos, R. R.\nSalakhutdinov, and A. J. Smola. Deep sets. In NIPS, pages\n3394–3404. 2017. 2, 4, 5, 6, 12\n10\n",
    "Supplementary Material\nA. Details of Baseline Methods\nIn this section, we describe two important embedding\nlearning baselines i.e., Matching Network (MatchNet) [49]\nand Prototypical Network (ProtoNet) [43], to implement the\nprediction function f(xtest; Dtrain) in the few-shot learn-\ning framework.\nMatchNet and ProtoNet.\nBoth MatchNet and ProtoNet\nstress the learning of the embedding function E from the\nsource task data DS with a meta-learning routine similar to\nAlg. 1 in the main text. We omit the super-script S since\nthe prediction strategies can apply to tasks from both SEEN\nand UNSEEN sets.\nGiven the training data Dtrain = {xi, yi}NM\ni=1 of an M-\nshot N-way classiﬁcation task, we can obtain the embed-\nding of each training instance based on the function E:1\nφ(xi) = E(xi), ∀xi ∈Xtrain\n(8)\nTo classify a test instance xtest, we perform the nearest\nneighbor classiﬁcation , i.e.,\nˆytest ∝exp\n\u0000γ · sim(φxtest, φxi)\n\u0001\n· yi\n(9)\n=\nexp\n\u0000γ · sim(φxtest , φxi)\n\u0001\nP\nxi′∈Xtrain exp\n\u0000γ · sim(φxtest , φxi′)\n\u0001 · yi\n=\nX\n(xi,yi)∈Dtrain\nexp\n\u0000γ · sim(φxtest , φxi)\n\u0001\nP\nxi′∈Xtrain exp\n\u0000γ · sim(φxtest , φxi′)\n\u0001 · yi\nHere, MatchNet ﬁnds the most similar training instance to\nthe test one, and assigns the label of the nearest neigh-\nbor to the test instance. Note that sim represents the co-\nsine similarity, and γ > 0 is the scalar temperature value\nover the similarity score, which is found important em-\npirically [33]. During the experiments, we tune this tem-\nperature value carefully, ranging from the reciprocal of\n{0.1, 1, 16, 32, 64, 128}.2\nThe ProtoNet has two key differences compared with the\nMatchNet. First, when M > 1 in the target task, ProtoNet\ncomputes the mean of the same class embeddings as the\nclass center (prototype) in advance and classiﬁes a test in-\nstance by computing its similarity to the nearest class center\n(prototype). In addition, it uses the negative distance rather\n1In the following, we use φ(xi) and φxi exchangeably to represent the\nembedding of an instance xi based on the mapping φ.\n2In experiments, we ﬁnd the temperature scale over logits inﬂuences\nthe model training a lot when we optimize based on pre-trained weights.\nthan the cosine similarity as the similarity metric:\ncn = 1\nM\nX\nyi=n\nφ(xi), ∀n = 1, . . . , N\n(10)\nˆytest ∝exp\n\u0000γ · ∥φxtest −cn∥2\n2\n\u0001\n· yn\n=\nN\nX\nn=1\nexp\n\u0000−γ∥φxtest −cn∥2\n2\n\u0001\nPN\nn′=1 exp\n\u0000−γ∥φxtest −cn′∥2\n2\n\u0001yn\n(11)\nSimilar to the aforementioned scalar temperature for Match-\nNet, in Eq. 11 we also consider the scale γ. Here we abuse\nthe notation by using yi = n to enumerate the instances\nwith label n, and denote yn as the one-hot coding of the\nn-th class. Thus Eq. 11 outputs the probability to classify\nxtest to the N classes.\nIn the experiments, we ﬁnd ProtoNet incorporates bet-\nter with FEAT. When there is more than one shot in each\nclass, we average all instances per class in advance by\nEq. 10 before inputting them to the set-to-set transforma-\ntion. This pre-average manner makes more precise embed-\nding for each class and facilitates the “downstream” em-\nbedding adaptation. We will validate this in the additional\nexperiments.\nB. Details of the Set-to-Set Functions\nIn this section, we provide details about four imple-\nmentations of the set-to-set embedding adaptation function\nT, i.e., the BILSTM, DEEPSETS, GCN, and the TRANS-\nFORMER. The last one is the key component in our Few-\nshot Embedding Adaptation with Transformer (FEAT) ap-\nproach. Then we will introduce the conﬁguration of the\nmulti-layer/multi-head transformer, and the setup of the\ntransformer for the transductive Few-Shot Learning (FSL).\nB.1. BiLSTM as the Set-to-Set Transformation\nBidirectional LSTM (BILSTM) [16, 49] is one of the\ncommon choice to instantiate the set-to-set transformation,\nwhere the addition between the input and the hidden layer\noutputs of each BILSTM cell leads to the adapted embed-\nding. In detail, we have\n{\n⃗\nφ(x), ⃗φ(x)} = BILSTM({φ(x)}); ∀x ∈Xtrain\n(12)\nWhere\n⃗\nφ(x) and ⃗φ(x) are the hidden layer outputs of the\ntwo LSTM models for each instance embedding in the input\nset. Then we get the adapted embedding as\nψ(x) = φ(x) +\n⃗\nφ(x) + ⃗φ(x)\n(13)\nIt is notable that the output of the BILSTM suppose to de-\npend on the order of the input set. Vinyals et al. [49] pro-\npose to use the Fully Conditional Embedding to encode\nthe context of both the test instance and the support set\n11\n",
    "Classification \nScores\nEmbedding \nAdaptation\nCNN\nCNN\nCNN\nCNN\nSoft Nearest \nNeighbor\nSet-to-Set Function\n(a) Embedding Adaptation\n(b) Transformer as the Set-to-Set Function\n(c) DeepSets as Set-to-Set Function\nLayer Norm\nFC\nFC\nFC\nFC\nFC\nFC\nFC\nFC\nFC\nFC\nFC\nSUM\nCAT\nScaled Dot\nProduct\nTrain Instance\nTest Instance\nTask Agnostic\nEmbedding\nTask Specific \nEmbedding\nFigure 5: Illustration of two embedding adpatation methods considered in the paper. (a) shows the main ﬂow of Few-Shot Embedding\nAdaptation, while (b) and (c) demonstrate the workﬂow of Transformer and DeepSets respectively.\ninstances based on BILSTM and LSTM w/ Attention mod-\nule. Different from [49], we apply the set-to-set embedding\nadaptation only over the support set, which leads to a fully\ninductive learning setting.\nB.2. DeepSets as the Set-to-Set Transformation\nDeep sets [56] suggests a generic aggregation function\nover a set should be the transformed sum of all elements in\nthis set. Therefore, a very simple set-to-set transformation\nbaseline involves two components, an instance centric rep-\nresentation combined with a set context representation. For\nany instance x ∈Xtrain, we deﬁne its complementary set\nas x∁. Then we implement the set transformation by:\nψ(x) = φ(x) + g([φ(x);\nX\nxi′∈x∁\nh(φ(xi′))])\n(14)\nIn Eq. 14, g and h are transformations which map the em-\nbedding into another space and increase the representation\nability of the embedding. Two-layer multi-layer perception\n(MLP) with ReLU activation is used to implement these two\nmappings. For each instance, embeddings in its comple-\nmentary set are ﬁrst combined into a vector as the context,\nand then this vector is concatenated with the input embed-\nding to obtain the residual component of the adapted em-\nbedding. This conditioned embedding takes other instances\nin the set into consideration, and keeps the “set (permutation\ninvariant)” property. Finally, we determine the label with\nthe newly adapted embedding ψ as Eq. 11. An illustration\nof the DeepSets notation in the embedding adaptation can\nbe found in Figure 5 (c). The summation operator in Eq. 14\ncould also be replaced as the maximum operator, and we\nﬁnd the maximum operator works better than summation\noperator in our experiments.\nB.3. GCN as the Set-to-Set Transformation\nGraph Convolutional Networks (GCN) [21, 41] propa-\ngate the relationship between instances in the set. We ﬁrst\nconstruct a degree matrix A ∈RNK×NK to represent the\nsimilarity between instances in a set. If two instances xi and\nxj come from the same class, then we set the corresponding\nelement Aij in A to 1, otherwise we have Aij = 0. Based\non A, we build the “normalized” adjacency matrix S for a\ngiven set with added self-loops S = D−1\n2 (A + I)D−1\n2 .\nI ∈RNK×NK is the identity matrix, and D is the diagonal\nmatrix whose elements are equal to the sum of elements in\nthe corresponding row of A+I, i.e., Dii = P\nj Aij +1 and\nDij = 0 if i ̸= j. Let Φ0 = {φx ; ∀x ∈Xtrain} be the\nconcatenation of all the instance embeddings in the training\nset Xtrain. We use the super-script to denote the generation\nof the instance embedding matrix. The relationship between\ninstances could be propagated based on S, i.e.,\nΦt+1 = ReLU(SΦtW) , t = 0, 1, . . . , T −1\n(15)\nW is a learned a projection matrix for feature transforma-\ntion. In GCN, the embedding in the set is transformed based\non Eq. 15 multiple times (we propagate the embedding set\ntwo times during the experiments), and the ﬁnal propagated\nembedding set ΦT gives rise to the ψx.\nB.4. Transformer as the Set-to-Set Transformation\nIn this section, we describe in details about our Few-Shot\nEmbedding Adaptation w/ Transformer (FEAT) approach,\nspeciﬁcally how to use the transformer architecture [47] to\nimplement the set-to-set function T, where self-attention\nmechanism facilitates the instance embedding adaptation\nwith consideration of the contextual embeddings.\nAs mentioned before, the transformer is a store of triplets\nin the form of (query, key, and value). Elements in the query\nset are the ones we want to do the transformation. The trans-\nformer ﬁrst matches a query point with each of the keys by\ncomputing the “query” – “key” similarities. Then the prox-\nimity of the key to the query point is used to weight the\ncorresponding values of each key. The transformed input\nacts as a residual value which will be added to the input.\n12\n",
    "Basic Transformer.\nFollowing the deﬁnitions in [47], we\nuse Q, K, and V to denote the set of the query, keys, and\nvalues, respectively. All these sets are implemented by dif-\nferent combinations of task instances.\nTo increase the ﬂexibility of the transformer, three sets\nof linear projections (WQ ∈Rd×d′, WK ∈Rd×d′, and\nWV ∈Rd×d′) are deﬁned, one for each set.3 The points in\nsets are ﬁrst projected by the corresponding projections\nQ = W ⊤\nQ\n\u0002\nφxq;\n∀xq ∈Q\n\u0003\n∈Rd′×|Q|\nK = W ⊤\nK\n\u0002\nφxk;\n∀xk ∈K\n\u0003\n∈Rd′×|K|\nV = W ⊤\nV\n\u0002\nφxv;\n∀xv ∈V\n\u0003\n∈Rd′×|V|\n(16)\n|Q|, |K|, and |V| are the number of elements in the sets\nQ, K, and V respectively. Since there is a one-to-one corre-\nspondence between elements in K and V we have |K| = |V|.\nThe similarity between a query point xq ∈Q and the list\nof keys K is then computed as “attention”:\nαqk ∝exp\n \nφ⊤\nxqWQ · K\n√\nd\n!\n; ∀xk ∈K\n(17)\nαq,: = softmax\n \nφ⊤\nxqWQ · K\n√\nd\n!\n∈R|K|\n(18)\nThe k-th element αqk in the vector αq,: reveals the particu-\nlar proximity between xk and xq. The computed attention\nvalues are then used as weights for the ﬁnal embedding xq:\n˜ψxq =\nX\nk\nαqkV:,k\n(19)\nψxq = τ\n\u0000φxq + W ⊤\nFC ˜ψxq\n\u0001\n(20)\nV:,k is the k-th column of V .\nWFC ∈Rd′×d is the\nprojection weights of a fully connected layer.\nτ com-\npletes a further transformation, which is implemented by\nthe dropout [44] and layer normalization [4]. The whole\nﬂow of transformer in our FEAT approach can be found in\nFigure 5 (b). With the help of transformer, the embeddings\nof all training set instances are adapted (we denote this ap-\nproach as FEAT).\nMulti-Head Multi-Layer Transformer.\nFollowing [47],\nan extended version of the transformer can be built with\nmultiple parallel attention heads and stacked layers. As-\nsume there are totally H heads, the transformer concate-\nnates multiple attention-transformed embeddings, and then\nuses a linear mapping to project the embedding to the orig-\ninal embedding space (with the original dimensionality).\nBesides, we can take the transformer as a feature encoder\nof the input query instance. Therefore, it can be applied\n3For notation simplicity, we omit the bias in the linear projection here.\nover the input query multiple times (with different sets of\nparameters), which gives rise to the multi-layer transformer.\nWe discuss the empirical performances with respect to the\nchange number of heads and layers in § D.\nB.5. Extension to transductive FSL\nFacilitated by the ﬂexible set-to-set transformer in\nEq. 20, our adaptation approach can naturally be extended\nto the transductive FSL setting.\nWhen classifying test instance xtest in the transdutive\nscenario, other test instances Xtest from the N categories\nwould also be available. Therefore, we enrich the trans-\nformer’s query and key/value sets\nQ = K = V = Xtrain ∪Xtest\n(21)\nIn this manner, the embedding adaptation procedure would\nalso consider the structure among unlabeled test instances.\nWhen the number of shots K > 1, we average the embed-\nding of labeled instances in each class ﬁrst before combin-\ning them with the test set embeddings.\nC. Implementation Details\nBackbone architecture.\nWe consider three backbones, as\nsuggested in the literature, as the instance embedding func-\ntion E for the purpose of fair comparisons. We resize the\ninput image to 84 × 84 × 3 before using the backbones.\n• ConvNet. The 4-layer convolution network [43, 46, 49]\ncontains 4 repeated blocks. In each block, there is a con-\nvolutional layer with 3 × 3 kernel, a Batch Normalization\nlayer [18], a ReLU, and a Max pooling with size 2. We\nset the number of convolutional channels in each block as\n64. A bit different from the literature, we add a global\nmax pooling layer at last to reduce the dimension of the\nembedding. Based on the empirical observations, this will\nnot inﬂuence the results, but reduces the computation bur-\nden of later transformations a lot.\n• ResNet. We use the 12-layer residual network in [25].4\nThe DropBlock [11] is used in this ResNet architecture\nto avoid over-ﬁtting. A bit different from the ResNet-12\nin [25], we apply a global average pooling after the ﬁnal\nlayer, which leads to 640 dimensional embeddings.5\n• WRN. We also consider the Wide residual network [40,\n55].\nWe use the WRN-28-10 structure as in [35, 40],\nwhich sets the depth to 28 and width to 10. After a global\n4The source code of the ResNet is publicly available on https://\ngithub.com/kjunelee/MetaOptNet\n5We use the ResNet backbone with input image size 80 × 80 ×\n3 from [35] in the old version of our paper [54], whose source\ncode of ResNet is publicly available on https://github.com/\njoe-siyuan-qiao/FewShot-CVPR.\nEmpirically we ﬁnd the\nResNet-12 [25] works better than our old ResNet architecture.\n13\n",
    "average pooling in the last layer of the backbone, we get\na 640 dimensional embedding for further prediction.\nDatasets.\nFour\ndatasets,\nMiniImageNet\n[49],\nTieredImageNet\n[38],\nCaltech-UCSD\nBirds\n(CUB)\n200-2011 [50], and OfﬁceHome [48] are investigated in\nthis paper. Each dataset is split into three parts based on\ndifferent non-overlapping sets of classes, for model training\n(a.k.a.\nmeta-training in the literature), model validation\n(a.k.a.\nmeta-val in the literature), and model evaluation\n(a.k.a.\nmeta-test in the literature).\nThe CUB dataset is\ninitially designed for ﬁne-grained classiﬁcation. It contains\nin total 11,788 images of birds over 200 species. On CUB,\nwe randomly sampled 100 species as SEEN classes, another\ntwo 50 species are used as two UNSEEN sets for model\nvalidation and evaluation [46]. For all images in the CUB\ndataset, we use the provided bounding box to crop the\nimages as a pre-processing [46].\nBefore input into the\nbackbone network, all images in the dataset are resized\nbased on the requirement of the network.\nPre-training strategy.\nAs mentioned before, we apply an\nadditional pre-training strategy as suggested in [35, 40].\nThe backbone network, appended with a softmax layer, is\ntrained to classify all classes in the SEEN class split (e.g., 64\nclasses in the MiniImageNet) with the cross-entropy loss.\nIn this stage, we apply image augmentations like random\ncrop, color jittering, and random ﬂip to increase the gen-\neralization ability of the model. After each epoch, we val-\nidate the performance of the pre-trained weights based on\nits few-shot classiﬁcation performance on the model vali-\ndation split. Speciﬁcally, we randomly sample 200 1-shot\nN-way few-shot learning tasks (N equals the number of\nclasses in the validation split, e.g., 16 in the MiniImageNet),\nwhich contains 1 instance per class in the support set and\n15 instances per class for evaluation. Based on the penulti-\nmate layer instance embeddings of the pre-trained weights,\nwe utilize the nearest neighbor classiﬁers over the few-shot\ntasks and evaluate the quality of the backbone. We select\nthe pre-trained weights with the best few-shot classiﬁca-\ntion accuracy on the validation set. The pre-trained weights\nare used to initialize the embedding backbone E, and the\nweights of the whole model are then optimized together dur-\ning the model training.\nTransformer Hyper-parameters.\nWe follow the archi-\ntecture as presented in [47] to build our FEAT model.\nThe hidden dimension d′ for the linear transformation in\nour FEAT model is set to 64 for ConvNet and 640 for\nResNet/WRN. The dropout rate in transformer is set as 0.5.\nWe empirically observed that the shallow transformer (with\none set of projection and one stacked layer) gives the best\noverall performance (also studied in § D.2).\nOptimization.\nFollowing the literature, different optimiz-\ners are used for the backbones during the model training.\nFor the ConvNet backbone, stochastic gradient descent with\nAdam [20] optimizer is employed, with the initial learning\nrate set to be 0.002. For the ResNet and WRN backbones,\nvanilla stochastic gradient descent with Nesterov accelera-\ntion is used with an initial rate of 0.001. We ﬁx the weight\ndecay in SGD as 5e-4 and momentum as 0.9. The sched-\nule of the optimizers is tuned over the validation part of\nthe dataset. As the backbone network is initialized with the\npre-trained weights, we scale the learning rate for those pa-\nrameters by 0.1.\nD. Additional Experimental Results\nIn this section, we will show more experimental results\nover the MiniImageNet/CUB dataset, the ablation studies,\nand the extended few-shot learning.\nD.1. Main Results\nThe full results of all methods on the MiniImageNet can\nbe found in Table 5. The results of MAML [10] optimized\nover the pre-trained embedding network are also included.\nWe re-implement the ConvNet backbone of MAML and cite\nthe MAML results over the ResNet backbone from [40]. It\nis also noteworthy that the FEAT gets the best performance\namong all popular methods and baselines.\nWe also investigate the Wide ResNet (WRN) back-\nbone over MiniImageNet, which is also the popular one\nused in [35, 40].\nSimpleShot [51] is a recent proposed\nembedding-based few-shot learning approach that takes full\nadvantage of the pre-trained embeddings. We cite the re-\nsults of PFA [35], LEO [40], and SimpleShot [51] from\ntheir papers. The results can be found in Table 6. We re-\nimplement ProtoNet and our FEAT approach with WRN.\nIt is notable that in this case, our FEAT achieves much\nhigher promising results than the current state-of-the-art\napproaches. Table 7 shows the classiﬁcation results with\nWRN on the TieredImageNet data set, where our FEAT still\nkeeps its superiority when dealing with 1-shot tasks.\nTable 8 shows the 5-way 1-shot and 5-shot classiﬁcation\nresults on the CUB dataset based on the ConvNet back-\nbone. The results on CUB are consistent with the trend\non the MiniImageNet dataset. Embedding adaptation in-\ndeed assists the embedding encoder for the few-shot clas-\nsiﬁcation tasks.\nFacilitated by the set function property,\nthe DEEPSETS works better than the BILSTM counterpart.\nAmong all the results, the transformer based FEAT gets the\ntop tier results.\nD.2. Ablation Studies\nIn this section, we perform further analyses for our pro-\nposed FEAT and its ablated variants classifying in the Pro-\n14\n",
    "Table 5: Few-shot classiﬁcation accuracy± 95% conﬁdence interval on MiniImageNet with ConvNet and ResNet backbones. Our imple-\nmentation methods are measured over 10,000 test trials.\nSetups →\n1-Shot 5-Way\n5-Shot 5-Way\nBackbone Network →\nConvNet\nResNet\nConvNet\nResNet\nMatchNet [49]\n43.40± 0.78\n-\n51.09± 0.71\n-\nMAML [10]\n48.70± 1.84\n-\n63.11± 0.92\n-\nProtoNet [43]\n49.42± 0.78\n-\n68.20± 0.66\n-\nRelationNet [45]\n51.38± 0.82\n-\n67.07± 0.69\n-\nPFA [35]\n54.53± 0.40\n-\n67.87± 0.20\n-\nTADAM [33]\n-\n58.50± 0.30\n-\n76.70± 0.30\nMetaOptNet [25]\n-\n62.64± 0.61\n-\n78.63± 0.46\nBaselines\nMAML\n49.24± 0.21\n58.05± 0.10\n67.92± 0.17\n72.41± 0.20\nMatchNet\n52.87± 0.20\n65.64± 0.20\n67.49± 0.17\n78.72± 0.15\nProtoNet\n52.61± 0.20\n62.39± 0.21\n71.33± 0.16\n80.53± 0.14\nEmbedding Adaptation\nBILSTM\n52.13± 0.20\n63.90± 0.21\n69.15± 0.16\n80.63± 0.14\nDEEPSETS\n54.41± 0.20\n64.14± 0.22\n70.96± 0.16\n80.93± 0.14\nGCN\n53.25± 0.20\n64.50± 0.20\n70.59± 0.16\n81.65± 0.14\nOurs: FEAT\n55.15± 0.20\n66.78± 0.20\n71.61± 0.16\n82.05± 0.14\nTable 6:\nFew-shot classiﬁcation performance with Wide\nResNet (WRN)-28-10 backbone on MiniImageNet dataset (mean\naccuracy±95% conﬁdence interval). Our implementation meth-\nods are measured over 10,000 test trials.\nSetups →\n1-Shot 5-Way\n5-Shot 5-Way\nPFA [35]\n59.60± 0.41\n73.74± 0.19\nLEO [40]\n61.76± 0.08\n77.59± 0.12\nSimpleShot [51]\n63.50± 0.20\n80.33± 0.14\nProtoNet (Ours)\n62.60± 0.20\n79.97± 0.14\nOurs: FEAT\n65.10 ± 0.20\n81.11 ± 0.14\nTable 7: Few-shot classiﬁcation performance with Wide ResNet\n(WRN)-28-10 backbone on TieredImageNet dataset (mean\naccuracy±95% conﬁdence interval). Our implementation meth-\nods are measured over 10,000 test trials.\nSetups →\n1-Shot 5-Way\n5-Shot 5-Way\nLEO [40]\n66.33± 0.05\n81.44± 0.09\nSimpleShot [51]\n69.75± 0.20\n85.31± 0.15\nOurs: FEAT\n70.41 ± 0.23\n84.38 ± 0.16\ntoNet manner, on the MiniImageNet dataset, using the Con-\nvNet as the backbone network.\nDo the adapted embeddings improve the pre-adapted\nembeddings?\nWe report few-shot classiﬁcation results by\nTable 8: Few-shot classiﬁcation performance with ConvNet back-\nbone on CUB dataset (mean accuracy±95% conﬁdence interval).\nOur implementation methods are measured over 10,000 test trials.\nSetups →\n1-Shot 5-Way\n5-Shot 5-Way\nMatchNet [49]\n61.16 ± 0.89\n72.86 ± 0.70\nMAML [10]\n55.92 ± 0.95\n72.09 ± 0.76\nProtoNet [43]\n51.31 ± 0.91\n70.77 ± 0.69\nRelationNet [45]\n62.45 ± 0.98\n76.11 ± 0.69\nInstance Embedding\nMatchNet\n67.73 ± 0.23\n79.00 ± 0.16\nProtoNet\n63.72 ± 0.22\n81.50 ± 0.15\nEmbedding Adaptation\nBILSTM\n62.05 ± 0.23\n73.51 ± 0.19\nDEEPSETS\n67.22 ± 0.23\n79.65 ± 0.16\nGCN\n67.83 ± 0.23\n80.26 ± 0.15\nOurs: FEAT\n68.87 ± 0.22\n82.90 ± 0.15\nTable 9: Ablation studies on whether the embedding adaptation\nimproves the discerning quality of the embeddings. After embed-\nding adaptation, FEAT improves w.r.t. the before-adaptation em-\nbeddings a lot for Few-shot classiﬁcation.\n1-Shot 5-Way\n5-Shot 5-Way\nPre-Adapt\n51.60± 0.20\n70.40± 0.16\nPost-Adapt\n55.15± 0.20\n71.61± 0.16\n15\n",
    "5\n10\n15\n20\nNumber of categories per task\n30\n35\n40\n45\n50\n55\n60\n65\n70\n75\nMean accuracy (in %)\n70.7\n55.9\n47.5\n41.9\n71.3\n56.5\n48.2\n42.4\n71.5\n57.0\n48.8\n43.2\nMethods\nBILSTM\nDeepSets\nFEAT\n(a) Task Interpolation\n5\n10\n15\n20\nNumber of categories per task\n30\n35\n40\n45\n50\n55\n60\n65\n70\n75\n69.2\n52.9\n43.6\n37.5\n71.0\n55.9\n47.4\n41.8\n71.2\n56.7\n48.2\n42.6\nMethods\nBILSTM\nDeepSets\nFEAT\n(b) Task Extrapolation\nFigure 6: Interpolation and Extrapolation of few-shot tasks. We\ntrain different embedding adaptation models on 5-shot 20-way or\n5-way classiﬁcation tasks and evaluate models on unseen tasks\nwith different number of classes (N={5, 10, 15, 20}). It veri-\nﬁes both the interpolation and extrapolation ability of FEAT on a\nvarying number of ways in few-shot classiﬁcation.\nTable 10: Ablation studies on the position to average the same-\nclass embeddings when there are multiple shots per class in FEAT\n(tested on the 5-Way tasks with different numbers of shots). “Pre-\nAvg” and “Post-Avg” means we get the embedding center for each\nclass before or after the set-to-set transformation, respectively.\nSetups →\nPre-Avg\nPost-Avg\n5\n71.61± 0.16\n70.70± 0.16\n15\n77.76± 0.14\n76.58± 0.14\n30\n79.66± 0.13\n78.77± 0.13\nTable 11: Ablation studies on the number of heads in the Trans-\nformer of FEAT (with number of layers ﬁxes to one).\nSetups →\n1-Shot 5-Way\n5-Shot 5-Way\n1\n55.15± 0.20\n71.57± 0.16\n2\n54.91± 0.20\n71.44± 0.16\n4\n55.05± 0.20\n71.63± 0.16\n8\n55.22± 0.20\n71.39± 0.16\nTable 12: Ablation studies on the number of layers in the Trans-\nformer of FEAT (with number of heads ﬁxes to one).\nSetups →\n1-Shot 5-Way\n5-Shot 5-Way\n1\n55.15± 0.20\n71.57± 0.16\n2\n55.42± 0.20\n71.44± 0.16\n3\n54.96± 0.20\n71.63± 0.16\nusing the pre-adapted embeddings of support data (i.e., the\nembedding before adaptation), against those using adapted\nembeddings, for constructing classiﬁers. Table 9 shows that\ntask-speciﬁc embeddings after adaptation improves over\ntask-agnostic embeddings in few-shot classiﬁcations.\nCan FEAT possesses the characteristic of the set func-\ntion?\nWe test three set-to-set transformation implementa-\ntions, namely the BILSTM, the DEEPSETS, and the Trans-\nformer (FEAT), w.r.t. two important properties of the set\nfunction, i.e., task interpolation and task extrapolation. In\nparticular, the few-shot learning model is ﬁrst trained with\n5-shot 20-way tasks. Then the learned model is required\nto evaluate different 5-shot tasks with N = {5, 10, 15, 20}\n(Extrapolation). Similarly, for interpolation, the model is\ntrained with 5-shot 20-way tasks in advance and then eval-\nuated on the previous multi-way tasks. The classiﬁcation\nchange results can be found in Figure 6 (a) and (b). BILSTM\ncannot deal with the size change of the set, especially in the\ntask extrapolation. In both cases, FEAT still gets improve-\nments in all conﬁgurations of N.\nWhen to average the same-class embeddings?\nWhen\nthere is more than one instance per class, i.e. M > 1, we av-\nerage the instances in the same class and use the class center\nto make predictions as in Eq. 10. There are two positions\nto construct the prototypes in FEAT — before the set-to-set\ntransformation (Pre-Avg) and after the set-to-set transfor-\nmation (Post-Avg). In Pre-Avg, we adapt the embeddings\nof the centers, and a test instance is predicted based on\nits distance to the nearest adapted center; while in Post-\nAvg, the instance embeddings are adapted by the set-to-set\nfunction ﬁrst, and the class centers are computed based on\nthe adapted instance embeddings. We investigate the two\nchoices in Table 10, where we ﬁx the number of ways to\n5 (N = 5) and change the number of shots (M) among\n{5, 15, 30}. The results demonstrate the Pre-Avg version\nperforms better than the Post-Avg in all cases, which shows\na more precise input of the set-to-set function by averaging\nthe instances in the same class leads to better results. So we\nuse the Pre-Avg strategy as a default option in our experi-\nments.\nWill deeper and multi-head transformer help?\nIn our\ncurrent implementation of the set-to-set transformation\nfunction, we make use of a shallow and simple transformer,\ni.e., one layer and one head (set of projection). From [47],\nthe transformer can be equipped with complex components\nusing multiple heads and deeper stacked layers. We evalu-\nate this augmented structure, with the number of attention\nheads increases to 2, 4, 8, as well as with the number of\nlayers increases to 2 and 3. As in Table 11 and Table 12,\nwe empirically observe that more complicated structures do\nnot result in improved performance. We ﬁnd that with more\nlayers of transformer stacked, the difﬁculty of optimization\nincreases and it becomes harder to train models until their\nconvergence. Whilst for models with more heads, the mod-\nels seem to over-ﬁt heavily on the training data, even with\nthe usage of auxiliary loss term (like the contrastive loss in\n16\n",
    "Table 13: Ablation studies on effects of the contrastive learning\nof the set-to-set function on FEAT.\nSetups →\n1-Shot 5-Way\n5-Shot 5-Way\nλ = 10\n53.92 ± 0.20\n70.41 ± 0.16\nλ = 1\n54.84 ± 0.20\n71.00 ± 0.16\nλ = 0.1\n55.15 ± 0.20\n71.61 ± 0.16\nλ = 0.01\n54.67 ± 0.20\n71.26 ± 0.16\nTable 14: Ablation studies on the prediction strategy (with cosine\nsimilarity or euclidean distance) of FEAT.\nSetups →\n1-Shot 5-Way\n5-Shot 5-Way\nBackbone →\nConvNet\nResNet\nConvNet\nResNet\nCosine Similarity-based Prediction\nFEAT\n54.64± 0.20\n66.26± 0.20\n71.72± 0.16\n81.83± 0.15\nEuclidean Distance-based Prediction\nFEAT\n55.15± 0.20\n66.78± 0.20\n71.61± 0.16\n82.05± 0.14\nour approach). It might require some careful regularizations\nto prevent over-ﬁtting, which we leave for future work.\nThe effectiveness of contrastive loss.\nTable 13 show the\nfew-shot classiﬁcation results with different weight values\n(λ) of the contrastive loss term for FEAT. From the results,\nwe can ﬁnd that the balance of the contrastive term in the\nlearning objective can inﬂuence the ﬁnal results. Empiri-\ncally, we set λ = 0.1 in our experiments.\nThe inﬂuence of the prediction strategy.\nWe investi-\ngate two embedding-based prediction ways for the few-shot\nclassiﬁcation, i.e., based on the cosine similarity and the\nnegative euclidean distance to measure the relationship be-\ntween objects, respectively. We compare these two choices\nin Table 14.\nTwo strategies in Table 14 only differ in\ntheir similarity measures. In other words, with more than\none shot per class in the task training set, we average the\nsame class embeddings ﬁrst, and then make classiﬁcation\nby computing the cosine similarity or the negative euclidean\ndistance between a test instance and a class prototype. Dur-\ning the optimization, we tune the logits scale temperature\nfor both these methods. We ﬁnd that using the euclidean\ndistance usually requires small temperatures (e.g., γ =\n1\n64)\nwhile a large temperature (e.g., γ = 1) works well with the\nnormalized cosine similarity. The former choice achieves a\nslightly better performance than the latter one.\nD.3. Few-Shot Domain Generalization\nWe show that FEAT learns to adapt the intrinsic structure\nof tasks, and generalize across domains, i.e., predicting\ntest instances even when the visual appearance is changed.\nTable 15: Cross-Domain 1-shot 5-way classiﬁcation results of the\nFEAT approach.\nC →C\nC →R\nR →R\nSupervised\n34.38±0.16\n29.49±0.16\n37.43±0.16\nProtoNet\n35.51±0.16\n29.47±0.16\n37.24±0.16\nFEAT\n36.83±0.17\n30.89±0.17\n38.49±0.16\nTable 16: Results of models for transductive FSL with ConvNet\nbackbone on MiniImageNet. We cite the results of Semi-ProtoNet\nand TPN from [38] and [34] respectively. For TEAM [34], the au-\nthors do not report the conﬁdence intervals, so we set them to 0.00\nin the table.\nFEAT† and FEAT‡ adapt embeddings with the joint\nset of labeled training and unlabeled test instances, while make\nprediction via ProtoNet and Semi-ProtoNet respectively.\nSetups →\n1-Shot 5-Way\n5-Shot 5-Way\nStandard\nProtoNet\n52.61 ± 0.20\n71.33 ± 0.16\nFEAT\n55.15 ± 0.20\n71.61 ± 0.16\nTransductive\nSemi-ProtoNet [38]\n50.41 ± 0.31\n64.39 ± 0.24\nTPN [30]\n55.51 ± 0.84\n69.86 ± 0.67\nTEAM [34]\n56.57 ± 0.00\n72.04 ± 0.00\nSemi-ProtoNet (Ours)\n55.50 ± 0.10\n71.76 ± 0.08\nFEAT†\n56.49 ± 0.16\n72.65 ± 0.20\nFEAT‡\n57.04 ± 0.16\n72.89 ± 0.20\nSetups. We train a few-shot learning model in the standard\ndomain and evaluate it with cross-domain tasks, where the\nN-categories are aligned but domains are different. In de-\ntail, a model is trained on tasks from the “Clipart” domain\nof OfﬁceHome dataset [48], then the model is required to\ngeneralize to both “Clipart (C)” and “Real World (R)” in-\nstances. In other words, we need to classify complex real\nimages by seeing only a few sketches, or even based on the\ninstances in the “Real World (R)” domain.\nResults. Table 15 gives the quantitative results. Here, the\n“supervised” refers to a model trained with standard clas-\nsiﬁcation and then is used for the nearest neighbor classi-\nﬁer with its penultimate layer’s output feature. We observe\nthat ProtoNet can outperform this baseline on tasks when\nevaluating instances from “Clipart” but not ones from “real\nworld”. However, FEAT can improve over “real world” few-\nshot classiﬁcation even only seeing the support data from\n“Clipart”. Besides, when the support set and the test set\nof the target task are sampled from the same but new do-\nmains, e.g., the training and test instances both come from\n“real world”, FEAT also improves the classiﬁcation accuracy\nw.r.t. the baseline methods. It veriﬁes the domain general-\nization ability of the FEAT approach.\n17\n",
    "D.4. Additional Discussions on Transductive FSL\nWe list the results of the transductive few-shot classiﬁ-\ncation in Table 16, where the unlabeled test instances ar-\nrive simultaneously, so that the common structure among\nthe unlabeled test instances could be captured. We com-\npare with three approaches, Semi-ProtoNet [38], TPN [30],\nand TEAM [34]. Semi-ProtoNet utilizes the unlabeled in-\nstances to facilitate the computation of the class center and\nmakes predictions similar to the prototypical network; TPN\nmeta learns a label propagation way to take the unlabeled in-\nstances relationship into consideration; TEAM explores the\npairwise constraints in each task, and formulates the em-\nbedding adaptation into a semi-deﬁnite programming form.\nWe cite the results of Semi-ProtoNet from [38], and cite\nthe results of TPN and TEAM from [34].\nWe also re-\nimplement Semi-ProtoNet with our pre-trained backbone\n(the same pre-trained ConvNet weights as the standard few-\nshot learning setting) for a fair comparison.\nIn this setting, our model leverages the unlabeled test in-\nstances to augment the transformer as discussed in § B.4\nand the embedding adaptation takes the relationship of all\ntest instances into consideration. Based on the adapted em-\nbedding by the joint set of labeled training instances and\nunlabeled test instances, we can make predictions with two\nstrategies. First, we still compute the center of the labeled\ninstances, while such adapted embeddings are inﬂuenced by\nthe unlabeled instances (we denote this approach as FEAT†,\nwhich works the same way as standard FEAT except the aug-\nmented input of the embedding transformation function);\nSecond, we consider to take advantage of the unlabeled in-\nstances and use their adapted embeddings to construct a bet-\nter class prototype as in Semi-ProtoNet (we denote this ap-\nproach as FEAT‡).\nBy using more unlabeled test instances in the transduc-\ntive environment, FEAT† achieves further performance im-\nprovement compared with the standard FEAT, which veriﬁes\nthe unlabeled instances could assist the embedding adapta-\ntion of the labeled ones. With more accurate class center\nestimation, FEAT‡ gets a further improvement. The per-\nformance gain induced by the transductive FEAT is more\nsigniﬁcant in the one-shot learning setting compared with\nthe ﬁve-shot scenario, since the helpfulness of unlabeled in-\nstance decreases when there are more labeled instances.\nD.5. More Generalized FSL Results\nHere we show the full results of FEAT in the general-\nized few-shot learning setting in Table 17, which includes\nboth the 1-shot and 5-shot performance. All methods are\nevaluated on instances composed by SEEN classes, UNSEEN\nclasses, and both of them (COMBINED), respectively. In\nthe 5-shot scenario, the performance improvement mainly\ncomes from the improvement of over the UNSEEN tasks.\nTable 17: Results of generalized FEAT with ConvNet backbone on\nMiniImageNet. All methods are evaluated on instances composed\nby SEEN classes, UNSEEN classes, and both of them (COMBINED),\nrespectively.\nMeasures →\nSEEN\nUNSEEN\nCOMBINED\n1-shot learning\nProtoNet\n41.73±0.03\n48.64±0.20\n35.69±0.03\nFEAT\n43.94±0.03\n49.72±0.20\n40.50±0.03\n5-shot learning\nProtoNet\n41.06±0.03\n64.94±0.17\n38.04±0.02\nFEAT\n44.94±0.03\n65.33±0.16\n41.68±0.03\nRandom Chance\n1.56\n20.00\n1.45\nTable 18: The top-5 low-shot learning accuracy over all classes\non the large scale ImageNet [39] dataset (w/ ResNet-50).\nUNSEEN\n1-Shot\n2-Shot\n5-Shot\n10-Shot\n20-Shot\nProtoNet [43]\n49.6\n64.0\n74.4\n78.1\n80.0\nPMN [52]\n53.3\n65.2\n75.9\n80.1\n82.6\nFEAT\n53.8\n65.4\n76.0\n81.2\n83.6\nAll\n1-Shot\n2-Shot\n5-Shot\n10-Shot\n20-Shot\nProtoNet [43]\n61.4\n71.4\n78.0\n80.0\n81.1\nPMN [52]\n64.8\n72.1\n78.8\n81.7\n83.3\nFEAT\n65.1\n72.5\n79.3\n82.1\n83.9\nAll w/ Prior\n1-Shot\n2-Shot\n5-Shot\n10-Shot\n20-Shot\nProtoNet [43]\n62.9\n70.5\n77.1\n79.5\n80.8\nPMN [52]\n63.4\n70.8\n77.9\n80.9\n82.7\nFEAT\n63.8\n71.2\n78.1\n81.3\n83.4\nD.6. Large-Scale Low-Shot Learning\nSimilar to the generalized few-shot learning, the large-\nscale low-shot learning [12, 14, 52] considers the few-shot\nclassiﬁcation ability on both SEEN and UNSEEN classes on\nthe full ImageNet [39] dataset. There are in total 389 SEEN\nclasses and 611 UNSEEN classes [14]. We follow the setting\n(including the splits) of the prior work [14] and use features\nextracted based on the pre-trained ResNet-50 [15]. Three\nevaluation protocols are evaluated, namely the top-5 few-\nshot accuracy on the UNSEEN classes, on the combined set\nof both SEEN and UNSEEN classes, and the calibrated accu-\nracy on weighted by selected set prior on the combined set\nof both SEEN and UNSEEN classes. The results are listed in\nTable 18. We observe that FEAT achieves better results than\nothers, which further validates FEAT’s superiority in gener-\nalized classiﬁcation setup, a large scale learning setup.\n18\n"
  ],
  "full_text": "Few-Shot Learning via Embedding Adaptation with Set-to-Set Functions\nHan-Jia Ye*\nNanjing University\nyehj@lamda.nju.edu.cn\nHexiang Hu\nUSC\nhexiangh@usc.edu\nDe-Chuan Zhan\nNanjing University\nzhandc@lamda.nju.edu.cn\nFei Sha†\nUSC & Google\nfsha@google.com\nAbstract\nLearning with limited data is a key challenge for vi-\nsual recognition. Many few-shot learning methods address\nthis challenge by learning an instance embedding function\nfrom seen classes and apply the function to instances from\nunseen classes with limited labels. This style of transfer\nlearning is task-agnostic: the embedding function is not\nlearned optimally discriminative with respect to the unseen\nclasses, where discerning among them leads to the tar-\nget task. In this paper, we propose a novel approach to\nadapt the instance embeddings to the target classiﬁcation\ntask with a set-to-set function, yielding embeddings that are\ntask-speciﬁc and are discriminative. We empirically investi-\ngated various instantiations of such set-to-set functions and\nobserved the Transformer is most effective — as it naturally\nsatisﬁes key properties of our desired model. We denote this\nmodel as FEAT (few-shot embedding adaptation w/ Trans-\nformer) and validate it on both the standard few-shot classi-\nﬁcation benchmark and four extended few-shot learning set-\ntings with essential use cases, i.e., cross-domain, transduc-\ntive, generalized few-shot learning, and low-shot learning.\nIt archived consistent improvements over baseline models\nas well as previous methods, and established the new state-\nof-the-art results on two benchmarks.\n1. Introduction\nFew-shot visual recognition [10, 23, 24, 27, 49] emerged\nas a promising direction in tackling the challenge of learn-\ning new visual concepts with limited annotations.\nCon-\ncretely, it distinguishes two sets of visual concepts: SEEN\nand UNSEEN ones. The target task is to construct visual\nclassiﬁers to identify classes from the UNSEEN where each\nclass has a very small number of exemplars (“few-shot”).\nThe main idea is to discover transferable visual knowl-\nedge in the SEEN classes, which have ample labeled in-\nstances, and leverage it to construct the desired classiﬁer.\nFor example, state-of-the-art approaches for few-shot learn-\n*Work mostly done when the author was a visiting scholar at USC.\n†On leave from USC\ning [40, 43, 46, 49] usually learn a discriminative instance\nembedding model on the SEEN categories, and apply it to\nvisual data in UNSEEN categories. In this common embed-\nding space, non-parametric classiﬁers (e.g., nearest neigh-\nbors) are then used to avoid learning complicated recogni-\ntion models from a small number of examples.\nSuch approaches suffer from one important limitation.\nAssuming a common embedding space implies that the dis-\ncovered knowledge – discriminative visual features – on\nthe SEEN classes are equally effective for any classiﬁcation\ntasks constructed for an arbitrary set of UNSEEN classes. In\nconcrete words, suppose we have two different target tasks:\ndiscerning “cat” versus “dog” and discerning “cat” versus\n“tiger”. Intuitively, each task uses a different set of discrim-\ninative features. Thus, the most desired embedding model\nﬁrst needs to be able to extract discerning features for either\ntask at the same time. This could be a challenging aspect in\nits own right as the current approaches are agnostic to what\nthose “downstream” target tasks are and could accidentally\nde-emphasize selecting features for future use. Secondly,\neven if both sets of discriminative features are extracted,\nthey do not necessarily lead to the optimal performance for\na speciﬁc target task. The most useful features for discern-\ning “cat” versus “tiger” could be irrelevant and noise to the\ntask of discerning “cat” versus “dog”!\nWhat is missing from the current few-shot learning ap-\nproaches is an adaptation strategy that tailors the visual\nknowledge extracted from the SEEN classes to the UNSEEN\nones in a target task. In other words, we desire separate em-\nbedding spaces where each one of them is customized such\nthat the visual features are most discriminative for a given\ntask. Towards this, we propose a few-shot model-based em-\nbedding adaptation method that adjusts the instance embed-\nding models derived from the SEEN classes. Such model-\nbased embedding adaptation requires a set-to-set function: a\nfunction mapping that takes all instances from the few-shot\nsupport set and outputs the set of adapted support instance\nembeddings, with elements in the set co-adapting with each\nother. Such output embeddings are then assembled as the\nprototypes for each visual category and serve as the near-\nest neighbor classiﬁers.\nFigure 1 qualitatively illustrates\n1\narXiv:1812.03664v6  [cs.LG]  13 Jun 2021\n\n\nMalamute\nAnt\nSchool bus\nGolden retriever\nTheater curtain\nAdaptation\nLion\nSchool bus\nHourglass\nVase\nTrifle\nAdaptation\nTrifle\nScoreboard\nGolden retriever\nDalmatian\nVase\nAdaptation\nGolden retriever\nNematode\nLion\nDalmatian\nMalamute\nAdaptation\n(a) Acc↑: 40.33% →55.33%\n(b) Acc↑: 48.00% →69.60%\n(c) Acc↑: 43.60% →63.33%\n(d) Acc↓: 56.33% →47.13%\nFigure 1: Qualitative visualization of model-based embedding adaptation procedure (implemented using FEAT) on test tasks (refer to\n§ 5.2.2 for more details). Each ﬁgure shows the locations of PCA projected support embeddings (class prototypes) before and after the\nadaptation of FEAT. Values below are the 1-shot 5-way classiﬁcation accuracy before and after the the adaptation. Interestingly, the\nembedding adaptation step of FEAT pushes the support embeddings apart from the clutter and toward their own clusters, such that they can\nbetter ﬁts the test data of its categories. (Best view in colors!)\nthe embedding adaptation procedure (as results of our best\nmodel). These class prototypes spread out in the embedding\nspace toward the samples cluster of each category, indicat-\ning the effectiveness of embedding adaptation.\nIn this paper, we implement the set-to-set transformation\nusing a variety of function approximators, including bidi-\nrectional LSTM [16] (Bi-LSTM), deep sets [56], graph con-\nvolutional network (GCN) [21], and Transformer [29, 47].\nOur experimental results (refer to § 5.2.1) suggest that\nTransformer is the most parameter efﬁcient choice that at\nthe same time best implements the key properties of the de-\nsired set-to-set transformation, including contextualization,\npermutation invariance, interpolation and extrapolation ca-\npabilities (see § 4.1). As a consequence, we choose the\nset-to-set function instantiated with Transformer to be our\nﬁnal model and denote it as FEAT (Few-shot Embedding\nAdaptation with Transformer). We further conduct compre-\nhensive analysis on FEAT and evaluate it on many extended\ntasks, including few-shot domain generalization, transduc-\ntive few-shot learning, and generalized few-shot learning.\nOur overall contribution is three-fold.\n• We formulate the few-shot learning as a model-based em-\nbedding adaptation to make instance embeddings task-\nspeciﬁc, via using a set-to-set transformation.\n• We instantiate such set-to-set transformation with various\nfunction approximators, validating and analyzing their\nfew-shot learning ability, task interpolation ability, and\nextrapolation ability, etc. It concludes our model (FEAT)\nthat uses the Transformer as the set-to-set function.\n• We evaluate our FEAT model on a variety of extended\nfew-shot learning tasks, where it achieves superior per-\nformances compared with strong baseline approaches.\n2. Related Work\nMethods speciﬁcally designed for few-shot learning fall\nbroadly into two categories. The ﬁrst is to control how a\nclassiﬁer for the target task should be constructed.\nOne\nfruitful idea is the meta-learning framework where the\nclassiﬁers are optimized in anticipation that a future up-\ndate due to data from a new task performs well on that\ntask [2, 3, 10, 13, 26, 32, 36, 40], or the classiﬁer itself is\ndirectly meta-predicted by the new task data [35, 53].\nAnother line of approach has focused on learning gener-\nalizable instance embeddings [1, 5, 6, 17, 22, 31, 42, 46, 49]\nand uses those embeddings on simple classiﬁers such as\nnearest neighbor rules. The key assumption is that the em-\nbeddings capture all necessarily discriminative representa-\ntions of data such that simple classiﬁers are sufﬁced, hence\navoiding the danger of overﬁtting on a small number of la-\nbeled instances. Early work such as [22] ﬁrst validated the\nimportance of embedding in one-shot learning, whilst [49]\nproposes to learn the embedding with a soft nearest neigh-\nbor objective, following a meta-learning routine. Recent\nadvances have leveraged different objective functions for\nlearning such embedding models, e.g., considering the class\nprototypes [43], decision ranking [46], and similarity com-\nparison [45]. Most recently, [41] utilizes the graph convo-\nlution network [21] to unify the embedding learning.\nOur work follows the second school of thoughts. The\nmain difference is that we do not assume the embed-\ndings learned on SEEN classes, being agnostic to the tar-\nget tasks, are necessarily discriminative for those tasks.\nIn contrast, we propose to adapt those embeddings for\neach target task with a set-to-set function so that the trans-\nformed embeddings are better aligned with the discrimina-\ntion needed in those tasks. We show empirically that such\ntask-speciﬁc embeddings perform better than task-agnostic\nones.\nMetaOptNet [25] and CTM [28] follow the same\nspirit of learning task-speciﬁc embedding (or classiﬁers) via\neither explicitly optimization of target task or using concen-\ntrator and projector to make distance metric task-speciﬁc.\n2\n\n\nClassification \nScores\nCNN\nCNN\nCNN\nCNN\nSoft Nearest \nNeighbor\n(a) Instance Embedding\nClassification \nScores\nEmbedding \nAdaptation\nCNN\nCNN\nCNN\nCNN\nSoft Nearest \nNeighbor\nTrain Instance\nTest Instance\nTask Agnostic\nEmbedding\nTask Specific \nEmbedding\n(b) Embedding Adaptation\nSet-to-Set Function\nFigure 2:\nIllustration of the proposed Few-Shot Embedding\nAdaptation Transformer (FEAT). Existing methods usually use the\nsame embedding function E for all tasks. We propose to adapt the\nembeddings to each target few-shot learning task with a set-to-set\nfunction such as Transformer, BiLSTM, DeepSets, and GCN.\n3. Learning Embedding for Task-agnostic FSL\nIn the standard formulation of few-shot learning\n(FSL) [10, 49], a task is represented as a M-shot N-way\nclassiﬁcation problem with N classes sampled from a set\nof visual concepts U and M (training/support) examples\nper class.\nWe denote the training set (also referred as\nsupport sets in the literature) as Dtrain = {xi, yi}NM\ni=1 ,\nwith the instance xi ∈RD and the one-hot labeling vec-\ntor yi ∈{0, 1}N. We will use “support set” and “train-\ning set” interchangeably in the paper. In FSL, M is of-\nten small (e.g., M = 1 or M = 5).\nThe goal is to\nﬁnd a function f that classiﬁes a test instance xtest by\nˆytest = f(xtest; Dtrain) ∈{0, 1}N.\nGiven a small number of training instances, it is chal-\nlenging to construct complex classiﬁers f(·). To this end,\nthe learning algorithm is also supplied with additional data\nconsisting of ample labeled instances. These additional data\nare drawn from visual classes S, which does not overlap\nwith U. We refer to the original task as the target task which\ndiscerns N UNSEEN classes U. To avoid confusion, we de-\nnote the data from the SEEN classes S as DS.\nTo learn f(·) using DS, we synthesize many M-shot N-\nway FSL tasks by sampling the data in the meta-learning\nmanner [10, 49]. Each sampling gives rise to a task to clas-\nsify a test set instance xS\ntest into one of the N SEEN classes\nby f(·), where the test instances set DS\ntest is composed of\nthe labeled instances with the same distribution as DS\ntrain.\nFormally, the function f(·) is learnt to minimize the aver-\naged error over those sampled tasks\nf ∗= arg min\nf\nX\n(xS\ntest,yS\ntest)∈DS\ntest\nℓ(f(xS\ntest; DS\ntrain), yS\ntest)\n(1)\nwhere the loss ℓ(·) measures the discrepancy between the\nprediction and the true label. For simplicity, we have as-\nsumed we only synthesize one task with test set DS\ntest. The\noptimal f ∗is then applied to the original target task.\nWe consider the approach based on learning embeddings\nAlgorithm 1 Training strategy of embedding adaptation\nRequire: Seen class set S\n1: for all iteration = 1,...,MaxIteration do\n2:\nSample N-way M-shot (DS\ntrain, DS\ntest) from S\n3:\nCompute φx = E(x), for x ∈X S\ntrain ∪X S\ntest\n4:\nfor all (xS\ntest, yS\ntest) ∈DS\ntest do\n5:\nCompute {ψx ; ∀x ∈X S\ntrain} with T via Eq. 3\n6:\nPredict ˆyS\ntest with {ψx} as Eq. 4\n7:\nCompute ℓ(ˆyS\ntest, yS\ntest) with Eq. 1\n8:\nend for\n9:\nCompute ∇E,T\nP\n(xS\ntest,yS\ntest)∈DS\ntest ℓ(ˆyS\ntest, yS\ntest)\n10:\nUpdate E and T with ∇E,T use SGD\n11: end for\n12: return Embedding function E and set function T.\nfor FSL [43, 49] (see Figure 2 (a) for an overview). In par-\nticular, the classiﬁer f(·) is composed of two elements. The\nﬁrst is an embedding function φx = E(x) ∈Rd that maps\nan instance x to a representation space. The second compo-\nnent applies the nearest neighbor classiﬁers in this space:\nˆytest = f(φxtest; {φx, ∀(x, y) ∈Dtrain})\n(2)\n∝exp\n\u0000sim(φxtest, φx)\n\u0001\n· y, ∀(x, y) ∈Dtrain\nNote that only the embedding function is learned by opti-\nmizing the loss in Eq. 1. For reasons to be made clear in\nbelow, we refer this embedding function as task-agnostic.\n4. Adapting Embedding for Task-speciﬁc FSL\nIn what follows, we describe our approach for few-shot\nlearning (FSL). We start by describing the main idea (§ 4.1,\nalso illustrated in Figure 2), then introduce the set-to-set\nadaptation function (§ 4.2). Last are learning (§ 4.3) and\nimplementations details (§ 4.4).\n4.1. Adapting to Task-Speciﬁc Embeddings\nThe key difference between our approach and traditional\nones is to learn task-speciﬁc embeddings. We argue that the\nembedding φx is not ideal. In particular, the embeddings do\nnot necessarily highlight the most discriminative represen-\ntation for a speciﬁc target task. To this end, we introduce an\nadaption step where the embedding function φx (more pre-\ncisely, its values on instances) is transformed. This trans-\nformation is a set-to-set function that contextualizes over\nthe image instances of a set, to enable strong co-adaptation\nof each item.\nInstance functions fails to have such co-\nadaptation property.\nFurthermore, the set-to-set-function\nreceives instances as bags, or sets without orders, requiring\nthe function to output the set of reﬁned instance embeddings\n3\n\n\nwhile being permutation-invariant. Concretely,\n{ψx ; ∀x ∈Xtrain} = T ({φx ; ∀x ∈Xtrain})\n(3)\n= T (π {φx ; ∀x ∈Xtrain}))\nwhere Xtrain is a set of all the instances in the training set\nDtrain for the target task. π(·) is a permutation operator\nover a set. Thus the set of adapted embedding will not\nchange if we apply a permutation over the input embedding\nset. With adapted embedding ψx, the test instance xtest can\nbe classiﬁed by computing nearest neighbors w.r.t. Dtrain:\nˆytest = f(φxtest; {ψx, ∀(x, y) ∈Dtrain})\n(4)\nOur approach is generally applicable to different types of\ntask-agnostic embedding function E and similarity measure\nsim(·, ·), e.g., the (normalized) cosine similarity [49] or the\nnegative distance [43]. Both the embedding function E and\nthe set transformation function T are optimized over syn-\nthesized FSL tasks sampled from DS, sketched in Alg. 1.\nIts key difference from conventional FSL is in the line 4 to\nline 8 where the embeddings are transformed.\n4.2. Embedding Adaptation via Set-to-set Functions\nNext, we explain various choices as the instantiations of\nthe set-to-set embedding adaptation function.\nBidirectional LSTM (BILSTM) [16, 49] is one of the\ncommon choice to instantiate the set-to-set transformation,\nwhere the addition between the input and the hidden layer\noutputs of each BILSTM cell leads to the adapted embed-\nding. It is notable that the output of the BILSTM suppose to\ndepend on the order of the input set. Note that using BIL-\nSTM as embedding adaptation model is similar but different\nfrom the fully conditional embedding [49], where the later\none contextualizes both training and test instance embed-\nding altogether, which results in a transductive setting.\nDeepSets [56] is inherently a permutation-invariant trans-\nformation function. It is worth noting that DEEPSETS aggre-\ngates the instances in a set into a holistic set vector. We con-\nsider two components to implement such DeepSets transfor-\nmation, an instance centric set vector combined with a set\ncontext vector. For x ∈Xtrain, we deﬁne its complemen-\ntary set as x∁. Then we implement the DEEPSETS by:\nψx = φx + g([φx;\nX\nxi′∈x∁\nh(φxi′)])\n(5)\nIn Eq. 5, g and h are two-layer multi-layer perception\n(MLP) with ReLU activation which map the embedding\ninto another space and increase the representation ability\nof the embedding. For each instance, embeddings in its\ncomplementary set is ﬁrst combined into a set vector as the\ncontext, and then this vector is concatenated with the input\nembedding to obtain the residual component of adapted em-\nbedding. This conditioned embedding takes other instances\nin the set into consideration, and keeps the “set (permutation\ninvariant)” property. In practice, we ﬁnd using the maxi-\nmum operator in Eq. 5 works better than the sum operator\nsuggested in [56].\nGraph Convolutional Networks (GCN) [21, 41] propa-\ngate the relationship between instances in the set. We ﬁrst\nconstruct the degree matrix A to represent the similarity be-\ntween instances in a set. If two instances come from the\nsame class, then we set the corresponding element in A to\n1, otherwise to 0. Based on A, we build the “normalized”\nadjacency matrix S for a given set with added self-loops\nS = D−1\n2 (A + I)D−1\n2 . I is the identity matrix, and D is\nthe diagonal matrix whose elements are equal to the sum of\nelements in the corresponding row of A + I.\nLet Φ0 = {φx ; ∀x ∈Xtrain}, the relationship between\ninstances could be propagated based on S, i.e.,\nΦt+1 = ReLU(SΦtW) , t = 0, 1, . . . , T −1\n(6)\nW is a projection matrix for feature transformation.\nIn\nGCN, the embedding in the set is transformed based on\nEq. 6 multiple times, and the ﬁnal ΦT gives rise to the {ψx}.\nTransformer. [47] We use the Transformer architec-\nture [47] to implement T. In particular, we employ self-\nattention mechanism [29, 47] to transform each instance\nembedding with consideration to its contextual instances.\nNote that it naturally satisﬁes the desired properties of T\nbecause it outputs reﬁned instance embeddings and is per-\nmutation invariant. We denote it as Few-Shot Embedding\nAdaptation with Transformer (FEAT).\nTransformer is a store of triplets in the form of (query\nQ, key K, and value V). To compute proximity and re-\nturn values, those points are ﬁrst linearly mapped into some\nspace K = W ⊤\nK\n\u0002\nφxk; ∀xk ∈K\n\u0003\n∈Rd×|K|, which\nis also the same for Q and V with WQ and WV respec-\ntively. Transformer computes what is the right value for a\nquery point — the query xq ∈Q is ﬁrst matched against\na list of keys K where each key has a value V . The ﬁ-\nnal value is then returned as the sum of all the values\nweighted by the proximity of the key to the query point,\ni.e. ψxq = φxq + P\nk αqkV:,k, where\nαqk ∝exp\n \nφ⊤\nxqWQ · K\n√\nd\n!\nand V:,k is the k-th column of V . In the standard FSL setup,\nwe have Q = K = V = Xtrain.\n4.3. Contrastive Learning of Set-to-Set Functions\nTo facilitate the learning of embedding adaptation, we\napply a contrastive objective in addition to the general one.\n4\n\n\nIt is designed to make sure that instances embeddings af-\nter adaptation is similar to the same class neighbors and\ndissimilar to those from different classes. Speciﬁcally, the\nembedding adaptation function T is applied to instances of\neach n of the N class in DS\ntrain ∪DS\ntest, which gives rise to\nthe transformed embedding ψ′\nx and class centers {cn}N\nn=1.\nThen we apply the contrastive objective to make sure train-\ning instances are close to its own class center than other\ncenters. The total objective function (together with Eq. 1) is\nshown as following:\nL(ˆytest, ytest) = ℓ(ˆytest, ytest)\n(7)\n+λ · ℓ\n\u0000softmax\n\u0000sim(ψ′\nxtest, cn)\n\u0001\n, ytest\n\u0001\nThis contrastive learning makes the set transformation ex-\ntract common characteristic for instances of the same cate-\ngory, so as to preserve the category-wise similarity.\n4.4. Implementation details\nWe consider three different types of convolutional net-\nworks as the backbone for instance embedding function E:\n1) A 4-layer convolution network (ConvNet) [43, 46, 49]\nand 2) the 12-layer residual network (ResNet) used in [25],\nand 3) the Wide Residual Network (WideResNet) [40, 55].\nWe apply an additional pre-training stage for the backbones\nover the SEEN classes, based on which our re-implemented\nmethods are further optimized. To achieve more precise em-\nbedding, we average the same-class instances in the train-\ning set before the embedding adaptation with the set-to-\nset transformation. Adam [20] and SGD are used to op-\ntimize ConvNet and ResNet variants respectively. More-\nover, we follow the most standard implementations for the\nfour set-to-set functions — BiLSTM [16], DeepSets [56],\nGraph Convolutional Networks (GCN) [21] and Trans-\nformer (FEAT) [47].\nWe refer readers to supplementary\nmaterial (SM) for complete details and ablation studies of\neach set-to-set functions. Our implementation is available\nat https://github.com/Sha-Lab/FEAT.\n5. Experiments\nIn this section, we ﬁrst evaluate a variety of models for\nembedding adaptation in § 5.2 with standard FSL. It con-\ncludes that FEAT (with Transformer) is the most effective\napproach among different instantiations. Next, we perform\nablation studies in § 5.2.2 to analyze FEAT in details. Even-\ntually, we evaluate FEAT on many extended few-shot learn-\ning tasks to study its general applicability (§ 5.3).\nThis\nstudy includes few-shot domain generalization, transduc-\ntive few-shot learning, generalized few-shot learning, and\nlarge-scale low-shot learning (refer to SM).\n5.1. Experimental Setups\nDatasets.\nMiniImageNet [49] and TieredImageNet [38]\ndatasets are subsets of the ImageNet [39]. MiniImageNet\nincludes a total number of 100 classes and 600 examples\nper class. We follow the setup provided by [36], and use\n64 classes as SEEN categories, 16 and 20 as two sets of\nUNSEEN categories for model validation and evaluation re-\nspectively.\nTieredImageNet is a large-scale dataset with\nmore categories, which contains 351, 97, and 160 categories\nfor model training, validation, and evaluation, respectively.\nIn addition to these, we investigate the OfﬁceHome [48]\ndataset to validate the generalization ability of FEAT across\ndomains. There are four domains in OfﬁceHome, and two\nof them (“Clipart” and “Real World”) are selected, which\ncontains 8722 images. After randomly splitting all classes,\n25 classes serve as the seen classes to train the model, and\nthe remaining 15 and 25 classes are used as two UNSEEN\nfor evaluation. Please refer to SM for more details.\nEvaluation protocols. Previous approaches [10, 43, 46]\nusually follow the original setting of [49] and evaluate the\nmodels on 600 sampled target tasks (15 test instances per\nclass).\nIn a later study [40], it was suggested that such\nan evaluation process could potentially introduce high vari-\nances. Therefore, we follow the new and more trustworthy\nevaluation setting to evaluate both baseline models and our\napproach on 10,000 sampled tasks. We report the mean ac-\ncuracy (in %) as well as the 95% conﬁdence interval.\nBaseline and embedding adaptation methods.\nWe re-\nimplement the prototypical network (ProtoNet) [43] as a\ntask-agnostic embedding baseline model. This is known\nas a very strong approach [8] when the backbone archi-\ntecture is deep, i.e., residual networks [15]. As suggested\nby [33], we tune the scalar temperature carefully to scale\nthe logits of both approaches in our re-implementation. As\nmentioned, we implement the embedding adaptation model\nwith four different function approximators, and denote them\nas BILSTM, DEEPSETS, GCN, and FEAT (i.e. Transformer).\nThe concrete details of each model are included in the SM.\nBackbone pre-training.\nInstead of optimizing from\nscratch, we apply an additional pre-training strategy as\nsuggested in [35, 40]. The backbone network, appended\nwith a softmax layer, is trained to classify all SEEN\nclasses with the cross-entropy loss (e.g., 64 classes in the\nMiniImageNet).\nThe classiﬁcation performance over the\npenultimate layer embeddings of sampled 1-shot tasks from\nthe model validation split is evaluated to select the best pre-\ntrained model, whose weights are then used to initialize the\nembedding function E in the few-shot learning.\n5.2. Standard Few-Shot Image Classiﬁcation\nWe compare our proposed FEAT method with the in-\nstance embedding baselines as well as previous methods on\n5\n\n\nTable 1: Few-shot classiﬁcation accuracy on MiniImageNet. ⋆\nCTM [28] and SimpleShot [51] utilize the ResNet-18. (see SM\nfor the full table with conﬁdence intervals and WRN results.).\nSetups →\n1-Shot 5-Way\n5-Shot 5-Way\nBackbone →\nConvNet\nResNet\nConvNet\nResNet\nMatchNet [49]\n43.40\n-\n51.09\n-\nMAML [10]\n48.70\n-\n63.11\n-\nProtoNet [43]\n49.42\n-\n68.20\n-\nRelationNet [45]\n51.38\n-\n67.07\n-\nPFA [35]\n54.53\n59.60\n67.87\n73.74\nTADAM [33]\n-\n58.50\n-\n76.70\nMetaOptNet [25]\n-\n62.64\n-\n78.63\nCTM [28]\n-\n64.12\n-\n80.51\nSimpleShot [51]\n49.69\n62.85\n66.92\n80.02\nInstance embedding\nProtoNet\n52.61\n62.39\n71.33\n80.53\nEmbedding adaptation\nBILSTM\n52.13\n63.90\n69.15\n80.62\nDEEPSETS\n54.41\n64.14\n70.96\n80.93\nGCN\n53.25\n64.50\n70.59\n81.65\nFEAT\n55.15\n66.78\n71.61\n82.05\nthe standard MiniImageNet [49] and TieredImageNet [38]\nbenchmarks, and then perform detailed analysis on the ab-\nlated models. We include additional results with CUB [50]\ndataset in SM, which shares a similar observation.\n5.2.1. Main Results\nComparison to previous State-of-the-arts. Table 1 and\nTable 2 show the results of our method and others on the\nMiniImageNet and TieredImageNet. First, we observe that\nthe best embedding adaptation method (FEAT) outperforms\nthe instance embedding baseline on both datasets, indi-\ncating the effectiveness of learning task-speciﬁc embed-\nding space. Meanwhile, the FEAT model performs signif-\nicantly better than the current state-of-the-art methods on\nMiniImageNet dataset. On the TieredImageNet, we observe\nthat the ProtoNet baseline is already better than some pre-\nvious state-of-the-arts based on the 12-layer ResNet back-\nbone.\nThis might due to the effectiveness of the pre-\ntraining stage on the TieredImageNet as it is larger than\nMiniImageNet and a fully converged model can be itself\nvery effective. Based on this, all embedding adaptation ap-\nproaches further improves over ProtoNet almost in all cases,\nwith FEAT achieving the best performances among all ap-\nproaches. Note that here our pre-training strategy is most\nsimilar to the one used in PFA [35], while we further ﬁne-\ntune the backbone. Temperature scaling of the logits inﬂu-\nences the performance a lot when ﬁne-tuning over the pre-\ntrained weights. Additionally, we list some recent methods\n(SimpleShot [51], and CTM [28]) using different backbone\nTable 2: Few-shot classiﬁcation accuracy and 95% conﬁdence in-\nterval on TieredImageNet with the ResNet backbone.\nSetups →\n1-Shot 5-Way\n5-Shot 5-Way\nProtoNet [43]\n53.31 ± 0.89\n72.69 ± 0.74\nRelationNet [45]\n54.48 ± 0.93\n71.32 ± 0.78\nMetaOptNet [25]\n65.99 ± 0.72\n81.56 ± 0.63\nCTM [28]\n68.41 ± 0.39\n84.28 ± 1.73\nSimpleShot [51]\n69.09 ± 0.22\n84.58 ± 0.16\nInstance embedding\nProtoNet\n68.23 ± 0.23\n84.03 ± 0.16\nEmbedding adaptation\nBILSTM\n68.14 ± 0.23\n84.23 ± 0.16\nDEEPSETS\n68.59 ± 0.24\n84.36 ± 0.16\nGCN\n68.20 ± 0.23\n84.64 ± 0.16\nFEAT\n70.80 ± 0.23\n84.79 ± 0.16\nTable 3: Number of parameters introduced by each set-to-set\nfunction in additional to the backbone’s parameters.\nBILSTM\nDEEPSETS\nGCN\nFEAT\nConvNet\n25K\n82K\n33K\n16K\nResNet\n2.5M\n8.2M\n3.3M\n1.6M\narchitectures such as ResNet-18 for reference.\nComparison among the embedding adaptation models.\nAmong the four embedding adaptation methods, BILSTM in\nmost cases achieves the worst performances and sometimes\neven performs worse than ProtoNet. This is partially due\nto the fact that BILSTM can not easily implement the re-\nquired permutation invariant property (also shown in [56]),\nwhich confuses the learning process of embedding adapta-\ntion. Secondly, we ﬁnd that DEEPSETS and GCN have the\nability to adapt discriminative task-speciﬁc embeddings but\ndo not achieve consistent performance improvement over\nthe baseline ProtoNet especially on MiniImageNet with the\nConvNet backbone. A potential explanation is that, such\nmodels when jointly learned with the backbone model, can\nmake the optimization process more difﬁcult, which leads\nto the varying ﬁnal performances. In contrast, we observe\nthat FEAT can consistently improve ProtoNet and other em-\nbedding adaptation approaches in all cases, without addi-\ntional bells and whistles. It shows that the Transformer as a\nset-to-set function can implement rich interactions between\ninstances, which provides its high expressiveness to model\nthe embedding adaptation process.\nInterpolation and extrapolation of classiﬁcation ways.\nNext, we study different set-to-set functions on their capa-\nbility of interpolating and extrapolating across the number\nof classiﬁcation ways. To do so, we train each variant of em-\n6\n\n\n5\n10\n15\n20\nNumber of categories per task\n0\n10\n20\n30\n40\n50\n60\n70\nMean accuracy (in %)\n52.5\n36.8\n29.3\n24.6\n55.0\n38.6\n30.6\n25.8\n53.2\n37.1\n29.5\n24.9\n55.1\n39.1\n31.3\n26.4\nMethods\nRandom\nBILSTM\nDeepSets\nGCN\nFEAT\n5\n10\n15\n20\nNumber of categories per task\n0\n10\n20\n30\n40\n50\n60\n70\n52.1\n35.5\n27.5\n22.9\n54.4\n36.9\n27.3\n20.6\n54.1\n37.9\n30.1\n25.3\n55.1\n39.1\n31.1\n26.2\nMethods\nRandom\nBILSTM\nDeepSets\nGCN\nFEAT\n(a) Way Interpolation\n(b) Way Extrapolation\nFigure 3: Interpolation and Extrapolation of few-shot tasks\nfrom the “way” perspective. First, We train various embedding\nadaptation models on 1-shot 20-way (a) or 5-way (b) classiﬁcation\ntasks and evaluate models on unseen tasks with different number\nof classes (N={5, 10, 15, 20}). It shows that FEAT is superior in\nterms of way interpolation and extrapolation ability.\nbedding adaptation functions with both 1-shot 20-way and\n1-shot 5-way tasks, and measure the performance change as\na function to the number of categories in the test time. We\nreport the mean accuracies evaluated on few-shot classiﬁ-\ncation with N = {5, 10, 15, 20} classes, and show results\nin Figure 3. Surprisingly, we observe that FEAT achieves\nalmost the same numerical performances in both extrapo-\nlation and interpolation scenarios, which further displays\nits strong capability of learning the set-to-set transforma-\ntion. Meanwhile, we observe that DEEPSETS works well\nwith interpolation but fails with extrapolation as its perfor-\nmance drops signiﬁcantly with the larger N. In contrast,\nGCN achieves strong extrapolation performances but does\nnot work as effectively in interpolation. BILSTM performs\nthe worst in both cases, as it is by design not permutation\ninvariant and may have ﬁtted an arbitrary dependency be-\ntween instances.\nParameter efﬁciency. Table 3 shows the number of ad-\nditional parameters each set-to-set function has introduced.\nFrom this, we observe that with both ConvNet and ResNet\nbackbones, FEAT has the smallest number of parameters\ncompared with all other approaches while achieving best\nperformances from various aspects (as results discussed\nabove), which highlights its high parameter efﬁciency.\nAll above, we conclude that: 1) learning embedding\nadaptation with a set-to-set model is very effective in mod-\neling task-speciﬁc embeddings for few-shot learning 2)\nFEAT is the most parameter-efﬁcient function approximater\nthat achieves the best empirical performances, together with\nnice permutation invariant property and strong interpola-\ntion/extrapolation capability over the classiﬁcation way.\n5.2.2. Ablation Studies\nWe analyze\nFEAT and its ablated variants on the\nMiniImageNet dataset with ConvNet backbone.\nHow does the embedding adaptation looks like qualita-\ntively? We sample four few-shot learning tasks and learn\na principal component analysis (PCA) model (that projects\nembeddings into 2-D space) using the instance embeddings\nof the test data. We then apply this learned PCA projection\nto both the support set’s pre-adapted and post-adapted em-\nbeddings. The results are shown in Figure 1 (the beginning\nof the paper). In three out of four examples, post-adaptation\nembeddings of FEAT improve over the pre-adaption embed-\ndings. Interestingly, we found that the embedding adap-\ntation step of FEAT has the tendency of pushing the sup-\nport embeddings apart from the clutter, such that they can\nbetter ﬁt the test data of its categories. In the negative ex-\nample where post-adaptation degenerates the performances,\nwe observe that the embedding adaptation step has pushed\ntwo support embeddings “Golden Retriever” and “Lion” too\nclose to each other. It has qualitatively shown that the adap-\ntation is crucial to obtain superior performances and helps\nto contrast against task-agnostic embeddings.\n5.3. Extended Few-Shot Learning Tasks\nIn this section, we evaluate FEAT on 3 different few-shot\nlearning tasks. Speciﬁcally, cross-domain FSL, transductive\nFSL [30, 38], and generalized FSL [7]. We overview the\nsetups brieﬂy and please refer to SM for details.\nFS Domain Generalization assumes that examples in UN-\nSEEN support and test set can come from the different do-\nmains, e.g., sampled from different distributions [9, 19].\nThe example of this task can be found in Figure 4. It re-\nquires a model to recognize the intrinsic property than tex-\nture of objects, and is de facto analogical recognition.\nTransductive FSL. The key difference between standard\nand transductive FSL is whether test instances arrive one\nat a time or all simultaneously. The latter setup allows the\nstructure of unlabeled test instances to be utilized. There-\nfore, the prediction would depend on both the training (sup-\nport) instances and all the available test instances in the tar-\nget task from UNSEEN categories.\nGeneralized FSL. Prior works assumed the test instances\ncoming from unseen classes only. Different from them, the\ngeneralized FSL setting considers test instances from both\nSEEN and UNSEEN classes [37]. In other words, during the\nmodel evaluation, while support instances all come from U,\nthe test instances come from S ∪U, and the classiﬁer is\nrequired to predict on both SEEN and UNSEEN categories. .\n5.3.1. Few-Shot Domain Generalization\nWe show that FEAT learns to adapt the intrinsic structure\nof tasks, and generalizes across domains, i.e., predicting\ntest instances even when the visual appearance is changed.\nSetups. We train the FSL model in the standard domain and\nevaluate with cross-domain tasks, where the N-categories\nare aligned but domains are different. In detail, a model is\n7\n\n\nC →C\nC →R\nSupervised\n34.38±0.16\n29.49±0.16\nProtoNet\n35.51±0.16\n29.47±0.16\nFEAT\n36.83±0.17\n30.89±0.17\n1-Shot\n5-Shot\nTPN [30]\n55.51\n69.86\nTEAM [34]\n56.57\n72.04\nFEAT\n57.04 ± 0.20\n72.89 ± 0.16\nSEEN\nUNSEEN\nCOMBINED\nRandom\n1.56 ±0.00 20.00±0.00\n1.45±0.00\nProtoNet 41.73±0.03 48.64±0.20\n35.69±0.03\nFEAT\n43.94±0.03 49.72±0.20\n40.50±0.03\n(a) Few-shot domain generalization\n(b) Transductive few-shot learning\n(c) Generalized few-shot learning\nTable 4: We evaluate our model on three additional few-shot learning tasks: (a) Few-shot domain generalization, (b) Transductive few-shot\nlearning, and (c) Generalized few-shot learning. We observe that FEAT consistently outperform all previous methods or baselines.\nDrill\nBed\nTV\nFlower\nScrewdriver\n𝒟𝒟𝐭𝐭𝐭𝐭𝐭𝐭𝐭𝐭𝐭𝐭from \n“Clipart”\n𝒟𝒟𝐭𝐭𝐭𝐭𝐭𝐭𝐭𝐭from \n“Real World”\nClassify\nTest Set\nTrain Set\nTrain Set\nTest Set\n𝒟𝒟𝐭𝐭𝐭𝐭𝐭𝐭𝐭𝐭𝐭𝐭from \n“Clipart”\n𝒟𝒟𝐭𝐭𝐭𝐭𝐭𝐭𝐭𝐭from \n“Real World”\nBed\nCurtains\nRefrigerator\nSneakers\nDrill\nClassify\nFigure 4: Qualitative results of few-shot domain-generalization\nfor FEAT. Correctly classiﬁed examples are shown in red boxes\nand incorrectly ones are shown in blue boxes. We visualize one\ntask that FEAT succeeds (top) and one that fails (bottom).\ntrained on tasks from the “Clipart” domain of OfﬁceHome\ndataset [48], then the model is required to generalize to both\n“Clipart (C)” and“Real World (R)” test instances. In other\nwords, we need to classify complex real images by seeing\nonly a few sketches (Figure 4 gives an overview of data).\nResults. Table 4 (a) gives the quantitative results and Fig-\nure 4 qualitatively examines it. Here, the “supervised” de-\nnotes a model trained with the standard classiﬁcation strat-\negy and then its penultimate layer’s output feature is used\nas the nearest neighbor classiﬁer. We observe that ProtoNet\ncan outperform this baseline on tasks when evaluating in-\nstances from “Clipart” but not ones from “real world”.\nHowever, FEAT improves over “real world” few-shot classi-\nﬁcation even only seeing the support data from “Clipart”.\n5.3.2. Transductive Few-Shot Learning\nWe show that without additional efforts in modeling,\nFEAT outperforms existing methods in transductive FSL.\nSetups. We further study this semi-supervised learning set-\nting to see how well FEAT can incorporate test instances into\njoint embedding adaptation. Speciﬁcally, we use the unla-\nbeled test instances to augment the key and value sets of\nTransformer (refer to SM for details), so that the embedding\nadaptation takes relationship of all test instances into con-\nsideration. We evaluate this setting on the transductive pro-\ntocol of MiniImageNet [38]. With the adapted embedding,\nFEAT makes predictions based on Semi-ProtoNet [38].\nResults.\nWe compare with two previous approaches,\nTPN [30] and TEAM [34]. The results are shown in Ta-\nble 4 (b). We observe that FEAT improves its standard FSL\nperformance (refer to Table 1) and also outperforms previ-\nous semi-supervised approaches by a margin.\n5.3.3. Generalized Few-Shot Learning\nWe show that FEAT performs well on generalized few-\nshot classiﬁcation of both SEEN and UNSEEN classes.\nSetups. In this scenario, we evaluate not only on classi-\nfying test instances from a N-way M-shot task from UN-\nSEEN set U, but also on all available SEEN classes from S.\nTo do so, we hold out 150 instances from each of the 64\nseen classes in MiniImageNet for validation and evaluation.\nNext, given a 1-shot 5-way training set Dtrain, we consider\nthree evaluation protocols based on different class sets [7]:\nUNSEEN measures the mean accuracy on test instances only\nfrom U (5-Way few-shot classiﬁcation); SEEN measures the\nmean accuracy on test instances only from S (64-Way clas-\nsiﬁcation); COMBINED measures the mean accuracy on test\ninstances from S ∪U (69-Way mixed classiﬁcation).\nResults.\nThe results can be found in Table 4 (c).\nWe\nobserve that again FEAT outperforms baseline ProtoNet.\nTo calibrate the prediction score on SEEN and UNSEEN\nclasses [7, 52], we select a constant seen/unseen class prob-\nability over the validation set, and subtract this calibration\nfactor from seen classes’ prediction score. Then we take the\nprediction with maximum score value after calibration.\n6. Discussion\nA common embedding space fails to tailor discriminative\nvisual knowledge for a target task especially when there are\na few labeled training data. We propose to do embedding\nadaptation with a set-to-set function and instantiate it with\ntransformer (FEAT), which customizes task-speciﬁc embed-\nding spaces via a self-attention architecture. The adapted\nembedding space leverages the relationship between target\n8\n\n\ntask training instances, which leads to discriminative in-\nstance representations.\nFEAT achieves the state-of-the-art\nperformance on benchmarks, and its superiority can gener-\nalize to tasks like cross-domain, transductive, and general-\nized few-shot classiﬁcations.\nAcknowledgments.This work is partially supported by The\nNational Key R&D Program of China (2018YFB1004300),\nDARPA# FA8750-18-2-0117, NSF IIS-1065243, 1451412,\n1513966/ 1632803/1833137, 1208500, CCF-1139148, a\nGoogle Research Award, an Alfred P. Sloan Research Fel-\nlowship, ARO# W911NF-12-1-0241 and W911NF-15-1-\n0484, China Scholarship Council (CSC), NSFC (61773198,\n61773198, 61632004), and NSFC-NRF joint research\nproject 61861146001.\nReferences\n[1] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label-\nembedding for attribute-based classiﬁcation. In CVPR, pages\n819–826, 2013. 2\n[2] M. Andrychowicz, M. Denil, S. G. Colmenarejo, M. W.\nHoffman, D. Pfau, T. Schaul, and N. de Freitas. Learning\nto learn by gradient descent by gradient descent. In NIPS,\npages 3981–3989. 2016. 2\n[3] A. Antoniou, H. Edwards, and A. J. Storkey. How to train\nyour MAML. In ICLR, 2019. 2\n[4] L. J. Ba, R. Kiros, and G. E. Hinton. Layer normalization.\nCoRR, abs/1607.06450, 2016. 13\n[5] S. Changpinyo, W.-L. Chao, B. Gong, and F. Sha.\nSyn-\nthesized classiﬁers for zero-shot learning. In CVPR, pages\n5327–5336, 2016. 2\n[6] S. Changpinyo, W.-L. Chao, and F. Sha. Predicting visual\nexemplars of unseen classes for zero-shot learning. In ICCV,\npages 3496–3505, 2017. 2\n[7] W.-L. Chao, S. Changpinyo, B. Gong, and F. Sha. An empir-\nical study and analysis of generalized zero-shot learning for\nobject recognition in the wild. In ECCV, pages 52–68, 2016.\n7, 8\n[8] W.-Y. Chen, Y.-C. Liu, Z. Kira, Y.-C. F. Wang, and J.-B.\nHuang. A closer look at few-shot classiﬁcation. In ICLR,\n2019. 5\n[9] N. Dong and E. P. Xing. Domain adaption in one-shot learn-\ning. In ECML PKDD, pages 573–588, 2018. 7\n[10] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-\nlearning for fast adaptation of deep networks.\nIn ICML,\npages 1126–1135, 2017. 1, 2, 3, 5, 6, 14, 15\n[11] G. Ghiasi, T.-Y. Lin, and Q. V. Le. Dropblock: A regulariza-\ntion method for convolutional networks. In NeurIPS, pages\n10750–10760. 2018. 13\n[12] S. Gidaris and N. Komodakis.\nDynamic few-shot visual\nlearning without forgetting.\nIn CVPR, pages 4367–4375,\n2018. 18\n[13] L.-Y. Gui, Y.-X. Wang, D. Ramanan, and J. M. F. Moura.\nFew-shot human motion prediction via meta-learning.\nIn\nECCV, pages 441–459, 2018. 2\n[14] B. Hariharan and R. B. Girshick. Low-shot visual recogni-\ntion by shrinking and hallucinating features. In ICCV, pages\n3037–3046, 2017. 18\n[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In CVPR, pages 770–778, 2016. 5,\n18\n[16] S. Hochreiter and J. Schmidhuber. Long short-term memory.\nNeural Computation, 9(8):1735–1780, 1997. 2, 4, 5, 11\n[17] K. Hsu, S. Levine, and C. Finn. Unsupervised learning via\nmeta-learning. In ICLR, 2019. 2\n[18] S. Ioffe and C. Szegedy. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift. In\nICML, pages 448–456, 2015. 13\n[19] B. Kang and J. Feng. Transferable meta learning across do-\nmains. In UAI, pages 177–187, 2018. 7\n[20] D. P. Kingma and J. Ba. Adam: A method for stochastic\noptimization. In ICLR, 2015. 5, 14\n[21] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation\nwith graph convolutional networks. In ICLR, 2017. 2, 4, 5,\n12\n[22] G. Koch, R. Zemel, and R. Salakhutdinov.\nSiamese neu-\nral networks for one-shot image recognition. In ICML Deep\nLearning Workshop, volume 2, 2015. 2\n[23] B. M. Lake, R. Salakhutdinov, J. Gross, and J. B. Tenen-\nbaum.\nOne shot learning of simple visual concepts.\nIn\nCogSci, 2011. 1\n[24] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-\nlevel concept learning through probabilistic program induc-\ntion. Science, 350(6266):1332–1338, 2015. 1\n[25] K. Lee, S. Maji, A. Ravichandran, and S. Soatto.\nMeta-\nlearning with differentiable convex optimization. In CVPR,\npages 10657–10665, 2019. 2, 5, 6, 13, 15\n[26] Y. Lee and S. Choi.\nGradient-based meta-learning with\nlearned layerwise metric and subspace.\nIn ICML, pages\n2933–2942, 2018. 2\n[27] F.-F. Li, R. Fergus, and P. Perona. One-shot learning of ob-\nject categories. TPAMI, 28(4):594–611, 2006. 1\n[28] H. Li, D. Eigen, S. Dodge, M. Zeiler, and X. Wang. Find-\ning task-relevant features for few-shot learning by category\ntraversal. In CVPR, pages 1–10, 2019. 2, 6\n[29] Z. Lin, M. Feng, C. N. dos Santos, M. Yu, B. Xiang, B. Zhou,\nand Y. Bengio. A structured self-attentive sentence embed-\nding. In ICLR, 2017. 2, 4\n[30] Y. Liu, J. Lee, M. Park, S. Kim, E. Yang, S. J. Hwang, and\nY. Yang. Learning to propagate labels: Transductive propa-\ngation network for few-shot learning. In ICLR, 2019. 7, 8,\n17, 18\n[31] L. Metz, N. Maheswaranathan, B. Cheung, and J. Sohl-\nDickstein.\nLearning unsupervised learning rules.\nCoRR,\nabs/1804.00222, 2018. 2\n[32] A. Nichol, J. Achiam, and J. Schulman. On ﬁrst-order meta-\nlearning algorithms. CoRR, abs/1803.02999, 2018. 2\n[33] B. N. Oreshkin, P. R. L´opez, and A. Lacoste. TADAM: task\ndependent adaptive metric for improved few-shot learning.\nIn NeurIPS, pages 719–729. 2018. 5, 6, 11, 15\n[34] L. Qiao, Y. Shi, J. Li, Y. Wang, T. Huang, and Y. Tian. Trans-\nductive episodic-wise adaptive metric for few-shot learning.\nIn ICCV, pages 3603–3612, 2019. 8, 17, 18\n[35] S. Qiao, C. Liu, W. Shen, and A. L. Yuille. Few-shot image\nrecognition by predicting parameters from activations.\nIn\nCVPR, pages 7229–7238, 2018. 2, 5, 6, 13, 14, 15\n[36] S. Ravi and H. Larochelle. Optimization as a model for few-\nshot learning. In ICLR, 2017. 2, 5\n9\n\n\n[37] M. Ren, R. Liao, E. Fetaya, and R. S. Zemel. Incremen-\ntal few-shot learning with attention attractor networks. In\nNeurIPS, pages 5276–5286, 2019. 7\n[38] M. Ren, E. Triantaﬁllou, S. Ravi, J. Snell, K. Swersky, J. B.\nTenenbaum, H. Larochelle, and R. S. Zemel. Meta-learning\nfor semi-supervised few-shot classiﬁcation. In ICLR, 2018.\n5, 6, 7, 8, 14, 17, 18\n[39] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein,\nA. C. Berg, and F.-F. Li. Imagenet large scale visual recog-\nnition challenge. IJCV, 115(3):211–252, 2015. 5, 18\n[40] A. A. Rusu, D. Rao, J. Sygnowski, O. Vinyals, R. Pascanu,\nS. Osindero, and R. Hadsell. Meta-learning with latent em-\nbedding optimization. In ICLR, 2019. 1, 2, 5, 13, 14, 15\n[41] V. G. Satorras and J. B. Estrach.\nFew-shot learning with\ngraph neural networks. In ICLR, 2018. 2, 4, 12\n[42] T. R. Scott, K. Ridgeway, and M. C. Mozer. Adapted deep\nembeddings: A synthesis of methods for k-shot inductive\ntransfer learning. In NeurIPS, pages 76–85. 2018. 2\n[43] J. Snell, K. Swersky, and R. S. Zemel. Prototypical networks\nfor few-shot learning. In NIPS, pages 4080–4090. 2017. 1,\n2, 3, 4, 5, 6, 11, 13, 15, 18\n[44] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov. Dropout: a simple way to prevent neural\nnetworks from overﬁtting. JMLR, 15(1):1929–1958, 2014.\n13\n[45] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. S. Torr, and\nT. M. Hospedales. Learning to compare: Relation network\nfor few-shot learning. In CVPR, pages 1199–1208, 2018. 2,\n6, 15\n[46] E. Triantaﬁllou, R. S. Zemel, and R. Urtasun.\nFew-shot\nlearning through an information retrieval lens.\nIn NIPS,\npages 2252–2262. 2017. 1, 2, 5, 13, 14\n[47] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all\nyou need. In NIPS, pages 6000–6010. 2017. 2, 4, 5, 12, 13,\n14, 16\n[48] H. Venkateswara, J. Eusebio, S. Chakraborty, and S. Pan-\nchanathan. Deep hashing network for unsupervised domain\nadaptation. In CVPR, pages 5385–5394, 2017. 5, 8, 14, 17\n[49] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and\nD. Wierstra. Matching networks for one shot learning. In\nNIPS, pages 3630–3638. 2016. 1, 2, 3, 4, 5, 6, 11, 12, 13,\n14, 15\n[50] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.\nThe Caltech-UCSD Birds-200-2011 Dataset. Technical Re-\nport CNS-TR-2011-001, California Institute of Technology,\n2011. 6, 14\n[51] Y. Wang, W.-L. Chao, K. Q. Weinberger, and L. van der\nMaaten.\nSimpleshot: Revisiting nearest-neighbor classiﬁ-\ncation for few-shot learning. CoRR, abs/1911.04623, 2019.\n6, 14, 15\n[52] Y.-X. Wang, R. B. Girshick, M. Hebert, and B. Hariharan.\nLow-shot learning from imaginary data.\nIn CVPR, pages\n7278–7286, 2018. 8, 18\n[53] X.-S. Wei, P. Wang, L. Liu, C. Shen, and J. Wu.\nPiece-\nwise classiﬁer mappings: Learning ﬁne-grained learners for\nnovel categories with few examples. TIP, 28(12):6116–6125,\n2019. 2\n[54] H.-J. Ye, H. Hu, D.-C. Zhan, and F. Sha.\nLearn-\ning embedding adaptation for few-shot learning.\nCoRR,\nabs/1812.03664, 2018. 13\n[55] S. Zagoruyko and N. Komodakis. Wide residual networks.\nIn BMVC, 2016. 5, 13\n[56] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. P´oczos, R. R.\nSalakhutdinov, and A. J. Smola. Deep sets. In NIPS, pages\n3394–3404. 2017. 2, 4, 5, 6, 12\n10\n\n\nSupplementary Material\nA. Details of Baseline Methods\nIn this section, we describe two important embedding\nlearning baselines i.e., Matching Network (MatchNet) [49]\nand Prototypical Network (ProtoNet) [43], to implement the\nprediction function f(xtest; Dtrain) in the few-shot learn-\ning framework.\nMatchNet and ProtoNet.\nBoth MatchNet and ProtoNet\nstress the learning of the embedding function E from the\nsource task data DS with a meta-learning routine similar to\nAlg. 1 in the main text. We omit the super-script S since\nthe prediction strategies can apply to tasks from both SEEN\nand UNSEEN sets.\nGiven the training data Dtrain = {xi, yi}NM\ni=1 of an M-\nshot N-way classiﬁcation task, we can obtain the embed-\nding of each training instance based on the function E:1\nφ(xi) = E(xi), ∀xi ∈Xtrain\n(8)\nTo classify a test instance xtest, we perform the nearest\nneighbor classiﬁcation , i.e.,\nˆytest ∝exp\n\u0000γ · sim(φxtest, φxi)\n\u0001\n· yi\n(9)\n=\nexp\n\u0000γ · sim(φxtest , φxi)\n\u0001\nP\nxi′∈Xtrain exp\n\u0000γ · sim(φxtest , φxi′)\n\u0001 · yi\n=\nX\n(xi,yi)∈Dtrain\nexp\n\u0000γ · sim(φxtest , φxi)\n\u0001\nP\nxi′∈Xtrain exp\n\u0000γ · sim(φxtest , φxi′)\n\u0001 · yi\nHere, MatchNet ﬁnds the most similar training instance to\nthe test one, and assigns the label of the nearest neigh-\nbor to the test instance. Note that sim represents the co-\nsine similarity, and γ > 0 is the scalar temperature value\nover the similarity score, which is found important em-\npirically [33]. During the experiments, we tune this tem-\nperature value carefully, ranging from the reciprocal of\n{0.1, 1, 16, 32, 64, 128}.2\nThe ProtoNet has two key differences compared with the\nMatchNet. First, when M > 1 in the target task, ProtoNet\ncomputes the mean of the same class embeddings as the\nclass center (prototype) in advance and classiﬁes a test in-\nstance by computing its similarity to the nearest class center\n(prototype). In addition, it uses the negative distance rather\n1In the following, we use φ(xi) and φxi exchangeably to represent the\nembedding of an instance xi based on the mapping φ.\n2In experiments, we ﬁnd the temperature scale over logits inﬂuences\nthe model training a lot when we optimize based on pre-trained weights.\nthan the cosine similarity as the similarity metric:\ncn = 1\nM\nX\nyi=n\nφ(xi), ∀n = 1, . . . , N\n(10)\nˆytest ∝exp\n\u0000γ · ∥φxtest −cn∥2\n2\n\u0001\n· yn\n=\nN\nX\nn=1\nexp\n\u0000−γ∥φxtest −cn∥2\n2\n\u0001\nPN\nn′=1 exp\n\u0000−γ∥φxtest −cn′∥2\n2\n\u0001yn\n(11)\nSimilar to the aforementioned scalar temperature for Match-\nNet, in Eq. 11 we also consider the scale γ. Here we abuse\nthe notation by using yi = n to enumerate the instances\nwith label n, and denote yn as the one-hot coding of the\nn-th class. Thus Eq. 11 outputs the probability to classify\nxtest to the N classes.\nIn the experiments, we ﬁnd ProtoNet incorporates bet-\nter with FEAT. When there is more than one shot in each\nclass, we average all instances per class in advance by\nEq. 10 before inputting them to the set-to-set transforma-\ntion. This pre-average manner makes more precise embed-\nding for each class and facilitates the “downstream” em-\nbedding adaptation. We will validate this in the additional\nexperiments.\nB. Details of the Set-to-Set Functions\nIn this section, we provide details about four imple-\nmentations of the set-to-set embedding adaptation function\nT, i.e., the BILSTM, DEEPSETS, GCN, and the TRANS-\nFORMER. The last one is the key component in our Few-\nshot Embedding Adaptation with Transformer (FEAT) ap-\nproach. Then we will introduce the conﬁguration of the\nmulti-layer/multi-head transformer, and the setup of the\ntransformer for the transductive Few-Shot Learning (FSL).\nB.1. BiLSTM as the Set-to-Set Transformation\nBidirectional LSTM (BILSTM) [16, 49] is one of the\ncommon choice to instantiate the set-to-set transformation,\nwhere the addition between the input and the hidden layer\noutputs of each BILSTM cell leads to the adapted embed-\nding. In detail, we have\n{\n⃗\nφ(x), ⃗φ(x)} = BILSTM({φ(x)}); ∀x ∈Xtrain\n(12)\nWhere\n⃗\nφ(x) and ⃗φ(x) are the hidden layer outputs of the\ntwo LSTM models for each instance embedding in the input\nset. Then we get the adapted embedding as\nψ(x) = φ(x) +\n⃗\nφ(x) + ⃗φ(x)\n(13)\nIt is notable that the output of the BILSTM suppose to de-\npend on the order of the input set. Vinyals et al. [49] pro-\npose to use the Fully Conditional Embedding to encode\nthe context of both the test instance and the support set\n11\n\n\nClassification \nScores\nEmbedding \nAdaptation\nCNN\nCNN\nCNN\nCNN\nSoft Nearest \nNeighbor\nSet-to-Set Function\n(a) Embedding Adaptation\n(b) Transformer as the Set-to-Set Function\n(c) DeepSets as Set-to-Set Function\nLayer Norm\nFC\nFC\nFC\nFC\nFC\nFC\nFC\nFC\nFC\nFC\nFC\nSUM\nCAT\nScaled Dot\nProduct\nTrain Instance\nTest Instance\nTask Agnostic\nEmbedding\nTask Specific \nEmbedding\nFigure 5: Illustration of two embedding adpatation methods considered in the paper. (a) shows the main ﬂow of Few-Shot Embedding\nAdaptation, while (b) and (c) demonstrate the workﬂow of Transformer and DeepSets respectively.\ninstances based on BILSTM and LSTM w/ Attention mod-\nule. Different from [49], we apply the set-to-set embedding\nadaptation only over the support set, which leads to a fully\ninductive learning setting.\nB.2. DeepSets as the Set-to-Set Transformation\nDeep sets [56] suggests a generic aggregation function\nover a set should be the transformed sum of all elements in\nthis set. Therefore, a very simple set-to-set transformation\nbaseline involves two components, an instance centric rep-\nresentation combined with a set context representation. For\nany instance x ∈Xtrain, we deﬁne its complementary set\nas x∁. Then we implement the set transformation by:\nψ(x) = φ(x) + g([φ(x);\nX\nxi′∈x∁\nh(φ(xi′))])\n(14)\nIn Eq. 14, g and h are transformations which map the em-\nbedding into another space and increase the representation\nability of the embedding. Two-layer multi-layer perception\n(MLP) with ReLU activation is used to implement these two\nmappings. For each instance, embeddings in its comple-\nmentary set are ﬁrst combined into a vector as the context,\nand then this vector is concatenated with the input embed-\nding to obtain the residual component of the adapted em-\nbedding. This conditioned embedding takes other instances\nin the set into consideration, and keeps the “set (permutation\ninvariant)” property. Finally, we determine the label with\nthe newly adapted embedding ψ as Eq. 11. An illustration\nof the DeepSets notation in the embedding adaptation can\nbe found in Figure 5 (c). The summation operator in Eq. 14\ncould also be replaced as the maximum operator, and we\nﬁnd the maximum operator works better than summation\noperator in our experiments.\nB.3. GCN as the Set-to-Set Transformation\nGraph Convolutional Networks (GCN) [21, 41] propa-\ngate the relationship between instances in the set. We ﬁrst\nconstruct a degree matrix A ∈RNK×NK to represent the\nsimilarity between instances in a set. If two instances xi and\nxj come from the same class, then we set the corresponding\nelement Aij in A to 1, otherwise we have Aij = 0. Based\non A, we build the “normalized” adjacency matrix S for a\ngiven set with added self-loops S = D−1\n2 (A + I)D−1\n2 .\nI ∈RNK×NK is the identity matrix, and D is the diagonal\nmatrix whose elements are equal to the sum of elements in\nthe corresponding row of A+I, i.e., Dii = P\nj Aij +1 and\nDij = 0 if i ̸= j. Let Φ0 = {φx ; ∀x ∈Xtrain} be the\nconcatenation of all the instance embeddings in the training\nset Xtrain. We use the super-script to denote the generation\nof the instance embedding matrix. The relationship between\ninstances could be propagated based on S, i.e.,\nΦt+1 = ReLU(SΦtW) , t = 0, 1, . . . , T −1\n(15)\nW is a learned a projection matrix for feature transforma-\ntion. In GCN, the embedding in the set is transformed based\non Eq. 15 multiple times (we propagate the embedding set\ntwo times during the experiments), and the ﬁnal propagated\nembedding set ΦT gives rise to the ψx.\nB.4. Transformer as the Set-to-Set Transformation\nIn this section, we describe in details about our Few-Shot\nEmbedding Adaptation w/ Transformer (FEAT) approach,\nspeciﬁcally how to use the transformer architecture [47] to\nimplement the set-to-set function T, where self-attention\nmechanism facilitates the instance embedding adaptation\nwith consideration of the contextual embeddings.\nAs mentioned before, the transformer is a store of triplets\nin the form of (query, key, and value). Elements in the query\nset are the ones we want to do the transformation. The trans-\nformer ﬁrst matches a query point with each of the keys by\ncomputing the “query” – “key” similarities. Then the prox-\nimity of the key to the query point is used to weight the\ncorresponding values of each key. The transformed input\nacts as a residual value which will be added to the input.\n12\n\n\nBasic Transformer.\nFollowing the deﬁnitions in [47], we\nuse Q, K, and V to denote the set of the query, keys, and\nvalues, respectively. All these sets are implemented by dif-\nferent combinations of task instances.\nTo increase the ﬂexibility of the transformer, three sets\nof linear projections (WQ ∈Rd×d′, WK ∈Rd×d′, and\nWV ∈Rd×d′) are deﬁned, one for each set.3 The points in\nsets are ﬁrst projected by the corresponding projections\nQ = W ⊤\nQ\n\u0002\nφxq;\n∀xq ∈Q\n\u0003\n∈Rd′×|Q|\nK = W ⊤\nK\n\u0002\nφxk;\n∀xk ∈K\n\u0003\n∈Rd′×|K|\nV = W ⊤\nV\n\u0002\nφxv;\n∀xv ∈V\n\u0003\n∈Rd′×|V|\n(16)\n|Q|, |K|, and |V| are the number of elements in the sets\nQ, K, and V respectively. Since there is a one-to-one corre-\nspondence between elements in K and V we have |K| = |V|.\nThe similarity between a query point xq ∈Q and the list\nof keys K is then computed as “attention”:\nαqk ∝exp\n \nφ⊤\nxqWQ · K\n√\nd\n!\n; ∀xk ∈K\n(17)\nαq,: = softmax\n \nφ⊤\nxqWQ · K\n√\nd\n!\n∈R|K|\n(18)\nThe k-th element αqk in the vector αq,: reveals the particu-\nlar proximity between xk and xq. The computed attention\nvalues are then used as weights for the ﬁnal embedding xq:\n˜ψxq =\nX\nk\nαqkV:,k\n(19)\nψxq = τ\n\u0000φxq + W ⊤\nFC ˜ψxq\n\u0001\n(20)\nV:,k is the k-th column of V .\nWFC ∈Rd′×d is the\nprojection weights of a fully connected layer.\nτ com-\npletes a further transformation, which is implemented by\nthe dropout [44] and layer normalization [4]. The whole\nﬂow of transformer in our FEAT approach can be found in\nFigure 5 (b). With the help of transformer, the embeddings\nof all training set instances are adapted (we denote this ap-\nproach as FEAT).\nMulti-Head Multi-Layer Transformer.\nFollowing [47],\nan extended version of the transformer can be built with\nmultiple parallel attention heads and stacked layers. As-\nsume there are totally H heads, the transformer concate-\nnates multiple attention-transformed embeddings, and then\nuses a linear mapping to project the embedding to the orig-\ninal embedding space (with the original dimensionality).\nBesides, we can take the transformer as a feature encoder\nof the input query instance. Therefore, it can be applied\n3For notation simplicity, we omit the bias in the linear projection here.\nover the input query multiple times (with different sets of\nparameters), which gives rise to the multi-layer transformer.\nWe discuss the empirical performances with respect to the\nchange number of heads and layers in § D.\nB.5. Extension to transductive FSL\nFacilitated by the ﬂexible set-to-set transformer in\nEq. 20, our adaptation approach can naturally be extended\nto the transductive FSL setting.\nWhen classifying test instance xtest in the transdutive\nscenario, other test instances Xtest from the N categories\nwould also be available. Therefore, we enrich the trans-\nformer’s query and key/value sets\nQ = K = V = Xtrain ∪Xtest\n(21)\nIn this manner, the embedding adaptation procedure would\nalso consider the structure among unlabeled test instances.\nWhen the number of shots K > 1, we average the embed-\nding of labeled instances in each class ﬁrst before combin-\ning them with the test set embeddings.\nC. Implementation Details\nBackbone architecture.\nWe consider three backbones, as\nsuggested in the literature, as the instance embedding func-\ntion E for the purpose of fair comparisons. We resize the\ninput image to 84 × 84 × 3 before using the backbones.\n• ConvNet. The 4-layer convolution network [43, 46, 49]\ncontains 4 repeated blocks. In each block, there is a con-\nvolutional layer with 3 × 3 kernel, a Batch Normalization\nlayer [18], a ReLU, and a Max pooling with size 2. We\nset the number of convolutional channels in each block as\n64. A bit different from the literature, we add a global\nmax pooling layer at last to reduce the dimension of the\nembedding. Based on the empirical observations, this will\nnot inﬂuence the results, but reduces the computation bur-\nden of later transformations a lot.\n• ResNet. We use the 12-layer residual network in [25].4\nThe DropBlock [11] is used in this ResNet architecture\nto avoid over-ﬁtting. A bit different from the ResNet-12\nin [25], we apply a global average pooling after the ﬁnal\nlayer, which leads to 640 dimensional embeddings.5\n• WRN. We also consider the Wide residual network [40,\n55].\nWe use the WRN-28-10 structure as in [35, 40],\nwhich sets the depth to 28 and width to 10. After a global\n4The source code of the ResNet is publicly available on https://\ngithub.com/kjunelee/MetaOptNet\n5We use the ResNet backbone with input image size 80 × 80 ×\n3 from [35] in the old version of our paper [54], whose source\ncode of ResNet is publicly available on https://github.com/\njoe-siyuan-qiao/FewShot-CVPR.\nEmpirically we ﬁnd the\nResNet-12 [25] works better than our old ResNet architecture.\n13\n\n\naverage pooling in the last layer of the backbone, we get\na 640 dimensional embedding for further prediction.\nDatasets.\nFour\ndatasets,\nMiniImageNet\n[49],\nTieredImageNet\n[38],\nCaltech-UCSD\nBirds\n(CUB)\n200-2011 [50], and OfﬁceHome [48] are investigated in\nthis paper. Each dataset is split into three parts based on\ndifferent non-overlapping sets of classes, for model training\n(a.k.a.\nmeta-training in the literature), model validation\n(a.k.a.\nmeta-val in the literature), and model evaluation\n(a.k.a.\nmeta-test in the literature).\nThe CUB dataset is\ninitially designed for ﬁne-grained classiﬁcation. It contains\nin total 11,788 images of birds over 200 species. On CUB,\nwe randomly sampled 100 species as SEEN classes, another\ntwo 50 species are used as two UNSEEN sets for model\nvalidation and evaluation [46]. For all images in the CUB\ndataset, we use the provided bounding box to crop the\nimages as a pre-processing [46].\nBefore input into the\nbackbone network, all images in the dataset are resized\nbased on the requirement of the network.\nPre-training strategy.\nAs mentioned before, we apply an\nadditional pre-training strategy as suggested in [35, 40].\nThe backbone network, appended with a softmax layer, is\ntrained to classify all classes in the SEEN class split (e.g., 64\nclasses in the MiniImageNet) with the cross-entropy loss.\nIn this stage, we apply image augmentations like random\ncrop, color jittering, and random ﬂip to increase the gen-\neralization ability of the model. After each epoch, we val-\nidate the performance of the pre-trained weights based on\nits few-shot classiﬁcation performance on the model vali-\ndation split. Speciﬁcally, we randomly sample 200 1-shot\nN-way few-shot learning tasks (N equals the number of\nclasses in the validation split, e.g., 16 in the MiniImageNet),\nwhich contains 1 instance per class in the support set and\n15 instances per class for evaluation. Based on the penulti-\nmate layer instance embeddings of the pre-trained weights,\nwe utilize the nearest neighbor classiﬁers over the few-shot\ntasks and evaluate the quality of the backbone. We select\nthe pre-trained weights with the best few-shot classiﬁca-\ntion accuracy on the validation set. The pre-trained weights\nare used to initialize the embedding backbone E, and the\nweights of the whole model are then optimized together dur-\ning the model training.\nTransformer Hyper-parameters.\nWe follow the archi-\ntecture as presented in [47] to build our FEAT model.\nThe hidden dimension d′ for the linear transformation in\nour FEAT model is set to 64 for ConvNet and 640 for\nResNet/WRN. The dropout rate in transformer is set as 0.5.\nWe empirically observed that the shallow transformer (with\none set of projection and one stacked layer) gives the best\noverall performance (also studied in § D.2).\nOptimization.\nFollowing the literature, different optimiz-\ners are used for the backbones during the model training.\nFor the ConvNet backbone, stochastic gradient descent with\nAdam [20] optimizer is employed, with the initial learning\nrate set to be 0.002. For the ResNet and WRN backbones,\nvanilla stochastic gradient descent with Nesterov accelera-\ntion is used with an initial rate of 0.001. We ﬁx the weight\ndecay in SGD as 5e-4 and momentum as 0.9. The sched-\nule of the optimizers is tuned over the validation part of\nthe dataset. As the backbone network is initialized with the\npre-trained weights, we scale the learning rate for those pa-\nrameters by 0.1.\nD. Additional Experimental Results\nIn this section, we will show more experimental results\nover the MiniImageNet/CUB dataset, the ablation studies,\nand the extended few-shot learning.\nD.1. Main Results\nThe full results of all methods on the MiniImageNet can\nbe found in Table 5. The results of MAML [10] optimized\nover the pre-trained embedding network are also included.\nWe re-implement the ConvNet backbone of MAML and cite\nthe MAML results over the ResNet backbone from [40]. It\nis also noteworthy that the FEAT gets the best performance\namong all popular methods and baselines.\nWe also investigate the Wide ResNet (WRN) back-\nbone over MiniImageNet, which is also the popular one\nused in [35, 40].\nSimpleShot [51] is a recent proposed\nembedding-based few-shot learning approach that takes full\nadvantage of the pre-trained embeddings. We cite the re-\nsults of PFA [35], LEO [40], and SimpleShot [51] from\ntheir papers. The results can be found in Table 6. We re-\nimplement ProtoNet and our FEAT approach with WRN.\nIt is notable that in this case, our FEAT achieves much\nhigher promising results than the current state-of-the-art\napproaches. Table 7 shows the classiﬁcation results with\nWRN on the TieredImageNet data set, where our FEAT still\nkeeps its superiority when dealing with 1-shot tasks.\nTable 8 shows the 5-way 1-shot and 5-shot classiﬁcation\nresults on the CUB dataset based on the ConvNet back-\nbone. The results on CUB are consistent with the trend\non the MiniImageNet dataset. Embedding adaptation in-\ndeed assists the embedding encoder for the few-shot clas-\nsiﬁcation tasks.\nFacilitated by the set function property,\nthe DEEPSETS works better than the BILSTM counterpart.\nAmong all the results, the transformer based FEAT gets the\ntop tier results.\nD.2. Ablation Studies\nIn this section, we perform further analyses for our pro-\nposed FEAT and its ablated variants classifying in the Pro-\n14\n\n\nTable 5: Few-shot classiﬁcation accuracy± 95% conﬁdence interval on MiniImageNet with ConvNet and ResNet backbones. Our imple-\nmentation methods are measured over 10,000 test trials.\nSetups →\n1-Shot 5-Way\n5-Shot 5-Way\nBackbone Network →\nConvNet\nResNet\nConvNet\nResNet\nMatchNet [49]\n43.40± 0.78\n-\n51.09± 0.71\n-\nMAML [10]\n48.70± 1.84\n-\n63.11± 0.92\n-\nProtoNet [43]\n49.42± 0.78\n-\n68.20± 0.66\n-\nRelationNet [45]\n51.38± 0.82\n-\n67.07± 0.69\n-\nPFA [35]\n54.53± 0.40\n-\n67.87± 0.20\n-\nTADAM [33]\n-\n58.50± 0.30\n-\n76.70± 0.30\nMetaOptNet [25]\n-\n62.64± 0.61\n-\n78.63± 0.46\nBaselines\nMAML\n49.24± 0.21\n58.05± 0.10\n67.92± 0.17\n72.41± 0.20\nMatchNet\n52.87± 0.20\n65.64± 0.20\n67.49± 0.17\n78.72± 0.15\nProtoNet\n52.61± 0.20\n62.39± 0.21\n71.33± 0.16\n80.53± 0.14\nEmbedding Adaptation\nBILSTM\n52.13± 0.20\n63.90± 0.21\n69.15± 0.16\n80.63± 0.14\nDEEPSETS\n54.41± 0.20\n64.14± 0.22\n70.96± 0.16\n80.93± 0.14\nGCN\n53.25± 0.20\n64.50± 0.20\n70.59± 0.16\n81.65± 0.14\nOurs: FEAT\n55.15± 0.20\n66.78± 0.20\n71.61± 0.16\n82.05± 0.14\nTable 6:\nFew-shot classiﬁcation performance with Wide\nResNet (WRN)-28-10 backbone on MiniImageNet dataset (mean\naccuracy±95% conﬁdence interval). Our implementation meth-\nods are measured over 10,000 test trials.\nSetups →\n1-Shot 5-Way\n5-Shot 5-Way\nPFA [35]\n59.60± 0.41\n73.74± 0.19\nLEO [40]\n61.76± 0.08\n77.59± 0.12\nSimpleShot [51]\n63.50± 0.20\n80.33± 0.14\nProtoNet (Ours)\n62.60± 0.20\n79.97± 0.14\nOurs: FEAT\n65.10 ± 0.20\n81.11 ± 0.14\nTable 7: Few-shot classiﬁcation performance with Wide ResNet\n(WRN)-28-10 backbone on TieredImageNet dataset (mean\naccuracy±95% conﬁdence interval). Our implementation meth-\nods are measured over 10,000 test trials.\nSetups →\n1-Shot 5-Way\n5-Shot 5-Way\nLEO [40]\n66.33± 0.05\n81.44± 0.09\nSimpleShot [51]\n69.75± 0.20\n85.31± 0.15\nOurs: FEAT\n70.41 ± 0.23\n84.38 ± 0.16\ntoNet manner, on the MiniImageNet dataset, using the Con-\nvNet as the backbone network.\nDo the adapted embeddings improve the pre-adapted\nembeddings?\nWe report few-shot classiﬁcation results by\nTable 8: Few-shot classiﬁcation performance with ConvNet back-\nbone on CUB dataset (mean accuracy±95% conﬁdence interval).\nOur implementation methods are measured over 10,000 test trials.\nSetups →\n1-Shot 5-Way\n5-Shot 5-Way\nMatchNet [49]\n61.16 ± 0.89\n72.86 ± 0.70\nMAML [10]\n55.92 ± 0.95\n72.09 ± 0.76\nProtoNet [43]\n51.31 ± 0.91\n70.77 ± 0.69\nRelationNet [45]\n62.45 ± 0.98\n76.11 ± 0.69\nInstance Embedding\nMatchNet\n67.73 ± 0.23\n79.00 ± 0.16\nProtoNet\n63.72 ± 0.22\n81.50 ± 0.15\nEmbedding Adaptation\nBILSTM\n62.05 ± 0.23\n73.51 ± 0.19\nDEEPSETS\n67.22 ± 0.23\n79.65 ± 0.16\nGCN\n67.83 ± 0.23\n80.26 ± 0.15\nOurs: FEAT\n68.87 ± 0.22\n82.90 ± 0.15\nTable 9: Ablation studies on whether the embedding adaptation\nimproves the discerning quality of the embeddings. After embed-\nding adaptation, FEAT improves w.r.t. the before-adaptation em-\nbeddings a lot for Few-shot classiﬁcation.\n1-Shot 5-Way\n5-Shot 5-Way\nPre-Adapt\n51.60± 0.20\n70.40± 0.16\nPost-Adapt\n55.15± 0.20\n71.61± 0.16\n15\n\n\n5\n10\n15\n20\nNumber of categories per task\n30\n35\n40\n45\n50\n55\n60\n65\n70\n75\nMean accuracy (in %)\n70.7\n55.9\n47.5\n41.9\n71.3\n56.5\n48.2\n42.4\n71.5\n57.0\n48.8\n43.2\nMethods\nBILSTM\nDeepSets\nFEAT\n(a) Task Interpolation\n5\n10\n15\n20\nNumber of categories per task\n30\n35\n40\n45\n50\n55\n60\n65\n70\n75\n69.2\n52.9\n43.6\n37.5\n71.0\n55.9\n47.4\n41.8\n71.2\n56.7\n48.2\n42.6\nMethods\nBILSTM\nDeepSets\nFEAT\n(b) Task Extrapolation\nFigure 6: Interpolation and Extrapolation of few-shot tasks. We\ntrain different embedding adaptation models on 5-shot 20-way or\n5-way classiﬁcation tasks and evaluate models on unseen tasks\nwith different number of classes (N={5, 10, 15, 20}). It veri-\nﬁes both the interpolation and extrapolation ability of FEAT on a\nvarying number of ways in few-shot classiﬁcation.\nTable 10: Ablation studies on the position to average the same-\nclass embeddings when there are multiple shots per class in FEAT\n(tested on the 5-Way tasks with different numbers of shots). “Pre-\nAvg” and “Post-Avg” means we get the embedding center for each\nclass before or after the set-to-set transformation, respectively.\nSetups →\nPre-Avg\nPost-Avg\n5\n71.61± 0.16\n70.70± 0.16\n15\n77.76± 0.14\n76.58± 0.14\n30\n79.66± 0.13\n78.77± 0.13\nTable 11: Ablation studies on the number of heads in the Trans-\nformer of FEAT (with number of layers ﬁxes to one).\nSetups →\n1-Shot 5-Way\n5-Shot 5-Way\n1\n55.15± 0.20\n71.57± 0.16\n2\n54.91± 0.20\n71.44± 0.16\n4\n55.05± 0.20\n71.63± 0.16\n8\n55.22± 0.20\n71.39± 0.16\nTable 12: Ablation studies on the number of layers in the Trans-\nformer of FEAT (with number of heads ﬁxes to one).\nSetups →\n1-Shot 5-Way\n5-Shot 5-Way\n1\n55.15± 0.20\n71.57± 0.16\n2\n55.42± 0.20\n71.44± 0.16\n3\n54.96± 0.20\n71.63± 0.16\nusing the pre-adapted embeddings of support data (i.e., the\nembedding before adaptation), against those using adapted\nembeddings, for constructing classiﬁers. Table 9 shows that\ntask-speciﬁc embeddings after adaptation improves over\ntask-agnostic embeddings in few-shot classiﬁcations.\nCan FEAT possesses the characteristic of the set func-\ntion?\nWe test three set-to-set transformation implementa-\ntions, namely the BILSTM, the DEEPSETS, and the Trans-\nformer (FEAT), w.r.t. two important properties of the set\nfunction, i.e., task interpolation and task extrapolation. In\nparticular, the few-shot learning model is ﬁrst trained with\n5-shot 20-way tasks. Then the learned model is required\nto evaluate different 5-shot tasks with N = {5, 10, 15, 20}\n(Extrapolation). Similarly, for interpolation, the model is\ntrained with 5-shot 20-way tasks in advance and then eval-\nuated on the previous multi-way tasks. The classiﬁcation\nchange results can be found in Figure 6 (a) and (b). BILSTM\ncannot deal with the size change of the set, especially in the\ntask extrapolation. In both cases, FEAT still gets improve-\nments in all conﬁgurations of N.\nWhen to average the same-class embeddings?\nWhen\nthere is more than one instance per class, i.e. M > 1, we av-\nerage the instances in the same class and use the class center\nto make predictions as in Eq. 10. There are two positions\nto construct the prototypes in FEAT — before the set-to-set\ntransformation (Pre-Avg) and after the set-to-set transfor-\nmation (Post-Avg). In Pre-Avg, we adapt the embeddings\nof the centers, and a test instance is predicted based on\nits distance to the nearest adapted center; while in Post-\nAvg, the instance embeddings are adapted by the set-to-set\nfunction ﬁrst, and the class centers are computed based on\nthe adapted instance embeddings. We investigate the two\nchoices in Table 10, where we ﬁx the number of ways to\n5 (N = 5) and change the number of shots (M) among\n{5, 15, 30}. The results demonstrate the Pre-Avg version\nperforms better than the Post-Avg in all cases, which shows\na more precise input of the set-to-set function by averaging\nthe instances in the same class leads to better results. So we\nuse the Pre-Avg strategy as a default option in our experi-\nments.\nWill deeper and multi-head transformer help?\nIn our\ncurrent implementation of the set-to-set transformation\nfunction, we make use of a shallow and simple transformer,\ni.e., one layer and one head (set of projection). From [47],\nthe transformer can be equipped with complex components\nusing multiple heads and deeper stacked layers. We evalu-\nate this augmented structure, with the number of attention\nheads increases to 2, 4, 8, as well as with the number of\nlayers increases to 2 and 3. As in Table 11 and Table 12,\nwe empirically observe that more complicated structures do\nnot result in improved performance. We ﬁnd that with more\nlayers of transformer stacked, the difﬁculty of optimization\nincreases and it becomes harder to train models until their\nconvergence. Whilst for models with more heads, the mod-\nels seem to over-ﬁt heavily on the training data, even with\nthe usage of auxiliary loss term (like the contrastive loss in\n16\n\n\nTable 13: Ablation studies on effects of the contrastive learning\nof the set-to-set function on FEAT.\nSetups →\n1-Shot 5-Way\n5-Shot 5-Way\nλ = 10\n53.92 ± 0.20\n70.41 ± 0.16\nλ = 1\n54.84 ± 0.20\n71.00 ± 0.16\nλ = 0.1\n55.15 ± 0.20\n71.61 ± 0.16\nλ = 0.01\n54.67 ± 0.20\n71.26 ± 0.16\nTable 14: Ablation studies on the prediction strategy (with cosine\nsimilarity or euclidean distance) of FEAT.\nSetups →\n1-Shot 5-Way\n5-Shot 5-Way\nBackbone →\nConvNet\nResNet\nConvNet\nResNet\nCosine Similarity-based Prediction\nFEAT\n54.64± 0.20\n66.26± 0.20\n71.72± 0.16\n81.83± 0.15\nEuclidean Distance-based Prediction\nFEAT\n55.15± 0.20\n66.78± 0.20\n71.61± 0.16\n82.05± 0.14\nour approach). It might require some careful regularizations\nto prevent over-ﬁtting, which we leave for future work.\nThe effectiveness of contrastive loss.\nTable 13 show the\nfew-shot classiﬁcation results with different weight values\n(λ) of the contrastive loss term for FEAT. From the results,\nwe can ﬁnd that the balance of the contrastive term in the\nlearning objective can inﬂuence the ﬁnal results. Empiri-\ncally, we set λ = 0.1 in our experiments.\nThe inﬂuence of the prediction strategy.\nWe investi-\ngate two embedding-based prediction ways for the few-shot\nclassiﬁcation, i.e., based on the cosine similarity and the\nnegative euclidean distance to measure the relationship be-\ntween objects, respectively. We compare these two choices\nin Table 14.\nTwo strategies in Table 14 only differ in\ntheir similarity measures. In other words, with more than\none shot per class in the task training set, we average the\nsame class embeddings ﬁrst, and then make classiﬁcation\nby computing the cosine similarity or the negative euclidean\ndistance between a test instance and a class prototype. Dur-\ning the optimization, we tune the logits scale temperature\nfor both these methods. We ﬁnd that using the euclidean\ndistance usually requires small temperatures (e.g., γ =\n1\n64)\nwhile a large temperature (e.g., γ = 1) works well with the\nnormalized cosine similarity. The former choice achieves a\nslightly better performance than the latter one.\nD.3. Few-Shot Domain Generalization\nWe show that FEAT learns to adapt the intrinsic structure\nof tasks, and generalize across domains, i.e., predicting\ntest instances even when the visual appearance is changed.\nTable 15: Cross-Domain 1-shot 5-way classiﬁcation results of the\nFEAT approach.\nC →C\nC →R\nR →R\nSupervised\n34.38±0.16\n29.49±0.16\n37.43±0.16\nProtoNet\n35.51±0.16\n29.47±0.16\n37.24±0.16\nFEAT\n36.83±0.17\n30.89±0.17\n38.49±0.16\nTable 16: Results of models for transductive FSL with ConvNet\nbackbone on MiniImageNet. We cite the results of Semi-ProtoNet\nand TPN from [38] and [34] respectively. For TEAM [34], the au-\nthors do not report the conﬁdence intervals, so we set them to 0.00\nin the table.\nFEAT† and FEAT‡ adapt embeddings with the joint\nset of labeled training and unlabeled test instances, while make\nprediction via ProtoNet and Semi-ProtoNet respectively.\nSetups →\n1-Shot 5-Way\n5-Shot 5-Way\nStandard\nProtoNet\n52.61 ± 0.20\n71.33 ± 0.16\nFEAT\n55.15 ± 0.20\n71.61 ± 0.16\nTransductive\nSemi-ProtoNet [38]\n50.41 ± 0.31\n64.39 ± 0.24\nTPN [30]\n55.51 ± 0.84\n69.86 ± 0.67\nTEAM [34]\n56.57 ± 0.00\n72.04 ± 0.00\nSemi-ProtoNet (Ours)\n55.50 ± 0.10\n71.76 ± 0.08\nFEAT†\n56.49 ± 0.16\n72.65 ± 0.20\nFEAT‡\n57.04 ± 0.16\n72.89 ± 0.20\nSetups. We train a few-shot learning model in the standard\ndomain and evaluate it with cross-domain tasks, where the\nN-categories are aligned but domains are different. In de-\ntail, a model is trained on tasks from the “Clipart” domain\nof OfﬁceHome dataset [48], then the model is required to\ngeneralize to both “Clipart (C)” and “Real World (R)” in-\nstances. In other words, we need to classify complex real\nimages by seeing only a few sketches, or even based on the\ninstances in the “Real World (R)” domain.\nResults. Table 15 gives the quantitative results. Here, the\n“supervised” refers to a model trained with standard clas-\nsiﬁcation and then is used for the nearest neighbor classi-\nﬁer with its penultimate layer’s output feature. We observe\nthat ProtoNet can outperform this baseline on tasks when\nevaluating instances from “Clipart” but not ones from “real\nworld”. However, FEAT can improve over “real world” few-\nshot classiﬁcation even only seeing the support data from\n“Clipart”. Besides, when the support set and the test set\nof the target task are sampled from the same but new do-\nmains, e.g., the training and test instances both come from\n“real world”, FEAT also improves the classiﬁcation accuracy\nw.r.t. the baseline methods. It veriﬁes the domain general-\nization ability of the FEAT approach.\n17\n\n\nD.4. Additional Discussions on Transductive FSL\nWe list the results of the transductive few-shot classiﬁ-\ncation in Table 16, where the unlabeled test instances ar-\nrive simultaneously, so that the common structure among\nthe unlabeled test instances could be captured. We com-\npare with three approaches, Semi-ProtoNet [38], TPN [30],\nand TEAM [34]. Semi-ProtoNet utilizes the unlabeled in-\nstances to facilitate the computation of the class center and\nmakes predictions similar to the prototypical network; TPN\nmeta learns a label propagation way to take the unlabeled in-\nstances relationship into consideration; TEAM explores the\npairwise constraints in each task, and formulates the em-\nbedding adaptation into a semi-deﬁnite programming form.\nWe cite the results of Semi-ProtoNet from [38], and cite\nthe results of TPN and TEAM from [34].\nWe also re-\nimplement Semi-ProtoNet with our pre-trained backbone\n(the same pre-trained ConvNet weights as the standard few-\nshot learning setting) for a fair comparison.\nIn this setting, our model leverages the unlabeled test in-\nstances to augment the transformer as discussed in § B.4\nand the embedding adaptation takes the relationship of all\ntest instances into consideration. Based on the adapted em-\nbedding by the joint set of labeled training instances and\nunlabeled test instances, we can make predictions with two\nstrategies. First, we still compute the center of the labeled\ninstances, while such adapted embeddings are inﬂuenced by\nthe unlabeled instances (we denote this approach as FEAT†,\nwhich works the same way as standard FEAT except the aug-\nmented input of the embedding transformation function);\nSecond, we consider to take advantage of the unlabeled in-\nstances and use their adapted embeddings to construct a bet-\nter class prototype as in Semi-ProtoNet (we denote this ap-\nproach as FEAT‡).\nBy using more unlabeled test instances in the transduc-\ntive environment, FEAT† achieves further performance im-\nprovement compared with the standard FEAT, which veriﬁes\nthe unlabeled instances could assist the embedding adapta-\ntion of the labeled ones. With more accurate class center\nestimation, FEAT‡ gets a further improvement. The per-\nformance gain induced by the transductive FEAT is more\nsigniﬁcant in the one-shot learning setting compared with\nthe ﬁve-shot scenario, since the helpfulness of unlabeled in-\nstance decreases when there are more labeled instances.\nD.5. More Generalized FSL Results\nHere we show the full results of FEAT in the general-\nized few-shot learning setting in Table 17, which includes\nboth the 1-shot and 5-shot performance. All methods are\nevaluated on instances composed by SEEN classes, UNSEEN\nclasses, and both of them (COMBINED), respectively. In\nthe 5-shot scenario, the performance improvement mainly\ncomes from the improvement of over the UNSEEN tasks.\nTable 17: Results of generalized FEAT with ConvNet backbone on\nMiniImageNet. All methods are evaluated on instances composed\nby SEEN classes, UNSEEN classes, and both of them (COMBINED),\nrespectively.\nMeasures →\nSEEN\nUNSEEN\nCOMBINED\n1-shot learning\nProtoNet\n41.73±0.03\n48.64±0.20\n35.69±0.03\nFEAT\n43.94±0.03\n49.72±0.20\n40.50±0.03\n5-shot learning\nProtoNet\n41.06±0.03\n64.94±0.17\n38.04±0.02\nFEAT\n44.94±0.03\n65.33±0.16\n41.68±0.03\nRandom Chance\n1.56\n20.00\n1.45\nTable 18: The top-5 low-shot learning accuracy over all classes\non the large scale ImageNet [39] dataset (w/ ResNet-50).\nUNSEEN\n1-Shot\n2-Shot\n5-Shot\n10-Shot\n20-Shot\nProtoNet [43]\n49.6\n64.0\n74.4\n78.1\n80.0\nPMN [52]\n53.3\n65.2\n75.9\n80.1\n82.6\nFEAT\n53.8\n65.4\n76.0\n81.2\n83.6\nAll\n1-Shot\n2-Shot\n5-Shot\n10-Shot\n20-Shot\nProtoNet [43]\n61.4\n71.4\n78.0\n80.0\n81.1\nPMN [52]\n64.8\n72.1\n78.8\n81.7\n83.3\nFEAT\n65.1\n72.5\n79.3\n82.1\n83.9\nAll w/ Prior\n1-Shot\n2-Shot\n5-Shot\n10-Shot\n20-Shot\nProtoNet [43]\n62.9\n70.5\n77.1\n79.5\n80.8\nPMN [52]\n63.4\n70.8\n77.9\n80.9\n82.7\nFEAT\n63.8\n71.2\n78.1\n81.3\n83.4\nD.6. Large-Scale Low-Shot Learning\nSimilar to the generalized few-shot learning, the large-\nscale low-shot learning [12, 14, 52] considers the few-shot\nclassiﬁcation ability on both SEEN and UNSEEN classes on\nthe full ImageNet [39] dataset. There are in total 389 SEEN\nclasses and 611 UNSEEN classes [14]. We follow the setting\n(including the splits) of the prior work [14] and use features\nextracted based on the pre-trained ResNet-50 [15]. Three\nevaluation protocols are evaluated, namely the top-5 few-\nshot accuracy on the UNSEEN classes, on the combined set\nof both SEEN and UNSEEN classes, and the calibrated accu-\nracy on weighted by selected set prior on the combined set\nof both SEEN and UNSEEN classes. The results are listed in\nTable 18. We observe that FEAT achieves better results than\nothers, which further validates FEAT’s superiority in gener-\nalized classiﬁcation setup, a large scale learning setup.\n18\n"
}