{
  "filename": "1812.03664v6.pdf",
  "num_pages": 18,
  "pages": [
    "Few-Shot Learning via Embedding Adaptation with Set-to-Set Functions\nHan-Jia Ye*\nNanjing University\nyehj@lamda.nju.edu.cn\nHexiang Hu\nUSC\nhexiangh@usc.edu\nDe-Chuan Zhan\nNanjing University\nzhandc@lamda.nju.edu.cn\nFei Shaâ€ \nUSC & Google\nfsha@google.com\nAbstract\nLearning with limited data is a key challenge for vi-\nsual recognition. Many few-shot learning methods address\nthis challenge by learning an instance embedding function\nfrom seen classes and apply the function to instances from\nunseen classes with limited labels. This style of transfer\nlearning is task-agnostic: the embedding function is not\nlearned optimally discriminative with respect to the unseen\nclasses, where discerning among them leads to the tar-\nget task. In this paper, we propose a novel approach to\nadapt the instance embeddings to the target classiï¬cation\ntask with a set-to-set function, yielding embeddings that are\ntask-speciï¬c and are discriminative. We empirically investi-\ngated various instantiations of such set-to-set functions and\nobserved the Transformer is most effective â€” as it naturally\nsatisï¬es key properties of our desired model. We denote this\nmodel as FEAT (few-shot embedding adaptation w/ Trans-\nformer) and validate it on both the standard few-shot classi-\nï¬cation benchmark and four extended few-shot learning set-\ntings with essential use cases, i.e., cross-domain, transduc-\ntive, generalized few-shot learning, and low-shot learning.\nIt archived consistent improvements over baseline models\nas well as previous methods, and established the new state-\nof-the-art results on two benchmarks.\n1. Introduction\nFew-shot visual recognition [10, 23, 24, 27, 49] emerged\nas a promising direction in tackling the challenge of learn-\ning new visual concepts with limited annotations.\nCon-\ncretely, it distinguishes two sets of visual concepts: SEEN\nand UNSEEN ones. The target task is to construct visual\nclassiï¬ers to identify classes from the UNSEEN where each\nclass has a very small number of exemplars (â€œfew-shotâ€).\nThe main idea is to discover transferable visual knowl-\nedge in the SEEN classes, which have ample labeled in-\nstances, and leverage it to construct the desired classiï¬er.\nFor example, state-of-the-art approaches for few-shot learn-\n*Work mostly done when the author was a visiting scholar at USC.\nâ€ On leave from USC\ning [40, 43, 46, 49] usually learn a discriminative instance\nembedding model on the SEEN categories, and apply it to\nvisual data in UNSEEN categories. In this common embed-\nding space, non-parametric classiï¬ers (e.g., nearest neigh-\nbors) are then used to avoid learning complicated recogni-\ntion models from a small number of examples.\nSuch approaches suffer from one important limitation.\nAssuming a common embedding space implies that the dis-\ncovered knowledge â€“ discriminative visual features â€“ on\nthe SEEN classes are equally effective for any classiï¬cation\ntasks constructed for an arbitrary set of UNSEEN classes. In\nconcrete words, suppose we have two different target tasks:\ndiscerning â€œcatâ€ versus â€œdogâ€ and discerning â€œcatâ€ versus\nâ€œtigerâ€. Intuitively, each task uses a different set of discrim-\ninative features. Thus, the most desired embedding model\nï¬rst needs to be able to extract discerning features for either\ntask at the same time. This could be a challenging aspect in\nits own right as the current approaches are agnostic to what\nthose â€œdownstreamâ€ target tasks are and could accidentally\nde-emphasize selecting features for future use. Secondly,\neven if both sets of discriminative features are extracted,\nthey do not necessarily lead to the optimal performance for\na speciï¬c target task. The most useful features for discern-\ning â€œcatâ€ versus â€œtigerâ€ could be irrelevant and noise to the\ntask of discerning â€œcatâ€ versus â€œdogâ€!\nWhat is missing from the current few-shot learning ap-\nproaches is an adaptation strategy that tailors the visual\nknowledge extracted from the SEEN classes to the UNSEEN\nones in a target task. In other words, we desire separate em-\nbedding spaces where each one of them is customized such\nthat the visual features are most discriminative for a given\ntask. Towards this, we propose a few-shot model-based em-\nbedding adaptation method that adjusts the instance embed-\nding models derived from the SEEN classes. Such model-\nbased embedding adaptation requires a set-to-set function: a\nfunction mapping that takes all instances from the few-shot\nsupport set and outputs the set of adapted support instance\nembeddings, with elements in the set co-adapting with each\nother. Such output embeddings are then assembled as the\nprototypes for each visual category and serve as the near-\nest neighbor classiï¬ers.\nFigure 1 qualitatively illustrates\n1\narXiv:1812.03664v6  [cs.LG]  13 Jun 2021\n",
    "Malamute\nAnt\nSchool bus\nGolden retriever\nTheater curtain\nAdaptation\nLion\nSchool bus\nHourglass\nVase\nTrifle\nAdaptation\nTrifle\nScoreboard\nGolden retriever\nDalmatian\nVase\nAdaptation\nGolden retriever\nNematode\nLion\nDalmatian\nMalamute\nAdaptation\n(a) Accâ†‘: 40.33% â†’55.33%\n(b) Accâ†‘: 48.00% â†’69.60%\n(c) Accâ†‘: 43.60% â†’63.33%\n(d) Accâ†“: 56.33% â†’47.13%\nFigure 1: Qualitative visualization of model-based embedding adaptation procedure (implemented using FEAT) on test tasks (refer to\nÂ§ 5.2.2 for more details). Each ï¬gure shows the locations of PCA projected support embeddings (class prototypes) before and after the\nadaptation of FEAT. Values below are the 1-shot 5-way classiï¬cation accuracy before and after the the adaptation. Interestingly, the\nembedding adaptation step of FEAT pushes the support embeddings apart from the clutter and toward their own clusters, such that they can\nbetter ï¬ts the test data of its categories. (Best view in colors!)\nthe embedding adaptation procedure (as results of our best\nmodel). These class prototypes spread out in the embedding\nspace toward the samples cluster of each category, indicat-\ning the effectiveness of embedding adaptation.\nIn this paper, we implement the set-to-set transformation\nusing a variety of function approximators, including bidi-\nrectional LSTM [16] (Bi-LSTM), deep sets [56], graph con-\nvolutional network (GCN) [21], and Transformer [29, 47].\nOur experimental results (refer to Â§ 5.2.1) suggest that\nTransformer is the most parameter efï¬cient choice that at\nthe same time best implements the key properties of the de-\nsired set-to-set transformation, including contextualization,\npermutation invariance, interpolation and extrapolation ca-\npabilities (see Â§ 4.1). As a consequence, we choose the\nset-to-set function instantiated with Transformer to be our\nï¬nal model and denote it as FEAT (Few-shot Embedding\nAdaptation with Transformer). We further conduct compre-\nhensive analysis on FEAT and evaluate it on many extended\ntasks, including few-shot domain generalization, transduc-\ntive few-shot learning, and generalized few-shot learning.\nOur overall contribution is three-fold.\nâ€¢ We formulate the few-shot learning as a model-based em-\nbedding adaptation to make instance embeddings task-\nspeciï¬c, via using a set-to-set transformation.\nâ€¢ We instantiate such set-to-set transformation with various\nfunction approximators, validating and analyzing their\nfew-shot learning ability, task interpolation ability, and\nextrapolation ability, etc. It concludes our model (FEAT)\nthat uses the Transformer as the set-to-set function.\nâ€¢ We evaluate our FEAT model on a variety of extended\nfew-shot learning tasks, where it achieves superior per-\nformances compared with strong baseline approaches.\n2. Related Work\nMethods speciï¬cally designed for few-shot learning fall\nbroadly into two categories. The ï¬rst is to control how a\nclassiï¬er for the target task should be constructed.\nOne\nfruitful idea is the meta-learning framework where the\nclassiï¬ers are optimized in anticipation that a future up-\ndate due to data from a new task performs well on that\ntask [2, 3, 10, 13, 26, 32, 36, 40], or the classiï¬er itself is\ndirectly meta-predicted by the new task data [35, 53].\nAnother line of approach has focused on learning gener-\nalizable instance embeddings [1, 5, 6, 17, 22, 31, 42, 46, 49]\nand uses those embeddings on simple classiï¬ers such as\nnearest neighbor rules. The key assumption is that the em-\nbeddings capture all necessarily discriminative representa-\ntions of data such that simple classiï¬ers are sufï¬ced, hence\navoiding the danger of overï¬tting on a small number of la-\nbeled instances. Early work such as [22] ï¬rst validated the\nimportance of embedding in one-shot learning, whilst [49]\nproposes to learn the embedding with a soft nearest neigh-\nbor objective, following a meta-learning routine. Recent\nadvances have leveraged different objective functions for\nlearning such embedding models, e.g., considering the class\nprototypes [43], decision ranking [46], and similarity com-\nparison [45]. Most recently, [41] utilizes the graph convo-\nlution network [21] to unify the embedding learning.\nOur work follows the second school of thoughts. The\nmain difference is that we do not assume the embed-\ndings learned on SEEN classes, being agnostic to the tar-\nget tasks, are necessarily discriminative for those tasks.\nIn contrast, we propose to adapt those embeddings for\neach target task with a set-to-set function so that the trans-\nformed embeddings are better aligned with the discrimina-\ntion needed in those tasks. We show empirically that such\ntask-speciï¬c embeddings perform better than task-agnostic\nones.\nMetaOptNet [25] and CTM [28] follow the same\nspirit of learning task-speciï¬c embedding (or classiï¬ers) via\neither explicitly optimization of target task or using concen-\ntrator and projector to make distance metric task-speciï¬c.\n2\n",
    "Classification \nScores\nCNN\nCNN\nCNN\nCNN\nSoft Nearest \nNeighbor\n(a) Instance Embedding\nClassification \nScores\nEmbedding \nAdaptation\nCNN\nCNN\nCNN\nCNN\nSoft Nearest \nNeighbor\nTrain Instance\nTest Instance\nTask Agnostic\nEmbedding\nTask Specific \nEmbedding\n(b) Embedding Adaptation\nSet-to-Set Function\nFigure 2:\nIllustration of the proposed Few-Shot Embedding\nAdaptation Transformer (FEAT). Existing methods usually use the\nsame embedding function E for all tasks. We propose to adapt the\nembeddings to each target few-shot learning task with a set-to-set\nfunction such as Transformer, BiLSTM, DeepSets, and GCN.\n3. Learning Embedding for Task-agnostic FSL\nIn the standard formulation of few-shot learning\n(FSL) [10, 49], a task is represented as a M-shot N-way\nclassiï¬cation problem with N classes sampled from a set\nof visual concepts U and M (training/support) examples\nper class.\nWe denote the training set (also referred as\nsupport sets in the literature) as Dtrain = {xi, yi}NM\ni=1 ,\nwith the instance xi âˆˆRD and the one-hot labeling vec-\ntor yi âˆˆ{0, 1}N. We will use â€œsupport setâ€ and â€œtrain-\ning setâ€ interchangeably in the paper. In FSL, M is of-\nten small (e.g., M = 1 or M = 5).\nThe goal is to\nï¬nd a function f that classiï¬es a test instance xtest by\nË†ytest = f(xtest; Dtrain) âˆˆ{0, 1}N.\nGiven a small number of training instances, it is chal-\nlenging to construct complex classiï¬ers f(Â·). To this end,\nthe learning algorithm is also supplied with additional data\nconsisting of ample labeled instances. These additional data\nare drawn from visual classes S, which does not overlap\nwith U. We refer to the original task as the target task which\ndiscerns N UNSEEN classes U. To avoid confusion, we de-\nnote the data from the SEEN classes S as DS.\nTo learn f(Â·) using DS, we synthesize many M-shot N-\nway FSL tasks by sampling the data in the meta-learning\nmanner [10, 49]. Each sampling gives rise to a task to clas-\nsify a test set instance xS\ntest into one of the N SEEN classes\nby f(Â·), where the test instances set DS\ntest is composed of\nthe labeled instances with the same distribution as DS\ntrain.\nFormally, the function f(Â·) is learnt to minimize the aver-\naged error over those sampled tasks\nf âˆ—= arg min\nf\nX\n(xS\ntest,yS\ntest)âˆˆDS\ntest\nâ„“(f(xS\ntest; DS\ntrain), yS\ntest)\n(1)\nwhere the loss â„“(Â·) measures the discrepancy between the\nprediction and the true label. For simplicity, we have as-\nsumed we only synthesize one task with test set DS\ntest. The\noptimal f âˆ—is then applied to the original target task.\nWe consider the approach based on learning embeddings\nAlgorithm 1 Training strategy of embedding adaptation\nRequire: Seen class set S\n1: for all iteration = 1,...,MaxIteration do\n2:\nSample N-way M-shot (DS\ntrain, DS\ntest) from S\n3:\nCompute Ï†x = E(x), for x âˆˆX S\ntrain âˆªX S\ntest\n4:\nfor all (xS\ntest, yS\ntest) âˆˆDS\ntest do\n5:\nCompute {Ïˆx ; âˆ€x âˆˆX S\ntrain} with T via Eq. 3\n6:\nPredict Ë†yS\ntest with {Ïˆx} as Eq. 4\n7:\nCompute â„“(Ë†yS\ntest, yS\ntest) with Eq. 1\n8:\nend for\n9:\nCompute âˆ‡E,T\nP\n(xS\ntest,yS\ntest)âˆˆDS\ntest â„“(Ë†yS\ntest, yS\ntest)\n10:\nUpdate E and T with âˆ‡E,T use SGD\n11: end for\n12: return Embedding function E and set function T.\nfor FSL [43, 49] (see Figure 2 (a) for an overview). In par-\nticular, the classiï¬er f(Â·) is composed of two elements. The\nï¬rst is an embedding function Ï†x = E(x) âˆˆRd that maps\nan instance x to a representation space. The second compo-\nnent applies the nearest neighbor classiï¬ers in this space:\nË†ytest = f(Ï†xtest; {Ï†x, âˆ€(x, y) âˆˆDtrain})\n(2)\nâˆexp\n\u0000sim(Ï†xtest, Ï†x)\n\u0001\nÂ· y, âˆ€(x, y) âˆˆDtrain\nNote that only the embedding function is learned by opti-\nmizing the loss in Eq. 1. For reasons to be made clear in\nbelow, we refer this embedding function as task-agnostic.\n4. Adapting Embedding for Task-speciï¬c FSL\nIn what follows, we describe our approach for few-shot\nlearning (FSL). We start by describing the main idea (Â§ 4.1,\nalso illustrated in Figure 2), then introduce the set-to-set\nadaptation function (Â§ 4.2). Last are learning (Â§ 4.3) and\nimplementations details (Â§ 4.4).\n4.1. Adapting to Task-Speciï¬c Embeddings\nThe key difference between our approach and traditional\nones is to learn task-speciï¬c embeddings. We argue that the\nembedding Ï†x is not ideal. In particular, the embeddings do\nnot necessarily highlight the most discriminative represen-\ntation for a speciï¬c target task. To this end, we introduce an\nadaption step where the embedding function Ï†x (more pre-\ncisely, its values on instances) is transformed. This trans-\nformation is a set-to-set function that contextualizes over\nthe image instances of a set, to enable strong co-adaptation\nof each item.\nInstance functions fails to have such co-\nadaptation property.\nFurthermore, the set-to-set-function\nreceives instances as bags, or sets without orders, requiring\nthe function to output the set of reï¬ned instance embeddings\n3\n",
    "while being permutation-invariant. Concretely,\n{Ïˆx ; âˆ€x âˆˆXtrain} = T ({Ï†x ; âˆ€x âˆˆXtrain})\n(3)\n= T (Ï€ {Ï†x ; âˆ€x âˆˆXtrain}))\nwhere Xtrain is a set of all the instances in the training set\nDtrain for the target task. Ï€(Â·) is a permutation operator\nover a set. Thus the set of adapted embedding will not\nchange if we apply a permutation over the input embedding\nset. With adapted embedding Ïˆx, the test instance xtest can\nbe classiï¬ed by computing nearest neighbors w.r.t. Dtrain:\nË†ytest = f(Ï†xtest; {Ïˆx, âˆ€(x, y) âˆˆDtrain})\n(4)\nOur approach is generally applicable to different types of\ntask-agnostic embedding function E and similarity measure\nsim(Â·, Â·), e.g., the (normalized) cosine similarity [49] or the\nnegative distance [43]. Both the embedding function E and\nthe set transformation function T are optimized over syn-\nthesized FSL tasks sampled from DS, sketched in Alg. 1.\nIts key difference from conventional FSL is in the line 4 to\nline 8 where the embeddings are transformed.\n4.2. Embedding Adaptation via Set-to-set Functions\nNext, we explain various choices as the instantiations of\nthe set-to-set embedding adaptation function.\nBidirectional LSTM (BILSTM) [16, 49] is one of the\ncommon choice to instantiate the set-to-set transformation,\nwhere the addition between the input and the hidden layer\noutputs of each BILSTM cell leads to the adapted embed-\nding. It is notable that the output of the BILSTM suppose to\ndepend on the order of the input set. Note that using BIL-\nSTM as embedding adaptation model is similar but different\nfrom the fully conditional embedding [49], where the later\none contextualizes both training and test instance embed-\nding altogether, which results in a transductive setting.\nDeepSets [56] is inherently a permutation-invariant trans-\nformation function. It is worth noting that DEEPSETS aggre-\ngates the instances in a set into a holistic set vector. We con-\nsider two components to implement such DeepSets transfor-\nmation, an instance centric set vector combined with a set\ncontext vector. For x âˆˆXtrain, we deï¬ne its complemen-\ntary set as xâˆ. Then we implement the DEEPSETS by:\nÏˆx = Ï†x + g([Ï†x;\nX\nxiâ€²âˆˆxâˆ\nh(Ï†xiâ€²)])\n(5)\nIn Eq. 5, g and h are two-layer multi-layer perception\n(MLP) with ReLU activation which map the embedding\ninto another space and increase the representation ability\nof the embedding. For each instance, embeddings in its\ncomplementary set is ï¬rst combined into a set vector as the\ncontext, and then this vector is concatenated with the input\nembedding to obtain the residual component of adapted em-\nbedding. This conditioned embedding takes other instances\nin the set into consideration, and keeps the â€œset (permutation\ninvariant)â€ property. In practice, we ï¬nd using the maxi-\nmum operator in Eq. 5 works better than the sum operator\nsuggested in [56].\nGraph Convolutional Networks (GCN) [21, 41] propa-\ngate the relationship between instances in the set. We ï¬rst\nconstruct the degree matrix A to represent the similarity be-\ntween instances in a set. If two instances come from the\nsame class, then we set the corresponding element in A to\n1, otherwise to 0. Based on A, we build the â€œnormalizedâ€\nadjacency matrix S for a given set with added self-loops\nS = Dâˆ’1\n2 (A + I)Dâˆ’1\n2 . I is the identity matrix, and D is\nthe diagonal matrix whose elements are equal to the sum of\nelements in the corresponding row of A + I.\nLet Î¦0 = {Ï†x ; âˆ€x âˆˆXtrain}, the relationship between\ninstances could be propagated based on S, i.e.,\nÎ¦t+1 = ReLU(SÎ¦tW) , t = 0, 1, . . . , T âˆ’1\n(6)\nW is a projection matrix for feature transformation.\nIn\nGCN, the embedding in the set is transformed based on\nEq. 6 multiple times, and the ï¬nal Î¦T gives rise to the {Ïˆx}.\nTransformer. [47] We use the Transformer architec-\nture [47] to implement T. In particular, we employ self-\nattention mechanism [29, 47] to transform each instance\nembedding with consideration to its contextual instances.\nNote that it naturally satisï¬es the desired properties of T\nbecause it outputs reï¬ned instance embeddings and is per-\nmutation invariant. We denote it as Few-Shot Embedding\nAdaptation with Transformer (FEAT).\nTransformer is a store of triplets in the form of (query\nQ, key K, and value V). To compute proximity and re-\nturn values, those points are ï¬rst linearly mapped into some\nspace K = W âŠ¤\nK\n\u0002\nÏ†xk; âˆ€xk âˆˆK\n\u0003\nâˆˆRdÃ—|K|, which\nis also the same for Q and V with WQ and WV respec-\ntively. Transformer computes what is the right value for a\nquery point â€” the query xq âˆˆQ is ï¬rst matched against\na list of keys K where each key has a value V . The ï¬-\nnal value is then returned as the sum of all the values\nweighted by the proximity of the key to the query point,\ni.e. Ïˆxq = Ï†xq + P\nk Î±qkV:,k, where\nÎ±qk âˆexp\n \nÏ†âŠ¤\nxqWQ Â· K\nâˆš\nd\n!\nand V:,k is the k-th column of V . In the standard FSL setup,\nwe have Q = K = V = Xtrain.\n4.3. Contrastive Learning of Set-to-Set Functions\nTo facilitate the learning of embedding adaptation, we\napply a contrastive objective in addition to the general one.\n4\n",
    "It is designed to make sure that instances embeddings af-\nter adaptation is similar to the same class neighbors and\ndissimilar to those from different classes. Speciï¬cally, the\nembedding adaptation function T is applied to instances of\neach n of the N class in DS\ntrain âˆªDS\ntest, which gives rise to\nthe transformed embedding Ïˆâ€²\nx and class centers {cn}N\nn=1.\nThen we apply the contrastive objective to make sure train-\ning instances are close to its own class center than other\ncenters. The total objective function (together with Eq. 1) is\nshown as following:\nL(Ë†ytest, ytest) = â„“(Ë†ytest, ytest)\n(7)\n+Î» Â· â„“\n\u0000softmax\n\u0000sim(Ïˆâ€²\nxtest, cn)\n\u0001\n, ytest\n\u0001\nThis contrastive learning makes the set transformation ex-\ntract common characteristic for instances of the same cate-\ngory, so as to preserve the category-wise similarity.\n4.4. Implementation details\nWe consider three different types of convolutional net-\nworks as the backbone for instance embedding function E:\n1) A 4-layer convolution network (ConvNet) [43, 46, 49]\nand 2) the 12-layer residual network (ResNet) used in [25],\nand 3) the Wide Residual Network (WideResNet) [40, 55].\nWe apply an additional pre-training stage for the backbones\nover the SEEN classes, based on which our re-implemented\nmethods are further optimized. To achieve more precise em-\nbedding, we average the same-class instances in the train-\ning set before the embedding adaptation with the set-to-\nset transformation. Adam [20] and SGD are used to op-\ntimize ConvNet and ResNet variants respectively. More-\nover, we follow the most standard implementations for the\nfour set-to-set functions â€” BiLSTM [16], DeepSets [56],\nGraph Convolutional Networks (GCN) [21] and Trans-\nformer (FEAT) [47].\nWe refer readers to supplementary\nmaterial (SM) for complete details and ablation studies of\neach set-to-set functions. Our implementation is available\nat https://github.com/Sha-Lab/FEAT.\n5. Experiments\nIn this section, we ï¬rst evaluate a variety of models for\nembedding adaptation in Â§ 5.2 with standard FSL. It con-\ncludes that FEAT (with Transformer) is the most effective\napproach among different instantiations. Next, we perform\nablation studies in Â§ 5.2.2 to analyze FEAT in details. Even-\ntually, we evaluate FEAT on many extended few-shot learn-\ning tasks to study its general applicability (Â§ 5.3).\nThis\nstudy includes few-shot domain generalization, transduc-\ntive few-shot learning, generalized few-shot learning, and\nlarge-scale low-shot learning (refer to SM).\n5.1. Experimental Setups\nDatasets.\nMiniImageNet [49] and TieredImageNet [38]\ndatasets are subsets of the ImageNet [39]. MiniImageNet\nincludes a total number of 100 classes and 600 examples\nper class. We follow the setup provided by [36], and use\n64 classes as SEEN categories, 16 and 20 as two sets of\nUNSEEN categories for model validation and evaluation re-\nspectively.\nTieredImageNet is a large-scale dataset with\nmore categories, which contains 351, 97, and 160 categories\nfor model training, validation, and evaluation, respectively.\nIn addition to these, we investigate the Ofï¬ceHome [48]\ndataset to validate the generalization ability of FEAT across\ndomains. There are four domains in Ofï¬ceHome, and two\nof them (â€œClipartâ€ and â€œReal Worldâ€) are selected, which\ncontains 8722 images. After randomly splitting all classes,\n25 classes serve as the seen classes to train the model, and\nthe remaining 15 and 25 classes are used as two UNSEEN\nfor evaluation. Please refer to SM for more details.\nEvaluation protocols. Previous approaches [10, 43, 46]\nusually follow the original setting of [49] and evaluate the\nmodels on 600 sampled target tasks (15 test instances per\nclass).\nIn a later study [40], it was suggested that such\nan evaluation process could potentially introduce high vari-\nances. Therefore, we follow the new and more trustworthy\nevaluation setting to evaluate both baseline models and our\napproach on 10,000 sampled tasks. We report the mean ac-\ncuracy (in %) as well as the 95% conï¬dence interval.\nBaseline and embedding adaptation methods.\nWe re-\nimplement the prototypical network (ProtoNet) [43] as a\ntask-agnostic embedding baseline model. This is known\nas a very strong approach [8] when the backbone archi-\ntecture is deep, i.e., residual networks [15]. As suggested\nby [33], we tune the scalar temperature carefully to scale\nthe logits of both approaches in our re-implementation. As\nmentioned, we implement the embedding adaptation model\nwith four different function approximators, and denote them\nas BILSTM, DEEPSETS, GCN, and FEAT (i.e. Transformer).\nThe concrete details of each model are included in the SM.\nBackbone pre-training.\nInstead of optimizing from\nscratch, we apply an additional pre-training strategy as\nsuggested in [35, 40]. The backbone network, appended\nwith a softmax layer, is trained to classify all SEEN\nclasses with the cross-entropy loss (e.g., 64 classes in the\nMiniImageNet).\nThe classiï¬cation performance over the\npenultimate layer embeddings of sampled 1-shot tasks from\nthe model validation split is evaluated to select the best pre-\ntrained model, whose weights are then used to initialize the\nembedding function E in the few-shot learning.\n5.2. Standard Few-Shot Image Classiï¬cation\nWe compare our proposed FEAT method with the in-\nstance embedding baselines as well as previous methods on\n5\n",
    "Table 1: Few-shot classiï¬cation accuracy on MiniImageNet. â‹†\nCTM [28] and SimpleShot [51] utilize the ResNet-18. (see SM\nfor the full table with conï¬dence intervals and WRN results.).\nSetups â†’\n1-Shot 5-Way\n5-Shot 5-Way\nBackbone â†’\nConvNet\nResNet\nConvNet\nResNet\nMatchNet [49]\n43.40\n-\n51.09\n-\nMAML [10]\n48.70\n-\n63.11\n-\nProtoNet [43]\n49.42\n-\n68.20\n-\nRelationNet [45]\n51.38\n-\n67.07\n-\nPFA [35]\n54.53\n59.60\n67.87\n73.74\nTADAM [33]\n-\n58.50\n-\n76.70\nMetaOptNet [25]\n-\n62.64\n-\n78.63\nCTM [28]\n-\n64.12\n-\n80.51\nSimpleShot [51]\n49.69\n62.85\n66.92\n80.02\nInstance embedding\nProtoNet\n52.61\n62.39\n71.33\n80.53\nEmbedding adaptation\nBILSTM\n52.13\n63.90\n69.15\n80.62\nDEEPSETS\n54.41\n64.14\n70.96\n80.93\nGCN\n53.25\n64.50\n70.59\n81.65\nFEAT\n55.15\n66.78\n71.61\n82.05\nthe standard MiniImageNet [49] and TieredImageNet [38]\nbenchmarks, and then perform detailed analysis on the ab-\nlated models. We include additional results with CUB [50]\ndataset in SM, which shares a similar observation.\n5.2.1. Main Results\nComparison to previous State-of-the-arts. Table 1 and\nTable 2 show the results of our method and others on the\nMiniImageNet and TieredImageNet. First, we observe that\nthe best embedding adaptation method (FEAT) outperforms\nthe instance embedding baseline on both datasets, indi-\ncating the effectiveness of learning task-speciï¬c embed-\nding space. Meanwhile, the FEAT model performs signif-\nicantly better than the current state-of-the-art methods on\nMiniImageNet dataset. On the TieredImageNet, we observe\nthat the ProtoNet baseline is already better than some pre-\nvious state-of-the-arts based on the 12-layer ResNet back-\nbone.\nThis might due to the effectiveness of the pre-\ntraining stage on the TieredImageNet as it is larger than\nMiniImageNet and a fully converged model can be itself\nvery effective. Based on this, all embedding adaptation ap-\nproaches further improves over ProtoNet almost in all cases,\nwith FEAT achieving the best performances among all ap-\nproaches. Note that here our pre-training strategy is most\nsimilar to the one used in PFA [35], while we further ï¬ne-\ntune the backbone. Temperature scaling of the logits inï¬‚u-\nences the performance a lot when ï¬ne-tuning over the pre-\ntrained weights. Additionally, we list some recent methods\n(SimpleShot [51], and CTM [28]) using different backbone\nTable 2: Few-shot classiï¬cation accuracy and 95% conï¬dence in-\nterval on TieredImageNet with the ResNet backbone.\nSetups â†’\n1-Shot 5-Way\n5-Shot 5-Way\nProtoNet [43]\n53.31 Â± 0.89\n72.69 Â± 0.74\nRelationNet [45]\n54.48 Â± 0.93\n71.32 Â± 0.78\nMetaOptNet [25]\n65.99 Â± 0.72\n81.56 Â± 0.63\nCTM [28]\n68.41 Â± 0.39\n84.28 Â± 1.73\nSimpleShot [51]\n69.09 Â± 0.22\n84.58 Â± 0.16\nInstance embedding\nProtoNet\n68.23 Â± 0.23\n84.03 Â± 0.16\nEmbedding adaptation\nBILSTM\n68.14 Â± 0.23\n84.23 Â± 0.16\nDEEPSETS\n68.59 Â± 0.24\n84.36 Â± 0.16\nGCN\n68.20 Â± 0.23\n84.64 Â± 0.16\nFEAT\n70.80 Â± 0.23\n84.79 Â± 0.16\nTable 3: Number of parameters introduced by each set-to-set\nfunction in additional to the backboneâ€™s parameters.\nBILSTM\nDEEPSETS\nGCN\nFEAT\nConvNet\n25K\n82K\n33K\n16K\nResNet\n2.5M\n8.2M\n3.3M\n1.6M\narchitectures such as ResNet-18 for reference.\nComparison among the embedding adaptation models.\nAmong the four embedding adaptation methods, BILSTM in\nmost cases achieves the worst performances and sometimes\neven performs worse than ProtoNet. This is partially due\nto the fact that BILSTM can not easily implement the re-\nquired permutation invariant property (also shown in [56]),\nwhich confuses the learning process of embedding adapta-\ntion. Secondly, we ï¬nd that DEEPSETS and GCN have the\nability to adapt discriminative task-speciï¬c embeddings but\ndo not achieve consistent performance improvement over\nthe baseline ProtoNet especially on MiniImageNet with the\nConvNet backbone. A potential explanation is that, such\nmodels when jointly learned with the backbone model, can\nmake the optimization process more difï¬cult, which leads\nto the varying ï¬nal performances. In contrast, we observe\nthat FEAT can consistently improve ProtoNet and other em-\nbedding adaptation approaches in all cases, without addi-\ntional bells and whistles. It shows that the Transformer as a\nset-to-set function can implement rich interactions between\ninstances, which provides its high expressiveness to model\nthe embedding adaptation process.\nInterpolation and extrapolation of classiï¬cation ways.\nNext, we study different set-to-set functions on their capa-\nbility of interpolating and extrapolating across the number\nof classiï¬cation ways. To do so, we train each variant of em-\n6\n",
    "5\n10\n15\n20\nNumber of categories per task\n0\n10\n20\n30\n40\n50\n60\n70\nMean accuracy (in %)\n52.5\n36.8\n29.3\n24.6\n55.0\n38.6\n30.6\n25.8\n53.2\n37.1\n29.5\n24.9\n55.1\n39.1\n31.3\n26.4\nMethods\nRandom\nBILSTM\nDeepSets\nGCN\nFEAT\n5\n10\n15\n20\nNumber of categories per task\n0\n10\n20\n30\n40\n50\n60\n70\n52.1\n35.5\n27.5\n22.9\n54.4\n36.9\n27.3\n20.6\n54.1\n37.9\n30.1\n25.3\n55.1\n39.1\n31.1\n26.2\nMethods\nRandom\nBILSTM\nDeepSets\nGCN\nFEAT\n(a) Way Interpolation\n(b) Way Extrapolation\nFigure 3: Interpolation and Extrapolation of few-shot tasks\nfrom the â€œwayâ€ perspective. First, We train various embedding\nadaptation models on 1-shot 20-way (a) or 5-way (b) classiï¬cation\ntasks and evaluate models on unseen tasks with different number\nof classes (N={5, 10, 15, 20}). It shows that FEAT is superior in\nterms of way interpolation and extrapolation ability.\nbedding adaptation functions with both 1-shot 20-way and\n1-shot 5-way tasks, and measure the performance change as\na function to the number of categories in the test time. We\nreport the mean accuracies evaluated on few-shot classiï¬-\ncation with N = {5, 10, 15, 20} classes, and show results\nin Figure 3. Surprisingly, we observe that FEAT achieves\nalmost the same numerical performances in both extrapo-\nlation and interpolation scenarios, which further displays\nits strong capability of learning the set-to-set transforma-\ntion. Meanwhile, we observe that DEEPSETS works well\nwith interpolation but fails with extrapolation as its perfor-\nmance drops signiï¬cantly with the larger N. In contrast,\nGCN achieves strong extrapolation performances but does\nnot work as effectively in interpolation. BILSTM performs\nthe worst in both cases, as it is by design not permutation\ninvariant and may have ï¬tted an arbitrary dependency be-\ntween instances.\nParameter efï¬ciency. Table 3 shows the number of ad-\nditional parameters each set-to-set function has introduced.\nFrom this, we observe that with both ConvNet and ResNet\nbackbones, FEAT has the smallest number of parameters\ncompared with all other approaches while achieving best\nperformances from various aspects (as results discussed\nabove), which highlights its high parameter efï¬ciency.\nAll above, we conclude that: 1) learning embedding\nadaptation with a set-to-set model is very effective in mod-\neling task-speciï¬c embeddings for few-shot learning 2)\nFEAT is the most parameter-efï¬cient function approximater\nthat achieves the best empirical performances, together with\nnice permutation invariant property and strong interpola-\ntion/extrapolation capability over the classiï¬cation way.\n5.2.2. Ablation Studies\nWe analyze\nFEAT and its ablated variants on the\nMiniImageNet dataset with ConvNet backbone.\nHow does the embedding adaptation looks like qualita-\ntively? We sample four few-shot learning tasks and learn\na principal component analysis (PCA) model (that projects\nembeddings into 2-D space) using the instance embeddings\nof the test data. We then apply this learned PCA projection\nto both the support setâ€™s pre-adapted and post-adapted em-\nbeddings. The results are shown in Figure 1 (the beginning\nof the paper). In three out of four examples, post-adaptation\nembeddings of FEAT improve over the pre-adaption embed-\ndings. Interestingly, we found that the embedding adap-\ntation step of FEAT has the tendency of pushing the sup-\nport embeddings apart from the clutter, such that they can\nbetter ï¬t the test data of its categories. In the negative ex-\nample where post-adaptation degenerates the performances,\nwe observe that the embedding adaptation step has pushed\ntwo support embeddings â€œGolden Retrieverâ€ and â€œLionâ€ too\nclose to each other. It has qualitatively shown that the adap-\ntation is crucial to obtain superior performances and helps\nto contrast against task-agnostic embeddings.\n5.3. Extended Few-Shot Learning Tasks\nIn this section, we evaluate FEAT on 3 different few-shot\nlearning tasks. Speciï¬cally, cross-domain FSL, transductive\nFSL [30, 38], and generalized FSL [7]. We overview the\nsetups brieï¬‚y and please refer to SM for details.\nFS Domain Generalization assumes that examples in UN-\nSEEN support and test set can come from the different do-\nmains, e.g., sampled from different distributions [9, 19].\nThe example of this task can be found in Figure 4. It re-\nquires a model to recognize the intrinsic property than tex-\nture of objects, and is de facto analogical recognition.\nTransductive FSL. The key difference between standard\nand transductive FSL is whether test instances arrive one\nat a time or all simultaneously. The latter setup allows the\nstructure of unlabeled test instances to be utilized. There-\nfore, the prediction would depend on both the training (sup-\nport) instances and all the available test instances in the tar-\nget task from UNSEEN categories.\nGeneralized FSL. Prior works assumed the test instances\ncoming from unseen classes only. Different from them, the\ngeneralized FSL setting considers test instances from both\nSEEN and UNSEEN classes [37]. In other words, during the\nmodel evaluation, while support instances all come from U,\nthe test instances come from S âˆªU, and the classiï¬er is\nrequired to predict on both SEEN and UNSEEN categories. .\n5.3.1. Few-Shot Domain Generalization\nWe show that FEAT learns to adapt the intrinsic structure\nof tasks, and generalizes across domains, i.e., predicting\ntest instances even when the visual appearance is changed.\nSetups. We train the FSL model in the standard domain and\nevaluate with cross-domain tasks, where the N-categories\nare aligned but domains are different. In detail, a model is\n7\n",
    "C â†’C\nC â†’R\nSupervised\n34.38Â±0.16\n29.49Â±0.16\nProtoNet\n35.51Â±0.16\n29.47Â±0.16\nFEAT\n36.83Â±0.17\n30.89Â±0.17\n1-Shot\n5-Shot\nTPN [30]\n55.51\n69.86\nTEAM [34]\n56.57\n72.04\nFEAT\n57.04 Â± 0.20\n72.89 Â± 0.16\nSEEN\nUNSEEN\nCOMBINED\nRandom\n1.56 Â±0.00 20.00Â±0.00\n1.45Â±0.00\nProtoNet 41.73Â±0.03 48.64Â±0.20\n35.69Â±0.03\nFEAT\n43.94Â±0.03 49.72Â±0.20\n40.50Â±0.03\n(a) Few-shot domain generalization\n(b) Transductive few-shot learning\n(c) Generalized few-shot learning\nTable 4: We evaluate our model on three additional few-shot learning tasks: (a) Few-shot domain generalization, (b) Transductive few-shot\nlearning, and (c) Generalized few-shot learning. We observe that FEAT consistently outperform all previous methods or baselines.\nDrill\nBed\nTV\nFlower\nScrewdriver\nğ’Ÿğ’Ÿğ­ğ­ğ­ğ­ğ­ğ­ğ­ğ­ğ­ğ­from \nâ€œClipartâ€\nğ’Ÿğ’Ÿğ­ğ­ğ­ğ­ğ­ğ­ğ­ğ­from \nâ€œReal Worldâ€\nClassify\nTest Set\nTrain Set\nTrain Set\nTest Set\nğ’Ÿğ’Ÿğ­ğ­ğ­ğ­ğ­ğ­ğ­ğ­ğ­ğ­from \nâ€œClipartâ€\nğ’Ÿğ’Ÿğ­ğ­ğ­ğ­ğ­ğ­ğ­ğ­from \nâ€œReal Worldâ€\nBed\nCurtains\nRefrigerator\nSneakers\nDrill\nClassify\nFigure 4: Qualitative results of few-shot domain-generalization\nfor FEAT. Correctly classiï¬ed examples are shown in red boxes\nand incorrectly ones are shown in blue boxes. We visualize one\ntask that FEAT succeeds (top) and one that fails (bottom).\ntrained on tasks from the â€œClipartâ€ domain of Ofï¬ceHome\ndataset [48], then the model is required to generalize to both\nâ€œClipart (C)â€ andâ€œReal World (R)â€ test instances. In other\nwords, we need to classify complex real images by seeing\nonly a few sketches (Figure 4 gives an overview of data).\nResults. Table 4 (a) gives the quantitative results and Fig-\nure 4 qualitatively examines it. Here, the â€œsupervisedâ€ de-\nnotes a model trained with the standard classiï¬cation strat-\negy and then its penultimate layerâ€™s output feature is used\nas the nearest neighbor classiï¬er. We observe that ProtoNet\ncan outperform this baseline on tasks when evaluating in-\nstances from â€œClipartâ€ but not ones from â€œreal worldâ€.\nHowever, FEAT improves over â€œreal worldâ€ few-shot classi-\nï¬cation even only seeing the support data from â€œClipartâ€.\n5.3.2. Transductive Few-Shot Learning\nWe show that without additional efforts in modeling,\nFEAT outperforms existing methods in transductive FSL.\nSetups. We further study this semi-supervised learning set-\nting to see how well FEAT can incorporate test instances into\njoint embedding adaptation. Speciï¬cally, we use the unla-\nbeled test instances to augment the key and value sets of\nTransformer (refer to SM for details), so that the embedding\nadaptation takes relationship of all test instances into con-\nsideration. We evaluate this setting on the transductive pro-\ntocol of MiniImageNet [38]. With the adapted embedding,\nFEAT makes predictions based on Semi-ProtoNet [38].\nResults.\nWe compare with two previous approaches,\nTPN [30] and TEAM [34]. The results are shown in Ta-\nble 4 (b). We observe that FEAT improves its standard FSL\nperformance (refer to Table 1) and also outperforms previ-\nous semi-supervised approaches by a margin.\n5.3.3. Generalized Few-Shot Learning\nWe show that FEAT performs well on generalized few-\nshot classiï¬cation of both SEEN and UNSEEN classes.\nSetups. In this scenario, we evaluate not only on classi-\nfying test instances from a N-way M-shot task from UN-\nSEEN set U, but also on all available SEEN classes from S.\nTo do so, we hold out 150 instances from each of the 64\nseen classes in MiniImageNet for validation and evaluation.\nNext, given a 1-shot 5-way training set Dtrain, we consider\nthree evaluation protocols based on different class sets [7]:\nUNSEEN measures the mean accuracy on test instances only\nfrom U (5-Way few-shot classiï¬cation); SEEN measures the\nmean accuracy on test instances only from S (64-Way clas-\nsiï¬cation); COMBINED measures the mean accuracy on test\ninstances from S âˆªU (69-Way mixed classiï¬cation).\nResults.\nThe results can be found in Table 4 (c).\nWe\nobserve that again FEAT outperforms baseline ProtoNet.\nTo calibrate the prediction score on SEEN and UNSEEN\nclasses [7, 52], we select a constant seen/unseen class prob-\nability over the validation set, and subtract this calibration\nfactor from seen classesâ€™ prediction score. Then we take the\nprediction with maximum score value after calibration.\n6. Discussion\nA common embedding space fails to tailor discriminative\nvisual knowledge for a target task especially when there are\na few labeled training data. We propose to do embedding\nadaptation with a set-to-set function and instantiate it with\ntransformer (FEAT), which customizes task-speciï¬c embed-\nding spaces via a self-attention architecture. The adapted\nembedding space leverages the relationship between target\n8\n",
    "task training instances, which leads to discriminative in-\nstance representations.\nFEAT achieves the state-of-the-art\nperformance on benchmarks, and its superiority can gener-\nalize to tasks like cross-domain, transductive, and general-\nized few-shot classiï¬cations.\nAcknowledgments.This work is partially supported by The\nNational Key R&D Program of China (2018YFB1004300),\nDARPA# FA8750-18-2-0117, NSF IIS-1065243, 1451412,\n1513966/ 1632803/1833137, 1208500, CCF-1139148, a\nGoogle Research Award, an Alfred P. Sloan Research Fel-\nlowship, ARO# W911NF-12-1-0241 and W911NF-15-1-\n0484, China Scholarship Council (CSC), NSFC (61773198,\n61773198, 61632004), and NSFC-NRF joint research\nproject 61861146001.\nReferences\n[1] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label-\nembedding for attribute-based classiï¬cation. In CVPR, pages\n819â€“826, 2013. 2\n[2] M. Andrychowicz, M. Denil, S. G. Colmenarejo, M. W.\nHoffman, D. Pfau, T. Schaul, and N. de Freitas. Learning\nto learn by gradient descent by gradient descent. In NIPS,\npages 3981â€“3989. 2016. 2\n[3] A. Antoniou, H. Edwards, and A. J. Storkey. How to train\nyour MAML. In ICLR, 2019. 2\n[4] L. J. Ba, R. Kiros, and G. E. Hinton. Layer normalization.\nCoRR, abs/1607.06450, 2016. 13\n[5] S. Changpinyo, W.-L. Chao, B. Gong, and F. Sha.\nSyn-\nthesized classiï¬ers for zero-shot learning. In CVPR, pages\n5327â€“5336, 2016. 2\n[6] S. Changpinyo, W.-L. Chao, and F. Sha. Predicting visual\nexemplars of unseen classes for zero-shot learning. In ICCV,\npages 3496â€“3505, 2017. 2\n[7] W.-L. Chao, S. Changpinyo, B. Gong, and F. Sha. An empir-\nical study and analysis of generalized zero-shot learning for\nobject recognition in the wild. In ECCV, pages 52â€“68, 2016.\n7, 8\n[8] W.-Y. Chen, Y.-C. Liu, Z. Kira, Y.-C. F. Wang, and J.-B.\nHuang. A closer look at few-shot classiï¬cation. In ICLR,\n2019. 5\n[9] N. Dong and E. P. Xing. Domain adaption in one-shot learn-\ning. In ECML PKDD, pages 573â€“588, 2018. 7\n[10] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-\nlearning for fast adaptation of deep networks.\nIn ICML,\npages 1126â€“1135, 2017. 1, 2, 3, 5, 6, 14, 15\n[11] G. Ghiasi, T.-Y. Lin, and Q. V. Le. Dropblock: A regulariza-\ntion method for convolutional networks. In NeurIPS, pages\n10750â€“10760. 2018. 13\n[12] S. Gidaris and N. Komodakis.\nDynamic few-shot visual\nlearning without forgetting.\nIn CVPR, pages 4367â€“4375,\n2018. 18\n[13] L.-Y. Gui, Y.-X. Wang, D. Ramanan, and J. M. F. Moura.\nFew-shot human motion prediction via meta-learning.\nIn\nECCV, pages 441â€“459, 2018. 2\n[14] B. Hariharan and R. B. Girshick. Low-shot visual recogni-\ntion by shrinking and hallucinating features. In ICCV, pages\n3037â€“3046, 2017. 18\n[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In CVPR, pages 770â€“778, 2016. 5,\n18\n[16] S. Hochreiter and J. Schmidhuber. Long short-term memory.\nNeural Computation, 9(8):1735â€“1780, 1997. 2, 4, 5, 11\n[17] K. Hsu, S. Levine, and C. Finn. Unsupervised learning via\nmeta-learning. In ICLR, 2019. 2\n[18] S. Ioffe and C. Szegedy. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift. In\nICML, pages 448â€“456, 2015. 13\n[19] B. Kang and J. Feng. Transferable meta learning across do-\nmains. In UAI, pages 177â€“187, 2018. 7\n[20] D. P. Kingma and J. Ba. Adam: A method for stochastic\noptimization. In ICLR, 2015. 5, 14\n[21] T. N. Kipf and M. Welling. Semi-supervised classiï¬cation\nwith graph convolutional networks. In ICLR, 2017. 2, 4, 5,\n12\n[22] G. Koch, R. Zemel, and R. Salakhutdinov.\nSiamese neu-\nral networks for one-shot image recognition. In ICML Deep\nLearning Workshop, volume 2, 2015. 2\n[23] B. M. Lake, R. Salakhutdinov, J. Gross, and J. B. Tenen-\nbaum.\nOne shot learning of simple visual concepts.\nIn\nCogSci, 2011. 1\n[24] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-\nlevel concept learning through probabilistic program induc-\ntion. Science, 350(6266):1332â€“1338, 2015. 1\n[25] K. Lee, S. Maji, A. Ravichandran, and S. Soatto.\nMeta-\nlearning with differentiable convex optimization. In CVPR,\npages 10657â€“10665, 2019. 2, 5, 6, 13, 15\n[26] Y. Lee and S. Choi.\nGradient-based meta-learning with\nlearned layerwise metric and subspace.\nIn ICML, pages\n2933â€“2942, 2018. 2\n[27] F.-F. Li, R. Fergus, and P. Perona. One-shot learning of ob-\nject categories. TPAMI, 28(4):594â€“611, 2006. 1\n[28] H. Li, D. Eigen, S. Dodge, M. Zeiler, and X. Wang. Find-\ning task-relevant features for few-shot learning by category\ntraversal. In CVPR, pages 1â€“10, 2019. 2, 6\n[29] Z. Lin, M. Feng, C. N. dos Santos, M. Yu, B. Xiang, B. Zhou,\nand Y. Bengio. A structured self-attentive sentence embed-\nding. In ICLR, 2017. 2, 4\n[30] Y. Liu, J. Lee, M. Park, S. Kim, E. Yang, S. J. Hwang, and\nY. Yang. Learning to propagate labels: Transductive propa-\ngation network for few-shot learning. In ICLR, 2019. 7, 8,\n17, 18\n[31] L. Metz, N. Maheswaranathan, B. Cheung, and J. Sohl-\nDickstein.\nLearning unsupervised learning rules.\nCoRR,\nabs/1804.00222, 2018. 2\n[32] A. Nichol, J. Achiam, and J. Schulman. On ï¬rst-order meta-\nlearning algorithms. CoRR, abs/1803.02999, 2018. 2\n[33] B. N. Oreshkin, P. R. LÂ´opez, and A. Lacoste. TADAM: task\ndependent adaptive metric for improved few-shot learning.\nIn NeurIPS, pages 719â€“729. 2018. 5, 6, 11, 15\n[34] L. Qiao, Y. Shi, J. Li, Y. Wang, T. Huang, and Y. Tian. Trans-\nductive episodic-wise adaptive metric for few-shot learning.\nIn ICCV, pages 3603â€“3612, 2019. 8, 17, 18\n[35] S. Qiao, C. Liu, W. Shen, and A. L. Yuille. Few-shot image\nrecognition by predicting parameters from activations.\nIn\nCVPR, pages 7229â€“7238, 2018. 2, 5, 6, 13, 14, 15\n[36] S. Ravi and H. Larochelle. Optimization as a model for few-\nshot learning. In ICLR, 2017. 2, 5\n9\n",
    "[37] M. Ren, R. Liao, E. Fetaya, and R. S. Zemel. Incremen-\ntal few-shot learning with attention attractor networks. In\nNeurIPS, pages 5276â€“5286, 2019. 7\n[38] M. Ren, E. Triantaï¬llou, S. Ravi, J. Snell, K. Swersky, J. B.\nTenenbaum, H. Larochelle, and R. S. Zemel. Meta-learning\nfor semi-supervised few-shot classiï¬cation. In ICLR, 2018.\n5, 6, 7, 8, 14, 17, 18\n[39] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein,\nA. C. Berg, and F.-F. Li. Imagenet large scale visual recog-\nnition challenge. IJCV, 115(3):211â€“252, 2015. 5, 18\n[40] A. A. Rusu, D. Rao, J. Sygnowski, O. Vinyals, R. Pascanu,\nS. Osindero, and R. Hadsell. Meta-learning with latent em-\nbedding optimization. In ICLR, 2019. 1, 2, 5, 13, 14, 15\n[41] V. G. Satorras and J. B. Estrach.\nFew-shot learning with\ngraph neural networks. In ICLR, 2018. 2, 4, 12\n[42] T. R. Scott, K. Ridgeway, and M. C. Mozer. Adapted deep\nembeddings: A synthesis of methods for k-shot inductive\ntransfer learning. In NeurIPS, pages 76â€“85. 2018. 2\n[43] J. Snell, K. Swersky, and R. S. Zemel. Prototypical networks\nfor few-shot learning. In NIPS, pages 4080â€“4090. 2017. 1,\n2, 3, 4, 5, 6, 11, 13, 15, 18\n[44] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov. Dropout: a simple way to prevent neural\nnetworks from overï¬tting. JMLR, 15(1):1929â€“1958, 2014.\n13\n[45] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. S. Torr, and\nT. M. Hospedales. Learning to compare: Relation network\nfor few-shot learning. In CVPR, pages 1199â€“1208, 2018. 2,\n6, 15\n[46] E. Triantaï¬llou, R. S. Zemel, and R. Urtasun.\nFew-shot\nlearning through an information retrieval lens.\nIn NIPS,\npages 2252â€“2262. 2017. 1, 2, 5, 13, 14\n[47] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all\nyou need. In NIPS, pages 6000â€“6010. 2017. 2, 4, 5, 12, 13,\n14, 16\n[48] H. Venkateswara, J. Eusebio, S. Chakraborty, and S. Pan-\nchanathan. Deep hashing network for unsupervised domain\nadaptation. In CVPR, pages 5385â€“5394, 2017. 5, 8, 14, 17\n[49] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and\nD. Wierstra. Matching networks for one shot learning. In\nNIPS, pages 3630â€“3638. 2016. 1, 2, 3, 4, 5, 6, 11, 12, 13,\n14, 15\n[50] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.\nThe Caltech-UCSD Birds-200-2011 Dataset. Technical Re-\nport CNS-TR-2011-001, California Institute of Technology,\n2011. 6, 14\n[51] Y. Wang, W.-L. Chao, K. Q. Weinberger, and L. van der\nMaaten.\nSimpleshot: Revisiting nearest-neighbor classiï¬-\ncation for few-shot learning. CoRR, abs/1911.04623, 2019.\n6, 14, 15\n[52] Y.-X. Wang, R. B. Girshick, M. Hebert, and B. Hariharan.\nLow-shot learning from imaginary data.\nIn CVPR, pages\n7278â€“7286, 2018. 8, 18\n[53] X.-S. Wei, P. Wang, L. Liu, C. Shen, and J. Wu.\nPiece-\nwise classiï¬er mappings: Learning ï¬ne-grained learners for\nnovel categories with few examples. TIP, 28(12):6116â€“6125,\n2019. 2\n[54] H.-J. Ye, H. Hu, D.-C. Zhan, and F. Sha.\nLearn-\ning embedding adaptation for few-shot learning.\nCoRR,\nabs/1812.03664, 2018. 13\n[55] S. Zagoruyko and N. Komodakis. Wide residual networks.\nIn BMVC, 2016. 5, 13\n[56] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. PÂ´oczos, R. R.\nSalakhutdinov, and A. J. Smola. Deep sets. In NIPS, pages\n3394â€“3404. 2017. 2, 4, 5, 6, 12\n10\n",
    "Supplementary Material\nA. Details of Baseline Methods\nIn this section, we describe two important embedding\nlearning baselines i.e., Matching Network (MatchNet) [49]\nand Prototypical Network (ProtoNet) [43], to implement the\nprediction function f(xtest; Dtrain) in the few-shot learn-\ning framework.\nMatchNet and ProtoNet.\nBoth MatchNet and ProtoNet\nstress the learning of the embedding function E from the\nsource task data DS with a meta-learning routine similar to\nAlg. 1 in the main text. We omit the super-script S since\nthe prediction strategies can apply to tasks from both SEEN\nand UNSEEN sets.\nGiven the training data Dtrain = {xi, yi}NM\ni=1 of an M-\nshot N-way classiï¬cation task, we can obtain the embed-\nding of each training instance based on the function E:1\nÏ†(xi) = E(xi), âˆ€xi âˆˆXtrain\n(8)\nTo classify a test instance xtest, we perform the nearest\nneighbor classiï¬cation , i.e.,\nË†ytest âˆexp\n\u0000Î³ Â· sim(Ï†xtest, Ï†xi)\n\u0001\nÂ· yi\n(9)\n=\nexp\n\u0000Î³ Â· sim(Ï†xtest , Ï†xi)\n\u0001\nP\nxiâ€²âˆˆXtrain exp\n\u0000Î³ Â· sim(Ï†xtest , Ï†xiâ€²)\n\u0001 Â· yi\n=\nX\n(xi,yi)âˆˆDtrain\nexp\n\u0000Î³ Â· sim(Ï†xtest , Ï†xi)\n\u0001\nP\nxiâ€²âˆˆXtrain exp\n\u0000Î³ Â· sim(Ï†xtest , Ï†xiâ€²)\n\u0001 Â· yi\nHere, MatchNet ï¬nds the most similar training instance to\nthe test one, and assigns the label of the nearest neigh-\nbor to the test instance. Note that sim represents the co-\nsine similarity, and Î³ > 0 is the scalar temperature value\nover the similarity score, which is found important em-\npirically [33]. During the experiments, we tune this tem-\nperature value carefully, ranging from the reciprocal of\n{0.1, 1, 16, 32, 64, 128}.2\nThe ProtoNet has two key differences compared with the\nMatchNet. First, when M > 1 in the target task, ProtoNet\ncomputes the mean of the same class embeddings as the\nclass center (prototype) in advance and classiï¬es a test in-\nstance by computing its similarity to the nearest class center\n(prototype). In addition, it uses the negative distance rather\n1In the following, we use Ï†(xi) and Ï†xi exchangeably to represent the\nembedding of an instance xi based on the mapping Ï†.\n2In experiments, we ï¬nd the temperature scale over logits inï¬‚uences\nthe model training a lot when we optimize based on pre-trained weights.\nthan the cosine similarity as the similarity metric:\ncn = 1\nM\nX\nyi=n\nÏ†(xi), âˆ€n = 1, . . . , N\n(10)\nË†ytest âˆexp\n\u0000Î³ Â· âˆ¥Ï†xtest âˆ’cnâˆ¥2\n2\n\u0001\nÂ· yn\n=\nN\nX\nn=1\nexp\n\u0000âˆ’Î³âˆ¥Ï†xtest âˆ’cnâˆ¥2\n2\n\u0001\nPN\nnâ€²=1 exp\n\u0000âˆ’Î³âˆ¥Ï†xtest âˆ’cnâ€²âˆ¥2\n2\n\u0001yn\n(11)\nSimilar to the aforementioned scalar temperature for Match-\nNet, in Eq. 11 we also consider the scale Î³. Here we abuse\nthe notation by using yi = n to enumerate the instances\nwith label n, and denote yn as the one-hot coding of the\nn-th class. Thus Eq. 11 outputs the probability to classify\nxtest to the N classes.\nIn the experiments, we ï¬nd ProtoNet incorporates bet-\nter with FEAT. When there is more than one shot in each\nclass, we average all instances per class in advance by\nEq. 10 before inputting them to the set-to-set transforma-\ntion. This pre-average manner makes more precise embed-\nding for each class and facilitates the â€œdownstreamâ€ em-\nbedding adaptation. We will validate this in the additional\nexperiments.\nB. Details of the Set-to-Set Functions\nIn this section, we provide details about four imple-\nmentations of the set-to-set embedding adaptation function\nT, i.e., the BILSTM, DEEPSETS, GCN, and the TRANS-\nFORMER. The last one is the key component in our Few-\nshot Embedding Adaptation with Transformer (FEAT) ap-\nproach. Then we will introduce the conï¬guration of the\nmulti-layer/multi-head transformer, and the setup of the\ntransformer for the transductive Few-Shot Learning (FSL).\nB.1. BiLSTM as the Set-to-Set Transformation\nBidirectional LSTM (BILSTM) [16, 49] is one of the\ncommon choice to instantiate the set-to-set transformation,\nwhere the addition between the input and the hidden layer\noutputs of each BILSTM cell leads to the adapted embed-\nding. In detail, we have\n{\nâƒ—\nÏ†(x), âƒ—Ï†(x)} = BILSTM({Ï†(x)}); âˆ€x âˆˆXtrain\n(12)\nWhere\nâƒ—\nÏ†(x) and âƒ—Ï†(x) are the hidden layer outputs of the\ntwo LSTM models for each instance embedding in the input\nset. Then we get the adapted embedding as\nÏˆ(x) = Ï†(x) +\nâƒ—\nÏ†(x) + âƒ—Ï†(x)\n(13)\nIt is notable that the output of the BILSTM suppose to de-\npend on the order of the input set. Vinyals et al. [49] pro-\npose to use the Fully Conditional Embedding to encode\nthe context of both the test instance and the support set\n11\n",
    "Classification \nScores\nEmbedding \nAdaptation\nCNN\nCNN\nCNN\nCNN\nSoft Nearest \nNeighbor\nSet-to-Set Function\n(a) Embedding Adaptation\n(b) Transformer as the Set-to-Set Function\n(c) DeepSets as Set-to-Set Function\nLayer Norm\nFC\nFC\nFC\nFC\nFC\nFC\nFC\nFC\nFC\nFC\nFC\nSUM\nCAT\nScaled Dot\nProduct\nTrain Instance\nTest Instance\nTask Agnostic\nEmbedding\nTask Specific \nEmbedding\nFigure 5: Illustration of two embedding adpatation methods considered in the paper. (a) shows the main ï¬‚ow of Few-Shot Embedding\nAdaptation, while (b) and (c) demonstrate the workï¬‚ow of Transformer and DeepSets respectively.\ninstances based on BILSTM and LSTM w/ Attention mod-\nule. Different from [49], we apply the set-to-set embedding\nadaptation only over the support set, which leads to a fully\ninductive learning setting.\nB.2. DeepSets as the Set-to-Set Transformation\nDeep sets [56] suggests a generic aggregation function\nover a set should be the transformed sum of all elements in\nthis set. Therefore, a very simple set-to-set transformation\nbaseline involves two components, an instance centric rep-\nresentation combined with a set context representation. For\nany instance x âˆˆXtrain, we deï¬ne its complementary set\nas xâˆ. Then we implement the set transformation by:\nÏˆ(x) = Ï†(x) + g([Ï†(x);\nX\nxiâ€²âˆˆxâˆ\nh(Ï†(xiâ€²))])\n(14)\nIn Eq. 14, g and h are transformations which map the em-\nbedding into another space and increase the representation\nability of the embedding. Two-layer multi-layer perception\n(MLP) with ReLU activation is used to implement these two\nmappings. For each instance, embeddings in its comple-\nmentary set are ï¬rst combined into a vector as the context,\nand then this vector is concatenated with the input embed-\nding to obtain the residual component of the adapted em-\nbedding. This conditioned embedding takes other instances\nin the set into consideration, and keeps the â€œset (permutation\ninvariant)â€ property. Finally, we determine the label with\nthe newly adapted embedding Ïˆ as Eq. 11. An illustration\nof the DeepSets notation in the embedding adaptation can\nbe found in Figure 5 (c). The summation operator in Eq. 14\ncould also be replaced as the maximum operator, and we\nï¬nd the maximum operator works better than summation\noperator in our experiments.\nB.3. GCN as the Set-to-Set Transformation\nGraph Convolutional Networks (GCN) [21, 41] propa-\ngate the relationship between instances in the set. We ï¬rst\nconstruct a degree matrix A âˆˆRNKÃ—NK to represent the\nsimilarity between instances in a set. If two instances xi and\nxj come from the same class, then we set the corresponding\nelement Aij in A to 1, otherwise we have Aij = 0. Based\non A, we build the â€œnormalizedâ€ adjacency matrix S for a\ngiven set with added self-loops S = Dâˆ’1\n2 (A + I)Dâˆ’1\n2 .\nI âˆˆRNKÃ—NK is the identity matrix, and D is the diagonal\nmatrix whose elements are equal to the sum of elements in\nthe corresponding row of A+I, i.e., Dii = P\nj Aij +1 and\nDij = 0 if i Ì¸= j. Let Î¦0 = {Ï†x ; âˆ€x âˆˆXtrain} be the\nconcatenation of all the instance embeddings in the training\nset Xtrain. We use the super-script to denote the generation\nof the instance embedding matrix. The relationship between\ninstances could be propagated based on S, i.e.,\nÎ¦t+1 = ReLU(SÎ¦tW) , t = 0, 1, . . . , T âˆ’1\n(15)\nW is a learned a projection matrix for feature transforma-\ntion. In GCN, the embedding in the set is transformed based\non Eq. 15 multiple times (we propagate the embedding set\ntwo times during the experiments), and the ï¬nal propagated\nembedding set Î¦T gives rise to the Ïˆx.\nB.4. Transformer as the Set-to-Set Transformation\nIn this section, we describe in details about our Few-Shot\nEmbedding Adaptation w/ Transformer (FEAT) approach,\nspeciï¬cally how to use the transformer architecture [47] to\nimplement the set-to-set function T, where self-attention\nmechanism facilitates the instance embedding adaptation\nwith consideration of the contextual embeddings.\nAs mentioned before, the transformer is a store of triplets\nin the form of (query, key, and value). Elements in the query\nset are the ones we want to do the transformation. The trans-\nformer ï¬rst matches a query point with each of the keys by\ncomputing the â€œqueryâ€ â€“ â€œkeyâ€ similarities. Then the prox-\nimity of the key to the query point is used to weight the\ncorresponding values of each key. The transformed input\nacts as a residual value which will be added to the input.\n12\n",
    "Basic Transformer.\nFollowing the deï¬nitions in [47], we\nuse Q, K, and V to denote the set of the query, keys, and\nvalues, respectively. All these sets are implemented by dif-\nferent combinations of task instances.\nTo increase the ï¬‚exibility of the transformer, three sets\nof linear projections (WQ âˆˆRdÃ—dâ€², WK âˆˆRdÃ—dâ€², and\nWV âˆˆRdÃ—dâ€²) are deï¬ned, one for each set.3 The points in\nsets are ï¬rst projected by the corresponding projections\nQ = W âŠ¤\nQ\n\u0002\nÏ†xq;\nâˆ€xq âˆˆQ\n\u0003\nâˆˆRdâ€²Ã—|Q|\nK = W âŠ¤\nK\n\u0002\nÏ†xk;\nâˆ€xk âˆˆK\n\u0003\nâˆˆRdâ€²Ã—|K|\nV = W âŠ¤\nV\n\u0002\nÏ†xv;\nâˆ€xv âˆˆV\n\u0003\nâˆˆRdâ€²Ã—|V|\n(16)\n|Q|, |K|, and |V| are the number of elements in the sets\nQ, K, and V respectively. Since there is a one-to-one corre-\nspondence between elements in K and V we have |K| = |V|.\nThe similarity between a query point xq âˆˆQ and the list\nof keys K is then computed as â€œattentionâ€:\nÎ±qk âˆexp\n \nÏ†âŠ¤\nxqWQ Â· K\nâˆš\nd\n!\n; âˆ€xk âˆˆK\n(17)\nÎ±q,: = softmax\n \nÏ†âŠ¤\nxqWQ Â· K\nâˆš\nd\n!\nâˆˆR|K|\n(18)\nThe k-th element Î±qk in the vector Î±q,: reveals the particu-\nlar proximity between xk and xq. The computed attention\nvalues are then used as weights for the ï¬nal embedding xq:\nËœÏˆxq =\nX\nk\nÎ±qkV:,k\n(19)\nÏˆxq = Ï„\n\u0000Ï†xq + W âŠ¤\nFC ËœÏˆxq\n\u0001\n(20)\nV:,k is the k-th column of V .\nWFC âˆˆRdâ€²Ã—d is the\nprojection weights of a fully connected layer.\nÏ„ com-\npletes a further transformation, which is implemented by\nthe dropout [44] and layer normalization [4]. The whole\nï¬‚ow of transformer in our FEAT approach can be found in\nFigure 5 (b). With the help of transformer, the embeddings\nof all training set instances are adapted (we denote this ap-\nproach as FEAT).\nMulti-Head Multi-Layer Transformer.\nFollowing [47],\nan extended version of the transformer can be built with\nmultiple parallel attention heads and stacked layers. As-\nsume there are totally H heads, the transformer concate-\nnates multiple attention-transformed embeddings, and then\nuses a linear mapping to project the embedding to the orig-\ninal embedding space (with the original dimensionality).\nBesides, we can take the transformer as a feature encoder\nof the input query instance. Therefore, it can be applied\n3For notation simplicity, we omit the bias in the linear projection here.\nover the input query multiple times (with different sets of\nparameters), which gives rise to the multi-layer transformer.\nWe discuss the empirical performances with respect to the\nchange number of heads and layers in Â§ D.\nB.5. Extension to transductive FSL\nFacilitated by the ï¬‚exible set-to-set transformer in\nEq. 20, our adaptation approach can naturally be extended\nto the transductive FSL setting.\nWhen classifying test instance xtest in the transdutive\nscenario, other test instances Xtest from the N categories\nwould also be available. Therefore, we enrich the trans-\nformerâ€™s query and key/value sets\nQ = K = V = Xtrain âˆªXtest\n(21)\nIn this manner, the embedding adaptation procedure would\nalso consider the structure among unlabeled test instances.\nWhen the number of shots K > 1, we average the embed-\nding of labeled instances in each class ï¬rst before combin-\ning them with the test set embeddings.\nC. Implementation Details\nBackbone architecture.\nWe consider three backbones, as\nsuggested in the literature, as the instance embedding func-\ntion E for the purpose of fair comparisons. We resize the\ninput image to 84 Ã— 84 Ã— 3 before using the backbones.\nâ€¢ ConvNet. The 4-layer convolution network [43, 46, 49]\ncontains 4 repeated blocks. In each block, there is a con-\nvolutional layer with 3 Ã— 3 kernel, a Batch Normalization\nlayer [18], a ReLU, and a Max pooling with size 2. We\nset the number of convolutional channels in each block as\n64. A bit different from the literature, we add a global\nmax pooling layer at last to reduce the dimension of the\nembedding. Based on the empirical observations, this will\nnot inï¬‚uence the results, but reduces the computation bur-\nden of later transformations a lot.\nâ€¢ ResNet. We use the 12-layer residual network in [25].4\nThe DropBlock [11] is used in this ResNet architecture\nto avoid over-ï¬tting. A bit different from the ResNet-12\nin [25], we apply a global average pooling after the ï¬nal\nlayer, which leads to 640 dimensional embeddings.5\nâ€¢ WRN. We also consider the Wide residual network [40,\n55].\nWe use the WRN-28-10 structure as in [35, 40],\nwhich sets the depth to 28 and width to 10. After a global\n4The source code of the ResNet is publicly available on https://\ngithub.com/kjunelee/MetaOptNet\n5We use the ResNet backbone with input image size 80 Ã— 80 Ã—\n3 from [35] in the old version of our paper [54], whose source\ncode of ResNet is publicly available on https://github.com/\njoe-siyuan-qiao/FewShot-CVPR.\nEmpirically we ï¬nd the\nResNet-12 [25] works better than our old ResNet architecture.\n13\n",
    "average pooling in the last layer of the backbone, we get\na 640 dimensional embedding for further prediction.\nDatasets.\nFour\ndatasets,\nMiniImageNet\n[49],\nTieredImageNet\n[38],\nCaltech-UCSD\nBirds\n(CUB)\n200-2011 [50], and Ofï¬ceHome [48] are investigated in\nthis paper. Each dataset is split into three parts based on\ndifferent non-overlapping sets of classes, for model training\n(a.k.a.\nmeta-training in the literature), model validation\n(a.k.a.\nmeta-val in the literature), and model evaluation\n(a.k.a.\nmeta-test in the literature).\nThe CUB dataset is\ninitially designed for ï¬ne-grained classiï¬cation. It contains\nin total 11,788 images of birds over 200 species. On CUB,\nwe randomly sampled 100 species as SEEN classes, another\ntwo 50 species are used as two UNSEEN sets for model\nvalidation and evaluation [46]. For all images in the CUB\ndataset, we use the provided bounding box to crop the\nimages as a pre-processing [46].\nBefore input into the\nbackbone network, all images in the dataset are resized\nbased on the requirement of the network.\nPre-training strategy.\nAs mentioned before, we apply an\nadditional pre-training strategy as suggested in [35, 40].\nThe backbone network, appended with a softmax layer, is\ntrained to classify all classes in the SEEN class split (e.g., 64\nclasses in the MiniImageNet) with the cross-entropy loss.\nIn this stage, we apply image augmentations like random\ncrop, color jittering, and random ï¬‚ip to increase the gen-\neralization ability of the model. After each epoch, we val-\nidate the performance of the pre-trained weights based on\nits few-shot classiï¬cation performance on the model vali-\ndation split. Speciï¬cally, we randomly sample 200 1-shot\nN-way few-shot learning tasks (N equals the number of\nclasses in the validation split, e.g., 16 in the MiniImageNet),\nwhich contains 1 instance per class in the support set and\n15 instances per class for evaluation. Based on the penulti-\nmate layer instance embeddings of the pre-trained weights,\nwe utilize the nearest neighbor classiï¬ers over the few-shot\ntasks and evaluate the quality of the backbone. We select\nthe pre-trained weights with the best few-shot classiï¬ca-\ntion accuracy on the validation set. The pre-trained weights\nare used to initialize the embedding backbone E, and the\nweights of the whole model are then optimized together dur-\ning the model training.\nTransformer Hyper-parameters.\nWe follow the archi-\ntecture as presented in [47] to build our FEAT model.\nThe hidden dimension dâ€² for the linear transformation in\nour FEAT model is set to 64 for ConvNet and 640 for\nResNet/WRN. The dropout rate in transformer is set as 0.5.\nWe empirically observed that the shallow transformer (with\none set of projection and one stacked layer) gives the best\noverall performance (also studied in Â§ D.2).\nOptimization.\nFollowing the literature, different optimiz-\ners are used for the backbones during the model training.\nFor the ConvNet backbone, stochastic gradient descent with\nAdam [20] optimizer is employed, with the initial learning\nrate set to be 0.002. For the ResNet and WRN backbones,\nvanilla stochastic gradient descent with Nesterov accelera-\ntion is used with an initial rate of 0.001. We ï¬x the weight\ndecay in SGD as 5e-4 and momentum as 0.9. The sched-\nule of the optimizers is tuned over the validation part of\nthe dataset. As the backbone network is initialized with the\npre-trained weights, we scale the learning rate for those pa-\nrameters by 0.1.\nD. Additional Experimental Results\nIn this section, we will show more experimental results\nover the MiniImageNet/CUB dataset, the ablation studies,\nand the extended few-shot learning.\nD.1. Main Results\nThe full results of all methods on the MiniImageNet can\nbe found in Table 5. The results of MAML [10] optimized\nover the pre-trained embedding network are also included.\nWe re-implement the ConvNet backbone of MAML and cite\nthe MAML results over the ResNet backbone from [40]. It\nis also noteworthy that the FEAT gets the best performance\namong all popular methods and baselines.\nWe also investigate the Wide ResNet (WRN) back-\nbone over MiniImageNet, which is also the popular one\nused in [35, 40].\nSimpleShot [51] is a recent proposed\nembedding-based few-shot learning approach that takes full\nadvantage of the pre-trained embeddings. We cite the re-\nsults of PFA [35], LEO [40], and SimpleShot [51] from\ntheir papers. The results can be found in Table 6. We re-\nimplement ProtoNet and our FEAT approach with WRN.\nIt is notable that in this case, our FEAT achieves much\nhigher promising results than the current state-of-the-art\napproaches. Table 7 shows the classiï¬cation results with\nWRN on the TieredImageNet data set, where our FEAT still\nkeeps its superiority when dealing with 1-shot tasks.\nTable 8 shows the 5-way 1-shot and 5-shot classiï¬cation\nresults on the CUB dataset based on the ConvNet back-\nbone. The results on CUB are consistent with the trend\non the MiniImageNet dataset. Embedding adaptation in-\ndeed assists the embedding encoder for the few-shot clas-\nsiï¬cation tasks.\nFacilitated by the set function property,\nthe DEEPSETS works better than the BILSTM counterpart.\nAmong all the results, the transformer based FEAT gets the\ntop tier results.\nD.2. Ablation Studies\nIn this section, we perform further analyses for our pro-\nposed FEAT and its ablated variants classifying in the Pro-\n14\n",
    "Table 5: Few-shot classiï¬cation accuracyÂ± 95% conï¬dence interval on MiniImageNet with ConvNet and ResNet backbones. Our imple-\nmentation methods are measured over 10,000 test trials.\nSetups â†’\n1-Shot 5-Way\n5-Shot 5-Way\nBackbone Network â†’\nConvNet\nResNet\nConvNet\nResNet\nMatchNet [49]\n43.40Â± 0.78\n-\n51.09Â± 0.71\n-\nMAML [10]\n48.70Â± 1.84\n-\n63.11Â± 0.92\n-\nProtoNet [43]\n49.42Â± 0.78\n-\n68.20Â± 0.66\n-\nRelationNet [45]\n51.38Â± 0.82\n-\n67.07Â± 0.69\n-\nPFA [35]\n54.53Â± 0.40\n-\n67.87Â± 0.20\n-\nTADAM [33]\n-\n58.50Â± 0.30\n-\n76.70Â± 0.30\nMetaOptNet [25]\n-\n62.64Â± 0.61\n-\n78.63Â± 0.46\nBaselines\nMAML\n49.24Â± 0.21\n58.05Â± 0.10\n67.92Â± 0.17\n72.41Â± 0.20\nMatchNet\n52.87Â± 0.20\n65.64Â± 0.20\n67.49Â± 0.17\n78.72Â± 0.15\nProtoNet\n52.61Â± 0.20\n62.39Â± 0.21\n71.33Â± 0.16\n80.53Â± 0.14\nEmbedding Adaptation\nBILSTM\n52.13Â± 0.20\n63.90Â± 0.21\n69.15Â± 0.16\n80.63Â± 0.14\nDEEPSETS\n54.41Â± 0.20\n64.14Â± 0.22\n70.96Â± 0.16\n80.93Â± 0.14\nGCN\n53.25Â± 0.20\n64.50Â± 0.20\n70.59Â± 0.16\n81.65Â± 0.14\nOurs: FEAT\n55.15Â± 0.20\n66.78Â± 0.20\n71.61Â± 0.16\n82.05Â± 0.14\nTable 6:\nFew-shot classiï¬cation performance with Wide\nResNet (WRN)-28-10 backbone on MiniImageNet dataset (mean\naccuracyÂ±95% conï¬dence interval). Our implementation meth-\nods are measured over 10,000 test trials.\nSetups â†’\n1-Shot 5-Way\n5-Shot 5-Way\nPFA [35]\n59.60Â± 0.41\n73.74Â± 0.19\nLEO [40]\n61.76Â± 0.08\n77.59Â± 0.12\nSimpleShot [51]\n63.50Â± 0.20\n80.33Â± 0.14\nProtoNet (Ours)\n62.60Â± 0.20\n79.97Â± 0.14\nOurs: FEAT\n65.10 Â± 0.20\n81.11 Â± 0.14\nTable 7: Few-shot classiï¬cation performance with Wide ResNet\n(WRN)-28-10 backbone on TieredImageNet dataset (mean\naccuracyÂ±95% conï¬dence interval). Our implementation meth-\nods are measured over 10,000 test trials.\nSetups â†’\n1-Shot 5-Way\n5-Shot 5-Way\nLEO [40]\n66.33Â± 0.05\n81.44Â± 0.09\nSimpleShot [51]\n69.75Â± 0.20\n85.31Â± 0.15\nOurs: FEAT\n70.41 Â± 0.23\n84.38 Â± 0.16\ntoNet manner, on the MiniImageNet dataset, using the Con-\nvNet as the backbone network.\nDo the adapted embeddings improve the pre-adapted\nembeddings?\nWe report few-shot classiï¬cation results by\nTable 8: Few-shot classiï¬cation performance with ConvNet back-\nbone on CUB dataset (mean accuracyÂ±95% conï¬dence interval).\nOur implementation methods are measured over 10,000 test trials.\nSetups â†’\n1-Shot 5-Way\n5-Shot 5-Way\nMatchNet [49]\n61.16 Â± 0.89\n72.86 Â± 0.70\nMAML [10]\n55.92 Â± 0.95\n72.09 Â± 0.76\nProtoNet [43]\n51.31 Â± 0.91\n70.77 Â± 0.69\nRelationNet [45]\n62.45 Â± 0.98\n76.11 Â± 0.69\nInstance Embedding\nMatchNet\n67.73 Â± 0.23\n79.00 Â± 0.16\nProtoNet\n63.72 Â± 0.22\n81.50 Â± 0.15\nEmbedding Adaptation\nBILSTM\n62.05 Â± 0.23\n73.51 Â± 0.19\nDEEPSETS\n67.22 Â± 0.23\n79.65 Â± 0.16\nGCN\n67.83 Â± 0.23\n80.26 Â± 0.15\nOurs: FEAT\n68.87 Â± 0.22\n82.90 Â± 0.15\nTable 9: Ablation studies on whether the embedding adaptation\nimproves the discerning quality of the embeddings. After embed-\nding adaptation, FEAT improves w.r.t. the before-adaptation em-\nbeddings a lot for Few-shot classiï¬cation.\n1-Shot 5-Way\n5-Shot 5-Way\nPre-Adapt\n51.60Â± 0.20\n70.40Â± 0.16\nPost-Adapt\n55.15Â± 0.20\n71.61Â± 0.16\n15\n",
    "5\n10\n15\n20\nNumber of categories per task\n30\n35\n40\n45\n50\n55\n60\n65\n70\n75\nMean accuracy (in %)\n70.7\n55.9\n47.5\n41.9\n71.3\n56.5\n48.2\n42.4\n71.5\n57.0\n48.8\n43.2\nMethods\nBILSTM\nDeepSets\nFEAT\n(a) Task Interpolation\n5\n10\n15\n20\nNumber of categories per task\n30\n35\n40\n45\n50\n55\n60\n65\n70\n75\n69.2\n52.9\n43.6\n37.5\n71.0\n55.9\n47.4\n41.8\n71.2\n56.7\n48.2\n42.6\nMethods\nBILSTM\nDeepSets\nFEAT\n(b) Task Extrapolation\nFigure 6: Interpolation and Extrapolation of few-shot tasks. We\ntrain different embedding adaptation models on 5-shot 20-way or\n5-way classiï¬cation tasks and evaluate models on unseen tasks\nwith different number of classes (N={5, 10, 15, 20}). It veri-\nï¬es both the interpolation and extrapolation ability of FEAT on a\nvarying number of ways in few-shot classiï¬cation.\nTable 10: Ablation studies on the position to average the same-\nclass embeddings when there are multiple shots per class in FEAT\n(tested on the 5-Way tasks with different numbers of shots). â€œPre-\nAvgâ€ and â€œPost-Avgâ€ means we get the embedding center for each\nclass before or after the set-to-set transformation, respectively.\nSetups â†’\nPre-Avg\nPost-Avg\n5\n71.61Â± 0.16\n70.70Â± 0.16\n15\n77.76Â± 0.14\n76.58Â± 0.14\n30\n79.66Â± 0.13\n78.77Â± 0.13\nTable 11: Ablation studies on the number of heads in the Trans-\nformer of FEAT (with number of layers ï¬xes to one).\nSetups â†’\n1-Shot 5-Way\n5-Shot 5-Way\n1\n55.15Â± 0.20\n71.57Â± 0.16\n2\n54.91Â± 0.20\n71.44Â± 0.16\n4\n55.05Â± 0.20\n71.63Â± 0.16\n8\n55.22Â± 0.20\n71.39Â± 0.16\nTable 12: Ablation studies on the number of layers in the Trans-\nformer of FEAT (with number of heads ï¬xes to one).\nSetups â†’\n1-Shot 5-Way\n5-Shot 5-Way\n1\n55.15Â± 0.20\n71.57Â± 0.16\n2\n55.42Â± 0.20\n71.44Â± 0.16\n3\n54.96Â± 0.20\n71.63Â± 0.16\nusing the pre-adapted embeddings of support data (i.e., the\nembedding before adaptation), against those using adapted\nembeddings, for constructing classiï¬ers. Table 9 shows that\ntask-speciï¬c embeddings after adaptation improves over\ntask-agnostic embeddings in few-shot classiï¬cations.\nCan FEAT possesses the characteristic of the set func-\ntion?\nWe test three set-to-set transformation implementa-\ntions, namely the BILSTM, the DEEPSETS, and the Trans-\nformer (FEAT), w.r.t. two important properties of the set\nfunction, i.e., task interpolation and task extrapolation. In\nparticular, the few-shot learning model is ï¬rst trained with\n5-shot 20-way tasks. Then the learned model is required\nto evaluate different 5-shot tasks with N = {5, 10, 15, 20}\n(Extrapolation). Similarly, for interpolation, the model is\ntrained with 5-shot 20-way tasks in advance and then eval-\nuated on the previous multi-way tasks. The classiï¬cation\nchange results can be found in Figure 6 (a) and (b). BILSTM\ncannot deal with the size change of the set, especially in the\ntask extrapolation. In both cases, FEAT still gets improve-\nments in all conï¬gurations of N.\nWhen to average the same-class embeddings?\nWhen\nthere is more than one instance per class, i.e. M > 1, we av-\nerage the instances in the same class and use the class center\nto make predictions as in Eq. 10. There are two positions\nto construct the prototypes in FEAT â€” before the set-to-set\ntransformation (Pre-Avg) and after the set-to-set transfor-\nmation (Post-Avg). In Pre-Avg, we adapt the embeddings\nof the centers, and a test instance is predicted based on\nits distance to the nearest adapted center; while in Post-\nAvg, the instance embeddings are adapted by the set-to-set\nfunction ï¬rst, and the class centers are computed based on\nthe adapted instance embeddings. We investigate the two\nchoices in Table 10, where we ï¬x the number of ways to\n5 (N = 5) and change the number of shots (M) among\n{5, 15, 30}. The results demonstrate the Pre-Avg version\nperforms better than the Post-Avg in all cases, which shows\na more precise input of the set-to-set function by averaging\nthe instances in the same class leads to better results. So we\nuse the Pre-Avg strategy as a default option in our experi-\nments.\nWill deeper and multi-head transformer help?\nIn our\ncurrent implementation of the set-to-set transformation\nfunction, we make use of a shallow and simple transformer,\ni.e., one layer and one head (set of projection). From [47],\nthe transformer can be equipped with complex components\nusing multiple heads and deeper stacked layers. We evalu-\nate this augmented structure, with the number of attention\nheads increases to 2, 4, 8, as well as with the number of\nlayers increases to 2 and 3. As in Table 11 and Table 12,\nwe empirically observe that more complicated structures do\nnot result in improved performance. We ï¬nd that with more\nlayers of transformer stacked, the difï¬culty of optimization\nincreases and it becomes harder to train models until their\nconvergence. Whilst for models with more heads, the mod-\nels seem to over-ï¬t heavily on the training data, even with\nthe usage of auxiliary loss term (like the contrastive loss in\n16\n",
    "Table 13: Ablation studies on effects of the contrastive learning\nof the set-to-set function on FEAT.\nSetups â†’\n1-Shot 5-Way\n5-Shot 5-Way\nÎ» = 10\n53.92 Â± 0.20\n70.41 Â± 0.16\nÎ» = 1\n54.84 Â± 0.20\n71.00 Â± 0.16\nÎ» = 0.1\n55.15 Â± 0.20\n71.61 Â± 0.16\nÎ» = 0.01\n54.67 Â± 0.20\n71.26 Â± 0.16\nTable 14: Ablation studies on the prediction strategy (with cosine\nsimilarity or euclidean distance) of FEAT.\nSetups â†’\n1-Shot 5-Way\n5-Shot 5-Way\nBackbone â†’\nConvNet\nResNet\nConvNet\nResNet\nCosine Similarity-based Prediction\nFEAT\n54.64Â± 0.20\n66.26Â± 0.20\n71.72Â± 0.16\n81.83Â± 0.15\nEuclidean Distance-based Prediction\nFEAT\n55.15Â± 0.20\n66.78Â± 0.20\n71.61Â± 0.16\n82.05Â± 0.14\nour approach). It might require some careful regularizations\nto prevent over-ï¬tting, which we leave for future work.\nThe effectiveness of contrastive loss.\nTable 13 show the\nfew-shot classiï¬cation results with different weight values\n(Î») of the contrastive loss term for FEAT. From the results,\nwe can ï¬nd that the balance of the contrastive term in the\nlearning objective can inï¬‚uence the ï¬nal results. Empiri-\ncally, we set Î» = 0.1 in our experiments.\nThe inï¬‚uence of the prediction strategy.\nWe investi-\ngate two embedding-based prediction ways for the few-shot\nclassiï¬cation, i.e., based on the cosine similarity and the\nnegative euclidean distance to measure the relationship be-\ntween objects, respectively. We compare these two choices\nin Table 14.\nTwo strategies in Table 14 only differ in\ntheir similarity measures. In other words, with more than\none shot per class in the task training set, we average the\nsame class embeddings ï¬rst, and then make classiï¬cation\nby computing the cosine similarity or the negative euclidean\ndistance between a test instance and a class prototype. Dur-\ning the optimization, we tune the logits scale temperature\nfor both these methods. We ï¬nd that using the euclidean\ndistance usually requires small temperatures (e.g., Î³ =\n1\n64)\nwhile a large temperature (e.g., Î³ = 1) works well with the\nnormalized cosine similarity. The former choice achieves a\nslightly better performance than the latter one.\nD.3. Few-Shot Domain Generalization\nWe show that FEAT learns to adapt the intrinsic structure\nof tasks, and generalize across domains, i.e., predicting\ntest instances even when the visual appearance is changed.\nTable 15: Cross-Domain 1-shot 5-way classiï¬cation results of the\nFEAT approach.\nC â†’C\nC â†’R\nR â†’R\nSupervised\n34.38Â±0.16\n29.49Â±0.16\n37.43Â±0.16\nProtoNet\n35.51Â±0.16\n29.47Â±0.16\n37.24Â±0.16\nFEAT\n36.83Â±0.17\n30.89Â±0.17\n38.49Â±0.16\nTable 16: Results of models for transductive FSL with ConvNet\nbackbone on MiniImageNet. We cite the results of Semi-ProtoNet\nand TPN from [38] and [34] respectively. For TEAM [34], the au-\nthors do not report the conï¬dence intervals, so we set them to 0.00\nin the table.\nFEATâ€  and FEATâ€¡ adapt embeddings with the joint\nset of labeled training and unlabeled test instances, while make\nprediction via ProtoNet and Semi-ProtoNet respectively.\nSetups â†’\n1-Shot 5-Way\n5-Shot 5-Way\nStandard\nProtoNet\n52.61 Â± 0.20\n71.33 Â± 0.16\nFEAT\n55.15 Â± 0.20\n71.61 Â± 0.16\nTransductive\nSemi-ProtoNet [38]\n50.41 Â± 0.31\n64.39 Â± 0.24\nTPN [30]\n55.51 Â± 0.84\n69.86 Â± 0.67\nTEAM [34]\n56.57 Â± 0.00\n72.04 Â± 0.00\nSemi-ProtoNet (Ours)\n55.50 Â± 0.10\n71.76 Â± 0.08\nFEATâ€ \n56.49 Â± 0.16\n72.65 Â± 0.20\nFEATâ€¡\n57.04 Â± 0.16\n72.89 Â± 0.20\nSetups. We train a few-shot learning model in the standard\ndomain and evaluate it with cross-domain tasks, where the\nN-categories are aligned but domains are different. In de-\ntail, a model is trained on tasks from the â€œClipartâ€ domain\nof Ofï¬ceHome dataset [48], then the model is required to\ngeneralize to both â€œClipart (C)â€ and â€œReal World (R)â€ in-\nstances. In other words, we need to classify complex real\nimages by seeing only a few sketches, or even based on the\ninstances in the â€œReal World (R)â€ domain.\nResults. Table 15 gives the quantitative results. Here, the\nâ€œsupervisedâ€ refers to a model trained with standard clas-\nsiï¬cation and then is used for the nearest neighbor classi-\nï¬er with its penultimate layerâ€™s output feature. We observe\nthat ProtoNet can outperform this baseline on tasks when\nevaluating instances from â€œClipartâ€ but not ones from â€œreal\nworldâ€. However, FEAT can improve over â€œreal worldâ€ few-\nshot classiï¬cation even only seeing the support data from\nâ€œClipartâ€. Besides, when the support set and the test set\nof the target task are sampled from the same but new do-\nmains, e.g., the training and test instances both come from\nâ€œreal worldâ€, FEAT also improves the classiï¬cation accuracy\nw.r.t. the baseline methods. It veriï¬es the domain general-\nization ability of the FEAT approach.\n17\n",
    "D.4. Additional Discussions on Transductive FSL\nWe list the results of the transductive few-shot classiï¬-\ncation in Table 16, where the unlabeled test instances ar-\nrive simultaneously, so that the common structure among\nthe unlabeled test instances could be captured. We com-\npare with three approaches, Semi-ProtoNet [38], TPN [30],\nand TEAM [34]. Semi-ProtoNet utilizes the unlabeled in-\nstances to facilitate the computation of the class center and\nmakes predictions similar to the prototypical network; TPN\nmeta learns a label propagation way to take the unlabeled in-\nstances relationship into consideration; TEAM explores the\npairwise constraints in each task, and formulates the em-\nbedding adaptation into a semi-deï¬nite programming form.\nWe cite the results of Semi-ProtoNet from [38], and cite\nthe results of TPN and TEAM from [34].\nWe also re-\nimplement Semi-ProtoNet with our pre-trained backbone\n(the same pre-trained ConvNet weights as the standard few-\nshot learning setting) for a fair comparison.\nIn this setting, our model leverages the unlabeled test in-\nstances to augment the transformer as discussed in Â§ B.4\nand the embedding adaptation takes the relationship of all\ntest instances into consideration. Based on the adapted em-\nbedding by the joint set of labeled training instances and\nunlabeled test instances, we can make predictions with two\nstrategies. First, we still compute the center of the labeled\ninstances, while such adapted embeddings are inï¬‚uenced by\nthe unlabeled instances (we denote this approach as FEATâ€ ,\nwhich works the same way as standard FEAT except the aug-\nmented input of the embedding transformation function);\nSecond, we consider to take advantage of the unlabeled in-\nstances and use their adapted embeddings to construct a bet-\nter class prototype as in Semi-ProtoNet (we denote this ap-\nproach as FEATâ€¡).\nBy using more unlabeled test instances in the transduc-\ntive environment, FEATâ€  achieves further performance im-\nprovement compared with the standard FEAT, which veriï¬es\nthe unlabeled instances could assist the embedding adapta-\ntion of the labeled ones. With more accurate class center\nestimation, FEATâ€¡ gets a further improvement. The per-\nformance gain induced by the transductive FEAT is more\nsigniï¬cant in the one-shot learning setting compared with\nthe ï¬ve-shot scenario, since the helpfulness of unlabeled in-\nstance decreases when there are more labeled instances.\nD.5. More Generalized FSL Results\nHere we show the full results of FEAT in the general-\nized few-shot learning setting in Table 17, which includes\nboth the 1-shot and 5-shot performance. All methods are\nevaluated on instances composed by SEEN classes, UNSEEN\nclasses, and both of them (COMBINED), respectively. In\nthe 5-shot scenario, the performance improvement mainly\ncomes from the improvement of over the UNSEEN tasks.\nTable 17: Results of generalized FEAT with ConvNet backbone on\nMiniImageNet. All methods are evaluated on instances composed\nby SEEN classes, UNSEEN classes, and both of them (COMBINED),\nrespectively.\nMeasures â†’\nSEEN\nUNSEEN\nCOMBINED\n1-shot learning\nProtoNet\n41.73Â±0.03\n48.64Â±0.20\n35.69Â±0.03\nFEAT\n43.94Â±0.03\n49.72Â±0.20\n40.50Â±0.03\n5-shot learning\nProtoNet\n41.06Â±0.03\n64.94Â±0.17\n38.04Â±0.02\nFEAT\n44.94Â±0.03\n65.33Â±0.16\n41.68Â±0.03\nRandom Chance\n1.56\n20.00\n1.45\nTable 18: The top-5 low-shot learning accuracy over all classes\non the large scale ImageNet [39] dataset (w/ ResNet-50).\nUNSEEN\n1-Shot\n2-Shot\n5-Shot\n10-Shot\n20-Shot\nProtoNet [43]\n49.6\n64.0\n74.4\n78.1\n80.0\nPMN [52]\n53.3\n65.2\n75.9\n80.1\n82.6\nFEAT\n53.8\n65.4\n76.0\n81.2\n83.6\nAll\n1-Shot\n2-Shot\n5-Shot\n10-Shot\n20-Shot\nProtoNet [43]\n61.4\n71.4\n78.0\n80.0\n81.1\nPMN [52]\n64.8\n72.1\n78.8\n81.7\n83.3\nFEAT\n65.1\n72.5\n79.3\n82.1\n83.9\nAll w/ Prior\n1-Shot\n2-Shot\n5-Shot\n10-Shot\n20-Shot\nProtoNet [43]\n62.9\n70.5\n77.1\n79.5\n80.8\nPMN [52]\n63.4\n70.8\n77.9\n80.9\n82.7\nFEAT\n63.8\n71.2\n78.1\n81.3\n83.4\nD.6. Large-Scale Low-Shot Learning\nSimilar to the generalized few-shot learning, the large-\nscale low-shot learning [12, 14, 52] considers the few-shot\nclassiï¬cation ability on both SEEN and UNSEEN classes on\nthe full ImageNet [39] dataset. There are in total 389 SEEN\nclasses and 611 UNSEEN classes [14]. We follow the setting\n(including the splits) of the prior work [14] and use features\nextracted based on the pre-trained ResNet-50 [15]. Three\nevaluation protocols are evaluated, namely the top-5 few-\nshot accuracy on the UNSEEN classes, on the combined set\nof both SEEN and UNSEEN classes, and the calibrated accu-\nracy on weighted by selected set prior on the combined set\nof both SEEN and UNSEEN classes. The results are listed in\nTable 18. We observe that FEAT achieves better results than\nothers, which further validates FEATâ€™s superiority in gener-\nalized classiï¬cation setup, a large scale learning setup.\n18\n"
  ],
  "full_text": "Few-Shot Learning via Embedding Adaptation with Set-to-Set Functions\nHan-Jia Ye*\nNanjing University\nyehj@lamda.nju.edu.cn\nHexiang Hu\nUSC\nhexiangh@usc.edu\nDe-Chuan Zhan\nNanjing University\nzhandc@lamda.nju.edu.cn\nFei Shaâ€ \nUSC & Google\nfsha@google.com\nAbstract\nLearning with limited data is a key challenge for vi-\nsual recognition. Many few-shot learning methods address\nthis challenge by learning an instance embedding function\nfrom seen classes and apply the function to instances from\nunseen classes with limited labels. This style of transfer\nlearning is task-agnostic: the embedding function is not\nlearned optimally discriminative with respect to the unseen\nclasses, where discerning among them leads to the tar-\nget task. In this paper, we propose a novel approach to\nadapt the instance embeddings to the target classiï¬cation\ntask with a set-to-set function, yielding embeddings that are\ntask-speciï¬c and are discriminative. We empirically investi-\ngated various instantiations of such set-to-set functions and\nobserved the Transformer is most effective â€” as it naturally\nsatisï¬es key properties of our desired model. We denote this\nmodel as FEAT (few-shot embedding adaptation w/ Trans-\nformer) and validate it on both the standard few-shot classi-\nï¬cation benchmark and four extended few-shot learning set-\ntings with essential use cases, i.e., cross-domain, transduc-\ntive, generalized few-shot learning, and low-shot learning.\nIt archived consistent improvements over baseline models\nas well as previous methods, and established the new state-\nof-the-art results on two benchmarks.\n1. Introduction\nFew-shot visual recognition [10, 23, 24, 27, 49] emerged\nas a promising direction in tackling the challenge of learn-\ning new visual concepts with limited annotations.\nCon-\ncretely, it distinguishes two sets of visual concepts: SEEN\nand UNSEEN ones. The target task is to construct visual\nclassiï¬ers to identify classes from the UNSEEN where each\nclass has a very small number of exemplars (â€œfew-shotâ€).\nThe main idea is to discover transferable visual knowl-\nedge in the SEEN classes, which have ample labeled in-\nstances, and leverage it to construct the desired classiï¬er.\nFor example, state-of-the-art approaches for few-shot learn-\n*Work mostly done when the author was a visiting scholar at USC.\nâ€ On leave from USC\ning [40, 43, 46, 49] usually learn a discriminative instance\nembedding model on the SEEN categories, and apply it to\nvisual data in UNSEEN categories. In this common embed-\nding space, non-parametric classiï¬ers (e.g., nearest neigh-\nbors) are then used to avoid learning complicated recogni-\ntion models from a small number of examples.\nSuch approaches suffer from one important limitation.\nAssuming a common embedding space implies that the dis-\ncovered knowledge â€“ discriminative visual features â€“ on\nthe SEEN classes are equally effective for any classiï¬cation\ntasks constructed for an arbitrary set of UNSEEN classes. In\nconcrete words, suppose we have two different target tasks:\ndiscerning â€œcatâ€ versus â€œdogâ€ and discerning â€œcatâ€ versus\nâ€œtigerâ€. Intuitively, each task uses a different set of discrim-\ninative features. Thus, the most desired embedding model\nï¬rst needs to be able to extract discerning features for either\ntask at the same time. This could be a challenging aspect in\nits own right as the current approaches are agnostic to what\nthose â€œdownstreamâ€ target tasks are and could accidentally\nde-emphasize selecting features for future use. Secondly,\neven if both sets of discriminative features are extracted,\nthey do not necessarily lead to the optimal performance for\na speciï¬c target task. The most useful features for discern-\ning â€œcatâ€ versus â€œtigerâ€ could be irrelevant and noise to the\ntask of discerning â€œcatâ€ versus â€œdogâ€!\nWhat is missing from the current few-shot learning ap-\nproaches is an adaptation strategy that tailors the visual\nknowledge extracted from the SEEN classes to the UNSEEN\nones in a target task. In other words, we desire separate em-\nbedding spaces where each one of them is customized such\nthat the visual features are most discriminative for a given\ntask. Towards this, we propose a few-shot model-based em-\nbedding adaptation method that adjusts the instance embed-\nding models derived from the SEEN classes. Such model-\nbased embedding adaptation requires a set-to-set function: a\nfunction mapping that takes all instances from the few-shot\nsupport set and outputs the set of adapted support instance\nembeddings, with elements in the set co-adapting with each\nother. Such output embeddings are then assembled as the\nprototypes for each visual category and serve as the near-\nest neighbor classiï¬ers.\nFigure 1 qualitatively illustrates\n1\narXiv:1812.03664v6  [cs.LG]  13 Jun 2021\n\n\nMalamute\nAnt\nSchool bus\nGolden retriever\nTheater curtain\nAdaptation\nLion\nSchool bus\nHourglass\nVase\nTrifle\nAdaptation\nTrifle\nScoreboard\nGolden retriever\nDalmatian\nVase\nAdaptation\nGolden retriever\nNematode\nLion\nDalmatian\nMalamute\nAdaptation\n(a) Accâ†‘: 40.33% â†’55.33%\n(b) Accâ†‘: 48.00% â†’69.60%\n(c) Accâ†‘: 43.60% â†’63.33%\n(d) Accâ†“: 56.33% â†’47.13%\nFigure 1: Qualitative visualization of model-based embedding adaptation procedure (implemented using FEAT) on test tasks (refer to\nÂ§ 5.2.2 for more details). Each ï¬gure shows the locations of PCA projected support embeddings (class prototypes) before and after the\nadaptation of FEAT. Values below are the 1-shot 5-way classiï¬cation accuracy before and after the the adaptation. Interestingly, the\nembedding adaptation step of FEAT pushes the support embeddings apart from the clutter and toward their own clusters, such that they can\nbetter ï¬ts the test data of its categories. (Best view in colors!)\nthe embedding adaptation procedure (as results of our best\nmodel). These class prototypes spread out in the embedding\nspace toward the samples cluster of each category, indicat-\ning the effectiveness of embedding adaptation.\nIn this paper, we implement the set-to-set transformation\nusing a variety of function approximators, including bidi-\nrectional LSTM [16] (Bi-LSTM), deep sets [56], graph con-\nvolutional network (GCN) [21], and Transformer [29, 47].\nOur experimental results (refer to Â§ 5.2.1) suggest that\nTransformer is the most parameter efï¬cient choice that at\nthe same time best implements the key properties of the de-\nsired set-to-set transformation, including contextualization,\npermutation invariance, interpolation and extrapolation ca-\npabilities (see Â§ 4.1). As a consequence, we choose the\nset-to-set function instantiated with Transformer to be our\nï¬nal model and denote it as FEAT (Few-shot Embedding\nAdaptation with Transformer). We further conduct compre-\nhensive analysis on FEAT and evaluate it on many extended\ntasks, including few-shot domain generalization, transduc-\ntive few-shot learning, and generalized few-shot learning.\nOur overall contribution is three-fold.\nâ€¢ We formulate the few-shot learning as a model-based em-\nbedding adaptation to make instance embeddings task-\nspeciï¬c, via using a set-to-set transformation.\nâ€¢ We instantiate such set-to-set transformation with various\nfunction approximators, validating and analyzing their\nfew-shot learning ability, task interpolation ability, and\nextrapolation ability, etc. It concludes our model (FEAT)\nthat uses the Transformer as the set-to-set function.\nâ€¢ We evaluate our FEAT model on a variety of extended\nfew-shot learning tasks, where it achieves superior per-\nformances compared with strong baseline approaches.\n2. Related Work\nMethods speciï¬cally designed for few-shot learning fall\nbroadly into two categories. The ï¬rst is to control how a\nclassiï¬er for the target task should be constructed.\nOne\nfruitful idea is the meta-learning framework where the\nclassiï¬ers are optimized in anticipation that a future up-\ndate due to data from a new task performs well on that\ntask [2, 3, 10, 13, 26, 32, 36, 40], or the classiï¬er itself is\ndirectly meta-predicted by the new task data [35, 53].\nAnother line of approach has focused on learning gener-\nalizable instance embeddings [1, 5, 6, 17, 22, 31, 42, 46, 49]\nand uses those embeddings on simple classiï¬ers such as\nnearest neighbor rules. The key assumption is that the em-\nbeddings capture all necessarily discriminative representa-\ntions of data such that simple classiï¬ers are sufï¬ced, hence\navoiding the danger of overï¬tting on a small number of la-\nbeled instances. Early work such as [22] ï¬rst validated the\nimportance of embedding in one-shot learning, whilst [49]\nproposes to learn the embedding with a soft nearest neigh-\nbor objective, following a meta-learning routine. Recent\nadvances have leveraged different objective functions for\nlearning such embedding models, e.g., considering the class\nprototypes [43], decision ranking [46], and similarity com-\nparison [45]. Most recently, [41] utilizes the graph convo-\nlution network [21] to unify the embedding learning.\nOur work follows the second school of thoughts. The\nmain difference is that we do not assume the embed-\ndings learned on SEEN classes, being agnostic to the tar-\nget tasks, are necessarily discriminative for those tasks.\nIn contrast, we propose to adapt those embeddings for\neach target task with a set-to-set function so that the trans-\nformed embeddings are better aligned with the discrimina-\ntion needed in those tasks. We show empirically that such\ntask-speciï¬c embeddings perform better than task-agnostic\nones.\nMetaOptNet [25] and CTM [28] follow the same\nspirit of learning task-speciï¬c embedding (or classiï¬ers) via\neither explicitly optimization of target task or using concen-\ntrator and projector to make distance metric task-speciï¬c.\n2\n\n\nClassification \nScores\nCNN\nCNN\nCNN\nCNN\nSoft Nearest \nNeighbor\n(a) Instance Embedding\nClassification \nScores\nEmbedding \nAdaptation\nCNN\nCNN\nCNN\nCNN\nSoft Nearest \nNeighbor\nTrain Instance\nTest Instance\nTask Agnostic\nEmbedding\nTask Specific \nEmbedding\n(b) Embedding Adaptation\nSet-to-Set Function\nFigure 2:\nIllustration of the proposed Few-Shot Embedding\nAdaptation Transformer (FEAT). Existing methods usually use the\nsame embedding function E for all tasks. We propose to adapt the\nembeddings to each target few-shot learning task with a set-to-set\nfunction such as Transformer, BiLSTM, DeepSets, and GCN.\n3. Learning Embedding for Task-agnostic FSL\nIn the standard formulation of few-shot learning\n(FSL) [10, 49], a task is represented as a M-shot N-way\nclassiï¬cation problem with N classes sampled from a set\nof visual concepts U and M (training/support) examples\nper class.\nWe denote the training set (also referred as\nsupport sets in the literature) as Dtrain = {xi, yi}NM\ni=1 ,\nwith the instance xi âˆˆRD and the one-hot labeling vec-\ntor yi âˆˆ{0, 1}N. We will use â€œsupport setâ€ and â€œtrain-\ning setâ€ interchangeably in the paper. In FSL, M is of-\nten small (e.g., M = 1 or M = 5).\nThe goal is to\nï¬nd a function f that classiï¬es a test instance xtest by\nË†ytest = f(xtest; Dtrain) âˆˆ{0, 1}N.\nGiven a small number of training instances, it is chal-\nlenging to construct complex classiï¬ers f(Â·). To this end,\nthe learning algorithm is also supplied with additional data\nconsisting of ample labeled instances. These additional data\nare drawn from visual classes S, which does not overlap\nwith U. We refer to the original task as the target task which\ndiscerns N UNSEEN classes U. To avoid confusion, we de-\nnote the data from the SEEN classes S as DS.\nTo learn f(Â·) using DS, we synthesize many M-shot N-\nway FSL tasks by sampling the data in the meta-learning\nmanner [10, 49]. Each sampling gives rise to a task to clas-\nsify a test set instance xS\ntest into one of the N SEEN classes\nby f(Â·), where the test instances set DS\ntest is composed of\nthe labeled instances with the same distribution as DS\ntrain.\nFormally, the function f(Â·) is learnt to minimize the aver-\naged error over those sampled tasks\nf âˆ—= arg min\nf\nX\n(xS\ntest,yS\ntest)âˆˆDS\ntest\nâ„“(f(xS\ntest; DS\ntrain), yS\ntest)\n(1)\nwhere the loss â„“(Â·) measures the discrepancy between the\nprediction and the true label. For simplicity, we have as-\nsumed we only synthesize one task with test set DS\ntest. The\noptimal f âˆ—is then applied to the original target task.\nWe consider the approach based on learning embeddings\nAlgorithm 1 Training strategy of embedding adaptation\nRequire: Seen class set S\n1: for all iteration = 1,...,MaxIteration do\n2:\nSample N-way M-shot (DS\ntrain, DS\ntest) from S\n3:\nCompute Ï†x = E(x), for x âˆˆX S\ntrain âˆªX S\ntest\n4:\nfor all (xS\ntest, yS\ntest) âˆˆDS\ntest do\n5:\nCompute {Ïˆx ; âˆ€x âˆˆX S\ntrain} with T via Eq. 3\n6:\nPredict Ë†yS\ntest with {Ïˆx} as Eq. 4\n7:\nCompute â„“(Ë†yS\ntest, yS\ntest) with Eq. 1\n8:\nend for\n9:\nCompute âˆ‡E,T\nP\n(xS\ntest,yS\ntest)âˆˆDS\ntest â„“(Ë†yS\ntest, yS\ntest)\n10:\nUpdate E and T with âˆ‡E,T use SGD\n11: end for\n12: return Embedding function E and set function T.\nfor FSL [43, 49] (see Figure 2 (a) for an overview). In par-\nticular, the classiï¬er f(Â·) is composed of two elements. The\nï¬rst is an embedding function Ï†x = E(x) âˆˆRd that maps\nan instance x to a representation space. The second compo-\nnent applies the nearest neighbor classiï¬ers in this space:\nË†ytest = f(Ï†xtest; {Ï†x, âˆ€(x, y) âˆˆDtrain})\n(2)\nâˆexp\n\u0000sim(Ï†xtest, Ï†x)\n\u0001\nÂ· y, âˆ€(x, y) âˆˆDtrain\nNote that only the embedding function is learned by opti-\nmizing the loss in Eq. 1. For reasons to be made clear in\nbelow, we refer this embedding function as task-agnostic.\n4. Adapting Embedding for Task-speciï¬c FSL\nIn what follows, we describe our approach for few-shot\nlearning (FSL). We start by describing the main idea (Â§ 4.1,\nalso illustrated in Figure 2), then introduce the set-to-set\nadaptation function (Â§ 4.2). Last are learning (Â§ 4.3) and\nimplementations details (Â§ 4.4).\n4.1. Adapting to Task-Speciï¬c Embeddings\nThe key difference between our approach and traditional\nones is to learn task-speciï¬c embeddings. We argue that the\nembedding Ï†x is not ideal. In particular, the embeddings do\nnot necessarily highlight the most discriminative represen-\ntation for a speciï¬c target task. To this end, we introduce an\nadaption step where the embedding function Ï†x (more pre-\ncisely, its values on instances) is transformed. This trans-\nformation is a set-to-set function that contextualizes over\nthe image instances of a set, to enable strong co-adaptation\nof each item.\nInstance functions fails to have such co-\nadaptation property.\nFurthermore, the set-to-set-function\nreceives instances as bags, or sets without orders, requiring\nthe function to output the set of reï¬ned instance embeddings\n3\n\n\nwhile being permutation-invariant. Concretely,\n{Ïˆx ; âˆ€x âˆˆXtrain} = T ({Ï†x ; âˆ€x âˆˆXtrain})\n(3)\n= T (Ï€ {Ï†x ; âˆ€x âˆˆXtrain}))\nwhere Xtrain is a set of all the instances in the training set\nDtrain for the target task. Ï€(Â·) is a permutation operator\nover a set. Thus the set of adapted embedding will not\nchange if we apply a permutation over the input embedding\nset. With adapted embedding Ïˆx, the test instance xtest can\nbe classiï¬ed by computing nearest neighbors w.r.t. Dtrain:\nË†ytest = f(Ï†xtest; {Ïˆx, âˆ€(x, y) âˆˆDtrain})\n(4)\nOur approach is generally applicable to different types of\ntask-agnostic embedding function E and similarity measure\nsim(Â·, Â·), e.g., the (normalized) cosine similarity [49] or the\nnegative distance [43]. Both the embedding function E and\nthe set transformation function T are optimized over syn-\nthesized FSL tasks sampled from DS, sketched in Alg. 1.\nIts key difference from conventional FSL is in the line 4 to\nline 8 where the embeddings are transformed.\n4.2. Embedding Adaptation via Set-to-set Functions\nNext, we explain various choices as the instantiations of\nthe set-to-set embedding adaptation function.\nBidirectional LSTM (BILSTM) [16, 49] is one of the\ncommon choice to instantiate the set-to-set transformation,\nwhere the addition between the input and the hidden layer\noutputs of each BILSTM cell leads to the adapted embed-\nding. It is notable that the output of the BILSTM suppose to\ndepend on the order of the input set. Note that using BIL-\nSTM as embedding adaptation model is similar but different\nfrom the fully conditional embedding [49], where the later\none contextualizes both training and test instance embed-\nding altogether, which results in a transductive setting.\nDeepSets [56] is inherently a permutation-invariant trans-\nformation function. It is worth noting that DEEPSETS aggre-\ngates the instances in a set into a holistic set vector. We con-\nsider two components to implement such DeepSets transfor-\nmation, an instance centric set vector combined with a set\ncontext vector. For x âˆˆXtrain, we deï¬ne its complemen-\ntary set as xâˆ. Then we implement the DEEPSETS by:\nÏˆx = Ï†x + g([Ï†x;\nX\nxiâ€²âˆˆxâˆ\nh(Ï†xiâ€²)])\n(5)\nIn Eq. 5, g and h are two-layer multi-layer perception\n(MLP) with ReLU activation which map the embedding\ninto another space and increase the representation ability\nof the embedding. For each instance, embeddings in its\ncomplementary set is ï¬rst combined into a set vector as the\ncontext, and then this vector is concatenated with the input\nembedding to obtain the residual component of adapted em-\nbedding. This conditioned embedding takes other instances\nin the set into consideration, and keeps the â€œset (permutation\ninvariant)â€ property. In practice, we ï¬nd using the maxi-\nmum operator in Eq. 5 works better than the sum operator\nsuggested in [56].\nGraph Convolutional Networks (GCN) [21, 41] propa-\ngate the relationship between instances in the set. We ï¬rst\nconstruct the degree matrix A to represent the similarity be-\ntween instances in a set. If two instances come from the\nsame class, then we set the corresponding element in A to\n1, otherwise to 0. Based on A, we build the â€œnormalizedâ€\nadjacency matrix S for a given set with added self-loops\nS = Dâˆ’1\n2 (A + I)Dâˆ’1\n2 . I is the identity matrix, and D is\nthe diagonal matrix whose elements are equal to the sum of\nelements in the corresponding row of A + I.\nLet Î¦0 = {Ï†x ; âˆ€x âˆˆXtrain}, the relationship between\ninstances could be propagated based on S, i.e.,\nÎ¦t+1 = ReLU(SÎ¦tW) , t = 0, 1, . . . , T âˆ’1\n(6)\nW is a projection matrix for feature transformation.\nIn\nGCN, the embedding in the set is transformed based on\nEq. 6 multiple times, and the ï¬nal Î¦T gives rise to the {Ïˆx}.\nTransformer. [47] We use the Transformer architec-\nture [47] to implement T. In particular, we employ self-\nattention mechanism [29, 47] to transform each instance\nembedding with consideration to its contextual instances.\nNote that it naturally satisï¬es the desired properties of T\nbecause it outputs reï¬ned instance embeddings and is per-\nmutation invariant. We denote it as Few-Shot Embedding\nAdaptation with Transformer (FEAT).\nTransformer is a store of triplets in the form of (query\nQ, key K, and value V). To compute proximity and re-\nturn values, those points are ï¬rst linearly mapped into some\nspace K = W âŠ¤\nK\n\u0002\nÏ†xk; âˆ€xk âˆˆK\n\u0003\nâˆˆRdÃ—|K|, which\nis also the same for Q and V with WQ and WV respec-\ntively. Transformer computes what is the right value for a\nquery point â€” the query xq âˆˆQ is ï¬rst matched against\na list of keys K where each key has a value V . The ï¬-\nnal value is then returned as the sum of all the values\nweighted by the proximity of the key to the query point,\ni.e. Ïˆxq = Ï†xq + P\nk Î±qkV:,k, where\nÎ±qk âˆexp\n \nÏ†âŠ¤\nxqWQ Â· K\nâˆš\nd\n!\nand V:,k is the k-th column of V . In the standard FSL setup,\nwe have Q = K = V = Xtrain.\n4.3. Contrastive Learning of Set-to-Set Functions\nTo facilitate the learning of embedding adaptation, we\napply a contrastive objective in addition to the general one.\n4\n\n\nIt is designed to make sure that instances embeddings af-\nter adaptation is similar to the same class neighbors and\ndissimilar to those from different classes. Speciï¬cally, the\nembedding adaptation function T is applied to instances of\neach n of the N class in DS\ntrain âˆªDS\ntest, which gives rise to\nthe transformed embedding Ïˆâ€²\nx and class centers {cn}N\nn=1.\nThen we apply the contrastive objective to make sure train-\ning instances are close to its own class center than other\ncenters. The total objective function (together with Eq. 1) is\nshown as following:\nL(Ë†ytest, ytest) = â„“(Ë†ytest, ytest)\n(7)\n+Î» Â· â„“\n\u0000softmax\n\u0000sim(Ïˆâ€²\nxtest, cn)\n\u0001\n, ytest\n\u0001\nThis contrastive learning makes the set transformation ex-\ntract common characteristic for instances of the same cate-\ngory, so as to preserve the category-wise similarity.\n4.4. Implementation details\nWe consider three different types of convolutional net-\nworks as the backbone for instance embedding function E:\n1) A 4-layer convolution network (ConvNet) [43, 46, 49]\nand 2) the 12-layer residual network (ResNet) used in [25],\nand 3) the Wide Residual Network (WideResNet) [40, 55].\nWe apply an additional pre-training stage for the backbones\nover the SEEN classes, based on which our re-implemented\nmethods are further optimized. To achieve more precise em-\nbedding, we average the same-class instances in the train-\ning set before the embedding adaptation with the set-to-\nset transformation. Adam [20] and SGD are used to op-\ntimize ConvNet and ResNet variants respectively. More-\nover, we follow the most standard implementations for the\nfour set-to-set functions â€” BiLSTM [16], DeepSets [56],\nGraph Convolutional Networks (GCN) [21] and Trans-\nformer (FEAT) [47].\nWe refer readers to supplementary\nmaterial (SM) for complete details and ablation studies of\neach set-to-set functions. Our implementation is available\nat https://github.com/Sha-Lab/FEAT.\n5. Experiments\nIn this section, we ï¬rst evaluate a variety of models for\nembedding adaptation in Â§ 5.2 with standard FSL. It con-\ncludes that FEAT (with Transformer) is the most effective\napproach among different instantiations. Next, we perform\nablation studies in Â§ 5.2.2 to analyze FEAT in details. Even-\ntually, we evaluate FEAT on many extended few-shot learn-\ning tasks to study its general applicability (Â§ 5.3).\nThis\nstudy includes few-shot domain generalization, transduc-\ntive few-shot learning, generalized few-shot learning, and\nlarge-scale low-shot learning (refer to SM).\n5.1. Experimental Setups\nDatasets.\nMiniImageNet [49] and TieredImageNet [38]\ndatasets are subsets of the ImageNet [39]. MiniImageNet\nincludes a total number of 100 classes and 600 examples\nper class. We follow the setup provided by [36], and use\n64 classes as SEEN categories, 16 and 20 as two sets of\nUNSEEN categories for model validation and evaluation re-\nspectively.\nTieredImageNet is a large-scale dataset with\nmore categories, which contains 351, 97, and 160 categories\nfor model training, validation, and evaluation, respectively.\nIn addition to these, we investigate the Ofï¬ceHome [48]\ndataset to validate the generalization ability of FEAT across\ndomains. There are four domains in Ofï¬ceHome, and two\nof them (â€œClipartâ€ and â€œReal Worldâ€) are selected, which\ncontains 8722 images. After randomly splitting all classes,\n25 classes serve as the seen classes to train the model, and\nthe remaining 15 and 25 classes are used as two UNSEEN\nfor evaluation. Please refer to SM for more details.\nEvaluation protocols. Previous approaches [10, 43, 46]\nusually follow the original setting of [49] and evaluate the\nmodels on 600 sampled target tasks (15 test instances per\nclass).\nIn a later study [40], it was suggested that such\nan evaluation process could potentially introduce high vari-\nances. Therefore, we follow the new and more trustworthy\nevaluation setting to evaluate both baseline models and our\napproach on 10,000 sampled tasks. We report the mean ac-\ncuracy (in %) as well as the 95% conï¬dence interval.\nBaseline and embedding adaptation methods.\nWe re-\nimplement the prototypical network (ProtoNet) [43] as a\ntask-agnostic embedding baseline model. This is known\nas a very strong approach [8] when the backbone archi-\ntecture is deep, i.e., residual networks [15]. As suggested\nby [33], we tune the scalar temperature carefully to scale\nthe logits of both approaches in our re-implementation. As\nmentioned, we implement the embedding adaptation model\nwith four different function approximators, and denote them\nas BILSTM, DEEPSETS, GCN, and FEAT (i.e. Transformer).\nThe concrete details of each model are included in the SM.\nBackbone pre-training.\nInstead of optimizing from\nscratch, we apply an additional pre-training strategy as\nsuggested in [35, 40]. The backbone network, appended\nwith a softmax layer, is trained to classify all SEEN\nclasses with the cross-entropy loss (e.g., 64 classes in the\nMiniImageNet).\nThe classiï¬cation performance over the\npenultimate layer embeddings of sampled 1-shot tasks from\nthe model validation split is evaluated to select the best pre-\ntrained model, whose weights are then used to initialize the\nembedding function E in the few-shot learning.\n5.2. Standard Few-Shot Image Classiï¬cation\nWe compare our proposed FEAT method with the in-\nstance embedding baselines as well as previous methods on\n5\n\n\nTable 1: Few-shot classiï¬cation accuracy on MiniImageNet. â‹†\nCTM [28] and SimpleShot [51] utilize the ResNet-18. (see SM\nfor the full table with conï¬dence intervals and WRN results.).\nSetups â†’\n1-Shot 5-Way\n5-Shot 5-Way\nBackbone â†’\nConvNet\nResNet\nConvNet\nResNet\nMatchNet [49]\n43.40\n-\n51.09\n-\nMAML [10]\n48.70\n-\n63.11\n-\nProtoNet [43]\n49.42\n-\n68.20\n-\nRelationNet [45]\n51.38\n-\n67.07\n-\nPFA [35]\n54.53\n59.60\n67.87\n73.74\nTADAM [33]\n-\n58.50\n-\n76.70\nMetaOptNet [25]\n-\n62.64\n-\n78.63\nCTM [28]\n-\n64.12\n-\n80.51\nSimpleShot [51]\n49.69\n62.85\n66.92\n80.02\nInstance embedding\nProtoNet\n52.61\n62.39\n71.33\n80.53\nEmbedding adaptation\nBILSTM\n52.13\n63.90\n69.15\n80.62\nDEEPSETS\n54.41\n64.14\n70.96\n80.93\nGCN\n53.25\n64.50\n70.59\n81.65\nFEAT\n55.15\n66.78\n71.61\n82.05\nthe standard MiniImageNet [49] and TieredImageNet [38]\nbenchmarks, and then perform detailed analysis on the ab-\nlated models. We include additional results with CUB [50]\ndataset in SM, which shares a similar observation.\n5.2.1. Main Results\nComparison to previous State-of-the-arts. Table 1 and\nTable 2 show the results of our method and others on the\nMiniImageNet and TieredImageNet. First, we observe that\nthe best embedding adaptation method (FEAT) outperforms\nthe instance embedding baseline on both datasets, indi-\ncating the effectiveness of learning task-speciï¬c embed-\nding space. Meanwhile, the FEAT model performs signif-\nicantly better than the current state-of-the-art methods on\nMiniImageNet dataset. On the TieredImageNet, we observe\nthat the ProtoNet baseline is already better than some pre-\nvious state-of-the-arts based on the 12-layer ResNet back-\nbone.\nThis might due to the effectiveness of the pre-\ntraining stage on the TieredImageNet as it is larger than\nMiniImageNet and a fully converged model can be itself\nvery effective. Based on this, all embedding adaptation ap-\nproaches further improves over ProtoNet almost in all cases,\nwith FEAT achieving the best performances among all ap-\nproaches. Note that here our pre-training strategy is most\nsimilar to the one used in PFA [35], while we further ï¬ne-\ntune the backbone. Temperature scaling of the logits inï¬‚u-\nences the performance a lot when ï¬ne-tuning over the pre-\ntrained weights. Additionally, we list some recent methods\n(SimpleShot [51], and CTM [28]) using different backbone\nTable 2: Few-shot classiï¬cation accuracy and 95% conï¬dence in-\nterval on TieredImageNet with the ResNet backbone.\nSetups â†’\n1-Shot 5-Way\n5-Shot 5-Way\nProtoNet [43]\n53.31 Â± 0.89\n72.69 Â± 0.74\nRelationNet [45]\n54.48 Â± 0.93\n71.32 Â± 0.78\nMetaOptNet [25]\n65.99 Â± 0.72\n81.56 Â± 0.63\nCTM [28]\n68.41 Â± 0.39\n84.28 Â± 1.73\nSimpleShot [51]\n69.09 Â± 0.22\n84.58 Â± 0.16\nInstance embedding\nProtoNet\n68.23 Â± 0.23\n84.03 Â± 0.16\nEmbedding adaptation\nBILSTM\n68.14 Â± 0.23\n84.23 Â± 0.16\nDEEPSETS\n68.59 Â± 0.24\n84.36 Â± 0.16\nGCN\n68.20 Â± 0.23\n84.64 Â± 0.16\nFEAT\n70.80 Â± 0.23\n84.79 Â± 0.16\nTable 3: Number of parameters introduced by each set-to-set\nfunction in additional to the backboneâ€™s parameters.\nBILSTM\nDEEPSETS\nGCN\nFEAT\nConvNet\n25K\n82K\n33K\n16K\nResNet\n2.5M\n8.2M\n3.3M\n1.6M\narchitectures such as ResNet-18 for reference.\nComparison among the embedding adaptation models.\nAmong the four embedding adaptation methods, BILSTM in\nmost cases achieves the worst performances and sometimes\neven performs worse than ProtoNet. This is partially due\nto the fact that BILSTM can not easily implement the re-\nquired permutation invariant property (also shown in [56]),\nwhich confuses the learning process of embedding adapta-\ntion. Secondly, we ï¬nd that DEEPSETS and GCN have the\nability to adapt discriminative task-speciï¬c embeddings but\ndo not achieve consistent performance improvement over\nthe baseline ProtoNet especially on MiniImageNet with the\nConvNet backbone. A potential explanation is that, such\nmodels when jointly learned with the backbone model, can\nmake the optimization process more difï¬cult, which leads\nto the varying ï¬nal performances. In contrast, we observe\nthat FEAT can consistently improve ProtoNet and other em-\nbedding adaptation approaches in all cases, without addi-\ntional bells and whistles. It shows that the Transformer as a\nset-to-set function can implement rich interactions between\ninstances, which provides its high expressiveness to model\nthe embedding adaptation process.\nInterpolation and extrapolation of classiï¬cation ways.\nNext, we study different set-to-set functions on their capa-\nbility of interpolating and extrapolating across the number\nof classiï¬cation ways. To do so, we train each variant of em-\n6\n\n\n5\n10\n15\n20\nNumber of categories per task\n0\n10\n20\n30\n40\n50\n60\n70\nMean accuracy (in %)\n52.5\n36.8\n29.3\n24.6\n55.0\n38.6\n30.6\n25.8\n53.2\n37.1\n29.5\n24.9\n55.1\n39.1\n31.3\n26.4\nMethods\nRandom\nBILSTM\nDeepSets\nGCN\nFEAT\n5\n10\n15\n20\nNumber of categories per task\n0\n10\n20\n30\n40\n50\n60\n70\n52.1\n35.5\n27.5\n22.9\n54.4\n36.9\n27.3\n20.6\n54.1\n37.9\n30.1\n25.3\n55.1\n39.1\n31.1\n26.2\nMethods\nRandom\nBILSTM\nDeepSets\nGCN\nFEAT\n(a) Way Interpolation\n(b) Way Extrapolation\nFigure 3: Interpolation and Extrapolation of few-shot tasks\nfrom the â€œwayâ€ perspective. First, We train various embedding\nadaptation models on 1-shot 20-way (a) or 5-way (b) classiï¬cation\ntasks and evaluate models on unseen tasks with different number\nof classes (N={5, 10, 15, 20}). It shows that FEAT is superior in\nterms of way interpolation and extrapolation ability.\nbedding adaptation functions with both 1-shot 20-way and\n1-shot 5-way tasks, and measure the performance change as\na function to the number of categories in the test time. We\nreport the mean accuracies evaluated on few-shot classiï¬-\ncation with N = {5, 10, 15, 20} classes, and show results\nin Figure 3. Surprisingly, we observe that FEAT achieves\nalmost the same numerical performances in both extrapo-\nlation and interpolation scenarios, which further displays\nits strong capability of learning the set-to-set transforma-\ntion. Meanwhile, we observe that DEEPSETS works well\nwith interpolation but fails with extrapolation as its perfor-\nmance drops signiï¬cantly with the larger N. In contrast,\nGCN achieves strong extrapolation performances but does\nnot work as effectively in interpolation. BILSTM performs\nthe worst in both cases, as it is by design not permutation\ninvariant and may have ï¬tted an arbitrary dependency be-\ntween instances.\nParameter efï¬ciency. Table 3 shows the number of ad-\nditional parameters each set-to-set function has introduced.\nFrom this, we observe that with both ConvNet and ResNet\nbackbones, FEAT has the smallest number of parameters\ncompared with all other approaches while achieving best\nperformances from various aspects (as results discussed\nabove), which highlights its high parameter efï¬ciency.\nAll above, we conclude that: 1) learning embedding\nadaptation with a set-to-set model is very effective in mod-\neling task-speciï¬c embeddings for few-shot learning 2)\nFEAT is the most parameter-efï¬cient function approximater\nthat achieves the best empirical performances, together with\nnice permutation invariant property and strong interpola-\ntion/extrapolation capability over the classiï¬cation way.\n5.2.2. Ablation Studies\nWe analyze\nFEAT and its ablated variants on the\nMiniImageNet dataset with ConvNet backbone.\nHow does the embedding adaptation looks like qualita-\ntively? We sample four few-shot learning tasks and learn\na principal component analysis (PCA) model (that projects\nembeddings into 2-D space) using the instance embeddings\nof the test data. We then apply this learned PCA projection\nto both the support setâ€™s pre-adapted and post-adapted em-\nbeddings. The results are shown in Figure 1 (the beginning\nof the paper). In three out of four examples, post-adaptation\nembeddings of FEAT improve over the pre-adaption embed-\ndings. Interestingly, we found that the embedding adap-\ntation step of FEAT has the tendency of pushing the sup-\nport embeddings apart from the clutter, such that they can\nbetter ï¬t the test data of its categories. In the negative ex-\nample where post-adaptation degenerates the performances,\nwe observe that the embedding adaptation step has pushed\ntwo support embeddings â€œGolden Retrieverâ€ and â€œLionâ€ too\nclose to each other. It has qualitatively shown that the adap-\ntation is crucial to obtain superior performances and helps\nto contrast against task-agnostic embeddings.\n5.3. Extended Few-Shot Learning Tasks\nIn this section, we evaluate FEAT on 3 different few-shot\nlearning tasks. Speciï¬cally, cross-domain FSL, transductive\nFSL [30, 38], and generalized FSL [7]. We overview the\nsetups brieï¬‚y and please refer to SM for details.\nFS Domain Generalization assumes that examples in UN-\nSEEN support and test set can come from the different do-\nmains, e.g., sampled from different distributions [9, 19].\nThe example of this task can be found in Figure 4. It re-\nquires a model to recognize the intrinsic property than tex-\nture of objects, and is de facto analogical recognition.\nTransductive FSL. The key difference between standard\nand transductive FSL is whether test instances arrive one\nat a time or all simultaneously. The latter setup allows the\nstructure of unlabeled test instances to be utilized. There-\nfore, the prediction would depend on both the training (sup-\nport) instances and all the available test instances in the tar-\nget task from UNSEEN categories.\nGeneralized FSL. Prior works assumed the test instances\ncoming from unseen classes only. Different from them, the\ngeneralized FSL setting considers test instances from both\nSEEN and UNSEEN classes [37]. In other words, during the\nmodel evaluation, while support instances all come from U,\nthe test instances come from S âˆªU, and the classiï¬er is\nrequired to predict on both SEEN and UNSEEN categories. .\n5.3.1. Few-Shot Domain Generalization\nWe show that FEAT learns to adapt the intrinsic structure\nof tasks, and generalizes across domains, i.e., predicting\ntest instances even when the visual appearance is changed.\nSetups. We train the FSL model in the standard domain and\nevaluate with cross-domain tasks, where the N-categories\nare aligned but domains are different. In detail, a model is\n7\n\n\nC â†’C\nC â†’R\nSupervised\n34.38Â±0.16\n29.49Â±0.16\nProtoNet\n35.51Â±0.16\n29.47Â±0.16\nFEAT\n36.83Â±0.17\n30.89Â±0.17\n1-Shot\n5-Shot\nTPN [30]\n55.51\n69.86\nTEAM [34]\n56.57\n72.04\nFEAT\n57.04 Â± 0.20\n72.89 Â± 0.16\nSEEN\nUNSEEN\nCOMBINED\nRandom\n1.56 Â±0.00 20.00Â±0.00\n1.45Â±0.00\nProtoNet 41.73Â±0.03 48.64Â±0.20\n35.69Â±0.03\nFEAT\n43.94Â±0.03 49.72Â±0.20\n40.50Â±0.03\n(a) Few-shot domain generalization\n(b) Transductive few-shot learning\n(c) Generalized few-shot learning\nTable 4: We evaluate our model on three additional few-shot learning tasks: (a) Few-shot domain generalization, (b) Transductive few-shot\nlearning, and (c) Generalized few-shot learning. We observe that FEAT consistently outperform all previous methods or baselines.\nDrill\nBed\nTV\nFlower\nScrewdriver\nğ’Ÿğ’Ÿğ­ğ­ğ­ğ­ğ­ğ­ğ­ğ­ğ­ğ­from \nâ€œClipartâ€\nğ’Ÿğ’Ÿğ­ğ­ğ­ğ­ğ­ğ­ğ­ğ­from \nâ€œReal Worldâ€\nClassify\nTest Set\nTrain Set\nTrain Set\nTest Set\nğ’Ÿğ’Ÿğ­ğ­ğ­ğ­ğ­ğ­ğ­ğ­ğ­ğ­from \nâ€œClipartâ€\nğ’Ÿğ’Ÿğ­ğ­ğ­ğ­ğ­ğ­ğ­ğ­from \nâ€œReal Worldâ€\nBed\nCurtains\nRefrigerator\nSneakers\nDrill\nClassify\nFigure 4: Qualitative results of few-shot domain-generalization\nfor FEAT. Correctly classiï¬ed examples are shown in red boxes\nand incorrectly ones are shown in blue boxes. We visualize one\ntask that FEAT succeeds (top) and one that fails (bottom).\ntrained on tasks from the â€œClipartâ€ domain of Ofï¬ceHome\ndataset [48], then the model is required to generalize to both\nâ€œClipart (C)â€ andâ€œReal World (R)â€ test instances. In other\nwords, we need to classify complex real images by seeing\nonly a few sketches (Figure 4 gives an overview of data).\nResults. Table 4 (a) gives the quantitative results and Fig-\nure 4 qualitatively examines it. Here, the â€œsupervisedâ€ de-\nnotes a model trained with the standard classiï¬cation strat-\negy and then its penultimate layerâ€™s output feature is used\nas the nearest neighbor classiï¬er. We observe that ProtoNet\ncan outperform this baseline on tasks when evaluating in-\nstances from â€œClipartâ€ but not ones from â€œreal worldâ€.\nHowever, FEAT improves over â€œreal worldâ€ few-shot classi-\nï¬cation even only seeing the support data from â€œClipartâ€.\n5.3.2. Transductive Few-Shot Learning\nWe show that without additional efforts in modeling,\nFEAT outperforms existing methods in transductive FSL.\nSetups. We further study this semi-supervised learning set-\nting to see how well FEAT can incorporate test instances into\njoint embedding adaptation. Speciï¬cally, we use the unla-\nbeled test instances to augment the key and value sets of\nTransformer (refer to SM for details), so that the embedding\nadaptation takes relationship of all test instances into con-\nsideration. We evaluate this setting on the transductive pro-\ntocol of MiniImageNet [38]. With the adapted embedding,\nFEAT makes predictions based on Semi-ProtoNet [38].\nResults.\nWe compare with two previous approaches,\nTPN [30] and TEAM [34]. The results are shown in Ta-\nble 4 (b). We observe that FEAT improves its standard FSL\nperformance (refer to Table 1) and also outperforms previ-\nous semi-supervised approaches by a margin.\n5.3.3. Generalized Few-Shot Learning\nWe show that FEAT performs well on generalized few-\nshot classiï¬cation of both SEEN and UNSEEN classes.\nSetups. In this scenario, we evaluate not only on classi-\nfying test instances from a N-way M-shot task from UN-\nSEEN set U, but also on all available SEEN classes from S.\nTo do so, we hold out 150 instances from each of the 64\nseen classes in MiniImageNet for validation and evaluation.\nNext, given a 1-shot 5-way training set Dtrain, we consider\nthree evaluation protocols based on different class sets [7]:\nUNSEEN measures the mean accuracy on test instances only\nfrom U (5-Way few-shot classiï¬cation); SEEN measures the\nmean accuracy on test instances only from S (64-Way clas-\nsiï¬cation); COMBINED measures the mean accuracy on test\ninstances from S âˆªU (69-Way mixed classiï¬cation).\nResults.\nThe results can be found in Table 4 (c).\nWe\nobserve that again FEAT outperforms baseline ProtoNet.\nTo calibrate the prediction score on SEEN and UNSEEN\nclasses [7, 52], we select a constant seen/unseen class prob-\nability over the validation set, and subtract this calibration\nfactor from seen classesâ€™ prediction score. Then we take the\nprediction with maximum score value after calibration.\n6. Discussion\nA common embedding space fails to tailor discriminative\nvisual knowledge for a target task especially when there are\na few labeled training data. We propose to do embedding\nadaptation with a set-to-set function and instantiate it with\ntransformer (FEAT), which customizes task-speciï¬c embed-\nding spaces via a self-attention architecture. The adapted\nembedding space leverages the relationship between target\n8\n\n\ntask training instances, which leads to discriminative in-\nstance representations.\nFEAT achieves the state-of-the-art\nperformance on benchmarks, and its superiority can gener-\nalize to tasks like cross-domain, transductive, and general-\nized few-shot classiï¬cations.\nAcknowledgments.This work is partially supported by The\nNational Key R&D Program of China (2018YFB1004300),\nDARPA# FA8750-18-2-0117, NSF IIS-1065243, 1451412,\n1513966/ 1632803/1833137, 1208500, CCF-1139148, a\nGoogle Research Award, an Alfred P. Sloan Research Fel-\nlowship, ARO# W911NF-12-1-0241 and W911NF-15-1-\n0484, China Scholarship Council (CSC), NSFC (61773198,\n61773198, 61632004), and NSFC-NRF joint research\nproject 61861146001.\nReferences\n[1] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label-\nembedding for attribute-based classiï¬cation. In CVPR, pages\n819â€“826, 2013. 2\n[2] M. Andrychowicz, M. Denil, S. G. Colmenarejo, M. W.\nHoffman, D. Pfau, T. Schaul, and N. de Freitas. Learning\nto learn by gradient descent by gradient descent. In NIPS,\npages 3981â€“3989. 2016. 2\n[3] A. Antoniou, H. Edwards, and A. J. Storkey. How to train\nyour MAML. In ICLR, 2019. 2\n[4] L. J. Ba, R. Kiros, and G. E. Hinton. Layer normalization.\nCoRR, abs/1607.06450, 2016. 13\n[5] S. Changpinyo, W.-L. Chao, B. Gong, and F. Sha.\nSyn-\nthesized classiï¬ers for zero-shot learning. In CVPR, pages\n5327â€“5336, 2016. 2\n[6] S. Changpinyo, W.-L. Chao, and F. Sha. Predicting visual\nexemplars of unseen classes for zero-shot learning. In ICCV,\npages 3496â€“3505, 2017. 2\n[7] W.-L. Chao, S. Changpinyo, B. Gong, and F. Sha. An empir-\nical study and analysis of generalized zero-shot learning for\nobject recognition in the wild. In ECCV, pages 52â€“68, 2016.\n7, 8\n[8] W.-Y. Chen, Y.-C. Liu, Z. Kira, Y.-C. F. Wang, and J.-B.\nHuang. A closer look at few-shot classiï¬cation. In ICLR,\n2019. 5\n[9] N. Dong and E. P. Xing. Domain adaption in one-shot learn-\ning. In ECML PKDD, pages 573â€“588, 2018. 7\n[10] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-\nlearning for fast adaptation of deep networks.\nIn ICML,\npages 1126â€“1135, 2017. 1, 2, 3, 5, 6, 14, 15\n[11] G. Ghiasi, T.-Y. Lin, and Q. V. Le. Dropblock: A regulariza-\ntion method for convolutional networks. In NeurIPS, pages\n10750â€“10760. 2018. 13\n[12] S. Gidaris and N. Komodakis.\nDynamic few-shot visual\nlearning without forgetting.\nIn CVPR, pages 4367â€“4375,\n2018. 18\n[13] L.-Y. Gui, Y.-X. Wang, D. Ramanan, and J. M. F. Moura.\nFew-shot human motion prediction via meta-learning.\nIn\nECCV, pages 441â€“459, 2018. 2\n[14] B. Hariharan and R. B. Girshick. Low-shot visual recogni-\ntion by shrinking and hallucinating features. In ICCV, pages\n3037â€“3046, 2017. 18\n[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In CVPR, pages 770â€“778, 2016. 5,\n18\n[16] S. Hochreiter and J. Schmidhuber. Long short-term memory.\nNeural Computation, 9(8):1735â€“1780, 1997. 2, 4, 5, 11\n[17] K. Hsu, S. Levine, and C. Finn. Unsupervised learning via\nmeta-learning. In ICLR, 2019. 2\n[18] S. Ioffe and C. Szegedy. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift. In\nICML, pages 448â€“456, 2015. 13\n[19] B. Kang and J. Feng. Transferable meta learning across do-\nmains. In UAI, pages 177â€“187, 2018. 7\n[20] D. P. Kingma and J. Ba. Adam: A method for stochastic\noptimization. In ICLR, 2015. 5, 14\n[21] T. N. Kipf and M. Welling. Semi-supervised classiï¬cation\nwith graph convolutional networks. In ICLR, 2017. 2, 4, 5,\n12\n[22] G. Koch, R. Zemel, and R. Salakhutdinov.\nSiamese neu-\nral networks for one-shot image recognition. In ICML Deep\nLearning Workshop, volume 2, 2015. 2\n[23] B. M. Lake, R. Salakhutdinov, J. Gross, and J. B. Tenen-\nbaum.\nOne shot learning of simple visual concepts.\nIn\nCogSci, 2011. 1\n[24] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-\nlevel concept learning through probabilistic program induc-\ntion. Science, 350(6266):1332â€“1338, 2015. 1\n[25] K. Lee, S. Maji, A. Ravichandran, and S. Soatto.\nMeta-\nlearning with differentiable convex optimization. In CVPR,\npages 10657â€“10665, 2019. 2, 5, 6, 13, 15\n[26] Y. Lee and S. Choi.\nGradient-based meta-learning with\nlearned layerwise metric and subspace.\nIn ICML, pages\n2933â€“2942, 2018. 2\n[27] F.-F. Li, R. Fergus, and P. Perona. One-shot learning of ob-\nject categories. TPAMI, 28(4):594â€“611, 2006. 1\n[28] H. Li, D. Eigen, S. Dodge, M. Zeiler, and X. Wang. Find-\ning task-relevant features for few-shot learning by category\ntraversal. In CVPR, pages 1â€“10, 2019. 2, 6\n[29] Z. Lin, M. Feng, C. N. dos Santos, M. Yu, B. Xiang, B. Zhou,\nand Y. Bengio. A structured self-attentive sentence embed-\nding. In ICLR, 2017. 2, 4\n[30] Y. Liu, J. Lee, M. Park, S. Kim, E. Yang, S. J. Hwang, and\nY. Yang. Learning to propagate labels: Transductive propa-\ngation network for few-shot learning. In ICLR, 2019. 7, 8,\n17, 18\n[31] L. Metz, N. Maheswaranathan, B. Cheung, and J. Sohl-\nDickstein.\nLearning unsupervised learning rules.\nCoRR,\nabs/1804.00222, 2018. 2\n[32] A. Nichol, J. Achiam, and J. Schulman. On ï¬rst-order meta-\nlearning algorithms. CoRR, abs/1803.02999, 2018. 2\n[33] B. N. Oreshkin, P. R. LÂ´opez, and A. Lacoste. TADAM: task\ndependent adaptive metric for improved few-shot learning.\nIn NeurIPS, pages 719â€“729. 2018. 5, 6, 11, 15\n[34] L. Qiao, Y. Shi, J. Li, Y. Wang, T. Huang, and Y. Tian. Trans-\nductive episodic-wise adaptive metric for few-shot learning.\nIn ICCV, pages 3603â€“3612, 2019. 8, 17, 18\n[35] S. Qiao, C. Liu, W. Shen, and A. L. Yuille. Few-shot image\nrecognition by predicting parameters from activations.\nIn\nCVPR, pages 7229â€“7238, 2018. 2, 5, 6, 13, 14, 15\n[36] S. Ravi and H. Larochelle. Optimization as a model for few-\nshot learning. In ICLR, 2017. 2, 5\n9\n\n\n[37] M. Ren, R. Liao, E. Fetaya, and R. S. Zemel. Incremen-\ntal few-shot learning with attention attractor networks. In\nNeurIPS, pages 5276â€“5286, 2019. 7\n[38] M. Ren, E. Triantaï¬llou, S. Ravi, J. Snell, K. Swersky, J. B.\nTenenbaum, H. Larochelle, and R. S. Zemel. Meta-learning\nfor semi-supervised few-shot classiï¬cation. In ICLR, 2018.\n5, 6, 7, 8, 14, 17, 18\n[39] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein,\nA. C. Berg, and F.-F. Li. Imagenet large scale visual recog-\nnition challenge. IJCV, 115(3):211â€“252, 2015. 5, 18\n[40] A. A. Rusu, D. Rao, J. Sygnowski, O. Vinyals, R. Pascanu,\nS. Osindero, and R. Hadsell. Meta-learning with latent em-\nbedding optimization. In ICLR, 2019. 1, 2, 5, 13, 14, 15\n[41] V. G. Satorras and J. B. Estrach.\nFew-shot learning with\ngraph neural networks. In ICLR, 2018. 2, 4, 12\n[42] T. R. Scott, K. Ridgeway, and M. C. Mozer. Adapted deep\nembeddings: A synthesis of methods for k-shot inductive\ntransfer learning. In NeurIPS, pages 76â€“85. 2018. 2\n[43] J. Snell, K. Swersky, and R. S. Zemel. Prototypical networks\nfor few-shot learning. In NIPS, pages 4080â€“4090. 2017. 1,\n2, 3, 4, 5, 6, 11, 13, 15, 18\n[44] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov. Dropout: a simple way to prevent neural\nnetworks from overï¬tting. JMLR, 15(1):1929â€“1958, 2014.\n13\n[45] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. S. Torr, and\nT. M. Hospedales. Learning to compare: Relation network\nfor few-shot learning. In CVPR, pages 1199â€“1208, 2018. 2,\n6, 15\n[46] E. Triantaï¬llou, R. S. Zemel, and R. Urtasun.\nFew-shot\nlearning through an information retrieval lens.\nIn NIPS,\npages 2252â€“2262. 2017. 1, 2, 5, 13, 14\n[47] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all\nyou need. In NIPS, pages 6000â€“6010. 2017. 2, 4, 5, 12, 13,\n14, 16\n[48] H. Venkateswara, J. Eusebio, S. Chakraborty, and S. Pan-\nchanathan. Deep hashing network for unsupervised domain\nadaptation. In CVPR, pages 5385â€“5394, 2017. 5, 8, 14, 17\n[49] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and\nD. Wierstra. Matching networks for one shot learning. In\nNIPS, pages 3630â€“3638. 2016. 1, 2, 3, 4, 5, 6, 11, 12, 13,\n14, 15\n[50] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.\nThe Caltech-UCSD Birds-200-2011 Dataset. Technical Re-\nport CNS-TR-2011-001, California Institute of Technology,\n2011. 6, 14\n[51] Y. Wang, W.-L. Chao, K. Q. Weinberger, and L. van der\nMaaten.\nSimpleshot: Revisiting nearest-neighbor classiï¬-\ncation for few-shot learning. CoRR, abs/1911.04623, 2019.\n6, 14, 15\n[52] Y.-X. Wang, R. B. Girshick, M. Hebert, and B. Hariharan.\nLow-shot learning from imaginary data.\nIn CVPR, pages\n7278â€“7286, 2018. 8, 18\n[53] X.-S. Wei, P. Wang, L. Liu, C. Shen, and J. Wu.\nPiece-\nwise classiï¬er mappings: Learning ï¬ne-grained learners for\nnovel categories with few examples. TIP, 28(12):6116â€“6125,\n2019. 2\n[54] H.-J. Ye, H. Hu, D.-C. Zhan, and F. Sha.\nLearn-\ning embedding adaptation for few-shot learning.\nCoRR,\nabs/1812.03664, 2018. 13\n[55] S. Zagoruyko and N. Komodakis. Wide residual networks.\nIn BMVC, 2016. 5, 13\n[56] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. PÂ´oczos, R. R.\nSalakhutdinov, and A. J. Smola. Deep sets. In NIPS, pages\n3394â€“3404. 2017. 2, 4, 5, 6, 12\n10\n\n\nSupplementary Material\nA. Details of Baseline Methods\nIn this section, we describe two important embedding\nlearning baselines i.e., Matching Network (MatchNet) [49]\nand Prototypical Network (ProtoNet) [43], to implement the\nprediction function f(xtest; Dtrain) in the few-shot learn-\ning framework.\nMatchNet and ProtoNet.\nBoth MatchNet and ProtoNet\nstress the learning of the embedding function E from the\nsource task data DS with a meta-learning routine similar to\nAlg. 1 in the main text. We omit the super-script S since\nthe prediction strategies can apply to tasks from both SEEN\nand UNSEEN sets.\nGiven the training data Dtrain = {xi, yi}NM\ni=1 of an M-\nshot N-way classiï¬cation task, we can obtain the embed-\nding of each training instance based on the function E:1\nÏ†(xi) = E(xi), âˆ€xi âˆˆXtrain\n(8)\nTo classify a test instance xtest, we perform the nearest\nneighbor classiï¬cation , i.e.,\nË†ytest âˆexp\n\u0000Î³ Â· sim(Ï†xtest, Ï†xi)\n\u0001\nÂ· yi\n(9)\n=\nexp\n\u0000Î³ Â· sim(Ï†xtest , Ï†xi)\n\u0001\nP\nxiâ€²âˆˆXtrain exp\n\u0000Î³ Â· sim(Ï†xtest , Ï†xiâ€²)\n\u0001 Â· yi\n=\nX\n(xi,yi)âˆˆDtrain\nexp\n\u0000Î³ Â· sim(Ï†xtest , Ï†xi)\n\u0001\nP\nxiâ€²âˆˆXtrain exp\n\u0000Î³ Â· sim(Ï†xtest , Ï†xiâ€²)\n\u0001 Â· yi\nHere, MatchNet ï¬nds the most similar training instance to\nthe test one, and assigns the label of the nearest neigh-\nbor to the test instance. Note that sim represents the co-\nsine similarity, and Î³ > 0 is the scalar temperature value\nover the similarity score, which is found important em-\npirically [33]. During the experiments, we tune this tem-\nperature value carefully, ranging from the reciprocal of\n{0.1, 1, 16, 32, 64, 128}.2\nThe ProtoNet has two key differences compared with the\nMatchNet. First, when M > 1 in the target task, ProtoNet\ncomputes the mean of the same class embeddings as the\nclass center (prototype) in advance and classiï¬es a test in-\nstance by computing its similarity to the nearest class center\n(prototype). In addition, it uses the negative distance rather\n1In the following, we use Ï†(xi) and Ï†xi exchangeably to represent the\nembedding of an instance xi based on the mapping Ï†.\n2In experiments, we ï¬nd the temperature scale over logits inï¬‚uences\nthe model training a lot when we optimize based on pre-trained weights.\nthan the cosine similarity as the similarity metric:\ncn = 1\nM\nX\nyi=n\nÏ†(xi), âˆ€n = 1, . . . , N\n(10)\nË†ytest âˆexp\n\u0000Î³ Â· âˆ¥Ï†xtest âˆ’cnâˆ¥2\n2\n\u0001\nÂ· yn\n=\nN\nX\nn=1\nexp\n\u0000âˆ’Î³âˆ¥Ï†xtest âˆ’cnâˆ¥2\n2\n\u0001\nPN\nnâ€²=1 exp\n\u0000âˆ’Î³âˆ¥Ï†xtest âˆ’cnâ€²âˆ¥2\n2\n\u0001yn\n(11)\nSimilar to the aforementioned scalar temperature for Match-\nNet, in Eq. 11 we also consider the scale Î³. Here we abuse\nthe notation by using yi = n to enumerate the instances\nwith label n, and denote yn as the one-hot coding of the\nn-th class. Thus Eq. 11 outputs the probability to classify\nxtest to the N classes.\nIn the experiments, we ï¬nd ProtoNet incorporates bet-\nter with FEAT. When there is more than one shot in each\nclass, we average all instances per class in advance by\nEq. 10 before inputting them to the set-to-set transforma-\ntion. This pre-average manner makes more precise embed-\nding for each class and facilitates the â€œdownstreamâ€ em-\nbedding adaptation. We will validate this in the additional\nexperiments.\nB. Details of the Set-to-Set Functions\nIn this section, we provide details about four imple-\nmentations of the set-to-set embedding adaptation function\nT, i.e., the BILSTM, DEEPSETS, GCN, and the TRANS-\nFORMER. The last one is the key component in our Few-\nshot Embedding Adaptation with Transformer (FEAT) ap-\nproach. Then we will introduce the conï¬guration of the\nmulti-layer/multi-head transformer, and the setup of the\ntransformer for the transductive Few-Shot Learning (FSL).\nB.1. BiLSTM as the Set-to-Set Transformation\nBidirectional LSTM (BILSTM) [16, 49] is one of the\ncommon choice to instantiate the set-to-set transformation,\nwhere the addition between the input and the hidden layer\noutputs of each BILSTM cell leads to the adapted embed-\nding. In detail, we have\n{\nâƒ—\nÏ†(x), âƒ—Ï†(x)} = BILSTM({Ï†(x)}); âˆ€x âˆˆXtrain\n(12)\nWhere\nâƒ—\nÏ†(x) and âƒ—Ï†(x) are the hidden layer outputs of the\ntwo LSTM models for each instance embedding in the input\nset. Then we get the adapted embedding as\nÏˆ(x) = Ï†(x) +\nâƒ—\nÏ†(x) + âƒ—Ï†(x)\n(13)\nIt is notable that the output of the BILSTM suppose to de-\npend on the order of the input set. Vinyals et al. [49] pro-\npose to use the Fully Conditional Embedding to encode\nthe context of both the test instance and the support set\n11\n\n\nClassification \nScores\nEmbedding \nAdaptation\nCNN\nCNN\nCNN\nCNN\nSoft Nearest \nNeighbor\nSet-to-Set Function\n(a) Embedding Adaptation\n(b) Transformer as the Set-to-Set Function\n(c) DeepSets as Set-to-Set Function\nLayer Norm\nFC\nFC\nFC\nFC\nFC\nFC\nFC\nFC\nFC\nFC\nFC\nSUM\nCAT\nScaled Dot\nProduct\nTrain Instance\nTest Instance\nTask Agnostic\nEmbedding\nTask Specific \nEmbedding\nFigure 5: Illustration of two embedding adpatation methods considered in the paper. (a) shows the main ï¬‚ow of Few-Shot Embedding\nAdaptation, while (b) and (c) demonstrate the workï¬‚ow of Transformer and DeepSets respectively.\ninstances based on BILSTM and LSTM w/ Attention mod-\nule. Different from [49], we apply the set-to-set embedding\nadaptation only over the support set, which leads to a fully\ninductive learning setting.\nB.2. DeepSets as the Set-to-Set Transformation\nDeep sets [56] suggests a generic aggregation function\nover a set should be the transformed sum of all elements in\nthis set. Therefore, a very simple set-to-set transformation\nbaseline involves two components, an instance centric rep-\nresentation combined with a set context representation. For\nany instance x âˆˆXtrain, we deï¬ne its complementary set\nas xâˆ. Then we implement the set transformation by:\nÏˆ(x) = Ï†(x) + g([Ï†(x);\nX\nxiâ€²âˆˆxâˆ\nh(Ï†(xiâ€²))])\n(14)\nIn Eq. 14, g and h are transformations which map the em-\nbedding into another space and increase the representation\nability of the embedding. Two-layer multi-layer perception\n(MLP) with ReLU activation is used to implement these two\nmappings. For each instance, embeddings in its comple-\nmentary set are ï¬rst combined into a vector as the context,\nand then this vector is concatenated with the input embed-\nding to obtain the residual component of the adapted em-\nbedding. This conditioned embedding takes other instances\nin the set into consideration, and keeps the â€œset (permutation\ninvariant)â€ property. Finally, we determine the label with\nthe newly adapted embedding Ïˆ as Eq. 11. An illustration\nof the DeepSets notation in the embedding adaptation can\nbe found in Figure 5 (c). The summation operator in Eq. 14\ncould also be replaced as the maximum operator, and we\nï¬nd the maximum operator works better than summation\noperator in our experiments.\nB.3. GCN as the Set-to-Set Transformation\nGraph Convolutional Networks (GCN) [21, 41] propa-\ngate the relationship between instances in the set. We ï¬rst\nconstruct a degree matrix A âˆˆRNKÃ—NK to represent the\nsimilarity between instances in a set. If two instances xi and\nxj come from the same class, then we set the corresponding\nelement Aij in A to 1, otherwise we have Aij = 0. Based\non A, we build the â€œnormalizedâ€ adjacency matrix S for a\ngiven set with added self-loops S = Dâˆ’1\n2 (A + I)Dâˆ’1\n2 .\nI âˆˆRNKÃ—NK is the identity matrix, and D is the diagonal\nmatrix whose elements are equal to the sum of elements in\nthe corresponding row of A+I, i.e., Dii = P\nj Aij +1 and\nDij = 0 if i Ì¸= j. Let Î¦0 = {Ï†x ; âˆ€x âˆˆXtrain} be the\nconcatenation of all the instance embeddings in the training\nset Xtrain. We use the super-script to denote the generation\nof the instance embedding matrix. The relationship between\ninstances could be propagated based on S, i.e.,\nÎ¦t+1 = ReLU(SÎ¦tW) , t = 0, 1, . . . , T âˆ’1\n(15)\nW is a learned a projection matrix for feature transforma-\ntion. In GCN, the embedding in the set is transformed based\non Eq. 15 multiple times (we propagate the embedding set\ntwo times during the experiments), and the ï¬nal propagated\nembedding set Î¦T gives rise to the Ïˆx.\nB.4. Transformer as the Set-to-Set Transformation\nIn this section, we describe in details about our Few-Shot\nEmbedding Adaptation w/ Transformer (FEAT) approach,\nspeciï¬cally how to use the transformer architecture [47] to\nimplement the set-to-set function T, where self-attention\nmechanism facilitates the instance embedding adaptation\nwith consideration of the contextual embeddings.\nAs mentioned before, the transformer is a store of triplets\nin the form of (query, key, and value). Elements in the query\nset are the ones we want to do the transformation. The trans-\nformer ï¬rst matches a query point with each of the keys by\ncomputing the â€œqueryâ€ â€“ â€œkeyâ€ similarities. Then the prox-\nimity of the key to the query point is used to weight the\ncorresponding values of each key. The transformed input\nacts as a residual value which will be added to the input.\n12\n\n\nBasic Transformer.\nFollowing the deï¬nitions in [47], we\nuse Q, K, and V to denote the set of the query, keys, and\nvalues, respectively. All these sets are implemented by dif-\nferent combinations of task instances.\nTo increase the ï¬‚exibility of the transformer, three sets\nof linear projections (WQ âˆˆRdÃ—dâ€², WK âˆˆRdÃ—dâ€², and\nWV âˆˆRdÃ—dâ€²) are deï¬ned, one for each set.3 The points in\nsets are ï¬rst projected by the corresponding projections\nQ = W âŠ¤\nQ\n\u0002\nÏ†xq;\nâˆ€xq âˆˆQ\n\u0003\nâˆˆRdâ€²Ã—|Q|\nK = W âŠ¤\nK\n\u0002\nÏ†xk;\nâˆ€xk âˆˆK\n\u0003\nâˆˆRdâ€²Ã—|K|\nV = W âŠ¤\nV\n\u0002\nÏ†xv;\nâˆ€xv âˆˆV\n\u0003\nâˆˆRdâ€²Ã—|V|\n(16)\n|Q|, |K|, and |V| are the number of elements in the sets\nQ, K, and V respectively. Since there is a one-to-one corre-\nspondence between elements in K and V we have |K| = |V|.\nThe similarity between a query point xq âˆˆQ and the list\nof keys K is then computed as â€œattentionâ€:\nÎ±qk âˆexp\n \nÏ†âŠ¤\nxqWQ Â· K\nâˆš\nd\n!\n; âˆ€xk âˆˆK\n(17)\nÎ±q,: = softmax\n \nÏ†âŠ¤\nxqWQ Â· K\nâˆš\nd\n!\nâˆˆR|K|\n(18)\nThe k-th element Î±qk in the vector Î±q,: reveals the particu-\nlar proximity between xk and xq. The computed attention\nvalues are then used as weights for the ï¬nal embedding xq:\nËœÏˆxq =\nX\nk\nÎ±qkV:,k\n(19)\nÏˆxq = Ï„\n\u0000Ï†xq + W âŠ¤\nFC ËœÏˆxq\n\u0001\n(20)\nV:,k is the k-th column of V .\nWFC âˆˆRdâ€²Ã—d is the\nprojection weights of a fully connected layer.\nÏ„ com-\npletes a further transformation, which is implemented by\nthe dropout [44] and layer normalization [4]. The whole\nï¬‚ow of transformer in our FEAT approach can be found in\nFigure 5 (b). With the help of transformer, the embeddings\nof all training set instances are adapted (we denote this ap-\nproach as FEAT).\nMulti-Head Multi-Layer Transformer.\nFollowing [47],\nan extended version of the transformer can be built with\nmultiple parallel attention heads and stacked layers. As-\nsume there are totally H heads, the transformer concate-\nnates multiple attention-transformed embeddings, and then\nuses a linear mapping to project the embedding to the orig-\ninal embedding space (with the original dimensionality).\nBesides, we can take the transformer as a feature encoder\nof the input query instance. Therefore, it can be applied\n3For notation simplicity, we omit the bias in the linear projection here.\nover the input query multiple times (with different sets of\nparameters), which gives rise to the multi-layer transformer.\nWe discuss the empirical performances with respect to the\nchange number of heads and layers in Â§ D.\nB.5. Extension to transductive FSL\nFacilitated by the ï¬‚exible set-to-set transformer in\nEq. 20, our adaptation approach can naturally be extended\nto the transductive FSL setting.\nWhen classifying test instance xtest in the transdutive\nscenario, other test instances Xtest from the N categories\nwould also be available. Therefore, we enrich the trans-\nformerâ€™s query and key/value sets\nQ = K = V = Xtrain âˆªXtest\n(21)\nIn this manner, the embedding adaptation procedure would\nalso consider the structure among unlabeled test instances.\nWhen the number of shots K > 1, we average the embed-\nding of labeled instances in each class ï¬rst before combin-\ning them with the test set embeddings.\nC. Implementation Details\nBackbone architecture.\nWe consider three backbones, as\nsuggested in the literature, as the instance embedding func-\ntion E for the purpose of fair comparisons. We resize the\ninput image to 84 Ã— 84 Ã— 3 before using the backbones.\nâ€¢ ConvNet. The 4-layer convolution network [43, 46, 49]\ncontains 4 repeated blocks. In each block, there is a con-\nvolutional layer with 3 Ã— 3 kernel, a Batch Normalization\nlayer [18], a ReLU, and a Max pooling with size 2. We\nset the number of convolutional channels in each block as\n64. A bit different from the literature, we add a global\nmax pooling layer at last to reduce the dimension of the\nembedding. Based on the empirical observations, this will\nnot inï¬‚uence the results, but reduces the computation bur-\nden of later transformations a lot.\nâ€¢ ResNet. We use the 12-layer residual network in [25].4\nThe DropBlock [11] is used in this ResNet architecture\nto avoid over-ï¬tting. A bit different from the ResNet-12\nin [25], we apply a global average pooling after the ï¬nal\nlayer, which leads to 640 dimensional embeddings.5\nâ€¢ WRN. We also consider the Wide residual network [40,\n55].\nWe use the WRN-28-10 structure as in [35, 40],\nwhich sets the depth to 28 and width to 10. After a global\n4The source code of the ResNet is publicly available on https://\ngithub.com/kjunelee/MetaOptNet\n5We use the ResNet backbone with input image size 80 Ã— 80 Ã—\n3 from [35] in the old version of our paper [54], whose source\ncode of ResNet is publicly available on https://github.com/\njoe-siyuan-qiao/FewShot-CVPR.\nEmpirically we ï¬nd the\nResNet-12 [25] works better than our old ResNet architecture.\n13\n\n\naverage pooling in the last layer of the backbone, we get\na 640 dimensional embedding for further prediction.\nDatasets.\nFour\ndatasets,\nMiniImageNet\n[49],\nTieredImageNet\n[38],\nCaltech-UCSD\nBirds\n(CUB)\n200-2011 [50], and Ofï¬ceHome [48] are investigated in\nthis paper. Each dataset is split into three parts based on\ndifferent non-overlapping sets of classes, for model training\n(a.k.a.\nmeta-training in the literature), model validation\n(a.k.a.\nmeta-val in the literature), and model evaluation\n(a.k.a.\nmeta-test in the literature).\nThe CUB dataset is\ninitially designed for ï¬ne-grained classiï¬cation. It contains\nin total 11,788 images of birds over 200 species. On CUB,\nwe randomly sampled 100 species as SEEN classes, another\ntwo 50 species are used as two UNSEEN sets for model\nvalidation and evaluation [46]. For all images in the CUB\ndataset, we use the provided bounding box to crop the\nimages as a pre-processing [46].\nBefore input into the\nbackbone network, all images in the dataset are resized\nbased on the requirement of the network.\nPre-training strategy.\nAs mentioned before, we apply an\nadditional pre-training strategy as suggested in [35, 40].\nThe backbone network, appended with a softmax layer, is\ntrained to classify all classes in the SEEN class split (e.g., 64\nclasses in the MiniImageNet) with the cross-entropy loss.\nIn this stage, we apply image augmentations like random\ncrop, color jittering, and random ï¬‚ip to increase the gen-\neralization ability of the model. After each epoch, we val-\nidate the performance of the pre-trained weights based on\nits few-shot classiï¬cation performance on the model vali-\ndation split. Speciï¬cally, we randomly sample 200 1-shot\nN-way few-shot learning tasks (N equals the number of\nclasses in the validation split, e.g., 16 in the MiniImageNet),\nwhich contains 1 instance per class in the support set and\n15 instances per class for evaluation. Based on the penulti-\nmate layer instance embeddings of the pre-trained weights,\nwe utilize the nearest neighbor classiï¬ers over the few-shot\ntasks and evaluate the quality of the backbone. We select\nthe pre-trained weights with the best few-shot classiï¬ca-\ntion accuracy on the validation set. The pre-trained weights\nare used to initialize the embedding backbone E, and the\nweights of the whole model are then optimized together dur-\ning the model training.\nTransformer Hyper-parameters.\nWe follow the archi-\ntecture as presented in [47] to build our FEAT model.\nThe hidden dimension dâ€² for the linear transformation in\nour FEAT model is set to 64 for ConvNet and 640 for\nResNet/WRN. The dropout rate in transformer is set as 0.5.\nWe empirically observed that the shallow transformer (with\none set of projection and one stacked layer) gives the best\noverall performance (also studied in Â§ D.2).\nOptimization.\nFollowing the literature, different optimiz-\ners are used for the backbones during the model training.\nFor the ConvNet backbone, stochastic gradient descent with\nAdam [20] optimizer is employed, with the initial learning\nrate set to be 0.002. For the ResNet and WRN backbones,\nvanilla stochastic gradient descent with Nesterov accelera-\ntion is used with an initial rate of 0.001. We ï¬x the weight\ndecay in SGD as 5e-4 and momentum as 0.9. The sched-\nule of the optimizers is tuned over the validation part of\nthe dataset. As the backbone network is initialized with the\npre-trained weights, we scale the learning rate for those pa-\nrameters by 0.1.\nD. Additional Experimental Results\nIn this section, we will show more experimental results\nover the MiniImageNet/CUB dataset, the ablation studies,\nand the extended few-shot learning.\nD.1. Main Results\nThe full results of all methods on the MiniImageNet can\nbe found in Table 5. The results of MAML [10] optimized\nover the pre-trained embedding network are also included.\nWe re-implement the ConvNet backbone of MAML and cite\nthe MAML results over the ResNet backbone from [40]. It\nis also noteworthy that the FEAT gets the best performance\namong all popular methods and baselines.\nWe also investigate the Wide ResNet (WRN) back-\nbone over MiniImageNet, which is also the popular one\nused in [35, 40].\nSimpleShot [51] is a recent proposed\nembedding-based few-shot learning approach that takes full\nadvantage of the pre-trained embeddings. We cite the re-\nsults of PFA [35], LEO [40], and SimpleShot [51] from\ntheir papers. The results can be found in Table 6. We re-\nimplement ProtoNet and our FEAT approach with WRN.\nIt is notable that in this case, our FEAT achieves much\nhigher promising results than the current state-of-the-art\napproaches. Table 7 shows the classiï¬cation results with\nWRN on the TieredImageNet data set, where our FEAT still\nkeeps its superiority when dealing with 1-shot tasks.\nTable 8 shows the 5-way 1-shot and 5-shot classiï¬cation\nresults on the CUB dataset based on the ConvNet back-\nbone. The results on CUB are consistent with the trend\non the MiniImageNet dataset. Embedding adaptation in-\ndeed assists the embedding encoder for the few-shot clas-\nsiï¬cation tasks.\nFacilitated by the set function property,\nthe DEEPSETS works better than the BILSTM counterpart.\nAmong all the results, the transformer based FEAT gets the\ntop tier results.\nD.2. Ablation Studies\nIn this section, we perform further analyses for our pro-\nposed FEAT and its ablated variants classifying in the Pro-\n14\n\n\nTable 5: Few-shot classiï¬cation accuracyÂ± 95% conï¬dence interval on MiniImageNet with ConvNet and ResNet backbones. Our imple-\nmentation methods are measured over 10,000 test trials.\nSetups â†’\n1-Shot 5-Way\n5-Shot 5-Way\nBackbone Network â†’\nConvNet\nResNet\nConvNet\nResNet\nMatchNet [49]\n43.40Â± 0.78\n-\n51.09Â± 0.71\n-\nMAML [10]\n48.70Â± 1.84\n-\n63.11Â± 0.92\n-\nProtoNet [43]\n49.42Â± 0.78\n-\n68.20Â± 0.66\n-\nRelationNet [45]\n51.38Â± 0.82\n-\n67.07Â± 0.69\n-\nPFA [35]\n54.53Â± 0.40\n-\n67.87Â± 0.20\n-\nTADAM [33]\n-\n58.50Â± 0.30\n-\n76.70Â± 0.30\nMetaOptNet [25]\n-\n62.64Â± 0.61\n-\n78.63Â± 0.46\nBaselines\nMAML\n49.24Â± 0.21\n58.05Â± 0.10\n67.92Â± 0.17\n72.41Â± 0.20\nMatchNet\n52.87Â± 0.20\n65.64Â± 0.20\n67.49Â± 0.17\n78.72Â± 0.15\nProtoNet\n52.61Â± 0.20\n62.39Â± 0.21\n71.33Â± 0.16\n80.53Â± 0.14\nEmbedding Adaptation\nBILSTM\n52.13Â± 0.20\n63.90Â± 0.21\n69.15Â± 0.16\n80.63Â± 0.14\nDEEPSETS\n54.41Â± 0.20\n64.14Â± 0.22\n70.96Â± 0.16\n80.93Â± 0.14\nGCN\n53.25Â± 0.20\n64.50Â± 0.20\n70.59Â± 0.16\n81.65Â± 0.14\nOurs: FEAT\n55.15Â± 0.20\n66.78Â± 0.20\n71.61Â± 0.16\n82.05Â± 0.14\nTable 6:\nFew-shot classiï¬cation performance with Wide\nResNet (WRN)-28-10 backbone on MiniImageNet dataset (mean\naccuracyÂ±95% conï¬dence interval). Our implementation meth-\nods are measured over 10,000 test trials.\nSetups â†’\n1-Shot 5-Way\n5-Shot 5-Way\nPFA [35]\n59.60Â± 0.41\n73.74Â± 0.19\nLEO [40]\n61.76Â± 0.08\n77.59Â± 0.12\nSimpleShot [51]\n63.50Â± 0.20\n80.33Â± 0.14\nProtoNet (Ours)\n62.60Â± 0.20\n79.97Â± 0.14\nOurs: FEAT\n65.10 Â± 0.20\n81.11 Â± 0.14\nTable 7: Few-shot classiï¬cation performance with Wide ResNet\n(WRN)-28-10 backbone on TieredImageNet dataset (mean\naccuracyÂ±95% conï¬dence interval). Our implementation meth-\nods are measured over 10,000 test trials.\nSetups â†’\n1-Shot 5-Way\n5-Shot 5-Way\nLEO [40]\n66.33Â± 0.05\n81.44Â± 0.09\nSimpleShot [51]\n69.75Â± 0.20\n85.31Â± 0.15\nOurs: FEAT\n70.41 Â± 0.23\n84.38 Â± 0.16\ntoNet manner, on the MiniImageNet dataset, using the Con-\nvNet as the backbone network.\nDo the adapted embeddings improve the pre-adapted\nembeddings?\nWe report few-shot classiï¬cation results by\nTable 8: Few-shot classiï¬cation performance with ConvNet back-\nbone on CUB dataset (mean accuracyÂ±95% conï¬dence interval).\nOur implementation methods are measured over 10,000 test trials.\nSetups â†’\n1-Shot 5-Way\n5-Shot 5-Way\nMatchNet [49]\n61.16 Â± 0.89\n72.86 Â± 0.70\nMAML [10]\n55.92 Â± 0.95\n72.09 Â± 0.76\nProtoNet [43]\n51.31 Â± 0.91\n70.77 Â± 0.69\nRelationNet [45]\n62.45 Â± 0.98\n76.11 Â± 0.69\nInstance Embedding\nMatchNet\n67.73 Â± 0.23\n79.00 Â± 0.16\nProtoNet\n63.72 Â± 0.22\n81.50 Â± 0.15\nEmbedding Adaptation\nBILSTM\n62.05 Â± 0.23\n73.51 Â± 0.19\nDEEPSETS\n67.22 Â± 0.23\n79.65 Â± 0.16\nGCN\n67.83 Â± 0.23\n80.26 Â± 0.15\nOurs: FEAT\n68.87 Â± 0.22\n82.90 Â± 0.15\nTable 9: Ablation studies on whether the embedding adaptation\nimproves the discerning quality of the embeddings. After embed-\nding adaptation, FEAT improves w.r.t. the before-adaptation em-\nbeddings a lot for Few-shot classiï¬cation.\n1-Shot 5-Way\n5-Shot 5-Way\nPre-Adapt\n51.60Â± 0.20\n70.40Â± 0.16\nPost-Adapt\n55.15Â± 0.20\n71.61Â± 0.16\n15\n\n\n5\n10\n15\n20\nNumber of categories per task\n30\n35\n40\n45\n50\n55\n60\n65\n70\n75\nMean accuracy (in %)\n70.7\n55.9\n47.5\n41.9\n71.3\n56.5\n48.2\n42.4\n71.5\n57.0\n48.8\n43.2\nMethods\nBILSTM\nDeepSets\nFEAT\n(a) Task Interpolation\n5\n10\n15\n20\nNumber of categories per task\n30\n35\n40\n45\n50\n55\n60\n65\n70\n75\n69.2\n52.9\n43.6\n37.5\n71.0\n55.9\n47.4\n41.8\n71.2\n56.7\n48.2\n42.6\nMethods\nBILSTM\nDeepSets\nFEAT\n(b) Task Extrapolation\nFigure 6: Interpolation and Extrapolation of few-shot tasks. We\ntrain different embedding adaptation models on 5-shot 20-way or\n5-way classiï¬cation tasks and evaluate models on unseen tasks\nwith different number of classes (N={5, 10, 15, 20}). It veri-\nï¬es both the interpolation and extrapolation ability of FEAT on a\nvarying number of ways in few-shot classiï¬cation.\nTable 10: Ablation studies on the position to average the same-\nclass embeddings when there are multiple shots per class in FEAT\n(tested on the 5-Way tasks with different numbers of shots). â€œPre-\nAvgâ€ and â€œPost-Avgâ€ means we get the embedding center for each\nclass before or after the set-to-set transformation, respectively.\nSetups â†’\nPre-Avg\nPost-Avg\n5\n71.61Â± 0.16\n70.70Â± 0.16\n15\n77.76Â± 0.14\n76.58Â± 0.14\n30\n79.66Â± 0.13\n78.77Â± 0.13\nTable 11: Ablation studies on the number of heads in the Trans-\nformer of FEAT (with number of layers ï¬xes to one).\nSetups â†’\n1-Shot 5-Way\n5-Shot 5-Way\n1\n55.15Â± 0.20\n71.57Â± 0.16\n2\n54.91Â± 0.20\n71.44Â± 0.16\n4\n55.05Â± 0.20\n71.63Â± 0.16\n8\n55.22Â± 0.20\n71.39Â± 0.16\nTable 12: Ablation studies on the number of layers in the Trans-\nformer of FEAT (with number of heads ï¬xes to one).\nSetups â†’\n1-Shot 5-Way\n5-Shot 5-Way\n1\n55.15Â± 0.20\n71.57Â± 0.16\n2\n55.42Â± 0.20\n71.44Â± 0.16\n3\n54.96Â± 0.20\n71.63Â± 0.16\nusing the pre-adapted embeddings of support data (i.e., the\nembedding before adaptation), against those using adapted\nembeddings, for constructing classiï¬ers. Table 9 shows that\ntask-speciï¬c embeddings after adaptation improves over\ntask-agnostic embeddings in few-shot classiï¬cations.\nCan FEAT possesses the characteristic of the set func-\ntion?\nWe test three set-to-set transformation implementa-\ntions, namely the BILSTM, the DEEPSETS, and the Trans-\nformer (FEAT), w.r.t. two important properties of the set\nfunction, i.e., task interpolation and task extrapolation. In\nparticular, the few-shot learning model is ï¬rst trained with\n5-shot 20-way tasks. Then the learned model is required\nto evaluate different 5-shot tasks with N = {5, 10, 15, 20}\n(Extrapolation). Similarly, for interpolation, the model is\ntrained with 5-shot 20-way tasks in advance and then eval-\nuated on the previous multi-way tasks. The classiï¬cation\nchange results can be found in Figure 6 (a) and (b). BILSTM\ncannot deal with the size change of the set, especially in the\ntask extrapolation. In both cases, FEAT still gets improve-\nments in all conï¬gurations of N.\nWhen to average the same-class embeddings?\nWhen\nthere is more than one instance per class, i.e. M > 1, we av-\nerage the instances in the same class and use the class center\nto make predictions as in Eq. 10. There are two positions\nto construct the prototypes in FEAT â€” before the set-to-set\ntransformation (Pre-Avg) and after the set-to-set transfor-\nmation (Post-Avg). In Pre-Avg, we adapt the embeddings\nof the centers, and a test instance is predicted based on\nits distance to the nearest adapted center; while in Post-\nAvg, the instance embeddings are adapted by the set-to-set\nfunction ï¬rst, and the class centers are computed based on\nthe adapted instance embeddings. We investigate the two\nchoices in Table 10, where we ï¬x the number of ways to\n5 (N = 5) and change the number of shots (M) among\n{5, 15, 30}. The results demonstrate the Pre-Avg version\nperforms better than the Post-Avg in all cases, which shows\na more precise input of the set-to-set function by averaging\nthe instances in the same class leads to better results. So we\nuse the Pre-Avg strategy as a default option in our experi-\nments.\nWill deeper and multi-head transformer help?\nIn our\ncurrent implementation of the set-to-set transformation\nfunction, we make use of a shallow and simple transformer,\ni.e., one layer and one head (set of projection). From [47],\nthe transformer can be equipped with complex components\nusing multiple heads and deeper stacked layers. We evalu-\nate this augmented structure, with the number of attention\nheads increases to 2, 4, 8, as well as with the number of\nlayers increases to 2 and 3. As in Table 11 and Table 12,\nwe empirically observe that more complicated structures do\nnot result in improved performance. We ï¬nd that with more\nlayers of transformer stacked, the difï¬culty of optimization\nincreases and it becomes harder to train models until their\nconvergence. Whilst for models with more heads, the mod-\nels seem to over-ï¬t heavily on the training data, even with\nthe usage of auxiliary loss term (like the contrastive loss in\n16\n\n\nTable 13: Ablation studies on effects of the contrastive learning\nof the set-to-set function on FEAT.\nSetups â†’\n1-Shot 5-Way\n5-Shot 5-Way\nÎ» = 10\n53.92 Â± 0.20\n70.41 Â± 0.16\nÎ» = 1\n54.84 Â± 0.20\n71.00 Â± 0.16\nÎ» = 0.1\n55.15 Â± 0.20\n71.61 Â± 0.16\nÎ» = 0.01\n54.67 Â± 0.20\n71.26 Â± 0.16\nTable 14: Ablation studies on the prediction strategy (with cosine\nsimilarity or euclidean distance) of FEAT.\nSetups â†’\n1-Shot 5-Way\n5-Shot 5-Way\nBackbone â†’\nConvNet\nResNet\nConvNet\nResNet\nCosine Similarity-based Prediction\nFEAT\n54.64Â± 0.20\n66.26Â± 0.20\n71.72Â± 0.16\n81.83Â± 0.15\nEuclidean Distance-based Prediction\nFEAT\n55.15Â± 0.20\n66.78Â± 0.20\n71.61Â± 0.16\n82.05Â± 0.14\nour approach). It might require some careful regularizations\nto prevent over-ï¬tting, which we leave for future work.\nThe effectiveness of contrastive loss.\nTable 13 show the\nfew-shot classiï¬cation results with different weight values\n(Î») of the contrastive loss term for FEAT. From the results,\nwe can ï¬nd that the balance of the contrastive term in the\nlearning objective can inï¬‚uence the ï¬nal results. Empiri-\ncally, we set Î» = 0.1 in our experiments.\nThe inï¬‚uence of the prediction strategy.\nWe investi-\ngate two embedding-based prediction ways for the few-shot\nclassiï¬cation, i.e., based on the cosine similarity and the\nnegative euclidean distance to measure the relationship be-\ntween objects, respectively. We compare these two choices\nin Table 14.\nTwo strategies in Table 14 only differ in\ntheir similarity measures. In other words, with more than\none shot per class in the task training set, we average the\nsame class embeddings ï¬rst, and then make classiï¬cation\nby computing the cosine similarity or the negative euclidean\ndistance between a test instance and a class prototype. Dur-\ning the optimization, we tune the logits scale temperature\nfor both these methods. We ï¬nd that using the euclidean\ndistance usually requires small temperatures (e.g., Î³ =\n1\n64)\nwhile a large temperature (e.g., Î³ = 1) works well with the\nnormalized cosine similarity. The former choice achieves a\nslightly better performance than the latter one.\nD.3. Few-Shot Domain Generalization\nWe show that FEAT learns to adapt the intrinsic structure\nof tasks, and generalize across domains, i.e., predicting\ntest instances even when the visual appearance is changed.\nTable 15: Cross-Domain 1-shot 5-way classiï¬cation results of the\nFEAT approach.\nC â†’C\nC â†’R\nR â†’R\nSupervised\n34.38Â±0.16\n29.49Â±0.16\n37.43Â±0.16\nProtoNet\n35.51Â±0.16\n29.47Â±0.16\n37.24Â±0.16\nFEAT\n36.83Â±0.17\n30.89Â±0.17\n38.49Â±0.16\nTable 16: Results of models for transductive FSL with ConvNet\nbackbone on MiniImageNet. We cite the results of Semi-ProtoNet\nand TPN from [38] and [34] respectively. For TEAM [34], the au-\nthors do not report the conï¬dence intervals, so we set them to 0.00\nin the table.\nFEATâ€  and FEATâ€¡ adapt embeddings with the joint\nset of labeled training and unlabeled test instances, while make\nprediction via ProtoNet and Semi-ProtoNet respectively.\nSetups â†’\n1-Shot 5-Way\n5-Shot 5-Way\nStandard\nProtoNet\n52.61 Â± 0.20\n71.33 Â± 0.16\nFEAT\n55.15 Â± 0.20\n71.61 Â± 0.16\nTransductive\nSemi-ProtoNet [38]\n50.41 Â± 0.31\n64.39 Â± 0.24\nTPN [30]\n55.51 Â± 0.84\n69.86 Â± 0.67\nTEAM [34]\n56.57 Â± 0.00\n72.04 Â± 0.00\nSemi-ProtoNet (Ours)\n55.50 Â± 0.10\n71.76 Â± 0.08\nFEATâ€ \n56.49 Â± 0.16\n72.65 Â± 0.20\nFEATâ€¡\n57.04 Â± 0.16\n72.89 Â± 0.20\nSetups. We train a few-shot learning model in the standard\ndomain and evaluate it with cross-domain tasks, where the\nN-categories are aligned but domains are different. In de-\ntail, a model is trained on tasks from the â€œClipartâ€ domain\nof Ofï¬ceHome dataset [48], then the model is required to\ngeneralize to both â€œClipart (C)â€ and â€œReal World (R)â€ in-\nstances. In other words, we need to classify complex real\nimages by seeing only a few sketches, or even based on the\ninstances in the â€œReal World (R)â€ domain.\nResults. Table 15 gives the quantitative results. Here, the\nâ€œsupervisedâ€ refers to a model trained with standard clas-\nsiï¬cation and then is used for the nearest neighbor classi-\nï¬er with its penultimate layerâ€™s output feature. We observe\nthat ProtoNet can outperform this baseline on tasks when\nevaluating instances from â€œClipartâ€ but not ones from â€œreal\nworldâ€. However, FEAT can improve over â€œreal worldâ€ few-\nshot classiï¬cation even only seeing the support data from\nâ€œClipartâ€. Besides, when the support set and the test set\nof the target task are sampled from the same but new do-\nmains, e.g., the training and test instances both come from\nâ€œreal worldâ€, FEAT also improves the classiï¬cation accuracy\nw.r.t. the baseline methods. It veriï¬es the domain general-\nization ability of the FEAT approach.\n17\n\n\nD.4. Additional Discussions on Transductive FSL\nWe list the results of the transductive few-shot classiï¬-\ncation in Table 16, where the unlabeled test instances ar-\nrive simultaneously, so that the common structure among\nthe unlabeled test instances could be captured. We com-\npare with three approaches, Semi-ProtoNet [38], TPN [30],\nand TEAM [34]. Semi-ProtoNet utilizes the unlabeled in-\nstances to facilitate the computation of the class center and\nmakes predictions similar to the prototypical network; TPN\nmeta learns a label propagation way to take the unlabeled in-\nstances relationship into consideration; TEAM explores the\npairwise constraints in each task, and formulates the em-\nbedding adaptation into a semi-deï¬nite programming form.\nWe cite the results of Semi-ProtoNet from [38], and cite\nthe results of TPN and TEAM from [34].\nWe also re-\nimplement Semi-ProtoNet with our pre-trained backbone\n(the same pre-trained ConvNet weights as the standard few-\nshot learning setting) for a fair comparison.\nIn this setting, our model leverages the unlabeled test in-\nstances to augment the transformer as discussed in Â§ B.4\nand the embedding adaptation takes the relationship of all\ntest instances into consideration. Based on the adapted em-\nbedding by the joint set of labeled training instances and\nunlabeled test instances, we can make predictions with two\nstrategies. First, we still compute the center of the labeled\ninstances, while such adapted embeddings are inï¬‚uenced by\nthe unlabeled instances (we denote this approach as FEATâ€ ,\nwhich works the same way as standard FEAT except the aug-\nmented input of the embedding transformation function);\nSecond, we consider to take advantage of the unlabeled in-\nstances and use their adapted embeddings to construct a bet-\nter class prototype as in Semi-ProtoNet (we denote this ap-\nproach as FEATâ€¡).\nBy using more unlabeled test instances in the transduc-\ntive environment, FEATâ€  achieves further performance im-\nprovement compared with the standard FEAT, which veriï¬es\nthe unlabeled instances could assist the embedding adapta-\ntion of the labeled ones. With more accurate class center\nestimation, FEATâ€¡ gets a further improvement. The per-\nformance gain induced by the transductive FEAT is more\nsigniï¬cant in the one-shot learning setting compared with\nthe ï¬ve-shot scenario, since the helpfulness of unlabeled in-\nstance decreases when there are more labeled instances.\nD.5. More Generalized FSL Results\nHere we show the full results of FEAT in the general-\nized few-shot learning setting in Table 17, which includes\nboth the 1-shot and 5-shot performance. All methods are\nevaluated on instances composed by SEEN classes, UNSEEN\nclasses, and both of them (COMBINED), respectively. In\nthe 5-shot scenario, the performance improvement mainly\ncomes from the improvement of over the UNSEEN tasks.\nTable 17: Results of generalized FEAT with ConvNet backbone on\nMiniImageNet. All methods are evaluated on instances composed\nby SEEN classes, UNSEEN classes, and both of them (COMBINED),\nrespectively.\nMeasures â†’\nSEEN\nUNSEEN\nCOMBINED\n1-shot learning\nProtoNet\n41.73Â±0.03\n48.64Â±0.20\n35.69Â±0.03\nFEAT\n43.94Â±0.03\n49.72Â±0.20\n40.50Â±0.03\n5-shot learning\nProtoNet\n41.06Â±0.03\n64.94Â±0.17\n38.04Â±0.02\nFEAT\n44.94Â±0.03\n65.33Â±0.16\n41.68Â±0.03\nRandom Chance\n1.56\n20.00\n1.45\nTable 18: The top-5 low-shot learning accuracy over all classes\non the large scale ImageNet [39] dataset (w/ ResNet-50).\nUNSEEN\n1-Shot\n2-Shot\n5-Shot\n10-Shot\n20-Shot\nProtoNet [43]\n49.6\n64.0\n74.4\n78.1\n80.0\nPMN [52]\n53.3\n65.2\n75.9\n80.1\n82.6\nFEAT\n53.8\n65.4\n76.0\n81.2\n83.6\nAll\n1-Shot\n2-Shot\n5-Shot\n10-Shot\n20-Shot\nProtoNet [43]\n61.4\n71.4\n78.0\n80.0\n81.1\nPMN [52]\n64.8\n72.1\n78.8\n81.7\n83.3\nFEAT\n65.1\n72.5\n79.3\n82.1\n83.9\nAll w/ Prior\n1-Shot\n2-Shot\n5-Shot\n10-Shot\n20-Shot\nProtoNet [43]\n62.9\n70.5\n77.1\n79.5\n80.8\nPMN [52]\n63.4\n70.8\n77.9\n80.9\n82.7\nFEAT\n63.8\n71.2\n78.1\n81.3\n83.4\nD.6. Large-Scale Low-Shot Learning\nSimilar to the generalized few-shot learning, the large-\nscale low-shot learning [12, 14, 52] considers the few-shot\nclassiï¬cation ability on both SEEN and UNSEEN classes on\nthe full ImageNet [39] dataset. There are in total 389 SEEN\nclasses and 611 UNSEEN classes [14]. We follow the setting\n(including the splits) of the prior work [14] and use features\nextracted based on the pre-trained ResNet-50 [15]. Three\nevaluation protocols are evaluated, namely the top-5 few-\nshot accuracy on the UNSEEN classes, on the combined set\nof both SEEN and UNSEEN classes, and the calibrated accu-\nracy on weighted by selected set prior on the combined set\nof both SEEN and UNSEEN classes. The results are listed in\nTable 18. We observe that FEAT achieves better results than\nothers, which further validates FEATâ€™s superiority in gener-\nalized classiï¬cation setup, a large scale learning setup.\n18\n"
}