{
  "filename": "1711.05101v3.pdf",
  "num_pages": 19,
  "pages": [
    "Published as a conference paper at ICLR 2019\nDECOUPLED WEIGHT DECAY REGULARIZATION\nIlya Loshchilov & Frank Hutter\nUniversity of Freiburg\nFreiburg, Germany,\n{ilya,fh}@cs.uni-freiburg.de\nABSTRACT\nL2 regularization and weight decay regularization are equivalent for standard\nstochastic gradient descent (when rescaled by the learning rate), but as we demon-\nstrate this is not the case for adaptive gradient algorithms, such as Adam. While\ncommon implementations of these algorithms employ L2 regularization (often\ncalling it “weight decay” in what may be misleading due to the inequivalence we\nexpose), we propose a simple modiﬁcation to recover the original formulation of\nweight decay regularization by decoupling the weight decay from the optimization\nsteps taken w.r.t. the loss function. We provide empirical evidence that our pro-\nposed modiﬁcation (i) decouples the optimal choice of weight decay factor from\nthe setting of the learning rate for both standard SGD and Adam and (ii) substan-\ntially improves Adam’s generalization performance, allowing it to compete with\nSGD with momentum on image classiﬁcation datasets (on which it was previously\ntypically outperformed by the latter). Our proposed decoupled weight decay has\nalready been adopted by many researchers, and the community has implemented\nit in TensorFlow and PyTorch; the complete source code for our experiments is\navailable at https://github.com/loshchil/AdamW-and-SGDW\n1\nINTRODUCTION\nAdaptive gradient methods, such as AdaGrad (Duchi et al., 2011), RMSProp (Tieleman & Hinton,\n2012), Adam (Kingma & Ba, 2014) and most recently AMSGrad (Reddi et al., 2018) have become\na default method of choice for training feed-forward and recurrent neural networks (Xu et al., 2015;\nRadford et al., 2015). Nevertheless, state-of-the-art results for popular image classiﬁcation datasets,\nsuch as CIFAR-10 and CIFAR-100 Krizhevsky (2009), are still obtained by applying SGD with\nmomentum (Gastaldi, 2017; Cubuk et al., 2018). Furthermore, Wilson et al. (2017) suggested that\nadaptive gradient methods do not generalize as well as SGD with momentum when tested on a\ndiverse set of deep learning tasks, such as image classiﬁcation, character-level language modeling\nand constituency parsing. Different hypotheses about the origins of this worse generalization have\nbeen investigated, such as the presence of sharp local minima (Keskar et al., 2016; Dinh et al.,\n2017) and inherent problems of adaptive gradient methods (Wilson et al., 2017). In this paper, we\ninvestigate whether it is better to use L2 regularization or weight decay regularization to train deep\nneural networks with SGD and Adam. We show that a major factor of the poor generalization of the\nmost popular adaptive gradient method, Adam, is due to the fact that L2 regularization is not nearly\nas effective for it as for SGD. Speciﬁcally, our analysis of Adam leads to the following observations:\nL2 regularization and weight decay are not identical. The two techniques can be made equiv-\nalent for SGD by a reparameterization of the weight decay factor based on the learning\nrate; however, as is often overlooked, this is not the case for Adam. In particular, when\ncombined with adaptive gradients, L2 regularization leads to weights with large historic\nparameter and/or gradient amplitudes being regularized less than they would be when us-\ning weight decay.\nL2 regularization is not effective in Adam. One possible explanation why Adam and other\nadaptive gradient methods might be outperformed by SGD with momentum is that common\ndeep learning libraries only implement L2 regularization, not the original weight decay.\nTherefore, on tasks/datasets where the use of L2 regularization is beneﬁcial for SGD (e.g.,\n1\narXiv:1711.05101v3  [cs.LG]  4 Jan 2019\n",
    "Published as a conference paper at ICLR 2019\non many popular image classiﬁcation datasets), Adam leads to worse results than SGD with\nmomentum (for which L2 regularization behaves as expected).\nWeight decay is equally effective in both SGD and Adam. For SGD, it is equivalent to L2\nregularization, while for Adam it is not.\nOptimal weight decay depends on the total number of batch passes/weight updates. Our\nempirical analysis of SGD and Adam suggests that the larger the runtime/number of batch\npasses to be performed, the smaller the optimal weight decay.\nAdam can substantially beneﬁt from a scheduled learning rate multiplier. The fact that Adam\nis an adaptive gradient algorithm and as such adapts the learning rate for each parameter\ndoes not rule out the possibility to substantially improve its performance by using a global\nlearning rate multiplier, scheduled, e.g., by cosine annealing.\nThe main contribution of this paper is to improve regularization in Adam by decoupling the weight\ndecay from the gradient-based update. In a comprehensive analysis, we show that Adam generalizes\nsubstantially better with decoupled weight decay than with L2 regularization, achieving 15% relative\nimprovement in test error (see Figures 2 and 3); this holds true for various image recognition datasets\n(CIFAR-10 and ImageNet32x32), training budgets (ranging from 100 to 1800 epochs), and learning\nrate schedules (ﬁxed, drop-step, and cosine annealing; see Figure 1). We also demonstrate that our\ndecoupled weight decay renders the optimal settings of the learning rate and the weight decay factor\nmuch more independent, thereby easing hyperparameter optimization (see Figure 2).\nThe main motivation of this paper is to improve Adam to make it competitive w.r.t. SGD with\nmomentum even for those problems where it did not use to be competitive. We hope that as a result,\npractitioners do not need to switch between Adam and SGD anymore, which in turn should reduce\nthe common issue of selecting dataset/task-speciﬁc training algorithms and their hyperparameters.\n2\nDECOUPLING THE WEIGHT DECAY FROM THE GRADIENT-BASED UPDATE\nIn the weight decay described by Hanson & Pratt (1988), the weights θ decay exponentially as\nθt+1 = (1 −λ)θt −α∇ft(θt),\n(1)\nwhere λ deﬁnes the rate of the weight decay per step and ∇ft(θt) is the t-th batch gradient to be\nmultiplied by a learning rate α. For standard SGD, it is equivalent to standard L2 regularization:\nProposition 1 (Weight decay = L2 reg for standard SGD). Standard SGD with base learning rate α\nexecutes the same steps on batch loss functions ft(θ) with weight decay λ (deﬁned in Equation 1)\nas it executes without weight decay on f reg\nt (θ) = ft(θ) + λ′\n2 ∥θ∥2\n2, with λ′ = λ\nα.\nThe proofs of this well-known fact, as well as our other propositions, are given in Appendix A.\nDue to this equivalence, L2 regularization is very frequently referred to as weight decay, including\nin popular deep learning libraries. However, as we will demonstrate later in this section, this equiva-\nlence does not hold for adaptive gradient methods. One fact that is often overlooked already for the\nsimple case of SGD is that in order for the equivalence to hold, the L2 regularizer λ′ has to be set to\nλ\nα, i.e., if there is an overall best weight decay value λ, the best value of λ′ is tightly coupled with\nthe learning rate α. In order to decouple the effects of these two hyperparameters, we advocate to\ndecouple the weight decay step as proposed by Hanson & Pratt (1988) (Equation 1).\nLooking ﬁrst at the case of SGD, we propose to decay the weights simultaneously with the update\nof θt based on gradient information in Line 9 of Algorithm 1. This yields our proposed variant of\nSGD with momentum using decoupled weight decay (SGDW). This simple modiﬁcation explicitly\ndecouples λ and α (although some problem-dependent implicit coupling may of course remain as\nfor any two hyperparameters). In order to account for a possible scheduling of both α and λ, we\nintroduce a scaling factor ηt delivered by a user-deﬁned procedure SetScheduleMultiplier(t).\nNow, let’s turn to adaptive gradient algorithms like the popular optimizer Adam Kingma & Ba\n(2014), which scale gradients by their historic magnitudes. Intuitively, when Adam is run on a loss\nfunction f plus L2 regularization, weights that tend to have large gradients in f do not get regularized\nas much as they would with decoupled weight decay, since the gradient of the regularizer gets scaled\n2\n",
    "Published as a conference paper at ICLR 2019\nAlgorithm 1 SGD with L2 regularization and SGD with decoupled weight decay (SGDW) , both\nwith momentum\n1: given initial learning rate α ∈IR, momentum factor β1 ∈IR, weight decay/L2 regularization factor λ ∈IR\n2: initialize time step t ←0, parameter vector θt=0 ∈IRn, ﬁrst moment vector mt=0 ←0, schedule\nmultiplier ηt=0 ∈IR\n3: repeat\n4:\nt ←t + 1\n5:\n∇ft(θt−1) ←SelectBatch(θt−1)\n▷select batch and return the corresponding gradient\n6:\ngt ←∇ft(θt−1) +λθt−1\n7:\nηt ←SetScheduleMultiplier(t)\n▷can be ﬁxed, decay, be used for warm restarts\n8:\nmt ←β1mt−1 + ηtαgt\n9:\nθt ←θt−1 −mt −ηtλθt−1\n10: until stopping criterion is met\n11: return optimized parameters θt\nAlgorithm 2 Adam with L2 regularization and Adam with decoupled weight decay (AdamW)\n1: given α = 0.001, β1 = 0.9, β2 = 0.999, ϵ = 10−8, λ ∈IR\n2: initialize time step t ←0, parameter vector θt=0 ∈IRn, ﬁrst moment vector mt=0 ←0, second moment\nvector vt=0 ←0, schedule multiplier ηt=0 ∈IR\n3: repeat\n4:\nt ←t + 1\n5:\n∇ft(θt−1) ←SelectBatch(θt−1)\n▷select batch and return the corresponding gradient\n6:\ngt ←∇ft(θt−1) +λθt−1\n7:\nmt ←β1mt−1 + (1 −β1)gt\n▷here and below all operations are element-wise\n8:\nvt ←β2vt−1 + (1 −β2)g2\nt\n9:\nˆmt ←mt/(1 −βt\n1)\n▷β1 is taken to the power of t\n10:\nˆvt ←vt/(1 −βt\n2)\n▷β2 is taken to the power of t\n11:\nηt ←SetScheduleMultiplier(t)\n▷can be ﬁxed, decay, or also be used for warm restarts\n12:\nθt ←θt−1 −ηt\n\u0010\nαˆmt/(\n√ˆvt + ϵ) +λθt−1\n\u0011\n13: until stopping criterion is met\n14: return optimized parameters θt\nalong with the gradient of f. This leads to an inequivalence of L2 and decoupled weight decay\nregularization for adaptive gradient algorithms:\nProposition 2 (Weight decay ̸= L2 reg for adaptive gradients). Let O denote an optimizer that has\niterates θt+1 ←θt −αMt∇ft(θt) when run on batch loss function ft(θ) without weight decay,\nand θt+1 ←(1 −λ)θt −αMt∇ft(θt) when run on ft(θ) with weight decay, respectively, with\nMt ̸= kI (where k ∈R). Then, for O there exists no L2 coefﬁcient λ′ such that running O on batch\nloss f reg\nt (θ) = ft(θ)+ λ′\n2 ∥θ∥2\n2 without weight decay is equivalent to running O on ft(θ) with decay\nλ ∈R+.\nWe decouple weight decay and loss-based gradient updates in Adam as shown in line 12 of Algo-\nrithm 2; this gives rise to our variant of Adam with decoupled weight decay (AdamW).\nHaving shown that L2 regularization and weight decay regularization differ for adaptive gradient\nalgorithms raises the question of how they differ and how to interpret their effects. Their equivalence\nfor standard SGD remains very helpful for intuition: both mechanisms push weights closer to zero,\nat the same rate. However, for adaptive gradient algorithms they differ: with L2 regularization, the\nsums of the gradient of the loss function and the gradient of the regularizer (i.e., the L2 norm of the\nweights) are adapted, whereas with decoupled weight decay, only the gradients of the loss function\nare adapted (with the weight decay step separated from the adaptive gradient mechanism). With\nL2 regularization both types of gradients are normalized by their typical (summed) magnitudes, and\ntherefore weights x with large typical gradient magnitude s are regularized by a smaller relative\namount than other weights. In contrast, decoupled weight decay regularizes all weights with the\nsame rate λ, effectively regularizing weights x with large s more than standard L2 regularization\n3\n",
    "Published as a conference paper at ICLR 2019\ndoes. We demonstrate this formally for a simple special case of adaptive gradient algorithm with a\nﬁxed preconditioner:\nProposition 3 (Weight decay = scale-adjusted L2 reg for adaptive gradient algorithm with ﬁxed\npreconditioner). Let O denote an algorithm with the same characteristics as in Proposition 2, and\nusing a ﬁxed preconditioner matrix Mt = diag(s)−1 (with si > 0 for all i). Then, O with base\nlearning rate α executes the same steps on batch loss functions ft(θ) with weight decay λ as it\nexecutes without weight decay on the scale-adjusted regularized batch loss\nf sreg\nt\n(θ) = ft(θ) + λ′\n2α\n\r\rθ ⊙√s\n\r\r2\n2 ,\n(2)\nwhere ⊙and √· denote element-wise multiplication and square root, respectively, and λ′ = λ\nα.\nWe note that this proposition does not directly apply to practical adaptive gradient algorithms, since\nthese change the preconditioner matrix at every step. Nevertheless, it can still provide intuition about\nthe equivalent loss function being optimized in each step: parameters θi with a large inverse pre-\nconditioner si (which in practice would be caused by historically large gradients in dimension i) are\nregularized relatively more than they would be with L2 regularization; speciﬁcally, the regularization\nis proportional to √si.\n3\nJUSTIFICATION OF DECOUPLED WEIGHT DECAY VIA A VIEW OF\nADAPTIVE GRADIENT METHODS AS BAYESIAN FILTERING\nWe now discuss a justiﬁcation of decoupled weight decay in the framework of Bayesian ﬁltering for\na uniﬁed theory of adaptive gradient algorithms due to Aitchison (2018). After we posted a prelim-\ninary version of our current paper on arXiv, Aitchison noted that his theory “gives us a theoretical\nframework in which we can understand the superiority of this weight decay over L2 regularization,\nbecause it is weight decay, rather than L2 regularization that emerges through the straightforward ap-\nplication of Bayesian ﬁltering.”(Aitchison, 2018). While full credit for this theory goes to Aitchison,\nwe summarize it here to shed some light on why weight decay may be favored over L2 regulariza-\ntion.\nAitchison (2018) views stochastic optimization of n parameters θ1, . . . , θn as a Bayesian ﬁltering\nproblem with the goal of inferring a distribution over the optimal values of each of the parameters θi\ngiven the current values of the other parameters θ−i(t) at time step t. When the other parameters do\nnot change this is an optimization problem, but when they do change it becomes one of “tracking”\nthe optimizer using Bayesian ﬁltering as follows. One is given a probability distribution P(θt |\ny1:t) of the optimizer at time step t that takes into account the data y1:t from the ﬁrst t mini\nbatches, a state transition prior P(θt+1 | θt) reﬂecting a (small) data-independent change in this\ndistribution from one step to the next, and a likelihood P(yt+1 | θt+1) derived from the mini batch\nat step t + 1. The posterior distribution P(θt+1 | y1:t+1) of the optimizer at time step t + 1\ncan then be computed (as usual in Bayesian ﬁltering) by marginalizing over θt to obtain the one-\nstep ahead predictions P(θt+1 | y1:t) and then applying Bayes’ rule to incorporate the likelihood\nP(yt+1 | θt+1). Aitchison (2018) assumes a Gaussian state transition distribution P(θt+1 | θt) and\nan approximate conjugate likelihood P(yt+1 | θt+1), leading to the following closed-form update\nof the ﬁltering distribution’s mean:\nµpost = µprior + Σpost × g,\n(3)\nwhere g is the gradient of the log likelihood of the mini batch at time t. This result implies a precon-\nditioner of the gradients that is given by the posterior uncertainty Σpost of the ﬁltering distribution:\nupdates are larger for parameters we are more uncertain about and smaller for parameters we are\nmore certain about. Aitchison (2018) goes on to show that popular adaptive gradient methods, such\nas Adam and RMSprop, as well as Kronecker-factorized methods are special cases of this frame-\nwork.\nDecoupled weight decay very naturally ﬁts into this uniﬁed framework as part of the state-transition\ndistribution: Aitchison (2018) assumes a slow change of the optimizer according to the following\nGaussian:\nP(θt+1 | θt) = N((I −A)θt, Q),\n(4)\n4\n",
    "Published as a conference paper at ICLR 2019\nFigure 1: Adam performs better with decoupled weight decay (bottom row, AdamW) than with L2\nregularization (top row, Adam). We show the ﬁnal test error of a 26 2x64d ResNet on CIFAR-10\nafter 100 epochs of training with ﬁxed learning rate (left column), step-drop learning rate (with drops\nat epoch indexes 30, 60 and 80, middle column) and cosine annealing (right column). AdamW leads\nto a more separable hyperparameter search space, especially when a learning rate schedule, such as\nstep-drop and cosine annealing is applied. Cosine annealing yields clearly superior results.\nwhere Q is the covariance of Gaussian perturbations of the weights, and A is a regularizer to avoid\nvalues growing unboundedly over time. When instantiated as A = λ × I, this regularizer A plays\nexactly the role of decoupled weight decay as described in Equation 1, since this leads to multiplying\nthe current mean estimate θt by (1 −λ) at each step. Notably, this regularization is also directly\napplied to the prior and does not depend on the uncertainty in each of the parameters (which would\nbe required for L2 regularization).\n4\nEXPERIMENTAL VALIDATION\nWe now evaluate the performance of decoupled weight decay under various training budgets\nand learning rate schedules. Our experimental setup follows that of Gastaldi (2017), who pro-\nposed, in addition to L2 regularization, to apply the new Shake-Shake regularization to a 3-branch\nresidual DNN that allowed to achieve new state-of-the-art results of 2.86% on the CIFAR-10\ndataset (Krizhevsky, 2009). We used the same model/source code based on fb.resnet.torch 1. We\nalways used a batch size of 128 and applied the regular data augmentation procedure for the CI-\nFAR datasets. The base networks are a 26 2x64d ResNet (i.e. the network has a depth of 26, 2\nresidual branches and the ﬁrst residual block has a width of 64) and a 26 2x96d ResNet with 11.6M\nand 25.6M parameters, respectively. For a detailed description of the network and the Shake-Shake\nmethod, we refer the interested reader to Gastaldi (2017). We also perform experiments on the Im-\nageNet32x32 dataset (Chrabaszcz et al., 2017), a downsampled version of the original ImageNet\ndataset with 1.2 million 32×32 pixels images.\n4.1\nEVALUATING DECOUPLED WEIGHT DECAY WITH DIFFERENT LEARNING RATE\nSCHEDULES\nIn our ﬁrst experiment, we compare Adam with L2 regularization to Adam with decoupled weight\ndecay (AdamW), using three different learning rate schedules: a ﬁxed learning rate, a drop-step\n1https://github.com/xgastaldi/shake-shake\n5\n",
    "Published as a conference paper at ICLR 2019\nFigure 2: The Top-1 test error of a 26 2x64d ResNet on CIFAR-10 measured after 100 epochs. The\nproposed SGDW and AdamW (right column) have a more separable hyperparameter space.\nschedule, and a cosine annealing schedule (Loshchilov & Hutter, 2016). Since Adam already adapts\nits parameterwise learning rates it is not as common to use a learning rate multiplier schedule with\nit as it is with SGD, but as our results show such schedules can substantially improve Adam’s per-\nformance, and we advocate not to overlook their use for adaptive gradient algorithms.\nFor each learning rate schedule and weight decay variant, we trained a 2x64d ResNet for 100 epochs,\nusing different settings of the initial learning rate α and the weight decay factor λ. Figure 1 shows\nthat decoupled weight decay outperforms L2 regularization for all learning rate schedules, with\nlarger differences for better learning rate schedules. We also note that decoupled weight decay leads\nto a more separable hyperparameter search space, especially when a learning rate schedule, such\nas step-drop and cosine annealing is applied. The ﬁgure also shows that cosine annealing clearly\noutperforms the other learning rate schedules; we thus used cosine annealing for the remainder of\nthe experiments.\n4.2\nDECOUPLING THE WEIGHT DECAY AND INITIAL LEARNING RATE PARAMETERS\nIn order to verify our hypothesis about the coupling of α and λ, in Figure 2 we compare the perfor-\nmance of L2 regularization vs. decoupled weight decay in SGD (SGD vs. SGDW, top row) and in\nAdam (Adam vs. AdamW, bottom row). In SGD (Figure 2, top left), L2 regularization is not decou-\npled from the learning rate (the common way as described in Algorithm 1), and the ﬁgure clearly\nshows that the basin of best hyperparameter settings (depicted by color and top-10 hyperparameter\nsettings by black circles) is not aligned with the x-axis or y-axis but lies on the diagonal. This sug-\ngests that the two hyperparameters are interdependent and need to be changed simultaneously, while\nonly changing one of them might substantially worsen results. Consider, e.g., the setting at the top\nleft black circle (α = 1/2, λ = 1/8 ∗0.001); only changing either α or λ by itself would worsen\nresults, while changing both of them could still yield clear improvements. We note that this coupling\nof initial learning rate and L2 regularization factor might have contributed to SGD’s reputation of\nbeing very sensitive to its hyperparameter settings.\nIn contrast, the results for SGD with decoupled weight decay (SGDW) in Figure 2 (top right) show\nthat weight decay and initial learning rate are decoupled. The proposed approach renders the two\nhyperparameters more separable: even if the learning rate is not well tuned yet (e.g., consider the\nvalue of 1/1024 in Figure 2, top right), leaving it ﬁxed and only optimizing the weight decay factor\n6\n",
    "Published as a conference paper at ICLR 2019\nFigure 3:\nLearning curves (top row) and generalization results (bottom row) obtained by a 26\n2x96d ResNet trained with Adam and AdamW on CIFAR-10. See text for details. SuppFigure 4 in\nthe Appendix shows the same qualitative results for ImageNet32x32.\nwould yield a good value (of 1/4*0.001). This is not the case for SGD with L2 regularization (see\nFigure 2, top left).\nThe results for Adam with L2 regularization are given in Figure 2 (bottom left). Adam’s best hy-\nperparameter settings performed clearly worse than SGD’s best ones (compare Figure 2, top left).\nWhile both methods used L2 regularization, Adam did not beneﬁt from it at all: its best results ob-\ntained for non-zero L2 regularization factors were comparable to the best ones obtained without the\nL2 regularization, i.e., when λ = 0. Similarly to the original SGD, the shape of the hyperparameter\nlandscape suggests that the two hyperparameters are coupled.\nIn contrast, the results for our new variant of Adam with decoupled weight decay (AdamW) in\nFigure 2 (bottom right) show that AdamW largely decouples weight decay and learning rate. The\nresults for the best hyperparameter settings were substantially better than the best ones of Adam\nwith L2 regularization and rivaled those of SGD and SGDW.\nIn summary, the results in Figure 2 support our hypothesis that the weight decay and learning rate\nhyperparameters can be decoupled, and that this in turn simpliﬁes the problem of hyperparameter\ntuning in SGD and improves Adam’s performance to be competitive w.r.t. SGD with momentum.\n4.3\nBETTER GENERALIZATION OF ADAMW\nWhile the previous experiment suggested that the basin of optimal hyperparameters of AdamW is\nbroader and deeper than the one of Adam, we next investigated the results for much longer runs of\n1800 epochs to compare the generalization capabilities of AdamW and Adam.\nWe ﬁxed the initial learning rate to 0.001 which represents both the default learning rate for Adam\nand the one which showed reasonably good results in our experiments. Figure 3 shows the results\nfor 12 settings of the L2 regularization of Adam and 7 settings of the normalized weight decay of\nAdamW (the normalized weight decay represents a rescaling formally deﬁned in Appendix B.1; it\namounts to a multiplicative factor which depends on the number of batch passes). Interestingly,\nwhile the dynamics of the learning curves of Adam and AdamW often coincided for the ﬁrst half\nof the training run, AdamW often led to lower training loss and test errors (see Figure 3 top left\nand top right, respectively). Importantly, the use of L2 weight decay in Adam did not yield as good\n7\n",
    "Published as a conference paper at ICLR 2019\nFigure 4:\nTop-1 test error on CIFAR-10 (left) and Top-5 test error on ImageNet32x32 (right).\nFor a better resolution and with training loss curves, see SuppFigure 5 and SuppFigure 6 in the\nsupplementary material.\nresults as decoupled weight decay in AdamW (see also Figure 3, bottom left). Next, we investigated\nwhether AdamW’s better results were only due to better convergence or due to better generalization.\nThe results in Figure 3 (bottom right) for the best settings of Adam and AdamW suggest that AdamW\ndid not only yield better training loss but also yielded better generalization performance for similar\ntraining loss values. The results on ImageNet32x32 (see SuppFigure 4 in the Appendix) yield the\nsame conclusion of substantially improved generalization performance.\n4.4\nADAMWR WITH WARM RESTARTS FOR BETTER ANYTIME PERFORMANCE\nIn order to improve the anytime performance of SGDW and AdamW we extended them with the\nwarm restarts we introduced in Loshchilov & Hutter (2016), to obtain SGDWR and AdamWR, re-\nspectively (see Section B.2 in the Appendix). As Figure 4 shows, AdamWR greatly sped up AdamW\non CIFAR-10 and ImageNet32x32, up to a factor of 10 (see the results at the ﬁrst restart). For the\ndefault learning rate of 0.001, AdamW achieved 15% relative improvement in test error compared to\nAdam both on CIFAR-10 (also see SuppFigure 5) and ImageNet32x32 (also see SuppFigure 6).\nAdamWR achieved the same improved results but with a much better anytime performance. These\nimprovements closed most of the gap between Adam and SGDWR on CIFAR-10 and yielded com-\nparable performance on ImageNet32x32.\n4.5\nUSE OF ADAMW ON OTHER DATASETS AND ARCHITECTURES\nSeveral other research groups have already successfully applied AdamW in citable works. For exam-\nple, Wang et al. (2018) used AdamW to train a novel architecture for face detection on the standard\nWIDER FACE dataset (Yang et al., 2016), obtaining almost 10x faster predictions than the previous\nstate of the art algorithms while achieving comparable performance. V¨olker et al. (2018) employed\nAdamW with cosine annealing to train convolutional neural networks to classify and characterize\nerror-related brain signals measured from intracranial electroencephalography (EEG) recordings.\nWhile their paper does not provide a comparison to Adam, they kindly provided us with a direct\ncomparison of the two on their best-performing problem-speciﬁc network architecture Deep4Net\nand a variant of ResNet. AdamW with the same hyperparameter setting as Adam yielded higher\ntest set accuracy on Deep4Net (73.68% versus 71.37%) and statistically signiﬁcantly higher test\nset accuracy on ResNet (72.04% versus 61.34%). Radford et al. (2018) employed AdamW to train\nTransformer (Vaswani et al., 2017) architectures to obtain new state-of-the-art results on a wide\nrange of benchmarks for natural language understanding. Zhang et al. (2018) compared L2 reg-\nularization vs. weight decay for SGD, Adam and the Kronecker-Factored Approximate Curvature\n(K-FAC) optimizer (Martens & Grosse, 2015) on the CIFAR datasets with ResNet and VGG archi-\ntectures, reporting that decoupled weight decay consistently outperformed L2 regularization in cases\nwhere they differ.\n8\n",
    "Published as a conference paper at ICLR 2019\n5\nCONCLUSION AND FUTURE WORK\nFollowing suggestions that adaptive gradient methods such as Adam might lead to worse generaliza-\ntion than SGD with momentum (Wilson et al., 2017), we identiﬁed and exposed the inequivalence\nof L2 regularization and weight decay for Adam. We empirically showed that our version of Adam\nwith decoupled weight decay yields substantially better generalization performance than the com-\nmon implementation of Adam with L2 regularization. We also proposed to use warm restarts for\nAdam to improve its anytime performance.\nOur results obtained on image classiﬁcation datasets must be veriﬁed on a wider range of tasks,\nespecially ones where the use of regularization is expected to be important. It would be interesting\nto integrate our ﬁndings on weight decay into other methods which attempt to improve Adam, e.g,\nnormalized direction-preserving Adam (Zhang et al., 2017). While we focused our experimental\nanalysis on Adam, we believe that similar results also hold for other adaptive gradient methods,\nsuch as AdaGrad (Duchi et al., 2011) and AMSGrad (Reddi et al., 2018).\n6\nACKNOWLEDGMENTS\nWe thank Patryk Chrabaszcz for help with running experiments with ImageNet32x32; Matthias\nFeurer and Robin Schirrmeister for providing valuable feedback on this paper in several iterations;\nand Martin V¨olker, Robin Schirrmeister, and Tonio Ball for providing us with a comparison of\nAdamW and Adam on their EEG data. We also thank the following members of the deep learning\ncommunity for implementing decoupled weight decay in various deep learning libraries:\n• Jingwei Zhang, Lei Tai, Robin Schirrmeister, and Kashif Rasul for their implementations\nin PyTorch (see https://github.com/pytorch/pytorch/pull/4429)\n• Phil Jund for his implementation in TensorFlow described at\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/opt/\nDecoupledWeightDecayExtension\n• Sylvain Gugger, Anand Saha, Jeremy Howard and other members of fast.ai for their imple-\nmentation available at https://github.com/sgugger/Adam-experiments\n• Guillaume Lambard for his implementation in Keras available at https://github.\ncom/GLambard/AdamW_Keras\n• Yagami Lin for his implementation in Caffe available at https://github.com/\nYagami123/Caffe-AdamW-AdamWR\nThis work was supported by the European Research Council (ERC) under the European Union’s\nHorizon 2020 research and innovation programme under grant no. 716721, by the German Research\nFoundation (DFG) under the BrainLinksBrainTools Cluster of Excellence (grant number EXC 1086)\nand through grant no. INST 37/935-1 FUGG, and by the German state of Baden-W¨urttemberg\nthrough bwHPC.\nREFERENCES\nLaurence Aitchison. A uniﬁed theory of adaptive stochastic gradient descent as Bayesian ﬁltering.\narXiv:1507.02030, 2018.\nPatryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. A downsampled variant of ImageNet as an\nalternative to the CIFAR datasets. arXiv:1707.08819, 2017.\nEkin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:\nLearning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018.\nLaurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize\nfor deep nets. arXiv:1703.04933, 2017.\nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and\nstochastic optimization. The Journal of Machine Learning Research, 12:2121–2159, 2011.\n9\n",
    "Published as a conference paper at ICLR 2019\nXavier Gastaldi. Shake-Shake regularization. arXiv preprint arXiv:1705.07485, 2017.\nStephen Jos´e Hanson and Lorien Y Pratt. Comparing biases for minimal network construction with\nback-propagation. In Proceedings of the 1st International Conference on Neural Information\nProcessing Systems, pp. 177–185, 1988.\nGao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger.\nSnapshot ensembles: Train 1, get m for free. arXiv:1704.00109, 2017.\nNitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-\nter Tang.\nOn large-batch training for deep learning: Generalization gap and sharp minima.\narXiv:1609.04836, 2016.\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv:1412.6980,\n2014.\nAlex Krizhevsky. Learning multiple layers of features from tiny images. 2009.\nHao Li, Zheng Xu, Gavin Taylor, and Tom Goldstein. Visualizing the loss landscape of neural nets.\narXiv preprint arXiv:1712.09913, 2017.\nIlya Loshchilov and Frank Hutter.\nSGDR: stochastic gradient descent with warm restarts.\narXiv:1608.03983, 2016.\nJames Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate\ncurvature. In International conference on machine learning, pp. 2408–2417, 2015.\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\nconvolutional generative adversarial networks. arXiv:1511.06434, 2015.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language un-\nderstanding by generative pre-training.\nURL https://s3-us-west-2. amazonaws. com/openai-\nassets/research-covers/language-unsupervised/language understanding paper. pdf, 2018.\nSashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. Inter-\nnational Conference on Learning Representations, 2018.\nLeslie N Smith. Cyclical learning rates for training neural networks. arXiv:1506.01186v3, 2016.\nTijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running\naverage of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26–\n31, 2012.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-\nmation Processing Systems, pp. 5998–6008, 2017.\nMartin V¨olker, Jiˇr´ı Hammer, Robin T Schirrmeister, Joos Behncke, Lukas DJ Fiederer, Andreas\nSchulze-Bonhage, Petr Marusiˇc, Wolfram Burgard, and Tonio Ball. Intracranial error detection\nvia deep learning. arXiv preprint arXiv:1805.01667, 2018.\nJianfeng Wang, Ye Yuan, Gang Yu, and Sun Jian. Sface: An efﬁcient network for face detection in\nlarge scale variations. arXiv preprint arXiv:1804.06559, 2018.\nAshia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht.\nThe\nmarginal value of adaptive gradient methods in machine learning. arXiv:1705.08292, 2017.\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich\nZemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual\nattention. In International Conference on Machine Learning, pp. 2048–2057, 2015.\nShuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang. Wider face: A face detection bench-\nmark. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n5525–5533, 2016.\n10\n",
    "Published as a conference paper at ICLR 2019\nGuodong Zhang, Chaoqi Wang, Bowen Xu, and Roger Grosse. Three mechanisms of weight decay\nregularization. arXiv preprint arXiv:1810.12281, 2018.\nZijun Zhang, Lin Ma, Zongpeng Li, and Chuan Wu.\nNormalized direction-preserving adam.\narXiv:1709.04546, 2017.\nBarret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V. Le. Learning transferable architectures\nfor scalable image recognition. In arXiv:1707.07012 [cs.CV], 2017.\n11\n",
    "Published as a conference paper at ICLR 2019\nAppendix\nA\nFORMAL ANALYSIS OF WEIGHT DECAY VS L2 REGULARIZATION\nProof of Proposition 1\nThe proof for this well-known fact is straight-forward. SGD without weight decay has the following\niterates on f reg\nt (θ) = ft(θ) + λ′\n2 ∥θ∥2\n2:\nθt+1 ←θt −α∇f reg\nt (θt) = θt −α∇ft(θt) −αλ′θt.\n(5)\nSGD with weight decay has the following iterates on ft(θ):\nθt+1 ←(1 −λ)θt −α∇ft(θt).\n(6)\nThese iterates are identical since λ′ = λ\nα.\nProof of Proposition 2\nSimilarly to the proof of Proposition 1, the iterates of O without weight decay on f reg\nt (θ) = ft(θ)+\n1\n2λ′ ∥θ∥2\n2 and O with weight decay λ on ft are, respectively:\nθt+1\n←\nθt −αλ′Mtθt −αMt∇ft(θt).\n(7)\nθt+1\n←\n(1 −λ)θt −αMt∇ft(θt).\n(8)\nThe equality of these iterates for all θt would imply λθt = αλ′Mtθt. This can only hold for all θt\nif Mt = kI, with k ∈R, which is not the case for O. Therefore, no L2 regularizer λ′ ∥θ∥2\n2 exists\nthat makes the iterates equivalent.\nProof of Proposition 3\nO without weight decay has the following iterates on f sreg\nt\n(θ) = ft(θ) + λ′\n2\n\r\rθ ⊙√s\n\r\r2\n2:\nθt+1\n←\nθt −α∇f sreg\nt\n(θt)/s\n(9)\n=\nθt −α∇ft(θt)/s −αλ′θt ⊙s/s\n(10)\n=\nθt −α∇ft(θt)/s −αλ′θt,\n(11)\nwhere the division by s is element-wise. O with weight decay has the following iterates on ft(θ):\nθt+1\n←\n(1 −λ)θt −α∇f(θt)/s\n(12)\n=\nθt −α∇f(θt)/s −λθt,\n(13)\nThese iterates are identical since λ′ = λ\nα.\nB\nADDITIONAL PRACTICAL IMPROVEMENTS OF ADAM\nHaving discussed decoupled weight decay for improving Adam’s generalization, in this section we\nintroduce two additional components to improve Adam’s performance in practice.\nB.1\nNORMALIZED WEIGHT DECAY\nOur preliminary experiments showed that different weight decay factors are optimal for different\ncomputational budgets (deﬁned in terms of the number of batch passes). Relatedly, Li et al. (2017)\ndemonstrated that a smaller batch size (for the same total number of epochs) leads to the shrinking\neffect of weight decay being more pronounced. Here, we propose to reduce this dependence by nor-\nmalizing the values of weight decay. Speciﬁcally, we replace the hyperparameter λ by a new (more\nrobust) normalized weight decay hyperparameter λnorm, and use this to set λ as λ = λnorm\nq\nb\nBT ,\nwhere b is the batch size, B is the total number of training points and T is the total number of\nepochs.2 Thus, λnorm can be interpreted as the weight decay used if only one batch pass is al-\nlowed. We emphasize that our choice of normalization is merely one possibility informed by few\nexperiments; a more lasting conclusion we draw is that using some normalization can substantially\nimprove results.\n2In the context of our AdamWR variant discussed in Section B.2, T is the total number of epochs in the\ncurrent restart.\n1\n",
    "Published as a conference paper at ICLR 2019\nB.2\nADAM WITH COSINE ANNEALING AND WARM RESTARTS\nWe now apply cosine annealing and warm restarts to Adam, following our recent work (Loshchilov\n& Hutter, 2016). There, we proposed Stochastic Gradient Descent with Warm Restarts (SGDR) to\nimprove the anytime performance of SGD by quickly cooling down the learning rate according to a\ncosine schedule and periodically increasing it. SGDR has been successfully adopted to lead to new\nstate-of-the-art results for popular image classiﬁcation benchmarks (Huang et al., 2017; Gastaldi,\n2017; Zoph et al., 2017), and we therefore already tried extending it to Adam shortly after proposing\nit. However, while our initial version of Adam with warm restarts had better anytime performance\nthan Adam, it was not competitive with SGD with warm restarts, precisely because L2 regularization\nwas not working as well as in SGD. Now, having ﬁxed this issue by means of the original weight\ndecay regularization (Section 2) and also having introduced normalized weight decay (Section B.1),\nour original work on cosine annealing and warm restarts directly carries over to Adam.\nIn the interest of keeping the presentation self-contained, we brieﬂy describe how SGDR schedules\nthe change of the effective learning rate in order to accelerate the training of DNNs. Here, we\ndecouple the initial learning rate α and its multiplier ηt used to obtain the actual learning rate at\niteration t (see, e.g., line 8 in Algorithm 1). In SGDR, we simulate a new warm-started run/restart of\nSGD once Ti epochs are performed, where i is the index of the run. Importantly, the restarts are not\nperformed from scratch but emulated by increasing ηt while the old value of θt is used as an initial\nsolution. The amount by which ηt is increased controls to which extent the previously acquired\ninformation (e.g., momentum) is used. Within the i-th run, the value of ηt decays according to a\ncosine annealing (Loshchilov & Hutter, 2016) learning rate for each batch as follows:\nηt = η(i)\nmin + 0.5(η(i)\nmax −η(i)\nmin)(1 + cos(πTcur/Ti)),\n(14)\nwhere η(i)\nmin and η(i)\nmax are ranges for the multiplier and Tcur accounts for how many epochs have\nbeen performed since the last restart. Tcur is updated at each batch iteration t and is thus not\nconstrained to integer values. Adjusting (e.g., decreasing) η(i)\nmin and η(i)\nmax at every i-th restart (see\nalso Smith (2016)) could potentially improve performance, but we do not consider that option here\nbecause it would involve additional hyperparameters. For η(i)\nmax = 1 and η(i)\nmin = 0, one can simplify\nEq. (14) to\nηt = 0.5 + 0.5 cos(πTcur/Ti).\n(15)\nIn order to achieve good anytime performance, one can start with an initially small Ti (e.g., from\n1% to 10% of the expected total budget) and multiply it by a factor of Tmult (e.g., Tmult = 2) at\nevery restart. The (i + 1)-th restart is triggered when Tcur = Ti by setting Tcur to 0. An example\nsetting of the schedule multiplier is given in C.\nOur proposed AdamWR algorithm represents AdamW (see Algorithm 2) with ηt following Eq. (15)\nand λ computed at each iteration using normalized weight decay described in Section B.1. We note\nthat normalized weight decay allowed us to use a constant parameter setting across short and long\nruns performed within AdamWR and SGDWR (SGDW with warm restarts).\nC\nAN EXAMPLE SETTING OF THE SCHEDULE MULTIPLIER\nAn example schedule of the schedule multiplier ηt is given in SuppFigure 1 for Ti=0 = 100 and\nTmult = 2. After the initial 100 epochs the learning rate will reach 0 because ηt=100 = 0. Then,\nsince Tcur = Ti=0, we restart by resetting Tcur = 0, causing the multiplier ηt to be reset to 1 due\nto Eq. (15). This multiplier will then decrease again from 1 to 0, but now over the course of 200\nepochs because Ti=1 = Ti=0Tmult = 200. Solutions obtained right before the restarts, when ηt = 0\n(e.g., at epoch indexes 100, 300, 700 and 1500 as shown in SuppFigure 1) are recommended by the\noptimizer as the solutions, with more recent solutions prioritized.\nD\nADDITIONAL RESULTS\nWe investigated whether the use of much longer runs (1800 epochs) of “standard Adam” (Adam\nwith L2 regularization and a ﬁxed learning rate) makes the use of cosine annealing unnecessary.\n2\n",
    "Published as a conference paper at ICLR 2019\n200\n400\n600\n800\n1000\n1200\n1400\n0\n0.2\n0.4\n0.6\n0.8\n1\nEpochs\nLearning rate multiplier η\nT0=100, Tmult=2\nSuppFigure 1: An example schedule of the learning rate multiplier as a function of epoch index.\nThe ﬁrst run is scheduled to converge at epoch Ti=0 = 100, then the budget for the next run is\ndoubled as Ti=1 = Ti=0Tmult = 200, etc.\nSuppFigure 2 shows the results of standard Adam for a 4 by 4 logarithmic grid of hyperparame-\nter settings (the coarseness of the grid is due to the high computational expense of runs for 1800\nepochs). Even after taking the low resolution of the grid into account, the results appear to be at best\ncomparable to the ones obtained with AdamW with 18 times less epochs and a smaller network (see\nSuppFigure 3, top row, middle). These results are not very surprising given Figure 1 in the main\npaper (which demonstrates both the improvements possible by using some learning rate schedule,\nsuch as cosine annealing, and the effectiveness of decoupled weight decay).\nOur experimental results with Adam and SGD suggest that the total runtime in terms of the number\nof epochs affect the basin of optimal hyperparameters (see SuppFigure 3). More speciﬁcally, the\ngreater the total number of epochs the smaller the values of the weight decay should be. SuppFigure\n4 shows that our remedy for this problem, the normalized weight decay deﬁned in Eq. (15), sim-\npliﬁes hyperparameter selection because the optimal values observed for short runs are similar to\nthe ones for much longer runs. We used our initial experiments on CIFAR-10 to suggest the square\nroot normalization we proposed in Eq. (15) and double-checked that this is not a coincidence on the\nImageNet32x32 dataset (Chrabaszcz et al., 2017), a downsampled version of the original ImageNet\ndataset with 1.2 million 32×32 pixels images, where an epoch is 24 times longer than on CIFAR-10.\nThis experiment also supported the square root scaling: the best values of the normalized weight de-\ncay observed on CIFAR-10 represented nearly optimal values for ImageNet32x32 (see SuppFigure\n3). In contrast, had we used the same raw weight decay values λ for ImageNet32x32 as for CIFAR-\n10 and for the same number of epochs, without the proposed normalization, λ would have been\nroughly 5 times too large for ImageNet32x32, leading to much worse performance. The optimal\nnormalized weight decay values were also very similar (e.g., λnorm = 0.025 and λnorm = 0.05)\nacross SGDW and AdamW. These results clearly show that normalizing weight decay can substan-\ntially improve performance; while square root scaling performed very well in our experiments we\nemphasize that these experiments were not very comprehensive and that even better scaling rules\nare likely to exist.\nSuppFigure 4 is the equivalent of Figure 3 in the main paper, but for ImageNet32x32 instead of for\nCIFAR-10. The qualitative results are identical: weight decay leads to better training loss (cross-\nentropy) than L2 regularization, and to an even greater improvement of test error.\nSuppFigure 5 and SuppFigure 6 are the equivalents of Figure 4 in the main paper but supplemented\nwith training loss curves in its bottom row. The results show that Adam and its variants with decou-\npled weight decay converge faster (in terms of training loss) on CIFAR-10 than the corresponding\nSGD variants (the difference for ImageNet32x32 is small). As is discussed in the main paper, when\nthe same values of training loss are considered, AdamW demonstrates better values of test error than\nAdam. Interestingly, SuppFigure 5 and SuppFigure 6 show that the restart variants AdamWR and\nSGDWR also demonstrate better generalization than AdamW and SGDW, respectively.\n3\n",
    "Published as a conference paper at ICLR 2019\nSuppFigure 2: Performance of “standard Adam”: Adam with L2 regularization and a ﬁxed learning\nrate. We show the ﬁnal test error of a 26 2x96d ResNet on CIFAR-10 after 1800 epochs of the\noriginal Adam for different settings of learning rate and weight decay used for L2 regularization.\n4\n",
    "Published as a conference paper at ICLR 2019\nSuppFigure 3: Effect of normalized weight decay. We show the ﬁnal test Top-1 error on CIFAR-\n10 (ﬁrst two rows for AdamW without and with normalized weight decay) and Top-5 error on\nImageNet32x32 (last two rows for AdamW and SGDW, both with normalized weight decay) of a\n26 2x64d ResNet after different numbers of epochs (see columns). While the optimal settings of the\nraw weight decay change signiﬁcantly for different runtime budgets (see the ﬁrst row), the values\nof the normalized weight decay remain very similar for different budgets (see the second row) and\ndifferent datasets (here, CIFAR-10 and ImageNet32x32), and even across AdamW and SGDW.\n5\n",
    "Published as a conference paper at ICLR 2019\nSuppFigure 4: Learning curves (top row) and generalization results (Top-5 errors in bottom row)\nobtained by a 26 2x96d ResNet trained with Adam and AdamW on ImageNet32x32.\n6\n",
    "Published as a conference paper at ICLR 2019\nSuppFigure 5: Test error curves (top row) and training loss curves (bottom row) for CIFAR-10.\n7\n",
    "Published as a conference paper at ICLR 2019\nSuppFigure\n6:\nTest error curves (top row) and training loss curves (bottom row) for Ima-\ngeNet32x32.\n8\n"
  ],
  "full_text": "Published as a conference paper at ICLR 2019\nDECOUPLED WEIGHT DECAY REGULARIZATION\nIlya Loshchilov & Frank Hutter\nUniversity of Freiburg\nFreiburg, Germany,\n{ilya,fh}@cs.uni-freiburg.de\nABSTRACT\nL2 regularization and weight decay regularization are equivalent for standard\nstochastic gradient descent (when rescaled by the learning rate), but as we demon-\nstrate this is not the case for adaptive gradient algorithms, such as Adam. While\ncommon implementations of these algorithms employ L2 regularization (often\ncalling it “weight decay” in what may be misleading due to the inequivalence we\nexpose), we propose a simple modiﬁcation to recover the original formulation of\nweight decay regularization by decoupling the weight decay from the optimization\nsteps taken w.r.t. the loss function. We provide empirical evidence that our pro-\nposed modiﬁcation (i) decouples the optimal choice of weight decay factor from\nthe setting of the learning rate for both standard SGD and Adam and (ii) substan-\ntially improves Adam’s generalization performance, allowing it to compete with\nSGD with momentum on image classiﬁcation datasets (on which it was previously\ntypically outperformed by the latter). Our proposed decoupled weight decay has\nalready been adopted by many researchers, and the community has implemented\nit in TensorFlow and PyTorch; the complete source code for our experiments is\navailable at https://github.com/loshchil/AdamW-and-SGDW\n1\nINTRODUCTION\nAdaptive gradient methods, such as AdaGrad (Duchi et al., 2011), RMSProp (Tieleman & Hinton,\n2012), Adam (Kingma & Ba, 2014) and most recently AMSGrad (Reddi et al., 2018) have become\na default method of choice for training feed-forward and recurrent neural networks (Xu et al., 2015;\nRadford et al., 2015). Nevertheless, state-of-the-art results for popular image classiﬁcation datasets,\nsuch as CIFAR-10 and CIFAR-100 Krizhevsky (2009), are still obtained by applying SGD with\nmomentum (Gastaldi, 2017; Cubuk et al., 2018). Furthermore, Wilson et al. (2017) suggested that\nadaptive gradient methods do not generalize as well as SGD with momentum when tested on a\ndiverse set of deep learning tasks, such as image classiﬁcation, character-level language modeling\nand constituency parsing. Different hypotheses about the origins of this worse generalization have\nbeen investigated, such as the presence of sharp local minima (Keskar et al., 2016; Dinh et al.,\n2017) and inherent problems of adaptive gradient methods (Wilson et al., 2017). In this paper, we\ninvestigate whether it is better to use L2 regularization or weight decay regularization to train deep\nneural networks with SGD and Adam. We show that a major factor of the poor generalization of the\nmost popular adaptive gradient method, Adam, is due to the fact that L2 regularization is not nearly\nas effective for it as for SGD. Speciﬁcally, our analysis of Adam leads to the following observations:\nL2 regularization and weight decay are not identical. The two techniques can be made equiv-\nalent for SGD by a reparameterization of the weight decay factor based on the learning\nrate; however, as is often overlooked, this is not the case for Adam. In particular, when\ncombined with adaptive gradients, L2 regularization leads to weights with large historic\nparameter and/or gradient amplitudes being regularized less than they would be when us-\ning weight decay.\nL2 regularization is not effective in Adam. One possible explanation why Adam and other\nadaptive gradient methods might be outperformed by SGD with momentum is that common\ndeep learning libraries only implement L2 regularization, not the original weight decay.\nTherefore, on tasks/datasets where the use of L2 regularization is beneﬁcial for SGD (e.g.,\n1\narXiv:1711.05101v3  [cs.LG]  4 Jan 2019\n\n\nPublished as a conference paper at ICLR 2019\non many popular image classiﬁcation datasets), Adam leads to worse results than SGD with\nmomentum (for which L2 regularization behaves as expected).\nWeight decay is equally effective in both SGD and Adam. For SGD, it is equivalent to L2\nregularization, while for Adam it is not.\nOptimal weight decay depends on the total number of batch passes/weight updates. Our\nempirical analysis of SGD and Adam suggests that the larger the runtime/number of batch\npasses to be performed, the smaller the optimal weight decay.\nAdam can substantially beneﬁt from a scheduled learning rate multiplier. The fact that Adam\nis an adaptive gradient algorithm and as such adapts the learning rate for each parameter\ndoes not rule out the possibility to substantially improve its performance by using a global\nlearning rate multiplier, scheduled, e.g., by cosine annealing.\nThe main contribution of this paper is to improve regularization in Adam by decoupling the weight\ndecay from the gradient-based update. In a comprehensive analysis, we show that Adam generalizes\nsubstantially better with decoupled weight decay than with L2 regularization, achieving 15% relative\nimprovement in test error (see Figures 2 and 3); this holds true for various image recognition datasets\n(CIFAR-10 and ImageNet32x32), training budgets (ranging from 100 to 1800 epochs), and learning\nrate schedules (ﬁxed, drop-step, and cosine annealing; see Figure 1). We also demonstrate that our\ndecoupled weight decay renders the optimal settings of the learning rate and the weight decay factor\nmuch more independent, thereby easing hyperparameter optimization (see Figure 2).\nThe main motivation of this paper is to improve Adam to make it competitive w.r.t. SGD with\nmomentum even for those problems where it did not use to be competitive. We hope that as a result,\npractitioners do not need to switch between Adam and SGD anymore, which in turn should reduce\nthe common issue of selecting dataset/task-speciﬁc training algorithms and their hyperparameters.\n2\nDECOUPLING THE WEIGHT DECAY FROM THE GRADIENT-BASED UPDATE\nIn the weight decay described by Hanson & Pratt (1988), the weights θ decay exponentially as\nθt+1 = (1 −λ)θt −α∇ft(θt),\n(1)\nwhere λ deﬁnes the rate of the weight decay per step and ∇ft(θt) is the t-th batch gradient to be\nmultiplied by a learning rate α. For standard SGD, it is equivalent to standard L2 regularization:\nProposition 1 (Weight decay = L2 reg for standard SGD). Standard SGD with base learning rate α\nexecutes the same steps on batch loss functions ft(θ) with weight decay λ (deﬁned in Equation 1)\nas it executes without weight decay on f reg\nt (θ) = ft(θ) + λ′\n2 ∥θ∥2\n2, with λ′ = λ\nα.\nThe proofs of this well-known fact, as well as our other propositions, are given in Appendix A.\nDue to this equivalence, L2 regularization is very frequently referred to as weight decay, including\nin popular deep learning libraries. However, as we will demonstrate later in this section, this equiva-\nlence does not hold for adaptive gradient methods. One fact that is often overlooked already for the\nsimple case of SGD is that in order for the equivalence to hold, the L2 regularizer λ′ has to be set to\nλ\nα, i.e., if there is an overall best weight decay value λ, the best value of λ′ is tightly coupled with\nthe learning rate α. In order to decouple the effects of these two hyperparameters, we advocate to\ndecouple the weight decay step as proposed by Hanson & Pratt (1988) (Equation 1).\nLooking ﬁrst at the case of SGD, we propose to decay the weights simultaneously with the update\nof θt based on gradient information in Line 9 of Algorithm 1. This yields our proposed variant of\nSGD with momentum using decoupled weight decay (SGDW). This simple modiﬁcation explicitly\ndecouples λ and α (although some problem-dependent implicit coupling may of course remain as\nfor any two hyperparameters). In order to account for a possible scheduling of both α and λ, we\nintroduce a scaling factor ηt delivered by a user-deﬁned procedure SetScheduleMultiplier(t).\nNow, let’s turn to adaptive gradient algorithms like the popular optimizer Adam Kingma & Ba\n(2014), which scale gradients by their historic magnitudes. Intuitively, when Adam is run on a loss\nfunction f plus L2 regularization, weights that tend to have large gradients in f do not get regularized\nas much as they would with decoupled weight decay, since the gradient of the regularizer gets scaled\n2\n\n\nPublished as a conference paper at ICLR 2019\nAlgorithm 1 SGD with L2 regularization and SGD with decoupled weight decay (SGDW) , both\nwith momentum\n1: given initial learning rate α ∈IR, momentum factor β1 ∈IR, weight decay/L2 regularization factor λ ∈IR\n2: initialize time step t ←0, parameter vector θt=0 ∈IRn, ﬁrst moment vector mt=0 ←0, schedule\nmultiplier ηt=0 ∈IR\n3: repeat\n4:\nt ←t + 1\n5:\n∇ft(θt−1) ←SelectBatch(θt−1)\n▷select batch and return the corresponding gradient\n6:\ngt ←∇ft(θt−1) +λθt−1\n7:\nηt ←SetScheduleMultiplier(t)\n▷can be ﬁxed, decay, be used for warm restarts\n8:\nmt ←β1mt−1 + ηtαgt\n9:\nθt ←θt−1 −mt −ηtλθt−1\n10: until stopping criterion is met\n11: return optimized parameters θt\nAlgorithm 2 Adam with L2 regularization and Adam with decoupled weight decay (AdamW)\n1: given α = 0.001, β1 = 0.9, β2 = 0.999, ϵ = 10−8, λ ∈IR\n2: initialize time step t ←0, parameter vector θt=0 ∈IRn, ﬁrst moment vector mt=0 ←0, second moment\nvector vt=0 ←0, schedule multiplier ηt=0 ∈IR\n3: repeat\n4:\nt ←t + 1\n5:\n∇ft(θt−1) ←SelectBatch(θt−1)\n▷select batch and return the corresponding gradient\n6:\ngt ←∇ft(θt−1) +λθt−1\n7:\nmt ←β1mt−1 + (1 −β1)gt\n▷here and below all operations are element-wise\n8:\nvt ←β2vt−1 + (1 −β2)g2\nt\n9:\nˆmt ←mt/(1 −βt\n1)\n▷β1 is taken to the power of t\n10:\nˆvt ←vt/(1 −βt\n2)\n▷β2 is taken to the power of t\n11:\nηt ←SetScheduleMultiplier(t)\n▷can be ﬁxed, decay, or also be used for warm restarts\n12:\nθt ←θt−1 −ηt\n\u0010\nαˆmt/(\n√ˆvt + ϵ) +λθt−1\n\u0011\n13: until stopping criterion is met\n14: return optimized parameters θt\nalong with the gradient of f. This leads to an inequivalence of L2 and decoupled weight decay\nregularization for adaptive gradient algorithms:\nProposition 2 (Weight decay ̸= L2 reg for adaptive gradients). Let O denote an optimizer that has\niterates θt+1 ←θt −αMt∇ft(θt) when run on batch loss function ft(θ) without weight decay,\nand θt+1 ←(1 −λ)θt −αMt∇ft(θt) when run on ft(θ) with weight decay, respectively, with\nMt ̸= kI (where k ∈R). Then, for O there exists no L2 coefﬁcient λ′ such that running O on batch\nloss f reg\nt (θ) = ft(θ)+ λ′\n2 ∥θ∥2\n2 without weight decay is equivalent to running O on ft(θ) with decay\nλ ∈R+.\nWe decouple weight decay and loss-based gradient updates in Adam as shown in line 12 of Algo-\nrithm 2; this gives rise to our variant of Adam with decoupled weight decay (AdamW).\nHaving shown that L2 regularization and weight decay regularization differ for adaptive gradient\nalgorithms raises the question of how they differ and how to interpret their effects. Their equivalence\nfor standard SGD remains very helpful for intuition: both mechanisms push weights closer to zero,\nat the same rate. However, for adaptive gradient algorithms they differ: with L2 regularization, the\nsums of the gradient of the loss function and the gradient of the regularizer (i.e., the L2 norm of the\nweights) are adapted, whereas with decoupled weight decay, only the gradients of the loss function\nare adapted (with the weight decay step separated from the adaptive gradient mechanism). With\nL2 regularization both types of gradients are normalized by their typical (summed) magnitudes, and\ntherefore weights x with large typical gradient magnitude s are regularized by a smaller relative\namount than other weights. In contrast, decoupled weight decay regularizes all weights with the\nsame rate λ, effectively regularizing weights x with large s more than standard L2 regularization\n3\n\n\nPublished as a conference paper at ICLR 2019\ndoes. We demonstrate this formally for a simple special case of adaptive gradient algorithm with a\nﬁxed preconditioner:\nProposition 3 (Weight decay = scale-adjusted L2 reg for adaptive gradient algorithm with ﬁxed\npreconditioner). Let O denote an algorithm with the same characteristics as in Proposition 2, and\nusing a ﬁxed preconditioner matrix Mt = diag(s)−1 (with si > 0 for all i). Then, O with base\nlearning rate α executes the same steps on batch loss functions ft(θ) with weight decay λ as it\nexecutes without weight decay on the scale-adjusted regularized batch loss\nf sreg\nt\n(θ) = ft(θ) + λ′\n2α\n\r\rθ ⊙√s\n\r\r2\n2 ,\n(2)\nwhere ⊙and √· denote element-wise multiplication and square root, respectively, and λ′ = λ\nα.\nWe note that this proposition does not directly apply to practical adaptive gradient algorithms, since\nthese change the preconditioner matrix at every step. Nevertheless, it can still provide intuition about\nthe equivalent loss function being optimized in each step: parameters θi with a large inverse pre-\nconditioner si (which in practice would be caused by historically large gradients in dimension i) are\nregularized relatively more than they would be with L2 regularization; speciﬁcally, the regularization\nis proportional to √si.\n3\nJUSTIFICATION OF DECOUPLED WEIGHT DECAY VIA A VIEW OF\nADAPTIVE GRADIENT METHODS AS BAYESIAN FILTERING\nWe now discuss a justiﬁcation of decoupled weight decay in the framework of Bayesian ﬁltering for\na uniﬁed theory of adaptive gradient algorithms due to Aitchison (2018). After we posted a prelim-\ninary version of our current paper on arXiv, Aitchison noted that his theory “gives us a theoretical\nframework in which we can understand the superiority of this weight decay over L2 regularization,\nbecause it is weight decay, rather than L2 regularization that emerges through the straightforward ap-\nplication of Bayesian ﬁltering.”(Aitchison, 2018). While full credit for this theory goes to Aitchison,\nwe summarize it here to shed some light on why weight decay may be favored over L2 regulariza-\ntion.\nAitchison (2018) views stochastic optimization of n parameters θ1, . . . , θn as a Bayesian ﬁltering\nproblem with the goal of inferring a distribution over the optimal values of each of the parameters θi\ngiven the current values of the other parameters θ−i(t) at time step t. When the other parameters do\nnot change this is an optimization problem, but when they do change it becomes one of “tracking”\nthe optimizer using Bayesian ﬁltering as follows. One is given a probability distribution P(θt |\ny1:t) of the optimizer at time step t that takes into account the data y1:t from the ﬁrst t mini\nbatches, a state transition prior P(θt+1 | θt) reﬂecting a (small) data-independent change in this\ndistribution from one step to the next, and a likelihood P(yt+1 | θt+1) derived from the mini batch\nat step t + 1. The posterior distribution P(θt+1 | y1:t+1) of the optimizer at time step t + 1\ncan then be computed (as usual in Bayesian ﬁltering) by marginalizing over θt to obtain the one-\nstep ahead predictions P(θt+1 | y1:t) and then applying Bayes’ rule to incorporate the likelihood\nP(yt+1 | θt+1). Aitchison (2018) assumes a Gaussian state transition distribution P(θt+1 | θt) and\nan approximate conjugate likelihood P(yt+1 | θt+1), leading to the following closed-form update\nof the ﬁltering distribution’s mean:\nµpost = µprior + Σpost × g,\n(3)\nwhere g is the gradient of the log likelihood of the mini batch at time t. This result implies a precon-\nditioner of the gradients that is given by the posterior uncertainty Σpost of the ﬁltering distribution:\nupdates are larger for parameters we are more uncertain about and smaller for parameters we are\nmore certain about. Aitchison (2018) goes on to show that popular adaptive gradient methods, such\nas Adam and RMSprop, as well as Kronecker-factorized methods are special cases of this frame-\nwork.\nDecoupled weight decay very naturally ﬁts into this uniﬁed framework as part of the state-transition\ndistribution: Aitchison (2018) assumes a slow change of the optimizer according to the following\nGaussian:\nP(θt+1 | θt) = N((I −A)θt, Q),\n(4)\n4\n\n\nPublished as a conference paper at ICLR 2019\nFigure 1: Adam performs better with decoupled weight decay (bottom row, AdamW) than with L2\nregularization (top row, Adam). We show the ﬁnal test error of a 26 2x64d ResNet on CIFAR-10\nafter 100 epochs of training with ﬁxed learning rate (left column), step-drop learning rate (with drops\nat epoch indexes 30, 60 and 80, middle column) and cosine annealing (right column). AdamW leads\nto a more separable hyperparameter search space, especially when a learning rate schedule, such as\nstep-drop and cosine annealing is applied. Cosine annealing yields clearly superior results.\nwhere Q is the covariance of Gaussian perturbations of the weights, and A is a regularizer to avoid\nvalues growing unboundedly over time. When instantiated as A = λ × I, this regularizer A plays\nexactly the role of decoupled weight decay as described in Equation 1, since this leads to multiplying\nthe current mean estimate θt by (1 −λ) at each step. Notably, this regularization is also directly\napplied to the prior and does not depend on the uncertainty in each of the parameters (which would\nbe required for L2 regularization).\n4\nEXPERIMENTAL VALIDATION\nWe now evaluate the performance of decoupled weight decay under various training budgets\nand learning rate schedules. Our experimental setup follows that of Gastaldi (2017), who pro-\nposed, in addition to L2 regularization, to apply the new Shake-Shake regularization to a 3-branch\nresidual DNN that allowed to achieve new state-of-the-art results of 2.86% on the CIFAR-10\ndataset (Krizhevsky, 2009). We used the same model/source code based on fb.resnet.torch 1. We\nalways used a batch size of 128 and applied the regular data augmentation procedure for the CI-\nFAR datasets. The base networks are a 26 2x64d ResNet (i.e. the network has a depth of 26, 2\nresidual branches and the ﬁrst residual block has a width of 64) and a 26 2x96d ResNet with 11.6M\nand 25.6M parameters, respectively. For a detailed description of the network and the Shake-Shake\nmethod, we refer the interested reader to Gastaldi (2017). We also perform experiments on the Im-\nageNet32x32 dataset (Chrabaszcz et al., 2017), a downsampled version of the original ImageNet\ndataset with 1.2 million 32×32 pixels images.\n4.1\nEVALUATING DECOUPLED WEIGHT DECAY WITH DIFFERENT LEARNING RATE\nSCHEDULES\nIn our ﬁrst experiment, we compare Adam with L2 regularization to Adam with decoupled weight\ndecay (AdamW), using three different learning rate schedules: a ﬁxed learning rate, a drop-step\n1https://github.com/xgastaldi/shake-shake\n5\n\n\nPublished as a conference paper at ICLR 2019\nFigure 2: The Top-1 test error of a 26 2x64d ResNet on CIFAR-10 measured after 100 epochs. The\nproposed SGDW and AdamW (right column) have a more separable hyperparameter space.\nschedule, and a cosine annealing schedule (Loshchilov & Hutter, 2016). Since Adam already adapts\nits parameterwise learning rates it is not as common to use a learning rate multiplier schedule with\nit as it is with SGD, but as our results show such schedules can substantially improve Adam’s per-\nformance, and we advocate not to overlook their use for adaptive gradient algorithms.\nFor each learning rate schedule and weight decay variant, we trained a 2x64d ResNet for 100 epochs,\nusing different settings of the initial learning rate α and the weight decay factor λ. Figure 1 shows\nthat decoupled weight decay outperforms L2 regularization for all learning rate schedules, with\nlarger differences for better learning rate schedules. We also note that decoupled weight decay leads\nto a more separable hyperparameter search space, especially when a learning rate schedule, such\nas step-drop and cosine annealing is applied. The ﬁgure also shows that cosine annealing clearly\noutperforms the other learning rate schedules; we thus used cosine annealing for the remainder of\nthe experiments.\n4.2\nDECOUPLING THE WEIGHT DECAY AND INITIAL LEARNING RATE PARAMETERS\nIn order to verify our hypothesis about the coupling of α and λ, in Figure 2 we compare the perfor-\nmance of L2 regularization vs. decoupled weight decay in SGD (SGD vs. SGDW, top row) and in\nAdam (Adam vs. AdamW, bottom row). In SGD (Figure 2, top left), L2 regularization is not decou-\npled from the learning rate (the common way as described in Algorithm 1), and the ﬁgure clearly\nshows that the basin of best hyperparameter settings (depicted by color and top-10 hyperparameter\nsettings by black circles) is not aligned with the x-axis or y-axis but lies on the diagonal. This sug-\ngests that the two hyperparameters are interdependent and need to be changed simultaneously, while\nonly changing one of them might substantially worsen results. Consider, e.g., the setting at the top\nleft black circle (α = 1/2, λ = 1/8 ∗0.001); only changing either α or λ by itself would worsen\nresults, while changing both of them could still yield clear improvements. We note that this coupling\nof initial learning rate and L2 regularization factor might have contributed to SGD’s reputation of\nbeing very sensitive to its hyperparameter settings.\nIn contrast, the results for SGD with decoupled weight decay (SGDW) in Figure 2 (top right) show\nthat weight decay and initial learning rate are decoupled. The proposed approach renders the two\nhyperparameters more separable: even if the learning rate is not well tuned yet (e.g., consider the\nvalue of 1/1024 in Figure 2, top right), leaving it ﬁxed and only optimizing the weight decay factor\n6\n\n\nPublished as a conference paper at ICLR 2019\nFigure 3:\nLearning curves (top row) and generalization results (bottom row) obtained by a 26\n2x96d ResNet trained with Adam and AdamW on CIFAR-10. See text for details. SuppFigure 4 in\nthe Appendix shows the same qualitative results for ImageNet32x32.\nwould yield a good value (of 1/4*0.001). This is not the case for SGD with L2 regularization (see\nFigure 2, top left).\nThe results for Adam with L2 regularization are given in Figure 2 (bottom left). Adam’s best hy-\nperparameter settings performed clearly worse than SGD’s best ones (compare Figure 2, top left).\nWhile both methods used L2 regularization, Adam did not beneﬁt from it at all: its best results ob-\ntained for non-zero L2 regularization factors were comparable to the best ones obtained without the\nL2 regularization, i.e., when λ = 0. Similarly to the original SGD, the shape of the hyperparameter\nlandscape suggests that the two hyperparameters are coupled.\nIn contrast, the results for our new variant of Adam with decoupled weight decay (AdamW) in\nFigure 2 (bottom right) show that AdamW largely decouples weight decay and learning rate. The\nresults for the best hyperparameter settings were substantially better than the best ones of Adam\nwith L2 regularization and rivaled those of SGD and SGDW.\nIn summary, the results in Figure 2 support our hypothesis that the weight decay and learning rate\nhyperparameters can be decoupled, and that this in turn simpliﬁes the problem of hyperparameter\ntuning in SGD and improves Adam’s performance to be competitive w.r.t. SGD with momentum.\n4.3\nBETTER GENERALIZATION OF ADAMW\nWhile the previous experiment suggested that the basin of optimal hyperparameters of AdamW is\nbroader and deeper than the one of Adam, we next investigated the results for much longer runs of\n1800 epochs to compare the generalization capabilities of AdamW and Adam.\nWe ﬁxed the initial learning rate to 0.001 which represents both the default learning rate for Adam\nand the one which showed reasonably good results in our experiments. Figure 3 shows the results\nfor 12 settings of the L2 regularization of Adam and 7 settings of the normalized weight decay of\nAdamW (the normalized weight decay represents a rescaling formally deﬁned in Appendix B.1; it\namounts to a multiplicative factor which depends on the number of batch passes). Interestingly,\nwhile the dynamics of the learning curves of Adam and AdamW often coincided for the ﬁrst half\nof the training run, AdamW often led to lower training loss and test errors (see Figure 3 top left\nand top right, respectively). Importantly, the use of L2 weight decay in Adam did not yield as good\n7\n\n\nPublished as a conference paper at ICLR 2019\nFigure 4:\nTop-1 test error on CIFAR-10 (left) and Top-5 test error on ImageNet32x32 (right).\nFor a better resolution and with training loss curves, see SuppFigure 5 and SuppFigure 6 in the\nsupplementary material.\nresults as decoupled weight decay in AdamW (see also Figure 3, bottom left). Next, we investigated\nwhether AdamW’s better results were only due to better convergence or due to better generalization.\nThe results in Figure 3 (bottom right) for the best settings of Adam and AdamW suggest that AdamW\ndid not only yield better training loss but also yielded better generalization performance for similar\ntraining loss values. The results on ImageNet32x32 (see SuppFigure 4 in the Appendix) yield the\nsame conclusion of substantially improved generalization performance.\n4.4\nADAMWR WITH WARM RESTARTS FOR BETTER ANYTIME PERFORMANCE\nIn order to improve the anytime performance of SGDW and AdamW we extended them with the\nwarm restarts we introduced in Loshchilov & Hutter (2016), to obtain SGDWR and AdamWR, re-\nspectively (see Section B.2 in the Appendix). As Figure 4 shows, AdamWR greatly sped up AdamW\non CIFAR-10 and ImageNet32x32, up to a factor of 10 (see the results at the ﬁrst restart). For the\ndefault learning rate of 0.001, AdamW achieved 15% relative improvement in test error compared to\nAdam both on CIFAR-10 (also see SuppFigure 5) and ImageNet32x32 (also see SuppFigure 6).\nAdamWR achieved the same improved results but with a much better anytime performance. These\nimprovements closed most of the gap between Adam and SGDWR on CIFAR-10 and yielded com-\nparable performance on ImageNet32x32.\n4.5\nUSE OF ADAMW ON OTHER DATASETS AND ARCHITECTURES\nSeveral other research groups have already successfully applied AdamW in citable works. For exam-\nple, Wang et al. (2018) used AdamW to train a novel architecture for face detection on the standard\nWIDER FACE dataset (Yang et al., 2016), obtaining almost 10x faster predictions than the previous\nstate of the art algorithms while achieving comparable performance. V¨olker et al. (2018) employed\nAdamW with cosine annealing to train convolutional neural networks to classify and characterize\nerror-related brain signals measured from intracranial electroencephalography (EEG) recordings.\nWhile their paper does not provide a comparison to Adam, they kindly provided us with a direct\ncomparison of the two on their best-performing problem-speciﬁc network architecture Deep4Net\nand a variant of ResNet. AdamW with the same hyperparameter setting as Adam yielded higher\ntest set accuracy on Deep4Net (73.68% versus 71.37%) and statistically signiﬁcantly higher test\nset accuracy on ResNet (72.04% versus 61.34%). Radford et al. (2018) employed AdamW to train\nTransformer (Vaswani et al., 2017) architectures to obtain new state-of-the-art results on a wide\nrange of benchmarks for natural language understanding. Zhang et al. (2018) compared L2 reg-\nularization vs. weight decay for SGD, Adam and the Kronecker-Factored Approximate Curvature\n(K-FAC) optimizer (Martens & Grosse, 2015) on the CIFAR datasets with ResNet and VGG archi-\ntectures, reporting that decoupled weight decay consistently outperformed L2 regularization in cases\nwhere they differ.\n8\n\n\nPublished as a conference paper at ICLR 2019\n5\nCONCLUSION AND FUTURE WORK\nFollowing suggestions that adaptive gradient methods such as Adam might lead to worse generaliza-\ntion than SGD with momentum (Wilson et al., 2017), we identiﬁed and exposed the inequivalence\nof L2 regularization and weight decay for Adam. We empirically showed that our version of Adam\nwith decoupled weight decay yields substantially better generalization performance than the com-\nmon implementation of Adam with L2 regularization. We also proposed to use warm restarts for\nAdam to improve its anytime performance.\nOur results obtained on image classiﬁcation datasets must be veriﬁed on a wider range of tasks,\nespecially ones where the use of regularization is expected to be important. It would be interesting\nto integrate our ﬁndings on weight decay into other methods which attempt to improve Adam, e.g,\nnormalized direction-preserving Adam (Zhang et al., 2017). While we focused our experimental\nanalysis on Adam, we believe that similar results also hold for other adaptive gradient methods,\nsuch as AdaGrad (Duchi et al., 2011) and AMSGrad (Reddi et al., 2018).\n6\nACKNOWLEDGMENTS\nWe thank Patryk Chrabaszcz for help with running experiments with ImageNet32x32; Matthias\nFeurer and Robin Schirrmeister for providing valuable feedback on this paper in several iterations;\nand Martin V¨olker, Robin Schirrmeister, and Tonio Ball for providing us with a comparison of\nAdamW and Adam on their EEG data. We also thank the following members of the deep learning\ncommunity for implementing decoupled weight decay in various deep learning libraries:\n• Jingwei Zhang, Lei Tai, Robin Schirrmeister, and Kashif Rasul for their implementations\nin PyTorch (see https://github.com/pytorch/pytorch/pull/4429)\n• Phil Jund for his implementation in TensorFlow described at\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/opt/\nDecoupledWeightDecayExtension\n• Sylvain Gugger, Anand Saha, Jeremy Howard and other members of fast.ai for their imple-\nmentation available at https://github.com/sgugger/Adam-experiments\n• Guillaume Lambard for his implementation in Keras available at https://github.\ncom/GLambard/AdamW_Keras\n• Yagami Lin for his implementation in Caffe available at https://github.com/\nYagami123/Caffe-AdamW-AdamWR\nThis work was supported by the European Research Council (ERC) under the European Union’s\nHorizon 2020 research and innovation programme under grant no. 716721, by the German Research\nFoundation (DFG) under the BrainLinksBrainTools Cluster of Excellence (grant number EXC 1086)\nand through grant no. INST 37/935-1 FUGG, and by the German state of Baden-W¨urttemberg\nthrough bwHPC.\nREFERENCES\nLaurence Aitchison. A uniﬁed theory of adaptive stochastic gradient descent as Bayesian ﬁltering.\narXiv:1507.02030, 2018.\nPatryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. A downsampled variant of ImageNet as an\nalternative to the CIFAR datasets. arXiv:1707.08819, 2017.\nEkin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:\nLearning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018.\nLaurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize\nfor deep nets. arXiv:1703.04933, 2017.\nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and\nstochastic optimization. The Journal of Machine Learning Research, 12:2121–2159, 2011.\n9\n\n\nPublished as a conference paper at ICLR 2019\nXavier Gastaldi. Shake-Shake regularization. arXiv preprint arXiv:1705.07485, 2017.\nStephen Jos´e Hanson and Lorien Y Pratt. Comparing biases for minimal network construction with\nback-propagation. In Proceedings of the 1st International Conference on Neural Information\nProcessing Systems, pp. 177–185, 1988.\nGao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger.\nSnapshot ensembles: Train 1, get m for free. arXiv:1704.00109, 2017.\nNitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-\nter Tang.\nOn large-batch training for deep learning: Generalization gap and sharp minima.\narXiv:1609.04836, 2016.\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv:1412.6980,\n2014.\nAlex Krizhevsky. Learning multiple layers of features from tiny images. 2009.\nHao Li, Zheng Xu, Gavin Taylor, and Tom Goldstein. Visualizing the loss landscape of neural nets.\narXiv preprint arXiv:1712.09913, 2017.\nIlya Loshchilov and Frank Hutter.\nSGDR: stochastic gradient descent with warm restarts.\narXiv:1608.03983, 2016.\nJames Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate\ncurvature. In International conference on machine learning, pp. 2408–2417, 2015.\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\nconvolutional generative adversarial networks. arXiv:1511.06434, 2015.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language un-\nderstanding by generative pre-training.\nURL https://s3-us-west-2. amazonaws. com/openai-\nassets/research-covers/language-unsupervised/language understanding paper. pdf, 2018.\nSashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. Inter-\nnational Conference on Learning Representations, 2018.\nLeslie N Smith. Cyclical learning rates for training neural networks. arXiv:1506.01186v3, 2016.\nTijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running\naverage of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26–\n31, 2012.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-\nmation Processing Systems, pp. 5998–6008, 2017.\nMartin V¨olker, Jiˇr´ı Hammer, Robin T Schirrmeister, Joos Behncke, Lukas DJ Fiederer, Andreas\nSchulze-Bonhage, Petr Marusiˇc, Wolfram Burgard, and Tonio Ball. Intracranial error detection\nvia deep learning. arXiv preprint arXiv:1805.01667, 2018.\nJianfeng Wang, Ye Yuan, Gang Yu, and Sun Jian. Sface: An efﬁcient network for face detection in\nlarge scale variations. arXiv preprint arXiv:1804.06559, 2018.\nAshia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht.\nThe\nmarginal value of adaptive gradient methods in machine learning. arXiv:1705.08292, 2017.\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich\nZemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual\nattention. In International Conference on Machine Learning, pp. 2048–2057, 2015.\nShuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang. Wider face: A face detection bench-\nmark. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n5525–5533, 2016.\n10\n\n\nPublished as a conference paper at ICLR 2019\nGuodong Zhang, Chaoqi Wang, Bowen Xu, and Roger Grosse. Three mechanisms of weight decay\nregularization. arXiv preprint arXiv:1810.12281, 2018.\nZijun Zhang, Lin Ma, Zongpeng Li, and Chuan Wu.\nNormalized direction-preserving adam.\narXiv:1709.04546, 2017.\nBarret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V. Le. Learning transferable architectures\nfor scalable image recognition. In arXiv:1707.07012 [cs.CV], 2017.\n11\n\n\nPublished as a conference paper at ICLR 2019\nAppendix\nA\nFORMAL ANALYSIS OF WEIGHT DECAY VS L2 REGULARIZATION\nProof of Proposition 1\nThe proof for this well-known fact is straight-forward. SGD without weight decay has the following\niterates on f reg\nt (θ) = ft(θ) + λ′\n2 ∥θ∥2\n2:\nθt+1 ←θt −α∇f reg\nt (θt) = θt −α∇ft(θt) −αλ′θt.\n(5)\nSGD with weight decay has the following iterates on ft(θ):\nθt+1 ←(1 −λ)θt −α∇ft(θt).\n(6)\nThese iterates are identical since λ′ = λ\nα.\nProof of Proposition 2\nSimilarly to the proof of Proposition 1, the iterates of O without weight decay on f reg\nt (θ) = ft(θ)+\n1\n2λ′ ∥θ∥2\n2 and O with weight decay λ on ft are, respectively:\nθt+1\n←\nθt −αλ′Mtθt −αMt∇ft(θt).\n(7)\nθt+1\n←\n(1 −λ)θt −αMt∇ft(θt).\n(8)\nThe equality of these iterates for all θt would imply λθt = αλ′Mtθt. This can only hold for all θt\nif Mt = kI, with k ∈R, which is not the case for O. Therefore, no L2 regularizer λ′ ∥θ∥2\n2 exists\nthat makes the iterates equivalent.\nProof of Proposition 3\nO without weight decay has the following iterates on f sreg\nt\n(θ) = ft(θ) + λ′\n2\n\r\rθ ⊙√s\n\r\r2\n2:\nθt+1\n←\nθt −α∇f sreg\nt\n(θt)/s\n(9)\n=\nθt −α∇ft(θt)/s −αλ′θt ⊙s/s\n(10)\n=\nθt −α∇ft(θt)/s −αλ′θt,\n(11)\nwhere the division by s is element-wise. O with weight decay has the following iterates on ft(θ):\nθt+1\n←\n(1 −λ)θt −α∇f(θt)/s\n(12)\n=\nθt −α∇f(θt)/s −λθt,\n(13)\nThese iterates are identical since λ′ = λ\nα.\nB\nADDITIONAL PRACTICAL IMPROVEMENTS OF ADAM\nHaving discussed decoupled weight decay for improving Adam’s generalization, in this section we\nintroduce two additional components to improve Adam’s performance in practice.\nB.1\nNORMALIZED WEIGHT DECAY\nOur preliminary experiments showed that different weight decay factors are optimal for different\ncomputational budgets (deﬁned in terms of the number of batch passes). Relatedly, Li et al. (2017)\ndemonstrated that a smaller batch size (for the same total number of epochs) leads to the shrinking\neffect of weight decay being more pronounced. Here, we propose to reduce this dependence by nor-\nmalizing the values of weight decay. Speciﬁcally, we replace the hyperparameter λ by a new (more\nrobust) normalized weight decay hyperparameter λnorm, and use this to set λ as λ = λnorm\nq\nb\nBT ,\nwhere b is the batch size, B is the total number of training points and T is the total number of\nepochs.2 Thus, λnorm can be interpreted as the weight decay used if only one batch pass is al-\nlowed. We emphasize that our choice of normalization is merely one possibility informed by few\nexperiments; a more lasting conclusion we draw is that using some normalization can substantially\nimprove results.\n2In the context of our AdamWR variant discussed in Section B.2, T is the total number of epochs in the\ncurrent restart.\n1\n\n\nPublished as a conference paper at ICLR 2019\nB.2\nADAM WITH COSINE ANNEALING AND WARM RESTARTS\nWe now apply cosine annealing and warm restarts to Adam, following our recent work (Loshchilov\n& Hutter, 2016). There, we proposed Stochastic Gradient Descent with Warm Restarts (SGDR) to\nimprove the anytime performance of SGD by quickly cooling down the learning rate according to a\ncosine schedule and periodically increasing it. SGDR has been successfully adopted to lead to new\nstate-of-the-art results for popular image classiﬁcation benchmarks (Huang et al., 2017; Gastaldi,\n2017; Zoph et al., 2017), and we therefore already tried extending it to Adam shortly after proposing\nit. However, while our initial version of Adam with warm restarts had better anytime performance\nthan Adam, it was not competitive with SGD with warm restarts, precisely because L2 regularization\nwas not working as well as in SGD. Now, having ﬁxed this issue by means of the original weight\ndecay regularization (Section 2) and also having introduced normalized weight decay (Section B.1),\nour original work on cosine annealing and warm restarts directly carries over to Adam.\nIn the interest of keeping the presentation self-contained, we brieﬂy describe how SGDR schedules\nthe change of the effective learning rate in order to accelerate the training of DNNs. Here, we\ndecouple the initial learning rate α and its multiplier ηt used to obtain the actual learning rate at\niteration t (see, e.g., line 8 in Algorithm 1). In SGDR, we simulate a new warm-started run/restart of\nSGD once Ti epochs are performed, where i is the index of the run. Importantly, the restarts are not\nperformed from scratch but emulated by increasing ηt while the old value of θt is used as an initial\nsolution. The amount by which ηt is increased controls to which extent the previously acquired\ninformation (e.g., momentum) is used. Within the i-th run, the value of ηt decays according to a\ncosine annealing (Loshchilov & Hutter, 2016) learning rate for each batch as follows:\nηt = η(i)\nmin + 0.5(η(i)\nmax −η(i)\nmin)(1 + cos(πTcur/Ti)),\n(14)\nwhere η(i)\nmin and η(i)\nmax are ranges for the multiplier and Tcur accounts for how many epochs have\nbeen performed since the last restart. Tcur is updated at each batch iteration t and is thus not\nconstrained to integer values. Adjusting (e.g., decreasing) η(i)\nmin and η(i)\nmax at every i-th restart (see\nalso Smith (2016)) could potentially improve performance, but we do not consider that option here\nbecause it would involve additional hyperparameters. For η(i)\nmax = 1 and η(i)\nmin = 0, one can simplify\nEq. (14) to\nηt = 0.5 + 0.5 cos(πTcur/Ti).\n(15)\nIn order to achieve good anytime performance, one can start with an initially small Ti (e.g., from\n1% to 10% of the expected total budget) and multiply it by a factor of Tmult (e.g., Tmult = 2) at\nevery restart. The (i + 1)-th restart is triggered when Tcur = Ti by setting Tcur to 0. An example\nsetting of the schedule multiplier is given in C.\nOur proposed AdamWR algorithm represents AdamW (see Algorithm 2) with ηt following Eq. (15)\nand λ computed at each iteration using normalized weight decay described in Section B.1. We note\nthat normalized weight decay allowed us to use a constant parameter setting across short and long\nruns performed within AdamWR and SGDWR (SGDW with warm restarts).\nC\nAN EXAMPLE SETTING OF THE SCHEDULE MULTIPLIER\nAn example schedule of the schedule multiplier ηt is given in SuppFigure 1 for Ti=0 = 100 and\nTmult = 2. After the initial 100 epochs the learning rate will reach 0 because ηt=100 = 0. Then,\nsince Tcur = Ti=0, we restart by resetting Tcur = 0, causing the multiplier ηt to be reset to 1 due\nto Eq. (15). This multiplier will then decrease again from 1 to 0, but now over the course of 200\nepochs because Ti=1 = Ti=0Tmult = 200. Solutions obtained right before the restarts, when ηt = 0\n(e.g., at epoch indexes 100, 300, 700 and 1500 as shown in SuppFigure 1) are recommended by the\noptimizer as the solutions, with more recent solutions prioritized.\nD\nADDITIONAL RESULTS\nWe investigated whether the use of much longer runs (1800 epochs) of “standard Adam” (Adam\nwith L2 regularization and a ﬁxed learning rate) makes the use of cosine annealing unnecessary.\n2\n\n\nPublished as a conference paper at ICLR 2019\n200\n400\n600\n800\n1000\n1200\n1400\n0\n0.2\n0.4\n0.6\n0.8\n1\nEpochs\nLearning rate multiplier η\nT0=100, Tmult=2\nSuppFigure 1: An example schedule of the learning rate multiplier as a function of epoch index.\nThe ﬁrst run is scheduled to converge at epoch Ti=0 = 100, then the budget for the next run is\ndoubled as Ti=1 = Ti=0Tmult = 200, etc.\nSuppFigure 2 shows the results of standard Adam for a 4 by 4 logarithmic grid of hyperparame-\nter settings (the coarseness of the grid is due to the high computational expense of runs for 1800\nepochs). Even after taking the low resolution of the grid into account, the results appear to be at best\ncomparable to the ones obtained with AdamW with 18 times less epochs and a smaller network (see\nSuppFigure 3, top row, middle). These results are not very surprising given Figure 1 in the main\npaper (which demonstrates both the improvements possible by using some learning rate schedule,\nsuch as cosine annealing, and the effectiveness of decoupled weight decay).\nOur experimental results with Adam and SGD suggest that the total runtime in terms of the number\nof epochs affect the basin of optimal hyperparameters (see SuppFigure 3). More speciﬁcally, the\ngreater the total number of epochs the smaller the values of the weight decay should be. SuppFigure\n4 shows that our remedy for this problem, the normalized weight decay deﬁned in Eq. (15), sim-\npliﬁes hyperparameter selection because the optimal values observed for short runs are similar to\nthe ones for much longer runs. We used our initial experiments on CIFAR-10 to suggest the square\nroot normalization we proposed in Eq. (15) and double-checked that this is not a coincidence on the\nImageNet32x32 dataset (Chrabaszcz et al., 2017), a downsampled version of the original ImageNet\ndataset with 1.2 million 32×32 pixels images, where an epoch is 24 times longer than on CIFAR-10.\nThis experiment also supported the square root scaling: the best values of the normalized weight de-\ncay observed on CIFAR-10 represented nearly optimal values for ImageNet32x32 (see SuppFigure\n3). In contrast, had we used the same raw weight decay values λ for ImageNet32x32 as for CIFAR-\n10 and for the same number of epochs, without the proposed normalization, λ would have been\nroughly 5 times too large for ImageNet32x32, leading to much worse performance. The optimal\nnormalized weight decay values were also very similar (e.g., λnorm = 0.025 and λnorm = 0.05)\nacross SGDW and AdamW. These results clearly show that normalizing weight decay can substan-\ntially improve performance; while square root scaling performed very well in our experiments we\nemphasize that these experiments were not very comprehensive and that even better scaling rules\nare likely to exist.\nSuppFigure 4 is the equivalent of Figure 3 in the main paper, but for ImageNet32x32 instead of for\nCIFAR-10. The qualitative results are identical: weight decay leads to better training loss (cross-\nentropy) than L2 regularization, and to an even greater improvement of test error.\nSuppFigure 5 and SuppFigure 6 are the equivalents of Figure 4 in the main paper but supplemented\nwith training loss curves in its bottom row. The results show that Adam and its variants with decou-\npled weight decay converge faster (in terms of training loss) on CIFAR-10 than the corresponding\nSGD variants (the difference for ImageNet32x32 is small). As is discussed in the main paper, when\nthe same values of training loss are considered, AdamW demonstrates better values of test error than\nAdam. Interestingly, SuppFigure 5 and SuppFigure 6 show that the restart variants AdamWR and\nSGDWR also demonstrate better generalization than AdamW and SGDW, respectively.\n3\n\n\nPublished as a conference paper at ICLR 2019\nSuppFigure 2: Performance of “standard Adam”: Adam with L2 regularization and a ﬁxed learning\nrate. We show the ﬁnal test error of a 26 2x96d ResNet on CIFAR-10 after 1800 epochs of the\noriginal Adam for different settings of learning rate and weight decay used for L2 regularization.\n4\n\n\nPublished as a conference paper at ICLR 2019\nSuppFigure 3: Effect of normalized weight decay. We show the ﬁnal test Top-1 error on CIFAR-\n10 (ﬁrst two rows for AdamW without and with normalized weight decay) and Top-5 error on\nImageNet32x32 (last two rows for AdamW and SGDW, both with normalized weight decay) of a\n26 2x64d ResNet after different numbers of epochs (see columns). While the optimal settings of the\nraw weight decay change signiﬁcantly for different runtime budgets (see the ﬁrst row), the values\nof the normalized weight decay remain very similar for different budgets (see the second row) and\ndifferent datasets (here, CIFAR-10 and ImageNet32x32), and even across AdamW and SGDW.\n5\n\n\nPublished as a conference paper at ICLR 2019\nSuppFigure 4: Learning curves (top row) and generalization results (Top-5 errors in bottom row)\nobtained by a 26 2x96d ResNet trained with Adam and AdamW on ImageNet32x32.\n6\n\n\nPublished as a conference paper at ICLR 2019\nSuppFigure 5: Test error curves (top row) and training loss curves (bottom row) for CIFAR-10.\n7\n\n\nPublished as a conference paper at ICLR 2019\nSuppFigure\n6:\nTest error curves (top row) and training loss curves (bottom row) for Ima-\ngeNet32x32.\n8\n"
}