{
  "filename": "1805.10123v4.pdf",
  "num_pages": 13,
  "pages": [
    "TADAM: Task dependent adaptive metric for\nimproved few-shot learning\nBoris N. Oreshkin\nElement AI\nboris@elementai.com\nPau Rodriguez\nElement AI, CVC-UAB\npau.rodriguez@elementai.com\nAlexandre Lacoste\nElement AI\nallac@elementai.com\nAbstract\nFew-shot learning has become essential for producing models that generalize\nfrom few examples. In this work, we identify that metric scaling and metric task\nconditioning are important to improve the performance of few-shot algorithms.\nOur analysis reveals that simple metric scaling completely changes the nature of\nfew-shot algorithm parameter updates. Metric scaling provides improvements\nup to 14% in accuracy for certain metrics on the mini-Imagenet 5-way 5-shot\nclassiﬁcation task. We further propose a simple and effective way of conditioning a\nlearner on the task sample set, resulting in learning a task-dependent metric space.\nMoreover, we propose and empirically test a practical end-to-end optimization\nprocedure based on auxiliary task co-training to learn a task-dependent metric\nspace. The resulting few-shot learning model based on the task-dependent scaled\nmetric achieves state of the art on mini-Imagenet. We conﬁrm these results on\nanother few-shot dataset that we introduce in this paper based on CIFAR100. Our\ncode is publicly available at https://github.com/ElementAI/TADAM.\n1\nIntroduction\nHumans can learn to identify new categories from few examples, even from a single one [2]. Few-shot\nlearning has recently attracted signiﬁcant attention [33, 28, 29, 24, 17, 16], as it aims to produce\nmodels that can generalize from small amounts of labeled data. In the few-shot setting, one aims to\nlearn a model that extracts information from a set of support examples (sample set) to predict the\nlabels of instances from a query set. Recently, this problem has been reframed into the meta-learning\nframework [22], i.e. the model is trained so that given a sample set or task, produces a classiﬁer for\nthat speciﬁc task. Thus, the model is exposed to different tasks (or episodes) during the training\nphase, and it is evaluated on a non-overlapping set of new tasks [33].\nTwo recent approaches have attracted signiﬁcant attention in the few-shot learning domain: Matching\nNetworks [33], and Prototypical Networks [28]. In both approaches, the sample set and the query set\nare embedded with a neural network, and nearest neighbor classiﬁcation is used given a metric in the\nembedded space. Since then, the problem of learning the most suitable metric for few-shot learning\nhas been of interest to the ﬁeld [33, 28, 29, 17, 16]. Learning a metric space in the context of few-shot\nlearning generally implies identifying a suitable similarity measure (e.g. cosine or Euclidean), a\nfeature extractor mapping raw inputs onto similarity space (e.g. convolutional stack for images or\nLSTM stack for text), a cost function to drive the parameter updates, and a training scheme (often\nepisodic). Although the individual components in this list have been explored, the relationships\nbetween them have not received considerable attention.\nIn the current work we aim to close this gap. We show that taking into account the interaction\nbetween the identiﬁed components leads to signiﬁcant improvements in the few-shot generalization.\nIn particular, we show that a non-trivial interaction between the similarity metric and the cost function\ncan be exploited to improve the performance of a given similarity metric via scaling. Using this\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.\narXiv:1805.10123v4  [cs.LG]  25 Jan 2019\n",
    "mechanism we close more than the 10% gap in performance between the cosine similarity and\nthe Euclidean distance reported in [28]. Even more importantly, we extend the very notion of the\nmetric space by making it task dependent via conditioning the feature extractor on the speciﬁc task.\nHowever, learning such a space is in general more challenging than learning a static one. Hence,\nwe ﬁnd a solution in exploiting the interaction between the conditioned feature extractor and the\ntraining procedure based on auxiliary co-training on a simpler task. Our proposed few-shot learning\narchitecture based on task-dependent scaled metric achieves superior performance on two challenging\nfew-shot image classiﬁcation datasets. It shows up to 8.5% absolute accuracy improvement over the\nbaseline (Snell et al. [28]), and 4.8% over the state-of-the-art [17] on the 5-shot, 5-way mini-Imagenet\nclassiﬁcation task, reaching 76.7% of accuracy, which is the best-reported accuracy on this dataset.\n1.1\nBackground\nWe consider the episodic M-shot, K-way classiﬁcation scenario. In this scenario, a learning algorithm\nis provided with a sample set S = {(xi, yi)}MK\ni=1 consisting of M examples for each of K classes and\na query set Q = {(xi, yi)}q\ni=1 for a task to be solved within a given episode. The sample set provides\nthe task information via observations xi ∈RDx and their respective class labels yi ∈{1, . . . , K}.\nGiven the information in the sample set S, the learning algorithm is able to classify individual\nsamples from the query set Q. Next, we deﬁne a similarity measure d : RDz×Dz →R. Note that\nd does not have to satisfy the classical metric properties (non-negativity, symmetry, subadditivity)\nto be useful in the context of few-shot learning. The dimensionality of metric input, Dz, will most\nnaturally be related to the size of embedding created by a (deep) feature extractor fφ : RDx →RDz,\nparameterized by φ, mapping x to z. Here φ ∈RDφ is a list of parameters deﬁning fφ, e.g. a list of\nweights in a neural network. The set of representations (fφ(xi), yi), ∀(xi, yi) ∈S can directly be\nused to solve the few-shot learning classiﬁcation problem by association. For example, Matching\nnetworks [33] use sample-wise attention mechanism to perform kernel label regression. Instead,\nSnell et al. [28] deﬁned a feature representation ck for each class k as the mean over embeddings\nbelonging to Sk: ck =\n1\nK\nP\nxi∈Sk fφ(xi). To learn φ, they minimize −log pφ(y = k|x) using the\nsoftmax over prototypes ck to deﬁne the likelihood: pφ(y = k|x) = softmax(−d(fφ(x), ck)).\n1.2\nSummary of contributions\nMetric Scaling: To our knowledge, this is the ﬁrst study to (i) propose metric scaling to improve\nperformance of few-shot algorithms, (ii) mathematically analyze its effects on objective function\nupdates and (iii) empirically demonstrate its positive effects on few-shot performance.\nTask Conditioning: We use a task encoding network to extract a task representation based on the\ntask’s sample set. This is used to inﬂuence the behavior of the feature extractor through FILM [19].\nAuxiliary task co-training: We show that co-training the feature extraction on a conventional\nsupervised classiﬁcation task reduces training complexity and provides better generalization.\n1.3\nRelated work\nThree main approaches for solving the few-shot classiﬁcation problem can be identiﬁed in the\nliterature. The ﬁrst one, which is used in this work, is the meta-learning approach, i.e. learning a\nmodel that, given a task (set of labeled data), produces a classiﬁer that generalizes across all tasks\n[31, 25]. This is the case of Matching Networks [33], which optionally use a Recurrent Neural\nNetwork (RNN) to accumulate information about a given task. In MAML [6], the parameters of an\narbitrary learner model are optimized so that they can be quickly adapted to a particular task. In\n“Optimization as a model” [22], a learner model is adapted to a new episodic task by a recurrent meta-\nlearner producing efﬁcient parameter updates. A more general approach was proposed by Santoro\net al. [24], where the meta-learner is trained to represent entries from a sample set in an external\nmemory. Similarly, adaResNet [17] uses memory and the sample set to produce shift coefﬁcients on\nthe neuron activations of the query set classiﬁer. Many recent approaches focus on learning a metric\non the episodic feature space. Prototypical networks [28] use a feed-forward neural network to embed\nthe task examples and perform nearest neighbor classiﬁcation with the class centroids. The relation\nnetwork approach by Sung et al. [29] introduces a separate learnable similarity metric. SNAIL\n[16] uses an explicit attention mechanism applicable both to supervised and to the sequence based\n2\n",
    "reinforcement learning tasks. It has also been shown that these approaches beneﬁt from leveraging\nunlabeled and simulated data [23, 34].\nA second approach aims to maximize the distance between examples from different classes [10].\nSimilarly, in [7], a contrastive loss function is used to learn to project data onto a manifold that is\ninvariant to deformations in the input space. In the same vein, in [5, 26, 30], triplet loss is used\nfor learning a representation for few-shot learning. The attentive recurrent comparators [27] go\nbeyond classical siamese approaches and use a recurrent architecture to learn to perform pairwise\ncomparisons and predict if the compared examples belong to the same class.\nThe third approach relies on Bayesian modeling of the prior distribution of the different categories\nlike in Li et al. [15], Bauer et al. [1], or Lake et al. [13], Edwards and Storkey [4], Lacoste et al. [12]\nwho rely on hierarchical Bayesian modeling.\nAs for task conditioning, [3, 18, 19] proposed conditional batch normalization for style transfer and\nvisual reasoning. Differently, we modify the conditioning scheme to adapt it to few-shot learning,\nintroducing γ0, β0 priors, and auxiliary co-training. In the few-shot learning context, task conditioning\nideas can be traced back to [33], although in an implicit form as there is no notion of task embedding.\nIn our work, we explicitly introduce a task representation (see Fig. 1) computed as the mean of the task\nclass centroids (task prototypes). This is much simpler than individual sample level LSTM/attention\nmodels in [33]. Conditioning in [33] is applied as a postprocessing of the output of a ﬁxed feature\nextractor. We propose to condition the feature extractor by predicting its own batch normalization\nparameters thus making feature extractor behaviour task-dynamic without cumbersome ﬁne-tuning\non support set. In order to train the task conditioned architecture we use multitask training with\na usual 64-way classiﬁcation task. Even though auxiliary co-training is beneﬁcial for learning in\ngeneral, “little is known on when multitask learning works and whether there are data characteristics\nthat help to determine its success” [20]. We show that combining task conditioning and auxiliary\nco-training is beneﬁcial in the context of few-shot learning.\nThe scaling and temperature adjustment in the softmax was discussed by Hinton et al. [9] in the\ncontext of model distillation. We propose to use it in the context of the few-shot learning scenario\nand provide novel theoretical and empirical results quantifying the effects of scaling parameter.\nThe rest of the paper is organized as follows. Section 2 describes our contributions in detail. Section 3\nhighlights the importance of each contribution via an ablation study. The study is performed over two\ndifferent benchmarks in the regime of 1-shot, 5-shot and 10-shot learning to verify if conclusions hold\nacross different setups. Finally, Section 4 concludes the paper and outlines future research directions.\n2\nModel Description\n2.1\nMetric Scaling\nSnell et al. [28] using approach described in detail in Section 1.1 found that the Euclidean distance\noutperformed the cosine distance used in Vinyals et al. [33]. We hypothesize that the improvement\ncould be directly attributed to the interaction of the different scaling of the metrics with the softmax.\nMoreover, the dimensionality of the output is known to have a direct impact on the output scale\neven for the Euclidean distance [32]. Hence, we propose to scale the distance metric by a learnable\ntemperature, α, pφ,α(y = k|x) = softmax(−αd(z, ck)), to enable the model to learn the best regime\nfor each similarity metric, thus improving the performances of all metrics. To further understand the\nrole of α, we analyze the class-wise cross-entropy loss function, Jk(φ, α),1\nJk(φ, α) =\nX\nxi∈Qk\nh\nαd(fφ(xi), ck) + log\nX\nj\nexp(−αd(fφ(xi), cj))\ni\n,\n(1)\nwhere Qk = {(xi, yi) ∈Q : yi = k} is the query set corresponding to the class k. Its gradient,\nwhich is used to update parameters φ is given by the following expression:\n∂\n∂φJk(φ, α) = α\nX\nxi∈Qk\n\"\n∂\n∂φd(fφ(xi), ck) −\nP\nj exp(−αd(fφ(xi), cj)) ∂\n∂φd(fφ(xi), cj)\nP\nj exp(−αd(fφ(xi), cj))\n#\n. (2)\n1Note that the total loss is simply J(φ, α) = P\nk Jk(φ, α)\n3\n",
    "yi\nSimilarity\nmetric\nfɸ(x, )\nx* \nsoftmax\nTEN network \nfɸ(x, )\nð \nTask\nrepresentation\nfɸ(x, 0)\nClass\nrepresentation\n\nyi\nyi\nxi\nxi\nFigure 1: Proposed few-shot architecture. Blocks with shared parameters have dashed border.\nAt ﬁrst glance, the effect of α on the expression of the derivative is twofold: (i) an overall scaling,\nand (ii) regulating the sharpness of weighting in the second term inside the brackets on the RHS.\nBelow we explore the behavior of the α-normalized2 gradient in the limits α →0 and α →∞.\nLemma 1 (Metric scaling). If the following assumptions hold:\nA1 : d(fφ(x), ck) ̸= d(fφ(x′), ck), ∀k, x ̸= x′ ∈Qk;\nA2 :\n\f\f\f ∂\n∂φd(fφ(x), c)\n\f\f\f < ∞, ∀x, c, φ,\nthen it is true that:\nlim\nα→0\n1\nα\n∂\n∂φJk(φ, α) =\nX\nxi∈Qk\nhK −1\nK\n∂\n∂φd(fφ(xi), ck) −1\nK\nX\nj̸=k\n∂\n∂φd(fφ(xi), cj)\ni\n,\n(3)\nlim\nα→∞\n1\nα\n∂\n∂φJk(φ, α) =\nX\nxi∈Qk\nh ∂\n∂φd(fφ(xi), ck) −∂\n∂φd(fφ(xi), cj∗\ni )\ni\n;\n(4)\nwhere j∗\ni = arg minj d(fφ(xi), cj).\nProof. Please refer to Appendix A.\nFrom Eq. (3), it is clear that for small α values, the ﬁrst term minimizes the embedding distance\nbetween query samples and their corresponding prototypes. The second term maximizes the embed-\nding distance between the samples and the prototypes of the non-belonging categories. For large α\nvalues (Eq. (4)), the ﬁrst term is the same as in Eq. (3); while the second term maximizes the distance\nof the sample with the closest wrongly assigned prototype cj∗\ni (if any). If j∗\ni = k (no error), the\nderivative contribution of the point xi is zero. This is equivalent to learning only from the hardest\nexamples resulting in association errors. Thus, the two different regimes of α favor either minimizing\nthe overlap of the sample distributions or correcting cluster assignments sample-wise.\nThe large α regime is more directly related to resolving the few-shot classiﬁcation errors. At the\nsame time, the update strategy generated in this regime has a drawback. As the optimization proceeds\nand the classiﬁcation accuracy increases, the number of incorrectly classiﬁed samples reduces on\naverage, and this leads to the reduction in the average effective batch size (more samples generate\nzero derivatives). Therefore, our hypothesis is that there is an optimal value of scaling parameter α\nfor a given combination of dataset, metric and task. Section 3.4 empirically demonstrates that the\noptimal value of α indeed exists and it can be e.g. cross-validated on a validation set.\n2.2\nTask conditioning\nUp until now we assumed the feature extractor fφ(·) to be task-independent. A dynamic task-\nconditioned feature extractor should be better suited for ﬁnding correct associations between given\nsample set class representations and query samples, this is implicitly done by Vinyals et al. [33]\nwith a bidirectional LSTM as a postprocessing of a ﬁxed feature extractor. Differently, we explicitly\ndeﬁne a dynamic feature extractor fφ(x, Γ), where Γ is the set of parameters predicted from a task\nrepresentation such that the performance of fφ(x, Γ) is optimized given the task sample set S. This\n2The effect of α-related gradient scaling is trivial.\n4\n",
    "Table 1: mini-Imagenet (Vinyals et al. [33]), 5-way classiﬁcation results. †Our re-implementation.\n1-shot\n5-shot\n10-shot\nMeta Nets [22]\n43.4\n60.6\n-\nMatching Networks [33]\n46.6\n60.0\n-\nMAML [6]\n48.7\n63.1\n-\nProto Nets [28]\n49.4\n68.2\n74.3†\nRelation Net [29]\n50.4\n65.3\n-\nSNAIL [16]\n55.7\n68.9\n-\nDiscriminative k-shot [1]\n56.3\n73.9\n78.5\nadaResNet [17]\n56.9\n71.9\n-\nOurs\n58.5\n76.7\n80.8\nis related to the FILM conditioning layer [19] and conditional batch normalization [3, 18] of the form\nhℓ+1 = γ ⊙hℓ+ β, where γ and β are scaling and shift vectors applied to the layer hℓ. Concretely,\nwe propose to use the mean of the class prototypes as the task representation, c = 1\nK\nP\nk ck, encode\nit with a task embedding network (TEN), and predict layer-level element-wise scale and shift vectors\nγ, β for each convolutional layer in the feature extractor (see Figures 1 and 2 in the Supplementary\nMaterials, Section S1). The task representation deﬁned as the mean of task class centroids (i) reduces\nthe dimensionality of the TEN input and (ii) replaces expensive RNN/CNN/attention modeling. On\nthe other hand, it is an effective way to cluster tasks. Tasks having larger number of similar classes in\ncommon will tend to cluster closer in the task representation space.\nOur implementation of the TEN (see Supplementary Materials, Section S1 for more details) uses\ntwo separate fully connected residual networks to generate vectors γ, β. Following the terminology\nin [18], the γ parameter is learned in the delta regime, i.e. predicting deviation from unity. The\nmost critical component in being able to successfully train the TEN was the addition of the scalar L2\npenalized post-multipliers γ0 and β0. They limit the effect of γ (and β) by encoding a prior belief\nthat all components of γ (and β) should be simultaneously close to zero for a given layer unless\ntask conditioning provides a signiﬁcant information gain for this layer. Mathematically, this can be\nexpressed as β = β0gθ(c) and γ = γ0hϕ(c) + 1, where gθ and hϕ are predictors of β and γ.\n2.3\nArchitecture\nThe overall proposed few-shot classiﬁcation architecture is depicted in Fig. 1 (see Supplementary\nMaterials, Section S1 for more details). We employ ResNet-12 [8] as the backbone feature extractor.\nIt has 4 blocks of depth 3 with 3x3 kernels and shortcut connections. 2x2 max-pool is applied at\nthe end of each block. Convolutional layer depth starts with 64 ﬁlters and is doubled after every\nmax-pool. Note that this architecture is similar in spirit to architectures used in [1] and [17], but we\ndo not use any projection layers before or after the main backbone ResNet. On the ﬁrst pass over\nsample set, the TEN predicts the values of γ and β parameters for each convolutional layer in the\nfeature extractor from the task representation. Next, the sample set and the query set are processed by\nthe feature extractor conditioned with the values of γ and β just generated. Both outputs are fed into\na similarity metric to ﬁnd an association between class prototypes and query instances. The output of\nsimilarity metric is scaled by scalar α and is fed into a softmax layer.\n2.4\nAuxiliary task co-training\nThe TEN (Section 2.2) introduces additional complexity into the architecture via task conditioning\nlayers inserted after the convolutional and batch norm blocks. We empirically observed that simulta-\nneously optimizing convolutional ﬁlters and the TEN is overly challenging. We solved the problem by\nauxiliary co-training with an additional logit head (the normal 64-way classiﬁcation in mini-Imagenet\ncase). The auxiliary task is sampled with a probability that is annealed over episodes. We annealed it\nusing an exponential decay schedule of the form 0.9⌊20t/T ⌋, where T is the total number of training\nepisodes, t is episode index. The initial auxiliary task selection probability was cross-validated to\nbe 0.9 and the number of decay steps was chosen to be 20. We observed signiﬁcant positive effects\nfrom the auxiliary task co-training (please refer to Section 3.4). The same positive effects were not\n5\n",
    "observed with simple pre-training of the feature extractor. We attribute this to the regularization\neffects achieved via back-propagating auxiliary task gradients together with those of the main task.\nIt is of interest to note that the few-shot co-training with an auxiliary classiﬁcation task is related to\ncurriculum learning [24]. The auxiliary classiﬁcation problem could be considered a part of a simpler\ncurriculum that helps the learner acquire minimal skill level necessary before tackling on harder\nfew-shot classiﬁcation tasks. Being effective at feature extraction (i.e. at task representation) forms a\n“prerequisite” at being effective at re-conditioning features based on the representation of a given task.\n3\nExperimental Results\nTable 1 presents our key result in the context of existing state-of-the art. The ﬁve ﬁrst rows show\napproaches that use the same feature extractor as [33], i.e. four stacked convolutions layers of 64\nﬁlters (32 in [22, 6] to avoid overﬁtting). In the following rows we include models like the one we\npropose, which is based on resnet [8]. Concretely, SNAIL [16], adaResNet [17], and our architecture\nuse four residual blocks of three stacked 3 × 3 convolutional layers, each block followed by max\npooling. Differently, the feature extractor proposed in [1] is based on a ResNet-34 architecture with a\nreduced number of features.\nAs it can be seen, the proposed algorithm signiﬁcantly improves over the existing state-of-the-art\nresults on the mini-Imagenet dataset. In the rest of the section we address the following research\nquestions: (i) can metric scaling improve few-shot classiﬁcation results? (Sections 3.2 and 3.4), (ii)\nwhat are the contributions of each components of our proposed architecture? (Section 3.4), (iii) can\ntask conditioning improve few-shot classiﬁcation results and how important it is at different feature\nextractor depths? (Sections 3.3 and 3.4), and (iv) can auxiliary classiﬁcation task co-training improve\naccuracy on the few-shot classiﬁcation task? (Section 3.4).\n3.1\nExperimental setup and datasets\nThe details of the experimental and training setup are provided in Supplementary Materials, Section S3.\nNote that we focused on mini-Imagenet [33] and Fewshot-CIFAR100 (introduced below) instead of\nOmniglot [14, 33, 28] as the former ones are more challenging, and the error rate is more sensitive to\nmodel improvements.\nmini-Imagenet. The mini-Imagenet dataset was proposed by Vinyals et al. [33]. It has 100 classes,\nwith 600 84 × 84 images per class. Each task is generated by sampling 5 classes uniformly and\n5 training samples per class, the remaining images from the 5 classes are used as query images to\ncompute accuracy. To perform meta-validation and meta-test on unseen tasks (and classes), we isolate\n16 and 20 classes from the original set of 100, leaving 64 classes for the training tasks. We use exactly\nthe same train/validation/test split as the one suggested by Ravi and Larochelle [22].\nFewshot-CIFAR100. We introduce a new image based dataset based on CIFAR100 [11] for few-shot\nlearning. We will refer to it as FC100. The main motivation for introducing this new dataset is\nto validate that the main results appearing in the experimental section generalize well beyond the\nmini-Imagenet. The secondary motivation is that the FC100 is suited for faster few-shot scenario\nprototyping than the mini-Imagenet and it presents a more challenging few-shot learning problem,\nbecause of reduced image size. On top of that, we propose a class split in FC100 to minimize the\ninformation overlap between splits to make it signiﬁcantly more challenging than e.g. Omniglot. The\noriginal CIFAR100 dataset consists of 32 × 32 color images belonging to 100 different classes, 600\nimages per class. The 100 classes are further grouped into 20 superclasses. We split the dataset by\nsuperclass, rather than by individual class to minimize the information overlap. Thus the train split\ncontains 60 classes belonging to 12 superclasses, the validation and test contain 20 classes belonging\nto 5 superclasses each. The exact class split is provided in Supplementary Materials, Section S2. The\ntasks are sampled uniformly at random within train, validation and test subsets. Therefore, each task\nwith high probability contains samples belonging to classes from several superclasses.\n3.2\nOn the similarity metric\nWe re-implemented prototypical networks [28], and use the Euclidean and the cosine similarity to\ntest the effects of scaling (see Section 2). We closely follow the experimental setup deﬁned by Snell\n6\n",
    "Table 2: Average classiﬁcation accuracy in percent with 95% conﬁdence interval. 5-shot, 5 way\nclassiﬁcation task. The three last rows correspond to our implementation, ﬁrst with euclidean distance,\nsecond with cosine distance, and third with the scaled cosine distance.\nmini-Imagenet\nFC100\n5-way train\n20-way train\n5-way train\n20-way train\nProto Nets [28]\n65.8 ± 0.7\n68.2 ± 0.7\nN/A\nN/A\nProto Nets\n67.7 ± 0.2\n68.9 ± 0.3\n51.1 ± 0.2\n50.3 ± 0.3\nPrototypical Cosine\n54.5 ± 1.1\n53.9 ± 0.6\n40.9 ± 0.6\n37.1 ± 1.9\nPrototypical Cosine Scaled\n68.2 ± 0.8\n68.1 ± 0.7\n51.0 ± 0.6\n49.6 ± 0.5\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nConv layer #\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n0\n0\n(a) Results on mini-Imagenet.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nConv layer #\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n0\n0\n(b) Results on FC100.\nFigure 2: Distribution of the absolute values of the TEN scaling and bias parameters γ0 and β0 across\nlayers of ResNet feature extractor. X-axes depict layer number in both subplots. Higher convolutional\nlayers are located closer to the ﬁnal softmax layer.\net al. [28] (same feature extractor and training procedure). The scaling parameter α used on the last\nrow was cross-validated on the validation set. Results are presented in Table 2.\nAs it can be seen in row two of Table 2, our re-implementation of Proto Nets [28] obtained slightly\nbetter performance (68.9% and 67.7%) in 20-way and 5-way training scenarios respectively by\nincreasing the number of training steps from 20K to 40K3.\nImportantly, we conﬁrm the hypothesis that the improvement attributed to the Euclidean distance\nin [28] was due to a scaling effect. Namely, we show that the scaled cosine similarity matches\nvery closely the performance of the Euclidean metric, with an improvement of 14 percentage points\non the mini-Imagenet (similar results on FC100) over the non-scaled version. In order to control\nfor the potential effect that the scaling parameter α may have on the learning rate as indicated by\nEquation (2) training was performed using multiple initial learning rates (covering the range between\n0.0005 and 0.01), obtaining similar accuracy each time. Hereinafter, we report the results with\nthe Euclidean metric for brevity, since the cosine produces similar results. Moreover, since the\nprototypical approach with Euclidean distance as well as with the scaled cosine are close and both\nare superior to [33], we base our results on [28].\n3.3\nTEN importance across layers\nWe hypothesized in Section 2.2 that the TEN conditioning should not be equally important at all\ndepths. Fig. 2 depicts the boxplot of the empirical observations of the learned TEN post-multipliers4\nγ0 and β0 at different depths of the feature extractor. We can see that for the multiplier γ, the absolute\nvalue of its scale γ0 tends to increase as we approach the softmax layer. Interestingly, peaks can be\nobserved every 3 layers (layers 3, 6, 9, 12). The peaks correspond to the location of the convolutional\nlayers preceding the max-pool layers. For the bias parameter β0, the only layer having a large absolute\nvalue of its scale is the last layer, before the softmax. We attribute the observed pattern to the fact\nthat the shallower layers in the feature extractor tend to be less task-speciﬁc than the deeper layers.\nFollowing this intuition, we performed experiments in which we (i) kept the TEN injection solely in\n3With 20K steps it was possible to recover the exact original performance reported in Snell et al. [28], which\nis not included in Table 2 for the sake of brevity.\n4Larger absolute values of γ0 and β0 imply a larger inﬂuence of their respective TEN layers\n7\n",
    "Table 3: Average classiﬁcation accuracy (%) with 95% conﬁdence interval on the 5 way classiﬁcation\ntask, and training with the Euclidean distance. The scale parameter is cross-validated on the validation\nset. AT: auxiliary co-training. TC: task conditioning with TEN.\nmini-Imagenet\nFC100\nα\nAT\nTC\n1-shot\n5-shot\n10-shot\n1-shot\n5-shot\n10-shot\n56.5 ± 0.4\n74.2 ± 0.2\n78.6 ± 0.4\n37.8 ± 0.4\n53.3 ± 0.5\n58.7 ± 0.4\n✓\n56.8 ± 0.3\n75.7 ± 0.2\n79.6 ± 0.4\n38.0 ± 0.3\n54.0 ± 0.5\n59.8 ± 0.3\n✓\n✓\n58.0 ± 0.3\n75.6 ± 0.4\n80.0 ± 0.3\n39.0 ± 0.4\n54.7 ± 0.5\n60.4 ± 0.4\n✓\n✓\n54.4 ± 0.3\n74.6 ± 0.3\n78.7 ± 0.4\n37.8 ± 0.2\n54.0 ± 0.7\n58.8 ± 0.3\n✓\n✓\n✓\n58.5 ± 0.3\n76.7 ± 0.3\n80.8 ± 0.3\n40.1 ± 0.4\n56.1 ± 0.4\n61.6 ± 0.5\n(a) Scaled Euclidean. mini-Imagenet.\n(b) Scaled Euclidean. FC100.\n(c) Scaled Euclidean with TEN. mini-Imagenet.\n(d) Scaled Euclidean with TEN. FC100.\nFigure 3: Metric scale parameter α cross-validation results.\nlayers preceding the max pool and (ii) kept the TEN injection only in the very last layer. Interestingly,\nwe saw that TEN layers with small weight still provide some positive contribution, although most of\nthe contribution is indeed provided by the layers preceding the max pool operation.\n3.4\nAblation study\nIn this section, we study the impact in generalization accuracy of the scaling, task conditioning,\nauxiliary co-training, and the feature extractor. Results are summarized in Table 3.\nFirst, we validated the hypothesis that there is an optimal value of the metric scaling parameter (α)\nfor a given combination of dataset and metric, which is reﬂected in the inverse U-shape of the curves\nin Fig. 3.\nSecond, we studied the effects of the task conditioning described in Section 2.2. No improvement\nwas observed for the task-conditioned ResNet-12 without auxiliary co-training (see Table 3). We\nobserved that learning useful features for the TEN and the main feature extractor at the same time\nis hard and gets stuck in local extrema. The problem is solved by co-training on the auxiliary task\nof predicting Imagenet labels using an additional fully-connected layer with softmax, see Section\n2.4. In effect, we observed that auxiliary co-training provides two beneﬁts: (i) making the initial\nconvergence easier, and (ii) providing regularization on the few-shot learning task by forcing the\nfeature extractor to perform well on two decoupled tasks. The latter beneﬁt can only be observed\nwhen the feature extraction unit is sufﬁciently decoupled on the main task and the auxiliary task via\nthe use of TEN (the feature extractor output is additionally adjusted on the target task using FILM).\nAs it can be seen in the last row of Tables 1 and 3, our model trained with TEN and auxiliary\nco-training outperforms all the baselines and achieves state-of-the-art results.\n4\nConclusions and Future Work\nWe proposed, analyzed, and empirically validated several improvements in the domain of few-shot\nlearning. We showed that the scaled cosine similarity performs at par with Euclidean distance,\n8\n",
    "unlike its unscaled counterpart. In fact, based on our results, we argue that the scaling factor is a\nnecessary standard component of any few-shot learning algorithm relying on a similarity metric\nand the cross-entropy loss function. This is especially important in the context of ﬁnding new more\neffective similarity measures for few-shot learning. Moreover, our theoretical analysis demonstrated\nthat simply scaling the similarity metric results in completely different regimes of parameter updates\nwhen using softmax and categorical cross-entropy. We also identiﬁed that the optimal performance\nis achieved in between two asymptotic regimes of the softmax. This poses the research question of\nexplicitly designing loss functions and the α schedules optimal for few-shot learning. We further\nproposed task representation conditioning as a way to improve the performance of a feature extractor\non the few-shot classiﬁcation task. In this context, designing more powerful task representations, for\nexample, based on higher order statistics of class embeddings, looks like a very promising venue for\nfuture work. The experimental results obtained on two independent challenging datasets demonstrated\nthat the proposed approach signiﬁcantly improves over existing results and achieves state-of-the-art\non few-shot image classiﬁcation task.\nAppendix\nA\nProof of Lemma 1\nFirst, consider the case α →0. Denoting zφ\ni = fφ(xi) we have:\nlim\nα→0\n1\nα\n∂\n∂φJk(φ, α) =\nX\nxi∈Qk\n∂\n∂φd(zφ\ni , ck) −lim\nα→0\nP\nj exp(−αd(zφ\ni , cj)) ∂\n∂φd(zφ\ni , cj)\nP\nj exp(−αd(zφ\ni , cj))\n=\nX\nxi∈Qk\n∂\n∂φd(zφ\ni , ck) −1\nK\nX\nj\n∂\n∂φd(zφ\ni , cj)\n=\nX\nxi∈Qk\nK −1\nK\n∂\n∂φd(zφ\ni , ck) −1\nK\nX\nj̸=k\n∂\n∂φd(zφ\ni , cj).\nSecond, consider the case α →∞:\nlim\nα→∞\n1\nα\n∂\n∂φJk(φ, α) =\nX\nxi∈Qk\n∂\n∂φd(zφ\ni , ck) −\nX\nj\nlim\nα→∞\nexp(−αd(zφ\ni , cj)) ∂\n∂φd(zφ\ni , cj)\nP\nℓexp(−αd(zφ\ni , cℓ))\n=\nX\nxi∈Qk\n∂\n∂φd(zφ\ni , ck) −\nX\nj\nlim\nα→∞\n∂\n∂φd(zφ\ni , cj)\n1 + P\nℓ̸=j exp(−α[d(zφ\ni , cℓ) −d(zφ\ni , cj)])\n.\nIt is obvious that whenever at least one of the exponential terms in the denominator in the expression\nabove has positive rate, corresponding to the case ∃ℓ̸= j : [d(zφ\ni , cℓ) −d(zφ\ni , cj)] < 0, the ratio\nconverges to zero as α →∞under assumption A2. The only case when the limit is non-zero is\nwhen cj is the prototype closest to the query point xi. If we deﬁne the index of this prototype as\nj∗\ni = arg minj d(zφ\ni , cj), then the following holds: ∀ℓ̸= j∗\ni : [d(zφ\ni , cℓ) −d(zφ\ni , cj∗\ni )] > 0, leading\n(under additional assumption A1) to:\nlim\nα→∞\n1\n1 + P\nℓ̸=j exp(−α[d(zφ\ni , cℓ) −d(zφ\ni , cj∗\ni )])\n= 1.\nTherefore, (4) follows.\nAcknowledgements\nAuthors acknowledge the support of the Spanish project TIN2015-65464-R (MINECO/FEDER), the\n2016FI B 01163 grant of Generalitat de Catalunya. Authors would like to thank Nicolas Chapados,\nAdam Salvail and Rachel Samson as well as anonymous reviewers for their careful reading of the\nmanuscript and for providing constructive feedback and valuable suggestions.\n9\n",
    "References\n[1] M. Bauer, M. Rojas-Carulla, J. B. ´Swi ˛atkowski, B. Schölkopf, and R. E. Turner. Discriminative\nk-shot learning using probabilistic models. arXiv preprint arXiv:1706.00326, 2017.\n[2] S. Carey and E. Bartlett. Acquiring a single new word. 1978.\n[3] V. Dumoulin, J. Shlens, and M. Kudlur. A learned representation for artistic style. ICLR, 2017.\n[4] H. Edwards and A. Storkey. Towards a neural statistician. arXiv preprint arXiv:1606.02185,\n2016.\n[5] M. Fink. Object classiﬁcation from a single example utilizing class relevance metrics. In NIPS,\npages 449–456, 2005.\n[6] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep\nnetworks. In ICML, pages 1126–1135, 2017.\n[7] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduction by learning an invariant\nmapping. In CVPR, volume 2, pages 1735–1742. IEEE, 2006.\n[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. CVPR,\npages 770–778, 2016.\n[9] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. In NIPS\nDeep Learning and Representation Learning Workshop, 2015. URL http://arxiv.org/\nabs/1503.02531.\n[10] G. Koch, R. Zemel, and R. Salakhutdinov. Siamese neural networks for one-shot image\nrecognition. In ICML Deep Learning Workshop, volume 2, 2015.\n[11] A. Krizhevsky. Learning multiple layers of features from tiny images. , University of Toronto,\n2009.\n[12] A. Lacoste, T. Boquet, N. Rostamzadeh, B. Oreshkin, W. Chung, and D. Krueger. Deep prior.\narXiv preprint arXiv:1712.05016, 2017.\n[13] B. M. Lake, R. R. Salakhutdinov, and J. Tenenbaum. One-shot learning by inverting a composi-\ntional causal process. In NIPS, pages 2526–2534, 2013.\n[14] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level concept learning through\nprobabilistic program induction. Science, 350(6266):1332–1338, 2015.\n[15] F.-F. Li, R. Fergus, and P. Perona. One-shot learning of object categories. PAMI, 28(4):594–611,\n2006.\n[16] N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel. A simple neural attentive meta-learner. In\nICLR, 2018.\n[17] T. Munkhdalai, X. Yuan, S. Mehri, and A. Trischler. Rapid adaptation with conditionally shifted\nneurons. In ICML, 2018.\n[18] E. Perez, H. de Vries, F. Strub, V. Dumoulin, and A. C. Courville. Learning visual reasoning\nwithout strong priors. CoRR, abs/1707.03017, 2017.\n[19] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville. Film: Visual reasoning with a\ngeneral conditioning layer. In AAAI, 2018.\n[20] B. Plank and H. M. Alonso. When is multitask learning effective? Semantic sequence prediction\nunder varying data conditions. In Proceedings of the 15th Conference of the European Chapter\nof the Association for Computational Linguistics, EACL 2017, Valencia, Spain, pages 44–53,\n2017.\n[21] P. Ramachandran, B. Zoph, and Q. V. Lea. Searching for activation functions. In ICLR, 2018.\n[22] S. Ravi and H. Larochelle. Optimization as a model for few-shot learning. In ICLR, 2016.\n[23] M. Ren, E. Triantaﬁllou, S. Ravi, J. Snell, K. Swersky, J. B. Tenenbaum, H. Larochelle,\nand R. S. Zemel. Meta-learning for semi-supervised few-shot classiﬁcation. arXiv preprint\narXiv:1803.00676, 2018.\n[24] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and T. Lillicrap. Meta-learning with\nmemory-augmented neural networks. In M. F. Balcan and K. Q. Weinberger, editors, ICML,\nvolume 48 of Proceedings of Machine Learning Research, pages 1842–1850, New York, New\nYork, USA, 20–22 Jun 2016. PMLR.\n10\n",
    "[25] J. Schmidhuber, J. Zhao, and M. Wiering. Shifting inductive bias with success-story algorithm,\nadaptive levin search, and incremental self-improvement. Machine Learning, 28(1):105–130,\n1997.\n[26] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uniﬁed embedding for face recognition\nand clustering. In CVPR, pages 815–823, 2015.\n[27] P. Shyam, S. Gupta, and A. Dukkipati. Attentive recurrent comparators. In ICML, pages\n3173–3181, 2017.\n[28] J. Snell, K. Swersky, and R. S. Zemel. Prototypical networks for few-shot learning. In NIPS,\npages 4080–4090, 2017.\n[29] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and T. M. Hospedales. Learning to compare:\nRelation network for few-shot learning. In CVPR, 2018.\n[30] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Web-scale training for face identiﬁcation. In\nCVPR, pages 2746–2754, 2015.\n[31] S. Thrun. Lifelong learning algorithms. In Learning to learn, pages 181–209. Springer, 1998.\n[32] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and\nI. Polosukhin. Attention is all you need. In NIPS, pages 6000–6010, 2017.\n[33] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and D. Wierstra. Matching networks for\none shot learning. In NIPS, pages 3630–3638. 2016.\n[34] Y.-X. Wang, R. Girshick, M. Hebert, and B. Hariharan. Low-Shot Learning from Imaginary\nData. In CVPR, 2018.\n11\n",
    "Supplementary Materials. TADAM: Task dependent adaptive metric for\nimproved few-shot learning\nS1\nArchitecture details\nConv\nBN\nTEN\nActivation\nTask  \nembedding   \nInput \nOutput \nConv block \nwith TEN \n(a) Convolutional block with TEN.\nConv block 1\nActivation 1\nConv block 3\nActivation 3\nInput \n...\nConv\nBN\n+ \nMax pool 2x2\nOutput \nTask\nembedding \nResnet block\n(b) Resnet block with TEN.\nFigure 1: Components of the ResNet-12 feature extractor.\nResNet-12 architecture details. The resnet blocks used in the ResNet-12 feature extractor are\nshown in Fig. 1. The feature extractor consists of 4 resnet blocks shown in Fig. 1b followed by a\nglobal average-pool. Each resnet block consists of 3 convolutional blocks shown in Fig. 1a followed\nby 2x2 max-pool. Each convolutional layer is followed by a batch norm layer and the swish-1\nactivation function proposed by Ramachandran et al. [21]. We found that the fully convolutional\narchitecture performs best as a few-shot feature extractor, both on mini-Imagenet and on FC100. We\nfound that inserting additional projection layers after the ResNet stack was always detrimental to the\nfew-shot performance. We cross-validated this result with multiple hyper-parameter settings for the\nprojection layers (number of layers, layer widths, and dropout). In addition to that, we observed that\nadding extra convolutional layers and max-pool layers before the ResNet stack was detrimental to the\nfew-shot performance. Therefore, we used fully convolutional, fully residual architecture in all our\nexperiments.\nThe hyperparameters for the convolutional layers are as follows. The number of ﬁlters for the ﬁrst\nResNet block was set to 64 and it was doubled after each max-pool block. The L2 regularizer weight\nwas cross-validated at 0.0005 for each layer.\nTEN architecture details. The detailed architecture of the TEN block is depicted in Fig. 2. Our\nimplementation of the TEN uses two separate fully connected residual networks to generate vectors\nγ, β. We cross-validated the number of layers to be 3. The ﬁrst layer projects the task representation\ninto the target width. The target width is equal to the number of ﬁlters of the convolutional layer that\nthe TEN block is conditioning (see Fig. 1a). The remaining layers operate at the target width and\neach of them has a skip connection. The L2 regularizer weight for γ0 and β0 was cross-validated\nat 0.01 for each layer. We found that smaller values led to considerable overﬁt. In addition to that,\nwe were not able to successfully train TEN without γ0 and β0, because the training tended to be\nstuck in local minima where the overall effect of introducing TEN was detrimental to the few-shot\nperformance of the architecture.\n1\n",
    "Task\nencoding \nResidual\nFC net\nβ0\n× \n× \nγ0\n1 \n+ \nInput \n× \n+ \nOutput \nL2 penalty\nL2 penalty\nResidual\nFC net\nβ\nγ \nFigure 2: Architecture of the TEN block.\nS2\nFew-shot CIFAR100 details\nTrain split. Super-class labels: {1, 2, 3, 4, 5, 6, 9, 10, 15, 17, 18, 19}; super-class names: {ﬁsh,\nﬂowers, food_containers, fruit_and_vegetables, household_electrical_devices, household_furniture,\nlarge_man-made_outdoor_things, large_natural_outdoor_scenes, reptiles, trees, vehicles_1, vehi-\ncles_2}.\nValidation split. Super-class labels: {8, 11, 13, 16}; super-class names: {large_carnivores,\nlarge_omnivores_and_herbivores, non-insect_invertebrates, small_mammals}.\nTest split. Super-class labels: {0, 7, 12, 14}; super-class names: {aquatic_mammals, insects,\nmedium_mammals, people}.\nWe would like to stress that we still sample all the tasks uniformly at random within train, validation\nand test subsets. Therefore, each task with very high probability contains samples belonging to\nclasses from several superclasses.\nS3\nTraining procedure details\nEpisode composition. The training procedure composes a few-shot training batch from several tasks,\nwhere a task is understood to be a ﬁxed selection of 5 classes. We found empirically that for the\n5-shot scenario the best number of tasks per batch was 2, for 10-shot it was 1 and for 1-shot it was 5.\nThe sample set in each training batch was created using the same number of shots as in the target\ndeployment (test) scenario. The images in the training query set were sampled uniformly at random.\nWe observed that the best results were obtained when the number of query images was approximately\nequal to the total number of sample images in the batch. Thus we used 32 query images per task for\n5-shot, 64 for 10-shot and 12 for 1-shot.\nThe auxiliary classiﬁcation task is based on the usual 64-way training (for mini-Imagenet). Co-\ntraining uses a ﬁxed batch of 64 image samples sampled uniformly at random from the training set.\nThe learning rate annealing schedule for the auxiliary task is synchronized with that of the main\nfew-shot task.\nOptimization, scheduling and learning rate. When training with auxiliary classiﬁcation task we\nused total 30000 episodes for training on mini-Imagenet and 10000 episodes for training on FC100.\nThe results obtained with no auxiliary classiﬁcation co-training used twice as many episodes. To\nobtain all our results we used SGD with momentum 0.9 and initial learning rate set at 0.1. The\nlearning rate was annealed by a factor of 10 halfway through the training and two more times every\n2500 episodes. The reported numbers are calculated using early-stopping based on validation set\nclassiﬁcation error tracking.\nClassiﬁcation accuracy evaluation. The accuracy is evaluated using 10 random restarts of the\noptimization procedure and based on 500 randomly generated tasks each having 100 random query\nsamples.\nReproducing results in [28]. To reproduce the results reported in [28] we used exactly the same\nsetup and network architecture reported in the original paper.\n2\n"
  ],
  "full_text": "TADAM: Task dependent adaptive metric for\nimproved few-shot learning\nBoris N. Oreshkin\nElement AI\nboris@elementai.com\nPau Rodriguez\nElement AI, CVC-UAB\npau.rodriguez@elementai.com\nAlexandre Lacoste\nElement AI\nallac@elementai.com\nAbstract\nFew-shot learning has become essential for producing models that generalize\nfrom few examples. In this work, we identify that metric scaling and metric task\nconditioning are important to improve the performance of few-shot algorithms.\nOur analysis reveals that simple metric scaling completely changes the nature of\nfew-shot algorithm parameter updates. Metric scaling provides improvements\nup to 14% in accuracy for certain metrics on the mini-Imagenet 5-way 5-shot\nclassiﬁcation task. We further propose a simple and effective way of conditioning a\nlearner on the task sample set, resulting in learning a task-dependent metric space.\nMoreover, we propose and empirically test a practical end-to-end optimization\nprocedure based on auxiliary task co-training to learn a task-dependent metric\nspace. The resulting few-shot learning model based on the task-dependent scaled\nmetric achieves state of the art on mini-Imagenet. We conﬁrm these results on\nanother few-shot dataset that we introduce in this paper based on CIFAR100. Our\ncode is publicly available at https://github.com/ElementAI/TADAM.\n1\nIntroduction\nHumans can learn to identify new categories from few examples, even from a single one [2]. Few-shot\nlearning has recently attracted signiﬁcant attention [33, 28, 29, 24, 17, 16], as it aims to produce\nmodels that can generalize from small amounts of labeled data. In the few-shot setting, one aims to\nlearn a model that extracts information from a set of support examples (sample set) to predict the\nlabels of instances from a query set. Recently, this problem has been reframed into the meta-learning\nframework [22], i.e. the model is trained so that given a sample set or task, produces a classiﬁer for\nthat speciﬁc task. Thus, the model is exposed to different tasks (or episodes) during the training\nphase, and it is evaluated on a non-overlapping set of new tasks [33].\nTwo recent approaches have attracted signiﬁcant attention in the few-shot learning domain: Matching\nNetworks [33], and Prototypical Networks [28]. In both approaches, the sample set and the query set\nare embedded with a neural network, and nearest neighbor classiﬁcation is used given a metric in the\nembedded space. Since then, the problem of learning the most suitable metric for few-shot learning\nhas been of interest to the ﬁeld [33, 28, 29, 17, 16]. Learning a metric space in the context of few-shot\nlearning generally implies identifying a suitable similarity measure (e.g. cosine or Euclidean), a\nfeature extractor mapping raw inputs onto similarity space (e.g. convolutional stack for images or\nLSTM stack for text), a cost function to drive the parameter updates, and a training scheme (often\nepisodic). Although the individual components in this list have been explored, the relationships\nbetween them have not received considerable attention.\nIn the current work we aim to close this gap. We show that taking into account the interaction\nbetween the identiﬁed components leads to signiﬁcant improvements in the few-shot generalization.\nIn particular, we show that a non-trivial interaction between the similarity metric and the cost function\ncan be exploited to improve the performance of a given similarity metric via scaling. Using this\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.\narXiv:1805.10123v4  [cs.LG]  25 Jan 2019\n\n\nmechanism we close more than the 10% gap in performance between the cosine similarity and\nthe Euclidean distance reported in [28]. Even more importantly, we extend the very notion of the\nmetric space by making it task dependent via conditioning the feature extractor on the speciﬁc task.\nHowever, learning such a space is in general more challenging than learning a static one. Hence,\nwe ﬁnd a solution in exploiting the interaction between the conditioned feature extractor and the\ntraining procedure based on auxiliary co-training on a simpler task. Our proposed few-shot learning\narchitecture based on task-dependent scaled metric achieves superior performance on two challenging\nfew-shot image classiﬁcation datasets. It shows up to 8.5% absolute accuracy improvement over the\nbaseline (Snell et al. [28]), and 4.8% over the state-of-the-art [17] on the 5-shot, 5-way mini-Imagenet\nclassiﬁcation task, reaching 76.7% of accuracy, which is the best-reported accuracy on this dataset.\n1.1\nBackground\nWe consider the episodic M-shot, K-way classiﬁcation scenario. In this scenario, a learning algorithm\nis provided with a sample set S = {(xi, yi)}MK\ni=1 consisting of M examples for each of K classes and\na query set Q = {(xi, yi)}q\ni=1 for a task to be solved within a given episode. The sample set provides\nthe task information via observations xi ∈RDx and their respective class labels yi ∈{1, . . . , K}.\nGiven the information in the sample set S, the learning algorithm is able to classify individual\nsamples from the query set Q. Next, we deﬁne a similarity measure d : RDz×Dz →R. Note that\nd does not have to satisfy the classical metric properties (non-negativity, symmetry, subadditivity)\nto be useful in the context of few-shot learning. The dimensionality of metric input, Dz, will most\nnaturally be related to the size of embedding created by a (deep) feature extractor fφ : RDx →RDz,\nparameterized by φ, mapping x to z. Here φ ∈RDφ is a list of parameters deﬁning fφ, e.g. a list of\nweights in a neural network. The set of representations (fφ(xi), yi), ∀(xi, yi) ∈S can directly be\nused to solve the few-shot learning classiﬁcation problem by association. For example, Matching\nnetworks [33] use sample-wise attention mechanism to perform kernel label regression. Instead,\nSnell et al. [28] deﬁned a feature representation ck for each class k as the mean over embeddings\nbelonging to Sk: ck =\n1\nK\nP\nxi∈Sk fφ(xi). To learn φ, they minimize −log pφ(y = k|x) using the\nsoftmax over prototypes ck to deﬁne the likelihood: pφ(y = k|x) = softmax(−d(fφ(x), ck)).\n1.2\nSummary of contributions\nMetric Scaling: To our knowledge, this is the ﬁrst study to (i) propose metric scaling to improve\nperformance of few-shot algorithms, (ii) mathematically analyze its effects on objective function\nupdates and (iii) empirically demonstrate its positive effects on few-shot performance.\nTask Conditioning: We use a task encoding network to extract a task representation based on the\ntask’s sample set. This is used to inﬂuence the behavior of the feature extractor through FILM [19].\nAuxiliary task co-training: We show that co-training the feature extraction on a conventional\nsupervised classiﬁcation task reduces training complexity and provides better generalization.\n1.3\nRelated work\nThree main approaches for solving the few-shot classiﬁcation problem can be identiﬁed in the\nliterature. The ﬁrst one, which is used in this work, is the meta-learning approach, i.e. learning a\nmodel that, given a task (set of labeled data), produces a classiﬁer that generalizes across all tasks\n[31, 25]. This is the case of Matching Networks [33], which optionally use a Recurrent Neural\nNetwork (RNN) to accumulate information about a given task. In MAML [6], the parameters of an\narbitrary learner model are optimized so that they can be quickly adapted to a particular task. In\n“Optimization as a model” [22], a learner model is adapted to a new episodic task by a recurrent meta-\nlearner producing efﬁcient parameter updates. A more general approach was proposed by Santoro\net al. [24], where the meta-learner is trained to represent entries from a sample set in an external\nmemory. Similarly, adaResNet [17] uses memory and the sample set to produce shift coefﬁcients on\nthe neuron activations of the query set classiﬁer. Many recent approaches focus on learning a metric\non the episodic feature space. Prototypical networks [28] use a feed-forward neural network to embed\nthe task examples and perform nearest neighbor classiﬁcation with the class centroids. The relation\nnetwork approach by Sung et al. [29] introduces a separate learnable similarity metric. SNAIL\n[16] uses an explicit attention mechanism applicable both to supervised and to the sequence based\n2\n\n\nreinforcement learning tasks. It has also been shown that these approaches beneﬁt from leveraging\nunlabeled and simulated data [23, 34].\nA second approach aims to maximize the distance between examples from different classes [10].\nSimilarly, in [7], a contrastive loss function is used to learn to project data onto a manifold that is\ninvariant to deformations in the input space. In the same vein, in [5, 26, 30], triplet loss is used\nfor learning a representation for few-shot learning. The attentive recurrent comparators [27] go\nbeyond classical siamese approaches and use a recurrent architecture to learn to perform pairwise\ncomparisons and predict if the compared examples belong to the same class.\nThe third approach relies on Bayesian modeling of the prior distribution of the different categories\nlike in Li et al. [15], Bauer et al. [1], or Lake et al. [13], Edwards and Storkey [4], Lacoste et al. [12]\nwho rely on hierarchical Bayesian modeling.\nAs for task conditioning, [3, 18, 19] proposed conditional batch normalization for style transfer and\nvisual reasoning. Differently, we modify the conditioning scheme to adapt it to few-shot learning,\nintroducing γ0, β0 priors, and auxiliary co-training. In the few-shot learning context, task conditioning\nideas can be traced back to [33], although in an implicit form as there is no notion of task embedding.\nIn our work, we explicitly introduce a task representation (see Fig. 1) computed as the mean of the task\nclass centroids (task prototypes). This is much simpler than individual sample level LSTM/attention\nmodels in [33]. Conditioning in [33] is applied as a postprocessing of the output of a ﬁxed feature\nextractor. We propose to condition the feature extractor by predicting its own batch normalization\nparameters thus making feature extractor behaviour task-dynamic without cumbersome ﬁne-tuning\non support set. In order to train the task conditioned architecture we use multitask training with\na usual 64-way classiﬁcation task. Even though auxiliary co-training is beneﬁcial for learning in\ngeneral, “little is known on when multitask learning works and whether there are data characteristics\nthat help to determine its success” [20]. We show that combining task conditioning and auxiliary\nco-training is beneﬁcial in the context of few-shot learning.\nThe scaling and temperature adjustment in the softmax was discussed by Hinton et al. [9] in the\ncontext of model distillation. We propose to use it in the context of the few-shot learning scenario\nand provide novel theoretical and empirical results quantifying the effects of scaling parameter.\nThe rest of the paper is organized as follows. Section 2 describes our contributions in detail. Section 3\nhighlights the importance of each contribution via an ablation study. The study is performed over two\ndifferent benchmarks in the regime of 1-shot, 5-shot and 10-shot learning to verify if conclusions hold\nacross different setups. Finally, Section 4 concludes the paper and outlines future research directions.\n2\nModel Description\n2.1\nMetric Scaling\nSnell et al. [28] using approach described in detail in Section 1.1 found that the Euclidean distance\noutperformed the cosine distance used in Vinyals et al. [33]. We hypothesize that the improvement\ncould be directly attributed to the interaction of the different scaling of the metrics with the softmax.\nMoreover, the dimensionality of the output is known to have a direct impact on the output scale\neven for the Euclidean distance [32]. Hence, we propose to scale the distance metric by a learnable\ntemperature, α, pφ,α(y = k|x) = softmax(−αd(z, ck)), to enable the model to learn the best regime\nfor each similarity metric, thus improving the performances of all metrics. To further understand the\nrole of α, we analyze the class-wise cross-entropy loss function, Jk(φ, α),1\nJk(φ, α) =\nX\nxi∈Qk\nh\nαd(fφ(xi), ck) + log\nX\nj\nexp(−αd(fφ(xi), cj))\ni\n,\n(1)\nwhere Qk = {(xi, yi) ∈Q : yi = k} is the query set corresponding to the class k. Its gradient,\nwhich is used to update parameters φ is given by the following expression:\n∂\n∂φJk(φ, α) = α\nX\nxi∈Qk\n\"\n∂\n∂φd(fφ(xi), ck) −\nP\nj exp(−αd(fφ(xi), cj)) ∂\n∂φd(fφ(xi), cj)\nP\nj exp(−αd(fφ(xi), cj))\n#\n. (2)\n1Note that the total loss is simply J(φ, α) = P\nk Jk(φ, α)\n3\n\n\nyi\nSimilarity\nmetric\nfɸ(x, )\nx* \nsoftmax\nTEN network \nfɸ(x, )\nð \nTask\nrepresentation\nfɸ(x, 0)\nClass\nrepresentation\n\nyi\nyi\nxi\nxi\nFigure 1: Proposed few-shot architecture. Blocks with shared parameters have dashed border.\nAt ﬁrst glance, the effect of α on the expression of the derivative is twofold: (i) an overall scaling,\nand (ii) regulating the sharpness of weighting in the second term inside the brackets on the RHS.\nBelow we explore the behavior of the α-normalized2 gradient in the limits α →0 and α →∞.\nLemma 1 (Metric scaling). If the following assumptions hold:\nA1 : d(fφ(x), ck) ̸= d(fφ(x′), ck), ∀k, x ̸= x′ ∈Qk;\nA2 :\n\f\f\f ∂\n∂φd(fφ(x), c)\n\f\f\f < ∞, ∀x, c, φ,\nthen it is true that:\nlim\nα→0\n1\nα\n∂\n∂φJk(φ, α) =\nX\nxi∈Qk\nhK −1\nK\n∂\n∂φd(fφ(xi), ck) −1\nK\nX\nj̸=k\n∂\n∂φd(fφ(xi), cj)\ni\n,\n(3)\nlim\nα→∞\n1\nα\n∂\n∂φJk(φ, α) =\nX\nxi∈Qk\nh ∂\n∂φd(fφ(xi), ck) −∂\n∂φd(fφ(xi), cj∗\ni )\ni\n;\n(4)\nwhere j∗\ni = arg minj d(fφ(xi), cj).\nProof. Please refer to Appendix A.\nFrom Eq. (3), it is clear that for small α values, the ﬁrst term minimizes the embedding distance\nbetween query samples and their corresponding prototypes. The second term maximizes the embed-\nding distance between the samples and the prototypes of the non-belonging categories. For large α\nvalues (Eq. (4)), the ﬁrst term is the same as in Eq. (3); while the second term maximizes the distance\nof the sample with the closest wrongly assigned prototype cj∗\ni (if any). If j∗\ni = k (no error), the\nderivative contribution of the point xi is zero. This is equivalent to learning only from the hardest\nexamples resulting in association errors. Thus, the two different regimes of α favor either minimizing\nthe overlap of the sample distributions or correcting cluster assignments sample-wise.\nThe large α regime is more directly related to resolving the few-shot classiﬁcation errors. At the\nsame time, the update strategy generated in this regime has a drawback. As the optimization proceeds\nand the classiﬁcation accuracy increases, the number of incorrectly classiﬁed samples reduces on\naverage, and this leads to the reduction in the average effective batch size (more samples generate\nzero derivatives). Therefore, our hypothesis is that there is an optimal value of scaling parameter α\nfor a given combination of dataset, metric and task. Section 3.4 empirically demonstrates that the\noptimal value of α indeed exists and it can be e.g. cross-validated on a validation set.\n2.2\nTask conditioning\nUp until now we assumed the feature extractor fφ(·) to be task-independent. A dynamic task-\nconditioned feature extractor should be better suited for ﬁnding correct associations between given\nsample set class representations and query samples, this is implicitly done by Vinyals et al. [33]\nwith a bidirectional LSTM as a postprocessing of a ﬁxed feature extractor. Differently, we explicitly\ndeﬁne a dynamic feature extractor fφ(x, Γ), where Γ is the set of parameters predicted from a task\nrepresentation such that the performance of fφ(x, Γ) is optimized given the task sample set S. This\n2The effect of α-related gradient scaling is trivial.\n4\n\n\nTable 1: mini-Imagenet (Vinyals et al. [33]), 5-way classiﬁcation results. †Our re-implementation.\n1-shot\n5-shot\n10-shot\nMeta Nets [22]\n43.4\n60.6\n-\nMatching Networks [33]\n46.6\n60.0\n-\nMAML [6]\n48.7\n63.1\n-\nProto Nets [28]\n49.4\n68.2\n74.3†\nRelation Net [29]\n50.4\n65.3\n-\nSNAIL [16]\n55.7\n68.9\n-\nDiscriminative k-shot [1]\n56.3\n73.9\n78.5\nadaResNet [17]\n56.9\n71.9\n-\nOurs\n58.5\n76.7\n80.8\nis related to the FILM conditioning layer [19] and conditional batch normalization [3, 18] of the form\nhℓ+1 = γ ⊙hℓ+ β, where γ and β are scaling and shift vectors applied to the layer hℓ. Concretely,\nwe propose to use the mean of the class prototypes as the task representation, c = 1\nK\nP\nk ck, encode\nit with a task embedding network (TEN), and predict layer-level element-wise scale and shift vectors\nγ, β for each convolutional layer in the feature extractor (see Figures 1 and 2 in the Supplementary\nMaterials, Section S1). The task representation deﬁned as the mean of task class centroids (i) reduces\nthe dimensionality of the TEN input and (ii) replaces expensive RNN/CNN/attention modeling. On\nthe other hand, it is an effective way to cluster tasks. Tasks having larger number of similar classes in\ncommon will tend to cluster closer in the task representation space.\nOur implementation of the TEN (see Supplementary Materials, Section S1 for more details) uses\ntwo separate fully connected residual networks to generate vectors γ, β. Following the terminology\nin [18], the γ parameter is learned in the delta regime, i.e. predicting deviation from unity. The\nmost critical component in being able to successfully train the TEN was the addition of the scalar L2\npenalized post-multipliers γ0 and β0. They limit the effect of γ (and β) by encoding a prior belief\nthat all components of γ (and β) should be simultaneously close to zero for a given layer unless\ntask conditioning provides a signiﬁcant information gain for this layer. Mathematically, this can be\nexpressed as β = β0gθ(c) and γ = γ0hϕ(c) + 1, where gθ and hϕ are predictors of β and γ.\n2.3\nArchitecture\nThe overall proposed few-shot classiﬁcation architecture is depicted in Fig. 1 (see Supplementary\nMaterials, Section S1 for more details). We employ ResNet-12 [8] as the backbone feature extractor.\nIt has 4 blocks of depth 3 with 3x3 kernels and shortcut connections. 2x2 max-pool is applied at\nthe end of each block. Convolutional layer depth starts with 64 ﬁlters and is doubled after every\nmax-pool. Note that this architecture is similar in spirit to architectures used in [1] and [17], but we\ndo not use any projection layers before or after the main backbone ResNet. On the ﬁrst pass over\nsample set, the TEN predicts the values of γ and β parameters for each convolutional layer in the\nfeature extractor from the task representation. Next, the sample set and the query set are processed by\nthe feature extractor conditioned with the values of γ and β just generated. Both outputs are fed into\na similarity metric to ﬁnd an association between class prototypes and query instances. The output of\nsimilarity metric is scaled by scalar α and is fed into a softmax layer.\n2.4\nAuxiliary task co-training\nThe TEN (Section 2.2) introduces additional complexity into the architecture via task conditioning\nlayers inserted after the convolutional and batch norm blocks. We empirically observed that simulta-\nneously optimizing convolutional ﬁlters and the TEN is overly challenging. We solved the problem by\nauxiliary co-training with an additional logit head (the normal 64-way classiﬁcation in mini-Imagenet\ncase). The auxiliary task is sampled with a probability that is annealed over episodes. We annealed it\nusing an exponential decay schedule of the form 0.9⌊20t/T ⌋, where T is the total number of training\nepisodes, t is episode index. The initial auxiliary task selection probability was cross-validated to\nbe 0.9 and the number of decay steps was chosen to be 20. We observed signiﬁcant positive effects\nfrom the auxiliary task co-training (please refer to Section 3.4). The same positive effects were not\n5\n\n\nobserved with simple pre-training of the feature extractor. We attribute this to the regularization\neffects achieved via back-propagating auxiliary task gradients together with those of the main task.\nIt is of interest to note that the few-shot co-training with an auxiliary classiﬁcation task is related to\ncurriculum learning [24]. The auxiliary classiﬁcation problem could be considered a part of a simpler\ncurriculum that helps the learner acquire minimal skill level necessary before tackling on harder\nfew-shot classiﬁcation tasks. Being effective at feature extraction (i.e. at task representation) forms a\n“prerequisite” at being effective at re-conditioning features based on the representation of a given task.\n3\nExperimental Results\nTable 1 presents our key result in the context of existing state-of-the art. The ﬁve ﬁrst rows show\napproaches that use the same feature extractor as [33], i.e. four stacked convolutions layers of 64\nﬁlters (32 in [22, 6] to avoid overﬁtting). In the following rows we include models like the one we\npropose, which is based on resnet [8]. Concretely, SNAIL [16], adaResNet [17], and our architecture\nuse four residual blocks of three stacked 3 × 3 convolutional layers, each block followed by max\npooling. Differently, the feature extractor proposed in [1] is based on a ResNet-34 architecture with a\nreduced number of features.\nAs it can be seen, the proposed algorithm signiﬁcantly improves over the existing state-of-the-art\nresults on the mini-Imagenet dataset. In the rest of the section we address the following research\nquestions: (i) can metric scaling improve few-shot classiﬁcation results? (Sections 3.2 and 3.4), (ii)\nwhat are the contributions of each components of our proposed architecture? (Section 3.4), (iii) can\ntask conditioning improve few-shot classiﬁcation results and how important it is at different feature\nextractor depths? (Sections 3.3 and 3.4), and (iv) can auxiliary classiﬁcation task co-training improve\naccuracy on the few-shot classiﬁcation task? (Section 3.4).\n3.1\nExperimental setup and datasets\nThe details of the experimental and training setup are provided in Supplementary Materials, Section S3.\nNote that we focused on mini-Imagenet [33] and Fewshot-CIFAR100 (introduced below) instead of\nOmniglot [14, 33, 28] as the former ones are more challenging, and the error rate is more sensitive to\nmodel improvements.\nmini-Imagenet. The mini-Imagenet dataset was proposed by Vinyals et al. [33]. It has 100 classes,\nwith 600 84 × 84 images per class. Each task is generated by sampling 5 classes uniformly and\n5 training samples per class, the remaining images from the 5 classes are used as query images to\ncompute accuracy. To perform meta-validation and meta-test on unseen tasks (and classes), we isolate\n16 and 20 classes from the original set of 100, leaving 64 classes for the training tasks. We use exactly\nthe same train/validation/test split as the one suggested by Ravi and Larochelle [22].\nFewshot-CIFAR100. We introduce a new image based dataset based on CIFAR100 [11] for few-shot\nlearning. We will refer to it as FC100. The main motivation for introducing this new dataset is\nto validate that the main results appearing in the experimental section generalize well beyond the\nmini-Imagenet. The secondary motivation is that the FC100 is suited for faster few-shot scenario\nprototyping than the mini-Imagenet and it presents a more challenging few-shot learning problem,\nbecause of reduced image size. On top of that, we propose a class split in FC100 to minimize the\ninformation overlap between splits to make it signiﬁcantly more challenging than e.g. Omniglot. The\noriginal CIFAR100 dataset consists of 32 × 32 color images belonging to 100 different classes, 600\nimages per class. The 100 classes are further grouped into 20 superclasses. We split the dataset by\nsuperclass, rather than by individual class to minimize the information overlap. Thus the train split\ncontains 60 classes belonging to 12 superclasses, the validation and test contain 20 classes belonging\nto 5 superclasses each. The exact class split is provided in Supplementary Materials, Section S2. The\ntasks are sampled uniformly at random within train, validation and test subsets. Therefore, each task\nwith high probability contains samples belonging to classes from several superclasses.\n3.2\nOn the similarity metric\nWe re-implemented prototypical networks [28], and use the Euclidean and the cosine similarity to\ntest the effects of scaling (see Section 2). We closely follow the experimental setup deﬁned by Snell\n6\n\n\nTable 2: Average classiﬁcation accuracy in percent with 95% conﬁdence interval. 5-shot, 5 way\nclassiﬁcation task. The three last rows correspond to our implementation, ﬁrst with euclidean distance,\nsecond with cosine distance, and third with the scaled cosine distance.\nmini-Imagenet\nFC100\n5-way train\n20-way train\n5-way train\n20-way train\nProto Nets [28]\n65.8 ± 0.7\n68.2 ± 0.7\nN/A\nN/A\nProto Nets\n67.7 ± 0.2\n68.9 ± 0.3\n51.1 ± 0.2\n50.3 ± 0.3\nPrototypical Cosine\n54.5 ± 1.1\n53.9 ± 0.6\n40.9 ± 0.6\n37.1 ± 1.9\nPrototypical Cosine Scaled\n68.2 ± 0.8\n68.1 ± 0.7\n51.0 ± 0.6\n49.6 ± 0.5\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nConv layer #\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n0\n0\n(a) Results on mini-Imagenet.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nConv layer #\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n0\n0\n(b) Results on FC100.\nFigure 2: Distribution of the absolute values of the TEN scaling and bias parameters γ0 and β0 across\nlayers of ResNet feature extractor. X-axes depict layer number in both subplots. Higher convolutional\nlayers are located closer to the ﬁnal softmax layer.\net al. [28] (same feature extractor and training procedure). The scaling parameter α used on the last\nrow was cross-validated on the validation set. Results are presented in Table 2.\nAs it can be seen in row two of Table 2, our re-implementation of Proto Nets [28] obtained slightly\nbetter performance (68.9% and 67.7%) in 20-way and 5-way training scenarios respectively by\nincreasing the number of training steps from 20K to 40K3.\nImportantly, we conﬁrm the hypothesis that the improvement attributed to the Euclidean distance\nin [28] was due to a scaling effect. Namely, we show that the scaled cosine similarity matches\nvery closely the performance of the Euclidean metric, with an improvement of 14 percentage points\non the mini-Imagenet (similar results on FC100) over the non-scaled version. In order to control\nfor the potential effect that the scaling parameter α may have on the learning rate as indicated by\nEquation (2) training was performed using multiple initial learning rates (covering the range between\n0.0005 and 0.01), obtaining similar accuracy each time. Hereinafter, we report the results with\nthe Euclidean metric for brevity, since the cosine produces similar results. Moreover, since the\nprototypical approach with Euclidean distance as well as with the scaled cosine are close and both\nare superior to [33], we base our results on [28].\n3.3\nTEN importance across layers\nWe hypothesized in Section 2.2 that the TEN conditioning should not be equally important at all\ndepths. Fig. 2 depicts the boxplot of the empirical observations of the learned TEN post-multipliers4\nγ0 and β0 at different depths of the feature extractor. We can see that for the multiplier γ, the absolute\nvalue of its scale γ0 tends to increase as we approach the softmax layer. Interestingly, peaks can be\nobserved every 3 layers (layers 3, 6, 9, 12). The peaks correspond to the location of the convolutional\nlayers preceding the max-pool layers. For the bias parameter β0, the only layer having a large absolute\nvalue of its scale is the last layer, before the softmax. We attribute the observed pattern to the fact\nthat the shallower layers in the feature extractor tend to be less task-speciﬁc than the deeper layers.\nFollowing this intuition, we performed experiments in which we (i) kept the TEN injection solely in\n3With 20K steps it was possible to recover the exact original performance reported in Snell et al. [28], which\nis not included in Table 2 for the sake of brevity.\n4Larger absolute values of γ0 and β0 imply a larger inﬂuence of their respective TEN layers\n7\n\n\nTable 3: Average classiﬁcation accuracy (%) with 95% conﬁdence interval on the 5 way classiﬁcation\ntask, and training with the Euclidean distance. The scale parameter is cross-validated on the validation\nset. AT: auxiliary co-training. TC: task conditioning with TEN.\nmini-Imagenet\nFC100\nα\nAT\nTC\n1-shot\n5-shot\n10-shot\n1-shot\n5-shot\n10-shot\n56.5 ± 0.4\n74.2 ± 0.2\n78.6 ± 0.4\n37.8 ± 0.4\n53.3 ± 0.5\n58.7 ± 0.4\n✓\n56.8 ± 0.3\n75.7 ± 0.2\n79.6 ± 0.4\n38.0 ± 0.3\n54.0 ± 0.5\n59.8 ± 0.3\n✓\n✓\n58.0 ± 0.3\n75.6 ± 0.4\n80.0 ± 0.3\n39.0 ± 0.4\n54.7 ± 0.5\n60.4 ± 0.4\n✓\n✓\n54.4 ± 0.3\n74.6 ± 0.3\n78.7 ± 0.4\n37.8 ± 0.2\n54.0 ± 0.7\n58.8 ± 0.3\n✓\n✓\n✓\n58.5 ± 0.3\n76.7 ± 0.3\n80.8 ± 0.3\n40.1 ± 0.4\n56.1 ± 0.4\n61.6 ± 0.5\n(a) Scaled Euclidean. mini-Imagenet.\n(b) Scaled Euclidean. FC100.\n(c) Scaled Euclidean with TEN. mini-Imagenet.\n(d) Scaled Euclidean with TEN. FC100.\nFigure 3: Metric scale parameter α cross-validation results.\nlayers preceding the max pool and (ii) kept the TEN injection only in the very last layer. Interestingly,\nwe saw that TEN layers with small weight still provide some positive contribution, although most of\nthe contribution is indeed provided by the layers preceding the max pool operation.\n3.4\nAblation study\nIn this section, we study the impact in generalization accuracy of the scaling, task conditioning,\nauxiliary co-training, and the feature extractor. Results are summarized in Table 3.\nFirst, we validated the hypothesis that there is an optimal value of the metric scaling parameter (α)\nfor a given combination of dataset and metric, which is reﬂected in the inverse U-shape of the curves\nin Fig. 3.\nSecond, we studied the effects of the task conditioning described in Section 2.2. No improvement\nwas observed for the task-conditioned ResNet-12 without auxiliary co-training (see Table 3). We\nobserved that learning useful features for the TEN and the main feature extractor at the same time\nis hard and gets stuck in local extrema. The problem is solved by co-training on the auxiliary task\nof predicting Imagenet labels using an additional fully-connected layer with softmax, see Section\n2.4. In effect, we observed that auxiliary co-training provides two beneﬁts: (i) making the initial\nconvergence easier, and (ii) providing regularization on the few-shot learning task by forcing the\nfeature extractor to perform well on two decoupled tasks. The latter beneﬁt can only be observed\nwhen the feature extraction unit is sufﬁciently decoupled on the main task and the auxiliary task via\nthe use of TEN (the feature extractor output is additionally adjusted on the target task using FILM).\nAs it can be seen in the last row of Tables 1 and 3, our model trained with TEN and auxiliary\nco-training outperforms all the baselines and achieves state-of-the-art results.\n4\nConclusions and Future Work\nWe proposed, analyzed, and empirically validated several improvements in the domain of few-shot\nlearning. We showed that the scaled cosine similarity performs at par with Euclidean distance,\n8\n\n\nunlike its unscaled counterpart. In fact, based on our results, we argue that the scaling factor is a\nnecessary standard component of any few-shot learning algorithm relying on a similarity metric\nand the cross-entropy loss function. This is especially important in the context of ﬁnding new more\neffective similarity measures for few-shot learning. Moreover, our theoretical analysis demonstrated\nthat simply scaling the similarity metric results in completely different regimes of parameter updates\nwhen using softmax and categorical cross-entropy. We also identiﬁed that the optimal performance\nis achieved in between two asymptotic regimes of the softmax. This poses the research question of\nexplicitly designing loss functions and the α schedules optimal for few-shot learning. We further\nproposed task representation conditioning as a way to improve the performance of a feature extractor\non the few-shot classiﬁcation task. In this context, designing more powerful task representations, for\nexample, based on higher order statistics of class embeddings, looks like a very promising venue for\nfuture work. The experimental results obtained on two independent challenging datasets demonstrated\nthat the proposed approach signiﬁcantly improves over existing results and achieves state-of-the-art\non few-shot image classiﬁcation task.\nAppendix\nA\nProof of Lemma 1\nFirst, consider the case α →0. Denoting zφ\ni = fφ(xi) we have:\nlim\nα→0\n1\nα\n∂\n∂φJk(φ, α) =\nX\nxi∈Qk\n∂\n∂φd(zφ\ni , ck) −lim\nα→0\nP\nj exp(−αd(zφ\ni , cj)) ∂\n∂φd(zφ\ni , cj)\nP\nj exp(−αd(zφ\ni , cj))\n=\nX\nxi∈Qk\n∂\n∂φd(zφ\ni , ck) −1\nK\nX\nj\n∂\n∂φd(zφ\ni , cj)\n=\nX\nxi∈Qk\nK −1\nK\n∂\n∂φd(zφ\ni , ck) −1\nK\nX\nj̸=k\n∂\n∂φd(zφ\ni , cj).\nSecond, consider the case α →∞:\nlim\nα→∞\n1\nα\n∂\n∂φJk(φ, α) =\nX\nxi∈Qk\n∂\n∂φd(zφ\ni , ck) −\nX\nj\nlim\nα→∞\nexp(−αd(zφ\ni , cj)) ∂\n∂φd(zφ\ni , cj)\nP\nℓexp(−αd(zφ\ni , cℓ))\n=\nX\nxi∈Qk\n∂\n∂φd(zφ\ni , ck) −\nX\nj\nlim\nα→∞\n∂\n∂φd(zφ\ni , cj)\n1 + P\nℓ̸=j exp(−α[d(zφ\ni , cℓ) −d(zφ\ni , cj)])\n.\nIt is obvious that whenever at least one of the exponential terms in the denominator in the expression\nabove has positive rate, corresponding to the case ∃ℓ̸= j : [d(zφ\ni , cℓ) −d(zφ\ni , cj)] < 0, the ratio\nconverges to zero as α →∞under assumption A2. The only case when the limit is non-zero is\nwhen cj is the prototype closest to the query point xi. If we deﬁne the index of this prototype as\nj∗\ni = arg minj d(zφ\ni , cj), then the following holds: ∀ℓ̸= j∗\ni : [d(zφ\ni , cℓ) −d(zφ\ni , cj∗\ni )] > 0, leading\n(under additional assumption A1) to:\nlim\nα→∞\n1\n1 + P\nℓ̸=j exp(−α[d(zφ\ni , cℓ) −d(zφ\ni , cj∗\ni )])\n= 1.\nTherefore, (4) follows.\nAcknowledgements\nAuthors acknowledge the support of the Spanish project TIN2015-65464-R (MINECO/FEDER), the\n2016FI B 01163 grant of Generalitat de Catalunya. Authors would like to thank Nicolas Chapados,\nAdam Salvail and Rachel Samson as well as anonymous reviewers for their careful reading of the\nmanuscript and for providing constructive feedback and valuable suggestions.\n9\n\n\nReferences\n[1] M. Bauer, M. Rojas-Carulla, J. B. ´Swi ˛atkowski, B. Schölkopf, and R. E. Turner. Discriminative\nk-shot learning using probabilistic models. arXiv preprint arXiv:1706.00326, 2017.\n[2] S. Carey and E. Bartlett. Acquiring a single new word. 1978.\n[3] V. Dumoulin, J. Shlens, and M. Kudlur. A learned representation for artistic style. ICLR, 2017.\n[4] H. Edwards and A. Storkey. Towards a neural statistician. arXiv preprint arXiv:1606.02185,\n2016.\n[5] M. Fink. Object classiﬁcation from a single example utilizing class relevance metrics. In NIPS,\npages 449–456, 2005.\n[6] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep\nnetworks. In ICML, pages 1126–1135, 2017.\n[7] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduction by learning an invariant\nmapping. In CVPR, volume 2, pages 1735–1742. IEEE, 2006.\n[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. CVPR,\npages 770–778, 2016.\n[9] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. In NIPS\nDeep Learning and Representation Learning Workshop, 2015. URL http://arxiv.org/\nabs/1503.02531.\n[10] G. Koch, R. Zemel, and R. Salakhutdinov. Siamese neural networks for one-shot image\nrecognition. In ICML Deep Learning Workshop, volume 2, 2015.\n[11] A. Krizhevsky. Learning multiple layers of features from tiny images. , University of Toronto,\n2009.\n[12] A. Lacoste, T. Boquet, N. Rostamzadeh, B. Oreshkin, W. Chung, and D. Krueger. Deep prior.\narXiv preprint arXiv:1712.05016, 2017.\n[13] B. M. Lake, R. R. Salakhutdinov, and J. Tenenbaum. One-shot learning by inverting a composi-\ntional causal process. In NIPS, pages 2526–2534, 2013.\n[14] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level concept learning through\nprobabilistic program induction. Science, 350(6266):1332–1338, 2015.\n[15] F.-F. Li, R. Fergus, and P. Perona. One-shot learning of object categories. PAMI, 28(4):594–611,\n2006.\n[16] N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel. A simple neural attentive meta-learner. In\nICLR, 2018.\n[17] T. Munkhdalai, X. Yuan, S. Mehri, and A. Trischler. Rapid adaptation with conditionally shifted\nneurons. In ICML, 2018.\n[18] E. Perez, H. de Vries, F. Strub, V. Dumoulin, and A. C. Courville. Learning visual reasoning\nwithout strong priors. CoRR, abs/1707.03017, 2017.\n[19] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville. Film: Visual reasoning with a\ngeneral conditioning layer. In AAAI, 2018.\n[20] B. Plank and H. M. Alonso. When is multitask learning effective? Semantic sequence prediction\nunder varying data conditions. In Proceedings of the 15th Conference of the European Chapter\nof the Association for Computational Linguistics, EACL 2017, Valencia, Spain, pages 44–53,\n2017.\n[21] P. Ramachandran, B. Zoph, and Q. V. Lea. Searching for activation functions. In ICLR, 2018.\n[22] S. Ravi and H. Larochelle. Optimization as a model for few-shot learning. In ICLR, 2016.\n[23] M. Ren, E. Triantaﬁllou, S. Ravi, J. Snell, K. Swersky, J. B. Tenenbaum, H. Larochelle,\nand R. S. Zemel. Meta-learning for semi-supervised few-shot classiﬁcation. arXiv preprint\narXiv:1803.00676, 2018.\n[24] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and T. Lillicrap. Meta-learning with\nmemory-augmented neural networks. In M. F. Balcan and K. Q. Weinberger, editors, ICML,\nvolume 48 of Proceedings of Machine Learning Research, pages 1842–1850, New York, New\nYork, USA, 20–22 Jun 2016. PMLR.\n10\n\n\n[25] J. Schmidhuber, J. Zhao, and M. Wiering. Shifting inductive bias with success-story algorithm,\nadaptive levin search, and incremental self-improvement. Machine Learning, 28(1):105–130,\n1997.\n[26] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uniﬁed embedding for face recognition\nand clustering. In CVPR, pages 815–823, 2015.\n[27] P. Shyam, S. Gupta, and A. Dukkipati. Attentive recurrent comparators. In ICML, pages\n3173–3181, 2017.\n[28] J. Snell, K. Swersky, and R. S. Zemel. Prototypical networks for few-shot learning. In NIPS,\npages 4080–4090, 2017.\n[29] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and T. M. Hospedales. Learning to compare:\nRelation network for few-shot learning. In CVPR, 2018.\n[30] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Web-scale training for face identiﬁcation. In\nCVPR, pages 2746–2754, 2015.\n[31] S. Thrun. Lifelong learning algorithms. In Learning to learn, pages 181–209. Springer, 1998.\n[32] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and\nI. Polosukhin. Attention is all you need. In NIPS, pages 6000–6010, 2017.\n[33] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and D. Wierstra. Matching networks for\none shot learning. In NIPS, pages 3630–3638. 2016.\n[34] Y.-X. Wang, R. Girshick, M. Hebert, and B. Hariharan. Low-Shot Learning from Imaginary\nData. In CVPR, 2018.\n11\n\n\nSupplementary Materials. TADAM: Task dependent adaptive metric for\nimproved few-shot learning\nS1\nArchitecture details\nConv\nBN\nTEN\nActivation\nTask  \nembedding   \nInput \nOutput \nConv block \nwith TEN \n(a) Convolutional block with TEN.\nConv block 1\nActivation 1\nConv block 3\nActivation 3\nInput \n...\nConv\nBN\n+ \nMax pool 2x2\nOutput \nTask\nembedding \nResnet block\n(b) Resnet block with TEN.\nFigure 1: Components of the ResNet-12 feature extractor.\nResNet-12 architecture details. The resnet blocks used in the ResNet-12 feature extractor are\nshown in Fig. 1. The feature extractor consists of 4 resnet blocks shown in Fig. 1b followed by a\nglobal average-pool. Each resnet block consists of 3 convolutional blocks shown in Fig. 1a followed\nby 2x2 max-pool. Each convolutional layer is followed by a batch norm layer and the swish-1\nactivation function proposed by Ramachandran et al. [21]. We found that the fully convolutional\narchitecture performs best as a few-shot feature extractor, both on mini-Imagenet and on FC100. We\nfound that inserting additional projection layers after the ResNet stack was always detrimental to the\nfew-shot performance. We cross-validated this result with multiple hyper-parameter settings for the\nprojection layers (number of layers, layer widths, and dropout). In addition to that, we observed that\nadding extra convolutional layers and max-pool layers before the ResNet stack was detrimental to the\nfew-shot performance. Therefore, we used fully convolutional, fully residual architecture in all our\nexperiments.\nThe hyperparameters for the convolutional layers are as follows. The number of ﬁlters for the ﬁrst\nResNet block was set to 64 and it was doubled after each max-pool block. The L2 regularizer weight\nwas cross-validated at 0.0005 for each layer.\nTEN architecture details. The detailed architecture of the TEN block is depicted in Fig. 2. Our\nimplementation of the TEN uses two separate fully connected residual networks to generate vectors\nγ, β. We cross-validated the number of layers to be 3. The ﬁrst layer projects the task representation\ninto the target width. The target width is equal to the number of ﬁlters of the convolutional layer that\nthe TEN block is conditioning (see Fig. 1a). The remaining layers operate at the target width and\neach of them has a skip connection. The L2 regularizer weight for γ0 and β0 was cross-validated\nat 0.01 for each layer. We found that smaller values led to considerable overﬁt. In addition to that,\nwe were not able to successfully train TEN without γ0 and β0, because the training tended to be\nstuck in local minima where the overall effect of introducing TEN was detrimental to the few-shot\nperformance of the architecture.\n1\n\n\nTask\nencoding \nResidual\nFC net\nβ0\n× \n× \nγ0\n1 \n+ \nInput \n× \n+ \nOutput \nL2 penalty\nL2 penalty\nResidual\nFC net\nβ\nγ \nFigure 2: Architecture of the TEN block.\nS2\nFew-shot CIFAR100 details\nTrain split. Super-class labels: {1, 2, 3, 4, 5, 6, 9, 10, 15, 17, 18, 19}; super-class names: {ﬁsh,\nﬂowers, food_containers, fruit_and_vegetables, household_electrical_devices, household_furniture,\nlarge_man-made_outdoor_things, large_natural_outdoor_scenes, reptiles, trees, vehicles_1, vehi-\ncles_2}.\nValidation split. Super-class labels: {8, 11, 13, 16}; super-class names: {large_carnivores,\nlarge_omnivores_and_herbivores, non-insect_invertebrates, small_mammals}.\nTest split. Super-class labels: {0, 7, 12, 14}; super-class names: {aquatic_mammals, insects,\nmedium_mammals, people}.\nWe would like to stress that we still sample all the tasks uniformly at random within train, validation\nand test subsets. Therefore, each task with very high probability contains samples belonging to\nclasses from several superclasses.\nS3\nTraining procedure details\nEpisode composition. The training procedure composes a few-shot training batch from several tasks,\nwhere a task is understood to be a ﬁxed selection of 5 classes. We found empirically that for the\n5-shot scenario the best number of tasks per batch was 2, for 10-shot it was 1 and for 1-shot it was 5.\nThe sample set in each training batch was created using the same number of shots as in the target\ndeployment (test) scenario. The images in the training query set were sampled uniformly at random.\nWe observed that the best results were obtained when the number of query images was approximately\nequal to the total number of sample images in the batch. Thus we used 32 query images per task for\n5-shot, 64 for 10-shot and 12 for 1-shot.\nThe auxiliary classiﬁcation task is based on the usual 64-way training (for mini-Imagenet). Co-\ntraining uses a ﬁxed batch of 64 image samples sampled uniformly at random from the training set.\nThe learning rate annealing schedule for the auxiliary task is synchronized with that of the main\nfew-shot task.\nOptimization, scheduling and learning rate. When training with auxiliary classiﬁcation task we\nused total 30000 episodes for training on mini-Imagenet and 10000 episodes for training on FC100.\nThe results obtained with no auxiliary classiﬁcation co-training used twice as many episodes. To\nobtain all our results we used SGD with momentum 0.9 and initial learning rate set at 0.1. The\nlearning rate was annealed by a factor of 10 halfway through the training and two more times every\n2500 episodes. The reported numbers are calculated using early-stopping based on validation set\nclassiﬁcation error tracking.\nClassiﬁcation accuracy evaluation. The accuracy is evaluated using 10 random restarts of the\noptimization procedure and based on 500 randomly generated tasks each having 100 random query\nsamples.\nReproducing results in [28]. To reproduce the results reported in [28] we used exactly the same\nsetup and network architecture reported in the original paper.\n2\n"
}