{
  "filename": "1703.03400v3.pdf",
  "num_pages": 13,
  "pages": [
    "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nChelsea Finn 1 Pieter Abbeel 1 2 Sergey Levine 1\nAbstract\nWe propose an algorithm for meta-learning that\nis model-agnostic, in the sense that it is com-\npatible with any model trained with gradient de-\nscent and applicable to a variety of different\nlearning problems, including classiﬁcation, re-\ngression, and reinforcement learning. The goal\nof meta-learning is to train a model on a vari-\nety of learning tasks, such that it can solve new\nlearning tasks using only a small number of train-\ning samples. In our approach, the parameters of\nthe model are explicitly trained such that a small\nnumber of gradient steps with a small amount\nof training data from a new task will produce\ngood generalization performance on that task. In\neffect, our method trains the model to be easy\nto ﬁne-tune. We demonstrate that this approach\nleads to state-of-the-art performance on two few-\nshot image classiﬁcation benchmarks, produces\ngood results on few-shot regression, and acceler-\nates ﬁne-tuning for policy gradient reinforcement\nlearning with neural network policies.\n1. Introduction\nLearning quickly is a hallmark of human intelligence,\nwhether it involves recognizing objects from a few exam-\nples or quickly learning new skills after just minutes of\nexperience. Our artiﬁcial agents should be able to do the\nsame, learning and adapting quickly from only a few exam-\nples, and continuing to adapt as more data becomes avail-\nable. This kind of fast and ﬂexible learning is challenging,\nsince the agent must integrate its prior experience with a\nsmall amount of new information, while avoiding overﬁt-\nting to the new data. Furthermore, the form of prior ex-\nperience and new data will depend on the task. As such,\nfor the greatest applicability, the mechanism for learning to\nlearn (or meta-learning) should be general to the task and\n1University of California, Berkeley 2OpenAI. Correspondence\nto: Chelsea Finn <cbﬁnn@eecs.berkeley.edu>.\nProceedings of the 34 th International Conference on Machine\nLearning, Sydney, Australia, PMLR 70, 2017. Copyright 2017\nby the author(s).\nthe form of computation required to complete the task.\nIn this work, we propose a meta-learning algorithm that\nis general and model-agnostic, in the sense that it can be\ndirectly applied to any learning problem and model that\nis trained with a gradient descent procedure. Our focus\nis on deep neural network models, but we illustrate how\nour approach can easily handle different architectures and\ndifferent problem settings, including classiﬁcation, regres-\nsion, and policy gradient reinforcement learning, with min-\nimal modiﬁcation. In meta-learning, the goal of the trained\nmodel is to quickly learn a new task from a small amount\nof new data, and the model is trained by the meta-learner\nto be able to learn on a large number of different tasks.\nThe key idea underlying our method is to train the model’s\ninitial parameters such that the model has maximal perfor-\nmance on a new task after the parameters have been up-\ndated through one or more gradient steps computed with\na small amount of data from that new task. Unlike prior\nmeta-learning methods that learn an update function or\nlearning rule (Schmidhuber, 1987; Bengio et al., 1992;\nAndrychowicz et al., 2016; Ravi & Larochelle, 2017), our\nalgorithm does not expand the number of learned param-\neters nor place constraints on the model architecture (e.g.\nby requiring a recurrent model (Santoro et al., 2016) or a\nSiamese network (Koch, 2015)), and it can be readily com-\nbined with fully connected, convolutional, or recurrent neu-\nral networks. It can also be used with a variety of loss func-\ntions, including differentiable supervised losses and non-\ndifferentiable reinforcement learning objectives.\nThe process of training a model’s parameters such that a\nfew gradient steps, or even a single gradient step, can pro-\nduce good results on a new task can be viewed from a fea-\nture learning standpoint as building an internal representa-\ntion that is broadly suitable for many tasks. If the internal\nrepresentation is suitable to many tasks, simply ﬁne-tuning\nthe parameters slightly (e.g. by primarily modifying the top\nlayer weights in a feedforward model) can produce good\nresults. In effect, our procedure optimizes for models that\nare easy and fast to ﬁne-tune, allowing the adaptation to\nhappen in the right space for fast learning. From a dynami-\ncal systems standpoint, our learning process can be viewed\nas maximizing the sensitivity of the loss functions of new\ntasks with respect to the parameters: when the sensitivity\nis high, small local changes to the parameters can lead to\narXiv:1703.03400v3  [cs.LG]  18 Jul 2017\n",
    "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nlarge improvements in the task loss.\nThe primary contribution of this work is a simple model-\nand task-agnostic algorithm for meta-learning that trains\na model’s parameters such that a small number of gradi-\nent updates will lead to fast learning on a new task. We\ndemonstrate the algorithm on different model types, includ-\ning fully connected and convolutional networks, and in sev-\neral distinct domains, including few-shot regression, image\nclassiﬁcation, and reinforcement learning. Our evaluation\nshows that our meta-learning algorithm compares favor-\nably to state-of-the-art one-shot learning methods designed\nspeciﬁcally for supervised classiﬁcation, while using fewer\nparameters, but that it can also be readily applied to regres-\nsion and can accelerate reinforcement learning in the pres-\nence of task variability, substantially outperforming direct\npretraining as initialization.\n2. Model-Agnostic Meta-Learning\nWe aim to train models that can achieve rapid adaptation, a\nproblem setting that is often formalized as few-shot learn-\ning. In this section, we will deﬁne the problem setup and\npresent the general form of our algorithm.\n2.1. Meta-Learning Problem Set-Up\nThe goal of few-shot meta-learning is to train a model that\ncan quickly adapt to a new task using only a few datapoints\nand training iterations. To accomplish this, the model or\nlearner is trained during a meta-learning phase on a set\nof tasks, such that the trained model can quickly adapt to\nnew tasks using only a small number of examples or trials.\nIn effect, the meta-learning problem treats entire tasks as\ntraining examples. In this section, we formalize this meta-\nlearning problem setting in a general manner, including\nbrief examples of different learning domains. We will dis-\ncuss two different learning domains in detail in Section 3.\nWe consider a model, denoted f, that maps observa-\ntions x to outputs a.\nDuring meta-learning, the model\nis trained to be able to adapt to a large or inﬁnite num-\nber of tasks.\nSince we would like to apply our frame-\nwork to a variety of learning problems, from classiﬁca-\ntion to reinforcement learning, we introduce a generic\nnotion of a learning task below.\nFormally, each task\nT\n= {L(x1, a1, . . . , xH, aH), q(x1), q(xt+1|xt, at), H}\nconsists of a loss function L, a distribution over initial ob-\nservations q(x1), a transition distribution q(xt+1|xt, at),\nand an episode length H. In i.i.d. supervised learning prob-\nlems, the length H = 1. The model may generate samples\nof length H by choosing an output at at each time t. The\nloss L(x1, a1, . . . , xH, aH) →R, provides task-speciﬁc\nfeedback, which might be in the form of a misclassiﬁcation\nloss or a cost function in a Markov decision process.\nmeta-learning\nlearning/adaptation\nθ\n∇L1\n∇L2\n∇L3\nθ∗\n1\nθ∗\n2\nθ∗\n3\nFigure 1. Diagram of our model-agnostic meta-learning algo-\nrithm (MAML), which optimizes for a representation θ that can\nquickly adapt to new tasks.\nIn our meta-learning scenario, we consider a distribution\nover tasks p(T ) that we want our model to be able to adapt\nto. In the K-shot learning setting, the model is trained to\nlearn a new task Ti drawn from p(T ) from only K samples\ndrawn from qi and feedback LTi generated by Ti. During\nmeta-training, a task Ti is sampled from p(T ), the model\nis trained with K samples and feedback from the corre-\nsponding loss LTi from Ti, and then tested on new samples\nfrom Ti. The model f is then improved by considering how\nthe test error on new data from qi changes with respect to\nthe parameters. In effect, the test error on sampled tasks Ti\nserves as the training error of the meta-learning process. At\nthe end of meta-training, new tasks are sampled from p(T ),\nand meta-performance is measured by the model’s perfor-\nmance after learning from K samples. Generally, tasks\nused for meta-testing are held out during meta-training.\n2.2. A Model-Agnostic Meta-Learning Algorithm\nIn contrast to prior work, which has sought to train re-\ncurrent neural networks that ingest entire datasets (San-\ntoro et al., 2016; Duan et al., 2016b) or feature embed-\ndings that can be combined with nonparametric methods at\ntest time (Vinyals et al., 2016; Koch, 2015), we propose a\nmethod that can learn the parameters of any standard model\nvia meta-learning in such a way as to prepare that model\nfor fast adaptation. The intuition behind this approach is\nthat some internal representations are more transferrable\nthan others. For example, a neural network might learn\ninternal features that are broadly applicable to all tasks in\np(T ), rather than a single individual task. How can we en-\ncourage the emergence of such general-purpose representa-\ntions? We take an explicit approach to this problem: since\nthe model will be ﬁne-tuned using a gradient-based learn-\ning rule on a new task, we will aim to learn a model in such\na way that this gradient-based learning rule can make rapid\nprogress on new tasks drawn from p(T ), without overﬁt-\nting. In effect, we will aim to ﬁnd model parameters that\nare sensitive to changes in the task, such that small changes\nin the parameters will produce large improvements on the\nloss function of any task drawn from p(T ), when altered in\nthe direction of the gradient of that loss (see Figure 1). We\n",
    "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nAlgorithm 1 Model-Agnostic Meta-Learning\nRequire: p(T ): distribution over tasks\nRequire: α, β: step size hyperparameters\n1: randomly initialize θ\n2: while not done do\n3:\nSample batch of tasks Ti ∼p(T )\n4:\nfor all Ti do\n5:\nEvaluate ∇θLTi(fθ) with respect to K examples\n6:\nCompute adapted parameters with gradient de-\nscent: θ′\ni = θ −α∇θLTi(fθ)\n7:\nend for\n8:\nUpdate θ ←θ −β∇θ\nP\nTi∼p(T ) LTi(fθ′\ni)\n9: end while\nmake no assumption on the form of the model, other than\nto assume that it is parametrized by some parameter vector\nθ, and that the loss function is smooth enough in θ that we\ncan use gradient-based learning techniques.\nFormally,\nwe\nconsider\na\nmodel\nrepresented\nby\na\nparametrized function fθ with parameters θ. When adapt-\ning to a new task Ti, the model’s parameters θ become θ′\ni.\nIn our method, the updated parameter vector θ′\ni is computed\nusing one or more gradient descent updates on task Ti. For\nexample, when using one gradient update,\nθ′\ni = θ −α∇θLTi(fθ).\nThe step size α may be ﬁxed as a hyperparameter or meta-\nlearned. For simplicity of notation, we will consider one\ngradient update for the rest of this section, but using multi-\nple gradient updates is a straightforward extension.\nThe model parameters are trained by optimizing for the per-\nformance of fθ′\ni with respect to θ across tasks sampled from\np(T ). More concretely, the meta-objective is as follows:\nmin\nθ\nX\nTi∼p(T )\nLTi(fθ′\ni) =\nX\nTi∼p(T )\nLTi(fθ−α∇θLTi(fθ))\nNote that the meta-optimization is performed over the\nmodel parameters θ, whereas the objective is computed us-\ning the updated model parameters θ′. In effect, our pro-\nposed method aims to optimize the model parameters such\nthat one or a small number of gradient steps on a new task\nwill produce maximally effective behavior on that task.\nThe meta-optimization across tasks is performed via\nstochastic gradient descent (SGD), such that the model pa-\nrameters θ are updated as follows:\nθ ←θ −β∇θ\nX\nTi∼p(T )\nLTi(fθ′\ni)\n(1)\nwhere β is the meta step size. The full algorithm, in the\ngeneral case, is outlined in Algorithm 1.\nThe MAML meta-gradient update involves a gradient\nthrough a gradient. Computationally, this requires an addi-\ntional backward pass through f to compute Hessian-vector\nproducts, which is supported by standard deep learning li-\nbraries such as TensorFlow (Abadi et al., 2016). In our\nexperiments, we also include a comparison to dropping\nthis backward pass and using a ﬁrst-order approximation,\nwhich we discuss in Section 5.2.\n3. Species of MAML\nIn this section, we discuss speciﬁc instantiations of our\nmeta-learning algorithm for supervised learning and rein-\nforcement learning. The domains differ in the form of loss\nfunction and in how data is generated by the task and pre-\nsented to the model, but the same basic adaptation mecha-\nnism can be applied in both cases.\n3.1. Supervised Regression and Classiﬁcation\nFew-shot learning is well-studied in the domain of super-\nvised tasks, where the goal is to learn a new function from\nonly a few input/output pairs for that task, using prior data\nfrom similar tasks for meta-learning. For example, the goal\nmight be to classify images of a Segway after seeing only\none or a few examples of a Segway, with a model that has\npreviously seen many other types of objects. Likewise, in\nfew-shot regression, the goal is to predict the outputs of\na continuous-valued function from only a few datapoints\nsampled from that function, after training on many func-\ntions with similar statistical properties.\nTo formalize the supervised regression and classiﬁcation\nproblems in the context of the meta-learning deﬁnitions in\nSection 2.1, we can deﬁne the horizon H = 1 and drop the\ntimestep subscript on xt, since the model accepts a single\ninput and produces a single output, rather than a sequence\nof inputs and outputs. The task Ti generates K i.i.d. ob-\nservations x from qi, and the task loss is represented by the\nerror between the model’s output for x and the correspond-\ning target values y for that observation and task.\nTwo common loss functions used for supervised classiﬁca-\ntion and regression are cross-entropy and mean-squared er-\nror (MSE), which we will describe below; though, other su-\npervised loss functions may be used as well. For regression\ntasks using mean-squared error, the loss takes the form:\nLTi(fφ) =\nX\nx(j),y(j)∼Ti\n∥fφ(x(j)) −y(j)∥2\n2,\n(2)\nwhere x(j), y(j) are an input/output pair sampled from task\nTi. In K-shot regression tasks, K input/output pairs are\nprovided for learning for each task.\nSimilarly, for discrete classiﬁcation tasks with a cross-\nentropy loss, the loss takes the form:\nLTi(fφ) =\nX\nx(j),y(j)∼Ti\ny(j) log fφ(x(j))\n+ (1 −y(j)) log(1 −fφ(x(j)))\n(3)\n",
    "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nAlgorithm 2 MAML for Few-Shot Supervised Learning\nRequire: p(T ): distribution over tasks\nRequire: α, β: step size hyperparameters\n1: randomly initialize θ\n2: while not done do\n3:\nSample batch of tasks Ti ∼p(T )\n4:\nfor all Ti do\n5:\nSample K datapoints D = {x(j), y(j)} from Ti\n6:\nEvaluate ∇θLTi(fθ) using D and LTi in Equation (2)\nor (3)\n7:\nCompute adapted parameters with gradient descent:\nθ′\ni = θ −α∇θLTi(fθ)\n8:\nSample datapoints D′\ni = {x(j), y(j)} from Ti for the\nmeta-update\n9:\nend for\n10:\nUpdate θ ←θ −β∇θ\nP\nTi∼p(T ) LTi(fθ′\ni) using each D′\ni\nand LTi in Equation 2 or 3\n11: end while\nAccording to the conventional terminology, K-shot classi-\nﬁcation tasks use K input/output pairs from each class, for\na total of NK data points for N-way classiﬁcation. Given a\ndistribution over tasks p(Ti), these loss functions can be di-\nrectly inserted into the equations in Section 2.2 to perform\nmeta-learning, as detailed in Algorithm 2.\n3.2. Reinforcement Learning\nIn reinforcement learning (RL), the goal of few-shot meta-\nlearning is to enable an agent to quickly acquire a policy for\na new test task using only a small amount of experience in\nthe test setting. A new task might involve achieving a new\ngoal or succeeding on a previously trained goal in a new\nenvironment. For example, an agent might learn to quickly\nﬁgure out how to navigate mazes so that, when faced with\na new maze, it can determine how to reliably reach the exit\nwith only a few samples. In this section, we will discuss\nhow MAML can be applied to meta-learning for RL.\nEach RL task Ti contains an initial state distribution qi(x1)\nand a transition distribution qi(xt+1|xt, at), and the loss\nLTi corresponds to the (negative) reward function R. The\nentire task is therefore a Markov decision process (MDP)\nwith horizon H, where the learner is allowed to query a\nlimited number of sample trajectories for few-shot learn-\ning. Any aspect of the MDP may change across tasks in\np(T ). The model being learned, fθ, is a policy that maps\nfrom states xt to a distribution over actions at at each\ntimestep t ∈{1, ..., H}. The loss for task Ti and model\nfφ takes the form\nLTi(fφ) = −Ext,at∼fφ,qTi\n\" H\nX\nt=1\nRi(xt, at)\n#\n.\n(4)\nIn K-shot reinforcement learning, K rollouts from fθ and\ntask Ti, (x1, a1, ...xH), and the corresponding rewards\nR(xt, at), may be used for adaptation on a new task Ti.\nAlgorithm 3 MAML for Reinforcement Learning\nRequire: p(T ): distribution over tasks\nRequire: α, β: step size hyperparameters\n1: randomly initialize θ\n2: while not done do\n3:\nSample batch of tasks Ti ∼p(T )\n4:\nfor all Ti do\n5:\nSample K trajectories D = {(x1, a1, ...xH)} using fθ\nin Ti\n6:\nEvaluate ∇θLTi(fθ) using D and LTi in Equation 4\n7:\nCompute adapted parameters with gradient descent:\nθ′\ni = θ −α∇θLTi(fθ)\n8:\nSample trajectories D′\ni = {(x1, a1, ...xH)} using fθ′\ni\nin Ti\n9:\nend for\n10:\nUpdate θ ←θ −β∇θ\nP\nTi∼p(T ) LTi(fθ′\ni) using each D′\ni\nand LTi in Equation 4\n11: end while\nSince the expected reward is generally not differentiable\ndue to unknown dynamics, we use policy gradient meth-\nods to estimate the gradient both for the model gradient\nupdate(s) and the meta-optimization. Since policy gradi-\nents are an on-policy algorithm, each additional gradient\nstep during the adaptation of fθ requires new samples from\nthe current policy fθi′. We detail the algorithm in Algo-\nrithm 3. This algorithm has the same structure as Algo-\nrithm 2, with the principal difference being that steps 5 and\n8 require sampling trajectories from the environment cor-\nresponding to task Ti. Practical implementations of this\nmethod may also use a variety of improvements recently\nproposed for policy gradient algorithms, including state\nor action-dependent baselines and trust regions (Schulman\net al., 2015).\n4. Related Work\nThe method that we propose in this paper addresses the\ngeneral problem of meta-learning (Thrun & Pratt, 1998;\nSchmidhuber, 1987; Naik & Mammone, 1992), which in-\ncludes few-shot learning. A popular approach for meta-\nlearning is to train a meta-learner that learns how to up-\ndate the parameters of the learner’s model (Bengio et al.,\n1992; Schmidhuber, 1992; Bengio et al., 1990). This ap-\nproach has been applied to learning to optimize deep net-\nworks (Hochreiter et al., 2001; Andrychowicz et al., 2016;\nLi & Malik, 2017), as well as for learning dynamically\nchanging recurrent networks (Ha et al., 2017). One recent\napproach learns both the weight initialization and the opti-\nmizer, for few-shot image recognition (Ravi & Larochelle,\n2017). Unlike these methods, the MAML learner’s weights\nare updated using the gradient, rather than a learned update;\nour method does not introduce additional parameters for\nmeta-learning nor require a particular learner architecture.\nFew-shot learning methods have also been developed for\n",
    "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nspeciﬁc tasks such as generative modeling (Edwards &\nStorkey, 2017; Rezende et al., 2016) and image recogni-\ntion (Vinyals et al., 2016). One successful approach for\nfew-shot classiﬁcation is to learn to compare new exam-\nples in a learned metric space using e.g.\nSiamese net-\nworks (Koch, 2015) or recurrence with attention mech-\nanisms (Vinyals et al., 2016; Shyam et al., 2017; Snell\net al., 2017). These approaches have generated some of the\nmost successful results, but are difﬁcult to directly extend\nto other problems, such as reinforcement learning.\nOur\nmethod, in contrast, is agnostic to the form of the model\nand to the particular learning task.\nAnother approach to meta-learning is to train memory-\naugmented models on many tasks, where the recurrent\nlearner is trained to adapt to new tasks as it is rolled out.\nSuch networks have been applied to few-shot image recog-\nnition (Santoro et al., 2016; Munkhdalai & Yu, 2017) and\nlearning “fast” reinforcement learning agents (Duan et al.,\n2016b; Wang et al., 2016).\nOur experiments show that\nour method outperforms the recurrent approach on few-\nshot classiﬁcation. Furthermore, unlike these methods, our\napproach simply provides a good weight initialization and\nuses the same gradient descent update for both the learner\nand meta-update. As a result, it is straightforward to ﬁne-\ntune the learner for additional gradient steps.\nOur approach is also related to methods for initialization of\ndeep networks. In computer vision, models pretrained on\nlarge-scale image classiﬁcation have been shown to learn\neffective features for a range of problems (Donahue et al.,\n2014).\nIn contrast, our method explicitly optimizes the\nmodel for fast adaptability, allowing it to adapt to new tasks\nwith only a few examples. Our method can also be viewed\nas explicitly maximizing sensitivity of new task losses to\nthe model parameters. A number of prior works have ex-\nplored sensitivity in deep networks, often in the context of\ninitialization (Saxe et al., 2014; Kirkpatrick et al., 2016).\nMost of these works have considered good random initial-\nizations, though a number of papers have addressed data-\ndependent initializers (Kr¨ahenb¨uhl et al., 2016; Salimans &\nKingma, 2016), including learned initializations (Husken\n& Goerick, 2000; Maclaurin et al., 2015). In contrast, our\nmethod explicitly trains the parameters for sensitivity on\na given task distribution, allowing for extremely efﬁcient\nadaptation for problems such as K-shot learning and rapid\nreinforcement learning in only one or a few gradient steps.\n5. Experimental Evaluation\nThe goal of our experimental evaluation is to answer the\nfollowing questions: (1) Can MAML enable fast learning\nof new tasks? (2) Can MAML be used for meta-learning\nin multiple different domains, including supervised regres-\nsion, classiﬁcation, and reinforcement learning? (3) Can a\nmodel learned with MAML continue to improve with addi-\ntional gradient updates and/or examples?\nAll of the meta-learning problems that we consider require\nsome amount of adaptation to new tasks at test-time. When\npossible, we compare our results to an oracle that receives\nthe identity of the task (which is a problem-dependent rep-\nresentation) as an additional input, as an upper bound on\nthe performance of the model. All of the experiments were\nperformed using TensorFlow (Abadi et al., 2016), which al-\nlows for automatic differentiation through the gradient up-\ndate(s) during meta-learning. The code is available online1.\n5.1. Regression\nWe start with a simple regression problem that illustrates\nthe basic principles of MAML. Each task involves regress-\ning from the input to the output of a sine wave, where the\namplitude and phase of the sinusoid are varied between\ntasks.\nThus, p(T ) is continuous, where the amplitude\nvaries within [0.1, 5.0] and the phase varies within [0, π],\nand the input and output both have a dimensionality of 1.\nDuring training and testing, datapoints x are sampled uni-\nformly from [−5.0, 5.0]. The loss is the mean-squared error\nbetween the prediction f(x) and true value. The regres-\nsor is a neural network model with 2 hidden layers of size\n40 with ReLU nonlinearities. When training with MAML,\nwe use one gradient update with K = 10 examples with\na ﬁxed step size α = 0.01, and use Adam as the meta-\noptimizer (Kingma & Ba, 2015). The baselines are like-\nwise trained with Adam. To evaluate performance, we ﬁne-\ntune a single meta-learned model on varying numbers of K\nexamples, and compare performance to two baselines: (a)\npretraining on all of the tasks, which entails training a net-\nwork to regress to random sinusoid functions and then, at\ntest-time, ﬁne-tuning with gradient descent on the K pro-\nvided points, using an automatically tuned step size, and\n(b) an oracle which receives the true amplitude and phase\nas input. In Appendix C, we show comparisons to addi-\ntional multi-task and adaptation methods.\nWe evaluate performance by ﬁne-tuning the model learned\nby MAML and the pretrained model on K = {5, 10, 20}\ndatapoints. During ﬁne-tuning, each gradient step is com-\nputed using the same K datapoints. The qualitative results,\nshown in Figure 2 and further expanded on in Appendix B\nshow that the learned model is able to quickly adapt with\nonly 5 datapoints, shown as purple triangles, whereas the\nmodel that is pretrained using standard supervised learning\non all tasks is unable to adequately adapt with so few dat-\napoints without catastrophic overﬁtting. Crucially, when\nthe K datapoints are all in one half of the input range, the\n1Code for the regression and supervised experiments is at\ngithub.com/cbfinn/maml and code for the RL experi-\nments is at github.com/cbfinn/maml_rl\n",
    "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nFigure 2. Few-shot adaptation for the simple regression task. Left: Note that MAML is able to estimate parts of the curve where there are\nno datapoints, indicating that the model has learned about the periodic structure of sine waves. Right: Fine-tuning of a model pretrained\non the same distribution of tasks without MAML, with a tuned step size. Due to the often contradictory outputs on the pre-training tasks,\nthis model is unable to recover a suitable representation and fails to extrapolate from the small number of test-time samples.\nFigure 3. Quantitative sinusoid regression results showing the\nlearning curve at meta test-time. Note that MAML continues to\nimprove with additional gradient steps without overﬁtting to the\nextremely small dataset during meta-testing, achieving a loss that\nis substantially lower than the baseline ﬁne-tuning approach.\nmodel trained with MAML can still infer the amplitude and\nphase in the other half of the range, demonstrating that the\nMAML trained model f has learned to model the periodic\nnature of the sine wave. Furthermore, we observe both in\nthe qualitative and quantitative results (Figure 3 and Ap-\npendix B) that the model learned with MAML continues\nto improve with additional gradient steps, despite being\ntrained for maximal performance after one gradient step.\nThis improvement suggests that MAML optimizes the pa-\nrameters such that they lie in a region that is amenable to\nfast adaptation and is sensitive to loss functions from p(T ),\nas discussed in Section 2.2, rather than overﬁtting to pa-\nrameters θ that only improve after one step.\n5.2. Classiﬁcation\nTo evaluate MAML in comparison to prior meta-learning\nand few-shot learning algorithms, we applied our method\nto few-shot image recognition on the Omniglot (Lake et al.,\n2011) and MiniImagenet datasets. The Omniglot dataset\nconsists of 20 instances of 1623 characters from 50 dif-\nferent alphabets. Each instance was drawn by a different\nperson. The MiniImagenet dataset was proposed by Ravi\n& Larochelle (2017), and involves 64 training classes, 12\nvalidation classes, and 24 test classes. The Omniglot and\nMiniImagenet image recognition tasks are the most com-\nmon recently used few-shot learning benchmarks (Vinyals\net al., 2016; Santoro et al., 2016; Ravi & Larochelle, 2017).\nWe follow the experimental protocol proposed by Vinyals\net al. (2016), which involves fast learning of N-way clas-\nsiﬁcation with 1 or 5 shots. The problem of N-way classi-\nﬁcation is set up as follows: select N unseen classes, pro-\nvide the model with K different instances of each of the N\nclasses, and evaluate the model’s ability to classify new in-\nstances within the N classes. For Omniglot, we randomly\nselect 1200 characters for training, irrespective of alphabet,\nand use the remaining for testing. The Omniglot dataset is\naugmented with rotations by multiples of 90 degrees, as\nproposed by Santoro et al. (2016).\nOur model follows the same architecture as the embedding\nfunction used by Vinyals et al. (2016), which has 4 mod-\nules with a 3 × 3 convolutions and 64 ﬁlters, followed by\nbatch normalization (Ioffe & Szegedy, 2015), a ReLU non-\nlinearity, and 2 × 2 max-pooling. The Omniglot images\nare downsampled to 28 × 28, so the dimensionality of the\nlast hidden layer is 64. As in the baseline classiﬁer used\nby Vinyals et al. (2016), the last layer is fed into a soft-\nmax. For Omniglot, we used strided convolutions instead\nof max-pooling. For MiniImagenet, we used 32 ﬁlters per\nlayer to reduce overﬁtting, as done by (Ravi & Larochelle,\n2017). In order to also provide a fair comparison against\nmemory-augmented neural networks (Santoro et al., 2016)\nand to test the ﬂexibility of MAML, we also provide re-\nsults for a non-convolutional network. For this, we use a\nnetwork with 4 hidden layers with sizes 256, 128, 64, 64,\neach including batch normalization and ReLU nonlineari-\nties, followed by a linear layer and softmax. For all models,\nthe loss function is the cross-entropy error between the pre-\ndicted and true class. Additional hyperparameter details are\nincluded in Appendix A.1.\nWe present the results in Table 1. The convolutional model\nlearned by MAML compares well to the state-of-the-art re-\nsults on this task, narrowly outperforming the prior meth-\nods. Some of these existing methods, such as matching\nnetworks, Siamese networks, and memory models are de-\nsigned with few-shot classiﬁcation in mind, and are not\nreadily applicable to domains such as reinforcement learn-\ning.\nAdditionally, the model learned with MAML uses\n",
    "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nTable 1. Few-shot classiﬁcation on held-out Omniglot characters (top) and the MiniImagenet test set (bottom). MAML achieves results\nthat are comparable to or outperform state-of-the-art convolutional and recurrent models. Siamese nets, matching nets, and the memory\nmodule approaches are all speciﬁc to classiﬁcation, and are not directly applicable to regression or RL scenarios. The ± shows 95%\nconﬁdence intervals over tasks. Note that the Omniglot results may not be strictly comparable since the train/test splits used in the prior\nwork were not available. The MiniImagenet evaluation of baseline methods and matching networks is from Ravi & Larochelle (2017).\n5-way Accuracy\n20-way Accuracy\nOmniglot (Lake et al., 2011)\n1-shot\n5-shot\n1-shot\n5-shot\nMANN, no conv (Santoro et al., 2016)\n82.8%\n94.9%\n–\n–\nMAML, no conv (ours)\n89.7 ± 1.1%\n97.5 ± 0.6%\n–\n–\nSiamese nets (Koch, 2015)\n97.3%\n98.4%\n88.2%\n97.0%\nmatching nets (Vinyals et al., 2016)\n98.1%\n98.9%\n93.8%\n98.5%\nneural statistician (Edwards & Storkey, 2017)\n98.1%\n99.5%\n93.2%\n98.1%\nmemory mod. (Kaiser et al., 2017)\n98.4%\n99.6%\n95.0%\n98.6%\nMAML (ours)\n98.7 ± 0.4%\n99.9 ± 0.1%\n95.8 ± 0.3%\n98.9 ± 0.2%\n5-way Accuracy\nMiniImagenet (Ravi & Larochelle, 2017)\n1-shot\n5-shot\nﬁne-tuning baseline\n28.86 ± 0.54%\n49.79 ± 0.79%\nnearest neighbor baseline\n41.08 ± 0.70%\n51.04 ± 0.65%\nmatching nets (Vinyals et al., 2016)\n43.56 ± 0.84%\n55.31 ± 0.73%\nmeta-learner LSTM (Ravi & Larochelle, 2017)\n43.44 ± 0.77%\n60.60 ± 0.71%\nMAML, ﬁrst order approx. (ours)\n48.07 ± 1.75%\n63.15 ± 0.91%\nMAML (ours)\n48.70 ± 1.84%\n63.11 ± 0.92%\nfewer overall parameters compared to matching networks\nand the meta-learner LSTM, since the algorithm does not\nintroduce any additional parameters beyond the weights\nof the classiﬁer itself. Compared to these prior methods,\nmemory-augmented neural networks (Santoro et al., 2016)\nspeciﬁcally, and recurrent meta-learning models in gen-\neral, represent a more broadly applicable class of meth-\nods that, like MAML, can be used for other tasks such as\nreinforcement learning (Duan et al., 2016b; Wang et al.,\n2016). However, as shown in the comparison, MAML sig-\nniﬁcantly outperforms memory-augmented networks and\nthe meta-learner LSTM on 5-way Omniglot and MiniIm-\nagenet classiﬁcation, both in the 1-shot and 5-shot case.\nA signiﬁcant computational expense in MAML comes\nfrom the use of second derivatives when backpropagat-\ning the meta-gradient through the gradient operator in\nthe meta-objective (see Equation (1)). On MiniImagenet,\nwe show a comparison to a ﬁrst-order approximation of\nMAML, where these second derivatives are omitted. Note\nthat the resulting method still computes the meta-gradient\nat the post-update parameter values θ′\ni, which provides for\neffective meta-learning. Surprisingly however, the perfor-\nmance of this method is nearly the same as that obtained\nwith full second derivatives, suggesting that most of the\nimprovement in MAML comes from the gradients of the\nobjective at the post-update parameter values, rather than\nthe second order updates from differentiating through the\ngradient update. Past work has observed that ReLU neu-\nral networks are locally almost linear (Goodfellow et al.,\n2015), which suggests that second derivatives may be close\nto zero in most cases, partially explaining the good perfor-\nFigure 4. Top: quantitative results from 2D navigation task, Bot-\ntom: qualitative comparison between model learned with MAML\nand with ﬁne-tuning from a pretrained network.\nmance of the ﬁrst-order approximation. This approxima-\ntion removes the need for computing Hessian-vector prod-\nucts in an additional backward pass, which we found led to\nroughly 33% speed-up in network computation.\n5.3. Reinforcement Learning\nTo evaluate MAML on reinforcement learning problems,\nwe constructed several sets of tasks based off of the sim-\nulated continuous control environments in the rllab bench-\nmark suite (Duan et al., 2016a). We discuss the individual\ndomains below. In all of the domains, the model trained\nby MAML is a neural network policy with two hidden lay-\ners of size 100, with ReLU nonlinearities. The gradient\nupdates are computed using vanilla policy gradient (RE-\nINFORCE) (Williams, 1992), and we use trust-region pol-\nicy optimization (TRPO) as the meta-optimizer (Schulman\net al., 2015). In order to avoid computing third derivatives,\n",
    "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nFigure 5. Reinforcement learning results for the half-cheetah and ant locomotion tasks, with the tasks shown on the far right. Each\ngradient step requires additional samples from the environment, unlike the supervised learning tasks. The results show that MAML can\nadapt to new goal velocities and directions substantially faster than conventional pretraining or random initialization, achieving good\nperforms in just two or three gradient steps. We exclude the goal velocity, random baseline curves, since the returns are much worse\n(< −200 for cheetah and < −25 for ant).\nwe use ﬁnite differences to compute the Hessian-vector\nproducts for TRPO. For both learning and meta-learning\nupdates, we use the standard linear feature baseline pro-\nposed by Duan et al. (2016a), which is ﬁtted separately at\neach iteration for each sampled task in the batch. We com-\npare to three baseline models: (a) pretraining one policy on\nall of the tasks and then ﬁne-tuning, (b) training a policy\nfrom randomly initialized weights, and (c) an oracle policy\nwhich receives the parameters of the task as input, which\nfor the tasks below corresponds to a goal position, goal di-\nrection, or goal velocity for the agent. The baseline models\nof (a) and (b) are ﬁne-tuned with gradient descent with a\nmanually tuned step size. Videos of the learned policies\ncan be viewed at sites.google.com/view/maml\n2D Navigation. In our ﬁrst meta-RL experiment, we study\na set of tasks where a point agent must move to different\ngoal positions in 2D, randomly chosen for each task within\na unit square. The observation is the current 2D position,\nand actions correspond to velocity commands clipped to be\nin the range [−0.1, 0.1]. The reward is the negative squared\ndistance to the goal, and episodes terminate when the agent\nis within 0.01 of the goal or at the horizon of H = 100. The\npolicy was trained with MAML to maximize performance\nafter 1 policy gradient update using 20 trajectories. Ad-\nditional hyperparameter settings for this problem and the\nfollowing RL problems are in Appendix A.2. In our evalu-\nation, we compare adaptation to a new task with up to 4 gra-\ndient updates, each with 40 samples. The results in Figure 4\nshow the adaptation performance of models that are initial-\nized with MAML, conventional pretraining on the same set\nof tasks, random initialization, and an oracle policy that\nreceives the goal position as input. The results show that\nMAML can learn a model that adapts much more quickly\nin a single gradient update, and furthermore continues to\nimprove with additional updates.\nLocomotion. To study how well MAML can scale to more\ncomplex deep RL problems, we also study adaptation on\nhigh-dimensional locomotion tasks with the MuJoCo sim-\nulator (Todorov et al., 2012). The tasks require two sim-\nulated robots – a planar cheetah and a 3D quadruped (the\n“ant”) – to run in a particular direction or at a particular\nvelocity. In the goal velocity experiments, the reward is\nthe negative absolute value between the current velocity of\nthe agent and a goal, which is chosen uniformly at random\nbetween 0.0 and 2.0 for the cheetah and between 0.0 and\n3.0 for the ant. In the goal direction experiments, the re-\nward is the magnitude of the velocity in either the forward\nor backward direction, chosen at random for each task in\np(T ). The horizon is H = 200, with 20 rollouts per gradi-\nent step for all problems except the ant forward/backward\ntask, which used 40 rollouts per step. The results in Fig-\nure 5 show that MAML learns a model that can quickly\nadapt its velocity and direction with even just a single gra-\ndient update, and continues to improve with more gradi-\nent steps. The results also show that, on these challenging\ntasks, the MAML initialization substantially outperforms\nrandom initialization and pretraining. In fact, pretraining\nis in some cases worse than random initialization, a fact\nobserved in prior RL work (Parisotto et al., 2016).\n6. Discussion and Future Work\nWe introduced a meta-learning method based on learning\neasily adaptable model parameters through gradient de-\nscent. Our approach has a number of beneﬁts. It is simple\nand does not introduce any learned parameters for meta-\nlearning. It can be combined with any model representation\nthat is amenable to gradient-based training, and any differ-\nentiable objective, including classiﬁcation, regression, and\nreinforcement learning. Lastly, since our method merely\nproduces a weight initialization, adaptation can be per-\nformed with any amount of data and any number of gra-\ndient steps, though we demonstrate state-of-the-art results\non classiﬁcation with only one or ﬁve examples per class.\nWe also show that our method can adapt an RL agent using\npolicy gradients and a very modest amount of experience.\nReusing knowledge from past tasks may be a crucial in-\ngredient in making high-capacity scalable models, such as\ndeep neural networks, amenable to fast training with small\ndatasets. We believe that this work is one step toward a sim-\nple and general-purpose meta-learning technique that can\nbe applied to any problem and any model. Further research\nin this area can make multitask initialization a standard in-\ngredient in deep learning and reinforcement learning.\n",
    "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nAcknowledgements\nThe authors would like to thank Xi Chen and Trevor Darrell\nfor helpful discussions, Yan Duan and Alex Lee for techni-\ncal advice, Nikhil Mishra, Haoran Tang, and Greg Kahn for\nfeedback on an early draft of the paper, and the anonymous\nreviewers for their comments. This work was supported in\npart by an ONR PECASE award and an NSF GRFP award.\nReferences\nAbadi, Mart´ın, Agarwal, Ashish, Barham, Paul, Brevdo,\nEugene, Chen, Zhifeng, Citro, Craig, Corrado, Greg S,\nDavis, Andy, Dean, Jeffrey, Devin, Matthieu, et al. Ten-\nsorﬂow: Large-scale machine learning on heterogeneous\ndistributed systems. arXiv preprint arXiv:1603.04467,\n2016.\nAndrychowicz, Marcin, Denil, Misha, Gomez, Sergio,\nHoffman, Matthew W, Pfau, David, Schaul, Tom, and\nde Freitas, Nando. Learning to learn by gradient descent\nby gradient descent. In Neural Information Processing\nSystems (NIPS), 2016.\nBengio, Samy, Bengio, Yoshua, Cloutier, Jocelyn, and\nGecsei, Jan. On the optimization of a synaptic learning\nrule. In Optimality in Artiﬁcial and Biological Neural\nNetworks, pp. 6–8, 1992.\nBengio, Yoshua, Bengio, Samy, and Cloutier, Jocelyn.\nLearning a synaptic learning rule.\nUniversit´e de\nMontr´eal, D´epartement d’informatique et de recherche\nop´erationnelle, 1990.\nDonahue, Jeff, Jia, Yangqing, Vinyals, Oriol, Hoffman,\nJudy, Zhang, Ning, Tzeng, Eric, and Darrell, Trevor. De-\ncaf: A deep convolutional activation feature for generic\nvisual recognition. In International Conference on Ma-\nchine Learning (ICML), 2014.\nDuan, Yan, Chen, Xi, Houthooft, Rein, Schulman, John,\nand Abbeel, Pieter. Benchmarking deep reinforcement\nlearning for continuous control. In International Con-\nference on Machine Learning (ICML), 2016a.\nDuan, Yan, Schulman, John, Chen, Xi, Bartlett, Peter L,\nSutskever, Ilya, and Abbeel, Pieter. Rl2: Fast reinforce-\nment learning via slow reinforcement learning.\narXiv\npreprint arXiv:1611.02779, 2016b.\nEdwards, Harrison and Storkey, Amos. Towards a neural\nstatistician. International Conference on Learning Rep-\nresentations (ICLR), 2017.\nGoodfellow, Ian J, Shlens, Jonathon, and Szegedy, Chris-\ntian. Explaining and harnessing adversarial examples.\nInternational Conference on Learning Representations\n(ICLR), 2015.\nHa, David, Dai, Andrew, and Le, Quoc V. Hypernetworks.\nInternational Conference on Learning Representations\n(ICLR), 2017.\nHochreiter, Sepp, Younger, A Steven, and Conwell, Pe-\nter R.\nLearning to learn using gradient descent.\nIn\nInternational Conference on Artiﬁcial Neural Networks.\nSpringer, 2001.\nHusken, Michael and Goerick, Christian. Fast learning for\nproblem classes using knowledge based network initial-\nization. In Neural Networks, 2000. IJCNN 2000, Pro-\nceedings of the IEEE-INNS-ENNS International Joint\nConference on, volume 6, pp. 619–624. IEEE, 2000.\nIoffe, Sergey and Szegedy, Christian. Batch normalization:\nAccelerating deep network training by reducing internal\ncovariate shift.\nInternational Conference on Machine\nLearning (ICML), 2015.\nKaiser, Lukasz, Nachum, Oﬁr, Roy, Aurko, and Bengio,\nSamy. Learning to remember rare events. International\nConference on Learning Representations (ICLR), 2017.\nKingma, Diederik and Ba, Jimmy. Adam: A method for\nstochastic optimization.\nInternational Conference on\nLearning Representations (ICLR), 2015.\nKirkpatrick, James, Pascanu, Razvan, Rabinowitz, Neil,\nVeness, Joel, Desjardins, Guillaume, Rusu, Andrei A,\nMilan, Kieran, Quan, John, Ramalho, Tiago, Grabska-\nBarwinska, Agnieszka, et al.\nOvercoming catas-\ntrophic forgetting in neural networks.\narXiv preprint\narXiv:1612.00796, 2016.\nKoch, Gregory. Siamese neural networks for one-shot im-\nage recognition. ICML Deep Learning Workshop, 2015.\nKr¨ahenb¨uhl, Philipp, Doersch, Carl, Donahue, Jeff, and\nDarrell, Trevor. Data-dependent initializations of con-\nvolutional neural networks. International Conference on\nLearning Representations (ICLR), 2016.\nLake, Brenden M, Salakhutdinov, Ruslan, Gross, Jason,\nand Tenenbaum, Joshua B. One shot learning of simple\nvisual concepts. In Conference of the Cognitive Science\nSociety (CogSci), 2011.\nLi, Ke and Malik, Jitendra. Learning to optimize. Interna-\ntional Conference on Learning Representations (ICLR),\n2017.\nMaclaurin, Dougal, Duvenaud, David, and Adams, Ryan.\nGradient-based hyperparameter optimization through re-\nversible learning. In International Conference on Ma-\nchine Learning (ICML), 2015.\n",
    "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nMunkhdalai, Tsendsuren and Yu, Hong.\nMeta net-\nworks. International Conferecence on Machine Learn-\ning (ICML), 2017.\nNaik, Devang K and Mammone, RJ. Meta-neural networks\nthat learn by learning. In International Joint Conference\non Neural Netowrks (IJCNN), 1992.\nParisotto, Emilio, Ba, Jimmy Lei, and Salakhutdinov, Rus-\nlan. Actor-mimic: Deep multitask and transfer reinforce-\nment learning.\nInternational Conference on Learning\nRepresentations (ICLR), 2016.\nRavi, Sachin and Larochelle, Hugo.\nOptimization as a\nmodel for few-shot learning. In International Confer-\nence on Learning Representations (ICLR), 2017.\nRei,\nMarek.\nOnline representation learning in re-\ncurrent neural language models.\narXiv preprint\narXiv:1508.03854, 2015.\nRezende, Danilo Jimenez, Mohamed, Shakir, Danihelka,\nIvo, Gregor, Karol, and Wierstra, Daan. One-shot gener-\nalization in deep generative models. International Con-\nference on Machine Learning (ICML), 2016.\nSalimans, Tim and Kingma, Diederik P. Weight normaliza-\ntion: A simple reparameterization to accelerate training\nof deep neural networks. In Neural Information Process-\ning Systems (NIPS), 2016.\nSantoro, Adam, Bartunov, Sergey, Botvinick, Matthew,\nWierstra, Daan, and Lillicrap, Timothy. Meta-learning\nwith memory-augmented neural networks. In Interna-\ntional Conference on Machine Learning (ICML), 2016.\nSaxe, Andrew, McClelland, James, and Ganguli, Surya.\nExact solutions to the nonlinear dynamics of learning in\ndeep linear neural networks. International Conference\non Learning Representations (ICLR), 2014.\nSchmidhuber, Jurgen.\nEvolutionary principles in self-\nreferential learning.\nOn learning how to learn: The\nmeta-meta-... hook.) Diploma thesis, Institut f. Infor-\nmatik, Tech. Univ. Munich, 1987.\nSchmidhuber, J¨urgen.\nLearning to control fast-weight\nmemories:\nAn alternative to dynamic recurrent net-\nworks. Neural Computation, 1992.\nSchulman, John, Levine, Sergey, Abbeel, Pieter, Jordan,\nMichael I, and Moritz, Philipp.\nTrust region policy\noptimization. In International Conference on Machine\nLearning (ICML), 2015.\nShyam, Pranav, Gupta, Shubham, and Dukkipati, Ambed-\nkar. Attentive recurrent comparators. International Con-\nferecence on Machine Learning (ICML), 2017.\nSnell, Jake, Swersky, Kevin, and Zemel, Richard S. Pro-\ntotypical networks for few-shot learning. arXiv preprint\narXiv:1703.05175, 2017.\nThrun, Sebastian and Pratt, Lorien.\nLearning to learn.\nSpringer Science & Business Media, 1998.\nTodorov, Emanuel, Erez, Tom, and Tassa, Yuval. Mujoco:\nA physics engine for model-based control.\nIn Inter-\nnational Conference on Intelligent Robots and Systems\n(IROS), 2012.\nVinyals, Oriol, Blundell, Charles, Lillicrap, Tim, Wierstra,\nDaan, et al. Matching networks for one shot learning. In\nNeural Information Processing Systems (NIPS), 2016.\nWang, Jane X, Kurth-Nelson, Zeb, Tirumala, Dhruva,\nSoyer, Hubert, Leibo, Joel Z, Munos, Remi, Blun-\ndell, Charles, Kumaran, Dharshan, and Botvinick,\nMatt. Learning to reinforcement learn. arXiv preprint\narXiv:1611.05763, 2016.\nWilliams, Ronald J. Simple statistical gradient-following\nalgorithms for connectionist reinforcement learning.\nMachine learning, 8(3-4):229–256, 1992.\n",
    "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nA. Additional Experiment Details\nIn this section, we provide additional details of the experi-\nmental set-up and hyperparameters.\nA.1. Classiﬁcation\nFor N-way, K-shot classiﬁcation, each gradient is com-\nputed using a batch size of NK examples. For Omniglot,\nthe 5-way convolutional and non-convolutional MAML\nmodels were each trained with 1 gradient step with step size\nα = 0.4 and a meta batch-size of 32 tasks. The network\nwas evaluated using 3 gradient steps with the same step\nsize α = 0.4. The 20-way convolutional MAML model\nwas trained and evaluated with 5 gradient steps with step\nsize α = 0.1. During training, the meta batch-size was set\nto 16 tasks. For MiniImagenet, both models were trained\nusing 5 gradient steps of size α = 0.01, and evaluated using\n10 gradient steps at test time. Following Ravi & Larochelle\n(2017), 15 examples per class were used for evaluating the\npost-update meta-gradient. We used a meta batch-size of\n4 and 2 tasks for 1-shot and 5-shot training respectively.\nAll models were trained for 60000 iterations on a single\nNVIDIA Pascal Titan X GPU.\nA.2. Reinforcement Learning\nIn all reinforcement learning experiments, the MAML pol-\nicy was trained using a single gradient step with α = 0.1.\nDuring evaluation, we found that halving the learning rate\nafter the ﬁrst gradient step produced superior performance.\nThus, the step size during adaptation was set to α = 0.1\nfor the ﬁrst step, and α = 0.05 for all future steps. The\nstep sizes for the baseline methods were manually tuned for\neach domain. In the 2D navigation, we used a meta batch\nsize of 20; in the locomotion problems, we used a meta\nbatch size of 40 tasks. The MAML models were trained\nfor up to 500 meta-iterations, and the model with the best\naverage return during training was used for evaluation. For\nthe ant goal velocity task, we added a positive reward bonus\nat each timestep to prevent the ant from ending the episode.\nB. Additional Sinusoid Results\nIn Figure 6, we show the full quantitative results of the\nMAML model trained on 10-shot learning and evaluated\non 5-shot, 10-shot, and 20-shot. In Figure 7, we show the\nqualitative performance of MAML and the pretrained base-\nline on randomly sampled sinusoids.\nC. Additional Comparisons\nIn this section, we include more thorough evaluations of\nour approach, including additional multi-task baselines and\na comparison representative of the approach of Rei (2015).\nC.1. Multi-task baselines\nThe pretraining baseline in the main text trained a single\nnetwork on all tasks, which we referred to as “pretraining\non all tasks”. To evaluate the model, as with MAML, we\nﬁne-tuned this model on each test task using K examples.\nIn the domains that we study, different tasks involve dif-\nferent output values for the same input. As a result, by\npre-training on all tasks, the model would learn to output\nthe average output for a particular input value. In some in-\nstances, this model may learn very little about the actual\ndomain, and instead learn about the range of the output\nspace.\nWe experimented with a multi-task method to provide a\npoint of comparison, where instead of averaging in the out-\nput space, we averaged in the parameter space. To achieve\naveraging in parameter space, we sequentially trained 500\nseparate models on 500 tasks drawn from p(T ).\nEach\nmodel was initialized randomly and trained on a large\namount of data from its assigned task. We then took the\naverage parameter vector across models and ﬁne-tuned on\n5 datapoints with a tuned step size. All of our experiments\nfor this method were on the sinusoid task because of com-\nputational requirements. The error of the individual regres-\nsors was low: less than 0.02 on their respective sine waves.\nWe tried three variants of this set-up. During training of\nthe individual regressors, we tried using one of the fol-\nlowing: no regularization, standard ℓ2 weight decay, and\nℓ2 weight regularization to the mean parameter vector thus\nfar of the trained regressors. The latter two variants en-\ncourage the individual models to ﬁnd parsimonious solu-\ntions. When using regularization, we set the magnitude of\nthe regularization to be as high as possible without signif-\nicantly deterring performance. In our results, we refer to\nthis approach as “multi-task”. As seen in the results in Ta-\nble 2, we ﬁnd averaging in the parameter space (multi-task)\nperformed worse than averaging in the output space (pre-\ntraining on all tasks). This suggests that it is difﬁcult to\nﬁnd parsimonious solutions to multiple tasks when training\non tasks separately, and that MAML is learning a solution\nthat is more sophisticated than the mean optimal parameter\nvector.\nC.2. Context vector adaptation\nRei (2015) developed a method which learns a context vec-\ntor that can be adapted online, with an application to re-\ncurrent language models. The parameters in this context\nvector are learned and adapted in the same way as the pa-\nrameters in the MAML model. To provide a comparison\nto using such a context vector for meta-learning problems,\nwe concatenated a set of free parameters z to the input x,\nand only allowed the gradient steps to modify z, rather than\nmodifying the model parameters θ, as in MAML. For im-\n",
    "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nFigure 6. Quantitative sinusoid regression results showing test-time learning curves with varying numbers of K test-time samples. Each\ngradient step is computed using the same K examples. Note that MAML continues to improve with additional gradient steps without\noverﬁtting to the extremely small dataset during meta-testing, and achieves a loss that is substantially lower than the baseline ﬁne-tuning\napproach.\nTable 2. Additional multi-task baselines on the sinusoid regres-\nsion domain, showing 5-shot mean squared error. The results sug-\ngest that MAML is learning a solution more sophisticated than the\nmean optimal parameter vector.\nnum. grad steps\n1\n5\n10\nmulti-task, no reg\n4.19\n3.85\n3.69\nmulti-task, l2 reg\n7.18\n5.69\n5.60\nmulti-task, reg to mean θ\n2.91\n2.72\n2.71\npretrain on all tasks\n2.41\n2.23\n2.19\nMAML (ours)\n0.67\n0.38\n0.35\nTable 3. 5-way Omniglot Classiﬁcation\n1-shot\n5-shot\ncontext vector\n94.9 ± 0.9%\n97.7 ± 0.3%\nMAML\n98.7 ± 0.4%\n99.9 ± 0.1%\nage inputs, z was concatenated channel-wise with the input\nimage. We ran this method on Omniglot and two RL do-\nmains following the same experimental protocol. We report\nthe results in Tables 3, 4, and 5. Learning an adaptable con-\ntext vector performed well on the toy pointmass problem,\nbut sub-par on more difﬁcult problems, likely due to a less\nﬂexible meta-optimization.\nTable 4. 2D Pointmass, average return\nnum. grad steps\n0\n1\n2\n3\ncontext vector\n−42.42\n−13.90\n−5.17\n−3.18\nMAML (ours)\n−40.41\n−11.68\n−3.33\n−3.23\nTable 5. Half-cheetah forward/backward, average return\nnum. grad steps\n0\n1\n2\n3\ncontext vector\n−40.49\n−44.08\n−38.27\n−42.50\nMAML (ours)\n−50.69\n293.19\n313.48\n315.65\n",
    "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nFigure 7. A random sample of qualitative results from the sinusoid regression task.\n"
  ],
  "full_text": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nChelsea Finn 1 Pieter Abbeel 1 2 Sergey Levine 1\nAbstract\nWe propose an algorithm for meta-learning that\nis model-agnostic, in the sense that it is com-\npatible with any model trained with gradient de-\nscent and applicable to a variety of different\nlearning problems, including classiﬁcation, re-\ngression, and reinforcement learning. The goal\nof meta-learning is to train a model on a vari-\nety of learning tasks, such that it can solve new\nlearning tasks using only a small number of train-\ning samples. In our approach, the parameters of\nthe model are explicitly trained such that a small\nnumber of gradient steps with a small amount\nof training data from a new task will produce\ngood generalization performance on that task. In\neffect, our method trains the model to be easy\nto ﬁne-tune. We demonstrate that this approach\nleads to state-of-the-art performance on two few-\nshot image classiﬁcation benchmarks, produces\ngood results on few-shot regression, and acceler-\nates ﬁne-tuning for policy gradient reinforcement\nlearning with neural network policies.\n1. Introduction\nLearning quickly is a hallmark of human intelligence,\nwhether it involves recognizing objects from a few exam-\nples or quickly learning new skills after just minutes of\nexperience. Our artiﬁcial agents should be able to do the\nsame, learning and adapting quickly from only a few exam-\nples, and continuing to adapt as more data becomes avail-\nable. This kind of fast and ﬂexible learning is challenging,\nsince the agent must integrate its prior experience with a\nsmall amount of new information, while avoiding overﬁt-\nting to the new data. Furthermore, the form of prior ex-\nperience and new data will depend on the task. As such,\nfor the greatest applicability, the mechanism for learning to\nlearn (or meta-learning) should be general to the task and\n1University of California, Berkeley 2OpenAI. Correspondence\nto: Chelsea Finn <cbﬁnn@eecs.berkeley.edu>.\nProceedings of the 34 th International Conference on Machine\nLearning, Sydney, Australia, PMLR 70, 2017. Copyright 2017\nby the author(s).\nthe form of computation required to complete the task.\nIn this work, we propose a meta-learning algorithm that\nis general and model-agnostic, in the sense that it can be\ndirectly applied to any learning problem and model that\nis trained with a gradient descent procedure. Our focus\nis on deep neural network models, but we illustrate how\nour approach can easily handle different architectures and\ndifferent problem settings, including classiﬁcation, regres-\nsion, and policy gradient reinforcement learning, with min-\nimal modiﬁcation. In meta-learning, the goal of the trained\nmodel is to quickly learn a new task from a small amount\nof new data, and the model is trained by the meta-learner\nto be able to learn on a large number of different tasks.\nThe key idea underlying our method is to train the model’s\ninitial parameters such that the model has maximal perfor-\nmance on a new task after the parameters have been up-\ndated through one or more gradient steps computed with\na small amount of data from that new task. Unlike prior\nmeta-learning methods that learn an update function or\nlearning rule (Schmidhuber, 1987; Bengio et al., 1992;\nAndrychowicz et al., 2016; Ravi & Larochelle, 2017), our\nalgorithm does not expand the number of learned param-\neters nor place constraints on the model architecture (e.g.\nby requiring a recurrent model (Santoro et al., 2016) or a\nSiamese network (Koch, 2015)), and it can be readily com-\nbined with fully connected, convolutional, or recurrent neu-\nral networks. It can also be used with a variety of loss func-\ntions, including differentiable supervised losses and non-\ndifferentiable reinforcement learning objectives.\nThe process of training a model’s parameters such that a\nfew gradient steps, or even a single gradient step, can pro-\nduce good results on a new task can be viewed from a fea-\nture learning standpoint as building an internal representa-\ntion that is broadly suitable for many tasks. If the internal\nrepresentation is suitable to many tasks, simply ﬁne-tuning\nthe parameters slightly (e.g. by primarily modifying the top\nlayer weights in a feedforward model) can produce good\nresults. In effect, our procedure optimizes for models that\nare easy and fast to ﬁne-tune, allowing the adaptation to\nhappen in the right space for fast learning. From a dynami-\ncal systems standpoint, our learning process can be viewed\nas maximizing the sensitivity of the loss functions of new\ntasks with respect to the parameters: when the sensitivity\nis high, small local changes to the parameters can lead to\narXiv:1703.03400v3  [cs.LG]  18 Jul 2017\n\n\nModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nlarge improvements in the task loss.\nThe primary contribution of this work is a simple model-\nand task-agnostic algorithm for meta-learning that trains\na model’s parameters such that a small number of gradi-\nent updates will lead to fast learning on a new task. We\ndemonstrate the algorithm on different model types, includ-\ning fully connected and convolutional networks, and in sev-\neral distinct domains, including few-shot regression, image\nclassiﬁcation, and reinforcement learning. Our evaluation\nshows that our meta-learning algorithm compares favor-\nably to state-of-the-art one-shot learning methods designed\nspeciﬁcally for supervised classiﬁcation, while using fewer\nparameters, but that it can also be readily applied to regres-\nsion and can accelerate reinforcement learning in the pres-\nence of task variability, substantially outperforming direct\npretraining as initialization.\n2. Model-Agnostic Meta-Learning\nWe aim to train models that can achieve rapid adaptation, a\nproblem setting that is often formalized as few-shot learn-\ning. In this section, we will deﬁne the problem setup and\npresent the general form of our algorithm.\n2.1. Meta-Learning Problem Set-Up\nThe goal of few-shot meta-learning is to train a model that\ncan quickly adapt to a new task using only a few datapoints\nand training iterations. To accomplish this, the model or\nlearner is trained during a meta-learning phase on a set\nof tasks, such that the trained model can quickly adapt to\nnew tasks using only a small number of examples or trials.\nIn effect, the meta-learning problem treats entire tasks as\ntraining examples. In this section, we formalize this meta-\nlearning problem setting in a general manner, including\nbrief examples of different learning domains. We will dis-\ncuss two different learning domains in detail in Section 3.\nWe consider a model, denoted f, that maps observa-\ntions x to outputs a.\nDuring meta-learning, the model\nis trained to be able to adapt to a large or inﬁnite num-\nber of tasks.\nSince we would like to apply our frame-\nwork to a variety of learning problems, from classiﬁca-\ntion to reinforcement learning, we introduce a generic\nnotion of a learning task below.\nFormally, each task\nT\n= {L(x1, a1, . . . , xH, aH), q(x1), q(xt+1|xt, at), H}\nconsists of a loss function L, a distribution over initial ob-\nservations q(x1), a transition distribution q(xt+1|xt, at),\nand an episode length H. In i.i.d. supervised learning prob-\nlems, the length H = 1. The model may generate samples\nof length H by choosing an output at at each time t. The\nloss L(x1, a1, . . . , xH, aH) →R, provides task-speciﬁc\nfeedback, which might be in the form of a misclassiﬁcation\nloss or a cost function in a Markov decision process.\nmeta-learning\nlearning/adaptation\nθ\n∇L1\n∇L2\n∇L3\nθ∗\n1\nθ∗\n2\nθ∗\n3\nFigure 1. Diagram of our model-agnostic meta-learning algo-\nrithm (MAML), which optimizes for a representation θ that can\nquickly adapt to new tasks.\nIn our meta-learning scenario, we consider a distribution\nover tasks p(T ) that we want our model to be able to adapt\nto. In the K-shot learning setting, the model is trained to\nlearn a new task Ti drawn from p(T ) from only K samples\ndrawn from qi and feedback LTi generated by Ti. During\nmeta-training, a task Ti is sampled from p(T ), the model\nis trained with K samples and feedback from the corre-\nsponding loss LTi from Ti, and then tested on new samples\nfrom Ti. The model f is then improved by considering how\nthe test error on new data from qi changes with respect to\nthe parameters. In effect, the test error on sampled tasks Ti\nserves as the training error of the meta-learning process. At\nthe end of meta-training, new tasks are sampled from p(T ),\nand meta-performance is measured by the model’s perfor-\nmance after learning from K samples. Generally, tasks\nused for meta-testing are held out during meta-training.\n2.2. A Model-Agnostic Meta-Learning Algorithm\nIn contrast to prior work, which has sought to train re-\ncurrent neural networks that ingest entire datasets (San-\ntoro et al., 2016; Duan et al., 2016b) or feature embed-\ndings that can be combined with nonparametric methods at\ntest time (Vinyals et al., 2016; Koch, 2015), we propose a\nmethod that can learn the parameters of any standard model\nvia meta-learning in such a way as to prepare that model\nfor fast adaptation. The intuition behind this approach is\nthat some internal representations are more transferrable\nthan others. For example, a neural network might learn\ninternal features that are broadly applicable to all tasks in\np(T ), rather than a single individual task. How can we en-\ncourage the emergence of such general-purpose representa-\ntions? We take an explicit approach to this problem: since\nthe model will be ﬁne-tuned using a gradient-based learn-\ning rule on a new task, we will aim to learn a model in such\na way that this gradient-based learning rule can make rapid\nprogress on new tasks drawn from p(T ), without overﬁt-\nting. In effect, we will aim to ﬁnd model parameters that\nare sensitive to changes in the task, such that small changes\nin the parameters will produce large improvements on the\nloss function of any task drawn from p(T ), when altered in\nthe direction of the gradient of that loss (see Figure 1). We\n\n\nModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nAlgorithm 1 Model-Agnostic Meta-Learning\nRequire: p(T ): distribution over tasks\nRequire: α, β: step size hyperparameters\n1: randomly initialize θ\n2: while not done do\n3:\nSample batch of tasks Ti ∼p(T )\n4:\nfor all Ti do\n5:\nEvaluate ∇θLTi(fθ) with respect to K examples\n6:\nCompute adapted parameters with gradient de-\nscent: θ′\ni = θ −α∇θLTi(fθ)\n7:\nend for\n8:\nUpdate θ ←θ −β∇θ\nP\nTi∼p(T ) LTi(fθ′\ni)\n9: end while\nmake no assumption on the form of the model, other than\nto assume that it is parametrized by some parameter vector\nθ, and that the loss function is smooth enough in θ that we\ncan use gradient-based learning techniques.\nFormally,\nwe\nconsider\na\nmodel\nrepresented\nby\na\nparametrized function fθ with parameters θ. When adapt-\ning to a new task Ti, the model’s parameters θ become θ′\ni.\nIn our method, the updated parameter vector θ′\ni is computed\nusing one or more gradient descent updates on task Ti. For\nexample, when using one gradient update,\nθ′\ni = θ −α∇θLTi(fθ).\nThe step size α may be ﬁxed as a hyperparameter or meta-\nlearned. For simplicity of notation, we will consider one\ngradient update for the rest of this section, but using multi-\nple gradient updates is a straightforward extension.\nThe model parameters are trained by optimizing for the per-\nformance of fθ′\ni with respect to θ across tasks sampled from\np(T ). More concretely, the meta-objective is as follows:\nmin\nθ\nX\nTi∼p(T )\nLTi(fθ′\ni) =\nX\nTi∼p(T )\nLTi(fθ−α∇θLTi(fθ))\nNote that the meta-optimization is performed over the\nmodel parameters θ, whereas the objective is computed us-\ning the updated model parameters θ′. In effect, our pro-\nposed method aims to optimize the model parameters such\nthat one or a small number of gradient steps on a new task\nwill produce maximally effective behavior on that task.\nThe meta-optimization across tasks is performed via\nstochastic gradient descent (SGD), such that the model pa-\nrameters θ are updated as follows:\nθ ←θ −β∇θ\nX\nTi∼p(T )\nLTi(fθ′\ni)\n(1)\nwhere β is the meta step size. The full algorithm, in the\ngeneral case, is outlined in Algorithm 1.\nThe MAML meta-gradient update involves a gradient\nthrough a gradient. Computationally, this requires an addi-\ntional backward pass through f to compute Hessian-vector\nproducts, which is supported by standard deep learning li-\nbraries such as TensorFlow (Abadi et al., 2016). In our\nexperiments, we also include a comparison to dropping\nthis backward pass and using a ﬁrst-order approximation,\nwhich we discuss in Section 5.2.\n3. Species of MAML\nIn this section, we discuss speciﬁc instantiations of our\nmeta-learning algorithm for supervised learning and rein-\nforcement learning. The domains differ in the form of loss\nfunction and in how data is generated by the task and pre-\nsented to the model, but the same basic adaptation mecha-\nnism can be applied in both cases.\n3.1. Supervised Regression and Classiﬁcation\nFew-shot learning is well-studied in the domain of super-\nvised tasks, where the goal is to learn a new function from\nonly a few input/output pairs for that task, using prior data\nfrom similar tasks for meta-learning. For example, the goal\nmight be to classify images of a Segway after seeing only\none or a few examples of a Segway, with a model that has\npreviously seen many other types of objects. Likewise, in\nfew-shot regression, the goal is to predict the outputs of\na continuous-valued function from only a few datapoints\nsampled from that function, after training on many func-\ntions with similar statistical properties.\nTo formalize the supervised regression and classiﬁcation\nproblems in the context of the meta-learning deﬁnitions in\nSection 2.1, we can deﬁne the horizon H = 1 and drop the\ntimestep subscript on xt, since the model accepts a single\ninput and produces a single output, rather than a sequence\nof inputs and outputs. The task Ti generates K i.i.d. ob-\nservations x from qi, and the task loss is represented by the\nerror between the model’s output for x and the correspond-\ning target values y for that observation and task.\nTwo common loss functions used for supervised classiﬁca-\ntion and regression are cross-entropy and mean-squared er-\nror (MSE), which we will describe below; though, other su-\npervised loss functions may be used as well. For regression\ntasks using mean-squared error, the loss takes the form:\nLTi(fφ) =\nX\nx(j),y(j)∼Ti\n∥fφ(x(j)) −y(j)∥2\n2,\n(2)\nwhere x(j), y(j) are an input/output pair sampled from task\nTi. In K-shot regression tasks, K input/output pairs are\nprovided for learning for each task.\nSimilarly, for discrete classiﬁcation tasks with a cross-\nentropy loss, the loss takes the form:\nLTi(fφ) =\nX\nx(j),y(j)∼Ti\ny(j) log fφ(x(j))\n+ (1 −y(j)) log(1 −fφ(x(j)))\n(3)\n\n\nModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nAlgorithm 2 MAML for Few-Shot Supervised Learning\nRequire: p(T ): distribution over tasks\nRequire: α, β: step size hyperparameters\n1: randomly initialize θ\n2: while not done do\n3:\nSample batch of tasks Ti ∼p(T )\n4:\nfor all Ti do\n5:\nSample K datapoints D = {x(j), y(j)} from Ti\n6:\nEvaluate ∇θLTi(fθ) using D and LTi in Equation (2)\nor (3)\n7:\nCompute adapted parameters with gradient descent:\nθ′\ni = θ −α∇θLTi(fθ)\n8:\nSample datapoints D′\ni = {x(j), y(j)} from Ti for the\nmeta-update\n9:\nend for\n10:\nUpdate θ ←θ −β∇θ\nP\nTi∼p(T ) LTi(fθ′\ni) using each D′\ni\nand LTi in Equation 2 or 3\n11: end while\nAccording to the conventional terminology, K-shot classi-\nﬁcation tasks use K input/output pairs from each class, for\na total of NK data points for N-way classiﬁcation. Given a\ndistribution over tasks p(Ti), these loss functions can be di-\nrectly inserted into the equations in Section 2.2 to perform\nmeta-learning, as detailed in Algorithm 2.\n3.2. Reinforcement Learning\nIn reinforcement learning (RL), the goal of few-shot meta-\nlearning is to enable an agent to quickly acquire a policy for\na new test task using only a small amount of experience in\nthe test setting. A new task might involve achieving a new\ngoal or succeeding on a previously trained goal in a new\nenvironment. For example, an agent might learn to quickly\nﬁgure out how to navigate mazes so that, when faced with\na new maze, it can determine how to reliably reach the exit\nwith only a few samples. In this section, we will discuss\nhow MAML can be applied to meta-learning for RL.\nEach RL task Ti contains an initial state distribution qi(x1)\nand a transition distribution qi(xt+1|xt, at), and the loss\nLTi corresponds to the (negative) reward function R. The\nentire task is therefore a Markov decision process (MDP)\nwith horizon H, where the learner is allowed to query a\nlimited number of sample trajectories for few-shot learn-\ning. Any aspect of the MDP may change across tasks in\np(T ). The model being learned, fθ, is a policy that maps\nfrom states xt to a distribution over actions at at each\ntimestep t ∈{1, ..., H}. The loss for task Ti and model\nfφ takes the form\nLTi(fφ) = −Ext,at∼fφ,qTi\n\" H\nX\nt=1\nRi(xt, at)\n#\n.\n(4)\nIn K-shot reinforcement learning, K rollouts from fθ and\ntask Ti, (x1, a1, ...xH), and the corresponding rewards\nR(xt, at), may be used for adaptation on a new task Ti.\nAlgorithm 3 MAML for Reinforcement Learning\nRequire: p(T ): distribution over tasks\nRequire: α, β: step size hyperparameters\n1: randomly initialize θ\n2: while not done do\n3:\nSample batch of tasks Ti ∼p(T )\n4:\nfor all Ti do\n5:\nSample K trajectories D = {(x1, a1, ...xH)} using fθ\nin Ti\n6:\nEvaluate ∇θLTi(fθ) using D and LTi in Equation 4\n7:\nCompute adapted parameters with gradient descent:\nθ′\ni = θ −α∇θLTi(fθ)\n8:\nSample trajectories D′\ni = {(x1, a1, ...xH)} using fθ′\ni\nin Ti\n9:\nend for\n10:\nUpdate θ ←θ −β∇θ\nP\nTi∼p(T ) LTi(fθ′\ni) using each D′\ni\nand LTi in Equation 4\n11: end while\nSince the expected reward is generally not differentiable\ndue to unknown dynamics, we use policy gradient meth-\nods to estimate the gradient both for the model gradient\nupdate(s) and the meta-optimization. Since policy gradi-\nents are an on-policy algorithm, each additional gradient\nstep during the adaptation of fθ requires new samples from\nthe current policy fθi′. We detail the algorithm in Algo-\nrithm 3. This algorithm has the same structure as Algo-\nrithm 2, with the principal difference being that steps 5 and\n8 require sampling trajectories from the environment cor-\nresponding to task Ti. Practical implementations of this\nmethod may also use a variety of improvements recently\nproposed for policy gradient algorithms, including state\nor action-dependent baselines and trust regions (Schulman\net al., 2015).\n4. Related Work\nThe method that we propose in this paper addresses the\ngeneral problem of meta-learning (Thrun & Pratt, 1998;\nSchmidhuber, 1987; Naik & Mammone, 1992), which in-\ncludes few-shot learning. A popular approach for meta-\nlearning is to train a meta-learner that learns how to up-\ndate the parameters of the learner’s model (Bengio et al.,\n1992; Schmidhuber, 1992; Bengio et al., 1990). This ap-\nproach has been applied to learning to optimize deep net-\nworks (Hochreiter et al., 2001; Andrychowicz et al., 2016;\nLi & Malik, 2017), as well as for learning dynamically\nchanging recurrent networks (Ha et al., 2017). One recent\napproach learns both the weight initialization and the opti-\nmizer, for few-shot image recognition (Ravi & Larochelle,\n2017). Unlike these methods, the MAML learner’s weights\nare updated using the gradient, rather than a learned update;\nour method does not introduce additional parameters for\nmeta-learning nor require a particular learner architecture.\nFew-shot learning methods have also been developed for\n\n\nModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nspeciﬁc tasks such as generative modeling (Edwards &\nStorkey, 2017; Rezende et al., 2016) and image recogni-\ntion (Vinyals et al., 2016). One successful approach for\nfew-shot classiﬁcation is to learn to compare new exam-\nples in a learned metric space using e.g.\nSiamese net-\nworks (Koch, 2015) or recurrence with attention mech-\nanisms (Vinyals et al., 2016; Shyam et al., 2017; Snell\net al., 2017). These approaches have generated some of the\nmost successful results, but are difﬁcult to directly extend\nto other problems, such as reinforcement learning.\nOur\nmethod, in contrast, is agnostic to the form of the model\nand to the particular learning task.\nAnother approach to meta-learning is to train memory-\naugmented models on many tasks, where the recurrent\nlearner is trained to adapt to new tasks as it is rolled out.\nSuch networks have been applied to few-shot image recog-\nnition (Santoro et al., 2016; Munkhdalai & Yu, 2017) and\nlearning “fast” reinforcement learning agents (Duan et al.,\n2016b; Wang et al., 2016).\nOur experiments show that\nour method outperforms the recurrent approach on few-\nshot classiﬁcation. Furthermore, unlike these methods, our\napproach simply provides a good weight initialization and\nuses the same gradient descent update for both the learner\nand meta-update. As a result, it is straightforward to ﬁne-\ntune the learner for additional gradient steps.\nOur approach is also related to methods for initialization of\ndeep networks. In computer vision, models pretrained on\nlarge-scale image classiﬁcation have been shown to learn\neffective features for a range of problems (Donahue et al.,\n2014).\nIn contrast, our method explicitly optimizes the\nmodel for fast adaptability, allowing it to adapt to new tasks\nwith only a few examples. Our method can also be viewed\nas explicitly maximizing sensitivity of new task losses to\nthe model parameters. A number of prior works have ex-\nplored sensitivity in deep networks, often in the context of\ninitialization (Saxe et al., 2014; Kirkpatrick et al., 2016).\nMost of these works have considered good random initial-\nizations, though a number of papers have addressed data-\ndependent initializers (Kr¨ahenb¨uhl et al., 2016; Salimans &\nKingma, 2016), including learned initializations (Husken\n& Goerick, 2000; Maclaurin et al., 2015). In contrast, our\nmethod explicitly trains the parameters for sensitivity on\na given task distribution, allowing for extremely efﬁcient\nadaptation for problems such as K-shot learning and rapid\nreinforcement learning in only one or a few gradient steps.\n5. Experimental Evaluation\nThe goal of our experimental evaluation is to answer the\nfollowing questions: (1) Can MAML enable fast learning\nof new tasks? (2) Can MAML be used for meta-learning\nin multiple different domains, including supervised regres-\nsion, classiﬁcation, and reinforcement learning? (3) Can a\nmodel learned with MAML continue to improve with addi-\ntional gradient updates and/or examples?\nAll of the meta-learning problems that we consider require\nsome amount of adaptation to new tasks at test-time. When\npossible, we compare our results to an oracle that receives\nthe identity of the task (which is a problem-dependent rep-\nresentation) as an additional input, as an upper bound on\nthe performance of the model. All of the experiments were\nperformed using TensorFlow (Abadi et al., 2016), which al-\nlows for automatic differentiation through the gradient up-\ndate(s) during meta-learning. The code is available online1.\n5.1. Regression\nWe start with a simple regression problem that illustrates\nthe basic principles of MAML. Each task involves regress-\ning from the input to the output of a sine wave, where the\namplitude and phase of the sinusoid are varied between\ntasks.\nThus, p(T ) is continuous, where the amplitude\nvaries within [0.1, 5.0] and the phase varies within [0, π],\nand the input and output both have a dimensionality of 1.\nDuring training and testing, datapoints x are sampled uni-\nformly from [−5.0, 5.0]. The loss is the mean-squared error\nbetween the prediction f(x) and true value. The regres-\nsor is a neural network model with 2 hidden layers of size\n40 with ReLU nonlinearities. When training with MAML,\nwe use one gradient update with K = 10 examples with\na ﬁxed step size α = 0.01, and use Adam as the meta-\noptimizer (Kingma & Ba, 2015). The baselines are like-\nwise trained with Adam. To evaluate performance, we ﬁne-\ntune a single meta-learned model on varying numbers of K\nexamples, and compare performance to two baselines: (a)\npretraining on all of the tasks, which entails training a net-\nwork to regress to random sinusoid functions and then, at\ntest-time, ﬁne-tuning with gradient descent on the K pro-\nvided points, using an automatically tuned step size, and\n(b) an oracle which receives the true amplitude and phase\nas input. In Appendix C, we show comparisons to addi-\ntional multi-task and adaptation methods.\nWe evaluate performance by ﬁne-tuning the model learned\nby MAML and the pretrained model on K = {5, 10, 20}\ndatapoints. During ﬁne-tuning, each gradient step is com-\nputed using the same K datapoints. The qualitative results,\nshown in Figure 2 and further expanded on in Appendix B\nshow that the learned model is able to quickly adapt with\nonly 5 datapoints, shown as purple triangles, whereas the\nmodel that is pretrained using standard supervised learning\non all tasks is unable to adequately adapt with so few dat-\napoints without catastrophic overﬁtting. Crucially, when\nthe K datapoints are all in one half of the input range, the\n1Code for the regression and supervised experiments is at\ngithub.com/cbfinn/maml and code for the RL experi-\nments is at github.com/cbfinn/maml_rl\n\n\nModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nFigure 2. Few-shot adaptation for the simple regression task. Left: Note that MAML is able to estimate parts of the curve where there are\nno datapoints, indicating that the model has learned about the periodic structure of sine waves. Right: Fine-tuning of a model pretrained\non the same distribution of tasks without MAML, with a tuned step size. Due to the often contradictory outputs on the pre-training tasks,\nthis model is unable to recover a suitable representation and fails to extrapolate from the small number of test-time samples.\nFigure 3. Quantitative sinusoid regression results showing the\nlearning curve at meta test-time. Note that MAML continues to\nimprove with additional gradient steps without overﬁtting to the\nextremely small dataset during meta-testing, achieving a loss that\nis substantially lower than the baseline ﬁne-tuning approach.\nmodel trained with MAML can still infer the amplitude and\nphase in the other half of the range, demonstrating that the\nMAML trained model f has learned to model the periodic\nnature of the sine wave. Furthermore, we observe both in\nthe qualitative and quantitative results (Figure 3 and Ap-\npendix B) that the model learned with MAML continues\nto improve with additional gradient steps, despite being\ntrained for maximal performance after one gradient step.\nThis improvement suggests that MAML optimizes the pa-\nrameters such that they lie in a region that is amenable to\nfast adaptation and is sensitive to loss functions from p(T ),\nas discussed in Section 2.2, rather than overﬁtting to pa-\nrameters θ that only improve after one step.\n5.2. Classiﬁcation\nTo evaluate MAML in comparison to prior meta-learning\nand few-shot learning algorithms, we applied our method\nto few-shot image recognition on the Omniglot (Lake et al.,\n2011) and MiniImagenet datasets. The Omniglot dataset\nconsists of 20 instances of 1623 characters from 50 dif-\nferent alphabets. Each instance was drawn by a different\nperson. The MiniImagenet dataset was proposed by Ravi\n& Larochelle (2017), and involves 64 training classes, 12\nvalidation classes, and 24 test classes. The Omniglot and\nMiniImagenet image recognition tasks are the most com-\nmon recently used few-shot learning benchmarks (Vinyals\net al., 2016; Santoro et al., 2016; Ravi & Larochelle, 2017).\nWe follow the experimental protocol proposed by Vinyals\net al. (2016), which involves fast learning of N-way clas-\nsiﬁcation with 1 or 5 shots. The problem of N-way classi-\nﬁcation is set up as follows: select N unseen classes, pro-\nvide the model with K different instances of each of the N\nclasses, and evaluate the model’s ability to classify new in-\nstances within the N classes. For Omniglot, we randomly\nselect 1200 characters for training, irrespective of alphabet,\nand use the remaining for testing. The Omniglot dataset is\naugmented with rotations by multiples of 90 degrees, as\nproposed by Santoro et al. (2016).\nOur model follows the same architecture as the embedding\nfunction used by Vinyals et al. (2016), which has 4 mod-\nules with a 3 × 3 convolutions and 64 ﬁlters, followed by\nbatch normalization (Ioffe & Szegedy, 2015), a ReLU non-\nlinearity, and 2 × 2 max-pooling. The Omniglot images\nare downsampled to 28 × 28, so the dimensionality of the\nlast hidden layer is 64. As in the baseline classiﬁer used\nby Vinyals et al. (2016), the last layer is fed into a soft-\nmax. For Omniglot, we used strided convolutions instead\nof max-pooling. For MiniImagenet, we used 32 ﬁlters per\nlayer to reduce overﬁtting, as done by (Ravi & Larochelle,\n2017). In order to also provide a fair comparison against\nmemory-augmented neural networks (Santoro et al., 2016)\nand to test the ﬂexibility of MAML, we also provide re-\nsults for a non-convolutional network. For this, we use a\nnetwork with 4 hidden layers with sizes 256, 128, 64, 64,\neach including batch normalization and ReLU nonlineari-\nties, followed by a linear layer and softmax. For all models,\nthe loss function is the cross-entropy error between the pre-\ndicted and true class. Additional hyperparameter details are\nincluded in Appendix A.1.\nWe present the results in Table 1. The convolutional model\nlearned by MAML compares well to the state-of-the-art re-\nsults on this task, narrowly outperforming the prior meth-\nods. Some of these existing methods, such as matching\nnetworks, Siamese networks, and memory models are de-\nsigned with few-shot classiﬁcation in mind, and are not\nreadily applicable to domains such as reinforcement learn-\ning.\nAdditionally, the model learned with MAML uses\n\n\nModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nTable 1. Few-shot classiﬁcation on held-out Omniglot characters (top) and the MiniImagenet test set (bottom). MAML achieves results\nthat are comparable to or outperform state-of-the-art convolutional and recurrent models. Siamese nets, matching nets, and the memory\nmodule approaches are all speciﬁc to classiﬁcation, and are not directly applicable to regression or RL scenarios. The ± shows 95%\nconﬁdence intervals over tasks. Note that the Omniglot results may not be strictly comparable since the train/test splits used in the prior\nwork were not available. The MiniImagenet evaluation of baseline methods and matching networks is from Ravi & Larochelle (2017).\n5-way Accuracy\n20-way Accuracy\nOmniglot (Lake et al., 2011)\n1-shot\n5-shot\n1-shot\n5-shot\nMANN, no conv (Santoro et al., 2016)\n82.8%\n94.9%\n–\n–\nMAML, no conv (ours)\n89.7 ± 1.1%\n97.5 ± 0.6%\n–\n–\nSiamese nets (Koch, 2015)\n97.3%\n98.4%\n88.2%\n97.0%\nmatching nets (Vinyals et al., 2016)\n98.1%\n98.9%\n93.8%\n98.5%\nneural statistician (Edwards & Storkey, 2017)\n98.1%\n99.5%\n93.2%\n98.1%\nmemory mod. (Kaiser et al., 2017)\n98.4%\n99.6%\n95.0%\n98.6%\nMAML (ours)\n98.7 ± 0.4%\n99.9 ± 0.1%\n95.8 ± 0.3%\n98.9 ± 0.2%\n5-way Accuracy\nMiniImagenet (Ravi & Larochelle, 2017)\n1-shot\n5-shot\nﬁne-tuning baseline\n28.86 ± 0.54%\n49.79 ± 0.79%\nnearest neighbor baseline\n41.08 ± 0.70%\n51.04 ± 0.65%\nmatching nets (Vinyals et al., 2016)\n43.56 ± 0.84%\n55.31 ± 0.73%\nmeta-learner LSTM (Ravi & Larochelle, 2017)\n43.44 ± 0.77%\n60.60 ± 0.71%\nMAML, ﬁrst order approx. (ours)\n48.07 ± 1.75%\n63.15 ± 0.91%\nMAML (ours)\n48.70 ± 1.84%\n63.11 ± 0.92%\nfewer overall parameters compared to matching networks\nand the meta-learner LSTM, since the algorithm does not\nintroduce any additional parameters beyond the weights\nof the classiﬁer itself. Compared to these prior methods,\nmemory-augmented neural networks (Santoro et al., 2016)\nspeciﬁcally, and recurrent meta-learning models in gen-\neral, represent a more broadly applicable class of meth-\nods that, like MAML, can be used for other tasks such as\nreinforcement learning (Duan et al., 2016b; Wang et al.,\n2016). However, as shown in the comparison, MAML sig-\nniﬁcantly outperforms memory-augmented networks and\nthe meta-learner LSTM on 5-way Omniglot and MiniIm-\nagenet classiﬁcation, both in the 1-shot and 5-shot case.\nA signiﬁcant computational expense in MAML comes\nfrom the use of second derivatives when backpropagat-\ning the meta-gradient through the gradient operator in\nthe meta-objective (see Equation (1)). On MiniImagenet,\nwe show a comparison to a ﬁrst-order approximation of\nMAML, where these second derivatives are omitted. Note\nthat the resulting method still computes the meta-gradient\nat the post-update parameter values θ′\ni, which provides for\neffective meta-learning. Surprisingly however, the perfor-\nmance of this method is nearly the same as that obtained\nwith full second derivatives, suggesting that most of the\nimprovement in MAML comes from the gradients of the\nobjective at the post-update parameter values, rather than\nthe second order updates from differentiating through the\ngradient update. Past work has observed that ReLU neu-\nral networks are locally almost linear (Goodfellow et al.,\n2015), which suggests that second derivatives may be close\nto zero in most cases, partially explaining the good perfor-\nFigure 4. Top: quantitative results from 2D navigation task, Bot-\ntom: qualitative comparison between model learned with MAML\nand with ﬁne-tuning from a pretrained network.\nmance of the ﬁrst-order approximation. This approxima-\ntion removes the need for computing Hessian-vector prod-\nucts in an additional backward pass, which we found led to\nroughly 33% speed-up in network computation.\n5.3. Reinforcement Learning\nTo evaluate MAML on reinforcement learning problems,\nwe constructed several sets of tasks based off of the sim-\nulated continuous control environments in the rllab bench-\nmark suite (Duan et al., 2016a). We discuss the individual\ndomains below. In all of the domains, the model trained\nby MAML is a neural network policy with two hidden lay-\ners of size 100, with ReLU nonlinearities. The gradient\nupdates are computed using vanilla policy gradient (RE-\nINFORCE) (Williams, 1992), and we use trust-region pol-\nicy optimization (TRPO) as the meta-optimizer (Schulman\net al., 2015). In order to avoid computing third derivatives,\n\n\nModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nFigure 5. Reinforcement learning results for the half-cheetah and ant locomotion tasks, with the tasks shown on the far right. Each\ngradient step requires additional samples from the environment, unlike the supervised learning tasks. The results show that MAML can\nadapt to new goal velocities and directions substantially faster than conventional pretraining or random initialization, achieving good\nperforms in just two or three gradient steps. We exclude the goal velocity, random baseline curves, since the returns are much worse\n(< −200 for cheetah and < −25 for ant).\nwe use ﬁnite differences to compute the Hessian-vector\nproducts for TRPO. For both learning and meta-learning\nupdates, we use the standard linear feature baseline pro-\nposed by Duan et al. (2016a), which is ﬁtted separately at\neach iteration for each sampled task in the batch. We com-\npare to three baseline models: (a) pretraining one policy on\nall of the tasks and then ﬁne-tuning, (b) training a policy\nfrom randomly initialized weights, and (c) an oracle policy\nwhich receives the parameters of the task as input, which\nfor the tasks below corresponds to a goal position, goal di-\nrection, or goal velocity for the agent. The baseline models\nof (a) and (b) are ﬁne-tuned with gradient descent with a\nmanually tuned step size. Videos of the learned policies\ncan be viewed at sites.google.com/view/maml\n2D Navigation. In our ﬁrst meta-RL experiment, we study\na set of tasks where a point agent must move to different\ngoal positions in 2D, randomly chosen for each task within\na unit square. The observation is the current 2D position,\nand actions correspond to velocity commands clipped to be\nin the range [−0.1, 0.1]. The reward is the negative squared\ndistance to the goal, and episodes terminate when the agent\nis within 0.01 of the goal or at the horizon of H = 100. The\npolicy was trained with MAML to maximize performance\nafter 1 policy gradient update using 20 trajectories. Ad-\nditional hyperparameter settings for this problem and the\nfollowing RL problems are in Appendix A.2. In our evalu-\nation, we compare adaptation to a new task with up to 4 gra-\ndient updates, each with 40 samples. The results in Figure 4\nshow the adaptation performance of models that are initial-\nized with MAML, conventional pretraining on the same set\nof tasks, random initialization, and an oracle policy that\nreceives the goal position as input. The results show that\nMAML can learn a model that adapts much more quickly\nin a single gradient update, and furthermore continues to\nimprove with additional updates.\nLocomotion. To study how well MAML can scale to more\ncomplex deep RL problems, we also study adaptation on\nhigh-dimensional locomotion tasks with the MuJoCo sim-\nulator (Todorov et al., 2012). The tasks require two sim-\nulated robots – a planar cheetah and a 3D quadruped (the\n“ant”) – to run in a particular direction or at a particular\nvelocity. In the goal velocity experiments, the reward is\nthe negative absolute value between the current velocity of\nthe agent and a goal, which is chosen uniformly at random\nbetween 0.0 and 2.0 for the cheetah and between 0.0 and\n3.0 for the ant. In the goal direction experiments, the re-\nward is the magnitude of the velocity in either the forward\nor backward direction, chosen at random for each task in\np(T ). The horizon is H = 200, with 20 rollouts per gradi-\nent step for all problems except the ant forward/backward\ntask, which used 40 rollouts per step. The results in Fig-\nure 5 show that MAML learns a model that can quickly\nadapt its velocity and direction with even just a single gra-\ndient update, and continues to improve with more gradi-\nent steps. The results also show that, on these challenging\ntasks, the MAML initialization substantially outperforms\nrandom initialization and pretraining. In fact, pretraining\nis in some cases worse than random initialization, a fact\nobserved in prior RL work (Parisotto et al., 2016).\n6. Discussion and Future Work\nWe introduced a meta-learning method based on learning\neasily adaptable model parameters through gradient de-\nscent. Our approach has a number of beneﬁts. It is simple\nand does not introduce any learned parameters for meta-\nlearning. It can be combined with any model representation\nthat is amenable to gradient-based training, and any differ-\nentiable objective, including classiﬁcation, regression, and\nreinforcement learning. Lastly, since our method merely\nproduces a weight initialization, adaptation can be per-\nformed with any amount of data and any number of gra-\ndient steps, though we demonstrate state-of-the-art results\non classiﬁcation with only one or ﬁve examples per class.\nWe also show that our method can adapt an RL agent using\npolicy gradients and a very modest amount of experience.\nReusing knowledge from past tasks may be a crucial in-\ngredient in making high-capacity scalable models, such as\ndeep neural networks, amenable to fast training with small\ndatasets. We believe that this work is one step toward a sim-\nple and general-purpose meta-learning technique that can\nbe applied to any problem and any model. Further research\nin this area can make multitask initialization a standard in-\ngredient in deep learning and reinforcement learning.\n\n\nModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nAcknowledgements\nThe authors would like to thank Xi Chen and Trevor Darrell\nfor helpful discussions, Yan Duan and Alex Lee for techni-\ncal advice, Nikhil Mishra, Haoran Tang, and Greg Kahn for\nfeedback on an early draft of the paper, and the anonymous\nreviewers for their comments. This work was supported in\npart by an ONR PECASE award and an NSF GRFP award.\nReferences\nAbadi, Mart´ın, Agarwal, Ashish, Barham, Paul, Brevdo,\nEugene, Chen, Zhifeng, Citro, Craig, Corrado, Greg S,\nDavis, Andy, Dean, Jeffrey, Devin, Matthieu, et al. Ten-\nsorﬂow: Large-scale machine learning on heterogeneous\ndistributed systems. arXiv preprint arXiv:1603.04467,\n2016.\nAndrychowicz, Marcin, Denil, Misha, Gomez, Sergio,\nHoffman, Matthew W, Pfau, David, Schaul, Tom, and\nde Freitas, Nando. Learning to learn by gradient descent\nby gradient descent. In Neural Information Processing\nSystems (NIPS), 2016.\nBengio, Samy, Bengio, Yoshua, Cloutier, Jocelyn, and\nGecsei, Jan. On the optimization of a synaptic learning\nrule. In Optimality in Artiﬁcial and Biological Neural\nNetworks, pp. 6–8, 1992.\nBengio, Yoshua, Bengio, Samy, and Cloutier, Jocelyn.\nLearning a synaptic learning rule.\nUniversit´e de\nMontr´eal, D´epartement d’informatique et de recherche\nop´erationnelle, 1990.\nDonahue, Jeff, Jia, Yangqing, Vinyals, Oriol, Hoffman,\nJudy, Zhang, Ning, Tzeng, Eric, and Darrell, Trevor. De-\ncaf: A deep convolutional activation feature for generic\nvisual recognition. In International Conference on Ma-\nchine Learning (ICML), 2014.\nDuan, Yan, Chen, Xi, Houthooft, Rein, Schulman, John,\nand Abbeel, Pieter. Benchmarking deep reinforcement\nlearning for continuous control. In International Con-\nference on Machine Learning (ICML), 2016a.\nDuan, Yan, Schulman, John, Chen, Xi, Bartlett, Peter L,\nSutskever, Ilya, and Abbeel, Pieter. Rl2: Fast reinforce-\nment learning via slow reinforcement learning.\narXiv\npreprint arXiv:1611.02779, 2016b.\nEdwards, Harrison and Storkey, Amos. Towards a neural\nstatistician. International Conference on Learning Rep-\nresentations (ICLR), 2017.\nGoodfellow, Ian J, Shlens, Jonathon, and Szegedy, Chris-\ntian. Explaining and harnessing adversarial examples.\nInternational Conference on Learning Representations\n(ICLR), 2015.\nHa, David, Dai, Andrew, and Le, Quoc V. Hypernetworks.\nInternational Conference on Learning Representations\n(ICLR), 2017.\nHochreiter, Sepp, Younger, A Steven, and Conwell, Pe-\nter R.\nLearning to learn using gradient descent.\nIn\nInternational Conference on Artiﬁcial Neural Networks.\nSpringer, 2001.\nHusken, Michael and Goerick, Christian. Fast learning for\nproblem classes using knowledge based network initial-\nization. In Neural Networks, 2000. IJCNN 2000, Pro-\nceedings of the IEEE-INNS-ENNS International Joint\nConference on, volume 6, pp. 619–624. IEEE, 2000.\nIoffe, Sergey and Szegedy, Christian. Batch normalization:\nAccelerating deep network training by reducing internal\ncovariate shift.\nInternational Conference on Machine\nLearning (ICML), 2015.\nKaiser, Lukasz, Nachum, Oﬁr, Roy, Aurko, and Bengio,\nSamy. Learning to remember rare events. International\nConference on Learning Representations (ICLR), 2017.\nKingma, Diederik and Ba, Jimmy. Adam: A method for\nstochastic optimization.\nInternational Conference on\nLearning Representations (ICLR), 2015.\nKirkpatrick, James, Pascanu, Razvan, Rabinowitz, Neil,\nVeness, Joel, Desjardins, Guillaume, Rusu, Andrei A,\nMilan, Kieran, Quan, John, Ramalho, Tiago, Grabska-\nBarwinska, Agnieszka, et al.\nOvercoming catas-\ntrophic forgetting in neural networks.\narXiv preprint\narXiv:1612.00796, 2016.\nKoch, Gregory. Siamese neural networks for one-shot im-\nage recognition. ICML Deep Learning Workshop, 2015.\nKr¨ahenb¨uhl, Philipp, Doersch, Carl, Donahue, Jeff, and\nDarrell, Trevor. Data-dependent initializations of con-\nvolutional neural networks. International Conference on\nLearning Representations (ICLR), 2016.\nLake, Brenden M, Salakhutdinov, Ruslan, Gross, Jason,\nand Tenenbaum, Joshua B. One shot learning of simple\nvisual concepts. In Conference of the Cognitive Science\nSociety (CogSci), 2011.\nLi, Ke and Malik, Jitendra. Learning to optimize. Interna-\ntional Conference on Learning Representations (ICLR),\n2017.\nMaclaurin, Dougal, Duvenaud, David, and Adams, Ryan.\nGradient-based hyperparameter optimization through re-\nversible learning. In International Conference on Ma-\nchine Learning (ICML), 2015.\n\n\nModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nMunkhdalai, Tsendsuren and Yu, Hong.\nMeta net-\nworks. International Conferecence on Machine Learn-\ning (ICML), 2017.\nNaik, Devang K and Mammone, RJ. Meta-neural networks\nthat learn by learning. In International Joint Conference\non Neural Netowrks (IJCNN), 1992.\nParisotto, Emilio, Ba, Jimmy Lei, and Salakhutdinov, Rus-\nlan. Actor-mimic: Deep multitask and transfer reinforce-\nment learning.\nInternational Conference on Learning\nRepresentations (ICLR), 2016.\nRavi, Sachin and Larochelle, Hugo.\nOptimization as a\nmodel for few-shot learning. In International Confer-\nence on Learning Representations (ICLR), 2017.\nRei,\nMarek.\nOnline representation learning in re-\ncurrent neural language models.\narXiv preprint\narXiv:1508.03854, 2015.\nRezende, Danilo Jimenez, Mohamed, Shakir, Danihelka,\nIvo, Gregor, Karol, and Wierstra, Daan. One-shot gener-\nalization in deep generative models. International Con-\nference on Machine Learning (ICML), 2016.\nSalimans, Tim and Kingma, Diederik P. Weight normaliza-\ntion: A simple reparameterization to accelerate training\nof deep neural networks. In Neural Information Process-\ning Systems (NIPS), 2016.\nSantoro, Adam, Bartunov, Sergey, Botvinick, Matthew,\nWierstra, Daan, and Lillicrap, Timothy. Meta-learning\nwith memory-augmented neural networks. In Interna-\ntional Conference on Machine Learning (ICML), 2016.\nSaxe, Andrew, McClelland, James, and Ganguli, Surya.\nExact solutions to the nonlinear dynamics of learning in\ndeep linear neural networks. International Conference\non Learning Representations (ICLR), 2014.\nSchmidhuber, Jurgen.\nEvolutionary principles in self-\nreferential learning.\nOn learning how to learn: The\nmeta-meta-... hook.) Diploma thesis, Institut f. Infor-\nmatik, Tech. Univ. Munich, 1987.\nSchmidhuber, J¨urgen.\nLearning to control fast-weight\nmemories:\nAn alternative to dynamic recurrent net-\nworks. Neural Computation, 1992.\nSchulman, John, Levine, Sergey, Abbeel, Pieter, Jordan,\nMichael I, and Moritz, Philipp.\nTrust region policy\noptimization. In International Conference on Machine\nLearning (ICML), 2015.\nShyam, Pranav, Gupta, Shubham, and Dukkipati, Ambed-\nkar. Attentive recurrent comparators. International Con-\nferecence on Machine Learning (ICML), 2017.\nSnell, Jake, Swersky, Kevin, and Zemel, Richard S. Pro-\ntotypical networks for few-shot learning. arXiv preprint\narXiv:1703.05175, 2017.\nThrun, Sebastian and Pratt, Lorien.\nLearning to learn.\nSpringer Science & Business Media, 1998.\nTodorov, Emanuel, Erez, Tom, and Tassa, Yuval. Mujoco:\nA physics engine for model-based control.\nIn Inter-\nnational Conference on Intelligent Robots and Systems\n(IROS), 2012.\nVinyals, Oriol, Blundell, Charles, Lillicrap, Tim, Wierstra,\nDaan, et al. Matching networks for one shot learning. In\nNeural Information Processing Systems (NIPS), 2016.\nWang, Jane X, Kurth-Nelson, Zeb, Tirumala, Dhruva,\nSoyer, Hubert, Leibo, Joel Z, Munos, Remi, Blun-\ndell, Charles, Kumaran, Dharshan, and Botvinick,\nMatt. Learning to reinforcement learn. arXiv preprint\narXiv:1611.05763, 2016.\nWilliams, Ronald J. Simple statistical gradient-following\nalgorithms for connectionist reinforcement learning.\nMachine learning, 8(3-4):229–256, 1992.\n\n\nModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nA. Additional Experiment Details\nIn this section, we provide additional details of the experi-\nmental set-up and hyperparameters.\nA.1. Classiﬁcation\nFor N-way, K-shot classiﬁcation, each gradient is com-\nputed using a batch size of NK examples. For Omniglot,\nthe 5-way convolutional and non-convolutional MAML\nmodels were each trained with 1 gradient step with step size\nα = 0.4 and a meta batch-size of 32 tasks. The network\nwas evaluated using 3 gradient steps with the same step\nsize α = 0.4. The 20-way convolutional MAML model\nwas trained and evaluated with 5 gradient steps with step\nsize α = 0.1. During training, the meta batch-size was set\nto 16 tasks. For MiniImagenet, both models were trained\nusing 5 gradient steps of size α = 0.01, and evaluated using\n10 gradient steps at test time. Following Ravi & Larochelle\n(2017), 15 examples per class were used for evaluating the\npost-update meta-gradient. We used a meta batch-size of\n4 and 2 tasks for 1-shot and 5-shot training respectively.\nAll models were trained for 60000 iterations on a single\nNVIDIA Pascal Titan X GPU.\nA.2. Reinforcement Learning\nIn all reinforcement learning experiments, the MAML pol-\nicy was trained using a single gradient step with α = 0.1.\nDuring evaluation, we found that halving the learning rate\nafter the ﬁrst gradient step produced superior performance.\nThus, the step size during adaptation was set to α = 0.1\nfor the ﬁrst step, and α = 0.05 for all future steps. The\nstep sizes for the baseline methods were manually tuned for\neach domain. In the 2D navigation, we used a meta batch\nsize of 20; in the locomotion problems, we used a meta\nbatch size of 40 tasks. The MAML models were trained\nfor up to 500 meta-iterations, and the model with the best\naverage return during training was used for evaluation. For\nthe ant goal velocity task, we added a positive reward bonus\nat each timestep to prevent the ant from ending the episode.\nB. Additional Sinusoid Results\nIn Figure 6, we show the full quantitative results of the\nMAML model trained on 10-shot learning and evaluated\non 5-shot, 10-shot, and 20-shot. In Figure 7, we show the\nqualitative performance of MAML and the pretrained base-\nline on randomly sampled sinusoids.\nC. Additional Comparisons\nIn this section, we include more thorough evaluations of\nour approach, including additional multi-task baselines and\na comparison representative of the approach of Rei (2015).\nC.1. Multi-task baselines\nThe pretraining baseline in the main text trained a single\nnetwork on all tasks, which we referred to as “pretraining\non all tasks”. To evaluate the model, as with MAML, we\nﬁne-tuned this model on each test task using K examples.\nIn the domains that we study, different tasks involve dif-\nferent output values for the same input. As a result, by\npre-training on all tasks, the model would learn to output\nthe average output for a particular input value. In some in-\nstances, this model may learn very little about the actual\ndomain, and instead learn about the range of the output\nspace.\nWe experimented with a multi-task method to provide a\npoint of comparison, where instead of averaging in the out-\nput space, we averaged in the parameter space. To achieve\naveraging in parameter space, we sequentially trained 500\nseparate models on 500 tasks drawn from p(T ).\nEach\nmodel was initialized randomly and trained on a large\namount of data from its assigned task. We then took the\naverage parameter vector across models and ﬁne-tuned on\n5 datapoints with a tuned step size. All of our experiments\nfor this method were on the sinusoid task because of com-\nputational requirements. The error of the individual regres-\nsors was low: less than 0.02 on their respective sine waves.\nWe tried three variants of this set-up. During training of\nthe individual regressors, we tried using one of the fol-\nlowing: no regularization, standard ℓ2 weight decay, and\nℓ2 weight regularization to the mean parameter vector thus\nfar of the trained regressors. The latter two variants en-\ncourage the individual models to ﬁnd parsimonious solu-\ntions. When using regularization, we set the magnitude of\nthe regularization to be as high as possible without signif-\nicantly deterring performance. In our results, we refer to\nthis approach as “multi-task”. As seen in the results in Ta-\nble 2, we ﬁnd averaging in the parameter space (multi-task)\nperformed worse than averaging in the output space (pre-\ntraining on all tasks). This suggests that it is difﬁcult to\nﬁnd parsimonious solutions to multiple tasks when training\non tasks separately, and that MAML is learning a solution\nthat is more sophisticated than the mean optimal parameter\nvector.\nC.2. Context vector adaptation\nRei (2015) developed a method which learns a context vec-\ntor that can be adapted online, with an application to re-\ncurrent language models. The parameters in this context\nvector are learned and adapted in the same way as the pa-\nrameters in the MAML model. To provide a comparison\nto using such a context vector for meta-learning problems,\nwe concatenated a set of free parameters z to the input x,\nand only allowed the gradient steps to modify z, rather than\nmodifying the model parameters θ, as in MAML. For im-\n\n\nModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nFigure 6. Quantitative sinusoid regression results showing test-time learning curves with varying numbers of K test-time samples. Each\ngradient step is computed using the same K examples. Note that MAML continues to improve with additional gradient steps without\noverﬁtting to the extremely small dataset during meta-testing, and achieves a loss that is substantially lower than the baseline ﬁne-tuning\napproach.\nTable 2. Additional multi-task baselines on the sinusoid regres-\nsion domain, showing 5-shot mean squared error. The results sug-\ngest that MAML is learning a solution more sophisticated than the\nmean optimal parameter vector.\nnum. grad steps\n1\n5\n10\nmulti-task, no reg\n4.19\n3.85\n3.69\nmulti-task, l2 reg\n7.18\n5.69\n5.60\nmulti-task, reg to mean θ\n2.91\n2.72\n2.71\npretrain on all tasks\n2.41\n2.23\n2.19\nMAML (ours)\n0.67\n0.38\n0.35\nTable 3. 5-way Omniglot Classiﬁcation\n1-shot\n5-shot\ncontext vector\n94.9 ± 0.9%\n97.7 ± 0.3%\nMAML\n98.7 ± 0.4%\n99.9 ± 0.1%\nage inputs, z was concatenated channel-wise with the input\nimage. We ran this method on Omniglot and two RL do-\nmains following the same experimental protocol. We report\nthe results in Tables 3, 4, and 5. Learning an adaptable con-\ntext vector performed well on the toy pointmass problem,\nbut sub-par on more difﬁcult problems, likely due to a less\nﬂexible meta-optimization.\nTable 4. 2D Pointmass, average return\nnum. grad steps\n0\n1\n2\n3\ncontext vector\n−42.42\n−13.90\n−5.17\n−3.18\nMAML (ours)\n−40.41\n−11.68\n−3.33\n−3.23\nTable 5. Half-cheetah forward/backward, average return\nnum. grad steps\n0\n1\n2\n3\ncontext vector\n−40.49\n−44.08\n−38.27\n−42.50\nMAML (ours)\n−50.69\n293.19\n313.48\n315.65\n\n\nModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nFigure 7. A random sample of qualitative results from the sinusoid regression task.\n"
}