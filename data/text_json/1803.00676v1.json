{
  "filename": "1803.00676v1.pdf",
  "num_pages": 15,
  "pages": [
    "Published as a conference paper at ICLR 2018\nMETA-LEARNING FOR SEMI-SUPERVISED FEW-SHOT\nCLASSIFICATION\nMengye Ren†⋊\n⋉, Eleni Triantaﬁllou∗†⋊\n⋉, Sachin Ravi∗§, Jake Snell†⋊\n⋉, Kevin Swersky¶,\nJoshua B. Tenenbaum♮, Hugo Larochelle¶‡ & Richard S. Zemel†‡⋊\n⋉\n†University of Toronto, §Princeton University, ¶Google Brain, ♮MIT, ‡CIFAR, ⋊\n⋉Vector Institute\n{mren,eleni}@cs.toronto.edu, sachinr@cs.princeton.edu,\njsnell@cs.toronto.edu, kswersky@google.com,\njbt@mit.edu, hugolarochelle@google.com, zemel@cs.toronto.edu\nABSTRACT\nIn few-shot classiﬁcation, we are interested in learning algorithms that train\na classiﬁer from only a handful of labeled examples.\nRecent progress in\nfew-shot classiﬁcation has featured meta-learning, in which a parameterized\nmodel for a learning algorithm is deﬁned and trained on episodes representing\ndifferent classiﬁcation problems, each with a small labeled training set and its\ncorresponding test set.\nIn this work, we advance this few-shot classiﬁcation\nparadigm towards a scenario where unlabeled examples are also available within\neach episode. We consider two situations: one where all unlabeled examples\nare assumed to belong to the same set of classes as the labeled examples of the\nepisode, as well as the more challenging situation where examples from other\ndistractor classes are also provided. To address this paradigm, we propose novel\nextensions of Prototypical Networks (Snell et al., 2017) that are augmented with\nthe ability to use unlabeled examples when producing prototypes. These models\nare trained in an end-to-end way on episodes, to learn to leverage the unlabeled\nexamples successfully. We evaluate these methods on versions of the Omniglot\nand miniImageNet benchmarks, adapted to this new framework augmented with\nunlabeled examples. We also propose a new split of ImageNet, consisting of a\nlarge set of classes, with a hierarchical structure. Our experiments conﬁrm that\nour Prototypical Networks can learn to improve their predictions due to unlabeled\nexamples, much like a semi-supervised algorithm would.\n1\nINTRODUCTION\nThe availability of large quantities of labeled data has enabled deep learning methods to achieve im-\npressive breakthroughs in several tasks related to artiﬁcial intelligence, such as speech recognition,\nobject recognition and machine translation. However, current deep learning approaches struggle in\ntackling problems for which labeled data are scarce. Speciﬁcally, while current methods excel at\ntackling a single problem with lots of labeled data, methods that can simultaneously solve a large\nvariety of problems that each have only a few labels are lacking. Humans on the other hand are\nreadily able to rapidly learn new classes, such as new types of fruit when we visit a tropical country.\nThis signiﬁcant gap between human and machine learning provides fertile ground for deep learning\ndevelopments.\nFor this reason, recently there has been an increasing body of work on few-shot learning, which\nconsiders the design of learning algorithms that speciﬁcally allow for better generalization on\nproblems with small labeled training sets. Here we focus on the case of few-shot classiﬁcation,\nwhere the given classiﬁcation problem is assumed to contain only a handful of labeled examples per\nclass. One approach to few-shot learning follows a form of meta-learning 1 (Thrun, 1998; Hochreiter\net al., 2001), which performs transfer learning from a pool of various classiﬁcation problems\n∗Equal contribution.\n1See the following blog post for an overview: http://bair.berkeley.edu/blog/2017/07/18/\nlearning-to-learn/\n1\narXiv:1803.00676v1  [cs.LG]  2 Mar 2018\n",
    "Published as a conference paper at ICLR 2018\ngenerated from large quantities of available labeled data, to new classiﬁcation problems from classes\nunseen at training time. Meta-learning may take the form of learning a shared metric (Vinyals et al.,\n2016; Snell et al., 2017), a common initialization for few-shot classiﬁers (Ravi & Larochelle, 2017;\nFinn et al., 2017) or a generic inference network (Santoro et al., 2016; Mishra et al., 2017).\n“goldﬁsh”\n“shark”\nUnlabeled Set\nSupport Set\nFigure 1: Consider a setup where the aim is to learn a\nclassiﬁer to distinguish between two previously unseen\nclasses, goldﬁsh and shark, given not only labeled\nexamples of these two classes, but also a larger pool\nof unlabeled examples, some of which may belong to\none of these two classes of interest. In this work we\naim to move a step closer to this more natural learning\nframework by incorporating in our learning episodes\nunlabeled data from the classes we aim to learn repre-\nsentations for (shown with dashed red borders) as well\nas from distractor classes .\nThese various meta-learning formulations have\nled to signiﬁcant progress recently in few-shot\nclassiﬁcation. However, this progress has been\nlimited in the setup of each few-shot learning\nepisode, which differs from how humans learn\nnew concepts in many dimensions.\nIn this\npaper we aim to generalize the setup in two\nways.\nFirst, we consider a scenario where\nthe new classes are learned in the presence of\nadditional unlabeled data.\nWhile there have\nbeen many successful applications of semi-\nsupervised learning to the regular setting of\na single classiﬁcation task (Chapelle et al.,\n2010) where classes at training and test time\nare the same, such work has not addressed the\nchallenge of performing transfer to new classes\nnever seen at training time, which we consider\nhere. Second, we consider the situation where\nthe new classes to be learned are not viewed in\nisolation. Instead, many of the unlabeled exam-\nples are from different classes; the presence of\nsuch distractor classes introduces an additional\nand more realistic level of difﬁculty to the few-\nshot problem.\nThis work is a ﬁrst study of this challenging semi-supervised form of few-shot learning. First, we\ndeﬁne the problem and propose benchmarks for evaluation that are adapted from the Omniglot and\nminiImageNet benchmarks used in ordinary few-shot learning. We perform an extensive empirical\ninvestigation of the two settings mentioned above, with and without distractor classes. Second, we\npropose and study three novel extensions of Prototypical Networks (Snell et al., 2017), a state-of-\nthe-art approach to few-shot learning, to the semi-supervised setting. Finally, we demonstrate in our\nexperiments that our semi-supervised variants successfully learn to leverage unlabeled examples and\noutperform purely supervised Prototypical Networks.\n2\nBACKGROUND\nWe start by deﬁning precisely the current paradigm for few-shot learning and the Prototypical\nNetwork approach to this problem.\n2.1\nFEW-SHOT LEARNING\nRecent progress on few-shot learning has been made possible by following an episodic paradigm.\nConsider a situation where we have a large labeled dataset for a set of classes Ctrain. However,\nafter training on examples from Ctrain, our ultimate goal is to produce classiﬁers for a disjoint set\nof new classes Ctest, for which only a few labeled examples will be available. The idea behind the\nepisodic paradigm is to simulate the types of few-shot problems that will be encountered at test,\ntaking advantage of the large quantities of available labeled data for classes Ctrain.\nSpeciﬁcally, models are trained on K-shot, N-way episodes constructed by ﬁrst sampling a\nsmall subset of N classes from Ctrain and then generating: 1) a training (support) set S =\n{(x1, y1), (x2, y2), . . . , (xN×K, yN×K)} containing K examples from each of the N classes and\n2) a test (query) set Q = {(x∗\n1, y∗\n1), (x∗\n2, y∗\n2), . . . , (x∗\nT , y∗\nT )} of different examples from the same\nN classes. Each xi ∈RD is an input vector of dimension D and yi ∈{1, 2, . . . , N} is a class\nlabel (similarly for x∗\ni and y∗\ni ). Training on such episodes is done by feeding the support set S to\n2\n",
    "Published as a conference paper at ICLR 2018\nthe model and updating its parameters to minimize the loss of its predictions for the examples in the\nquery set Q.\nOne way to think of this approach is that our model effectively trains to be a good learning algorithm.\nIndeed, much like a learning algorithm, the model must take in a set of labeled examples and produce\na predictor that can be applied to new examples. Moreover, training directly encourages the classiﬁer\nproduced by the model to have good generalization on the new examples of the query set. Due to\nthis analogy, training under this paradigm is often referred to as learning to learn or meta-learning.\nOn the other hand, referring to the content of episodes as training and test sets and to the process of\nlearning on these episodes as meta-learning or meta-training (as is sometimes done in the literature)\ncan be confusing. So for the sake of clarity, we will refer to the content of episodes as support and\nquery sets, and to the process of iterating over the training episodes simply as training.\n2.2\nPROTOTYPICAL NETWORKS\nPrototypical Network (Snell et al., 2017) is a few-shot learning model that has the virtue of being\nsimple and yet obtaining state-of-the-art performance. At a high-level, it uses the support set S to\nextract a prototype vector from each class, and classiﬁes the inputs in the query set based on their\ndistance to the prototype of each class.\nMore precisely, Prototypical Networks learn an embedding function h(x), parameterized as a neural\nnetwork, that maps examples into a space where examples from the same class are close and those\nfrom different classes are far. All parameters of Prototypical Networks lie in the embedding function.\nTo compute the prototype pc of each class c, a per-class average of the embedded examples is\nperformed:\npc =\nP\ni h(xi)zi,c\nP\ni zi,c\n, where zi,c = 1[yi = c].\n(1)\nThese prototypes deﬁne a predictor for the class of any new (query) example x∗, which assigns a\nprobability over any class c based on the distances between x∗and each prototype, as follows:\np(c|x∗, {pc}) =\nexp(−||h(x∗) −pc||2\n2)\nP\nc′ exp(−||h(x∗) −pc′||2\n2) .\n(2)\nThe loss function used to update Prototypical Networks for a given training episode is then simply\nthe average negative log-probability of the correct class assignments, for all query examples:\n−1\nT\nX\ni\nlog p(y∗\ni |x∗\ni , {pc}) .\n(3)\nTraining proceeds by minimizing the average loss, iterating over training episodes and performing a\ngradient descent update for each.\nGeneralization performance is measured on test set episodes, which contain images from classes\nin Ctest instead of Ctrain. For each test episode, we use the predictor produced by the Prototypical\nNetwork for the provided support set S to classify each of query input x∗into the most likely class\nˆy = argmaxc p(c|x∗, {pc}).\n3\nSEMI-SUPERVISED FEW-SHOT LEARNING\nWe now deﬁne the semi-supervised setting considered in this work for few-shot learning.\nThe training set is denoted as a tuple of labeled and unlabeled examples: (S, R). The labeled portion\nis the usual support set S of the few-shot learning literature, containing a list of tuples of inputs and\ntargets. In addition to classic few-shot learning, we introduce an unlabeled set R containing only\ninputs: R = {˜x1, ˜x2, . . . , ˜xM}. As in the purely supervised setting, our models are trained to\nperform well when predicting the labels for the examples in the episode’s query set Q. Figure 2\nshows a visualization of training and test episodes.\n3\n",
    "Published as a conference paper at ICLR 2018\n1\n2\n3\n?\n?\n1\n2\n3\n?\n?\nTraining\nTesting\nQuery Set\nQuery Set\nSupport Set\nSupport Set\nUnlabeled Set\nUnlabeled Set\n?\n?\nFigure 2: Example of the semi-supervised few-shot learning setup. Training involves iterating through training\nepisodes, consisting of a support set S, an unlabeled set R, and a query set Q. The goal is to use the labeled\nitems (shown with their numeric class label) in S and the unlabeled items in R within each episode to generalize\nto good performance on the corresponding query set. The unlabeled items in R may either be pertinent to the\nclasses we are considering (shown above with green plus signs) or they may be distractor items which belong\nto a class that is not relevant to the current episode (shown with red minus signs). However note that the model\ndoes not actually have ground truth information as to whether each unlabeled example is a distractor or not; the\nplus/minus signs are shown only for illustrative purposes. At test time, we are given new episodes consisting\nof novel classes not seen during training that we use to evaluate the meta-learning method.\n3.1\nSEMI-SUPERVISED PROTOTYPICAL NETWORKS\nIn their original formulation, Prototypical Networks do not specify a way to leverage the unlabeled\nset R. In what follows, we now propose various extensions that start from the basic deﬁnition\nof prototypes pc and provide a procedure for producing reﬁned prototypes ˜pc using the unlabeled\nexamples in R.\nBefore Reﬁnement\nAfter Reﬁnement\nFigure 3: Left: The prototypes are initialized based on the mean\nlocation of the examples of the corresponding class, as in ordinary\nPrototypical Networks. Support, unlabeled, and query examples have\nsolid, dashed, and white colored borders respectively. Right: The reﬁned\nprototypes obtained by incorporating the unlabeled examples, which\nclassiﬁes all query examples correctly.\nAfter the reﬁned prototypes are\nobtained, each of these models\nis trained with the same loss\nfunction for ordinary Prototypi-\ncal Networks of Equation 3, but\nreplacing pc with ˜pc. That is,\neach query example is classi-\nﬁed into one of the N classes\nbased on the proximity of its\nembedded position with the cor-\nresponding reﬁned prototypes,\nand the average negative log-\nprobability of the correct classi-\nﬁcation is used for training.\n3.1.1\nPROTOTYPICAL NETWORKS WITH SOFT k-MEANS\nWe ﬁrst consider a simple way of leveraging unlabeled examples for reﬁning prototypes, by\ntaking inspiration from semi-supervised clustering. Viewing each prototype as a cluster center,\nthe reﬁnement process could attempt to adjust the cluster locations to better ﬁt the examples in both\nthe support and unlabeled sets. Under this view, cluster assignments of the labeled examples in the\nsupport set are considered known and ﬁxed to each example’s label. The reﬁnement process must\ninstead estimate the cluster assignments of the unlabeled examples and adjust the cluster locations\n(the prototypes) accordingly.\n4\n",
    "Published as a conference paper at ICLR 2018\nOne natural choice would be to borrow from the inference performed by soft k-means. We prefer\nthis version of k-means over hard assignments since hard assignments would make the inference\nnon-differentiable. We start with the regular Prototypical Network’s prototypes pc (as speciﬁed in\nEquation 1) as the cluster locations. Then, the unlabeled examples get a partial assignment (˜zj,c) to\neach cluster based on their Euclidean distance to the cluster locations. Finally, reﬁned prototypes\nare obtained by incorporating these unlabeled examples.\nThis process can be summarized as follows:\n˜pc =\nP\ni h(xi)zi,c + P\nj h(˜xj)˜zj,c\nP\ni zi,c + P\nj ˜zj,c\n, where ˜zj,c =\nexp\n\u0000−||h(˜xj) −pc||2\n2\n\u0001\nP\nc′ exp (−||h(˜xj) −pc′||2\n2)\n(4)\nPredictions of each query input’s class is then modeled as in Equation 2, but using the reﬁned\nprototypes ˜pc.\nWe could perform several iterations of reﬁnement, as is usual in k-means. However, we have\nexperimented with various number of iterations and found results to not improve beyond a single\nreﬁnement step.\n3.1.2\nPROTOTYPICAL NETWORKS WITH SOFT k-MEANS WITH A DISTRACTOR CLUSTER\nThe soft k-means approach described above implicitly assumes that each unlabeled example belongs\nto either one of the N classes in the episode. However, it would be much more general to not make\nthat assumption and have a model robust to the existence of examples from other classes, which we\nrefer to as distractor classes. For example, such a situation would arise if we wanted to distinguish\nbetween pictures of unicycles and scooters, and decided to add an unlabeled set by downloading\nimages from the web. It then would not be realistic to assume that all these images are of unicycles\nor scooters. Even with a focused search, some may be from similar classes, such as bicycle.\nSince soft k-means distributes its soft assignments across all classes, distractor items could be\nharmful and interfere with the reﬁnement process, as prototypes would be adjusted to also partially\naccount for these distractors. A simple way to address this is to add an additional cluster whose\npurpose is to capture the distractors, thus preventing them from polluting the clusters of the classes\nof interest:\npc =\n( P\ni h(xi)zi,c\nP\ni zi,c\nfor c = 1...N\n0\nfor c = N + 1\n(5)\nHere we take the simplifying assumption that the distractor cluster has a prototype centered at the\norigin. We also consider introducing length-scales rc to represent variations in the within-cluster\ndistances, speciﬁcally for the distractor cluster:\n˜zj,c =\nexp\n\u0010\n−1\nr2c ||˜xj −pc||2\n2 −A(rc)\n\u0011\nP\nc′ exp\n\u0010\n−1\nr2c ||˜xj −pc′||2\n2 −A(rc′)\n\u0011, where A(r) = 1\n2 log(2π) + log(r)\n(6)\nFor simplicity, we set r1...N to 1 in our experiments, and only learn the length-scale of the distractor\ncluster rN+1.\n3.1.3\nPROTOTYPICAL NETWORKS WITH SOFT k-MEANS AND MASKING\nModeling distractor unlabeled examples with a single cluster is likely too simplistic. Indeed, it\nis inconsistent with our assumption that each cluster corresponds to one class, since distractor\nexamples may very well cover more than a single natural object category. Continuing with our\nunicycles and bicycles example, our web search for unlabeled images could accidentally include\nnot only bicycles, but other related objects such as tricycles or cars. This was also reﬂected in our\nexperiments, where we constructed the episode generating process so that it would sample distractor\nexamples from multiple classes.\nTo address this problem, we propose an improved variant: instead of capturing distractors with a\nhigh-variance catch-all cluster, we model distractors as examples that are not within some area of\nany of the legitimate class prototypes. This is done by incorporating a soft-masking mechanism on\n5\n",
    "Published as a conference paper at ICLR 2018\nthe contribution of unlabeled examples. At a high level, we want unlabeled examples that are closer\nto a prototype to be masked less than those that are farther.\nMore speciﬁcally, we modify the soft k-means reﬁnement as follows.\nWe start by computing\nnormalized distances ˜dj,c between examples ˜xj and prototypes pc:\n˜dj,c =\ndj,c\n1\nM\nP\nj dj,c\n, where dj,c = ||h(˜xj) −pc||2\n2\n(7)\nThen, soft thresholds βc and slopes γc are predicted for each prototype, by feeding to a small neural\nnetwork various statistics of the normalized distances for the prototype:\n[βc, γc] = MLP\n\u0012\u0014\nmin\nj ( ˜dj,c), max\nj ( ˜dj,c), var\nj ( ˜dj,c), skew\nj\n( ˜dj,c), kurt\nj ( ˜dj,c)\n\u0015\u0013\n(8)\nThis allows each threshold to use information on the amount of intra-cluster variation to determine\nhow aggressively it should cut out unlabeled examples.\nThen, soft masks mj,c for the contribution of each example to each prototype are computed, by\ncomparing to the threshold the normalized distances, as follows:\n˜pc =\nP\ni h(xi)zi,c + P\nj h(˜xj)˜zj,cmj,c\nP\ni zi,c + P\nj ˜zj,cmj,c\n, where mj,c = σ\n\u0010\n−γc\n\u0010\n˜dj,c −βc\n\u0011\u0011\n(9)\nwhere σ(·) is the sigmoid function.\nWhen training with this reﬁnement process, the model can now use its MLP in Equation 8 to learn\nto include or ignore entirely certain unlabeled examples. The use of soft masks makes this process\nentirely differentiable2. Finally, much like for regular soft k-means (with or without a distractor\ncluster), while we could recursively repeat the reﬁnement for multiple steps, we found a single step\nto perform well enough.\n4\nRELATED WORK\nWe summarize here the most relevant work from the literature on few-shot learning, semi-supervised\nlearning and clustering.\nThe best performing methods for few-shot learning use the episodic training framework prescribed\nby meta-learning. The approach within which our work falls is that of metric learning methods.\nPrevious work in metric-learning for few-shot-classiﬁcation includes Deep Siamese Networks (Koch\net al., 2015), Matching Networks (Vinyals et al., 2016), and Prototypical Networks (Snell et al.,\n2017), which is the model we extend to the semi-supervised setting in our work. The general idea\nhere is to learn an embedding function that embeds examples belonging to the same class close\ntogether while keeping embeddings from separate classes far apart. Distances between embeddings\nof items from the support set and query set are then used as a notion of similarity to do classiﬁcation.\nLastly, closely related to our work with regard to extending the few-shot learning setting, Bachman\net al. (2017) employ Matching Networks in an active learning framework where the model has a\nchoice of which unlabeled item to add to the support set over a certain number of time steps before\nclassifying the query set. Unlike our setting, their meta-learning agent can acquire ground-truth\nlabels from the unlabeled set, and they do not use distractor examples.\nOther meta-learning approaches to few-shot learning include learning how to use the support set\nto update a learner model so as to generalize to the query set. Recent work has involved learning\neither the weight initialization and/or update step that is used by a learner neural network (Ravi\n& Larochelle, 2017; Finn et al., 2017). Another approach is to train a generic neural architecture\nsuch as a memory-augmented recurrent network (Santoro et al., 2016) or a temporal convolutional\nnetwork (Mishra et al., 2017) to sequentially process the support set and perform accurate\npredictions of the labels of the query set examples. These other methods are also competitive for\nfew-shot learning, but we chose to extend Prototypical Networks in this work for its simplicity and\nefﬁciency.\n2We stop gradients from passing through the computation of the statistics in Equation 8, to avoid potential\nnumerical instabilities.\n6\n",
    "Published as a conference paper at ICLR 2018\nAs for the literature on semi-supervised learning, while it is quite vast (Zhu, 2005; Chapelle et al.,\n2010), the most relevant category to our work is related to self-training (Yarowsky, 1995; Rosenberg\net al., 2005). Here, a classiﬁer is ﬁrst trained on the initial training set. The classiﬁer is then used\nto classify unlabeled items, and the most conﬁdently predicted unlabeled items are added to the\ntraining set with the prediction of the classiﬁer as the assumed label. This is similar to our soft\nk-Means extension to Prototypical Networks. Indeed, since the soft assignments (Equation 4) match\nthe regular Prototypical Network’s classiﬁer output for new inputs (Equation 2), then the reﬁnement\ncan be thought of re-feeding to a Prototypical Network a new support set augmented with (soft)\nself-labels from the unlabeled set.\nOur algorithm is also related to transductive learning (Vapnik, 1998; Joachims, 1999; Fu et al., 2015),\nwhere the base classiﬁer gets reﬁned by seeing the unlabeled examples. In practice, one could use\nour method in a transductive setting where the unlabeled set is the same as the query set; however,\nhere to avoid our model memorizing labels of the unlabeled set during the meta-learning procedure,\nwe split out a separate unlabeled set that is different from the query set.\nIn addition to the original k-Means method (Lloyd, 1982), the most related work to our setup\ninvolving clustering algorithms considers applying k-Means in the presence of outliers (Hautamäki\net al., 2005; Chawla & Gionis, 2013; Gupta et al., 2017). The goal here is to correctly discover and\nignore the outliers so that they do not wrongly shift the cluster locations to form a bad partition of\nthe true data. This objective is also important in our setup as not ignoring outliers (or distractors)\nwill wrongly shift the prototypes and negatively inﬂuence classiﬁcation performance.\nOur contribution to the semi-supervised learning and clustering literature is to go beyond the\nclassical setting of training and evaluating within a single dataset, and consider the setting where\nwe must learn to transfer from a set of training classes Ctrain to a new set of test classes Ctest.\n5\nEXPERIMENTS\n5.1\nDATASETS\nWe evaluate the performance of our model on three datasets: two benchmark few-shot classiﬁcation\ndatasets and a novel large-scale dataset that we hope will be useful for future few-shot learning work.\nOmniglot (Lake et al., 2011) is a dataset of 1,623 handwritten characters from 50 alphabets. Each\ncharacter was drawn by 20 human subjects. We follow the few-shot setting proposed by Vinyals\net al. (2016), in which the images are resized to 28 × 28 pixels and rotations in multiples of 90◦are\napplied, yielding 6,492 classes in total. These are split into 4,112 training classes, 688 validation\nclasses, and 1,692 testing classes.\nminiImageNet (Vinyals et al., 2016) is a modiﬁed version of the ILSVRC-12 dataset (Russakovsky\net al., 2015), in which 600 images for each of 100 classes were randomly chosen to be part of the\ndataset. We rely on the class split used by Ravi & Larochelle (2017). These splits use 64 classes for\ntraining, 16 for validation, and 20 for test. All images are of size 84 × 84 pixels.\ntieredImageNet is our proposed dataset for few-shot classiﬁcation.\nLike miniImagenet, it is a\nsubset of ILSVRC-12. However, tieredImageNet represents a larger subset of ILSVRC-12 (608\nclasses rather than 100 for miniImageNet). Analogous to Omniglot, in which characters are grouped\ninto alphabets, tieredImageNet groups classes into broader categories corresponding to higher-level\nnodes in the ImageNet (Deng et al., 2009) hierarchy. There are 34 categories in total, with each\ncategory containing between 10 and 30 classes. These are split into 20 training, 6 validation and 8\ntesting categories (details of the dataset can be found in the supplementary material). This ensures\nthat all of the training classes are sufﬁciently distinct from the testing classes, unlike miniImageNet\nand other alternatives such as randImageNet proposed by Vinyals et al. (2016). For example, “pipe\norgan” is a training class and “electric guitar” is a test class in the Ravi & Larochelle (2017) split\nof miniImagenet, even though they are both musical instruments. This scenario would not occur in\ntieredImageNet since “musical instrument” is a high-level category and as such is not split between\ntraining and test classes. This represents a more realistic few-shot learning scenario since in general\nwe cannot assume that test classes will be similar to those seen in training. Additionally, the tiered\nstructure of tieredImageNet may be useful for few-shot learning approaches that can take advantage\nof hierarchical relationships between classes. We leave such interesting extensions for future work.\n7\n",
    "Published as a conference paper at ICLR 2018\nModels\nAcc.\nAcc. w/ D\nSupervised\n94.62 ± 0.09\n94.62 ± 0.09\nSemi-Supervised Inference\n97.45 ± 0.05\n95.08 ± 0.09\nSoft k-Means\n97.25 ± 0.10\n95.01 ± 0.09\nSoft k-Means+Cluster\n97.68 ± 0.07\n97.17 ± 0.04\nMasked Soft k-Means\n97.52 ± 0.07\n97.30 ± 0.08\nTable 1: Omniglot 1-shot classiﬁcation results. In this table as well as those below “w/ D” denotes “with\ndistractors”, where the unlabeled images contain irrelevant classes.\n5.2\nADAPTING THE DATASETS FOR SEMI-SUPERVISED LEARNING\nFor each dataset, we ﬁrst create an additional split to separate the images of each class into disjoint\nlabeled and unlabeled sets. For Omniglot and tieredImageNet we sample 10% of the images of\neach class to form the labeled split. The remaining 90% can only be used in the unlabeled portion\nof episodes. For miniImageNet we use 40% of the data for the labeled split and the remaining\n60% for the unlabeled, since we noticed that 10% was too small to achieve reasonable performance\nand avoid overﬁtting. We report the average classiﬁcation scores over 10 random splits of labeled\nand unlabeled portions of the training set, with uncertainty computed in standard error (standard\ndeviation divided by the square root of the total number of splits).\nWe would like to emphasize that due to this labeled/unlabeled split, we are using strictly less label\ninformation than in the previously-published work on these datasets. Because of this, we do not\nexpect our results to match the published numbers, which should instead be interpreted as an upper-\nbound for the performance of the semi-supervised models deﬁned in this work.\nEpisode construction then is performed as follows. For a given dataset, we create a training episode\nby ﬁrst sampling N classes uniformly at random from the set of training classes Ctrain. We then\nsample K images from the labeled split of each of these classes to form the support set, and M\nimages from the unlabeled split of each of these classes to form the unlabeled set. Optionally,\nwhen including distractors, we additionally sample H other classes from the set of training classes\nand M images from the unlabeled split of each to act as the distractors. These distractor images\nare added to the unlabeled set along with the unlabeled images of the N classes of interest (for a\ntotal of MN + MH unlabeled images). The query portion of the episode is comprised of a ﬁxed\nnumber of images from the labeled split of each of the N chosen classes. Test episodes are created\nanalogously, but with the N classes (and optionally the H distractor classes) sampled from Ctest.\nIn the experiments reported here we used H = N = 5, i.e. 5 classes for both the labeled classes\nand the distractor classes. We used M = 5 for training and M = 20 for testing in most cases,\nthus measuring the ability of the models to generalize to a larger unlabeled set size. Details of the\ndataset splits, including the speciﬁc classes assigned to train/validation/test sets, can be found in\nAppendices A and B.\nIn each dataset we compare our three semi-supervised models with two baselines. The ﬁrst baseline,\nreferred to as “Supervised” in our tables, is an ordinary Prototypical Network that is trained in a\npurely supervised way on the labeled split of each dataset. The second baseline, referred to as\n“Semi-Supervised Inference”, uses the embedding function learned by this supervised Prototypical\nNetwork, but performs semi-supervised reﬁnement of the prototypes at test time using a step of Soft\nk-Means reﬁnement. This is to be contrasted with our semi-supervised models that perform this\nreﬁnement both at training time and at test time, therefore learning a different embedding function.\nWe evaluate each model in two settings: one where all unlabeled examples belong to the classes of\ninterest, and a more challenging one that includes distractors. Details of the model hyperparameters\ncan be found in Appendix D and our online repository.3\n5.3\nRESULTS\nResults for Omniglot, miniImageNet and tieredImageNet are given in Tables 1, 2 and 5, respectively,\nwhile Figure 4 shows the performance of our models on tieredImageNet (our largest dataset) using\n3 Code available at https://github.com/renmengye/few-shot-ssl-public\n8\n",
    "Published as a conference paper at ICLR 2018\nModels\n1-shot Acc.\n5-shot Acc.\n1-shot Acc w/ D\n5-shot Acc. w/ D\nSupervised\n43.61 ± 0.27\n59.08 ± 0.22\n43.61 ± 0.27\n59.08 ± 0.22\nSemi-Supervised Inference\n48.98 ± 0.34\n63.77 ± 0.20\n47.42 ± 0.33\n62.62 ± 0.24\nSoft k-Means\n50.09 ± 0.45\n64.59 ± 0.28\n48.70 ± 0.32\n63.55 ± 0.28\nSoft k-Means+Cluster\n49.03 ± 0.24\n63.08 ± 0.18\n48.86 ± 0.32\n61.27 ± 0.24\nMasked Soft k-Means\n50.41 ± 0.31\n64.39 ± 0.24\n49.04 ± 0.31\n62.96 ± 0.14\nTable 2: miniImageNet 1/5-shot classiﬁcation results.\nModels\n1-shot Acc.\n5-shot Acc.\n1-shot Acc. w/ D\n5-shot Acc. w/ D\nSupervised\n46.52 ± 0.52\n66.15 ± 0.22\n46.52 ± 0.52\n66.15 ± 0.22\nSemi-Supervised Inference\n50.74 ± 0.75\n69.37 ± 0.26\n48.67 ± 0.60\n67.46 ± 0.24\nSoft k-Means\n51.52 ± 0.36\n70.25 ± 0.31\n49.88 ± 0.52\n68.32 ± 0.22\nSoft k-Means+Cluster\n51.85 ± 0.25\n69.42 ± 0.17\n51.36 ± 0.31\n67.56 ± 0.10\nMasked Soft k-Means\n52.39 ± 0.44\n69.88 ± 0.20\n51.38 ± 0.38\n69.08 ± 0.25\nTable 3: tieredImageNet 1/5-shot classiﬁcation results.\ndifferent values for M (number of items in the unlabeled set per class). Additional results comparing\nthe ProtoNet model to various baselines on these datasets, and analysis of the performance of the\nMasked Soft k-Means model can be found in Appendix C.\nAcross all three benchmarks, at least one of our proposed models outperforms the baselines,\ndemonstrating the effectiveness of our semi-supervised meta-learning procedure.\nIn the non-\ndistractor settings, all three proposed models outperform the baselines in almost all the experiments,\nwithout a clear winner between the three models across the datasets and shot numbers. In the\nscenario where training and testing includes distractors, Masked Soft k-Means shows the most\nrobust performance across all three datasets, attaining the best results in each case but one. In\nfact this model reaches performance that is close to the upper bound based on the results without\ndistractors.\nFrom Figure 4, we observe clear improvements in test accuracy when the number of items in the\nunlabeled set per class grows from 0 to 25. These models were trained with M = 5 and thus are\nshowing an ability to extrapolate in generalization. This conﬁrms that, through meta-training, the\nmodels learn to acquire a better representation that is improved by semi-supervised reﬁnement.\n6\nCONCLUSION\nIn this work, we propose a novel semi-supervised few-shot learning paradigm, where an unlabeled\nset is added to each episode. We also extend the setup to more realistic situations where the unlabeled\nset has novel classes distinct from the labeled classes. To address the problem that current few-\nshot classiﬁcation datasets are too small for a labeled vs. unlabeled split and also lack hierarchical\nlevels of labels, we introduce a new dataset, tieredImageNet. We propose several novel extensions\nof Prototypical Networks, and they show consistent improvements under semi-supervised settings\ncompared to our baselines. As future work, we are working on incorporating fast weights (Ba\net al., 2016; Finn et al., 2017) into our framework so that examples can have different embedding\nrepresentations given the contents in the episode.\nAcknowledgement\nSupported by grants from NSERC, Samsung, and the Intelligence Advanced\nResearch Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC)\ncontract number D16PC00003. The U.S. Government is authorized to reproduce and distribute\nreprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer:\nThe views and conclusions contained herein are those of the authors and should not be interpreted\nas necessarily representing the ofﬁcial policies or endorsements, either expressed or implied, of\nIARPA, DoI/IBC, or the U.S. Government.\n9\n",
    "Published as a conference paper at ICLR 2018\n44\n46\n48\n50\n52\n54\nTest Acc. (%)\n1-shot w/o Distractors\nSupervised\nSoft K-Means\nSoft K-Means+Cluster\nMasked Soft K-Means\n1-shot w/ Distractors\n0\n1\n2\n5\n10\n15\n20\n25\nNumber of Unlabeled Items Per Class\n65\n66\n67\n68\n69\n70\n71\nTest Acc. (%)\n5-shot w/o Distractors\n0\n1\n2\n5\n10\n15\n20\n25\nNumber of Unlabeled Items Per Class\n5-shot w/ Distractors\nFigure 4: Model Performance on tieredImageNet with different numbers of unlabeled items during test time.\nREFERENCES\nJimmy Ba, Geoffrey E. Hinton, Volodymyr Mnih, Joel Z. Leibo, and Catalin Ionescu. Using fast\nweights to attend to the recent past.\nIn Advances in Neural Information Processing Systems\n29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016,\nBarcelona, Spain, pp. 4331–4339, 2016.\nPhilip Bachman, Alessandro Sordoni, and Adam Trischler. Learning algorithms for active learning.\n2017.\nOlivier Chapelle, Bernhard Schölkopf, and Alexander Zien. Semi-Supervised Learning. The MIT\nPress, 1st edition, 2010. ISBN 0262514125, 9780262514125.\nSanjay Chawla and Aristides Gionis.\nk-means–: A uniﬁed approach to clustering and outlier\ndetection. In Proceedings of the 2013 SIAM International Conference on Data Mining, pp. 189–\n197. SIAM, 2013.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009.\nIEEE Conference on, pp. 248–255. IEEE, 2009.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation\nof deep networks. In 34th International Conference on Machine Learning, 2017.\nYanwei Fu, Timothy M. Hospedales, Tao Xiang, and Shaogang Gong. Transductive multi-view\nzero-shot learning. IEEE Trans. Pattern Anal. Mach. Intell., 37(11):2332–2345, 2015.\nShalmoli Gupta, Ravi Kumar, Kefu Lu, Benjamin Moseley, and Sergei Vassilvitskii. Local search\nmethods for k-means with outliers. Proceedings of the VLDB Endowment, 10(7):757–768, 2017.\nVille Hautamäki, Svetlana Cherednichenko, Ismo Kärkkäinen, Tomi Kinnunen, and Pasi Fränti.\nImproving k-means by outlier removal. In Scandinavian Conference on Image Analysis, pp. 978–\n987. Springer, 2005.\nSepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent.\nIn International Conference on Artiﬁcial Neural Networks, pp. 87–94. Springer, 2001.\nThorsten Joachims. Transductive inference for text classiﬁcation using support vector machines. In\nProceedings of the Sixteenth International Conference on Machine Learning, 1999.\n10\n",
    "Published as a conference paper at ICLR 2018\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nGregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot\nimage recognition. In ICML Deep Learning Workshop, volume 2, 2015.\nBrenden M. Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua B. Tenenbaum. One shot learning\nof simple visual concepts. In Proceedings of the 33th Annual Meeting of the Cognitive Science\nSociety, CogSci 2011, Boston, Massachusetts, USA, July 20-23, 2011, 2011.\nStuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):\n129–137, 1982.\nNikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. Meta-learning with temporal\nconvolutions. CoRR, abs/1707.03141, 2017.\nSachin Ravi and Hugo Larochelle.\nOptimization as a model for few-shot learning.\nIn 5th\nInternational Conference on Learning Representations, 2017.\nChuck Rosenberg, Martial Hebert, and Henry Schneiderman. Semi-supervised self-training of object\ndetection models. 2005.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\nrecognition challenge. International Journal of Computer Vision, 115(3):211–252, 2015.\nAdam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy P. Lillicrap. One-\nshot learning with memory-augmented neural networks. In 33rd International Conference on\nMachine Learning, 2016.\nJake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical networks for few-shot learning. In\nAdvances in Neural Information Processing Systems 30, 2017.\nSebastian Thrun. Lifelong learning algorithms. In Learning to learn, pp. 181–209. Springer, 1998.\nV.N. Vapnik. Statistical Learning Theory. Wiley, 1998. ISBN 9788126528929.\nOriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching\nnetworks for one shot learning. In Advances in Neural Information Processing Systems 29, pp.\n3630–3638, 2016.\nDavid Yarowsky.\nUnsupervised word sense disambiguation rivaling supervised methods.\nIn\nProceedings of the 33rd annual meeting on Association for Computational Linguistics, pp. 189–\n196. Association for Computational Linguistics, 1995.\nXiaojin Zhu. Semi-supervised learning literature survey. 2005.\n11\n",
    "Published as a conference paper at ICLR 2018\nA\nOMNIGLOT DATASET DETAILS\nWe used the following split details for experiments on Omniglot dataset. This is the same train/test\nsplit as (Vinyals et al., 2016), but we created our own validation split for selecting hyper-parameters.\nModels are trained on the train split only.\nTrain\nAlphabets:\nAlphabet_of_the_Magi,\nAngelic,\nAnglo-Saxon_Futhorc,\nArcadian,\nAsomtavruli_(Georgian),\nAtemayar_Qelisayer,\nAtlantean,\nAurek-Besh,\nAvesta,\nBalinese,\nBlackfoot_(Canadian_Aboriginal_Syllabics),\nBraille,\nBurmese_(Myanmar),\nCyrillic,\nFuturama,\nGe_ez,\nGlagolitic,\nGrantha,\nGreek,\nGujarati,\nGurmukhi\n(character\n01-41),\nInuktitut_(Canadian_Aboriginal_Syllabics), Japanese_(hiragana), Japanese_(katakana), Korean,\nLatin,\nMalay_(Jawi_-_Arabic),\nN_Ko,\nOjibwe_(Canadian_Aboriginal_Syllabics),\nSanskrit,\nSyriac_(Estrangelo), Tagalog, Tiﬁnagh\nValidation Alphabets: Armenian, Bengali, Early_Aramaic, Hebrew, Mkhedruli_(Geogian)\nTest Alphabets: Gurmukhi (character 42-45), Kannada, Keble, Malayalam, Manipuri, Mongolian,\nOld_Church_Slavonic_(Cyrillic), Oriya, Sylheti, Syriac_(Serto), Tengwar, Tibetan, ULOG\nB\ntieredIMAGENET DATASET DETAILS\nEach high-level category in tieredImageNet contains between 10 and 30 ILSVRC-12 classes (17.8\non average). In the ImageNet hierarchy, some classes have multiple parent nodes. Therefore, classes\nbelonging to more than one category were removed from the dataset to ensure separation between\ntraining and test categories. Test categories were chosen to reﬂect various levels of separation\nbetween training and test classes. Some test categories (such as “working dog”) are fairly similar to\ntraining categories, whereas others (such as “geological formation”) are quite different. The list of\ncategories is shown below and statistics of the dataset can be found in Table 4. A visualization of\nthe categories according to the ImageNet hierarchy is shown in Figure 5. The full list of classes per\ncategory will also be made public, however for the sake of brevity we do not include it here.\nTable 4: Statistics of the tieredImageNet dataset.\nTrain\nVal\nTest\nTotal\nCategories\n20\n6\n8\n34\nClasses\n351\n97\n160\n608\nImages\n448,695\n124,261\n206,209\n779,165\nTrain Categories: n02087551 (hound, hound dog), n02092468 (terrier), n02120997 (feline,\nfelid), n02370806 (ungulate, hoofed mammal), n02469914 (primate), n01726692 (snake, ser-\npent, ophidian), n01674216 (saurian), n01524359 (passerine, passeriform bird), n01844917\n(aquatic bird), n04081844 (restraint, constraint), n03574816 (instrument), n03800933 (mu-\nsical instrument, instrument), n03125870 (craft), n04451818 (tool), n03414162 (game\nequipment), n03278248 (electronic equipment), n03419014 (garment), n03297735 (estab-\nlishment), n02913152 (building, ediﬁce), n04014297 (protective covering, protective cover,\nprotection).\nValidation Categories: n02098550 (sporting dog, gun dog), n03257877 (durables, durable\ngoods, consumer durables), n03405265 (furnishing), n03699975 (machine), n03738472\n(mechanism), n03791235 (motor vehicle, automotive vehicle),\nTest Categories: n02103406 (working dog), n01473806 (aquatic vertebrate), n02159955 (in-\nsect), n04531098 (vessel), n03839993 (obstruction, obstructor, obstructer, impediment, imped-\nimenta), n09287968 (geological formation, formation), n00020090 (substance), n15046900\n(solid).\n12\n",
    "Published as a conference paper at ICLR 2018\nOmniglot\nminiImageNet\ntieredImageNet\nModels\n1-shot\n1-shot\n5-shot\n1-shot\n5-shot\n1-NN Pixel\n40.39 ± 0.36\n26.74 ± 0.48\n31.43 ± 0.51\n26.55 ± 0.50\n30.79 ± 0.53\n1-NN CNN rnd\n59.55 ± 0.46\n24.03 ± 0.38\n27.54 ± 0.42\n25.49 ± 0.45\n30.01 ± 0.47\n1-NN CNN pre\n52.53 ± 0.51\n32.90 ± 0.58\n40.79 ± 0.76\n32.76 ± 0.66\n40.26 ± 0.67\nLR Pixel\n49.15 ± 0.39\n24.50 ± 0.41\n33.33 ± 0.68\n25.70 ± 0.46\n36.30 ± 0.62\nLR CNN rnd\n57.80 ± 0.45\n24.10 ± 0.50\n28.40 ± 0.42\n26.55 ± 0.48\n32.51 ± 0.52\nLR CNN pre\n48.49 ± 0.47\n30.28 ± 0.54\n40.27 ± 0.59\n34.52 ± 0.68\n43.58 ± 0.72\nProtoNet\n94.62 ± 0.09\n43.61 ± 0.27\n59.08 ± 0.22\n46.52 ± 0.32\n66.15 ± 0.34\nTable 5: Few-shot learning baseline results using labeled/unlabeled splits. Baselines either takes inputs directly\nfrom the pixel space or use a CNN to extract features. “rnd” denotes using a randomly initialized CNN, and\n“pre” denotes using a CNN that is pretrained for supervised classiﬁcation for all training classes.\nC\nEXTRA EXPERIMENTAL RESULTS\nC.1\nFEW-SHOT CLASSIFICATION BASELINES\nWe provide baseline results on few-shot classiﬁcation using 1-nearest neighbor and logistic\nregression with either pixel inputs or CNN features. Compared with the baselines, Regular ProtoNet\nperforms signiﬁcantly better on all three few-shot classiﬁcation datasets.\nC.2\nNUMBER OF UNLABELED ITEMS\nFigure 6 shows test accuracy values with different number of unlabeled items during test time.\nFigure 7 shows our mask output value distribution of the Masked Soft k-Means model on Omniglot.\nThe mask values have a bi-modal distribution, corresponding to distractor and non-distractor items.\nD\nHYPERPARAMETER DETAILS\nFor Omniglot, we adopted the best hyperparameter settings found for ordinary Prototypical\nNetworks in Snell et al. (2017). In these settings, the learning rate was set to 1e-3, and cut in half\nevery 2K updates starting at update 2K. We trained for a total of 20K updates. For miniImagenet and\ntieredImageNet, we trained with a starting learning rate of 1e-3, which we also decayed. We started\nthe decay after 25K updates, and every 25K updates thereafter we cut it in half. We trained for a\ntotal of 200K updates. We used ADAM (Kingma & Ba, 2014) for the optimization of our models.\nFor the MLP used in the Masked Soft k-Means model, we use a single hidden layer with 20 hidden\nunits with a tanh non-linearity for all 3 datasets. We did not tune the hyparameters of this MLP so\nbetter performance may be attained with a more rigorous hyperparameter search.\n13\n",
    "Published as a conference paper at ICLR 2018\nTrain Categories\nValidation Categories\nTest Categories\nn15046900\nsolid\n[22 classes]\n(test)\nn04170037\nself-propelled vehicle\nn03791235\nmotor vehicle, automotive vehicle\n[22 classes]\n(validation)\nn02103406\nworking dog\n[30 classes]\n(test)\nn01661818\ndiapsid, diapsid reptile\nn01726692\nsnake, serpent, ophidian\n[17 classes]\n(train)\nn01674216\nsaurian\n[11 classes]\n(train)\nn00003553\nwhole, unit\nn00021939\nartifact, artefact\nn00004258\nliving thing, animate thing\nn03699975\nmachine\n[12 classes]\n(validation)\nn01466257\nchordate\nn01471682\nvertebrate, craniate\nn00020090\nsubstance\n[22 classes]\n(test)\nn04576211\nwheeled vehicle\nn03800933\nmusical instrument, instrument\n[26 classes]\n(train)\nn03278248\nelectronic equipment\n[11 classes]\n(train)\nn01524359\npasserine, passeriform bird\n[11 classes]\n(train)\nn02083346\ncanine, canid\nn02084071\ndog, domestic dog, Canis familiaris\nn03122748\ncovering\nn03051540\nclothing, article of clothing, vesture, wear,\nn04014297\nprotective covering, protective cover, protect\n[24 classes]\n(train)\nn01861778\nmammal, mammalian\nn01473806\naquatic vertebrate\n[16 classes]\n(test)\nn01661091\nreptile, reptilian\nn01503061\nbird\nn02087551\nhound, hound dog\n[19 classes]\n(train)\nn04341686\nstructure, construction\nn03839993\nobstruction, obstructor, obstructer, impedimen\n[10 classes]\n(test)\nn02913152\nbuilding, ediﬁce\n[14 classes]\n(train)\nn03297735\nestablishment\n[10 classes]\n(train)\nn03294048\nequipment\nn03414162\ngame equipment\n[12 classes]\n(train)\nn09287968\ngeological formation, formation\n[10 classes]\n(test)\nn03125870\ncraft\n[20 classes]\n(train)\nn03738472\nmechanism\n[12 classes]\n(validation)\nn03257877\ndurables, durable goods, consumer durables\n[12 classes]\n(validation)\nn01886756\nplacental, placental mammal, eutherian, euther\nn02370806\nungulate, hoofed mammal\n[17 classes]\n(train)\nn02075296\ncarnivore\nn02469914\nprimate\n[20 classes]\n(train)\nn02159955\ninsect\n[27 classes]\n(test)\nn00004475\norganism, being\nn00015388\nanimal, animate being, beast, brute, creature,\nn00001930\nphysical entity\nn00002684\nobject, physical object\nn00020827\nmatter\nn03419014\ngarment\n[25 classes]\n(train)\nn01317541\ndomestic animal, domesticated animal\nn01767661\narthropod\nn03076708\ncommodity, trade good, good\nn03093574\nconsumer goods\nn04531098\nvessel\n[23 classes]\n(test)\nn03575240\ninstrumentality, instrumentation\nn02098550\nsporting dog, gun dog\n[17 classes]\n(validation)\nn03574816\ninstrument\n[28 classes]\n(train)\nn01844917\naquatic bird\n[24 classes]\n(train)\nn03094503\ncontainer\nn04451818\ntool\n[12 classes]\n(train)\nn03563967\nimplement\nn02087122\nhunting dog\nn02120997\nfeline, felid\n[13 classes]\n(train)\nn04524313\nvehicle\nn01905661\ninvertebrate\nn02092468\nterrier\n[26 classes]\n(train)\nn03100490\nconveyance, transport\nn03183080\ndevice\nn03405265\nfurnishing\n[22 classes]\n(validation)\nn04081844\nrestraint, constraint\n[11 classes]\n(train)\nFigure 5: Hierarchy of tieredImagenet categories. Training categories are highlighted in red and test categories in blue. Each category indicates the number of associated classes\nfrom ILSVRC-12. Best viewed zoomed-in on electronic version.\n14\n",
    "Published as a conference paper at ICLR 2018\n44\n46\n48\n50\n52\n54\nTest Acc. (%)\n46.52\n46.52\n46.52\n46.52\n46.52\n46.52\n46.52\n46.52\n46.21\n45.89\n47.39\n49.46\n50.70\n51.20\n51.52\n51.66\n46.52\n46.04\n47.61\n49.73\n51.03\n51.54\n51.85\n52.06\n46.41\n46.49\n47.94\n50.19\n51.49\n52.08\n52.39\n52.61\n1-shot w/o Distractors\nSupervised\nSoft K-Means\nSoft K-Means+Cluster\nMasked Soft K-Means\n46.52\n46.52\n46.52\n46.52\n46.52\n46.52\n46.52\n46.52\n45.83\n45.70\n47.24\n48.82\n49.53\n49.77\n49.88\n49.95\n45.48\n46.43\n48.23\n49.99\n50.81\n51.11\n51.36\n51.38\n46.20\n46.63\n48.09\n49.87\n50.80\n51.18\n51.38\n51.49\n1-shot w/ Distractors\n0\n1\n2\n5\n10\n15\n20\n25\nNumber of Unlabeled Items Per Class\n65\n66\n67\n68\n69\n70\n71\nTest Acc. (%)\n66.15\n66.15\n66.15\n66.15\n66.15\n66.15\n66.15\n66.15\n66.66\n66.83\n67.21\n68.15\n69.23\n69.84\n70.25\n70.54\n66.15\n66.07\n66.46\n67.42\n68.43\n69.03\n69.42\n69.64\n66.26\n66.45\n66.85\n67.84\n68.84\n69.48\n69.88\n70.15\n5-shot w/o Distractors\n0\n1\n2\n5\n10\n15\n20\n25\nNumber of Unlabeled Items Per Class\n66.15\n66.15\n66.15\n66.15\n66.15\n66.15\n66.15\n66.15\n66.03\n65.79\n66.18\n67.14\n67.82\n68.15\n68.32\n68.45\n66.15\n65.29\n65.68\n66.55\n67.15\n67.39\n67.57\n67.64\n66.40\n66.53\n66.89\n67.73\n68.47\n68.85\n69.08\n69.24\n5-shot w/ Distractors\nFigure 6: Model Performance on tieredImageNet with different number of unlabeled items during test time.\nWe include test accuracy numbers in this chart.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMask values\n0.0%\n5.0%\n10.0%\n15.0%\n20.0%\n25.0%\nPercentage\nFigure 7: Mask values predicted by masked soft k-means on Omniglot.\n15\n"
  ],
  "full_text": "Published as a conference paper at ICLR 2018\nMETA-LEARNING FOR SEMI-SUPERVISED FEW-SHOT\nCLASSIFICATION\nMengye Ren†⋊\n⋉, Eleni Triantaﬁllou∗†⋊\n⋉, Sachin Ravi∗§, Jake Snell†⋊\n⋉, Kevin Swersky¶,\nJoshua B. Tenenbaum♮, Hugo Larochelle¶‡ & Richard S. Zemel†‡⋊\n⋉\n†University of Toronto, §Princeton University, ¶Google Brain, ♮MIT, ‡CIFAR, ⋊\n⋉Vector Institute\n{mren,eleni}@cs.toronto.edu, sachinr@cs.princeton.edu,\njsnell@cs.toronto.edu, kswersky@google.com,\njbt@mit.edu, hugolarochelle@google.com, zemel@cs.toronto.edu\nABSTRACT\nIn few-shot classiﬁcation, we are interested in learning algorithms that train\na classiﬁer from only a handful of labeled examples.\nRecent progress in\nfew-shot classiﬁcation has featured meta-learning, in which a parameterized\nmodel for a learning algorithm is deﬁned and trained on episodes representing\ndifferent classiﬁcation problems, each with a small labeled training set and its\ncorresponding test set.\nIn this work, we advance this few-shot classiﬁcation\nparadigm towards a scenario where unlabeled examples are also available within\neach episode. We consider two situations: one where all unlabeled examples\nare assumed to belong to the same set of classes as the labeled examples of the\nepisode, as well as the more challenging situation where examples from other\ndistractor classes are also provided. To address this paradigm, we propose novel\nextensions of Prototypical Networks (Snell et al., 2017) that are augmented with\nthe ability to use unlabeled examples when producing prototypes. These models\nare trained in an end-to-end way on episodes, to learn to leverage the unlabeled\nexamples successfully. We evaluate these methods on versions of the Omniglot\nand miniImageNet benchmarks, adapted to this new framework augmented with\nunlabeled examples. We also propose a new split of ImageNet, consisting of a\nlarge set of classes, with a hierarchical structure. Our experiments conﬁrm that\nour Prototypical Networks can learn to improve their predictions due to unlabeled\nexamples, much like a semi-supervised algorithm would.\n1\nINTRODUCTION\nThe availability of large quantities of labeled data has enabled deep learning methods to achieve im-\npressive breakthroughs in several tasks related to artiﬁcial intelligence, such as speech recognition,\nobject recognition and machine translation. However, current deep learning approaches struggle in\ntackling problems for which labeled data are scarce. Speciﬁcally, while current methods excel at\ntackling a single problem with lots of labeled data, methods that can simultaneously solve a large\nvariety of problems that each have only a few labels are lacking. Humans on the other hand are\nreadily able to rapidly learn new classes, such as new types of fruit when we visit a tropical country.\nThis signiﬁcant gap between human and machine learning provides fertile ground for deep learning\ndevelopments.\nFor this reason, recently there has been an increasing body of work on few-shot learning, which\nconsiders the design of learning algorithms that speciﬁcally allow for better generalization on\nproblems with small labeled training sets. Here we focus on the case of few-shot classiﬁcation,\nwhere the given classiﬁcation problem is assumed to contain only a handful of labeled examples per\nclass. One approach to few-shot learning follows a form of meta-learning 1 (Thrun, 1998; Hochreiter\net al., 2001), which performs transfer learning from a pool of various classiﬁcation problems\n∗Equal contribution.\n1See the following blog post for an overview: http://bair.berkeley.edu/blog/2017/07/18/\nlearning-to-learn/\n1\narXiv:1803.00676v1  [cs.LG]  2 Mar 2018\n\n\nPublished as a conference paper at ICLR 2018\ngenerated from large quantities of available labeled data, to new classiﬁcation problems from classes\nunseen at training time. Meta-learning may take the form of learning a shared metric (Vinyals et al.,\n2016; Snell et al., 2017), a common initialization for few-shot classiﬁers (Ravi & Larochelle, 2017;\nFinn et al., 2017) or a generic inference network (Santoro et al., 2016; Mishra et al., 2017).\n“goldﬁsh”\n“shark”\nUnlabeled Set\nSupport Set\nFigure 1: Consider a setup where the aim is to learn a\nclassiﬁer to distinguish between two previously unseen\nclasses, goldﬁsh and shark, given not only labeled\nexamples of these two classes, but also a larger pool\nof unlabeled examples, some of which may belong to\none of these two classes of interest. In this work we\naim to move a step closer to this more natural learning\nframework by incorporating in our learning episodes\nunlabeled data from the classes we aim to learn repre-\nsentations for (shown with dashed red borders) as well\nas from distractor classes .\nThese various meta-learning formulations have\nled to signiﬁcant progress recently in few-shot\nclassiﬁcation. However, this progress has been\nlimited in the setup of each few-shot learning\nepisode, which differs from how humans learn\nnew concepts in many dimensions.\nIn this\npaper we aim to generalize the setup in two\nways.\nFirst, we consider a scenario where\nthe new classes are learned in the presence of\nadditional unlabeled data.\nWhile there have\nbeen many successful applications of semi-\nsupervised learning to the regular setting of\na single classiﬁcation task (Chapelle et al.,\n2010) where classes at training and test time\nare the same, such work has not addressed the\nchallenge of performing transfer to new classes\nnever seen at training time, which we consider\nhere. Second, we consider the situation where\nthe new classes to be learned are not viewed in\nisolation. Instead, many of the unlabeled exam-\nples are from different classes; the presence of\nsuch distractor classes introduces an additional\nand more realistic level of difﬁculty to the few-\nshot problem.\nThis work is a ﬁrst study of this challenging semi-supervised form of few-shot learning. First, we\ndeﬁne the problem and propose benchmarks for evaluation that are adapted from the Omniglot and\nminiImageNet benchmarks used in ordinary few-shot learning. We perform an extensive empirical\ninvestigation of the two settings mentioned above, with and without distractor classes. Second, we\npropose and study three novel extensions of Prototypical Networks (Snell et al., 2017), a state-of-\nthe-art approach to few-shot learning, to the semi-supervised setting. Finally, we demonstrate in our\nexperiments that our semi-supervised variants successfully learn to leverage unlabeled examples and\noutperform purely supervised Prototypical Networks.\n2\nBACKGROUND\nWe start by deﬁning precisely the current paradigm for few-shot learning and the Prototypical\nNetwork approach to this problem.\n2.1\nFEW-SHOT LEARNING\nRecent progress on few-shot learning has been made possible by following an episodic paradigm.\nConsider a situation where we have a large labeled dataset for a set of classes Ctrain. However,\nafter training on examples from Ctrain, our ultimate goal is to produce classiﬁers for a disjoint set\nof new classes Ctest, for which only a few labeled examples will be available. The idea behind the\nepisodic paradigm is to simulate the types of few-shot problems that will be encountered at test,\ntaking advantage of the large quantities of available labeled data for classes Ctrain.\nSpeciﬁcally, models are trained on K-shot, N-way episodes constructed by ﬁrst sampling a\nsmall subset of N classes from Ctrain and then generating: 1) a training (support) set S =\n{(x1, y1), (x2, y2), . . . , (xN×K, yN×K)} containing K examples from each of the N classes and\n2) a test (query) set Q = {(x∗\n1, y∗\n1), (x∗\n2, y∗\n2), . . . , (x∗\nT , y∗\nT )} of different examples from the same\nN classes. Each xi ∈RD is an input vector of dimension D and yi ∈{1, 2, . . . , N} is a class\nlabel (similarly for x∗\ni and y∗\ni ). Training on such episodes is done by feeding the support set S to\n2\n\n\nPublished as a conference paper at ICLR 2018\nthe model and updating its parameters to minimize the loss of its predictions for the examples in the\nquery set Q.\nOne way to think of this approach is that our model effectively trains to be a good learning algorithm.\nIndeed, much like a learning algorithm, the model must take in a set of labeled examples and produce\na predictor that can be applied to new examples. Moreover, training directly encourages the classiﬁer\nproduced by the model to have good generalization on the new examples of the query set. Due to\nthis analogy, training under this paradigm is often referred to as learning to learn or meta-learning.\nOn the other hand, referring to the content of episodes as training and test sets and to the process of\nlearning on these episodes as meta-learning or meta-training (as is sometimes done in the literature)\ncan be confusing. So for the sake of clarity, we will refer to the content of episodes as support and\nquery sets, and to the process of iterating over the training episodes simply as training.\n2.2\nPROTOTYPICAL NETWORKS\nPrototypical Network (Snell et al., 2017) is a few-shot learning model that has the virtue of being\nsimple and yet obtaining state-of-the-art performance. At a high-level, it uses the support set S to\nextract a prototype vector from each class, and classiﬁes the inputs in the query set based on their\ndistance to the prototype of each class.\nMore precisely, Prototypical Networks learn an embedding function h(x), parameterized as a neural\nnetwork, that maps examples into a space where examples from the same class are close and those\nfrom different classes are far. All parameters of Prototypical Networks lie in the embedding function.\nTo compute the prototype pc of each class c, a per-class average of the embedded examples is\nperformed:\npc =\nP\ni h(xi)zi,c\nP\ni zi,c\n, where zi,c = 1[yi = c].\n(1)\nThese prototypes deﬁne a predictor for the class of any new (query) example x∗, which assigns a\nprobability over any class c based on the distances between x∗and each prototype, as follows:\np(c|x∗, {pc}) =\nexp(−||h(x∗) −pc||2\n2)\nP\nc′ exp(−||h(x∗) −pc′||2\n2) .\n(2)\nThe loss function used to update Prototypical Networks for a given training episode is then simply\nthe average negative log-probability of the correct class assignments, for all query examples:\n−1\nT\nX\ni\nlog p(y∗\ni |x∗\ni , {pc}) .\n(3)\nTraining proceeds by minimizing the average loss, iterating over training episodes and performing a\ngradient descent update for each.\nGeneralization performance is measured on test set episodes, which contain images from classes\nin Ctest instead of Ctrain. For each test episode, we use the predictor produced by the Prototypical\nNetwork for the provided support set S to classify each of query input x∗into the most likely class\nˆy = argmaxc p(c|x∗, {pc}).\n3\nSEMI-SUPERVISED FEW-SHOT LEARNING\nWe now deﬁne the semi-supervised setting considered in this work for few-shot learning.\nThe training set is denoted as a tuple of labeled and unlabeled examples: (S, R). The labeled portion\nis the usual support set S of the few-shot learning literature, containing a list of tuples of inputs and\ntargets. In addition to classic few-shot learning, we introduce an unlabeled set R containing only\ninputs: R = {˜x1, ˜x2, . . . , ˜xM}. As in the purely supervised setting, our models are trained to\nperform well when predicting the labels for the examples in the episode’s query set Q. Figure 2\nshows a visualization of training and test episodes.\n3\n\n\nPublished as a conference paper at ICLR 2018\n1\n2\n3\n?\n?\n1\n2\n3\n?\n?\nTraining\nTesting\nQuery Set\nQuery Set\nSupport Set\nSupport Set\nUnlabeled Set\nUnlabeled Set\n?\n?\nFigure 2: Example of the semi-supervised few-shot learning setup. Training involves iterating through training\nepisodes, consisting of a support set S, an unlabeled set R, and a query set Q. The goal is to use the labeled\nitems (shown with their numeric class label) in S and the unlabeled items in R within each episode to generalize\nto good performance on the corresponding query set. The unlabeled items in R may either be pertinent to the\nclasses we are considering (shown above with green plus signs) or they may be distractor items which belong\nto a class that is not relevant to the current episode (shown with red minus signs). However note that the model\ndoes not actually have ground truth information as to whether each unlabeled example is a distractor or not; the\nplus/minus signs are shown only for illustrative purposes. At test time, we are given new episodes consisting\nof novel classes not seen during training that we use to evaluate the meta-learning method.\n3.1\nSEMI-SUPERVISED PROTOTYPICAL NETWORKS\nIn their original formulation, Prototypical Networks do not specify a way to leverage the unlabeled\nset R. In what follows, we now propose various extensions that start from the basic deﬁnition\nof prototypes pc and provide a procedure for producing reﬁned prototypes ˜pc using the unlabeled\nexamples in R.\nBefore Reﬁnement\nAfter Reﬁnement\nFigure 3: Left: The prototypes are initialized based on the mean\nlocation of the examples of the corresponding class, as in ordinary\nPrototypical Networks. Support, unlabeled, and query examples have\nsolid, dashed, and white colored borders respectively. Right: The reﬁned\nprototypes obtained by incorporating the unlabeled examples, which\nclassiﬁes all query examples correctly.\nAfter the reﬁned prototypes are\nobtained, each of these models\nis trained with the same loss\nfunction for ordinary Prototypi-\ncal Networks of Equation 3, but\nreplacing pc with ˜pc. That is,\neach query example is classi-\nﬁed into one of the N classes\nbased on the proximity of its\nembedded position with the cor-\nresponding reﬁned prototypes,\nand the average negative log-\nprobability of the correct classi-\nﬁcation is used for training.\n3.1.1\nPROTOTYPICAL NETWORKS WITH SOFT k-MEANS\nWe ﬁrst consider a simple way of leveraging unlabeled examples for reﬁning prototypes, by\ntaking inspiration from semi-supervised clustering. Viewing each prototype as a cluster center,\nthe reﬁnement process could attempt to adjust the cluster locations to better ﬁt the examples in both\nthe support and unlabeled sets. Under this view, cluster assignments of the labeled examples in the\nsupport set are considered known and ﬁxed to each example’s label. The reﬁnement process must\ninstead estimate the cluster assignments of the unlabeled examples and adjust the cluster locations\n(the prototypes) accordingly.\n4\n\n\nPublished as a conference paper at ICLR 2018\nOne natural choice would be to borrow from the inference performed by soft k-means. We prefer\nthis version of k-means over hard assignments since hard assignments would make the inference\nnon-differentiable. We start with the regular Prototypical Network’s prototypes pc (as speciﬁed in\nEquation 1) as the cluster locations. Then, the unlabeled examples get a partial assignment (˜zj,c) to\neach cluster based on their Euclidean distance to the cluster locations. Finally, reﬁned prototypes\nare obtained by incorporating these unlabeled examples.\nThis process can be summarized as follows:\n˜pc =\nP\ni h(xi)zi,c + P\nj h(˜xj)˜zj,c\nP\ni zi,c + P\nj ˜zj,c\n, where ˜zj,c =\nexp\n\u0000−||h(˜xj) −pc||2\n2\n\u0001\nP\nc′ exp (−||h(˜xj) −pc′||2\n2)\n(4)\nPredictions of each query input’s class is then modeled as in Equation 2, but using the reﬁned\nprototypes ˜pc.\nWe could perform several iterations of reﬁnement, as is usual in k-means. However, we have\nexperimented with various number of iterations and found results to not improve beyond a single\nreﬁnement step.\n3.1.2\nPROTOTYPICAL NETWORKS WITH SOFT k-MEANS WITH A DISTRACTOR CLUSTER\nThe soft k-means approach described above implicitly assumes that each unlabeled example belongs\nto either one of the N classes in the episode. However, it would be much more general to not make\nthat assumption and have a model robust to the existence of examples from other classes, which we\nrefer to as distractor classes. For example, such a situation would arise if we wanted to distinguish\nbetween pictures of unicycles and scooters, and decided to add an unlabeled set by downloading\nimages from the web. It then would not be realistic to assume that all these images are of unicycles\nor scooters. Even with a focused search, some may be from similar classes, such as bicycle.\nSince soft k-means distributes its soft assignments across all classes, distractor items could be\nharmful and interfere with the reﬁnement process, as prototypes would be adjusted to also partially\naccount for these distractors. A simple way to address this is to add an additional cluster whose\npurpose is to capture the distractors, thus preventing them from polluting the clusters of the classes\nof interest:\npc =\n( P\ni h(xi)zi,c\nP\ni zi,c\nfor c = 1...N\n0\nfor c = N + 1\n(5)\nHere we take the simplifying assumption that the distractor cluster has a prototype centered at the\norigin. We also consider introducing length-scales rc to represent variations in the within-cluster\ndistances, speciﬁcally for the distractor cluster:\n˜zj,c =\nexp\n\u0010\n−1\nr2c ||˜xj −pc||2\n2 −A(rc)\n\u0011\nP\nc′ exp\n\u0010\n−1\nr2c ||˜xj −pc′||2\n2 −A(rc′)\n\u0011, where A(r) = 1\n2 log(2π) + log(r)\n(6)\nFor simplicity, we set r1...N to 1 in our experiments, and only learn the length-scale of the distractor\ncluster rN+1.\n3.1.3\nPROTOTYPICAL NETWORKS WITH SOFT k-MEANS AND MASKING\nModeling distractor unlabeled examples with a single cluster is likely too simplistic. Indeed, it\nis inconsistent with our assumption that each cluster corresponds to one class, since distractor\nexamples may very well cover more than a single natural object category. Continuing with our\nunicycles and bicycles example, our web search for unlabeled images could accidentally include\nnot only bicycles, but other related objects such as tricycles or cars. This was also reﬂected in our\nexperiments, where we constructed the episode generating process so that it would sample distractor\nexamples from multiple classes.\nTo address this problem, we propose an improved variant: instead of capturing distractors with a\nhigh-variance catch-all cluster, we model distractors as examples that are not within some area of\nany of the legitimate class prototypes. This is done by incorporating a soft-masking mechanism on\n5\n\n\nPublished as a conference paper at ICLR 2018\nthe contribution of unlabeled examples. At a high level, we want unlabeled examples that are closer\nto a prototype to be masked less than those that are farther.\nMore speciﬁcally, we modify the soft k-means reﬁnement as follows.\nWe start by computing\nnormalized distances ˜dj,c between examples ˜xj and prototypes pc:\n˜dj,c =\ndj,c\n1\nM\nP\nj dj,c\n, where dj,c = ||h(˜xj) −pc||2\n2\n(7)\nThen, soft thresholds βc and slopes γc are predicted for each prototype, by feeding to a small neural\nnetwork various statistics of the normalized distances for the prototype:\n[βc, γc] = MLP\n\u0012\u0014\nmin\nj ( ˜dj,c), max\nj ( ˜dj,c), var\nj ( ˜dj,c), skew\nj\n( ˜dj,c), kurt\nj ( ˜dj,c)\n\u0015\u0013\n(8)\nThis allows each threshold to use information on the amount of intra-cluster variation to determine\nhow aggressively it should cut out unlabeled examples.\nThen, soft masks mj,c for the contribution of each example to each prototype are computed, by\ncomparing to the threshold the normalized distances, as follows:\n˜pc =\nP\ni h(xi)zi,c + P\nj h(˜xj)˜zj,cmj,c\nP\ni zi,c + P\nj ˜zj,cmj,c\n, where mj,c = σ\n\u0010\n−γc\n\u0010\n˜dj,c −βc\n\u0011\u0011\n(9)\nwhere σ(·) is the sigmoid function.\nWhen training with this reﬁnement process, the model can now use its MLP in Equation 8 to learn\nto include or ignore entirely certain unlabeled examples. The use of soft masks makes this process\nentirely differentiable2. Finally, much like for regular soft k-means (with or without a distractor\ncluster), while we could recursively repeat the reﬁnement for multiple steps, we found a single step\nto perform well enough.\n4\nRELATED WORK\nWe summarize here the most relevant work from the literature on few-shot learning, semi-supervised\nlearning and clustering.\nThe best performing methods for few-shot learning use the episodic training framework prescribed\nby meta-learning. The approach within which our work falls is that of metric learning methods.\nPrevious work in metric-learning for few-shot-classiﬁcation includes Deep Siamese Networks (Koch\net al., 2015), Matching Networks (Vinyals et al., 2016), and Prototypical Networks (Snell et al.,\n2017), which is the model we extend to the semi-supervised setting in our work. The general idea\nhere is to learn an embedding function that embeds examples belonging to the same class close\ntogether while keeping embeddings from separate classes far apart. Distances between embeddings\nof items from the support set and query set are then used as a notion of similarity to do classiﬁcation.\nLastly, closely related to our work with regard to extending the few-shot learning setting, Bachman\net al. (2017) employ Matching Networks in an active learning framework where the model has a\nchoice of which unlabeled item to add to the support set over a certain number of time steps before\nclassifying the query set. Unlike our setting, their meta-learning agent can acquire ground-truth\nlabels from the unlabeled set, and they do not use distractor examples.\nOther meta-learning approaches to few-shot learning include learning how to use the support set\nto update a learner model so as to generalize to the query set. Recent work has involved learning\neither the weight initialization and/or update step that is used by a learner neural network (Ravi\n& Larochelle, 2017; Finn et al., 2017). Another approach is to train a generic neural architecture\nsuch as a memory-augmented recurrent network (Santoro et al., 2016) or a temporal convolutional\nnetwork (Mishra et al., 2017) to sequentially process the support set and perform accurate\npredictions of the labels of the query set examples. These other methods are also competitive for\nfew-shot learning, but we chose to extend Prototypical Networks in this work for its simplicity and\nefﬁciency.\n2We stop gradients from passing through the computation of the statistics in Equation 8, to avoid potential\nnumerical instabilities.\n6\n\n\nPublished as a conference paper at ICLR 2018\nAs for the literature on semi-supervised learning, while it is quite vast (Zhu, 2005; Chapelle et al.,\n2010), the most relevant category to our work is related to self-training (Yarowsky, 1995; Rosenberg\net al., 2005). Here, a classiﬁer is ﬁrst trained on the initial training set. The classiﬁer is then used\nto classify unlabeled items, and the most conﬁdently predicted unlabeled items are added to the\ntraining set with the prediction of the classiﬁer as the assumed label. This is similar to our soft\nk-Means extension to Prototypical Networks. Indeed, since the soft assignments (Equation 4) match\nthe regular Prototypical Network’s classiﬁer output for new inputs (Equation 2), then the reﬁnement\ncan be thought of re-feeding to a Prototypical Network a new support set augmented with (soft)\nself-labels from the unlabeled set.\nOur algorithm is also related to transductive learning (Vapnik, 1998; Joachims, 1999; Fu et al., 2015),\nwhere the base classiﬁer gets reﬁned by seeing the unlabeled examples. In practice, one could use\nour method in a transductive setting where the unlabeled set is the same as the query set; however,\nhere to avoid our model memorizing labels of the unlabeled set during the meta-learning procedure,\nwe split out a separate unlabeled set that is different from the query set.\nIn addition to the original k-Means method (Lloyd, 1982), the most related work to our setup\ninvolving clustering algorithms considers applying k-Means in the presence of outliers (Hautamäki\net al., 2005; Chawla & Gionis, 2013; Gupta et al., 2017). The goal here is to correctly discover and\nignore the outliers so that they do not wrongly shift the cluster locations to form a bad partition of\nthe true data. This objective is also important in our setup as not ignoring outliers (or distractors)\nwill wrongly shift the prototypes and negatively inﬂuence classiﬁcation performance.\nOur contribution to the semi-supervised learning and clustering literature is to go beyond the\nclassical setting of training and evaluating within a single dataset, and consider the setting where\nwe must learn to transfer from a set of training classes Ctrain to a new set of test classes Ctest.\n5\nEXPERIMENTS\n5.1\nDATASETS\nWe evaluate the performance of our model on three datasets: two benchmark few-shot classiﬁcation\ndatasets and a novel large-scale dataset that we hope will be useful for future few-shot learning work.\nOmniglot (Lake et al., 2011) is a dataset of 1,623 handwritten characters from 50 alphabets. Each\ncharacter was drawn by 20 human subjects. We follow the few-shot setting proposed by Vinyals\net al. (2016), in which the images are resized to 28 × 28 pixels and rotations in multiples of 90◦are\napplied, yielding 6,492 classes in total. These are split into 4,112 training classes, 688 validation\nclasses, and 1,692 testing classes.\nminiImageNet (Vinyals et al., 2016) is a modiﬁed version of the ILSVRC-12 dataset (Russakovsky\net al., 2015), in which 600 images for each of 100 classes were randomly chosen to be part of the\ndataset. We rely on the class split used by Ravi & Larochelle (2017). These splits use 64 classes for\ntraining, 16 for validation, and 20 for test. All images are of size 84 × 84 pixels.\ntieredImageNet is our proposed dataset for few-shot classiﬁcation.\nLike miniImagenet, it is a\nsubset of ILSVRC-12. However, tieredImageNet represents a larger subset of ILSVRC-12 (608\nclasses rather than 100 for miniImageNet). Analogous to Omniglot, in which characters are grouped\ninto alphabets, tieredImageNet groups classes into broader categories corresponding to higher-level\nnodes in the ImageNet (Deng et al., 2009) hierarchy. There are 34 categories in total, with each\ncategory containing between 10 and 30 classes. These are split into 20 training, 6 validation and 8\ntesting categories (details of the dataset can be found in the supplementary material). This ensures\nthat all of the training classes are sufﬁciently distinct from the testing classes, unlike miniImageNet\nand other alternatives such as randImageNet proposed by Vinyals et al. (2016). For example, “pipe\norgan” is a training class and “electric guitar” is a test class in the Ravi & Larochelle (2017) split\nof miniImagenet, even though they are both musical instruments. This scenario would not occur in\ntieredImageNet since “musical instrument” is a high-level category and as such is not split between\ntraining and test classes. This represents a more realistic few-shot learning scenario since in general\nwe cannot assume that test classes will be similar to those seen in training. Additionally, the tiered\nstructure of tieredImageNet may be useful for few-shot learning approaches that can take advantage\nof hierarchical relationships between classes. We leave such interesting extensions for future work.\n7\n\n\nPublished as a conference paper at ICLR 2018\nModels\nAcc.\nAcc. w/ D\nSupervised\n94.62 ± 0.09\n94.62 ± 0.09\nSemi-Supervised Inference\n97.45 ± 0.05\n95.08 ± 0.09\nSoft k-Means\n97.25 ± 0.10\n95.01 ± 0.09\nSoft k-Means+Cluster\n97.68 ± 0.07\n97.17 ± 0.04\nMasked Soft k-Means\n97.52 ± 0.07\n97.30 ± 0.08\nTable 1: Omniglot 1-shot classiﬁcation results. In this table as well as those below “w/ D” denotes “with\ndistractors”, where the unlabeled images contain irrelevant classes.\n5.2\nADAPTING THE DATASETS FOR SEMI-SUPERVISED LEARNING\nFor each dataset, we ﬁrst create an additional split to separate the images of each class into disjoint\nlabeled and unlabeled sets. For Omniglot and tieredImageNet we sample 10% of the images of\neach class to form the labeled split. The remaining 90% can only be used in the unlabeled portion\nof episodes. For miniImageNet we use 40% of the data for the labeled split and the remaining\n60% for the unlabeled, since we noticed that 10% was too small to achieve reasonable performance\nand avoid overﬁtting. We report the average classiﬁcation scores over 10 random splits of labeled\nand unlabeled portions of the training set, with uncertainty computed in standard error (standard\ndeviation divided by the square root of the total number of splits).\nWe would like to emphasize that due to this labeled/unlabeled split, we are using strictly less label\ninformation than in the previously-published work on these datasets. Because of this, we do not\nexpect our results to match the published numbers, which should instead be interpreted as an upper-\nbound for the performance of the semi-supervised models deﬁned in this work.\nEpisode construction then is performed as follows. For a given dataset, we create a training episode\nby ﬁrst sampling N classes uniformly at random from the set of training classes Ctrain. We then\nsample K images from the labeled split of each of these classes to form the support set, and M\nimages from the unlabeled split of each of these classes to form the unlabeled set. Optionally,\nwhen including distractors, we additionally sample H other classes from the set of training classes\nand M images from the unlabeled split of each to act as the distractors. These distractor images\nare added to the unlabeled set along with the unlabeled images of the N classes of interest (for a\ntotal of MN + MH unlabeled images). The query portion of the episode is comprised of a ﬁxed\nnumber of images from the labeled split of each of the N chosen classes. Test episodes are created\nanalogously, but with the N classes (and optionally the H distractor classes) sampled from Ctest.\nIn the experiments reported here we used H = N = 5, i.e. 5 classes for both the labeled classes\nand the distractor classes. We used M = 5 for training and M = 20 for testing in most cases,\nthus measuring the ability of the models to generalize to a larger unlabeled set size. Details of the\ndataset splits, including the speciﬁc classes assigned to train/validation/test sets, can be found in\nAppendices A and B.\nIn each dataset we compare our three semi-supervised models with two baselines. The ﬁrst baseline,\nreferred to as “Supervised” in our tables, is an ordinary Prototypical Network that is trained in a\npurely supervised way on the labeled split of each dataset. The second baseline, referred to as\n“Semi-Supervised Inference”, uses the embedding function learned by this supervised Prototypical\nNetwork, but performs semi-supervised reﬁnement of the prototypes at test time using a step of Soft\nk-Means reﬁnement. This is to be contrasted with our semi-supervised models that perform this\nreﬁnement both at training time and at test time, therefore learning a different embedding function.\nWe evaluate each model in two settings: one where all unlabeled examples belong to the classes of\ninterest, and a more challenging one that includes distractors. Details of the model hyperparameters\ncan be found in Appendix D and our online repository.3\n5.3\nRESULTS\nResults for Omniglot, miniImageNet and tieredImageNet are given in Tables 1, 2 and 5, respectively,\nwhile Figure 4 shows the performance of our models on tieredImageNet (our largest dataset) using\n3 Code available at https://github.com/renmengye/few-shot-ssl-public\n8\n\n\nPublished as a conference paper at ICLR 2018\nModels\n1-shot Acc.\n5-shot Acc.\n1-shot Acc w/ D\n5-shot Acc. w/ D\nSupervised\n43.61 ± 0.27\n59.08 ± 0.22\n43.61 ± 0.27\n59.08 ± 0.22\nSemi-Supervised Inference\n48.98 ± 0.34\n63.77 ± 0.20\n47.42 ± 0.33\n62.62 ± 0.24\nSoft k-Means\n50.09 ± 0.45\n64.59 ± 0.28\n48.70 ± 0.32\n63.55 ± 0.28\nSoft k-Means+Cluster\n49.03 ± 0.24\n63.08 ± 0.18\n48.86 ± 0.32\n61.27 ± 0.24\nMasked Soft k-Means\n50.41 ± 0.31\n64.39 ± 0.24\n49.04 ± 0.31\n62.96 ± 0.14\nTable 2: miniImageNet 1/5-shot classiﬁcation results.\nModels\n1-shot Acc.\n5-shot Acc.\n1-shot Acc. w/ D\n5-shot Acc. w/ D\nSupervised\n46.52 ± 0.52\n66.15 ± 0.22\n46.52 ± 0.52\n66.15 ± 0.22\nSemi-Supervised Inference\n50.74 ± 0.75\n69.37 ± 0.26\n48.67 ± 0.60\n67.46 ± 0.24\nSoft k-Means\n51.52 ± 0.36\n70.25 ± 0.31\n49.88 ± 0.52\n68.32 ± 0.22\nSoft k-Means+Cluster\n51.85 ± 0.25\n69.42 ± 0.17\n51.36 ± 0.31\n67.56 ± 0.10\nMasked Soft k-Means\n52.39 ± 0.44\n69.88 ± 0.20\n51.38 ± 0.38\n69.08 ± 0.25\nTable 3: tieredImageNet 1/5-shot classiﬁcation results.\ndifferent values for M (number of items in the unlabeled set per class). Additional results comparing\nthe ProtoNet model to various baselines on these datasets, and analysis of the performance of the\nMasked Soft k-Means model can be found in Appendix C.\nAcross all three benchmarks, at least one of our proposed models outperforms the baselines,\ndemonstrating the effectiveness of our semi-supervised meta-learning procedure.\nIn the non-\ndistractor settings, all three proposed models outperform the baselines in almost all the experiments,\nwithout a clear winner between the three models across the datasets and shot numbers. In the\nscenario where training and testing includes distractors, Masked Soft k-Means shows the most\nrobust performance across all three datasets, attaining the best results in each case but one. In\nfact this model reaches performance that is close to the upper bound based on the results without\ndistractors.\nFrom Figure 4, we observe clear improvements in test accuracy when the number of items in the\nunlabeled set per class grows from 0 to 25. These models were trained with M = 5 and thus are\nshowing an ability to extrapolate in generalization. This conﬁrms that, through meta-training, the\nmodels learn to acquire a better representation that is improved by semi-supervised reﬁnement.\n6\nCONCLUSION\nIn this work, we propose a novel semi-supervised few-shot learning paradigm, where an unlabeled\nset is added to each episode. We also extend the setup to more realistic situations where the unlabeled\nset has novel classes distinct from the labeled classes. To address the problem that current few-\nshot classiﬁcation datasets are too small for a labeled vs. unlabeled split and also lack hierarchical\nlevels of labels, we introduce a new dataset, tieredImageNet. We propose several novel extensions\nof Prototypical Networks, and they show consistent improvements under semi-supervised settings\ncompared to our baselines. As future work, we are working on incorporating fast weights (Ba\net al., 2016; Finn et al., 2017) into our framework so that examples can have different embedding\nrepresentations given the contents in the episode.\nAcknowledgement\nSupported by grants from NSERC, Samsung, and the Intelligence Advanced\nResearch Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC)\ncontract number D16PC00003. The U.S. Government is authorized to reproduce and distribute\nreprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer:\nThe views and conclusions contained herein are those of the authors and should not be interpreted\nas necessarily representing the ofﬁcial policies or endorsements, either expressed or implied, of\nIARPA, DoI/IBC, or the U.S. Government.\n9\n\n\nPublished as a conference paper at ICLR 2018\n44\n46\n48\n50\n52\n54\nTest Acc. (%)\n1-shot w/o Distractors\nSupervised\nSoft K-Means\nSoft K-Means+Cluster\nMasked Soft K-Means\n1-shot w/ Distractors\n0\n1\n2\n5\n10\n15\n20\n25\nNumber of Unlabeled Items Per Class\n65\n66\n67\n68\n69\n70\n71\nTest Acc. (%)\n5-shot w/o Distractors\n0\n1\n2\n5\n10\n15\n20\n25\nNumber of Unlabeled Items Per Class\n5-shot w/ Distractors\nFigure 4: Model Performance on tieredImageNet with different numbers of unlabeled items during test time.\nREFERENCES\nJimmy Ba, Geoffrey E. Hinton, Volodymyr Mnih, Joel Z. Leibo, and Catalin Ionescu. Using fast\nweights to attend to the recent past.\nIn Advances in Neural Information Processing Systems\n29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016,\nBarcelona, Spain, pp. 4331–4339, 2016.\nPhilip Bachman, Alessandro Sordoni, and Adam Trischler. Learning algorithms for active learning.\n2017.\nOlivier Chapelle, Bernhard Schölkopf, and Alexander Zien. Semi-Supervised Learning. The MIT\nPress, 1st edition, 2010. ISBN 0262514125, 9780262514125.\nSanjay Chawla and Aristides Gionis.\nk-means–: A uniﬁed approach to clustering and outlier\ndetection. In Proceedings of the 2013 SIAM International Conference on Data Mining, pp. 189–\n197. SIAM, 2013.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009.\nIEEE Conference on, pp. 248–255. IEEE, 2009.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation\nof deep networks. In 34th International Conference on Machine Learning, 2017.\nYanwei Fu, Timothy M. Hospedales, Tao Xiang, and Shaogang Gong. Transductive multi-view\nzero-shot learning. IEEE Trans. Pattern Anal. Mach. Intell., 37(11):2332–2345, 2015.\nShalmoli Gupta, Ravi Kumar, Kefu Lu, Benjamin Moseley, and Sergei Vassilvitskii. Local search\nmethods for k-means with outliers. Proceedings of the VLDB Endowment, 10(7):757–768, 2017.\nVille Hautamäki, Svetlana Cherednichenko, Ismo Kärkkäinen, Tomi Kinnunen, and Pasi Fränti.\nImproving k-means by outlier removal. In Scandinavian Conference on Image Analysis, pp. 978–\n987. Springer, 2005.\nSepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent.\nIn International Conference on Artiﬁcial Neural Networks, pp. 87–94. Springer, 2001.\nThorsten Joachims. Transductive inference for text classiﬁcation using support vector machines. In\nProceedings of the Sixteenth International Conference on Machine Learning, 1999.\n10\n\n\nPublished as a conference paper at ICLR 2018\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nGregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot\nimage recognition. In ICML Deep Learning Workshop, volume 2, 2015.\nBrenden M. Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua B. Tenenbaum. One shot learning\nof simple visual concepts. In Proceedings of the 33th Annual Meeting of the Cognitive Science\nSociety, CogSci 2011, Boston, Massachusetts, USA, July 20-23, 2011, 2011.\nStuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):\n129–137, 1982.\nNikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. Meta-learning with temporal\nconvolutions. CoRR, abs/1707.03141, 2017.\nSachin Ravi and Hugo Larochelle.\nOptimization as a model for few-shot learning.\nIn 5th\nInternational Conference on Learning Representations, 2017.\nChuck Rosenberg, Martial Hebert, and Henry Schneiderman. Semi-supervised self-training of object\ndetection models. 2005.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\nrecognition challenge. International Journal of Computer Vision, 115(3):211–252, 2015.\nAdam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy P. Lillicrap. One-\nshot learning with memory-augmented neural networks. In 33rd International Conference on\nMachine Learning, 2016.\nJake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical networks for few-shot learning. In\nAdvances in Neural Information Processing Systems 30, 2017.\nSebastian Thrun. Lifelong learning algorithms. In Learning to learn, pp. 181–209. Springer, 1998.\nV.N. Vapnik. Statistical Learning Theory. Wiley, 1998. ISBN 9788126528929.\nOriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching\nnetworks for one shot learning. In Advances in Neural Information Processing Systems 29, pp.\n3630–3638, 2016.\nDavid Yarowsky.\nUnsupervised word sense disambiguation rivaling supervised methods.\nIn\nProceedings of the 33rd annual meeting on Association for Computational Linguistics, pp. 189–\n196. Association for Computational Linguistics, 1995.\nXiaojin Zhu. Semi-supervised learning literature survey. 2005.\n11\n\n\nPublished as a conference paper at ICLR 2018\nA\nOMNIGLOT DATASET DETAILS\nWe used the following split details for experiments on Omniglot dataset. This is the same train/test\nsplit as (Vinyals et al., 2016), but we created our own validation split for selecting hyper-parameters.\nModels are trained on the train split only.\nTrain\nAlphabets:\nAlphabet_of_the_Magi,\nAngelic,\nAnglo-Saxon_Futhorc,\nArcadian,\nAsomtavruli_(Georgian),\nAtemayar_Qelisayer,\nAtlantean,\nAurek-Besh,\nAvesta,\nBalinese,\nBlackfoot_(Canadian_Aboriginal_Syllabics),\nBraille,\nBurmese_(Myanmar),\nCyrillic,\nFuturama,\nGe_ez,\nGlagolitic,\nGrantha,\nGreek,\nGujarati,\nGurmukhi\n(character\n01-41),\nInuktitut_(Canadian_Aboriginal_Syllabics), Japanese_(hiragana), Japanese_(katakana), Korean,\nLatin,\nMalay_(Jawi_-_Arabic),\nN_Ko,\nOjibwe_(Canadian_Aboriginal_Syllabics),\nSanskrit,\nSyriac_(Estrangelo), Tagalog, Tiﬁnagh\nValidation Alphabets: Armenian, Bengali, Early_Aramaic, Hebrew, Mkhedruli_(Geogian)\nTest Alphabets: Gurmukhi (character 42-45), Kannada, Keble, Malayalam, Manipuri, Mongolian,\nOld_Church_Slavonic_(Cyrillic), Oriya, Sylheti, Syriac_(Serto), Tengwar, Tibetan, ULOG\nB\ntieredIMAGENET DATASET DETAILS\nEach high-level category in tieredImageNet contains between 10 and 30 ILSVRC-12 classes (17.8\non average). In the ImageNet hierarchy, some classes have multiple parent nodes. Therefore, classes\nbelonging to more than one category were removed from the dataset to ensure separation between\ntraining and test categories. Test categories were chosen to reﬂect various levels of separation\nbetween training and test classes. Some test categories (such as “working dog”) are fairly similar to\ntraining categories, whereas others (such as “geological formation”) are quite different. The list of\ncategories is shown below and statistics of the dataset can be found in Table 4. A visualization of\nthe categories according to the ImageNet hierarchy is shown in Figure 5. The full list of classes per\ncategory will also be made public, however for the sake of brevity we do not include it here.\nTable 4: Statistics of the tieredImageNet dataset.\nTrain\nVal\nTest\nTotal\nCategories\n20\n6\n8\n34\nClasses\n351\n97\n160\n608\nImages\n448,695\n124,261\n206,209\n779,165\nTrain Categories: n02087551 (hound, hound dog), n02092468 (terrier), n02120997 (feline,\nfelid), n02370806 (ungulate, hoofed mammal), n02469914 (primate), n01726692 (snake, ser-\npent, ophidian), n01674216 (saurian), n01524359 (passerine, passeriform bird), n01844917\n(aquatic bird), n04081844 (restraint, constraint), n03574816 (instrument), n03800933 (mu-\nsical instrument, instrument), n03125870 (craft), n04451818 (tool), n03414162 (game\nequipment), n03278248 (electronic equipment), n03419014 (garment), n03297735 (estab-\nlishment), n02913152 (building, ediﬁce), n04014297 (protective covering, protective cover,\nprotection).\nValidation Categories: n02098550 (sporting dog, gun dog), n03257877 (durables, durable\ngoods, consumer durables), n03405265 (furnishing), n03699975 (machine), n03738472\n(mechanism), n03791235 (motor vehicle, automotive vehicle),\nTest Categories: n02103406 (working dog), n01473806 (aquatic vertebrate), n02159955 (in-\nsect), n04531098 (vessel), n03839993 (obstruction, obstructor, obstructer, impediment, imped-\nimenta), n09287968 (geological formation, formation), n00020090 (substance), n15046900\n(solid).\n12\n\n\nPublished as a conference paper at ICLR 2018\nOmniglot\nminiImageNet\ntieredImageNet\nModels\n1-shot\n1-shot\n5-shot\n1-shot\n5-shot\n1-NN Pixel\n40.39 ± 0.36\n26.74 ± 0.48\n31.43 ± 0.51\n26.55 ± 0.50\n30.79 ± 0.53\n1-NN CNN rnd\n59.55 ± 0.46\n24.03 ± 0.38\n27.54 ± 0.42\n25.49 ± 0.45\n30.01 ± 0.47\n1-NN CNN pre\n52.53 ± 0.51\n32.90 ± 0.58\n40.79 ± 0.76\n32.76 ± 0.66\n40.26 ± 0.67\nLR Pixel\n49.15 ± 0.39\n24.50 ± 0.41\n33.33 ± 0.68\n25.70 ± 0.46\n36.30 ± 0.62\nLR CNN rnd\n57.80 ± 0.45\n24.10 ± 0.50\n28.40 ± 0.42\n26.55 ± 0.48\n32.51 ± 0.52\nLR CNN pre\n48.49 ± 0.47\n30.28 ± 0.54\n40.27 ± 0.59\n34.52 ± 0.68\n43.58 ± 0.72\nProtoNet\n94.62 ± 0.09\n43.61 ± 0.27\n59.08 ± 0.22\n46.52 ± 0.32\n66.15 ± 0.34\nTable 5: Few-shot learning baseline results using labeled/unlabeled splits. Baselines either takes inputs directly\nfrom the pixel space or use a CNN to extract features. “rnd” denotes using a randomly initialized CNN, and\n“pre” denotes using a CNN that is pretrained for supervised classiﬁcation for all training classes.\nC\nEXTRA EXPERIMENTAL RESULTS\nC.1\nFEW-SHOT CLASSIFICATION BASELINES\nWe provide baseline results on few-shot classiﬁcation using 1-nearest neighbor and logistic\nregression with either pixel inputs or CNN features. Compared with the baselines, Regular ProtoNet\nperforms signiﬁcantly better on all three few-shot classiﬁcation datasets.\nC.2\nNUMBER OF UNLABELED ITEMS\nFigure 6 shows test accuracy values with different number of unlabeled items during test time.\nFigure 7 shows our mask output value distribution of the Masked Soft k-Means model on Omniglot.\nThe mask values have a bi-modal distribution, corresponding to distractor and non-distractor items.\nD\nHYPERPARAMETER DETAILS\nFor Omniglot, we adopted the best hyperparameter settings found for ordinary Prototypical\nNetworks in Snell et al. (2017). In these settings, the learning rate was set to 1e-3, and cut in half\nevery 2K updates starting at update 2K. We trained for a total of 20K updates. For miniImagenet and\ntieredImageNet, we trained with a starting learning rate of 1e-3, which we also decayed. We started\nthe decay after 25K updates, and every 25K updates thereafter we cut it in half. We trained for a\ntotal of 200K updates. We used ADAM (Kingma & Ba, 2014) for the optimization of our models.\nFor the MLP used in the Masked Soft k-Means model, we use a single hidden layer with 20 hidden\nunits with a tanh non-linearity for all 3 datasets. We did not tune the hyparameters of this MLP so\nbetter performance may be attained with a more rigorous hyperparameter search.\n13\n\n\nPublished as a conference paper at ICLR 2018\nTrain Categories\nValidation Categories\nTest Categories\nn15046900\nsolid\n[22 classes]\n(test)\nn04170037\nself-propelled vehicle\nn03791235\nmotor vehicle, automotive vehicle\n[22 classes]\n(validation)\nn02103406\nworking dog\n[30 classes]\n(test)\nn01661818\ndiapsid, diapsid reptile\nn01726692\nsnake, serpent, ophidian\n[17 classes]\n(train)\nn01674216\nsaurian\n[11 classes]\n(train)\nn00003553\nwhole, unit\nn00021939\nartifact, artefact\nn00004258\nliving thing, animate thing\nn03699975\nmachine\n[12 classes]\n(validation)\nn01466257\nchordate\nn01471682\nvertebrate, craniate\nn00020090\nsubstance\n[22 classes]\n(test)\nn04576211\nwheeled vehicle\nn03800933\nmusical instrument, instrument\n[26 classes]\n(train)\nn03278248\nelectronic equipment\n[11 classes]\n(train)\nn01524359\npasserine, passeriform bird\n[11 classes]\n(train)\nn02083346\ncanine, canid\nn02084071\ndog, domestic dog, Canis familiaris\nn03122748\ncovering\nn03051540\nclothing, article of clothing, vesture, wear,\nn04014297\nprotective covering, protective cover, protect\n[24 classes]\n(train)\nn01861778\nmammal, mammalian\nn01473806\naquatic vertebrate\n[16 classes]\n(test)\nn01661091\nreptile, reptilian\nn01503061\nbird\nn02087551\nhound, hound dog\n[19 classes]\n(train)\nn04341686\nstructure, construction\nn03839993\nobstruction, obstructor, obstructer, impedimen\n[10 classes]\n(test)\nn02913152\nbuilding, ediﬁce\n[14 classes]\n(train)\nn03297735\nestablishment\n[10 classes]\n(train)\nn03294048\nequipment\nn03414162\ngame equipment\n[12 classes]\n(train)\nn09287968\ngeological formation, formation\n[10 classes]\n(test)\nn03125870\ncraft\n[20 classes]\n(train)\nn03738472\nmechanism\n[12 classes]\n(validation)\nn03257877\ndurables, durable goods, consumer durables\n[12 classes]\n(validation)\nn01886756\nplacental, placental mammal, eutherian, euther\nn02370806\nungulate, hoofed mammal\n[17 classes]\n(train)\nn02075296\ncarnivore\nn02469914\nprimate\n[20 classes]\n(train)\nn02159955\ninsect\n[27 classes]\n(test)\nn00004475\norganism, being\nn00015388\nanimal, animate being, beast, brute, creature,\nn00001930\nphysical entity\nn00002684\nobject, physical object\nn00020827\nmatter\nn03419014\ngarment\n[25 classes]\n(train)\nn01317541\ndomestic animal, domesticated animal\nn01767661\narthropod\nn03076708\ncommodity, trade good, good\nn03093574\nconsumer goods\nn04531098\nvessel\n[23 classes]\n(test)\nn03575240\ninstrumentality, instrumentation\nn02098550\nsporting dog, gun dog\n[17 classes]\n(validation)\nn03574816\ninstrument\n[28 classes]\n(train)\nn01844917\naquatic bird\n[24 classes]\n(train)\nn03094503\ncontainer\nn04451818\ntool\n[12 classes]\n(train)\nn03563967\nimplement\nn02087122\nhunting dog\nn02120997\nfeline, felid\n[13 classes]\n(train)\nn04524313\nvehicle\nn01905661\ninvertebrate\nn02092468\nterrier\n[26 classes]\n(train)\nn03100490\nconveyance, transport\nn03183080\ndevice\nn03405265\nfurnishing\n[22 classes]\n(validation)\nn04081844\nrestraint, constraint\n[11 classes]\n(train)\nFigure 5: Hierarchy of tieredImagenet categories. Training categories are highlighted in red and test categories in blue. Each category indicates the number of associated classes\nfrom ILSVRC-12. Best viewed zoomed-in on electronic version.\n14\n\n\nPublished as a conference paper at ICLR 2018\n44\n46\n48\n50\n52\n54\nTest Acc. (%)\n46.52\n46.52\n46.52\n46.52\n46.52\n46.52\n46.52\n46.52\n46.21\n45.89\n47.39\n49.46\n50.70\n51.20\n51.52\n51.66\n46.52\n46.04\n47.61\n49.73\n51.03\n51.54\n51.85\n52.06\n46.41\n46.49\n47.94\n50.19\n51.49\n52.08\n52.39\n52.61\n1-shot w/o Distractors\nSupervised\nSoft K-Means\nSoft K-Means+Cluster\nMasked Soft K-Means\n46.52\n46.52\n46.52\n46.52\n46.52\n46.52\n46.52\n46.52\n45.83\n45.70\n47.24\n48.82\n49.53\n49.77\n49.88\n49.95\n45.48\n46.43\n48.23\n49.99\n50.81\n51.11\n51.36\n51.38\n46.20\n46.63\n48.09\n49.87\n50.80\n51.18\n51.38\n51.49\n1-shot w/ Distractors\n0\n1\n2\n5\n10\n15\n20\n25\nNumber of Unlabeled Items Per Class\n65\n66\n67\n68\n69\n70\n71\nTest Acc. (%)\n66.15\n66.15\n66.15\n66.15\n66.15\n66.15\n66.15\n66.15\n66.66\n66.83\n67.21\n68.15\n69.23\n69.84\n70.25\n70.54\n66.15\n66.07\n66.46\n67.42\n68.43\n69.03\n69.42\n69.64\n66.26\n66.45\n66.85\n67.84\n68.84\n69.48\n69.88\n70.15\n5-shot w/o Distractors\n0\n1\n2\n5\n10\n15\n20\n25\nNumber of Unlabeled Items Per Class\n66.15\n66.15\n66.15\n66.15\n66.15\n66.15\n66.15\n66.15\n66.03\n65.79\n66.18\n67.14\n67.82\n68.15\n68.32\n68.45\n66.15\n65.29\n65.68\n66.55\n67.15\n67.39\n67.57\n67.64\n66.40\n66.53\n66.89\n67.73\n68.47\n68.85\n69.08\n69.24\n5-shot w/ Distractors\nFigure 6: Model Performance on tieredImageNet with different number of unlabeled items during test time.\nWe include test accuracy numbers in this chart.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMask values\n0.0%\n5.0%\n10.0%\n15.0%\n20.0%\n25.0%\nPercentage\nFigure 7: Mask values predicted by masked soft k-means on Omniglot.\n15\n"
}