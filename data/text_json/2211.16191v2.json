{
  "filename": "2211.16191v2.pdf",
  "num_pages": 12,
  "pages": [
    "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 28, NOVEMBER 2022\n1\nSgVA-CLIP: Semantic-guided Visual Adapting of\nVision-Language Models for Few-shot Image\nClassiﬁcation\nFang Peng, Xiaoshan Yang, Linhui Xiao, Yaowei Wang and Changsheng Xu, Fellow, IEEE\nAbstract—Although signiﬁcant progress has been made in\nfew-shot learning, most of existing few-shot image classiﬁcation\nmethods require supervised pre-training on a large amount of\nsamples of base classes, which limits their generalization ability\nin real world application. Recently, large-scale Vision-Language\nPre-trained models (VLPs) have been gaining increasing attention\nin few-shot learning because they can provide a new paradigm for\ntransferable visual representation learning with easily available\ntext on the Web. However, the VLPs may neglect detailed visual\ninformation that is difﬁcult to describe by language sentences,\nbut important for learning an effective classiﬁer to distinguish\ndifferent images. To address the above problem, we propose a new\nframework, named Semantic-guided Visual Adapting (SgVA),\nwhich can effectively extend vision-language pre-trained models\nto produce discriminative adapted visual features by comprehen-\nsively using an implicit knowledge distillation, a vision-speciﬁc\ncontrastive loss, and a cross-modal contrastive loss. The implicit\nknowledge distillation is designed to transfer the ﬁne-grained\ncross-modal knowledge to guide the updating of the vision\nadapter. State-of-the-art results on 13 datasets demonstrate that\nthe adapted visual features can well complement the cross-modal\nfeatures to improve few-shot image classiﬁcation.\nIndex Terms—few-shot, image classiﬁcation, vision-language\nmodels.\nI. INTRODUCTION\nFew-shot learning refers to the task of learning a new\nconcept with only a few labeled samples, which is inspired\nby human learning ability. As labeling is often expensive in\nreal scenarios, few-shot learning has become an important\nand widely studied problem. However, with little supervision\ninformation, learning to recognize new classes is challenging\nbecause directly training the model from a few samples may\noverﬁt. A common idea in few-shot learning is to train the\nmodel from base classes with sufﬁcient samples to get prior\nknowledge and then migrate to the novel classes with a few\nFang Peng and Linhui Xiao are with the National Laboratory of Pat-\ntern Recognition, Institute of Automation, Chinese Academy of Sciences,\nBeijing 100190, China, also with the Peng Cheng Laboratory, Shenzhen\n518066, China, and also with the School of Artiﬁcial Intelligence, Uni-\nversity of Chinese Academy of Sciences, Beijing 100049, China (e-mail:\npengfang21@mails.ucas.ac.cn, xiaolinhui16@mails.ucas.ac.cn).\nYaowei Wang is with the Peng Cheng Laboratory, Shenzhen 518066, China\n(e-mail: wangyw@pcl.ac.cn).\nXiaoshan Yang and Changsheng Xu are with the National Laboratory\nof Pattern Recognition, Institute of Automation, Chinese Academy of Sci-\nences, Beijing 100190, China, also with the School of Artiﬁcial Intelligence,\nUniversity of Chinese Academy of Sciences, Beijing 100049, China, and\nalso with the Peng Cheng Laboratory, Shenzhen 518066, China (e-mail:\nxiaoshan.yang@nlpr.ia.ac.cn, csxu@nlpr.ia.ac.cn).\nChangsheng Xu is the corresponding author.\n(a) Previous VLP-based few-shot learning methods\nText \nEncoder\n(b) The proposed SgVA-CLIP\ncross-modal text embedding\nadapted visual feature\nAdapter \nLayer\nText \nEncoder\nImage \nEncoder\nAdapter \nLayer\nProjection \nLayer\nProjection \nLayer\nProjection \nLayer\nProjection \nLayer\nImplicit Knowledge\nDistillation\nhoneysucker\ntoucan\nAdapted\nVisual Space\nPre-trained \nCross-modal Space\nPre-trained \nCross-modal Space\ncross-modal visual embedding \nImage \nEncoder\nAdapter \nLayer\nprompt + \nhoneysucker\ntoucan\nprompt + \nFig. 1.\nSgVA-CLIP vs. previous VLP-based few-shot learning methods.\n(a) Previous VLP-based few-shot learning methods focus on enhancing the\ncross-modal alignment, which may neglect important task-speciﬁc visual in-\nformation (e.g., the two birds honeysucker and toucan have different beaks) for\ndistinguishing different images when the labeled samples are insufﬁcient. (b)\nSgVA-CLIP makes a comprehensive consideration of adapted visual feature\nspace and pre-trained cross-modal feature space. The adapted visual features\nprovide more discriminative visual information and thus can well complement\nthe cross-modal features to improve few-shot image classiﬁcation.\nexamples. Existing studies on few-shot image classiﬁcation\ncan be roughly divided into three categories, namely ﬁne-\ntuning based methods, data augmentation based methods and\nmeta learning based methods. Among them, the most widely\nstudied is meta-learning [1–3], which acquires the abstract\nlearning ability to generalize to new classes by learning\nmeta-knowledge from a set of different meta tasks. Although\nsigniﬁcant progress has been made in few-shot learning, most\nof existing few-shot learning methods require the network to\nbe pre-trained in a supervised manner on a large amount of\nlabeled data of base classes. As a result, the current few-shot\nlearning methods have limited generalization ability and is\nimpractical in the real world due to the shortage of supervised\ndata.\narXiv:2211.16191v2  [cs.CV]  20 Jan 2023\n",
    "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 28, NOVEMBER 2022\n2\nRecently, self-supervised learning [4–7] has emerged as a\npossible solution to alleviate the dependency on large-scale\nlabeled data. Self-supervised learning exploits pretext tasks\nto mine supervised information from large-scale unsupervised\ndata, thereby learning rich implicit priors and latent repre-\nsentations. With the development of self-supervised learning,\nVision-Language Pre-trained models (VLPs), e.g., CLIP [8],\nALIGN [9], and Florence [10], attract more and more attention\ndue to its signiﬁcant performance in a variety of downstream\ntasks, such as image classiﬁcation and visual question answer-\ning. VLPs like CLIP can provide effective visual and semantic\nknowledge of open-world concepts that are learned on large-\nscale image-text pairs, laying a good generalization foundation\nof few-shot image classiﬁcation.\nVLPs have been successfully applied to few-shot image\nclassiﬁcation with the help of carefully designed text prompts\n[8, 11], which can change the discrete class labels into\nlanguage sentences. For example, CLIP model [8] learns vision\nand language representations by aligning the image and text in\na cross-modal joint space, which allows images to be correctly\nclassiﬁed via image-text similarity. There are also VLPs-\nbased few-shot learning methods that focus on enhancing the\nimage-text alignment. Context Optimization (CoOp) [12] is\nproposed to improve the text embedding of CLIP by soft\nprompt engineering. CLIP-Adapter [13] ﬁne-tunes the image\nrepresentation by adjusting an extra bottleneck layer. ProGrad\n[14] proposes Prompt-aligned Gradient to prevent prompt\ntuning from forgetting the general knowledge learned from\nVLPs.\nExisting methods only consider the image-text alignment\nwhen transferring the VLPs to solve few-shot image classiﬁ-\ncation. Although relying on the image-text similarity can well\ncapture the visual and semantic knowledge learned by the pre-\ntrained model, it is sometimes unreliable to recognize objects\nwithout comprehensively considering the speciﬁc discrimina-\ntive visual information of the few-shot task. The reason is that\nto learn a good image-text alignment model, the pre-trained\nVLPs may neglect detailed visual information that is difﬁcult\nto describe by language sentences. However, the neglected\nvisual information is probably important for distinguishing\ndifferent images when the labeled samples are insufﬁcient.\nTo address the above problem, we propose a new frame-\nwork, named Semantic-guided Visual Adapting (SgVA), which\ncan effectively extend vision-language pre-trained models\n(e.g., CLIP) to produce discriminative adapted visual features\nwith the guidance of the ﬁne-grained cross-modal knowledge\nlearned by the pre-trained model. The adapted visual features\ncan well complement the cross-modal features to improve\nfew-shot image classiﬁcation. Fig. 1 shows the main idea of\nour work. Speciﬁcally, our method is extended from the pre-\ntrained CLIP. Given labeled support images and unlabeled\nquery images, we ﬁrstly extract the visual features for the\nimages from the output before the cross-modal projection layer\nof the CLIP model, which are referred to as pre-trained visual\nfeatures. And cross-modal embeddings for both the images and\nthe prompted texts of the class labels are extracted from the\noutput of the cross-modal projection layer. Next, we map the\npre-trained visual features to the adapted visual features by a\nvisual adapting layer. We update the visual adapting layer on\nthe few-shot samples by a vision-speciﬁc contrastive loss and\ncross-modal contrastive loss with the help of vision prototypes\nand cross-modal prototypes that are obtained by averaging the\ncorresponding sample features of a given class. Moreover, we\nadopt an implicit distillation to utilize the ﬁne-grained cross-\nmodal knowledge (i.e., relative similarities between samples\nand prototypes in the pre-trained cross-modal space) learned\nby the pre-trained model to guide the updating of the vision\nadapter. Finally, we infer the class label for a given query\nsample by jointly considering its distance to vision-speciﬁc\nprototypes and cross-modal prototypes.\nOur contributions are summarized as follows. We propose\na new framework of semantic-guided visual adapting, which\nﬂexibly extends the vision-language pre-trained models (e.g.,\nCLIP) to produce discriminative adapted visual features by\njointly using implicit knowledge distillation, vision-speciﬁc\ncontrastive loss, and cross-modal contrastive loss. We obtain\nnew state-of-the-art results in few-shot image classiﬁcation by\ncomprehensively considering the sample relations based on\nboth the adapted visual features and the cross-modal features,\nwhich demonstrates a strong complementarity between the two\nkinds of feature space and also provides a promising direction\nfor future research.\nII. RELATED WORK\nThis section reviews three topics closely related to our\nwork in terms of few-shot learning, prototype networks and\nknowledge distillation.\nA. Few-shot Learning\nFew-shot learning aims to learn a model that can recognize\nnew classes with a few training samples. The widely studied\nconventional few-shot learning methods include ﬁne-tuning\n[15, 16], data/feature augmentation [17, 18], and meta learning\n[1, 19]. Recently, Vision-language pre-trained models (VLPs)\n(e.g. CLIP [8] and ALIGN [9]) have been applied to few-shot\nlearning by transferring the powerful representation ability. In\norder to realize data-efﬁcient ﬁne-tuning, CoOp [12] improves\nthe ability of image-text alignment through continuous prompt\noptimization, and CLIP-Adapter [13] designs lightweight fea-\nture adapters to explore simple ﬁne-tuning. After that, Tip-\nAdapter [20], a training-free method, is proposed to save\ncomputational resources. Different from them, VT-CLIP [21]\nimproves the interaction of image and text branches of CLIP\nby cross-modal module. Other works like MUST [22] and\nUPL [23] think about unsupervised learning. Besides, WiSE-\nFT [24] and CoCoOp [25] consider both the accuracy of target\ndistribution and robustness to distribution shifts. Unlike the\nabove methods that focus on image-text contrastive learning,\nwe extend the pre-trained CLIP to learn more discriminative\nvisual features that can well complement the cross-modal\nfeatures in few-shot learning.\nB. Prototype Networks\nPrototype network [26] is proposed in 2017 to solve the\nproblem of few-shot classiﬁcation, which aims at learning a\n",
    "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 28, NOVEMBER 2022\n3\nmetric space where query samples can be accurately classiﬁed\nby calculating the distances between queries and prototypes.\nCompared with other few-shot learning methods, Prototype\nnetwork reﬂects a simpler inductive bias, which is beneﬁcial\nin the case of limited data. Owing to the potential of this\nparadigm, many variations have been developed since then.\nChen et al. [27] found that introducing an extra pre-training\nphase on the entire base classes could improve performance,\nbut it leads to poor generalization ability. Early prototype\nnetworks only employ visual information, but increasingly\nthere are approaches to explore how semantic knowledge\ncan enhance the performance. For example, Chen et al. [28]\nlearned semantic knowledge from unsupervised corpora, and\nproposed an adaptive modality mixing mechanism to combine\nthe visual and semantics knowledge, showing improvements in\nfew-shot learning. Frederik et al. [29] mapped text data to the\nvisual embedding space with the help of a generative model,\nand then designed a strategy to combine the real and generated\nfeatures through the nearest neighbor algorithm. Instead of\nonly forming a single metric space as in existing prototype\nnetworks, we construct two metric spaces including visual and\ncross-modal spaces to comprehensively conduct the few-shot\nlearning.\nC. Knowledge Distillation\nKnowledge Distillation (KD) [30–32] means transferring\nthe knowledge from the pre-trained complex model (teacher\nmodel) to a simpler structured network (student model). Owing\nto its superior performance in knowledge transferring and\nmodel enhancement, KD is widely used in model compression\nand transfer learning. In the process of KD, the output of\nteacher model is used as the supervision signal to train the\nstudent model through the distillation loss. And the optimiza-\ntion target is to make the class-level probability distribution of\nthe student model match the probability output of the teacher\nmodel. In terms of model compression, DistillBert [33] and\nTinyBert [34] use KD to explore smaller and faster models\nfor language representation learning. In addition to model\ncompression, KD also plays an important role in knowledge\ntransferring between different modalities. For example, the\nvision-language distillation framework DistillVLM [35] is\nproposed to improve vision-language tasks like image cap-\ntioning and VQA. Hafner et al. [36] propose a novel cross-\nmodal distillation method for robust person re-identiﬁcation,\nwhich transfers knowledge from RGB images to depth images.\nBesides, in [37], semantic knowledge is transmitted from\nlanguage model to a spoken language understanding module,\nso that the deﬁciency of speech data can be alleviated. The\nabove works show the beneﬁts of KD in both self-supervised\nlearning and multi-modal alignment. Different from existing\nmethods, we propose a knowledge distillation which can\nimplicitly transfer the ﬁne-grained relation knowledge learned\nin the cross-modal space to the visual space by conducting the\ndistillation in the cross-modal space.\nIII. THE PROPOSED METHOD\nA. Problem Deﬁnition\nThe few-shot learning methods usually depend on base\nclasses with ample samples to learn how to generalize to\nnovel classes. However, in the real few-shot scenario, sufﬁcient\nsamples of base classes may not be available. It is worth\nnoting that since the pre-trained vision and language model,\ne.g., CLIP, can provide a good foundation model for the few-\nshot learning, the base classes are not indispensable. In this\nwork, we consider both the standard meta-learning scenario\nwith base classes and the scenario without base classes.\nIn the standard meta-learning scenario, a series of meta-\ntasks (episodes) are created for training and testing. For each\nmeta-task in the meta-training phase, N (Way) base classes\nand K (Shot) samples for each base class are randomly\nselected to make up the Support Set S = {(xi, yi)}N×K\ni=1\n,\nwhere xi denotes the sampled image, and yi is the label of\nxi. Other M samples of the N (Way) classes are randomly\nselected to form the Query Set Q = {(xi, yi)}M\ni=1. In the meta-\ntest phase, the Support Set is created from K-shot labeled\nsamples of novel classes and the query set is created from\nunlabeled samples of novel classes.\nIn the scenario without base classes, inspired by the idea\nof self-supporting from [38], we build both the Support Set\nand the Query Set from the K-shot labeled samples of novel\nclasses in the meta-training phase. In the meta-test phase, the\nSupport Set is created from K-shot labeled samples of novel\nclasses and the Query Set is created from unlabeled samples\nof novel classes.\nB. Network Architecture\nThe proposed framework, namely Semantic-guided Visual\nAdapting (SgVA), aims to learn discriminative adapted visual\nfeatures with the guidance of the cross-modal knowledge\nlearned by the pre-trained CLIP model. The adapted visual fea-\ntures can well complement the cross-modal features in the few-\nshot image classiﬁcation. Fig. 2 shows the overall framework.\nGiven support images and query images in each episode, the\nclass labels of support samples are ﬁrstly transformed into text\ninput by L-length learnable prompt vectors as in [12]. Then,\nthe images and texts go through the frozen image and text\nencoders of CLIP to generate the pre-trained visual features\n(xv) and text features (xt) respectively, which are further\nmapped to cross-modal visual and text embeddings, i.e., xc v\nand xc t through different projection layers (i.e., φ and ψ).\nMeanwhile, the pre-trained visual features (xv) are mapped\nto the adapted visual features (xa) by a visual adapting layer.\nTo fully exploit sample relations in the visual space and\ncross-modal space, we build vision-speciﬁc prototypes and\ncross-modal prototypes that are obtained by averaging the\ncorresponding support sample features of a given class. Based\non the prototypes, we update the visual adapting layer on\nthe few-shot samples by an implicit knowledge distillation, a\nvision-speciﬁc contrastive loss, and a cross-modal contrastive\nloss. The implicit knowledge distillation utilizes ﬁne-grained\nsample relations in the cross-modal feature space to guide the\nlearning of the visual adapting to produce more discriminative\n",
    "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 28, NOVEMBER 2022\n4\n...\n...\n...\nFused \nPrediction \nModule\n \nSupport Samples\n...\n[CLS]\nAdapted Visual Space\nupdated\nfrozen\nfrozen\n...\nQuery Samples\n��_�\n��\n��_� \n[��(1),..,��(�)]\n�\nCLIP Image \nEncoder \n[V]1 [V]2\n[V]L\n...\nprompt vectors×L\n.\nCLIP Text \nEncoder\nProjection\n  Layer �\nProjection\n   Layer �\n��\n��_�\n��_�\n��\nVisual Adapter\n��\nBackpropagation of gradient \n��\nVision-specific\nContrastive Loss\nCross-modal\nContrastive Loss\n��\nImplicit Knowledge\nDistillation Loss\nProjection\n  Layer �\nPre-trained \nCross-modal Space\n[��_�(1),..,��_�(�)]\nMLP\n＋\nQuery\nSupport\nQuery\nSupport\nN-Way K-Shot\nK\nFig. 2.\nOverview of the proposed SgVA-CLIP. In the pre-trained cross-modal space, we build cross-modal prototypes that are obtained by averaging the\ncross-modal text embedding, i.e., xc t of the support samples. Meanwhile, in the adapted visual space, we build vision-speciﬁc prototypes that are obtained by\naveraging the adapted visual features after the visual adapting layer, i.e., xa of the corresponding support samples. The SgVA-CLIP can learn discriminative\nadapted visual features with the guidance of the cross-modal knowledge learned by the pre-trained CLIP. And the discriminative visual features can well\ncomplement the cross-modal features in the few-shot image classiﬁcation.\nvisual features. In the inference process, test image can be\nrecognized by jointly considering its distance to the vision-\nspeciﬁc prototypes and cross-modal prototypes.\nC. Shot-speciﬁc Text Prompt\nTo transform the class label into natural language sentence\nthat can be directly processed by CLIP, we adopt prompted\ntext with individual differences as in [39]. We independently\ninitialize an individual prompt ti for each shot from the same\nclass. We use K different prompts in each N-way K-shot\nepisode according to the number of shots and we use shared\nprompts for different classes, which is referred to as shot-\nspeciﬁc text prompt. As illustrated in [39], using a ﬁxed\namount of prompts instead of a universal prompt can capture\nsubtle differences among different samples and can also be\nmore efﬁcient than designing a speciﬁc prompt for every\ninput sample. The text prompt ti is formally deﬁned as the\nconcatenation of L learnable continuous vectors and the class\nname embedding:\nti = [V ]i,1[V ]i,2...[V ]i,L[CLS]i[.],\n(1)\nwhere each [V ]i,l, l ∈{1, ..., L}, is a learnable vector with the\nsame dimension as the class embedding [CLS]i. The class\nembedding is a 512-dimensional word embedding obtained\nfrom the pre-trained CLIP.\nD. Visual Adapting Layer\nThe adapting layer consists of a two-layer Multi-layer\nPerceptron (MLP) and an adaptive residual connection. The\nnew feature Newv(xv) acquired in the adapting layer can be\nrepresented as:\nNewv(xv) = ReLU(xv\nT W1)W2,\n(2)\nwhere xv denotes the pre-trained visual feature that is ob-\ntained by the image encoder of the pre-trained CLIP encoding\nthe image of the sample x before the linear projection to\nthe cross-modal embedding space, W1 and W2 are learnable\nweights of the two fully connected layers, and ReLU is\nactivation function. The pre-trained visual feature xv is ﬁxed\nin our framework. Then, the new feature is added to the\noriginal visual feature by an adaptive residual connection:\nxa = [Newv(xv), xv]Wa,\n(3)\nwhere xa is the adapted visual feature, and the Wa ∈R2 is a\nlearnable weight vector. [] denotes the operation of combining\nthe vectors as a matrix.\nE. Cross-modal Contrastive Loss\nFor the convenience of exploring the relations between\ndifferent classes in the cross-modal space, we ﬁrstly calculate\ncross-modal prototypes (pc t) of N classes by averaging the\ncross-modal text embeddings of the support samples for each\nclass. The cross-modal prototype of the k-th class, i.e., pc t(k)\nis represented by\npc t(k) =\n1\n|S(k)|\nX\n(xi,yi)∈S(k)\nxc t(i),\n(4)\nwhere xc t(i) denotes the cross-modal text embedding of the\ni-th sample that belongs to S(k), and S(k) is the set of all the\n",
    "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 28, NOVEMBER 2022\n5\nsupport samples of class k. As illustrated in Section III-B,\nxc t is obtained by the pre-trained CLIP that is ﬁxed in\nour framework. The prompted text (ti) of the i-th sample is\nencoded by the text encoder of CLIP into the uni-modal text\nfeature xt(i), which is then projected into the cross-modal\nembedding space as cross-modal text embedding xc t(i).\nThe purpose of cross-modal contrast loss is to learn a better\ntextual representation and lay a good semantic foundation\nfor guiding the visual adapting. For every query sample x,\nthe cosine similarity scores between the cross-modal visual\nfeature (xc v) and the cross-modal prototypes (pc t) are\ncalculated, where xc v is obtained by mapping the pre-trained\nvisual feature into the cross-modal embedding space. The\ncosine similarity is scaled by a temperature parameter τ1, and\nnormalized into a probability distribution via Softmax. Then,\nthe cross-modal contrastive loss Lcl i2t is deﬁned as the cross\nentropy loss over the probability distribution:\nLcl i2t = −log\nexp(< xc v, pc t(+) > /τ1)\nPN\nj=1exp(< xc v, pc t(j) > /τ1)\n,\n(5)\nwhere τ1 is a temperature parameter that is initialized to\n0.07 and pre-trained by CLIP [8]. And pc t(+) denotes the\nthe cross-modal prototype of the positive class, < ·, · >\ndenotes cosine similarity, and N is the number of classes. The\nuseful implicit knowledge is included in the relative distances\nbetween cross-modal features.\nF. Vision-speciﬁc Contrastive Loss\nSince the pre-trained CLIP may neglect important visual\ninformation that is difﬁcult to describe in natural language\nsentences, we utilize vision-speciﬁc contrastive loss to make\nthe adapted features (i.e., xa) retain more discriminative visual\ninformation that is speciﬁc to the current few-shot task. For\nevery query sample x, the cosine similarity scores between the\nadapted visual feature (xa) and the vision-speciﬁc prototypes\n(pa) of N classes are calculated by:\npa(k) =\n1\n|S(k)|\nX\n(xi,yi)∈S(k)\nxa(i),\n(6)\nwhere S(k) denotes the set of all the support samples of\nclass k and xa(i) is calculated by Eq. 2 and Eq. 3 for\nthe i-th sample. The purpose of vision-speciﬁc contrastive\nloss is to maximize the cosine similarity between xa and\nthe positive prototype while minimizing the cosine similarity\nbetween xa and negative prototypes. Formally, the vision-\nspeciﬁc contrastive loss Lcl i2i is calculated by:\nLcl i2i = −log\nexp(< xa, pa(+) > /τ1)\nPN\nj=1exp(< xa, pa(j) > /τ1)\n.\n(7)\nG. Implicit Knowledge Distillation\nWith the vision-speciﬁc contrastive loss, we can already\nmake the samples of the same class close to each other, and the\nsamples of different classes far from each other in the adapted\nunimodal vision space. However, the vision contrastive loss\ncannot provide the ﬁne-grained relations between different\n��_�(�)\nPre-trained Cross-modal Space\nAdapted Visiual Space\n��_�(�)\ndstudent\n��_�(�)\n��(�)\n��(�)\nadapter\n��(i ≠ k)\n��(�)\n��(j ≠ k)\nPre-trained Visual Space\n��_�(i ≠ k)\n��_�(j ≠ k)\nadapted visual feature  (��) \ncross-modal prototype  (��_�) \ncross-modal  visual embedding (��_�) \nvision-specific prototype  (��) \n  pre-trained visual feature  (��) \nd teacher\nAdapted Visual Space:\nPre-trained Cross-modal Space:\ncross-modal proxy embedding (��_�) \nProjection \n Layer �\nPre-trained Visual Space:\nFig. 3.\nInterpretation of the implicit knowledge distillation between the\nadapted visual space and the pre-trained cross-modal space. The adapted visual\nfeature xa is mapped to the cross-modal space as a proxy representation\nxc a. The knowledge distillation is performed in the pre-trained cross-modal\nspace by matching the ﬁne-grained sample-prototype relations for the proxy\nrepresentation xc a and the cross-modal visual embedding xc v.\nsamples. As shown in Fig. 1, the two kinds of birds have\ndifferent beaks locally, but the whole is similar in vision. If\nthe vision-speciﬁc contrastive loss is used alone, it is easy\nto ignore the details of beaks, and lead to misclassifying the\nbird. However, they are easy to distinguish in terms of text\nsemantics (i.e., honeysucker and toucan). The cross-modal\ninformation in the pre-trained cross-modal space provides the\nconstraint of relative semantic distances between samples,\nand instructs how far apart the semantically different samples\nshould be, even if they are visually similar as a whole, thus\nhelping to improve the discriminative ability of the adapted\nvisual features.\nAs a solution, we consider to take advantage of the relative\nsample relationships produced by the pre-trained CLIP model,\nwhich cannot be achieved by contrastive learning alone. It is\nworth noting that when compared with the visual data, the\nsemantic meaning of a given class is more likely to be shared\nby the pre-trained CLIP model and the downstream few-shot\ntask. Therefore, we adopt a knowledge distillation loss to\nguide the learning of the vision adapter by the ﬁne-grained\nrelationships in the cross-modal space.\nFig. 3 gives an illustration of the proposed implicit knowl-\nedge distillation. By mapping the adapted visual feature xa\nto the cross-modal space through the projection layer (i.e.,\nφ), a proxy representation of it in the cross-modal space,\nnamely xc a is obtained. Then, we can utilize the ﬁne-grained\nrelations of the cross-modal visual feature xc v to constrain\nthe proxy representation xc a. This means that we can use\ndistances between samples obtained in the cross-modal space\nas extra supervision to implicitly guide the learning of the\nvision adapting layer. The knowledge distillation is performed\nin the cross-modal space, and the gradients are backpropagated\nto the visual adapting layer.\nMore speciﬁcally, for a cross-modal visual feature xc v,\n",
    "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 28, NOVEMBER 2022\n6\nwe calculate its distances to each of the cross-modal text\nprototypes as a teacher:\ndtea(k) =< xc v, pc t(k) > /τ1,\n(8)\nwhere τ1 is a temperature parameter that is initialized to 0.07,\nand learned by the pre-trained CLIP. The distances between\nxc a and each of the cross-modal text prototypes pc t are\ncalculated as a student:\ndstu(k) =< xc a, pc t(k) > /τ1.\n(9)\nFinally, the knowledge distillation loss is deﬁned as:\nLKD =−\nN\nX\nk=1\nexp(dtea(k)/τ2)\nPN\nj=1 exp(dtea(j)/τ2)\nlog\n\u0010\nexp(dstu(k)/τ2)\nPN\nj=1 exp(dstu(j)/τ2)\n\u0011\n,\n(10)\nwhere τ2 is a hyperparameter that is set to 5 in this paper. The\nparameter analysis of τ2 is shown in Table VIII.\nH. Optimization and Inference\nOur framework can be learned in an end-to-end form for\nfew-shot image classiﬁcation. The framework is optimized\nby jointly considering vision-speciﬁc contrastive loss, cross-\nmodal contrastive loss, and knowledge distillation loss. The\noverall loss is deﬁned as:\nLoss = Lcl i2t + Lcl i2i + LKD.\n(11)\nFor a given test image in the inference phase, we conduct\nfew-shot classiﬁcation by comprehensively considering its\ndistance to the vision-speciﬁc prototypes and cross-modal\nprototypes:\nda(k) =< xa, pa(k) >,\nk = 1, ..., N,\n(12)\ndc t(k) =< xc v, pc t(k) >,\nk = 1, ..., N,\n(13)\nwhere < ·, · > denotes cosine similarity. Speciﬁcally, the\npredicted class ˆy with the maximum posterior probability is\ncalculated by applying the Naive Bayes:\nˆy = argmax\nyk\n2N\nX\nj=1\nlog P\n\u0000d(j) | Y =yk\n\u0001\n,\n(14)\nwhere d is a 2N-dimensional vector obtained by concatenating\nda and dc t.\nIV. EXPERIMENT AND RESULTS\nA. Datasets\nFor the standard meta-learning scenario, we choose mini-\nImagenet [40] and tieredImagenet [41], which are common\nbenchmarks for few-shot learning. The purpose is to validate\nthe generalization ability of our model on novel classes.\nThe miniImagenet dataset contains 100 classes sampled from\nILSVRC-2012 [42], and is split to 64, 16, 20 classes for\ntraining, validation, and testing respectively. Similarly, the\ntieredImagenet dataset includes 608 classes sampled from\nILSVRC-2012, and is divided into 351, 97, 160 classes for\ntraining, validation, and testing respectively. The setting of\ntieredImagenet is more challenging, because the base classes\nand novel classes come from different super categories.\nFor the scenario without base classes, we follow CLIP [8]\nand CoOp [12] to select 11 image classiﬁcation datasets to\nevaluate the performance, namely ImageNet [43], Stanford-\nCars [44], UCF101 [45], Caltech101 [46], Flowers102 [47],\nSUN397 [48], DTD [49], EuroSAT [50], FGVCAircraft [51],\nOxfordPets [52], and Food101 [53]. These datasets cover a\nseries of diverse visual tasks including the classiﬁcation of\ngeneral objects, scenes, actions, and ﬁne-grained categories.\nB. Implementation Details\nFor the conventional setting of few-shot learning, we evalu-\nate our model on two widely used datasets, i.e., miniImagenet\nand tieredImagenet. We train the overall framework on base\nclasses for 100 epochs with 5-way 1-shot/5-shot tasks. And in\neach episode, 15 query images per class are randomly sampled.\nIn test phase, we randomly 600 episodes on novel classes, and\nreport the mean accuracy together with the 95% conﬁdence\ninterval. For the setting without base classes, we follow the\nfew-shot evaluation protocol adopted in CLIP [8] to evaluate\nthe model performance on 11 datasets, and set up 1, 2, 4, 8,\n16 shots from all the classes to train the model and then test it\non full test set. The optimizer is SGD with momentum of 0.9\nand weight decay of 0.0005. The temperature τ1 is obtained\nfrom the pre-trained CLIP, while the temperature τ2 in implicit\nknowledge distillation is set to 5, whose parameter analysis is\nshown in Table VIII. The length of prompt vectors is set to\n4, which is same length as the hand-crafted prompt “a photo\nof a”. We conduct all experiments on a single Nvidia V100\nGPU.\nC. Baselines\nFor the miniImagenet and tieredImagenet datasets, we com-\npare our method with 8 baselines, including PEMnE-BMS*\n[54], HCTransformers [55], CLIP LP+LN [56], P>M>F [57],\ncluster-FSL [58], PT+MAP [59], EPNet [60] and EASY\n[61] which are state-of-the-art methods on miniImagenet and\ntieredImagenet. For the other 11 datasets, we compare our\nmethod with 5 baselines, namely Zero-shot CLIP [8], CoOp\n[12] and CLIP-Adapter [13], ProGrad [14], which are state-\nof-the-art few-shot learning methods based on VLP models.\nFor fair comparison, we follow their way of dataset splitting\nand adopt classiﬁcation accuracy as the evaluation metric.\nD. Performance Comparison\nTable I shows the results of our SgVA-CLIP and other\nstate-of-the-art methods on miniImagenet and tieredImagenet.\nCompared to the other methods, our SgVA-CLIP achieves new\nstate-of-the-art performance on both miniImagenet and tiered-\nImageNet. Although the P>M>F [57] uses extra data, SgVA-\nCLIP still outperforms it by 2.65% and 0.32% respectively\non the 5-way 1-shot and 5-way 5-shot tasks of miniImagenet.\nCompared with CLIP LP+LN [56], which uses the same pre-\ntrained parameters of CLIP as ours, SgVA-CLIP exceeds it by\n5.87% and 0.78% respectively on the miniImagenet. Besides,\n",
    "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 28, NOVEMBER 2022\n7\nTABLE I\nCOMPARISON WITH STATE-OF-THE-ART FEW-SHOT LEARNING METHODS (WITH BASE CLASSES) ON MINIIMAGENET AND TIEREDIMAGENET.\nminiImagenet (%)\ntieredImagenet (%)\nMethod\nVision\nBackbone\n5-way 1-shot\n5-way 5-shot\n5-way 1-shot\n5-way 5-shot\nEPNet+SSL\nWRN-28-10\n79.22 ± 0.92\n88.05 ± 0.51\n83.69 ± 0.99\n89.34 ± 0.59\nEASY\n3×ResNet12\n83.02 ± 0.23\n88.57 ± 0.12\n84.29 ± 0.24\n89.76 ± 0.14\nPT+MAP\nWRN/DenseNet121\n82.92 ± 0.26\n88.82 ± 0.13\n85.67 ± 0.26\n90.45 ± 0.14\ncluster-FSL\nWRN-28-10\n85.74 ± 0.76\n90.18 ± 0.43\n82.63 ± 0.79\n89.16 ± 0.35\nHCTransformers\nViT-S\n74.62 ± 0.20\n89.19 ± 0.13\n79.57 ± 0.20\n91.72 ± 0.11\nPEMnE-BMS*\nDenseNet121\n85.54\n91.53\n86.07 ± 0.25\n91.09 ± 0.14\nCLIP LP+LN\nViT-B/16\n92.08\n97.94\n-\n-\nP>M>F (with ext. data)\nViT-B/16\n95.30\n98.40\n-\n-\nSgVA-CLIP (ours)\nViT-B/16\n97.95 ± 0.19\n98.72 ± 0.13\n95.73 ± 0.37\n96.21 ± 0.37\nTABLE II\nTHE BENEFIT OF KNOWLEDGE DISTILLATION.\nMethod\nLKD\nVision\nBackbone\nImageNet (%)\n16-shot\nSUN397 (%)\n16-shot\nVision\nBackbone\nImageNet (%)\n16-shot\nSUN397 (%)\n16-shot\nVision-speciﬁc w/o KD\n×\n63.61\n69.00\n50.11\n61.70\nVision-speciﬁc w/ KD\n✓\n66.95 (+3.34)\n71.22 (+2.22)\n58.75 (+8.64)\n66.56 (+4.86)\nSgVA-CLIP w/o KD\n×\n72.94\n76.12\n64.42\n71.24\nSgVA-CLIP w/ KD\n✓\nViT-B/16\n73.30 (+0.36)\n76.42 (+0.30)\nResNet50\n65.70 (+1.28)\n71.99 (+0.75)\non the tieredImageNet, SgVA-CLIP surpasses the second best\nresult by 9.66% and 4.49% respectively. It is worth noting that\nthe improvement of our method is larger when the number of\nshots is fewer, which demonstrates its generality capability in\nfew-shot learning.\nFig. 4 shows the comparison results of our method and 5\nrecent baselines over 11 datasets with 1, 2, 4, 8, 16 shots.\nFor fair comparison, we adopt the vision backbone ResNet50\nfrom CLIP as in [12, 13, 20, 21]. Compared with CoOp\n[12], our method achieves considerable improvement. CoOp\nonly considers the cross-modal information extraction from a\nperspective of prompt learning, but neglects the vision-speciﬁc\ninformation that is more discriminative. By comprehensively\nexploiting the vision information and the cross-modal informa-\ntion, our method outperforms CoOp on 11 datasets. Typically,\nthe average performance gains over all datasets with 1, 2,\n4, 8, 16 shots are 6.45%, 5.07%, 4.19%, 3.87% and 3.32%\nrespectively. Besides, the average gains over all the shots has\nreached 7.9% and 7.09% respectively on the Food101 and\nFGVAircraft dataset.\nCompared with ProGrad [14], which proposes a prompt-\naligned gradient updating scheme, our SgVA-CLIP achieves\nsigniﬁcant performance and exceeds it by 2.21% on average.\nTypically, the mean performance gains over 11 datasets with 1,\n2, 4, 8, 16 shots are 2.15%, 2.24%, 1.93%, 2.21% and 2.53%\nrespectively. Although a novel gradient updating strategy is\nproposed from the perspective of overcoming the improperly\nbiased tuning, ProGrad gains limited improvement and cannot\ncapture the more discriminative visual information.\nCompared with CLIP-Adapter [13], which is a visual\nadapter based on CLIP, our SgVA-CLIP surpasses it on most\ndatasets. The average performance gains over CLIP-Adapter\nTABLE III\nTHE SIGNIFICANT TEST OF KNOWLEDGE DISTILLATION ON FIVE TRIALS.\nMethod\nLKD\nVision\nBckbone\nMean acc on\nImageNet(%)\n16-shot\nstd T value P value\nSgVA-CLIP w/o KD\n×\nViT-B/16\n72.94\n0.36\n3.70\n3.6e-4\nSgVA-CLIP w/ KD\n✓\n73.30\n0.11\nacross all shots on the 11 datasets is 1.97%. By training\nan extra visual bottleneck layer, CLIP-Adapter can enhance\nthe alignment of the visual features with the text features.\nHowever, it only considers the visual information that is\nrelated to text. Different from it, our SgVA-CLIP proposes\na semantic-guided adapting mechanism, which produces more\ndiscriminative visual features that can well complement the\ncross-modal features, showing a more promising perspective\nfor few-shot learning.\nThe overall experiment results demonstrate that the implicit\nknowledge distillation can promote the visual adapting and\nproduce more effective task-speciﬁc visual features for few-\nshot learning.\nE. Ablation Study\nAblation study of the implicit knowledge distillation.\nTo evaluate the effectiveness of the adopted knowledge dis-\ntillation, we conduct experiment on ImageNet and SUN397.\nThe results are shown in Table II. We can observe that with\nResNet50 as the vision backbone, KD improves the accuracy\nof vision-based model by 8.64% on ImageNet and 4.86% on\nSUN397. And KD enhances the SgVA-CLIP by 1.28% and\n",
    "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 28, NOVEMBER 2022\n8\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of labeled training examples per class\n58\n60\n62\n64\n66\nScore (%)\nZero-shot\nCLIP\nImageNet\nSgVA-CLIP(ours)\nProGrad\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of labeled training examples per class\n58\n60\n62\n64\n66\n68\n70\n72\nScore (%)\nZero-shot\nCLIP\nSUN397\nSgVA-CLIP(ours)\nProGrad\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of labeled training examples per class\n68\n70\n72\n74\n76\n78\nScore (%)\nZero-shot\nCLIP\nFood101\nSgVA-CLIP(ours)\nProGrad\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of labeled training examples per class\n45\n50\n55\n60\n65\nScore (%)\nZero-shot\nCLIP\nDTD\nSgVA-CLIP(ours)\nProGrad\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of labeled training examples per class\n65\n70\n75\n80\n85\n90\n95\nScore (%)\nZero-shot\nCLIP\nFlowers102\nSgVA-CLIP(ours)\nProGrad\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of labeled training examples per class\n55\n60\n65\n70\n75\nScore (%)\nZero-shot\nCLIP\nStanfordCars\nSgVA-CLIP(ours)\nProGrad\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of labeled training examples per class\n10\n15\n20\n25\n30\n35\n40\nScore (%)\nZero-shot\nCLIP\nFGVCAircraft\nSgVA-CLIP(ours)\nProGrad\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of labeled training examples per class\n78\n80\n82\n84\n86\n88\n90\nScore (%)\nZero-shot\nCLIP\nOxfordPets\nSgVA-CLIP(ours)\nProGrad\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of labeled training examples per class\n40\n50\n60\n70\n80\nScore (%)\nZero-shot\nCLIP\nEuroSAT\nSgVA-CLIP(ours)\nProGrad\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of labeled training examples per class\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0\n77.5\nScore (%)\nZero-shot\nCLIP\nUCF101\nSgVA-CLIP(ours)\nProGrad\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of labeled training examples per class\n86\n87\n88\n89\n90\n91\n92\n93\nScore (%)\nZero-shot\nCLIP\nCaltech101\nSgVA-CLIP(ours)\nProGrad\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of labeled training examples per class\n57.5\n60.0\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0\nScore (%)\nZero-shot\nCLIP\nAverage over 11 datasets\nSgVA-CLIP(ours)\nProGrad\nCLIP-Adapter\nCoOp\nZero-shot CLIP\nFig. 4. Comparison with state-of-the-art few-shot learning methods (without base classes) on 11 datasets.\nTABLE IV\nABLATION STUDY OF DIRECT AND IMPLICIT KNOWLEDGE DISTILLATION.\nKnowledge\nDistillation\nVision\nBackbone\nImageNet (%)\n16-shot\nSUN397 (%)\n16-shot\nAverage (%)\nDirect\nViT-B/16\n68.98\n75.70\n72.34\nImplicit\n73.30\n76.42\n74.86\nDirect\nResNet50\n59.88\n66.38\n63.13\nImplicit\n65.64\n71.99\n68.82\n0.75% respectively. We also note that when ResNet50 is used\nas the vision backbone, the performance gain of KD is greater\nthan using the ViT-B/16 backbone.\nTo do the signiﬁcance test for knowledge distillation, we\ncompute the P-value by repeating the experiments 5 times with\ndifferent random seeds. With the backbone of ViT-B/16, the\nresults of ﬁve trials on ImageNet (16-shot) are shown in Table\nIII, where std refers to the standard deviation. The P value of\nT test was calculated as 3.6e −4 < 0.05, demonstrating that\nthere are signiﬁcant differences between SgVA-CLIP w/ KD\nand SgVA-CLIP w/o KD.\nImplicit knowledge distillation vs. direct distillation.\nThe direct distillation uses the Kullback-Leibler divergence\nto match the sample relations in the cross-modal space and\nthe sample relations in the vision space. As shown in Table\nIV, compared with direct distillation, the proposed implicit\nknowledge distillation has an average performance gain of\n2.52% and 5.69% respectively on ViT-B/16 and ResNet50\nTABLE V\nTHE COMPLEMENTARITY BETWEEN THE VISION SPACE AND THE\nCROSS-MODAL SPACE.\nMethod\nVision\nBackboneLcl i2t Lcl i2i LKD\nImageNetSUN397\n16-shot\n16-shot\nVision-speciﬁc Prediction ViT-B/16\n×\n✓\n×\n63.61\n69.00\nCross-modal Prediction ViT-B/16\n✓\n×\n×\n71.48\n73.50\nFused Prediction\nViT-B/16\n✓\n✓\n×\n72.94\n76.12\nbackbone. Because in the direct distillation, the distribution\ngap between the two spaces may have a negative impact on\nthe distillation.\nComplementarity between the vision space and the\ncross-modal space. We compare the classiﬁcation results on\nImageNet and SUN397 obtained using the visual features\nand/or the cross-modal features. The results are shown in Table\nV, which demonstrate that comprehensively considering the\nvisual feature and the cross-modal feature is better than simply\nusing the one of them.\nFor the 16-shot learning task, fusing the results predicted\nfrom the vision features and the cross-modal features will\nincrease the accuracy by 1.46% and 2.62% on ImageNet and\nSUN397 when compared with the cross-modal results, and the\naccuracy is increased by 9.33% and 7.12% respectively when\ncompared with the vision-based results.\nAblation study of the visual adapting layer and the\nlearnable prompt. With the visual adapter removed, the\n",
    "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 28, NOVEMBER 2022\n9\nTABLE VI\nABLATION STUDY OF ADAPTER AND PROMPT.\nminiImagenet (%)\ntieredImagenet (%)\nVision\nBackbone\nVisual\nAdapter\nLearnable\nPrompt\n5w-1s\n5w-5s\n5w-1s\n5w-5s\n×\n×\n93.07\n97.45\n89.72\n93.73\n✓\n×\n95.71\n97.82\n92.30\n95.25\n×\n✓\n96.63\n97.80\n94.88\n95.21\nViT-B/16\n✓\n✓\n97.95\n98.72\n95.73\n96.21\nTABLE VII\nCOMPARISON OF DIFFERENT VISION BACKBONES.\nVision\nBackbone\nMethod\nImageNet (%)\nSUN397 (%)\nAverage (%)\n16-shot\n16-shot\nResNet50\nCoOp\n62.95\n69.26\n66.11\nSgVA-CLIP\n65.70\n71.99\n68.85\nResNet101\nCoOp\n66.60\n71.19\n68.90\nSgVA-CLIP\n68.51\n73.00\n70.76\nViT-B/32\nCoOp\n66.85\n72.38\n69.62\nSgVA-CLIP\n68.26\n74.04\n71.15\nViT-B/16\nCoOp\n71.92\n75.29\n73.61\nSgVA-CLIP\n73.30\n76.42\n74.86\ndiscriminative adapted visual features are replaced by the pre-\ntrained visual features. And when prompt is not learnable, we\nfollow CLIP [8] and use hand-crafted prompt, i.e. ’a photo\nof a’. Note that the learnable continuous prompt has the same\nlength as the hand-crafted prompt, i.e. 4. And the 5-way 1-shot\nand 5-way 5-shot tasks are abbreviated as 5w-1s and 5w-5s\nrespectively in Table VI.\nThe baseline is that of removing both the Visual Adapter\nLayer and the learnable prompt. As shown in Table VI, the\nVisual Adapter Layer improves the accuracy on the 5-way\n1-shot task of miniImagenet from 93.07% to 95.71%, and\nthe learnable prompt elevates the accuracy to 96.63%. With\nthe Visual Adapter Layer and the learnable prompt applied\ntogether, the accuracy rate reaches 97.95%, 4.88% higher than\nthe baseline.\nF. Results on Different Vision Backbones\nThe results in Fig. 4 are based on the backbone ResNet50\nfor fair comparison with other methods, but SgVA-CLIP is\nalso effective on other vision backbones. Considering that\nonly CoOp [12] did a comprehensive analysis experiment of\nViT-B/16, ViT-B/32, ResNet50 and ResNet101 backbones, we\nreport more comparison results with CoOp as shown in Table\nVII. SgVA-CLIP surpasses CoOp by 2.74%, 1.86%, 1.53%\nand 1.25% on average respectively on ViT-B/16, ViT-B/32,\nResNet50 and ResNet101 backbones.\nG. Parameter Analysis\nParameter analysis of the temperature τ2 in distillation.\nTo analyze the effect of the temperature τ2 in distillation, we\nconduct experiments on ImageNet and SUN397 with different\nsettings. It is worth noting that the tenperature τ1 used in\nthe cross-modal contrastive loss and vision-speciﬁc contrastive\nloss is outside the scope of parameter analysis because it is\na pre-trained parameter of CLIP. The temperature τ2 controls\nTABLE VIII\nEFFECT OF TEMPERATURE IN KNOWLEDGE DISTILLATION.\nDistillation\nTemperature τ2\nVision\nBackbone\nImageNet (%)\n16-shot\nSUN397 (%)\n16-shot\nAverage (%)\n5\nResNet50\n65.64\n71.99\n68.82\n10\n65.70\n71.86\n68.78\n15\n65.28\n71.58\n68.43\n20\n64.78\n71.49\n68.14\n25\n64.63\n71.37\n68.00\nTABLE IX\nEFFECT OF THE HIDDEN DIMENSION IN THE VISUAL ADAPTING LAYER.\nVision\nBackbone\nHidden\nDimension\nImageNet (%)\nSUN397 (%)\nAverage (%)\n16-shot\n16-shot\nResNet50\n512\n65.28\n71.19\n68.24\n1024\n65.16\n71.37\n68.27\n2048\n65.53\n71.86\n68.70\n4096\n65.64\n71.99\n68.82\n8192\n65.48\n71.96\n68.72\nthe smoothness of the soft labels. As shown in Table VIII, we\nobtain the best performance when τ2 is 5.\nParameter analysis of the hidden dimension in the visual\nadapting layer. Table IX shows the results of using different\nhidden dimensions of the visual adapter layer. We observe\nthat either too small or too large dimension will deteriorate\nthe performance and the best adapter dimension is 4096,\nwhich is able to preserve enough visual information without\nredundancy. Therefore, we set the dimension to 4096 in the\nexperiment.\nH. Visualization\nIn Fig. 5, we sample 10 classes and display the distribution\nof the adapted visual features obtained by SgVA-CLIP and\nthe pre-trained visual features obtained by CLIP. The visual-\nization results show that the adapted visual features are more\ndiscriminative than the pre-trained visual features, and thus it\nis important to consider the adapted visual features in few-shot\nclassiﬁcation.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n \n \n(a)\n \nThe pre-trained visual features\n \n \n \n(b)\n \nAdapted\n \nvisual\n \nfeatures\nFig. 5. Visualization of the distribution of the pre-trained visual features and\nadapted visual features.\nV. CONCLUSION\nWe present SgVA-CLIP, a new VLP-based few-shot clas-\nsiﬁcation approach, which can comprehensively consider uni-\n",
    "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 28, NOVEMBER 2022\n10\nmodal vision correlation and cross-modal image-text corre-\nlation. SgVA-CLIP focuses on the contrastive learning in\ntwo spaces and knowledge distillation between them, so that\nﬁne-grained cross-modal knowledge sharing can promote the\nlearning of discriminative adapted unimodal vision represen-\ntations. With the CLIP model frozen and only a few external\nparameters updated, the representation ability of CLIP can be\nquickly migrated to downstream classiﬁcation tasks by a few\nlabeled data. According to the experimental results, SgVA-\nCLIP outperforms competitive baselines on 13 datasets under\ndifferent few-shot settings. In future work, we will combine\nSgVA-CLIP with other efﬁcient tuning methods and explore\nthe application of SgVA-CLIP in more downstream tasks.\nREFERENCES\n[1] T. M. Hospedales, A. Antoniou, P. Micaelli, and A. J.\nStorkey, “Meta-learning in neural networks: A survey,”\nIEEE transactions on pattern analysis and machine in-\ntelligence, 2021.\n[2] X. Zhong, C. Gu, M. Ye, W. Huang, and C.-W. Lin,\n“Graph complemented latent representation for few-shot\nimage classiﬁcation,” IEEE Transactions on Multimedia,\npp. 1–1, 2022.\n[3] Y. Li, Z. Liu, L. Yao, and X. Chang, “Attribute-modulated\ngenerative meta learning for zero-shot learning,” IEEE\nTransactions on Multimedia, pp. 1–1, 2021.\n[4] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton,\n“A simple framework for contrastive learning of visual\nrepresentations,” in International conference on machine\nlearning, pp. 1597–1607, PMLR, 2020.\n[5] K. He, X. Chen, S. Xie, Y. Li, P. Doll´ar, and R. Girshick,\n“Masked autoencoders are scalable vision learners,” in\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 16000–16009, 2022.\n[6] Z. Tao, X. Liu, Y. Xia, X. Wang, L. Yang, X. Huang,\nand T.-S. Chua, “Self-supervised learning for multimedia\nrecommendation,” IEEE Transactions on Multimedia,\npp. 1–10, 2022.\n[7] Y. Liu, J. Wu, L. Qu, T. Gan, J. Yin, and L. Nie, “Self-\nsupervised correlation learning for cross-modal retrieval,”\nIEEE Transactions on Multimedia, pp. 1–1, 2022.\n[8] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh,\nS. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark,\net al., “Learning transferable visual models from natural\nlanguage supervision,” in International Conference on\nMachine Learning, pp. 8748–8763, PMLR, 2021.\n[9] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham,\nQ. Le, Y.-H. Sung, Z. Li, and T. Duerig, “Scaling up\nvisual and vision-language representation learning with\nnoisy text supervision,” in International Conference on\nMachine Learning, pp. 4904–4916, PMLR, 2021.\n[10] L. Yuan, D. Chen, Y.-L. Chen, N. Codella, X. Dai, J. Gao,\nH. Hu, X. Huang, B. Li, C. Li, et al., “Florence: A new\nfoundation model for computer vision,” arXiv preprint\narXiv:2111.11432, 2021.\n[11] W. Xia, Q. Wang, Q. Gao, M. Yang, and X. Gao, “Self-\nconsistent contrastive attributed graph clustering with\npseudo-label prompt,” IEEE Transactions on Multimedia,\npp. 1–13, 2022.\n[12] K. Zhou, J. Yang, C. C. Loy, and Z. Liu, “Learning\nto prompt for vision-language models,” International\nJournal of Computer Vision, pp. 1–12, 2022.\n[13] P. Gao, S. Geng, R. Zhang, T. Ma, R. Fang, Y. Zhang,\nH. Li, and Y. Qiao, “Clip-adapter: Better vision-\nlanguage models with feature adapters,” arXiv preprint\narXiv:2110.04544, 2021.\n[14] B. Zhu, Y. Niu, Y. Han, Y. Wu, and H. Zhang, “Prompt-\naligned gradient for prompt tuning,” 2022.\n[15] A. Vedaldi, Y. Jia, E. Shelhamer, J. Donahue, S. Karayev,\nJ. Long, and T. Darrell, “Convolutional architecture for\nfast feature embedding,” Cornell University, 2014.\n[16] Z. Shen, Z. Liu, J. Qin, M. Savvides, and K.-T. Cheng,\n“Partial is better than all: Revisiting ﬁne-tuning strategy\nfor few-shot learning,” in Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence, vol. 35, pp. 9594–9602,\n2021.\n[17] V. Kumar, H. Glaude, C. de Lichy, and W. Camp-\nbell, “A closer look at feature space data augmenta-\ntion for few-shot intent classiﬁcation,” arXiv preprint\narXiv:1910.04176, 2019.\n[18] S. W. Yoon, J. Seo, and J. Moon, “Tapnet: Neural net-\nwork augmented with task-adaptive projection for few-\nshot learning,” in International Conference on Machine\nLearning, pp. 7115–7123, PMLR, 2019.\n[19] Q. Sun, Y. Liu, T.-S. Chua, and B. Schiele, “Meta-\ntransfer learning for few-shot learning,” in Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 403–412, 2019.\n[20] R. Zhang, R. Fang, P. Gao, W. Zhang, K. Li, J. Dai,\nY. Qiao, and H. Li, “Tip-adapter: Training-free clip-\nadapter for better vision-language modeling,” arXiv\npreprint arXiv:2111.03930, 2021.\n[21] R. Zhang, L. Qiu, W. Zhang, and Z. Zeng, “Vt-clip:\nEnhancing vision-language models with visual-guided\ntexts,” arXiv preprint arXiv:2112.02399, 2021.\n[22] J. Li, S. Savarese, and S. C. Hoi, “Masked unsupervised\nself-training for zero-shot image classiﬁcation,” arXiv\npreprint arXiv:2206.02967, 2022.\n[23] T. Huang, J. Chu, and F. Wei, “Unsupervised prompt\nlearning for vision-language models,” arXiv preprint\narXiv:2204.03649, 2022.\n[24] M. Wortsman, G. Ilharco, J. W. Kim, M. Li, S. Kornblith,\nR. Roelofs, R. G. Lopes, H. Hajishirzi, A. Farhadi,\nH. Namkoong, et al., “Robust ﬁne-tuning of zero-shot\nmodels,” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 7959–\n7971, 2022.\n[25] K. Zhou, J. Yang, C. C. Loy, and Z. Liu, “Conditional\nprompt learning for vision-language models,” in Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 16816–16825, 2022.\n[26] J. Snell, K. Swersky, and R. Zemel, “Prototypical net-\nworks for few-shot learning,” Advances in neural infor-\nmation processing systems, vol. 30, 2017.\n[27] Y. Chen, X. Wang, Z. Liu, H. Xu, and T. Darrell, “A new\n",
    "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 28, NOVEMBER 2022\n11\nmeta-baseline for few-shot learning,” 2020.\n[28] C. Xing, N. Rostamzadeh, B. Oreshkin, and P. O.\nO Pinheiro, “Adaptive cross-modal few-shot learning,”\nAdvances in Neural Information Processing Systems,\nvol. 32, 2019.\n[29] F. Pahde, M. Puscas, T. Klein, and M. Nabi, “Multimodal\nprototypical networks for few-shot learning,” in Proceed-\nings of the IEEE/CVF Winter Conference on Applications\nof Computer Vision, pp. 2644–2653, 2021.\n[30] C. Buciluˇa, R. Caruana, and A. Niculescu-Mizil, “Model\ncompression,” in Proceedings of the 12th ACM SIGKDD\ninternational conference on Knowledge discovery and\ndata mining, pp. 535–541, 2006.\n[31] G. Hinton, O. Vinyals, J. Dean, et al., “Distilling\nthe knowledge in a neural network,” arXiv preprint\narXiv:1503.02531, vol. 2, no. 7, 2015.\n[32] X. Li, Q. Sun, L. Jiao, F. Liu, X. Liu, L. Li, P. Chen, and\nY. Zuo, “d3k: Dynastic data-free knowledge distillation,”\nIEEE Transactions on Multimedia, pp. 1–14, 2023.\n[33] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distilbert,\na distilled version of bert: smaller, faster, cheaper and\nlighter,” arXiv preprint arXiv:1910.01108, 2019.\n[34] X.\nJiao,\nY.\nYin,\nL.\nShang,\nX.\nJiang,\nX.\nChen,\nL. Li, F. Wang, and Q. Liu, “Tinybert: Distilling\nbert for natural language understanding,” arXiv preprint\narXiv:1909.10351, 2019.\n[35] Z. Fang, J. Wang, X. Hu, L. Wang, Y. Yang, and Z. Liu,\n“Compressing visual-linguistic model via knowledge dis-\ntillation,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 1428–1438, 2021.\n[36] F. M. Hafner, A. Bhuyian, J. F. Kooij, and E. Granger,\n“Cross-modal\ndistillation\nfor\nrgb-depth\nperson\nre-\nidentiﬁcation,” Computer Vision and Image Understand-\ning, vol. 216, p. 103352, 2022.\n[37] W. I. Cho, D. Kwak, J. W. Yoon, and N. S. Kim, “Speech\nto text adaptation: Towards an efﬁcient cross-modal dis-\ntillation,” arXiv preprint arXiv:2005.08213, 2020.\n[38] Q. Fan, W. Pei, Y.-W. Tai, and C.-K. Tang, “Self-\nsupport few-shot semantic segmentation,” in European\nConference on Computer Vision, pp. 701–719, Springer,\n2022.\n[39] Y. Zhang, H. Fei, D. Li, T. Yu, and P. Li, “Prompting\nthrough prototype: A prototype-based prompt learning\non pretrained vision-language models,” arXiv preprint\narXiv:2210.10841, 2022.\n[40] O. Vinyals, C. Blundell, T. Lillicrap, k. kavukcuoglu, and\nD. Wierstra, “Matching networks for one shot learning,”\nin Advances in Neural Information Processing Systems\n(D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and\nR. Garnett, eds.), vol. 29, Curran Associates, Inc., 2016.\n[41] M. Ren, E. Triantaﬁllou, S. Ravi, J. Snell, K. Swersky,\nJ. B. Tenenbaum, H. Larochelle, and R. S. Zemel,\n“Meta-learning for semi-supervised few-shot classiﬁca-\ntion,” arXiv preprint arXiv:1803.00676, 2018.\n[42] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bern-\nstein, et al., “Imagenet large scale visual recognition\nchallenge,” International journal of computer vision,\nvol. 115, no. 3, pp. 211–252, 2015.\n[43] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and\nL. Fei-Fei, “Imagenet: A large-scale hierarchical image\ndatabase,” in 2009 IEEE conference on computer vision\nand pattern recognition, pp. 248–255, Ieee, 2009.\n[44] J. Krause, M. Stark, J. Deng, and L. Fei-Fei, “3d ob-\nject representations for ﬁne-grained categorization,” in\nProceedings of the IEEE international conference on\ncomputer vision workshops, pp. 554–561, 2013.\n[45] K. Soomro, A. R. Zamir, and M. Shah, “Ucf101: A\ndataset of 101 human actions classes from videos in the\nwild,” arXiv preprint arXiv:1212.0402, 2012.\n[46] L. Fei-Fei, R. Fergus, and P. Perona, “Learning gen-\nerative visual models from few training examples: An\nincremental bayesian approach tested on 101 object\ncategories,” in 2004 conference on computer vision and\npattern recognition workshop, pp. 178–178, IEEE, 2004.\n[47] M.-E. Nilsback and A. Zisserman, “Automated ﬂower\nclassiﬁcation over a large number of classes,” in 2008\nSixth Indian Conference on Computer Vision, Graphics\n& Image Processing, pp. 722–729, IEEE, 2008.\n[48] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Tor-\nralba, “Sun database: Large-scale scene recognition from\nabbey to zoo,” in 2010 IEEE computer society conference\non computer vision and pattern recognition, pp. 3485–\n3492, IEEE, 2010.\n[49] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and\nA. Vedaldi, “Describing textures in the wild,” in Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, pp. 3606–3613, 2014.\n[50] P. Helber, B. Bischke, A. Dengel, and D. Borth, “Eurosat:\nA novel dataset and deep learning benchmark for land\nuse and land cover classiﬁcation,” IEEE Journal of Se-\nlected Topics in Applied Earth Observations and Remote\nSensing, vol. 12, no. 7, pp. 2217–2226, 2019.\n[51] S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and\nA. Vedaldi, “Fine-grained visual classiﬁcation of air-\ncraft,” arXiv preprint arXiv:1306.5151, 2013.\n[52] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. Jawahar,\n“Cats and dogs,” in 2012 IEEE conference on computer\nvision and pattern recognition, pp. 3498–3505, IEEE,\n2012.\n[53] L. Bossard, M. Guillaumin, and L. V. Gool, “Food-101–\nmining discriminative components with random forests,”\nin European conference on computer vision, pp. 446–\n461, Springer, 2014.\n[54] Y. Hu, S. Pateux, and V. Gripon, “Squeezing backbone\nfeature distributions to the max for efﬁcient few-shot\nlearning,” Algorithms, vol. 15, no. 5, p. 147, 2022.\n[55] Y. He, W. Liang, D. Zhao, H.-Y. Zhou, W. Ge, Y. Yu,\nand W. Zhang, “Attribute surrogates learning and spectral\ntokens pooling in transformers for few-shot learning,” in\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 9119–9129, 2022.\n[56] K. Kim, M. Laskin, I. Mordatch, and D. Pathak, “How to\nadapt your large-scale vision-and-language model,” 2021.\n[57] S. X. Hu, D. Li, J. St¨uhmer, M. Kim, and T. M.\nHospedales, “Pushing the limits of simple pipelines for\n",
    "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 28, NOVEMBER 2022\n12\nfew-shot learning: External data and ﬁne-tuning make a\ndifference,” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 9068–\n9077, 2022.\n[58] J. Ling, L. Liao, M. Yang, and J. Shuai, “Semi-supervised\nfew-shot learning via multi-factor clustering,” in Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 14564–14573, 2022.\n[59] Y. Hu, V. Gripon, and S. Pateux, “Leveraging the fea-\nture distribution in transfer-based few-shot learning,” in\nInternational Conference on Artiﬁcial Neural Networks,\npp. 487–499, Springer, 2021.\n[60] P. Rodr´ıguez, I. Laradji, A. Drouin, and A. Lacoste,\n“Embedding propagation: Smoother manifold for few-\nshot classiﬁcation,” in European Conference on Com-\nputer Vision, pp. 121–138, Springer, 2020.\n[61] Y. Bendou, Y. Hu, R. Lafargue, G. Lioi, B. Pas-\ndeloup, S. Pateux, and V. Gripon, “Easy: Ensem-\nble augmented-shot y-shaped learning: State-of-the-art\nfew-shot classiﬁcation with simple ingredients,” arXiv\npreprint arXiv:2201.09699, 2022.\n"
  ],
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 28, NOVEMBER 2022\n1\nSgVA-CLIP: Semantic-guided Visual Adapting of\nVision-Language Models for Few-shot Image\nClassiﬁcation\nFang Peng, Xiaoshan Yang, Linhui Xiao, Yaowei Wang and Changsheng Xu, Fellow, IEEE\nAbstract—Although signiﬁcant progress has been made in\nfew-shot learning, most of existing few-shot image classiﬁcation\nmethods require supervised pre-training on a large amount of\nsamples of base classes, which limits their generalization ability\nin real world application. Recently, large-scale Vision-Language\nPre-trained models (VLPs) have been gaining increasing attention\nin few-shot learning because they can provide a new paradigm for\ntransferable visual representation learning with easily available\ntext on the Web. However, the VLPs may neglect detailed visual\ninformation that is difﬁcult to describe by language sentences,\nbut important for learning an effective classiﬁer to distinguish\ndifferent images. To address the above problem, we propose a new\nframework, named Semantic-guided Visual Adapting (SgVA),\nwhich can effectively extend vision-language pre-trained models\nto produce discriminative adapted visual features by comprehen-\nsively using an implicit knowledge distillation, a vision-speciﬁc\ncontrastive loss, and a cross-modal contrastive loss. The implicit\nknowledge distillation is designed to transfer the ﬁne-grained\ncross-modal knowledge to guide the updating of the vision\nadapter. State-of-the-art results on 13 datasets demonstrate that\nthe adapted visual features can well complement the cross-modal\nfeatures to improve few-shot image classiﬁcation.\nIndex Terms—few-shot, image classiﬁcation, vision-language\nmodels.\nI. INTRODUCTION\nFew-shot learning refers to the task of learning a new\nconcept with only a few labeled samples, which is inspired\nby human learning ability. As labeling is often expensive in\nreal scenarios, few-shot learning has become an important\nand widely studied problem. However, with little supervision\ninformation, learning to recognize new classes is challenging\nbecause directly training the model from a few samples may\noverﬁt. A common idea in few-shot learning is to train the\nmodel from base classes with sufﬁcient samples to get prior\nknowledge and then migrate to the novel classes with a few\nFang Peng and Linhui Xiao are with the National Laboratory of Pat-\ntern Recognition, Institute of Automation, Chinese Academy of Sciences,\nBeijing 100190, China, also with the Peng Cheng Laboratory, Shenzhen\n518066, China, and also with the School of Artiﬁcial Intelligence, Uni-\nversity of Chinese Academy of Sciences, Beijing 100049, China (e-mail:\npengfang21@mails.ucas.ac.cn, xiaolinhui16@mails.ucas.ac.cn).\nYaowei Wang is with the Peng Cheng Laboratory, Shenzhen 518066, China\n(e-mail: wangyw@pcl.ac.cn).\nXiaoshan Yang and Changsheng Xu are with the National Laboratory\nof Pattern Recognition, Institute of Automation, Chinese Academy of Sci-\nences, Beijing 100190, China, also with the School of Artiﬁcial Intelligence,\nUniversity of Chinese Academy of Sciences, Beijing 100049, China, and\nalso with the Peng Cheng Laboratory, Shenzhen 518066, China (e-mail:\nxiaoshan.yang@nlpr.ia.ac.cn, csxu@nlpr.ia.ac.cn).\nChangsheng Xu is the corresponding author.\n(a) Previous VLP-based few-shot learning methods\nText \nEncoder\n(b) The proposed SgVA-CLIP\ncross-modal text embedding\nadapted visual feature\nAdapter \nLayer\nText \nEncoder\nImage \nEncoder\nAdapter \nLayer\nProjection \nLayer\nProjection \nLayer\nProjection \nLayer\nProjection \nLayer\nImplicit Knowledge\nDistillation\nhoneysucker\ntoucan\nAdapted\nVisual Space\nPre-trained \nCross-modal Space\nPre-trained \nCross-modal Space\ncross-modal visual embedding \nImage \nEncoder\nAdapter \nLayer\nprompt + \nhoneysucker\ntoucan\nprompt + \nFig. 1.\nSgVA-CLIP vs. previous VLP-based few-shot learning methods.\n(a) Previous VLP-based few-shot learning methods focus on enhancing the\ncross-modal alignment, which may neglect important task-speciﬁc visual in-\nformation (e.g., the two birds honeysucker and toucan have different beaks) for\ndistinguishing different images when the labeled samples are insufﬁcient. (b)\nSgVA-CLIP makes a comprehensive consideration of adapted visual feature\nspace and pre-trained cross-modal feature space. The adapted visual features\nprovide more discriminative visual information and thus can well complement\nthe cross-modal features to improve few-shot image classiﬁcation.\nexamples. Existing studies on few-shot image classiﬁcation\ncan be roughly divided into three categories, namely ﬁne-\ntuning based methods, data augmentation based methods and\nmeta learning based methods. Among them, the most widely\nstudied is meta-learning [1–3], which acquires the abstract\nlearning ability to generalize to new classes by learning\nmeta-knowledge from a set of different meta tasks. Although\nsigniﬁcant progress has been made in few-shot learning, most\nof existing few-shot learning methods require the network to\nbe pre-trained in a supervised manner on a large amount of\nlabeled data of base classes. As a result, the current few-shot\nlearning methods have limited generalization ability and is\nimpractical in the real world due to the shortage of supervised\ndata.\narXiv:2211.16191v2  [cs.CV]  20 Jan 2023\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 28, NOVEMBER 2022\n2\nRecently, self-supervised learning [4–7] has emerged as a\npossible solution to alleviate the dependency on large-scale\nlabeled data. Self-supervised learning exploits pretext tasks\nto mine supervised information from large-scale unsupervised\ndata, thereby learning rich implicit priors and latent repre-\nsentations. With the development of self-supervised learning,\nVision-Language Pre-trained models (VLPs), e.g., CLIP [8],\nALIGN [9], and Florence [10], attract more and more attention\ndue to its signiﬁcant performance in a variety of downstream\ntasks, such as image classiﬁcation and visual question answer-\ning. VLPs like CLIP can provide effective visual and semantic\nknowledge of open-world concepts that are learned on large-\nscale image-text pairs, laying a good generalization foundation\nof few-shot image classiﬁcation.\nVLPs have been successfully applied to few-shot image\nclassiﬁcation with the help of carefully designed text prompts\n[8, 11], which can change the discrete class labels into\nlanguage sentences. For example, CLIP model [8] learns vision\nand language representations by aligning the image and text in\na cross-modal joint space, which allows images to be correctly\nclassiﬁed via image-text similarity. There are also VLPs-\nbased few-shot learning methods that focus on enhancing the\nimage-text alignment. Context Optimization (CoOp) [12] is\nproposed to improve the text embedding of CLIP by soft\nprompt engineering. CLIP-Adapter [13] ﬁne-tunes the image\nrepresentation by adjusting an extra bottleneck layer. ProGrad\n[14] proposes Prompt-aligned Gradient to prevent prompt\ntuning from forgetting the general knowledge learned from\nVLPs.\nExisting methods only consider the image-text alignment\nwhen transferring the VLPs to solve few-shot image classiﬁ-\ncation. Although relying on the image-text similarity can well\ncapture the visual and semantic knowledge learned by the pre-\ntrained model, it is sometimes unreliable to recognize objects\nwithout comprehensively considering the speciﬁc discrimina-\ntive visual information of the few-shot task. The reason is that\nto learn a good image-text alignment model, the pre-trained\nVLPs may neglect detailed visual information that is difﬁcult\nto describe by language sentences. However, the neglected\nvisual information is probably important for distinguishing\ndifferent images when the labeled samples are insufﬁcient.\nTo address the above problem, we propose a new frame-\nwork, named Semantic-guided Visual Adapting (SgVA), which\ncan effectively extend vision-language pre-trained models\n(e.g., CLIP) to produce discriminative adapted visual features\nwith the guidance of the ﬁne-grained cross-modal knowledge\nlearned by the pre-trained model. The adapted visual features\ncan well complement the cross-modal features to improve\nfew-shot image classiﬁcation. Fig. 1 shows the main idea of\nour work. Speciﬁcally, our method is extended from the pre-\ntrained CLIP. Given labeled support images and unlabeled\nquery images, we ﬁrstly extract the visual features for the\nimages from the output before the cross-modal projection layer\nof the CLIP model, which are referred to as pre-trained visual\nfeatures. And cross-modal embeddings for both the images and\nthe prompted texts of the class labels are extracted from the\noutput of the cross-modal projection layer. Next, we map the\npre-trained visual features to the adapted visual features by a\nvisual adapting layer. We update the visual adapting layer on\nthe few-shot samples by a vision-speciﬁc contrastive loss and\ncross-modal contrastive loss with the help of vision prototypes\nand cross-modal prototypes that are obtained by averaging the\ncorresponding sample features of a given class. Moreover, we\nadopt an implicit distillation to utilize the ﬁne-grained cross-\nmodal knowledge (i.e., relative similarities between samples\nand prototypes in the pre-trained cross-modal space) learned\nby the pre-trained model to guide the updating of the vision\nadapter. Finally, we infer the class label for a given query\nsample by jointly considering its distance to vision-speciﬁc\nprototypes and cross-modal prototypes.\nOur contributions are summarized as follows. We propose\na new framework of semantic-guided visual adapting, which\nﬂexibly extends the vision-language pre-trained models (e.g.,\nCLIP) to produce discriminative adapted visual features by\njointly using implicit knowledge distillation, vision-speciﬁc\ncontrastive loss, and cross-modal contrastive loss. We obtain\nnew state-of-the-art results in few-shot image classiﬁcation by\ncomprehensively considering the sample relations based on\nboth the adapted visual features and the cross-modal features,\nwhich demonstrates a strong complementarity between the two\nkinds of feature space and also provides a promising direction\nfor future research.\nII. RELATED WORK\nThis section reviews three topics closely related to our\nwork in terms of few-shot learning, prototype networks and\nknowledge distillation.\nA. Few-shot Learning\nFew-shot learning aims to learn a model that can recognize\nnew classes with a few training samples. The widely studied\nconventional few-shot learning methods include ﬁne-tuning\n[15, 16], data/feature augmentation [17, 18], and meta learning\n[1, 19]. Recently, Vision-language pre-trained models (VLPs)\n(e.g. CLIP [8] and ALIGN [9]) have been applied to few-shot\nlearning by transferring the powerful representation ability. In\norder to realize data-efﬁcient ﬁne-tuning, CoOp [12] improves\nthe ability of image-text alignment through continuous prompt\noptimization, and CLIP-Adapter [13] designs lightweight fea-\nture adapters to explore simple ﬁne-tuning. After that, Tip-\nAdapter [20], a training-free method, is proposed to save\ncomputational resources. Different from them, VT-CLIP [21]\nimproves the interaction of image and text branches of CLIP\nby cross-modal module. Other works like MUST [22] and\nUPL [23] think about unsupervised learning. Besides, WiSE-\nFT [24] and CoCoOp [25] consider both the accuracy of target\ndistribution and robustness to distribution shifts. Unlike the\nabove methods that focus on image-text contrastive learning,\nwe extend the pre-trained CLIP to learn more discriminative\nvisual features that can well complement the cross-modal\nfeatures in few-shot learning.\nB. Prototype Networks\nPrototype network [26] is proposed in 2017 to solve the\nproblem of few-shot classiﬁcation, which aims at learning a\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 28, NOVEMBER 2022\n3\nmetric space where query samples can be accurately classiﬁed\nby calculating the distances between queries and prototypes.\nCompared with other few-shot learning methods, Prototype\nnetwork reﬂects a simpler inductive bias, which is beneﬁcial\nin the case of limited data. Owing to the potential of this\nparadigm, many variations have been developed since then.\nChen et al. [27] found that introducing an extra pre-training\nphase on the entire base classes could improve performance,\nbut it leads to poor generalization ability. Early prototype\nnetworks only employ visual information, but increasingly\nthere are approaches to explore how semantic knowledge\ncan enhance the performance. For example, Chen et al. [28]\nlearned semantic knowledge from unsupervised corpora, and\nproposed an adaptive modality mixing mechanism to combine\nthe visual and semantics knowledge, showing improvements in\nfew-shot learning. Frederik et al. [29] mapped text data to the\nvisual embedding space with the help of a generative model,\nand then designed a strategy to combine the real and generated\nfeatures through the nearest neighbor algorithm. Instead of\nonly forming a single metric space as in existing prototype\nnetworks, we construct two metric spaces including visual and\ncross-modal spaces to comprehensively conduct the few-shot\nlearning.\nC. Knowledge Distillation\nKnowledge Distillation (KD) [30–32] means transferring\nthe knowledge from the pre-trained complex model (teacher\nmodel) to a simpler structured network (student model). Owing\nto its superior performance in knowledge transferring and\nmodel enhancement, KD is widely used in model compression\nand transfer learning. In the process of KD, the output of\nteacher model is used as the supervision signal to train the\nstudent model through the distillation loss. And the optimiza-\ntion target is to make the class-level probability distribution of\nthe student model match the probability output of the teacher\nmodel. In terms of model compression, DistillBert [33] and\nTinyBert [34] use KD to explore smaller and faster models\nfor language representation learning. In addition to model\ncompression, KD also plays an important role in knowledge\ntransferring between different modalities. For example, the\nvision-language distillation framework DistillVLM [35] is\nproposed to improve vision-language tasks like image cap-\ntioning and VQA. Hafner et al. [36] propose a novel cross-\nmodal distillation method for robust person re-identiﬁcation,\nwhich transfers knowledge from RGB images to depth images.\nBesides, in [37], semantic knowledge is transmitted from\nlanguage model to a spoken language understanding module,\nso that the deﬁciency of speech data can be alleviated. The\nabove works show the beneﬁts of KD in both self-supervised\nlearning and multi-modal alignment. Different from existing\nmethods, we propose a knowledge distillation which can\nimplicitly transfer the ﬁne-grained relation knowledge learned\nin the cross-modal space to the visual space by conducting the\ndistillation in the cross-modal space.\nIII. THE PROPOSED METHOD\nA. Problem Deﬁnition\nThe few-shot learning methods usually depend on base\nclasses with ample samples to learn how to generalize to\nnovel classes. However, in the real few-shot scenario, sufﬁcient\nsamples of base classes may not be available. It is worth\nnoting that since the pre-trained vision and language model,\ne.g., CLIP, can provide a good foundation model for the few-\nshot learning, the base classes are not indispensable. In this\nwork, we consider both the standard meta-learning scenario\nwith base classes and the scenario without base classes.\nIn the standard meta-learning scenario, a series of meta-\ntasks (episodes) are created for training and testing. For each\nmeta-task in the meta-training phase, N (Way) base classes\nand K (Shot) samples for each base class are randomly\nselected to make up the Support Set S = {(xi, yi)}N×K\ni=1\n,\nwhere xi denotes the sampled image, and yi is the label of\nxi. Other M samples of the N (Way) classes are randomly\nselected to form the Query Set Q = {(xi, yi)}M\ni=1. In the meta-\ntest phase, the Support Set is created from K-shot labeled\nsamples of novel classes and the query set is created from\nunlabeled samples of novel classes.\nIn the scenario without base classes, inspired by the idea\nof self-supporting from [38], we build both the Support Set\nand the Query Set from the K-shot labeled samples of novel\nclasses in the meta-training phase. In the meta-test phase, the\nSupport Set is created from K-shot labeled samples of novel\nclasses and the Query Set is created from unlabeled samples\nof novel classes.\nB. Network Architecture\nThe proposed framework, namely Semantic-guided Visual\nAdapting (SgVA), aims to learn discriminative adapted visual\nfeatures with the guidance of the cross-modal knowledge\nlearned by the pre-trained CLIP model. The adapted visual fea-\ntures can well complement the cross-modal features in the few-\nshot image classiﬁcation. Fig. 2 shows the overall framework.\nGiven support images and query images in each episode, the\nclass labels of support samples are ﬁrstly transformed into text\ninput by L-length learnable prompt vectors as in [12]. Then,\nthe images and texts go through the frozen image and text\nencoders of CLIP to generate the pre-trained visual features\n(xv) and text features (xt) respectively, which are further\nmapped to cross-modal visual and text embeddings, i.e., xc v\nand xc t through different projection layers (i.e., φ and ψ).\nMeanwhile, the pre-trained visual features (xv) are mapped\nto the adapted visual features (xa) by a visual adapting layer.\nTo fully exploit sample relations in the visual space and\ncross-modal space, we build vision-speciﬁc prototypes and\ncross-modal prototypes that are obtained by averaging the\ncorresponding support sample features of a given class. Based\non the prototypes, we update the visual adapting layer on\nthe few-shot samples by an implicit knowledge distillation, a\nvision-speciﬁc contrastive loss, and a cross-modal contrastive\nloss. The implicit knowledge distillation utilizes ﬁne-grained\nsample relations in the cross-modal feature space to guide the\nlearning of the visual adapting to produce more discriminative\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 28, NOVEMBER 2022\n4\n...\n...\n...\nFused \nPrediction \nModule\n \nSupport Samples\n...\n[CLS]\nAdapted Visual Space\nupdated\nfrozen\nfrozen\n...\nQuery Samples\n��_�\n��\n��_� \n[��(1),..,��(�)]\n�\nCLIP Image \nEncoder \n[V]1 [V]2\n[V]L\n...\nprompt vectors×L\n.\nCLIP Text \nEncoder\nProjection\n  Layer �\nProjection\n   Layer �\n��\n��_�\n��_�\n��\nVisual Adapter\n��\nBackpropagation of gradient \n��\nVision-specific\nContrastive Loss\nCross-modal\nContrastive Loss\n��\nImplicit Knowledge\nDistillation Loss\nProjection\n  Layer �\nPre-trained \nCross-modal Space\n[��_�(1),..,��_�(�)]\nMLP\n＋\nQuery\nSupport\nQuery\nSupport\nN-Way K-Shot\nK\nFig. 2.\nOverview of the proposed SgVA-CLIP. In the pre-trained cross-modal space, we build cross-modal prototypes that are obtained by averaging the\ncross-modal text embedding, i.e., xc t of the support samples. Meanwhile, in the adapted visual space, we build vision-speciﬁc prototypes that are obtained by\naveraging the adapted visual features after the visual adapting layer, i.e., xa of the corresponding support samples. The SgVA-CLIP can learn discriminative\nadapted visual features with the guidance of the cross-modal knowledge learned by the pre-trained CLIP. And the discriminative visual features can well\ncomplement the cross-modal features in the few-shot image classiﬁcation.\nvisual features. In the inference process, test image can be\nrecognized by jointly considering its distance to the vision-\nspeciﬁc prototypes and cross-modal prototypes.\nC. Shot-speciﬁc Text Prompt\nTo transform the class label into natural language sentence\nthat can be directly processed by CLIP, we adopt prompted\ntext with individual differences as in [39]. We independently\ninitialize an individual prompt ti for each shot from the same\nclass. We use K different prompts in each N-way K-shot\nepisode according to the number of shots and we use shared\nprompts for different classes, which is referred to as shot-\nspeciﬁc text prompt. As illustrated in [39], using a ﬁxed\namount of prompts instead of a universal prompt can capture\nsubtle differences among different samples and can also be\nmore efﬁcient than designing a speciﬁc prompt for every\ninput sample. The text prompt ti is formally deﬁned as the\nconcatenation of L learnable continuous vectors and the class\nname embedding:\nti = [V ]i,1[V ]i,2...[V ]i,L[CLS]i[.],\n(1)\nwhere each [V ]i,l, l ∈{1, ..., L}, is a learnable vector with the\nsame dimension as the class embedding [CLS]i. The class\nembedding is a 512-dimensional word embedding obtained\nfrom the pre-trained CLIP.\nD. Visual Adapting Layer\nThe adapting layer consists of a two-layer Multi-layer\nPerceptron (MLP) and an adaptive residual connection. The\nnew feature Newv(xv) acquired in the adapting layer can be\nrepresented as:\nNewv(xv) = ReLU(xv\nT W1)W2,\n(2)\nwhere xv denotes the pre-trained visual feature that is ob-\ntained by the image encoder of the pre-trained CLIP encoding\nthe image of the sample x before the linear projection to\nthe cross-modal embedding space, W1 and W2 are learnable\nweights of the two fully connected layers, and ReLU is\nactivation function. The pre-trained visual feature xv is ﬁxed\nin our framework. Then, the new feature is added to the\noriginal visual feature by an adaptive residual connection:\nxa = [Newv(xv), xv]Wa,\n(3)\nwhere xa is the adapted visual feature, and the Wa ∈R2 is a\nlearnable weight vector. [] denotes the operation of combining\nthe vectors as a matrix.\nE. Cross-modal Contrastive Loss\nFor the convenience of exploring the relations between\ndifferent classes in the cross-modal space, we ﬁrstly calculate\ncross-modal prototypes (pc t) of N classes by averaging the\ncross-modal text embeddings of the support samples for each\nclass. The cross-modal prototype of the k-th class, i.e., pc t(k)\nis represented by\npc t(k) =\n1\n|S(k)|\nX\n(xi,yi)∈S(k)\nxc t(i),\n(4)\nwhere xc t(i) denotes the cross-modal text embedding of the\ni-th sample that belongs to S(k), and S(k) is the set of all the\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 28, NOVEMBER 2022\n5\nsupport samples of class k. As illustrated in Section III-B,\nxc t is obtained by the pre-trained CLIP that is ﬁxed in\nour framework. The prompted text (ti) of the i-th sample is\nencoded by the text encoder of CLIP into the uni-modal text\nfeature xt(i), which is then projected into the cross-modal\nembedding space as cross-modal text embedding xc t(i).\nThe purpose of cross-modal contrast loss is to learn a better\ntextual representation and lay a good semantic foundation\nfor guiding the visual adapting. For every query sample x,\nthe cosine similarity scores between the cross-modal visual\nfeature (xc v) and the cross-modal prototypes (pc t) are\ncalculated, where xc v is obtained by mapping the pre-trained\nvisual feature into the cross-modal embedding space. The\ncosine similarity is scaled by a temperature parameter τ1, and\nnormalized into a probability distribution via Softmax. Then,\nthe cross-modal contrastive loss Lcl i2t is deﬁned as the cross\nentropy loss over the probability distribution:\nLcl i2t = −log\nexp(< xc v, pc t(+) > /τ1)\nPN\nj=1exp(< xc v, pc t(j) > /τ1)\n,\n(5)\nwhere τ1 is a temperature parameter that is initialized to\n0.07 and pre-trained by CLIP [8]. And pc t(+) denotes the\nthe cross-modal prototype of the positive class, < ·, · >\ndenotes cosine similarity, and N is the number of classes. The\nuseful implicit knowledge is included in the relative distances\nbetween cross-modal features.\nF. Vision-speciﬁc Contrastive Loss\nSince the pre-trained CLIP may neglect important visual\ninformation that is difﬁcult to describe in natural language\nsentences, we utilize vision-speciﬁc contrastive loss to make\nthe adapted features (i.e., xa) retain more discriminative visual\ninformation that is speciﬁc to the current few-shot task. For\nevery query sample x, the cosine similarity scores between the\nadapted visual feature (xa) and the vision-speciﬁc prototypes\n(pa) of N classes are calculated by:\npa(k) =\n1\n|S(k)|\nX\n(xi,yi)∈S(k)\nxa(i),\n(6)\nwhere S(k) denotes the set of all the support samples of\nclass k and xa(i) is calculated by Eq. 2 and Eq. 3 for\nthe i-th sample. The purpose of vision-speciﬁc contrastive\nloss is to maximize the cosine similarity between xa and\nthe positive prototype while minimizing the cosine similarity\nbetween xa and negative prototypes. Formally, the vision-\nspeciﬁc contrastive loss Lcl i2i is calculated by:\nLcl i2i = −log\nexp(< xa, pa(+) > /τ1)\nPN\nj=1exp(< xa, pa(j) > /τ1)\n.\n(7)\nG. Implicit Knowledge Distillation\nWith the vision-speciﬁc contrastive loss, we can already\nmake the samples of the same class close to each other, and the\nsamples of different classes far from each other in the adapted\nunimodal vision space. However, the vision contrastive loss\ncannot provide the ﬁne-grained relations between different\n��_�(�)\nPre-trained Cross-modal Space\nAdapted Visiual Space\n��_�(�)\ndstudent\n��_�(�)\n��(�)\n��(�)\nadapter\n��(i ≠ k)\n��(�)\n��(j ≠ k)\nPre-trained Visual Space\n��_�(i ≠ k)\n��_�(j ≠ k)\nadapted visual feature  (��) \ncross-modal prototype  (��_�) \ncross-modal  visual embedding (��_�) \nvision-specific prototype  (��) \n  pre-trained visual feature  (��) \nd teacher\nAdapted Visual Space:\nPre-trained Cross-modal Space:\ncross-modal proxy embedding (��_�) \nProjection \n Layer �\nPre-trained Visual Space:\nFig. 3.\nInterpretation of the implicit knowledge distillation between the\nadapted visual space and the pre-trained cross-modal space. The adapted visual\nfeature xa is mapped to the cross-modal space as a proxy representation\nxc a. The knowledge distillation is performed in the pre-trained cross-modal\nspace by matching the ﬁne-grained sample-prototype relations for the proxy\nrepresentation xc a and the cross-modal visual embedding xc v.\nsamples. As shown in Fig. 1, the two kinds of birds have\ndifferent beaks locally, but the whole is similar in vision. If\nthe vision-speciﬁc contrastive loss is used alone, it is easy\nto ignore the details of beaks, and lead to misclassifying the\nbird. However, they are easy to distinguish in terms of text\nsemantics (i.e., honeysucker and toucan). The cross-modal\ninformation in the pre-trained cross-modal space provides the\nconstraint of relative semantic distances between samples,\nand instructs how far apart the semantically different samples\nshould be, even if they are visually similar as a whole, thus\nhelping to improve the discriminative ability of the adapted\nvisual features.\nAs a solution, we consider to take advantage of the relative\nsample relationships produced by the pre-trained CLIP model,\nwhich cannot be achieved by contrastive learning alone. It is\nworth noting that when compared with the visual data, the\nsemantic meaning of a given class is more likely to be shared\nby the pre-trained CLIP model and the downstream few-shot\ntask. Therefore, we adopt a knowledge distillation loss to\nguide the learning of the vision adapter by the ﬁne-grained\nrelationships in the cross-modal space.\nFig. 3 gives an illustration of the proposed implicit knowl-\nedge distillation. By mapping the adapted visual feature xa\nto the cross-modal space through the projection layer (i.e.,\nφ), a proxy representation of it in the cross-modal space,\nnamely xc a is obtained. Then, we can utilize the ﬁne-grained\nrelations of the cross-modal visual feature xc v to constrain\nthe proxy representation xc a. This means that we can use\ndistances between samples obtained in the cross-modal space\nas extra supervision to implicitly guide the learning of the\nvision adapting layer. The knowledge distillation is performed\nin the cross-modal space, and the gradients are backpropagated\nto the visual adapting layer.\nMore speciﬁcally, for a cross-modal visual feature xc v,\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 28, NOVEMBER 2022\n6\nwe calculate its distances to each of the cross-modal text\nprototypes as a teacher:\ndtea(k) =< xc v, pc t(k) > /τ1,\n(8)\nwhere τ1 is a temperature parameter that is initialized to 0.07,\nand learned by the pre-trained CLIP. The distances between\nxc a and each of the cross-modal text prototypes pc t are\ncalculated as a student:\ndstu(k) =< xc a, pc t(k) > /τ1.\n(9)\nFinally, the knowledge distillation loss is deﬁned as:\nLKD =−\nN\nX\nk=1\nexp(dtea(k)/τ2)\nPN\nj=1 exp(dtea(j)/τ2)\nlog\n\u0010\nexp(dstu(k)/τ2)\nPN\nj=1 exp(dstu(j)/τ2)\n\u0011\n,\n(10)\nwhere τ2 is a hyperparameter that is set to 5 in this paper. The\nparameter analysis of τ2 is shown in Table VIII.\nH. Optimization and Inference\nOur framework can be learned in an end-to-end form for\nfew-shot image classiﬁcation. The framework is optimized\nby jointly considering vision-speciﬁc contrastive loss, cross-\nmodal contrastive loss, and knowledge distillation loss. The\noverall loss is deﬁned as:\nLoss = Lcl i2t + Lcl i2i + LKD.\n(11)\nFor a given test image in the inference phase, we conduct\nfew-shot classiﬁcation by comprehensively considering its\ndistance to the vision-speciﬁc prototypes and cross-modal\nprototypes:\nda(k) =< xa, pa(k) >,\nk = 1, ..., N,\n(12)\ndc t(k) =< xc v, pc t(k) >,\nk = 1, ..., N,\n(13)\nwhere < ·, · > denotes cosine similarity. Speciﬁcally, the\npredicted class ˆy with the maximum posterior probability is\ncalculated by applying the Naive Bayes:\nˆy = argmax\nyk\n2N\nX\nj=1\nlog P\n\u0000d(j) | Y =yk\n\u0001\n,\n(14)\nwhere d is a 2N-dimensional vector obtained by concatenating\nda and dc t.\nIV. EXPERIMENT AND RESULTS\nA. Datasets\nFor the standard meta-learning scenario, we choose mini-\nImagenet [40] and tieredImagenet [41], which are common\nbenchmarks for few-shot learning. The purpose is to validate\nthe generalization ability of our model on novel classes.\nThe miniImagenet dataset contains 100 classes sampled from\nILSVRC-2012 [42], and is split to 64, 16, 20 classes for\ntraining, validation, and testing respectively. Similarly, the\ntieredImagenet dataset includes 608 classes sampled from\nILSVRC-2012, and is divided into 351, 97, 160 classes for\ntraining, validation, and testing respectively. The setting of\ntieredImagenet is more challenging, because the base classes\nand novel classes come from different super categories.\nFor the scenario without base classes, we follow CLIP [8]\nand CoOp [12] to select 11 image classiﬁcation datasets to\nevaluate the performance, namely ImageNet [43], Stanford-\nCars [44], UCF101 [45], Caltech101 [46], Flowers102 [47],\nSUN397 [48], DTD [49], EuroSAT [50], FGVCAircraft [51],\nOxfordPets [52], and Food101 [53]. These datasets cover a\nseries of diverse visual tasks including the classiﬁcation of\ngeneral objects, scenes, actions, and ﬁne-grained categories.\nB. Implementation Details\nFor the conventional setting of few-shot learning, we evalu-\nate our model on two widely used datasets, i.e., miniImagenet\nand tieredImagenet. We train the overall framework on base\nclasses for 100 epochs with 5-way 1-shot/5-shot tasks. And in\neach episode, 15 query images per class are randomly sampled.\nIn test phase, we randomly 600 episodes on novel classes, and\nreport the mean accuracy together with the 95% conﬁdence\ninterval. For the setting without base classes, we follow the\nfew-shot evaluation protocol adopted in CLIP [8] to evaluate\nthe model performance on 11 datasets, and set up 1, 2, 4, 8,\n16 shots from all the classes to train the model and then test it\non full test set. The optimizer is SGD with momentum of 0.9\nand weight decay of 0.0005. The temperature τ1 is obtained\nfrom the pre-trained CLIP, while the temperature τ2 in implicit\nknowledge distillation is set to 5, whose parameter analysis is\nshown in Table VIII. The length of prompt vectors is set to\n4, which is same length as the hand-crafted prompt “a photo\nof a”. We conduct all experiments on a single Nvidia V100\nGPU.\nC. Baselines\nFor the miniImagenet and tieredImagenet datasets, we com-\npare our method with 8 baselines, including PEMnE-BMS*\n[54], HCTransformers [55], CLIP LP+LN [56], P>M>F [57],\ncluster-FSL [58], PT+MAP [59], EPNet [60] and EASY\n[61] which are state-of-the-art methods on miniImagenet and\ntieredImagenet. For the other 11 datasets, we compare our\nmethod with 5 baselines, namely Zero-shot CLIP [8], CoOp\n[12] and CLIP-Adapter [13], ProGrad [14], which are state-\nof-the-art few-shot learning methods based on VLP models.\nFor fair comparison, we follow their way of dataset splitting\nand adopt classiﬁcation accuracy as the evaluation metric.\nD. Performance Comparison\nTable I shows the results of our SgVA-CLIP and other\nstate-of-the-art methods on miniImagenet and tieredImagenet.\nCompared to the other methods, our SgVA-CLIP achieves new\nstate-of-the-art performance on both miniImagenet and tiered-\nImageNet. Although the P>M>F [57] uses extra data, SgVA-\nCLIP still outperforms it by 2.65% and 0.32% respectively\non the 5-way 1-shot and 5-way 5-shot tasks of miniImagenet.\nCompared with CLIP LP+LN [56], which uses the same pre-\ntrained parameters of CLIP as ours, SgVA-CLIP exceeds it by\n5.87% and 0.78% respectively on the miniImagenet. Besides,\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 28, NOVEMBER 2022\n7\nTABLE I\nCOMPARISON WITH STATE-OF-THE-ART FEW-SHOT LEARNING METHODS (WITH BASE CLASSES) ON MINIIMAGENET AND TIEREDIMAGENET.\nminiImagenet (%)\ntieredImagenet (%)\nMethod\nVision\nBackbone\n5-way 1-shot\n5-way 5-shot\n5-way 1-shot\n5-way 5-shot\nEPNet+SSL\nWRN-28-10\n79.22 ± 0.92\n88.05 ± 0.51\n83.69 ± 0.99\n89.34 ± 0.59\nEASY\n3×ResNet12\n83.02 ± 0.23\n88.57 ± 0.12\n84.29 ± 0.24\n89.76 ± 0.14\nPT+MAP\nWRN/DenseNet121\n82.92 ± 0.26\n88.82 ± 0.13\n85.67 ± 0.26\n90.45 ± 0.14\ncluster-FSL\nWRN-28-10\n85.74 ± 0.76\n90.18 ± 0.43\n82.63 ± 0.79\n89.16 ± 0.35\nHCTransformers\nViT-S\n74.62 ± 0.20\n89.19 ± 0.13\n79.57 ± 0.20\n91.72 ± 0.11\nPEMnE-BMS*\nDenseNet121\n85.54\n91.53\n86.07 ± 0.25\n91.09 ± 0.14\nCLIP LP+LN\nViT-B/16\n92.08\n97.94\n-\n-\nP>M>F (with ext. data)\nViT-B/16\n95.30\n98.40\n-\n-\nSgVA-CLIP (ours)\nViT-B/16\n97.95 ± 0.19\n98.72 ± 0.13\n95.73 ± 0.37\n96.21 ± 0.37\nTABLE II\nTHE BENEFIT OF KNOWLEDGE DISTILLATION.\nMethod\nLKD\nVision\nBackbone\nImageNet (%)\n16-shot\nSUN397 (%)\n16-shot\nVision\nBackbone\nImageNet (%)\n16-shot\nSUN397 (%)\n16-shot\nVision-speciﬁc w/o KD\n×\n63.61\n69.00\n50.11\n61.70\nVision-speciﬁc w/ KD\n✓\n66.95 (+3.34)\n71.22 (+2.22)\n58.75 (+8.64)\n66.56 (+4.86)\nSgVA-CLIP w/o KD\n×\n72.94\n76.12\n64.42\n71.24\nSgVA-CLIP w/ KD\n✓\nViT-B/16\n73.30 (+0.36)\n76.42 (+0.30)\nResNet50\n65.70 (+1.28)\n71.99 (+0.75)\non the tieredImageNet, SgVA-CLIP surpasses the second best\nresult by 9.66% and 4.49% respectively. It is worth noting that\nthe improvement of our method is larger when the number of\nshots is fewer, which demonstrates its generality capability in\nfew-shot learning.\nFig. 4 shows the comparison results of our method and 5\nrecent baselines over 11 datasets with 1, 2, 4, 8, 16 shots.\nFor fair comparison, we adopt the vision backbone ResNet50\nfrom CLIP as in [12, 13, 20, 21]. Compared with CoOp\n[12], our method achieves considerable improvement. CoOp\nonly considers the cross-modal information extraction from a\nperspective of prompt learning, but neglects the vision-speciﬁc\ninformation that is more discriminative. By comprehensively\nexploiting the vision information and the cross-modal informa-\ntion, our method outperforms CoOp on 11 datasets. Typically,\nthe average performance gains over all datasets with 1, 2,\n4, 8, 16 shots are 6.45%, 5.07%, 4.19%, 3.87% and 3.32%\nrespectively. Besides, the average gains over all the shots has\nreached 7.9% and 7.09% respectively on the Food101 and\nFGVAircraft dataset.\nCompared with ProGrad [14], which proposes a prompt-\naligned gradient updating scheme, our SgVA-CLIP achieves\nsigniﬁcant performance and exceeds it by 2.21% on average.\nTypically, the mean performance gains over 11 datasets with 1,\n2, 4, 8, 16 shots are 2.15%, 2.24%, 1.93%, 2.21% and 2.53%\nrespectively. Although a novel gradient updating strategy is\nproposed from the perspective of overcoming the improperly\nbiased tuning, ProGrad gains limited improvement and cannot\ncapture the more discriminative visual information.\nCompared with CLIP-Adapter [13], which is a visual\nadapter based on CLIP, our SgVA-CLIP surpasses it on most\ndatasets. The average performance gains over CLIP-Adapter\nTABLE III\nTHE SIGNIFICANT TEST OF KNOWLEDGE DISTILLATION ON FIVE TRIALS.\nMethod\nLKD\nVision\nBckbone\nMean acc on\nImageNet(%)\n16-shot\nstd T value P value\nSgVA-CLIP w/o KD\n×\nViT-B/16\n72.94\n0.36\n3.70\n3.6e-4\nSgVA-CLIP w/ KD\n✓\n73.30\n0.11\nacross all shots on the 11 datasets is 1.97%. By training\nan extra visual bottleneck layer, CLIP-Adapter can enhance\nthe alignment of the visual features with the text features.\nHowever, it only considers the visual information that is\nrelated to text. Different from it, our SgVA-CLIP proposes\na semantic-guided adapting mechanism, which produces more\ndiscriminative visual features that can well complement the\ncross-modal features, showing a more promising perspective\nfor few-shot learning.\nThe overall experiment results demonstrate that the implicit\nknowledge distillation can promote the visual adapting and\nproduce more effective task-speciﬁc visual features for few-\nshot learning.\nE. Ablation Study\nAblation study of the implicit knowledge distillation.\nTo evaluate the effectiveness of the adopted knowledge dis-\ntillation, we conduct experiment on ImageNet and SUN397.\nThe results are shown in Table II. We can observe that with\nResNet50 as the vision backbone, KD improves the accuracy\nof vision-based model by 8.64% on ImageNet and 4.86% on\nSUN397. And KD enhances the SgVA-CLIP by 1.28% and\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 28, NOVEMBER 2022\n8\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of labeled training examples per class\n58\n60\n62\n64\n66\nScore (%)\nZero-shot\nCLIP\nImageNet\nSgVA-CLIP(ours)\nProGrad\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of labeled training examples per class\n58\n60\n62\n64\n66\n68\n70\n72\nScore (%)\nZero-shot\nCLIP\nSUN397\nSgVA-CLIP(ours)\nProGrad\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of labeled training examples per class\n68\n70\n72\n74\n76\n78\nScore (%)\nZero-shot\nCLIP\nFood101\nSgVA-CLIP(ours)\nProGrad\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of labeled training examples per class\n45\n50\n55\n60\n65\nScore (%)\nZero-shot\nCLIP\nDTD\nSgVA-CLIP(ours)\nProGrad\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of labeled training examples per class\n65\n70\n75\n80\n85\n90\n95\nScore (%)\nZero-shot\nCLIP\nFlowers102\nSgVA-CLIP(ours)\nProGrad\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of labeled training examples per class\n55\n60\n65\n70\n75\nScore (%)\nZero-shot\nCLIP\nStanfordCars\nSgVA-CLIP(ours)\nProGrad\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of labeled training examples per class\n10\n15\n20\n25\n30\n35\n40\nScore (%)\nZero-shot\nCLIP\nFGVCAircraft\nSgVA-CLIP(ours)\nProGrad\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of labeled training examples per class\n78\n80\n82\n84\n86\n88\n90\nScore (%)\nZero-shot\nCLIP\nOxfordPets\nSgVA-CLIP(ours)\nProGrad\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of labeled training examples per class\n40\n50\n60\n70\n80\nScore (%)\nZero-shot\nCLIP\nEuroSAT\nSgVA-CLIP(ours)\nProGrad\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of labeled training examples per class\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0\n77.5\nScore (%)\nZero-shot\nCLIP\nUCF101\nSgVA-CLIP(ours)\nProGrad\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of labeled training examples per class\n86\n87\n88\n89\n90\n91\n92\n93\nScore (%)\nZero-shot\nCLIP\nCaltech101\nSgVA-CLIP(ours)\nProGrad\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of labeled training examples per class\n57.5\n60.0\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0\nScore (%)\nZero-shot\nCLIP\nAverage over 11 datasets\nSgVA-CLIP(ours)\nProGrad\nCLIP-Adapter\nCoOp\nZero-shot CLIP\nFig. 4. Comparison with state-of-the-art few-shot learning methods (without base classes) on 11 datasets.\nTABLE IV\nABLATION STUDY OF DIRECT AND IMPLICIT KNOWLEDGE DISTILLATION.\nKnowledge\nDistillation\nVision\nBackbone\nImageNet (%)\n16-shot\nSUN397 (%)\n16-shot\nAverage (%)\nDirect\nViT-B/16\n68.98\n75.70\n72.34\nImplicit\n73.30\n76.42\n74.86\nDirect\nResNet50\n59.88\n66.38\n63.13\nImplicit\n65.64\n71.99\n68.82\n0.75% respectively. We also note that when ResNet50 is used\nas the vision backbone, the performance gain of KD is greater\nthan using the ViT-B/16 backbone.\nTo do the signiﬁcance test for knowledge distillation, we\ncompute the P-value by repeating the experiments 5 times with\ndifferent random seeds. With the backbone of ViT-B/16, the\nresults of ﬁve trials on ImageNet (16-shot) are shown in Table\nIII, where std refers to the standard deviation. The P value of\nT test was calculated as 3.6e −4 < 0.05, demonstrating that\nthere are signiﬁcant differences between SgVA-CLIP w/ KD\nand SgVA-CLIP w/o KD.\nImplicit knowledge distillation vs. direct distillation.\nThe direct distillation uses the Kullback-Leibler divergence\nto match the sample relations in the cross-modal space and\nthe sample relations in the vision space. As shown in Table\nIV, compared with direct distillation, the proposed implicit\nknowledge distillation has an average performance gain of\n2.52% and 5.69% respectively on ViT-B/16 and ResNet50\nTABLE V\nTHE COMPLEMENTARITY BETWEEN THE VISION SPACE AND THE\nCROSS-MODAL SPACE.\nMethod\nVision\nBackboneLcl i2t Lcl i2i LKD\nImageNetSUN397\n16-shot\n16-shot\nVision-speciﬁc Prediction ViT-B/16\n×\n✓\n×\n63.61\n69.00\nCross-modal Prediction ViT-B/16\n✓\n×\n×\n71.48\n73.50\nFused Prediction\nViT-B/16\n✓\n✓\n×\n72.94\n76.12\nbackbone. Because in the direct distillation, the distribution\ngap between the two spaces may have a negative impact on\nthe distillation.\nComplementarity between the vision space and the\ncross-modal space. We compare the classiﬁcation results on\nImageNet and SUN397 obtained using the visual features\nand/or the cross-modal features. The results are shown in Table\nV, which demonstrate that comprehensively considering the\nvisual feature and the cross-modal feature is better than simply\nusing the one of them.\nFor the 16-shot learning task, fusing the results predicted\nfrom the vision features and the cross-modal features will\nincrease the accuracy by 1.46% and 2.62% on ImageNet and\nSUN397 when compared with the cross-modal results, and the\naccuracy is increased by 9.33% and 7.12% respectively when\ncompared with the vision-based results.\nAblation study of the visual adapting layer and the\nlearnable prompt. With the visual adapter removed, the\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 28, NOVEMBER 2022\n9\nTABLE VI\nABLATION STUDY OF ADAPTER AND PROMPT.\nminiImagenet (%)\ntieredImagenet (%)\nVision\nBackbone\nVisual\nAdapter\nLearnable\nPrompt\n5w-1s\n5w-5s\n5w-1s\n5w-5s\n×\n×\n93.07\n97.45\n89.72\n93.73\n✓\n×\n95.71\n97.82\n92.30\n95.25\n×\n✓\n96.63\n97.80\n94.88\n95.21\nViT-B/16\n✓\n✓\n97.95\n98.72\n95.73\n96.21\nTABLE VII\nCOMPARISON OF DIFFERENT VISION BACKBONES.\nVision\nBackbone\nMethod\nImageNet (%)\nSUN397 (%)\nAverage (%)\n16-shot\n16-shot\nResNet50\nCoOp\n62.95\n69.26\n66.11\nSgVA-CLIP\n65.70\n71.99\n68.85\nResNet101\nCoOp\n66.60\n71.19\n68.90\nSgVA-CLIP\n68.51\n73.00\n70.76\nViT-B/32\nCoOp\n66.85\n72.38\n69.62\nSgVA-CLIP\n68.26\n74.04\n71.15\nViT-B/16\nCoOp\n71.92\n75.29\n73.61\nSgVA-CLIP\n73.30\n76.42\n74.86\ndiscriminative adapted visual features are replaced by the pre-\ntrained visual features. And when prompt is not learnable, we\nfollow CLIP [8] and use hand-crafted prompt, i.e. ’a photo\nof a’. Note that the learnable continuous prompt has the same\nlength as the hand-crafted prompt, i.e. 4. And the 5-way 1-shot\nand 5-way 5-shot tasks are abbreviated as 5w-1s and 5w-5s\nrespectively in Table VI.\nThe baseline is that of removing both the Visual Adapter\nLayer and the learnable prompt. As shown in Table VI, the\nVisual Adapter Layer improves the accuracy on the 5-way\n1-shot task of miniImagenet from 93.07% to 95.71%, and\nthe learnable prompt elevates the accuracy to 96.63%. With\nthe Visual Adapter Layer and the learnable prompt applied\ntogether, the accuracy rate reaches 97.95%, 4.88% higher than\nthe baseline.\nF. Results on Different Vision Backbones\nThe results in Fig. 4 are based on the backbone ResNet50\nfor fair comparison with other methods, but SgVA-CLIP is\nalso effective on other vision backbones. Considering that\nonly CoOp [12] did a comprehensive analysis experiment of\nViT-B/16, ViT-B/32, ResNet50 and ResNet101 backbones, we\nreport more comparison results with CoOp as shown in Table\nVII. SgVA-CLIP surpasses CoOp by 2.74%, 1.86%, 1.53%\nand 1.25% on average respectively on ViT-B/16, ViT-B/32,\nResNet50 and ResNet101 backbones.\nG. Parameter Analysis\nParameter analysis of the temperature τ2 in distillation.\nTo analyze the effect of the temperature τ2 in distillation, we\nconduct experiments on ImageNet and SUN397 with different\nsettings. It is worth noting that the tenperature τ1 used in\nthe cross-modal contrastive loss and vision-speciﬁc contrastive\nloss is outside the scope of parameter analysis because it is\na pre-trained parameter of CLIP. The temperature τ2 controls\nTABLE VIII\nEFFECT OF TEMPERATURE IN KNOWLEDGE DISTILLATION.\nDistillation\nTemperature τ2\nVision\nBackbone\nImageNet (%)\n16-shot\nSUN397 (%)\n16-shot\nAverage (%)\n5\nResNet50\n65.64\n71.99\n68.82\n10\n65.70\n71.86\n68.78\n15\n65.28\n71.58\n68.43\n20\n64.78\n71.49\n68.14\n25\n64.63\n71.37\n68.00\nTABLE IX\nEFFECT OF THE HIDDEN DIMENSION IN THE VISUAL ADAPTING LAYER.\nVision\nBackbone\nHidden\nDimension\nImageNet (%)\nSUN397 (%)\nAverage (%)\n16-shot\n16-shot\nResNet50\n512\n65.28\n71.19\n68.24\n1024\n65.16\n71.37\n68.27\n2048\n65.53\n71.86\n68.70\n4096\n65.64\n71.99\n68.82\n8192\n65.48\n71.96\n68.72\nthe smoothness of the soft labels. As shown in Table VIII, we\nobtain the best performance when τ2 is 5.\nParameter analysis of the hidden dimension in the visual\nadapting layer. Table IX shows the results of using different\nhidden dimensions of the visual adapter layer. We observe\nthat either too small or too large dimension will deteriorate\nthe performance and the best adapter dimension is 4096,\nwhich is able to preserve enough visual information without\nredundancy. Therefore, we set the dimension to 4096 in the\nexperiment.\nH. Visualization\nIn Fig. 5, we sample 10 classes and display the distribution\nof the adapted visual features obtained by SgVA-CLIP and\nthe pre-trained visual features obtained by CLIP. The visual-\nization results show that the adapted visual features are more\ndiscriminative than the pre-trained visual features, and thus it\nis important to consider the adapted visual features in few-shot\nclassiﬁcation.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n \n \n(a)\n \nThe pre-trained visual features\n \n \n \n(b)\n \nAdapted\n \nvisual\n \nfeatures\nFig. 5. Visualization of the distribution of the pre-trained visual features and\nadapted visual features.\nV. CONCLUSION\nWe present SgVA-CLIP, a new VLP-based few-shot clas-\nsiﬁcation approach, which can comprehensively consider uni-\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 28, NOVEMBER 2022\n10\nmodal vision correlation and cross-modal image-text corre-\nlation. SgVA-CLIP focuses on the contrastive learning in\ntwo spaces and knowledge distillation between them, so that\nﬁne-grained cross-modal knowledge sharing can promote the\nlearning of discriminative adapted unimodal vision represen-\ntations. With the CLIP model frozen and only a few external\nparameters updated, the representation ability of CLIP can be\nquickly migrated to downstream classiﬁcation tasks by a few\nlabeled data. According to the experimental results, SgVA-\nCLIP outperforms competitive baselines on 13 datasets under\ndifferent few-shot settings. In future work, we will combine\nSgVA-CLIP with other efﬁcient tuning methods and explore\nthe application of SgVA-CLIP in more downstream tasks.\nREFERENCES\n[1] T. M. Hospedales, A. Antoniou, P. Micaelli, and A. J.\nStorkey, “Meta-learning in neural networks: A survey,”\nIEEE transactions on pattern analysis and machine in-\ntelligence, 2021.\n[2] X. Zhong, C. Gu, M. Ye, W. Huang, and C.-W. Lin,\n“Graph complemented latent representation for few-shot\nimage classiﬁcation,” IEEE Transactions on Multimedia,\npp. 1–1, 2022.\n[3] Y. Li, Z. Liu, L. Yao, and X. Chang, “Attribute-modulated\ngenerative meta learning for zero-shot learning,” IEEE\nTransactions on Multimedia, pp. 1–1, 2021.\n[4] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton,\n“A simple framework for contrastive learning of visual\nrepresentations,” in International conference on machine\nlearning, pp. 1597–1607, PMLR, 2020.\n[5] K. He, X. Chen, S. Xie, Y. Li, P. Doll´ar, and R. Girshick,\n“Masked autoencoders are scalable vision learners,” in\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 16000–16009, 2022.\n[6] Z. Tao, X. Liu, Y. Xia, X. Wang, L. Yang, X. Huang,\nand T.-S. Chua, “Self-supervised learning for multimedia\nrecommendation,” IEEE Transactions on Multimedia,\npp. 1–10, 2022.\n[7] Y. Liu, J. Wu, L. Qu, T. Gan, J. Yin, and L. Nie, “Self-\nsupervised correlation learning for cross-modal retrieval,”\nIEEE Transactions on Multimedia, pp. 1–1, 2022.\n[8] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh,\nS. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark,\net al., “Learning transferable visual models from natural\nlanguage supervision,” in International Conference on\nMachine Learning, pp. 8748–8763, PMLR, 2021.\n[9] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham,\nQ. Le, Y.-H. Sung, Z. Li, and T. Duerig, “Scaling up\nvisual and vision-language representation learning with\nnoisy text supervision,” in International Conference on\nMachine Learning, pp. 4904–4916, PMLR, 2021.\n[10] L. Yuan, D. Chen, Y.-L. Chen, N. Codella, X. Dai, J. Gao,\nH. Hu, X. Huang, B. Li, C. Li, et al., “Florence: A new\nfoundation model for computer vision,” arXiv preprint\narXiv:2111.11432, 2021.\n[11] W. Xia, Q. Wang, Q. Gao, M. Yang, and X. Gao, “Self-\nconsistent contrastive attributed graph clustering with\npseudo-label prompt,” IEEE Transactions on Multimedia,\npp. 1–13, 2022.\n[12] K. Zhou, J. Yang, C. C. Loy, and Z. Liu, “Learning\nto prompt for vision-language models,” International\nJournal of Computer Vision, pp. 1–12, 2022.\n[13] P. Gao, S. Geng, R. Zhang, T. Ma, R. Fang, Y. Zhang,\nH. Li, and Y. Qiao, “Clip-adapter: Better vision-\nlanguage models with feature adapters,” arXiv preprint\narXiv:2110.04544, 2021.\n[14] B. Zhu, Y. Niu, Y. Han, Y. Wu, and H. Zhang, “Prompt-\naligned gradient for prompt tuning,” 2022.\n[15] A. Vedaldi, Y. Jia, E. Shelhamer, J. Donahue, S. Karayev,\nJ. Long, and T. Darrell, “Convolutional architecture for\nfast feature embedding,” Cornell University, 2014.\n[16] Z. Shen, Z. Liu, J. Qin, M. Savvides, and K.-T. Cheng,\n“Partial is better than all: Revisiting ﬁne-tuning strategy\nfor few-shot learning,” in Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence, vol. 35, pp. 9594–9602,\n2021.\n[17] V. Kumar, H. Glaude, C. de Lichy, and W. Camp-\nbell, “A closer look at feature space data augmenta-\ntion for few-shot intent classiﬁcation,” arXiv preprint\narXiv:1910.04176, 2019.\n[18] S. W. Yoon, J. Seo, and J. Moon, “Tapnet: Neural net-\nwork augmented with task-adaptive projection for few-\nshot learning,” in International Conference on Machine\nLearning, pp. 7115–7123, PMLR, 2019.\n[19] Q. Sun, Y. Liu, T.-S. Chua, and B. Schiele, “Meta-\ntransfer learning for few-shot learning,” in Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 403–412, 2019.\n[20] R. Zhang, R. Fang, P. Gao, W. Zhang, K. Li, J. Dai,\nY. Qiao, and H. Li, “Tip-adapter: Training-free clip-\nadapter for better vision-language modeling,” arXiv\npreprint arXiv:2111.03930, 2021.\n[21] R. Zhang, L. Qiu, W. Zhang, and Z. Zeng, “Vt-clip:\nEnhancing vision-language models with visual-guided\ntexts,” arXiv preprint arXiv:2112.02399, 2021.\n[22] J. Li, S. Savarese, and S. C. Hoi, “Masked unsupervised\nself-training for zero-shot image classiﬁcation,” arXiv\npreprint arXiv:2206.02967, 2022.\n[23] T. Huang, J. Chu, and F. Wei, “Unsupervised prompt\nlearning for vision-language models,” arXiv preprint\narXiv:2204.03649, 2022.\n[24] M. Wortsman, G. Ilharco, J. W. Kim, M. Li, S. Kornblith,\nR. Roelofs, R. G. Lopes, H. Hajishirzi, A. Farhadi,\nH. Namkoong, et al., “Robust ﬁne-tuning of zero-shot\nmodels,” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 7959–\n7971, 2022.\n[25] K. Zhou, J. Yang, C. C. Loy, and Z. Liu, “Conditional\nprompt learning for vision-language models,” in Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 16816–16825, 2022.\n[26] J. Snell, K. Swersky, and R. Zemel, “Prototypical net-\nworks for few-shot learning,” Advances in neural infor-\nmation processing systems, vol. 30, 2017.\n[27] Y. Chen, X. Wang, Z. Liu, H. Xu, and T. Darrell, “A new\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 28, NOVEMBER 2022\n11\nmeta-baseline for few-shot learning,” 2020.\n[28] C. Xing, N. Rostamzadeh, B. Oreshkin, and P. O.\nO Pinheiro, “Adaptive cross-modal few-shot learning,”\nAdvances in Neural Information Processing Systems,\nvol. 32, 2019.\n[29] F. Pahde, M. Puscas, T. Klein, and M. Nabi, “Multimodal\nprototypical networks for few-shot learning,” in Proceed-\nings of the IEEE/CVF Winter Conference on Applications\nof Computer Vision, pp. 2644–2653, 2021.\n[30] C. Buciluˇa, R. Caruana, and A. Niculescu-Mizil, “Model\ncompression,” in Proceedings of the 12th ACM SIGKDD\ninternational conference on Knowledge discovery and\ndata mining, pp. 535–541, 2006.\n[31] G. Hinton, O. Vinyals, J. Dean, et al., “Distilling\nthe knowledge in a neural network,” arXiv preprint\narXiv:1503.02531, vol. 2, no. 7, 2015.\n[32] X. Li, Q. Sun, L. Jiao, F. Liu, X. Liu, L. Li, P. Chen, and\nY. Zuo, “d3k: Dynastic data-free knowledge distillation,”\nIEEE Transactions on Multimedia, pp. 1–14, 2023.\n[33] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distilbert,\na distilled version of bert: smaller, faster, cheaper and\nlighter,” arXiv preprint arXiv:1910.01108, 2019.\n[34] X.\nJiao,\nY.\nYin,\nL.\nShang,\nX.\nJiang,\nX.\nChen,\nL. Li, F. Wang, and Q. Liu, “Tinybert: Distilling\nbert for natural language understanding,” arXiv preprint\narXiv:1909.10351, 2019.\n[35] Z. Fang, J. Wang, X. Hu, L. Wang, Y. Yang, and Z. Liu,\n“Compressing visual-linguistic model via knowledge dis-\ntillation,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 1428–1438, 2021.\n[36] F. M. Hafner, A. Bhuyian, J. F. Kooij, and E. Granger,\n“Cross-modal\ndistillation\nfor\nrgb-depth\nperson\nre-\nidentiﬁcation,” Computer Vision and Image Understand-\ning, vol. 216, p. 103352, 2022.\n[37] W. I. Cho, D. Kwak, J. W. Yoon, and N. S. Kim, “Speech\nto text adaptation: Towards an efﬁcient cross-modal dis-\ntillation,” arXiv preprint arXiv:2005.08213, 2020.\n[38] Q. Fan, W. Pei, Y.-W. Tai, and C.-K. Tang, “Self-\nsupport few-shot semantic segmentation,” in European\nConference on Computer Vision, pp. 701–719, Springer,\n2022.\n[39] Y. Zhang, H. Fei, D. Li, T. Yu, and P. Li, “Prompting\nthrough prototype: A prototype-based prompt learning\non pretrained vision-language models,” arXiv preprint\narXiv:2210.10841, 2022.\n[40] O. Vinyals, C. Blundell, T. Lillicrap, k. kavukcuoglu, and\nD. Wierstra, “Matching networks for one shot learning,”\nin Advances in Neural Information Processing Systems\n(D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and\nR. Garnett, eds.), vol. 29, Curran Associates, Inc., 2016.\n[41] M. Ren, E. Triantaﬁllou, S. Ravi, J. Snell, K. Swersky,\nJ. B. Tenenbaum, H. Larochelle, and R. S. Zemel,\n“Meta-learning for semi-supervised few-shot classiﬁca-\ntion,” arXiv preprint arXiv:1803.00676, 2018.\n[42] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bern-\nstein, et al., “Imagenet large scale visual recognition\nchallenge,” International journal of computer vision,\nvol. 115, no. 3, pp. 211–252, 2015.\n[43] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and\nL. Fei-Fei, “Imagenet: A large-scale hierarchical image\ndatabase,” in 2009 IEEE conference on computer vision\nand pattern recognition, pp. 248–255, Ieee, 2009.\n[44] J. Krause, M. Stark, J. Deng, and L. Fei-Fei, “3d ob-\nject representations for ﬁne-grained categorization,” in\nProceedings of the IEEE international conference on\ncomputer vision workshops, pp. 554–561, 2013.\n[45] K. Soomro, A. R. Zamir, and M. Shah, “Ucf101: A\ndataset of 101 human actions classes from videos in the\nwild,” arXiv preprint arXiv:1212.0402, 2012.\n[46] L. Fei-Fei, R. Fergus, and P. Perona, “Learning gen-\nerative visual models from few training examples: An\nincremental bayesian approach tested on 101 object\ncategories,” in 2004 conference on computer vision and\npattern recognition workshop, pp. 178–178, IEEE, 2004.\n[47] M.-E. Nilsback and A. Zisserman, “Automated ﬂower\nclassiﬁcation over a large number of classes,” in 2008\nSixth Indian Conference on Computer Vision, Graphics\n& Image Processing, pp. 722–729, IEEE, 2008.\n[48] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Tor-\nralba, “Sun database: Large-scale scene recognition from\nabbey to zoo,” in 2010 IEEE computer society conference\non computer vision and pattern recognition, pp. 3485–\n3492, IEEE, 2010.\n[49] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and\nA. Vedaldi, “Describing textures in the wild,” in Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, pp. 3606–3613, 2014.\n[50] P. Helber, B. Bischke, A. Dengel, and D. Borth, “Eurosat:\nA novel dataset and deep learning benchmark for land\nuse and land cover classiﬁcation,” IEEE Journal of Se-\nlected Topics in Applied Earth Observations and Remote\nSensing, vol. 12, no. 7, pp. 2217–2226, 2019.\n[51] S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and\nA. Vedaldi, “Fine-grained visual classiﬁcation of air-\ncraft,” arXiv preprint arXiv:1306.5151, 2013.\n[52] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. Jawahar,\n“Cats and dogs,” in 2012 IEEE conference on computer\nvision and pattern recognition, pp. 3498–3505, IEEE,\n2012.\n[53] L. Bossard, M. Guillaumin, and L. V. Gool, “Food-101–\nmining discriminative components with random forests,”\nin European conference on computer vision, pp. 446–\n461, Springer, 2014.\n[54] Y. Hu, S. Pateux, and V. Gripon, “Squeezing backbone\nfeature distributions to the max for efﬁcient few-shot\nlearning,” Algorithms, vol. 15, no. 5, p. 147, 2022.\n[55] Y. He, W. Liang, D. Zhao, H.-Y. Zhou, W. Ge, Y. Yu,\nand W. Zhang, “Attribute surrogates learning and spectral\ntokens pooling in transformers for few-shot learning,” in\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 9119–9129, 2022.\n[56] K. Kim, M. Laskin, I. Mordatch, and D. Pathak, “How to\nadapt your large-scale vision-and-language model,” 2021.\n[57] S. X. Hu, D. Li, J. St¨uhmer, M. Kim, and T. M.\nHospedales, “Pushing the limits of simple pipelines for\n\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 28, NOVEMBER 2022\n12\nfew-shot learning: External data and ﬁne-tuning make a\ndifference,” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 9068–\n9077, 2022.\n[58] J. Ling, L. Liao, M. Yang, and J. Shuai, “Semi-supervised\nfew-shot learning via multi-factor clustering,” in Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 14564–14573, 2022.\n[59] Y. Hu, V. Gripon, and S. Pateux, “Leveraging the fea-\nture distribution in transfer-based few-shot learning,” in\nInternational Conference on Artiﬁcial Neural Networks,\npp. 487–499, Springer, 2021.\n[60] P. Rodr´ıguez, I. Laradji, A. Drouin, and A. Lacoste,\n“Embedding propagation: Smoother manifold for few-\nshot classiﬁcation,” in European Conference on Com-\nputer Vision, pp. 121–138, Springer, 2020.\n[61] Y. Bendou, Y. Hu, R. Lafargue, G. Lioi, B. Pas-\ndeloup, S. Pateux, and V. Gripon, “Easy: Ensem-\nble augmented-shot y-shaped learning: State-of-the-art\nfew-shot classiﬁcation with simple ingredients,” arXiv\npreprint arXiv:2201.09699, 2022.\n"
}