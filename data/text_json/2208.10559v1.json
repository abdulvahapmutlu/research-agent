{
  "filename": "2208.10559v1.pdf",
  "num_pages": 13,
  "pages": [
    "Transductive Decoupled Variational Inference for Few-Shot Classiﬁcation\nAnuj Singh 1,\nHadi Jamali-Rad 1, 2\n1Delft University of Technology, The Netherlands,\n2Shell Global Solutions International B.V., Amsterdam, The Netherlands\nAbstract\nThe versatility to learn from a handful of samples is the hall-\nmark of human intelligence. Few-shot learning is an endeav-\nour to transcend this capability down to machines. Inspired\nby the promise and power of probabilistic deep learning, we\npropose a novel variational inference network for few-shot\nclassiﬁcation (coined as TRIDENT) to decouple the repre-\nsentation of an image into semantic and label latent vari-\nables, and simultaneously infer them in an intertwined fash-\nion. To induce task-awareness, as part of the inference me-\nchanics of TRIDENT, we exploit information across both\nquery and support images of a few-shot task using a novel\nbuilt-in attention-based transductive feature extraction mod-\nule (we call AttFEX). Our extensive experimental results\ncorroborate the efﬁcacy of TRIDENT and demonstrate that,\nusing the simplest of backbones, it sets a new state-of-the-art\nin the most commonly adopted datasets miniImageNet and\ntieredImageNet (offering up to 4% and 5% improvements,\nrespectively), as well as for the recent challenging cross-\ndomain miniImagenet →CUB scenario offering a signiﬁcant\nmargin (up to 20% improvement) beyond the best existing\ncross-domain baselines.1\nIntroduction\nDeep learning algorithms are usually data hungry and re-\nquire massive amounts of training data to reach a satisfac-\ntory level of performance on any task. To tackle this limita-\ntion, few-shot classiﬁcation aims to learn to classify images\nfrom various unseen tasks in a data-deﬁcient setting. In this\nexciting space, metric learning proposes to learn a shared\nfeature extractor to embed the samples into a metric space of\nclass prototypes (Sung et al. 2018; Vinyals et al. 2016; Snell,\nSwersky, and Zemel 2017; Wang et al. 2019; Liu, Song,\nand Qin 2020; Bateni et al. 2020). Due to limited data per\nclass, these prototypes suffer from sample-bias and fail to\nefﬁciently represent class characteristics. Furthermore, shar-\ning a feature extractor across tasks implies that the discrim-\ninative information learnt from the seen classes are equally\neffective on any arbitrary unseen classes, which is not true\nin most cases. Task-aware few-shot learning approaches\n(Bateni et al. 2022; Ye et al. 2020) address these limitations\nby exploiting information hidden in the unlabeled data. As a\n1Code and experimentation can be found in our GitHub reposi-\ntory: https://github.com/anujinho/trident\nresult, the model learns task-speciﬁc embeddings by align-\ning the features of the labelled and unlabelled task instances\nfor optimal distance metric based label assignment. Since\nthe alignment of these embeddings is still subject to the\nrelevance of the characteristics captured by the shared fea-\nture extractors, task-aware methods sometimes fail to extract\nmeaningful representations particularly relevant to classiﬁ-\ncation. Probabilistic methods address sample-bias by relax-\ning the need to ﬁnd point estimates to approximate data-\ndependent distributions of either high-dimensional model\nweights (Nguyen, Do, and Carneiro 2019; Ravi and Beat-\nson 2019; Gordon et al. 2019; Hu et al. 2020) or lower-\ndimensional class prototypes (Sun et al. 2021; Zhang et al.\n2019). However, inferring a high-dimensional posterior of\nmodel parameters is inefﬁcient in low-data regimes and esti-\nmating distributions of class prototypes involves using hand-\ncrafted non-parametric aggregation techniques which may\nnot be well suited for every unseen task.\nAlthough ﬁt for purpose, all these approaches seem to\noverlook an important perspective. An image is composed of\ndifferent attributes such as style, design, and context, which\nare not necessarily relevant discriminative characteristics for\nclassiﬁcation. Here, we refer to these attributes as semantic\ninformation. On the other hand, other class-characterizing\nattributes (such as wings of a bird, trunk of an elephant,\nhump on a camel’s back) are critical for classiﬁcation, ir-\nrespective of context. We refer to such attributes as label in-\nformation. Typically, contextual information is majorly gov-\nerned by semantic attributes, whereas the label character-\nistics are subtly embedded throughout an image. In other\nwords, semantic information can be predominantly present\nacross an image, whereas attending to subtle label infor-\nmation determines how effective a classiﬁcation algorithm\nwould be. Thus, we argue that attention to label-speciﬁc\ninformation should be ingrained into the mechanics of the\nclassiﬁer, decoupling it from semantic information. This be-\ncomes even more important in a few-shot setting where the\nnetwork has to quickly learn from little data. Building upon\nthis idea, we propose transductive variational inference of\ndecoupled latent variables (coined as TRIDENT), to simul-\ntaneously infer decoupled label and semantic information\nusing two intertwined variational networks. To induce task-\nawareness while constructing the variational inference me-\nchanics of TRIDENT, we introduce a novel atention-based\narXiv:2208.10559v1  [cs.CV]  22 Aug 2022\n",
    "Figure 1: High-level process ﬂow of TRIDENT. Inferred label latent variable zl contains class-characterizing information, as is reﬂected by\nbetter separation of the distributions when compared to their semantic latent counterparts zs. AttFEX module generates task-aware feature\nmaps by exploiting information from both support and query images, which compensates for the lack of label vectors Y in inferring zl.\ntransductive feature extraction module (we call AttFEX)\nwhich further enhances the discriminative power of the in-\nferred label attributes. This way TRIDENT infers distribu-\ntions instead of point estimates and injects a handcrafted\ninductive-bias into the network to guide the classiﬁcation\nprocess. Our main contributions can be summarized as:\n1. We propose TRIDENT, a variational inference network\nto simultaneously infer two salient decoupled attributes\nof an image (label and semantic), by inferring these two\nusing two intertwined variational sub-networks (Fig. 1).\n2. We introduce an attention-based transductive feature\nextraction module, AttFEX, to enable TRIDENT see\nthrough and compare all images within a task, inducing\ntask-cognizance in the inference of label information.\n3. We perform extensive evaluations to demonstrate that\nTRIDENT sets a new state-of-the-art by outperforming\nall existing baselines on the most commonly adopted\ndatasets miniImagenet and tieredImagenet (up to 4% and\n5%), as well as for the challenging cross-domain scenario\nof miniImagenet →CUB (up to 20% improvement).\nRelated Work\nMetric-based learning. This body of work revolves around\nmapping input samples into a lower-dimensional embedding\nspace and then classifying the unlabelled samples based on a\ndistance or similarity metric. By parameterizing these map-\npings with neural networks and using differentiable similar-\nity metrics for classiﬁcation, these networks can be trained\nin an episodic manner (Vinyals et al. 2016) to perform few-\nshot classiﬁcation. Prototypical Nets (Snell, Swersky, and\nZemel 2017), Simple Shot (Wang et al. 2019), Relation Net-\nworks (Sung et al. 2018), Matching Networks (Vinyals et al.\n2016) variants of Graph Neural Nets (Satorras and Estrach\n2018; Yang et al. 2020), Simple CNAPS (Bateni et al. 2020),\nare a few examples of seminal ideas in this space.\nTransductive Feature-Extraction and Inference. Trans-\nductive feature extraction or task-aware learning is a vari-\nant of the metric-learning with an adaptation mechanism\nthat aligns support and query feature vectors in the embed-\nding space for better representation of task-speciﬁc discrim-\ninative information. This not only improves the discrimi-\nnative ability of classiﬁers across tasks, but also alleviates\nthe problem of overﬁtting on limited support set since in-\nformation from the query set is also used for extracting fea-\ntures of images in a task. CNAPS (Requeima et al. 2019),\nTransductive-CNAPS (Bateni et al. 2022), FEAT (Ye et al.\n2020), Assoc-Align (Afrasiyabi, Lalonde, and Gagn´e 2020),\nTPMN (Wu et al. 2021) and CTM (Li et al. 2019) are prime\nexamples of such methods. Next to transduction for task-\naware feature extraction, there are methods that use trans-\nductive inference to classify all the query samples at once\nby jointly assigning them labels, as opposed to their in-\nductive counterparts where prediction is done on the sam-\nples one at a time. This is either done by iteratively prop-\nagating labels from the support to the query samples or by\nﬁne-tuning a pre-trained backbone using an additional en-\ntropy loss on all query samples, which encourages conﬁdent\nclass predictions at query samples. TPN (Liu et al. 2019),\nEnt-Min (Dhillon et al. 2020), TIM (Boudiaf et al. 2020),\nTransductive-CNAPS (Bateni et al. 2022), LaplacianShot\n(Ziko et al. 2020), DPGN (Yang et al. 2020) and ReRank\n(SHEN et al. 2021) are a few notable examples in this space\nthat usually report state-of-the-art results in certain few-shot\nclassiﬁcation settings (Liu et al. 2019).\nOptimization-based meta-learning. These methods search\nfor model parameters that are sensitive to task objective\nfunctions for fast gradient-based adaptation to new tasks.\nMAML (Finn, Abbeel, and Levine 2017), its variants (Ra-\njeswaran et al. 2019; Nichol, Achiam, and Schulman 2018a)\nand SNAIL (Mishra et al. 2018) are a few prominent exam-\nples while LEO (Rusu et al. 2019) efﬁciently meta-updates\nits parameters in a lower dimensional latent space.\nProbabilistic learning. The estimated parameters of typi-\ncal gradient-based meta-learning methods discussed earlier\n(Finn, Abbeel, and Levine 2017; Rusu et al. 2019; Mishra\net al. 2018; Nichol, Achiam, and Schulman 2018a; Ra-\njeswaran et al. 2019), have high variance due to the small\ntask sample size. To deal with this, a natural extension is\nto model the uncertainty by treating these parameters as\n",
    "latent variables in a Bayesian framework as proposed in\nNeural Statistician (Edwards and Storkey 2017), PLATI-\nPUS (Finn, Xu, and Levine 2018), VAMPIRE (Nguyen,\nDo, and Carneiro 2019), ABML (Ravi and Beatson 2019),\nVERSA (Gordon et al. 2019), SIB (Hu et al. 2020),\nSAMOVAR (Iakovleva, Verbeek, and Alahari 2020). Meth-\nods like ABPML (Sun et al. 2021) and VariationalFSL\n(Zhang et al. 2019) infer latent variables of class proto-\ntypes to perform classiﬁcation and avoid inferring high-\ndimensional model parameters. ABPML (Sun et al. 2021)\nand VariationalFSL (Zhang et al. 2019) are the closest to our\napproach. In contrast to these two methods, we avoid hand-\ncrafting class-level aggregations, as well as we enhance vari-\national inference by incorporating an inductive bias through\ndecoupling of label and semantic information.\nProblem Deﬁnition\nConsider a labelled dataset D = {(xi, yi) | i ∈[1, N ′]} of\nimages xi and class labels yi. This dataset D is divided into\nthree disjoint subsets: D = {Dtr ∪Dval ∪Dtest}, re-\nspectively, referring to the training, validation, and test sub-\nsets. The validation dataset Dval is used for model selection\nand the testing dataset Dtest for ﬁnal evaluation. Follow-\ning standard few-shot classiﬁcation settings (Vinyals et al.\n2016; Sung et al. 2018; Snell, Swersky, and Zemel 2017),\nwe use episodic training on a set of tasks Ti ∼p(T ). The\ntasks are constructed by drawing K random samples from\nN different classes, which we denote as an (N-way, K-\nshot) task. Concretely, each task Ti is composed of a support\nand a query set. The support set S = {(xS\nkn, yS\nkn) | k ∈\n[1, K], n ∈[1, N]} contains K samples per class and the\nquery set Q = {(xQ\nkn, yQ\nkn) | k ∈[1, Q], n ∈[1, N]} con-\ntains Q samples per class. For a given task, the NQ query\nand NK support images are mutually exclusive to assess the\ngeneralization performance.\nThe Proposed Method: TRIDENT\nLet us start with the high-level idea. The proposed approach\nis devised to learn meaningful representations that capture\ntwo pivotal characteristics of an image by modelling them\nas separate latent variables: (i) zs representing semantics,\nand (ii) zl embodying class labels. Inferring these two latent\nvariables simultaneously allows zl to learn meaningful dis-\ntributions of class-discriminating characteristics decoupled\nfrom semantic features represented by zs. We argue that\nlearning zl as the sole latent variable for classiﬁcation re-\nsults in capturing a mixture of true label and other semantic\ninformation. This in turn can lead to sub-optimal classiﬁca-\ntion performance, especially in a few-shot setting where the\ninformation per class is scarce and the network has to adapt\nand generalize quickly. By inferring decoupled label and se-\nmantics latent variables, we inject a handcrafted inductive-\nbias that incorporates only relevant characteristics, and thus,\nameliorates the network’s classiﬁcation performance.\nGenerative Process\nThe directed graphical model in Fig. 2 illustrates the com-\nmon underlying generative process p such that pi\n=\nFigure 2: Generative Model of TRIDENT. Dotted lines indicate\nvariational inference and solid lines refer to generative processes.\nThe inference and generative parameters are color coded to corre-\nspond to their respective architectures indicated in Fig.1 and Fig.4.\np(xi, yi | zli, zsi). For the sake of brevity, in the follow-\ning we drop the sample index i as we always refer to\nterms associated with a single data sample. We work on\nthe logical premise that the label latent variable zl is re-\nsponsible for generating class label as well as for image\nreconstruction, whereas the semantic latent variable zs is\nonly responsible for image reconstruction (solid lines in\nthe ﬁgure). Formally, the data is explained by the genera-\ntive processes: pθ1(y | zl) = Cat(y | zl) and pθ2(x | zl, zs) =\ngθ2(x; zl, zs), where Cat(.) refers to a multinomial distri-\nbution and gθ2(x; zl, zs) is a suitable likelihood function\nsuch as a Gaussian or Bernoulli distribution. The likelihoods\nof both these generative processes are parameterized us-\ning deep neural networks and the priors of the latent vari-\nables are chosen to be standard multivariate Gaussian dis-\ntributions (Kingma and Welling 2014; Kingma et al. 2014):\np(zs) = N(zs | 0, I) and p(zl) = N(zl | 0, I).\nVariational Inference of Decoupled Zl and Zs\nComputing exact posterior distributions is intractable due\nto high dimensionality and non-linearity of the deep neural\nnetwork parameter space. Following (Kingma and Welling\n2014; Kingma et al. 2014), we instead construct an approx-\nimate posterior over the latent variables by introducing a\nﬁxed-form distribution q(zl, zs | x, y) parameterized by φ.\nBy using qφ(.) as an inference network, the inference is ren-\ndered tractable, scalable and amortized since φ now acts as\nthe global variational parameter. We assume qφ has a fac-\ntorized form qφ (zs, zl | x, y) = qφ1 (zl | x, zs) qφ2 (zs | x),\nwhere qφ1(.), qφ2(.) are assumed to be multivariate Gaus-\nsian distributions. As is also depicted in Fig. 2, we use zs as\ninput to qφ1(.) to infer zl because of their conditional depen-\ndence given x. This way we forge a path to allow necessary\nsemantic latent information ﬂow through the label inference\nnetwork. On the other hand, the opposite direction (using zl\nto infer zs) is unnecessary, because label information does\nnot directly contribute to the extraction of semantic features.\nWe will further reﬂect on this design choice in the next sub-\nsection. Neural networks are then used to parameterize both\ninference networks as:\nqφ2 (zs | x) = N\n\u0000zs | µφ2(x), diag(σ2\nφ2(x))\n\u0001\n,\nqφ1 (zl | x, zs) = N\n\u0000zl | µφ1(x, zs), diag(σ2\nφ1(x, zs))\n\u0001\n.\n(1)\nTo ﬁnd the optimal approximate posterior, we derive the\nevidence lower bound (ELBO) on the marginal likelihood of\n",
    "the data to form our objective function:\np(x, y) =\nZZ\np(x, y | zs, zl) p(zs,zl) dzs dzl,\n= Eq(zs,zl | x)\n\u0014p(x | zl, zs)p(y | zl)p(zl)p(zs)\nq(zl, zs | x)\n\u0015\n.\nln p(x, y) ⩾Eq(zs,zl|x)\n\u0014\nln\n\u0012p(x | zl, zs)p(y | zl)p(zl)p(zs)\nq(zs, zl | x)\n\u0013\u0015\n,\n= Eqφ2\n\u0014\nEqφ1\n\u0014\nln\n\u0012p(x | zs, zl)p(y | zl)p(zs)p(zl)\nq(zs | x)q(zl | x, zs)\n\u0013\u0015\u0015\n.\nDenoting Ψ = (θ1, θ2, φ1, φ2), the ELBO can be given by\nL(Ψ) = −Eqφ2 Eqφ1 [ln pθ2(x | zs, zl) + ln pθ1(y | zl)] +\nDKL\n\u0000qφ1(zl | x, zs)∥p(zl)\n\u0001\n+ DKL\n\u0000qφ2(zs | x)∥p(zs)\n\u0001\n,\n(2)\nwhere the second line follows the graphical model in Fig 2,\nand E(.) and ln(.) denote the expectation operator and the\nnatural logarithm, respectively. We avoid computing biased\ngradients by following the re-parameterization trick from\n(Kingma and Welling 2014). Assuming Gaussian distribu-\ntions for the priors as well as the variational distributions\nallows us to compute the KL Divergences of zl and zs\n(last two terms in (2)) analytically (Kingma and Welling\n2014). By considering a multivariate Gaussian distribution\nand a multinomial distribution as the likelihood functions\nfor pθ2 (x | zs, zl) and pθ1 (y | zl), respectively, the negative\nlog-likelihood of x becomes the mean squared error (MSE)\nbetween the reconstructed images ˜x and the ground-truth im-\nages x while the negative log-likelihood of y becomes the\ncross-entropy between the actual labels y and the predicted\nlabels ˜y. After working (2) out, we arrive at our overall ob-\njective function L = LR + LC, where:\nLR = α1∥x −˜x∥2 −KL(µs, σs),\nLC = −α2\nN\nX\nn=1\nyn ln pθ1(˜y = n | zl) −KL(µl, σl),\n(3)\nwhere KL(µ, σ) = 1\n2\nPD\nd=1\n\u00001+2 ln(σd)−(µd)2−(σd)2\u0001\n,\nD denotes the dimension of the latent space, N is the to-\ntal number of classes in an (N-way, K-shot) task, α1, α2\nare constant scaling factors, µs and σ2\ns denote the mean and\nvariance vectors of semantic latent distribution, and µl and\nσ2\nl denote the mean and variance vectors of label latent dis-\ntribution. The hyper-parameters α1, α2 only scale the evi-\ndence lower-bound appropriately, since the reconstruction\nloss is in practice three orders of magnitude greater than the\ncross-entropy loss. As such, this scaling helps convergence\nbut impacts the tightness of the ELBO slightly; nonetheless,\n(2) and (3) are still considered variational inference by con-\nsensus among the literature (Higgins et al. 2017; Joy et al.\n2021; Mathieu et al. 2019; Dupont 2018). The loss is cal-\nculated for each given task on query and support sets sepa-\nrately; i.e., Lg = Lg\nR + Lg\nC with g ∈{Si, Qi}. Note that in\n(1) we deliberately choose to exclude the label information y\nas input to qφ1(.) to be able to exploit the associated genera-\ntive network pθ1(y | zl) as a classiﬁer. The consequence and\nthe proposed solution to accommodate this design choice are\ndiscussed in the next subsection.\nSupport Set Feature Maps\nQuery Set Feature Maps\nFigure 3: AttFEX module depicting colors as images and shades\nas feature maps. We illustrate only 3 image feature maps and 3\nchannels instead of 32 for N, for the sake of simplicity.\nAttFEX for Transductive Feature Extraction\nWe ﬁrst extract the feature maps of all images in the\ntask using a convolutional block F = ConvEnc(X) where\nX ∈RN(K+Q)×C×W ×H, F ∈RN(K+Q)×C′×W ′×H′.\nThe feature map tensor F is then transposed into F′ ∈\nRC′×N(K+Q)×W ′×H′ and fed into two consecutive 1 × 1\nconvolution blocks. This helps the network utilize informa-\ntion across corresponding pixels of all images in a task Ti\nwhich acts as a parametric comparison of classes. We lever-\nage the fact that ConvEnc already extracts local pixel infor-\nmation by using larger kernels, and thus, use parameter-light\n1 × 1 convolutions subsequently to focus only on individual\npixels. Let F′\ni denote the ith channel (or feature map layer)\nout of total of C′ available and ReLU denote the rectiﬁed lin-\near unit activation. The 1 × 1 convolution block (Conv1×1)\nis formulated as follows:\nMi = ReLU\n\u0000Conv1×1(F′\ni, WM)\n\u0001\n, ∀i ∈[1, C′];\nNj = ReLU\n\u0000Conv1×1(Mj, WN)\n\u0001\n, ∀j ∈[1, C′];\n(4)\nwhere N ∈RC′×32×W ′×H′ and WM ∈R64×N(K+Q)×1×1,\nWN ∈R32×64×1×1 denote the learnable weights. Next, we\nwant to blend information across feature maps for which we\nuse a self-attention mechanism (Vaswani et al. 2017) across\nNj, ∀j ∈[1, 32]. To do so, we feed N to query, key and\nvalue extraction networks fq(, ; WQ), fk(.; WK), fv(.; WV )\nwhich are also designed to be 1 × 1 convolutions as:\nQi = ReLU (Conv1×1(Ni, WQ)) ,\n∀i ∈[1, C′];\nKi = ReLU (Conv1×1(Ni, WK)) ,\n∀i ∈[1, C′];\nVi = ReLU (Conv1×1(Ni, WV )) ,\n∀i ∈[1, C′];\n(5)\nwhere WQ, WK, WV\n∈R1×32×1×1 are the learnable\nweights and Q, K, V ∈RC′×1×W ′×H′ are the query, key\nand value tensors. Next, each feature map Nj is mapped to\nits output tensor Gj by computing a weighted sum of the\nvalues, where each weight (within parentheses in (6)) mea-\nsures the compatibility (or similarity) between the query and\nits corresponding key tensor using an inner-product:\nGi =\nC′\nX\nj=1\n \nexp (Qi · Kj)\n√dk. PC′\nk=1 exp (Qi · Kk)\n!\nVi,\n(6)\n",
    "where dk = W ′ × H′, and Gi ∈R1×C′×W ′×H′, ∀i. Fi-\nnally, we transform the original feature maps F by apply-\ning a Hadamard product between the feature mask G and F,\nthus, rendering the required feature maps transductive:\n˜F\nS = G ◦FS\nor\n˜F\nQ = G ◦FQ.\nHere, FS and FQ represent the feature maps corresponding\nto the support and query images, respectively. As a result of\noperating on this channel-pixel distribution across images in\na task, FS and FQ are rendered task-aware. Note that the\nquery tensor Q must not be confused with the query set Q\nof a task.\nAlgorithmic Overview and Training Strategy\nFigure 4: TRIDENT is comprised of two intertwined variational\nnetworks. Zg\ns is concatenated with the output of AttFEX, and used\nfor inferring Zg\nl , where g ∈{S, Q}. Next, both Zg\nl and Zg\ns are used\nto reconstruct images ˜X\ng while Zg\nl is used to extract ˜Y g.\nOverview of TRIDENT. The complete architecture of\nTRIDENT is illustrated in Fig. 4. The ConvEnc feature ex-\ntractor and the linear layers µφ2(.), σ2\nφ2(.) constitute the in-\nference network qφ2 of the semantic latent variable (bottom\nrow of Fig. 4). The AttFEX module, another ConvEnc,\nand linear layers µφ1(.) and σ2\nφ1(.) make up the infer-\nence network qφ1 of the label latent variable (top row of\nFig. 4). The proposed approach, TRIDENT, is described in\nAlgorithm 1. Note that TRIDENT is trained in a MAML\n(Finn, Abbeel, and Levine 2017) fashion, where depend-\ning on the inner or outer loop, the support or query set\n(g ∈{S, Q}) will be the reference, respectively. First,\nthe lower ConvEnc block extracts feature maps Xg\nCE =\nConvEnc(Xg). Xg\nCE’s are then ﬂattened and passed onto\nµφ2(.), σ2\nφ2(.), which respectively output the mean and vari-\nance vectors of the semantic latent distribution, as discussed\nin (1). This is done either for the entire support or the query\nimages Xg, where g ∈{S, Q} for a given task Ti. We\nthen sample a set of vectors Zg\ns (subscript s for seman-\ntic) from their corresponding Gaussian distributions using\nthe re-parameterization trick (line 1, Algorithm 1). Upon\npassing X = XS ∪XQ through the upper ConvEnc, the\nAttFEX module of qφ1 comes into play to create task-\ncognizant feature maps ˜F\ng for either S or Q (line 2). Zg\ns\ntogether with ˜F\ng are passed onto the linear layers µφ1(.),\nσ2\nφ1(.) to generate the mean and variance vectors of the label\nAlgorithm 1: TRIDENT\nRequire: XS, XQ, Y g, Xg\nCE, where g ∈{S, Q}\n1 Sample: Zg\ns ∼qφ2\n\u0000Zs | µφ2(Xg\nCE), diag\n\u0000σ2\nφ2(Xg\nCE)\n\u0001\u0001\n2 Compute task-cognizant embeddings:\n[˜F\nS, ˜F\nQ] = AttFEX(ConvEnc(X)); X = XS ∪XQ\n3 Concatenate Zg\ns and ˜Fg into [ ˜Fg, Zg\ns] and sample:\nZg\nl ∼qφ1\n\u0000Zl | µφ1([˜F\ng, Zg\ns]), diag(σ2\nφ1([˜F\ng, Zg\ns]))\n\u0001\n4 Reconstruct Xg using ˜X\ng = pθ2(X | Zg\nl , Zg\ns)\n5 Extract class-conditional probabilities using:\np\n\u0000 ˜Y g | Zg\nl\n\u0001\n= softmax\n\u0000pθ1(Y g | Zg\nl )\n\u0001\n6 Compute Lg = Lg\nR + Lg\nC using (3)\nReturn: Lg\nAlgorithm 2: End to End Meta-Training of TRIDENT\nRequire: Dtr, α, β, B\n1 Randomly initialise Ψ = (φ1, φ2, θ1, θ2)\n2 while not converged do\n3\nSample B tasks Ti = Si ∪Qi from Dtr\n4\nfor each task Ti do\n5\nfor number of adaptation steps do\n6\nCompute LSi(Ψ) = TRIDENT(Ti −{Y Qi})\n7\nEvaluate ∇(Ψ)LSi(Ψ)\n8\nΨ ←Ψ −α∇ΨLSi(Ψ)\n9\nend\n10\n(Ψ′)i = Ψ\n11\nend\n12\nCompute\nLQi(Ψ′\ni) = TRIDENT(Ti −{Y Si}); ∀i ∈[1, B]\n13\nMeta-update on Qi: Ψ ←Ψ −β∇Ψ\nPB\ni=1 LQi(Ψ′\ni)\n14 end\nlatent Gaussian distributions (line 3). After sampling the set\nof vectors Zg\nl (subscript l for label) from their correspond-\ning distributions, we use Zg\nl and Zg\ns to reconstruct the input\nimages ˜X\ng using the generative network pθ2 (line 4). Next,\nZg\nl ’s are input to the classiﬁer network pθ1 to generate the\nclass logits, which are normalized using a softmax(.),\nresulting in class-conditional probabilities p( ˜Y g | Zg\nl ) (line\n5). Finally (in line 6), using the outputs of all the compo-\nnents discussed earlier, we calculate the loss Lg as formu-\nlated in (2) and (3).\nTraining strategy. An important as-\npect of the training procedure of TRIDENT is that its set\nof parameters Ψ = (θ1, θ2, φ1, φ2) are meta-learnt by back-\npropagating through the adaptation procedure on the support\nset, as proposed in MAML (Finn, Abbeel, and Levine 2017)\nand illustrated here in Algorithm 2. This increases the sen-\nsitivity of the parameters Ψ towards the loss function for\nfast adaptation to unseen tasks and reduces generalization\nerrors on the query set Q. First, we randomly initialize the\nparameters Ψ (line 1, Algorithm 2) to compute the objective\nfunction over the support set LSi(Ψ) using equation (3) in\nthe main manuscript, and perform a number of gradient de-\n",
    "Table 1: Accuracies in (% ± std). The predominant methodology of the baselines: Ind.: inductive inference, TF: transductive feature\nextraction methods, TI: transductive inference methods. Conv: convolutional blocks, RN: ResNet backbone, †: extra data. Style: best and\nsecond best. TRIDENT employs a transductive feature extraction module (TF), and the simplest of backbones (Conv4).\nminiImagenet\ntieredImagenet\nmini→CUB\nMethods\nBackbone\nApproach\n5-way 1-shot\n5-way 5-shot\n5-way 1-shot\n5-way 5-shot\n5-way 1-shot\n5-way 5-shot\nMAML (Finn, Abbeel, and Levine 2017)\nConv4\nInd.\n48.70 ± 1.84\n63.11 ± 0.92\n51.67 ± 1.81\n70.30 ± 0.08\n34.01 ± 1.25\n48.83 ± 0.62\nABML (Ravi and Beatson 2019)\nConv4\nInd.\n40.88 ± 0.25\n58.19 ± 0.17\n-\n-\n31.51 ± 0.32\n47.80 ± 0.51\nOVE(PL) (Patacchiola et al. 2020)\nConv4\nInd.\n48.00 ± 0.24\n67.14 ± 0.23\n-\n-\n37.49 ± 0.11\n57.23 ± 0.31\nDKT+Cos (Patacchiola et al. 2020)\nConv4\nInd.\n48.64 ± 0.45\n62.85 ± 0.37\n-\n-\n40.22 ± 0.54\n55.65 ± 0.05\nBOIL (Oh et al. 2021)\nConv4\nInd.\n49.61 ± 0.16\n48.58 ± 0.27\n66.45 ± 0.37\n69.37 ± 0.12\n-\n-\nLFWT(Tseng et al. 2020)\nRN10\nTF+TI\n66.32 ± 0.80\n81.98 ± 0.55\n-\n-\n47.47 ± 0.75\n66.98 ± 0.68\nFRN(Wertheimer, Tang, and Hariharan 2021)\nRN12\nInd.\n66.45 ± 0.19\n82.83 ± 0.13\n71.16 ± 0.22\n86.01 ± 0.15\n54.11 ± 0.19\n77.09 ± 0.15\nDPGN(Yang et al. 2020)\nRN12\nTF+TI\n67.77\n84.6\n72.45\n87.24\n-\n-\nPAL(Ma et al. 2021)\nRN12\nTF+TI\n69.37 ± 0.64\n84.40 ± 0.44\n72.25 ± 0.72\n86.95 ± 0.47\n-\n-\nProto-Completion(Zhang et al. 2021a)\nRN12\nTF+TI\n73.13 ± 0.85\n82.06 ± 0.54\n81.04 ± 0.89\n87.42 ± 0.57\n-\n-\nTPMN(Wu et al. 2021)\nRN12\nTF+TI\n67.64 ± 0.63\n83.44 ± 0.43\n72.24 ± 0.70\n86.55 ± 0.63\n-\n-\nLIF-EMD(Li, Wang, and Hu 2021)\nRN12\nTF+TI\n68.94 ± 0.28\n85.07 ± 0.50\n73.76 ± 0.32\n87.83 ± 0.59\n-\n-\nTransd-CNAPS(Bateni et al. 2022)\nRN18\nTF+TI\n55.6 ± 0.9\n73.1 ± 0.7\n65.9 ± 1.0\n81.8 ± 0.7\n-\n-\nBaseline++(Chen et al. 2019)\nRN18\nTF\n51.87 ± 0.77\n75.68 ± 0.63\n-\n-\n42.85 ± 0.69\n62.04 ± 0.76\nFEAT(Ye et al. 2020)\nRN18\nTF\n66.78\n82.05\n70.80\n84.79\n50.67 ± 0.78\n71.08 ± 0.73\nSimpleShot(Wang et al. 2019)\nWRN\nInd.\n63.32\n80.28\n69.98\n85.45\n48.56\n65.63\nAssoc-Align(Afrasiyabi, Lalonde, and Gagn´e 2020)\nWRN\nTF\n65.92 ± 0.60\n82.85 ± 0.55\n74.40 ± 0.68\n86.61 ± 0.59\n47.25 ± 0.76\n72.37 ± 0.89\nReRank(SHEN et al. 2021)\nWRN\nTF+TI\n72.4±0.6\n80.2±0.4\n79.5±0.6\n84.8±0.4\n-\n-\nTIM-GD(Boudiaf et al. 2020)\nWRN\nTI\n77.8\n87.4\n82.1\n89.8\n-\n71\nLaplacianShot(Ziko et al. 2020)\nWRN\nTI\n74.9\n84.07\n80.22\n87.49\n55.46\n66.33\nS2M2(Mangla et al. 2020)\nWRN\nTF\n64.93 ± 0.18\n83.18 ± 0.11\n73.71 ± 0.22\n88.59 ± 0.14\n48.24 ± 0.84\n70.44 ± 0.75\nMetaQDA(Zhang et al. 2021b)\nWRN\nTF\n67.83 ± 0.64\n84.28 ± 0.69\n74.33 ± 0.65\n89.56 ± 0.79\n53.75 ± 0.72\n71.84 ± 0.66\nPT+MAP(Hu, Gripon, and Pateux 2021)\nWRN\nTF+TI\n82.92 ± 0.26\n88.82 ± 0.13\n85.67 ± 0.26\n90.45 ± 0.14\n62.49 ± 0.32\n76.51 ± 0.18\nPEMnE-BMS(Hu, Pateux, and Gripon 2022)\nWRN\nTF+TI\n83.35 ± 0.25\n89.53 ± 0.13\n86.07 ± 0.25\n91.09 ± 0.14\n63.90 ± 0.31\n79.15 ± 0.18\nTransd-CNAPS+FETI(Bateni et al. 2022)\nRN18†\nTF+TI\n79.9 ± 0.8\n91.50 ± 0.4\n73.8 ± 0.1\n87.7 ± 0.6\n-\n-\nTRIDENT(Ours)\nConv4\nTF\n86.11 ± 0.59\n95.95 ± 0.28\n86.97 ± 0.50\n96.57 ± 0.17\n84.61 ± 0.33\n80.74 ± 0.35\nscent steps on the parameters Ψ to adapt them to the support\nset (lines 5 to 9). This is called the inner-update and is done\nseparately for all the support sets corresponding to their B\ndifferent tasks (line 3). Once the inner-update is computed\nfor each of the B parameter sets, the loss is evaluated on the\nquery set LQi(Ψ′\ni) (line 12), following which a meta-update\nis conducted over all the corresponding query sets, which\ninvolves computing a gradient through a gradient procedure\nas described in (Finn, Abbeel, and Levine 2017) (line 13).\nExperimental Evaluation\nThe goal of this section is to address the following four\nquestions: (i) How well does TRIDENT perform when com-\npared against the state-of-the-art methods for few-shot clas-\nsiﬁcation? (ii) How reliable is TRIDENT in terms of the\nconﬁdence and uncertainty metrics? (iii) How well does\nTRIDENT perform in a cross-domain setting where there is\na domain shift between the training and testing datasets? (iv)\nDoes TRIDENT actually decouple latent variables?\nBenchmark Datasets. We evaluate TRIDENT on the three\nmost commonly adopted datasets: miniImagenet (Ravi and\nLarochelle 2017), tieredImagenet (Ren et al. 2018) and CUB\n(Welinder et al. 2010). miniImagenet and tieredImagenet are\nsubsets of ImageNet (Deng et al. 2009) utilized for few-shot\nclassiﬁcation. Further details on these datasets can be found\nin the Appendix.\nImplementational Details. We use PyTorch (Paszke et al.\n2019) and learn2learn (Arnold et al. 2020) for all our im-\nplementations. We use a commonly adopted Conv4 ar-\nchitecture (Ravi and Larochelle 2017; Finn, Abbeel, and\nLevine 2017; Patacchiola et al. 2020; Afrasiyabi, Lalonde,\nand Gagn´e 2020; Wang et al. 2019; Boudiaf et al. 2020) as\nConvEnc to obtain the generic feature maps. Following the\nstandard setting in the literature (Finn, Abbeel, and Levine\n2017; Ravi and Larochelle 2017), the Conv4 has four con-\nvolutional blocks where each block has a 3 × 3 convolu-\ntion layer with 32 feature maps, followed by a batch nor-\nmalization (BN) (Ioffe and Szegedy 2015) layer, a 2 × 2\nmax-pooling layer and a LeakyReLU(0.2) activation.\nThe generative network pθ1 for zl is a classiﬁer with two\nlinear layers and a LeakyReLU(0.2) activation in be-\ntween, while pθ2 for zs consists of four blocks of a 2-\nD upsampling layer, followed by a 3 × 3 convolution and\nLeakyReLU(0.2) activation. Both latent variables zl and\nzs have a dimensionality of 64. Following (Nichol, Achiam,\nand Schulman 2018b; Liu et al. 2019; Vaswani et al. 2017),\nimages are resized to 84 × 84 for all conﬁgurations and we\ntrain and report test accuracy of (5-way, 1 and 5-shot) set-\ntings with 10 query images per class for all datasets. Hyper-\nparameter settings can be found in the Appendix.\nEvaluation Results\nWe report test accuracies indicating 95% conﬁdence inter-\nvals over 600 tasks for miniImagenet, and 2000 tasks for\nboth tieredImagenet and CUB, as is customary across the\nliterature (Chen et al. 2019; Dhillon et al. 2020; Bateni\net al. 2022). We compare our performance against a wide\nvariety of state-of-the-art few-shot classiﬁcation methods\nsuch as: (i) metric-learning (Wang et al. 2019; Bateni et al.\n2020; Afrasiyabi, Lalonde, and Gagn´e 2020; Yang et al.\n2020), (ii) transductive feature-extraction based (Oreshkin,\nRodr´ıguez L´opez, and Lacoste 2018; Ye et al. 2020; Li\net al. 2019; Xu et al. 2021), (iii) optimization-based (Finn,\nAbbeel, and Levine 2017; Mishra et al. 2018; Oh et al.\n2021; Lee et al. 2019; Rusu et al. 2019), (iv) transductive\ninference-based (Bateni et al. 2022; Boudiaf et al. 2020;\n",
    "Table 2: Parameter count of TRIDENT against competitors.\nConv4\nµφ\nσφ\nAttFEX\nTRIDENT\nConv4\nRN18\nWRN\nqφ1\n28896\n51264\n51264\n6994\nqφ2\n28896\n51264\n51264\n-\npθ1+ pθ2\n2245 + 132009\n412,238\n190, 410\n12.4M\n36.482M\nZiko et al. 2020; Liu et al. 2019), and (v) Bayesian (Iakovl-\neva, Verbeek, and Alahari 2020; Zhang et al. 2019; Hu et al.\n2020; Patacchiola et al. 2020; Ravi and Beatson 2019) ap-\nproaches. Previous works (Liu et al. 2019), (Hou et al. 2019)\nhave demonstrated the superiority of transductive inference\nmethods over their inductive counterparts. In this light, we\ncompare against a larger number of transductive (18 base-\nlines) rather than inductive (7 baselines) methods for a fair\ncomparison.\nIt is important to note that TRIDENT is only a transduc-\ntive feature-extraction based method as we utilize the query\nset images to extract task-aware feature embeddings; it is\nnot a transductive inference based method since we per-\nform inference of class-labels over the entire domain of def-\ninition and not just for the selected query samples (Vap-\nnik 2006; Gammerman, Vovk, and Vapnik 1998). The re-\nsults on miniImagenet and tieredImagenet for both (5-way,\n1 and 5-shot) settings are summarized in Table 1. We ac-\ncentuate on the fact that we also compare against Transd-\nCNAPS+FETI (Bateni et al. 2022), where the authors pre-\ntrain the ResNet-18 backbone on the entire train split\nof Imagenet. We, however, avoid training on additional\ndatasets, in favor of fair comparison with the rest of liter-\nature. Regardless of the choice of backbone (simplest in our\ncase), TRIDENT sets a new state-of-the-art on miniImagenet\nand tieredImagenet for both (5-way, 1 and 5-shot) settings,\noffering up to 5% gain over the prior art. Recently, a more\nchallenging cross-domain setting has been proposed for few-\nshot classiﬁcation to assess its generalization capabilities to\nunseen datasets. The commonly adopted setting is where one\ntrains on miniImagenet and tests on CUB (Chen et al. 2019).\nThe results of this experiment are also presented in Table 1.\nWe compare against any existing baselines for which this\ncross-domain experiment has been conducted. As can be\nseen, and to the best of our knowledge, TRIDENT again sets\na new state-of-the-art by a signiﬁcant margin of 20% for (5-\nway, 1-shot) setting, and 1.5% for (5-way, 5-shot) setting.\nComputational Complexity. Most of the reported base-\nlines in Table 1 use stronger backbones such as ResNet12,\nResNet18 and WRN which contain 11.5, 12.4 and 36.4 mil-\nlions of parameters respectively. On the other hand, we use\nthree Conv4s along with two fully connected layers and an\nAttFEX module which accounts for 410,958 and 412,238\nparameters in the (5-way, 1-shot) and (5-way, 5-shot) sce-\nnarios, respectively. This is summarized in details in Table 2.\nEven though we are more parameter heavy than approaches\nthat use a single Conv4 as feature extractor, TRIDENT’s\ntotal parameters still lies in the same order of magnitude as\nthese approaches. In summary, when it comes to complexity\nin parameter space, we are considerably more efﬁcient than\nthe vast majority of the cited competitors.\nReliability Metrics. A complementary set of metrics are\ntypically used in probabilistic settings to measure the uncer-\ntainty and reliability of predictions. More speciﬁcally, ex-\npected calibration error (ECE) and maximum calibration er-\nror (MCE) respectively measure the expected and maximum\nTable 3: Calibration errors of TRIDENT. Style: best and\nsecond best.\nMetrics\nMAML\nPLATIPUS\nABPML\nABML\nBMAML\nVAMPIRE\nTRIDENT\nECE\n0.046\n0.032\n0.013\n0.026\n0.025\n0.008\n0.0036\n5-way,\n1-shot\nMCE\n0.073\n0.108\n0.037\n0.058\n0.092\n0.038\n0.029\nECE\n0.032\n-\n0.006\n-\n0.027\n-\n0.0015\n5-way,\n5-shot\nMCE\n0.044\n-\n0.030\n-\n0.049\n-\n0.018\nbinned difference between conﬁdence and accuracy (Guo\net al. 2017). This is illustrated in Table 3 where TRIDENT\noffers superior calibration on miniImagenet (5-way, 1 and\n5-shot) as compared to other probabilistic approaches, and\nMAML (Finn, Abbeel, and Levine 2017).\nAblation Study\nWe analyze the classiﬁcation performance of TRIDENT\nacross various paramaters and hyper-parameters, as is sum-\nmarized in Table 4. We use miniImagenet (5-way, 1-shot)\nsetting to carry out ablation study experiments. To cover\ndifferent design perspectives, we carry out ablation on: (i)\nMAML-style training parameters: meta-batch size B and\nnumber of inner adaption steps n, (ii) latent space dimen-\nsionality: zl and zs to assess the impact of their size, (iii)\nAttFEX features: number of features extracted by WM,\nWN. Looking at the results, TRIDENT’s performance is\ndirectly proportional to the number of tasks and inner-\nadaptation steps, as is previously demonstrated in (Antreas\nAntoniou, Harrison Edwards, and Amos J. Storkey 2019;\nFinn, Abbeel, and Levine 2017) for MAML based training.\nRegarding latent space dimensions, a correlation between a\nhigher dimension of zl and zs and a better performance can\nbe observed. Even though, the results show that increasing\nboth dimensions beyond 64 leads to performance degrada-\ntion. As such, (64, 64) seems to be the sweet spot. Finally,\non feature space dimensions of AttFEX, the performance\nimproves when WM > WN, and the best performance is\nachieved when the parameters are set to (64, 32). Notably,\nthe exact set of parameters return the best performance for\n(5-way, 5-shot) setting.\nDecoupling Analysis\nAs a qualitative demonstration, we visualize the label and\nsemantic latent means (µl and µs) of query images for a\nrandomly selected (5-way, 5-shot) task from miniImagenet,\nbefore and after the MAML meta-update procedure. The\nUMAP (McInnes, Healy, and Melville 2018) plots in Fig. 5\nillustrate signiﬁcant improvement in class-conditional sep-\naration of query samples for label latent space upon meta-\nadaption, whereas negligible improvement is visible on the\nsemantic latent space. This is a qualitative evidence that ZL\ncaptures more class-discriminating information as compared\nto ZS. To substantiate this quantitatively, the clustering ca-\npacity of these latent spaces is also measured by the Davies-\nBouldin score (DBI) (Davies and Bouldin 1979), where, the\nlower the DBI score, the better both the inter-cluster sep-\naration and intra-cluster “tightness”. Fig. 5 shows that the\nDBI score drops signiﬁcantly more after meta-adaptation in\nthe case of ZL as compared to ZS, indicating better cluster-\ning of features in the former than the latter. This aligns with\nthe proposed decoupling strategy of TRIDENT and corrob-\norates the validity of our proposition to put an emphasis on\n",
    "Table 4: Ablation study for miniImagenet (5-way, 1-shot) tasks. Accuracies in (% ± std.).\n(B, n)\n(5, 3)\n(5, 5)\n(10, 3)\n(10, 5)\n(20, 3)\n(20, 5)\n-\n67.43 ± 0.75\n69.21 ± 0.66\n74.6 ± 0.84\n80.82 ± 0.68\n86.11 ± 0.59\n(dim(zl),\ndim(zs))\n(32, 32)\n(32, 64)\n(32, 128)\n(64, 32)\n(64, 64)\n(64, 128)\n(128, 32)\n(128, 64)\n(128, 128)\n76.29 ± 0.72\n75.44 ± 0.81\n79.1 ± 0.57\n82.93 ± 0.8\n86.11 ± 0.59\n85.62 ± 0.52\n81.49 ± 0.65\n82.89 ± 0.48\n84.42 ± 0.59\n(dim(WM),\ndim(WN))\n(32, 32)\n(32, 64)\n(32, 128)\n(64, 32)\n(64, 64)\n(64, 128)\n(128, 32)\n(128, 64)\n(128, 128)\n78.4 ± 0.23\n77.89 ± 0.39\n79.55 ± 0.87\n86.11 ± 0.59\n84.87 ± 0.45\n82.11 ± 0.35\n84.67 ± 0.7\n85.8 ± 0.58\n83.92 ± 0.63\nFigure 5: Better class separation upon meta-update is conﬁrmed\nby lower DBI scores. Different colors/markers indicate classes.\nlabel latent information for the downstream few-shot task.\nConcluding Remarks\nWe introduce a novel variational inference network (coined\nas TRIDENT) that simultaneously infers decoupled latent\nvariables representing semantic and label information of\nan image. The proposed network is comprised of two in-\ntertwined variational sub-networks responsible for inferring\nthe semantic and label information separately, the latter be-\ning enhanced using an attention-based transductive feature\nextraction module (AttFEX). Our extensive experimental\nresults corroborate the efﬁcacy of this transductive decou-\npling strategy on a variety of few-shot classiﬁcation settings\ndemonstrating superior performance and setting a new state-\nof-the-art for the most commonly adopted dataset mini and\ntieredImagenet as well as for the recent challenging cross-\ndomain scenario of miniImagenet →CUB. As future work,\nwe plan to demonstrate the applicability of TRIDENT in\nsemi-supervised and unsupervised settings by including the\nlikelihood of unlabelled samples derived from the graphi-\ncal model. This would render TRIDENT as an all-inclusive\nholistic approach towards solving few-shot classiﬁcation.\nReferences\nAfrasiyabi, A.; Lalonde, J.-F.; and Gagn´e, C. 2020. Asso-\nciative alignment for few-shot image classiﬁcation. In Eu-\nropean Conference on Computer Vision, 18–35. Springer.\nAntreas Antoniou; Harrison Edwards; and Amos J. Storkey.\n2019. How to train your MAML. In ICLR (Poster). Open-\nReview.net.\nArnold, S. M. R.; Mahajan, P.; Datta, D.; Bunner, I.; and\nZarkias, K. S. 2020.\nlearn2learn: A Library for Meta-\nLearning Research.\nBateni, P.; Barber, J.; van de Meent, J.-W.; and Wood, F.\n2022. Enhancing Few-Shot Image Classiﬁcation With Un-\nlabelled Examples. In Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision (WACV),\n2796–2805.\nBateni, P.; Goyal, R.; Masrani, V.; Wood, F.; and Sigal, L.\n2020. Improved Few-Shot Visual Classiﬁcation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR).\nBoudiaf, M.; Ziko, I.; Rony, J.; Dolz, J.; Piantanida, P.; and\nBen Ayed, I. 2020.\nInformation Maximization for Few-\nShot Learning. In Larochelle, H.; Ranzato, M.; Hadsell, R.;\nBalcan, M. F.; and Lin, H., eds., Advances in Neural Infor-\nmation Processing Systems, volume 33, 2445–2457. Curran\nAssociates, Inc.\nBRIER, G. W. 1950. VERIFICATION OF FORECASTS\nEXPRESSED IN TERMS OF PROBABILITY.\nMonthly\nWeather Review, 78(1): 1 – 3.\nChen, W.-Y.; Liu, Y.-C.; Kira, Z.; Wang, Y.-C.; and Huang,\nJ.-B. 2019. A Closer Look at Few-shot Classiﬁcation. In\nInternational Conference on Learning Representations.\nDavies, D. L.; and Bouldin, D. W. 1979. A Cluster Separa-\ntion Measure. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, PAMI-1(2): 224–227.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, 248–255. Ieee.\nDhillon, G. S.; Chaudhari, P.; Ravichandran, A.; and Soatto,\nS. 2020. A Baseline for Few-Shot Image Classiﬁcation. In\nInternational Conference on Learning Representations.\nDupont, E. 2018. Learning Disentangled Joint Continuous\nand Discrete Representations. In Bengio, S.; Wallach, H.;\nLarochelle, H.; Grauman, K.; Cesa-Bianchi, N.; and Gar-\nnett, R., eds., Advances in Neural Information Processing\nSystems, volume 31. Curran Associates, Inc.\nEdwards, H.; and Storkey, A. J. 2017. Towards a Neural\nStatistician. ArXiv, abs/1606.02185.\nFinn, C.; Abbeel, P.; and Levine, S. 2017. Model-Agnostic\nMeta-Learning for Fast Adaptation of Deep Networks. In\n",
    "Proceedings of the 34th International Conference on Ma-\nchine Learning, volume 70 of Proceedings of Machine\nLearning Research, 1126–1135.\nFinn, C.; Xu, K.; and Levine, S. 2018. Probabilistic Model-\nAgnostic Meta-Learning.\nIn Bengio, S.; Wallach, H.;\nLarochelle, H.; Grauman, K.; Cesa-Bianchi, N.; and Gar-\nnett, R., eds., Advances in Neural Information Processing\nSystems, volume 31. Curran Associates, Inc.\nGaly-Fajou, T.; Wenzel, F.; Donner, C.; and Opper, M.\n2020. Multi-Class Gaussian Process Classiﬁcation Made\nConjugate: Efﬁcient Inference via Data Augmentation. In\nAdams, R. P.; and Gogate, V., eds., Proceedings of The 35th\nUncertainty in Artiﬁcial Intelligence Conference, volume\n115 of Proceedings of Machine Learning Research, 755–\n765. PMLR.\nGammerman, A.; Vovk, V.; and Vapnik, V. 1998. Learning\nby transduction, vol UAI’98.\nGordon, J.; Bronskill, J.; Bauer, M.; Nowozin, S.; and\nTurner, R. 2019. Meta-Learning Probabilistic Inference for\nPrediction. In International Conference on Learning Rep-\nresentations.\nGuo, C.; Pleiss, G.; Sun, Y.; and Weinberger, K. Q. 2017.\nOn Calibration of Modern Neural Networks.\nIn Pro-\nceedings of the 34th International Conference on Machine\nLearning, volume 70 of Proceedings of Machine Learning\nResearch, 1321–1330. PMLR.\nHiggins, I.; Matthey, L.; Pal, A.; Burgess, C.; Glorot, X.;\nBotvinick, M.; Mohamed, S.; and Lerchner, A. 2017. beta-\nVAE: Learning Basic Visual Concepts with a Constrained\nVariational Framework.\nIn International Conference on\nLearning Representations.\nHou, R.; Chang, H.; MA, B.; Shan, S.; and Chen, X. 2019.\nCross Attention Network for Few-shot Classiﬁcation.\nIn\nWallach, H.; Larochelle, H.; Beygelzimer, A.; d'Alch´e-Buc,\nF.; Fox, E.; and Garnett, R., eds., Advances in Neural Infor-\nmation Processing Systems, volume 32. Curran Associates,\nInc.\nHu, S. X.; Moreno, P.; Xiao, Y.; Shen, X.; Obozinski,\nG.; Lawrence, N.; and Damianou, A. 2020.\nEmpirical\nBayes Transductive Meta-Learning with Synthetic Gradi-\nents. In International Conference on Learning Representa-\ntions (ICLR).\nHu, Y.; Gripon, V.; and Pateux, S. 2021. Leveraging the fea-\nture distribution in transfer-based few-shot learning. In In-\nternational Conference on Artiﬁcial Neural Networks, 487–\n499. Springer.\nHu, Y.; Pateux, S.; and Gripon, V. 2022. Squeezing back-\nbone feature distributions to the max for efﬁcient few-shot\nlearning. Algorithms, 15(5): 147.\nIakovleva, E.; Verbeek, J.; and Alahari, K. 2020.\nMeta-\nLearning with Shared Amortized Variational Inference. In\nProceedings of the 37th International Conference on Ma-\nchine Learning, Proceedings of Machine Learning Re-\nsearch. PMLR.\nIoffe, S.; and Szegedy, C. 2015. Batch Normalization: Ac-\ncelerating Deep Network Training by Reducing Internal\nCovariate Shift. In Proceedings of the 32nd International\nConference on Machine Learning, volume 37 of Proceed-\nings of Machine Learning Research, 448–456. PMLR.\nJoy, T.; Schmon, S.; Torr, P.; N, S.; and Rainforth, T. 2021.\nCapturing Label Characteristics in {VAE}s.\nIn Interna-\ntional Conference on Learning Representations.\nKingma, D. P.; and Ba, J. 2015.\nAdam: A Method for\nStochastic Optimization. In ICLR (Poster).\nKingma, D. P.; Mohamed, S.; Jimenez Rezende, D.; and\nWelling, M. 2014.\nSemi-supervised Learning with Deep\nGenerative Models.\nIn Advances in Neural Information\nProcessing Systems, volume 27.\nKingma, D. P.; and Welling, M. 2014. Auto-Encoding Vari-\national Bayes. arXiv:1312.6114.\nLee, K.; Maji, S.; Ravichandran, A.; and Soatto, S. 2019.\nMeta-Learning with Differentiable Convex Optimization.\nIn CVPR.\nLi, H.; Eigen, D.; Dodge, S.; Zeiler, M.; and Wang, X. 2019.\nFinding Task-Relevant Features for Few-Shot Learning by\nCategory Traversal. In CVPR.\nLi, J.; Wang, Z.; and Hu, X. 2021. Learning Intact Features\nby Erasing-Inpainting for Few-shot Classiﬁcation.\nPro-\nceedings of the AAAI Conference on Artiﬁcial Intelligence,\n35(9): 8401–8409.\nLiu, J.; Song, L.; and Qin, Y. 2020. Prototype Rectiﬁcation\nfor Few-Shot Learning. In Computer Vision - ECCV 2020\n- 16th European Conference, Glasgow, UK, August 23-28,\n2020, Proceedings, Part I.\nLiu, Y.; Lee, J.; Park, M.; Kim, S.; Yang, E.; Hwang, S.; and\nYang, Y. 2019. Learning to Propagate Labels: Transductive\nPropagation Network for Few-shot Learning. In Interna-\ntional Conference on Learning Representations.\nLong, M.; CAO, Z.; Wang, J.; and Jordan, M. I. 2018. Con-\nditional Adversarial Domain Adaptation. In Advances in\nNeural Information Processing Systems, volume 31. Curran\nAssociates, Inc.\nMa, J.; Xie, H.; Han, G.; Chang, S.-F.; Galstyan, A.;\nand Abd-Almageed, W. 2021.\nPartner-assisted learning\nfor few-shot image classiﬁcation.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\n10573–10582.\nMangla, P.; Kumari, N.; Sinha, A.; Singh, M.; Krishna-\nmurthy, B.; and Balasubramanian, V. N. 2020. Charting the\nright manifold: Manifold mixup for few-shot learning. In\nProceedings of the IEEE/CVF winter conference on appli-\ncations of computer vision, 2218–2227.\nMathieu, E.; Rainforth, T.; Siddharth, N.; and Teh, Y. W.\n2019.\nDisentangling Disentanglement in Variational Au-\ntoencoders. In Chaudhuri, K.; and Salakhutdinov, R., eds.,\nProceedings of the 36th International Conference on Ma-\nchine Learning, volume 97 of Proceedings of Machine\nLearning Research, 4402–4412. PMLR.\nMcInnes, L.; Healy, J.; and Melville, J. 2018. Umap: Uni-\nform manifold approximation and projection for dimension\nreduction. arXiv preprint arXiv:1802.03426.\nMishra, N.; Rohaninejad, M.; Chen, X.; and Abbeel, P.\n2018. A Simple Neural Attentive Meta-Learner. In Inter-\nnational Conference on Learning Representations.\nNguyen, C. C.; Do, T.; and Carneiro, G. 2019. Uncertainty\nin Model-Agnostic Meta-Learning using Variational Infer-\nence. CoRR.\nNichol, A.; Achiam, J.; and Schulman, J. 2018a. On First-\nOrder Meta-Learning Algorithms. arXiv:1803.02999.\nNichol, A.; Achiam, J.; and Schulman, J. 2018b. On First-\nOrder Meta-Learning Algorithms. CoRR, abs/1803.02999.\n",
    "Oh, J.; Yoo, H.; Kim, C.; and Yun, S.-Y. 2021. BOIL: To-\nwards Representation Change for Few-shot Learning. In\nInternational Conference on Learning Representations.\nOreshkin, B.; Rodr´ıguez L´opez, P.; and Lacoste, A. 2018.\nTADAM: Task dependent adaptive metric for improved\nfew-shot learning. In Advances in Neural Information Pro-\ncessing Systems, volume 31.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga,\nL.; Desmaison, A.; Kopf, A.; Yang, E.; DeVito, Z.; Raison,\nM.; Tejani, A.; Chilamkurthy, S.; Steiner, B.; Fang, L.; Bai,\nJ.; and Chintala, S. 2019. PyTorch: An Imperative Style,\nHigh-Performance Deep Learning Library. In Advances in\nNeural Information Processing Systems 32.\nPatacchiola, M.; Turner, J.; Crowley, E. J.; O’Boyle, M.\nF. P.; and Storkey, A. J. 2020. Bayesian Meta-Learning for\nthe Few-Shot Setting via Deep Kernels. In NeurIPS.\nRajeswaran, A.; Finn, C.; Kakade, S. M.; and Levine, S.\n2019. Meta-Learning with Implicit Gradients. In Wallach,\nH.; Larochelle, H.; Beygelzimer, A.; d'Alch´e-Buc, F.; Fox,\nE.; and Garnett, R., eds., Advances in Neural Information\nProcessing Systems, volume 32. Curran Associates, Inc.\nRavi, S.; and Beatson, A. 2019. Amortized Bayesian Meta-\nLearning. In International Conference on Learning Repre-\nsentations.\nRavi, S.; and Larochelle, H. 2017. Optimization as a Model\nfor Few-Shot Learning. In ICLR.\nRen, M.; Ravi, S.; Triantaﬁllou, E.; Snell, J.; Swersky, K.;\nTenenbaum, J. B.; Larochelle, H.; and Zemel, R. S. 2018.\nMeta-Learning for Semi-Supervised Few-Shot Classiﬁca-\ntion. In International Conference on Learning Represen-\ntations.\nRequeima, J.; Gordon, J.; Bronskill, J.; Nowozin, S.; and\nTurner, R. E. 2019. Fast and Flexible Multi-Task Classi-\nﬁcation using Conditional Neural Adaptive Processes. In\nAdvances in Neural Information Processing Systems, vol-\nume 32.\nRusu, A. A.; Rao, D.; Sygnowski, J.; Vinyals, O.; Pascanu,\nR.; Osindero, S.; and Hadsell, R. 2019. Meta-Learning with\nLatent Embedding Optimization. In International Confer-\nence on Learning Representations.\nSatorras, V. G.; and Estrach, J. B. 2018. Few-Shot Learning\nwith Graph Neural Networks. In International Conference\non Learning Representations.\nSHEN, X.; Xiao, Y.; Hu, S. X.; Sbai, O.; and Aubry, M.\n2021. Re-ranking for image retrieval and transductive few-\nshot classiﬁcation. In Beygelzimer, A.; Dauphin, Y.; Liang,\nP.; and Vaughan, J. W., eds., Advances in Neural Informa-\ntion Processing Systems.\nSnell, J.; Swersky, K.; and Zemel, R. 2017. Prototypical\nNetworks for Few-shot Learning. In Advances in Neural\nInformation Processing Systems, volume 30.\nSnell, J.; and Zemel, R. 2021. Bayesian Few-Shot Classiﬁ-\ncation with One-vs-Each P´olya-Gamma Augmented Gaus-\nsian Processes. In International Conference on Learning\nRepresentations.\nSun, Z.; Wu, J.; Li, X.; Yang, W.; and Xue, J.-H. 2021.\nAmortized Bayesian Prototype Meta-learning: A New\nProbabilistic Meta-learning Approach to Few-shot Image\nClassiﬁcation.\nIn Proceedings of The 24th International\nConference on Artiﬁcial Intelligence and Statistics, Pro-\nceedings of Machine Learning Research.\nSung, F.; Yang, Y.; Zhang, L.; Xiang, T.; Torr, P. H.; and\nHospedales, T. M. 2018. Learning to Compare: Relation\nNetwork for Few-Shot Learning.\nIn Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR).\nTseng, H.-Y.; Lee, H.-Y.; Huang, J.-B.; and Yang, M.-\nH. 2020.\nCross-domain few-shot classiﬁcation via\nlearned feature-wise transformation.\narXiv preprint\narXiv:2001.08735.\nVapnik, V. N. 2006. Estimation of Dependences Based on\nEmpirical Data. Estimation of Dependences Based on Em-\npirical Data.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017.\nAttention is All you Need. In Advances in Neural Informa-\ntion Processing Systems, volume 30.\nVinyals, O.; Blundell, C.; Lillicrap, T.; kavukcuoglu, k.; and\nWierstra, D. 2016. Matching Networks for One Shot Learn-\ning. In Advances in Neural Information Processing Systems,\nvolume 29.\nWang, Y.; Chao, W.-L.; Weinberger, K. Q.; and van der\nMaaten, L. 2019. SimpleShot: Revisiting Nearest-Neighbor\nClassiﬁcation for Few-Shot Learning. arXiv:1911.04623.\nWelinder, P.; Branson, S.; Mita, T.; Wah, C.; Schroff, F.; Be-\nlongie, S.; and Perona, P. 2010. Caltech-UCSD Birds 200.\nTechnical Report CNS-TR-2010-001, California Institute of\nTechnology.\nWertheimer, D.; Tang, L.; and Hariharan, B. 2021. Few-\nShot Classiﬁcation With Feature Map Reconstruction Net-\nworks.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 8012–\n8021.\nWu, J.; Zhang, T.; Zhang, Y.; and Wu, F. 2021. Task-Aware\nPart Mining Network for Few-Shot Learning. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, 8433–8442.\nXu, W.; yifan xu; Wang, H.; and Tu, Z. 2021. Attentional\nConstellation Nets for Few-Shot Learning. In International\nConference on Learning Representations.\nYang, L.; Li, L.; Zhang, Z.; Zhou, X.; Zhou, E.; and Liu,\nY. 2020. DPGN: Distribution Propagation Graph Network\nfor Few-Shot Learning.\n2020 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 13387–\n13396.\nYe, H.-J.; Hu, H.; Zhan, D.-C.; and Sha, F. 2020.\nFew-\nShot Learning via Embedding Adaptation with Set-to-Set\nFunctions. In IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 8808–8817.\nYoon, J.; Kim, T.; Dia, O.; Kim, S.; Bengio, Y.; and Ahn,\nS. 2018.\nBayesian Model-Agnostic Meta-Learning.\nIn\nAdvances in Neural Information Processing Systems, vol-\nume 31.\nZhang, B.; Li, X.; Ye, Y.; Huang, Z.; and Zhang, L. 2021a.\nPrototype Completion With Primitive Knowledge for Few-\nShot Learning. In CVPR, 3754–3762.\nZhang, J.; Zhao, C.; Ni, B.; Xu, M.; and Yang, X. 2019.\nVariational Few-Shot Learning. In 2019 IEEE/CVF Inter-\nnational Conference on Computer Vision (ICCV), 1685–\n1694.\n",
    "Zhang, X.; Meng, D.; Gouk, H.; and Hospedales, T. M.\n2021b. Shallow bayesian meta learning for real-world few-\nshot recognition. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, 651–660.\nZiko, I. M.; Dolz, J.; Granger, E.; and Ayed, I. B. 2020.\nLaplacian Regularized Few-Shot Learning.\nIn ICML,\n11660–11670.\n",
    "Appendix\nImpact of AttFEX\nIn order to study the impact of the transductive feature ex-\ntractor AttFEX, we exclude it during training and train the\nremaining architecture. Training proceeds exactly as men-\ntioned before in the manuscript. As can be seen in Ta-\nTable 5: Impact of AttFEX on classiﬁcation accuracies.\nminiImagenet\ntieredImagenet\n(5-way, 1-shot)\n(5-way, 5-shot)\n(5-way, 1-shot)\n(5-way, 5-shot)\nAttFEX OFF\n67.68 ± 0.55\n78.53 ± 0.21\n69.32 ± 0.76\n79.32 ± 0.76\nAttFEX ON\n86.11 ± 0.59\n95.95 ± 0.28\n86.97 ± 0.50\n96.57 ± 0.17\nble 5, the exclusion of AttFEX from TRIDENT results in\na substantial drop in classiﬁcation performance across both\ndatasets and task settings. Empirically, this further substan-\ntiates the importance of AttFEX’s ability to render the fea-\nture maps transductive/task-aware. As explained earlier in\nthe main manuscript, it is imperative to include y in the input\nto qφ1(.) for mathematical correctness of the variational in-\nference formulation. However, in order to utilize TRIDENT\nas a classiﬁcation and not a label reconstruction network, we\nchoose not to input y to qφ1(.), but rather do so indirectly by\ninducing a semblance of label characteristics in the features\nextracted from the images in a task. Thus, it is important to\nrealize that this ability of AttFEX to render feature maps\ntransductive is not just an adhoc performance enhancer, but\nrather an essential part of TRIDENT since it allows us to not\nviolate our generative and inference mechanics.\nAdditional Details of Datasets\nminiImagenet (Vinyals et al. 2016) is a subset of ImageNet\n(Deng et al. 2009) for few-shot classiﬁcation. It contains 100\nclasses with 600 samples each. We follow the predominantly\nadopted settings of (Ravi and Larochelle 2017; Chen et al.\n2019) where we split the entire dataset into 64 classes for\ntraining, 16 for validation and 20 for testing. tieredImagenet\nis a larger subset of ImageNet (Deng et al. 2009) with 608\nclasses and 779, 165 total images, which are grouped into\n34 higher-level nodes in the ImageNet human-curated hier-\narchy. This set of nodes is partitioned into 20, 6, and 8 dis-\njoint sets of training, validation, and testing nodes, and the\ncorresponding classes form the respective meta-sets. CUB\n(Welinder et al. 2010) dataset has a total of 200 classes, split\ninto training, validation and test sets following (Chen et al.\n2019). We use this dataset to simulate the effect of a domain\nshift where the model is ﬁrst trained on a (5-way, 1 or 5-\nshot) conﬁguration of miniImagenet and then tested on the\ntest classes of CUB, as used in (Chen et al. 2019; Boudiaf\net al. 2020; Ziko et al. 2020; Long et al. 2018).\nImplementational Details\nLet α1 and α2 respectively denote the scaling factors of the\nMSE and cross-entropy terms in our objective functions LR\nand LC, as already deﬁned in Subsection 4.2. The terms\nα and β respectively denote the learning rates of the in-\nner and meta updates whereas B and n respectively denote\nTable 6: Hyperparameter values when training TRIDENT.\nminiImagenet\ntieredImagenet\nH.P.\n5-way, 1-shot\n5-way, 5-shot\n5-way, 1-shot\n5-way, 5-shot\nα1\n1e-2\n1e-2\n1e-2\n1e-2\nα2\n100\n100\n150\n150\nα\n1e-3\n1e-3\n1.5e-3\n1.7e-3\nβ\n1e-4\n1e-4\n1.5e-4\n1.7e-4\nB\n20\n20\n20\n20\nn\n5\n5\n5\n5\nthe number of sampled tasks and adaptation steps of the in-\nner-update of our end-to-end training process, as described\nin Algorithm 2. The hyperparameter values (H.P.) used\nfor training TRIDENT on miniImagenet and tieredImagenet\nare shown in Table 6. We apply the same hyperparame-\nters for the cross-domain testing scenario of miniImagenet\n→CUB used for training TRIDENT on miniImagenet, for\nthe given (N-way, K-shot) conﬁguration. Hyperparameters\nare kept ﬁxed throughout training, validation and testing for\na given conﬁguration. Adam (Kingma and Ba 2015) opti-\nmizer is used for inner and meta-updates. Finally, the query,\nkey and value extraction networks fq(, ; WQ), fk(.; WK),\nfv(.; WV ) of the AttFEX module only use Conv1×1(.)\nand not the LeakyReLU(0.2) activation function for\n(5-way, 1-shot) tasks, irrespective of the dataset. We ob-\nserved that utilizing BatchNorm (Ioffe and Szegedy 2015)\nin the decoder of zs (pθ2) to train TRIDENT on (5-way, 5-\nshot) tasks of miniImagenet and on (5-way, 1-shot) tasks of\ntieredImagenet leads to better scores and improved stabil-\nity during training. We used the ReLU activation function\ninstead of LeakyReLU(0.2) to carry out training on (5-\nway, 1-shot) tasks of tieredImagenet. Meta-learning objec-\ntives can lead to unstable optimization processes in prac-\ntice, especially when coupled with stochastic sampling in\nlatent spaces, as also previously observed in (Antreas Anto-\nniou, Harrison Edwards, and Amos J. Storkey 2019; Rusu\net al. 2019). For ease of experimentation we clip the meta-\ngradient norm at an absolute value of 1. TRIDENT con-\nverges in 82, 000 and 22, 500 epochs for (5-way, 1-shot)\nand (5-way, 5-shot) tasks of miniImagenet, respectively and\ntakes 67, 500 and 48, 000 epochs for convergence on (5-way,\n1-shot) and (5-way, 5-shot) tasks of tieredImagenet, respec-\ntively. This translates to an average training time of 110\nhours on an 11GB NVIDIA 1080Ti GPU. Note that we did\nnot employ any data augmentation, feature averaging or any\nother data apart from the corresponding training subset Dtr,\nduring training and evaluation.\nAdditional Calibration Results\nTo further examine the reliability and calibration of our\nmethod, we assess the ECE, MCE (Guo et al. 2017) and\nBrier scores (BRIER 1950) of TRIDENT on the challenging\ncross-domain scenario of miniImagenet →CUB for (5-way,\n5-shot) tasks. This table can be treated as an extension to\nTable 3 since here, we compare the calibration metrics for\nan additional scenario. When compared against other base-\nlines that report these metrics on the aforementioned sce-\n",
    "Table 7: Style: best and second best.\nMethods\nECE\nMCE\nBrier\nFeature Transfer(Chen et al. 2019)\n0.275\n0.646\n0.772\nBaseline(Chen et al. 2019)\n0.315\n0.537\n0.716\nMatching Nets(Vinyals et al. 2016)\n0.030\n0.079\n0.630\nProto Nets(Snell, Swersky, and Zemel 2017)\n0.009\n0.025\n0.604\nRelation Net(Sung et al. 2018)\n0.234\n0.554\n0.730\nDKT+Cos(Patacchiola et al. 2020)\n0.236\n0.426\n0.670\nBMAML(Yoon et al. 2018)\n0.048\n0.077\n0.619\nBMAML+Chaser(Yoon et al. 2018)\n0.066\n0.260\n0.639\nLogSoftGP(ML)(Galy-Fajou et al. 2020)\n0.220\n0.513\n0.709\nLogSoftGP(PL)(Galy-Fajou et al. 2020)\n0.022\n0.042\n0.564\nOVE(ML)(Snell and Zemel 2021)\n0.049\n0.066\n0.576\nOVE(PL)(Snell and Zemel 2021)\n0.020\n0.032\n0.556\nTRIDENT(Ours)\n0.009\n0.02\n0.276\nnario, TRIDENT proves to be the most calibrated with the\nbest reliability scores. This is shown in Table 7.\n"
  ],
  "full_text": "Transductive Decoupled Variational Inference for Few-Shot Classiﬁcation\nAnuj Singh 1,\nHadi Jamali-Rad 1, 2\n1Delft University of Technology, The Netherlands,\n2Shell Global Solutions International B.V., Amsterdam, The Netherlands\nAbstract\nThe versatility to learn from a handful of samples is the hall-\nmark of human intelligence. Few-shot learning is an endeav-\nour to transcend this capability down to machines. Inspired\nby the promise and power of probabilistic deep learning, we\npropose a novel variational inference network for few-shot\nclassiﬁcation (coined as TRIDENT) to decouple the repre-\nsentation of an image into semantic and label latent vari-\nables, and simultaneously infer them in an intertwined fash-\nion. To induce task-awareness, as part of the inference me-\nchanics of TRIDENT, we exploit information across both\nquery and support images of a few-shot task using a novel\nbuilt-in attention-based transductive feature extraction mod-\nule (we call AttFEX). Our extensive experimental results\ncorroborate the efﬁcacy of TRIDENT and demonstrate that,\nusing the simplest of backbones, it sets a new state-of-the-art\nin the most commonly adopted datasets miniImageNet and\ntieredImageNet (offering up to 4% and 5% improvements,\nrespectively), as well as for the recent challenging cross-\ndomain miniImagenet →CUB scenario offering a signiﬁcant\nmargin (up to 20% improvement) beyond the best existing\ncross-domain baselines.1\nIntroduction\nDeep learning algorithms are usually data hungry and re-\nquire massive amounts of training data to reach a satisfac-\ntory level of performance on any task. To tackle this limita-\ntion, few-shot classiﬁcation aims to learn to classify images\nfrom various unseen tasks in a data-deﬁcient setting. In this\nexciting space, metric learning proposes to learn a shared\nfeature extractor to embed the samples into a metric space of\nclass prototypes (Sung et al. 2018; Vinyals et al. 2016; Snell,\nSwersky, and Zemel 2017; Wang et al. 2019; Liu, Song,\nand Qin 2020; Bateni et al. 2020). Due to limited data per\nclass, these prototypes suffer from sample-bias and fail to\nefﬁciently represent class characteristics. Furthermore, shar-\ning a feature extractor across tasks implies that the discrim-\ninative information learnt from the seen classes are equally\neffective on any arbitrary unseen classes, which is not true\nin most cases. Task-aware few-shot learning approaches\n(Bateni et al. 2022; Ye et al. 2020) address these limitations\nby exploiting information hidden in the unlabeled data. As a\n1Code and experimentation can be found in our GitHub reposi-\ntory: https://github.com/anujinho/trident\nresult, the model learns task-speciﬁc embeddings by align-\ning the features of the labelled and unlabelled task instances\nfor optimal distance metric based label assignment. Since\nthe alignment of these embeddings is still subject to the\nrelevance of the characteristics captured by the shared fea-\nture extractors, task-aware methods sometimes fail to extract\nmeaningful representations particularly relevant to classiﬁ-\ncation. Probabilistic methods address sample-bias by relax-\ning the need to ﬁnd point estimates to approximate data-\ndependent distributions of either high-dimensional model\nweights (Nguyen, Do, and Carneiro 2019; Ravi and Beat-\nson 2019; Gordon et al. 2019; Hu et al. 2020) or lower-\ndimensional class prototypes (Sun et al. 2021; Zhang et al.\n2019). However, inferring a high-dimensional posterior of\nmodel parameters is inefﬁcient in low-data regimes and esti-\nmating distributions of class prototypes involves using hand-\ncrafted non-parametric aggregation techniques which may\nnot be well suited for every unseen task.\nAlthough ﬁt for purpose, all these approaches seem to\noverlook an important perspective. An image is composed of\ndifferent attributes such as style, design, and context, which\nare not necessarily relevant discriminative characteristics for\nclassiﬁcation. Here, we refer to these attributes as semantic\ninformation. On the other hand, other class-characterizing\nattributes (such as wings of a bird, trunk of an elephant,\nhump on a camel’s back) are critical for classiﬁcation, ir-\nrespective of context. We refer to such attributes as label in-\nformation. Typically, contextual information is majorly gov-\nerned by semantic attributes, whereas the label character-\nistics are subtly embedded throughout an image. In other\nwords, semantic information can be predominantly present\nacross an image, whereas attending to subtle label infor-\nmation determines how effective a classiﬁcation algorithm\nwould be. Thus, we argue that attention to label-speciﬁc\ninformation should be ingrained into the mechanics of the\nclassiﬁer, decoupling it from semantic information. This be-\ncomes even more important in a few-shot setting where the\nnetwork has to quickly learn from little data. Building upon\nthis idea, we propose transductive variational inference of\ndecoupled latent variables (coined as TRIDENT), to simul-\ntaneously infer decoupled label and semantic information\nusing two intertwined variational networks. To induce task-\nawareness while constructing the variational inference me-\nchanics of TRIDENT, we introduce a novel atention-based\narXiv:2208.10559v1  [cs.CV]  22 Aug 2022\n\n\nFigure 1: High-level process ﬂow of TRIDENT. Inferred label latent variable zl contains class-characterizing information, as is reﬂected by\nbetter separation of the distributions when compared to their semantic latent counterparts zs. AttFEX module generates task-aware feature\nmaps by exploiting information from both support and query images, which compensates for the lack of label vectors Y in inferring zl.\ntransductive feature extraction module (we call AttFEX)\nwhich further enhances the discriminative power of the in-\nferred label attributes. This way TRIDENT infers distribu-\ntions instead of point estimates and injects a handcrafted\ninductive-bias into the network to guide the classiﬁcation\nprocess. Our main contributions can be summarized as:\n1. We propose TRIDENT, a variational inference network\nto simultaneously infer two salient decoupled attributes\nof an image (label and semantic), by inferring these two\nusing two intertwined variational sub-networks (Fig. 1).\n2. We introduce an attention-based transductive feature\nextraction module, AttFEX, to enable TRIDENT see\nthrough and compare all images within a task, inducing\ntask-cognizance in the inference of label information.\n3. We perform extensive evaluations to demonstrate that\nTRIDENT sets a new state-of-the-art by outperforming\nall existing baselines on the most commonly adopted\ndatasets miniImagenet and tieredImagenet (up to 4% and\n5%), as well as for the challenging cross-domain scenario\nof miniImagenet →CUB (up to 20% improvement).\nRelated Work\nMetric-based learning. This body of work revolves around\nmapping input samples into a lower-dimensional embedding\nspace and then classifying the unlabelled samples based on a\ndistance or similarity metric. By parameterizing these map-\npings with neural networks and using differentiable similar-\nity metrics for classiﬁcation, these networks can be trained\nin an episodic manner (Vinyals et al. 2016) to perform few-\nshot classiﬁcation. Prototypical Nets (Snell, Swersky, and\nZemel 2017), Simple Shot (Wang et al. 2019), Relation Net-\nworks (Sung et al. 2018), Matching Networks (Vinyals et al.\n2016) variants of Graph Neural Nets (Satorras and Estrach\n2018; Yang et al. 2020), Simple CNAPS (Bateni et al. 2020),\nare a few examples of seminal ideas in this space.\nTransductive Feature-Extraction and Inference. Trans-\nductive feature extraction or task-aware learning is a vari-\nant of the metric-learning with an adaptation mechanism\nthat aligns support and query feature vectors in the embed-\nding space for better representation of task-speciﬁc discrim-\ninative information. This not only improves the discrimi-\nnative ability of classiﬁers across tasks, but also alleviates\nthe problem of overﬁtting on limited support set since in-\nformation from the query set is also used for extracting fea-\ntures of images in a task. CNAPS (Requeima et al. 2019),\nTransductive-CNAPS (Bateni et al. 2022), FEAT (Ye et al.\n2020), Assoc-Align (Afrasiyabi, Lalonde, and Gagn´e 2020),\nTPMN (Wu et al. 2021) and CTM (Li et al. 2019) are prime\nexamples of such methods. Next to transduction for task-\naware feature extraction, there are methods that use trans-\nductive inference to classify all the query samples at once\nby jointly assigning them labels, as opposed to their in-\nductive counterparts where prediction is done on the sam-\nples one at a time. This is either done by iteratively prop-\nagating labels from the support to the query samples or by\nﬁne-tuning a pre-trained backbone using an additional en-\ntropy loss on all query samples, which encourages conﬁdent\nclass predictions at query samples. TPN (Liu et al. 2019),\nEnt-Min (Dhillon et al. 2020), TIM (Boudiaf et al. 2020),\nTransductive-CNAPS (Bateni et al. 2022), LaplacianShot\n(Ziko et al. 2020), DPGN (Yang et al. 2020) and ReRank\n(SHEN et al. 2021) are a few notable examples in this space\nthat usually report state-of-the-art results in certain few-shot\nclassiﬁcation settings (Liu et al. 2019).\nOptimization-based meta-learning. These methods search\nfor model parameters that are sensitive to task objective\nfunctions for fast gradient-based adaptation to new tasks.\nMAML (Finn, Abbeel, and Levine 2017), its variants (Ra-\njeswaran et al. 2019; Nichol, Achiam, and Schulman 2018a)\nand SNAIL (Mishra et al. 2018) are a few prominent exam-\nples while LEO (Rusu et al. 2019) efﬁciently meta-updates\nits parameters in a lower dimensional latent space.\nProbabilistic learning. The estimated parameters of typi-\ncal gradient-based meta-learning methods discussed earlier\n(Finn, Abbeel, and Levine 2017; Rusu et al. 2019; Mishra\net al. 2018; Nichol, Achiam, and Schulman 2018a; Ra-\njeswaran et al. 2019), have high variance due to the small\ntask sample size. To deal with this, a natural extension is\nto model the uncertainty by treating these parameters as\n\n\nlatent variables in a Bayesian framework as proposed in\nNeural Statistician (Edwards and Storkey 2017), PLATI-\nPUS (Finn, Xu, and Levine 2018), VAMPIRE (Nguyen,\nDo, and Carneiro 2019), ABML (Ravi and Beatson 2019),\nVERSA (Gordon et al. 2019), SIB (Hu et al. 2020),\nSAMOVAR (Iakovleva, Verbeek, and Alahari 2020). Meth-\nods like ABPML (Sun et al. 2021) and VariationalFSL\n(Zhang et al. 2019) infer latent variables of class proto-\ntypes to perform classiﬁcation and avoid inferring high-\ndimensional model parameters. ABPML (Sun et al. 2021)\nand VariationalFSL (Zhang et al. 2019) are the closest to our\napproach. In contrast to these two methods, we avoid hand-\ncrafting class-level aggregations, as well as we enhance vari-\national inference by incorporating an inductive bias through\ndecoupling of label and semantic information.\nProblem Deﬁnition\nConsider a labelled dataset D = {(xi, yi) | i ∈[1, N ′]} of\nimages xi and class labels yi. This dataset D is divided into\nthree disjoint subsets: D = {Dtr ∪Dval ∪Dtest}, re-\nspectively, referring to the training, validation, and test sub-\nsets. The validation dataset Dval is used for model selection\nand the testing dataset Dtest for ﬁnal evaluation. Follow-\ning standard few-shot classiﬁcation settings (Vinyals et al.\n2016; Sung et al. 2018; Snell, Swersky, and Zemel 2017),\nwe use episodic training on a set of tasks Ti ∼p(T ). The\ntasks are constructed by drawing K random samples from\nN different classes, which we denote as an (N-way, K-\nshot) task. Concretely, each task Ti is composed of a support\nand a query set. The support set S = {(xS\nkn, yS\nkn) | k ∈\n[1, K], n ∈[1, N]} contains K samples per class and the\nquery set Q = {(xQ\nkn, yQ\nkn) | k ∈[1, Q], n ∈[1, N]} con-\ntains Q samples per class. For a given task, the NQ query\nand NK support images are mutually exclusive to assess the\ngeneralization performance.\nThe Proposed Method: TRIDENT\nLet us start with the high-level idea. The proposed approach\nis devised to learn meaningful representations that capture\ntwo pivotal characteristics of an image by modelling them\nas separate latent variables: (i) zs representing semantics,\nand (ii) zl embodying class labels. Inferring these two latent\nvariables simultaneously allows zl to learn meaningful dis-\ntributions of class-discriminating characteristics decoupled\nfrom semantic features represented by zs. We argue that\nlearning zl as the sole latent variable for classiﬁcation re-\nsults in capturing a mixture of true label and other semantic\ninformation. This in turn can lead to sub-optimal classiﬁca-\ntion performance, especially in a few-shot setting where the\ninformation per class is scarce and the network has to adapt\nand generalize quickly. By inferring decoupled label and se-\nmantics latent variables, we inject a handcrafted inductive-\nbias that incorporates only relevant characteristics, and thus,\nameliorates the network’s classiﬁcation performance.\nGenerative Process\nThe directed graphical model in Fig. 2 illustrates the com-\nmon underlying generative process p such that pi\n=\nFigure 2: Generative Model of TRIDENT. Dotted lines indicate\nvariational inference and solid lines refer to generative processes.\nThe inference and generative parameters are color coded to corre-\nspond to their respective architectures indicated in Fig.1 and Fig.4.\np(xi, yi | zli, zsi). For the sake of brevity, in the follow-\ning we drop the sample index i as we always refer to\nterms associated with a single data sample. We work on\nthe logical premise that the label latent variable zl is re-\nsponsible for generating class label as well as for image\nreconstruction, whereas the semantic latent variable zs is\nonly responsible for image reconstruction (solid lines in\nthe ﬁgure). Formally, the data is explained by the genera-\ntive processes: pθ1(y | zl) = Cat(y | zl) and pθ2(x | zl, zs) =\ngθ2(x; zl, zs), where Cat(.) refers to a multinomial distri-\nbution and gθ2(x; zl, zs) is a suitable likelihood function\nsuch as a Gaussian or Bernoulli distribution. The likelihoods\nof both these generative processes are parameterized us-\ning deep neural networks and the priors of the latent vari-\nables are chosen to be standard multivariate Gaussian dis-\ntributions (Kingma and Welling 2014; Kingma et al. 2014):\np(zs) = N(zs | 0, I) and p(zl) = N(zl | 0, I).\nVariational Inference of Decoupled Zl and Zs\nComputing exact posterior distributions is intractable due\nto high dimensionality and non-linearity of the deep neural\nnetwork parameter space. Following (Kingma and Welling\n2014; Kingma et al. 2014), we instead construct an approx-\nimate posterior over the latent variables by introducing a\nﬁxed-form distribution q(zl, zs | x, y) parameterized by φ.\nBy using qφ(.) as an inference network, the inference is ren-\ndered tractable, scalable and amortized since φ now acts as\nthe global variational parameter. We assume qφ has a fac-\ntorized form qφ (zs, zl | x, y) = qφ1 (zl | x, zs) qφ2 (zs | x),\nwhere qφ1(.), qφ2(.) are assumed to be multivariate Gaus-\nsian distributions. As is also depicted in Fig. 2, we use zs as\ninput to qφ1(.) to infer zl because of their conditional depen-\ndence given x. This way we forge a path to allow necessary\nsemantic latent information ﬂow through the label inference\nnetwork. On the other hand, the opposite direction (using zl\nto infer zs) is unnecessary, because label information does\nnot directly contribute to the extraction of semantic features.\nWe will further reﬂect on this design choice in the next sub-\nsection. Neural networks are then used to parameterize both\ninference networks as:\nqφ2 (zs | x) = N\n\u0000zs | µφ2(x), diag(σ2\nφ2(x))\n\u0001\n,\nqφ1 (zl | x, zs) = N\n\u0000zl | µφ1(x, zs), diag(σ2\nφ1(x, zs))\n\u0001\n.\n(1)\nTo ﬁnd the optimal approximate posterior, we derive the\nevidence lower bound (ELBO) on the marginal likelihood of\n\n\nthe data to form our objective function:\np(x, y) =\nZZ\np(x, y | zs, zl) p(zs,zl) dzs dzl,\n= Eq(zs,zl | x)\n\u0014p(x | zl, zs)p(y | zl)p(zl)p(zs)\nq(zl, zs | x)\n\u0015\n.\nln p(x, y) ⩾Eq(zs,zl|x)\n\u0014\nln\n\u0012p(x | zl, zs)p(y | zl)p(zl)p(zs)\nq(zs, zl | x)\n\u0013\u0015\n,\n= Eqφ2\n\u0014\nEqφ1\n\u0014\nln\n\u0012p(x | zs, zl)p(y | zl)p(zs)p(zl)\nq(zs | x)q(zl | x, zs)\n\u0013\u0015\u0015\n.\nDenoting Ψ = (θ1, θ2, φ1, φ2), the ELBO can be given by\nL(Ψ) = −Eqφ2 Eqφ1 [ln pθ2(x | zs, zl) + ln pθ1(y | zl)] +\nDKL\n\u0000qφ1(zl | x, zs)∥p(zl)\n\u0001\n+ DKL\n\u0000qφ2(zs | x)∥p(zs)\n\u0001\n,\n(2)\nwhere the second line follows the graphical model in Fig 2,\nand E(.) and ln(.) denote the expectation operator and the\nnatural logarithm, respectively. We avoid computing biased\ngradients by following the re-parameterization trick from\n(Kingma and Welling 2014). Assuming Gaussian distribu-\ntions for the priors as well as the variational distributions\nallows us to compute the KL Divergences of zl and zs\n(last two terms in (2)) analytically (Kingma and Welling\n2014). By considering a multivariate Gaussian distribution\nand a multinomial distribution as the likelihood functions\nfor pθ2 (x | zs, zl) and pθ1 (y | zl), respectively, the negative\nlog-likelihood of x becomes the mean squared error (MSE)\nbetween the reconstructed images ˜x and the ground-truth im-\nages x while the negative log-likelihood of y becomes the\ncross-entropy between the actual labels y and the predicted\nlabels ˜y. After working (2) out, we arrive at our overall ob-\njective function L = LR + LC, where:\nLR = α1∥x −˜x∥2 −KL(µs, σs),\nLC = −α2\nN\nX\nn=1\nyn ln pθ1(˜y = n | zl) −KL(µl, σl),\n(3)\nwhere KL(µ, σ) = 1\n2\nPD\nd=1\n\u00001+2 ln(σd)−(µd)2−(σd)2\u0001\n,\nD denotes the dimension of the latent space, N is the to-\ntal number of classes in an (N-way, K-shot) task, α1, α2\nare constant scaling factors, µs and σ2\ns denote the mean and\nvariance vectors of semantic latent distribution, and µl and\nσ2\nl denote the mean and variance vectors of label latent dis-\ntribution. The hyper-parameters α1, α2 only scale the evi-\ndence lower-bound appropriately, since the reconstruction\nloss is in practice three orders of magnitude greater than the\ncross-entropy loss. As such, this scaling helps convergence\nbut impacts the tightness of the ELBO slightly; nonetheless,\n(2) and (3) are still considered variational inference by con-\nsensus among the literature (Higgins et al. 2017; Joy et al.\n2021; Mathieu et al. 2019; Dupont 2018). The loss is cal-\nculated for each given task on query and support sets sepa-\nrately; i.e., Lg = Lg\nR + Lg\nC with g ∈{Si, Qi}. Note that in\n(1) we deliberately choose to exclude the label information y\nas input to qφ1(.) to be able to exploit the associated genera-\ntive network pθ1(y | zl) as a classiﬁer. The consequence and\nthe proposed solution to accommodate this design choice are\ndiscussed in the next subsection.\nSupport Set Feature Maps\nQuery Set Feature Maps\nFigure 3: AttFEX module depicting colors as images and shades\nas feature maps. We illustrate only 3 image feature maps and 3\nchannels instead of 32 for N, for the sake of simplicity.\nAttFEX for Transductive Feature Extraction\nWe ﬁrst extract the feature maps of all images in the\ntask using a convolutional block F = ConvEnc(X) where\nX ∈RN(K+Q)×C×W ×H, F ∈RN(K+Q)×C′×W ′×H′.\nThe feature map tensor F is then transposed into F′ ∈\nRC′×N(K+Q)×W ′×H′ and fed into two consecutive 1 × 1\nconvolution blocks. This helps the network utilize informa-\ntion across corresponding pixels of all images in a task Ti\nwhich acts as a parametric comparison of classes. We lever-\nage the fact that ConvEnc already extracts local pixel infor-\nmation by using larger kernels, and thus, use parameter-light\n1 × 1 convolutions subsequently to focus only on individual\npixels. Let F′\ni denote the ith channel (or feature map layer)\nout of total of C′ available and ReLU denote the rectiﬁed lin-\near unit activation. The 1 × 1 convolution block (Conv1×1)\nis formulated as follows:\nMi = ReLU\n\u0000Conv1×1(F′\ni, WM)\n\u0001\n, ∀i ∈[1, C′];\nNj = ReLU\n\u0000Conv1×1(Mj, WN)\n\u0001\n, ∀j ∈[1, C′];\n(4)\nwhere N ∈RC′×32×W ′×H′ and WM ∈R64×N(K+Q)×1×1,\nWN ∈R32×64×1×1 denote the learnable weights. Next, we\nwant to blend information across feature maps for which we\nuse a self-attention mechanism (Vaswani et al. 2017) across\nNj, ∀j ∈[1, 32]. To do so, we feed N to query, key and\nvalue extraction networks fq(, ; WQ), fk(.; WK), fv(.; WV )\nwhich are also designed to be 1 × 1 convolutions as:\nQi = ReLU (Conv1×1(Ni, WQ)) ,\n∀i ∈[1, C′];\nKi = ReLU (Conv1×1(Ni, WK)) ,\n∀i ∈[1, C′];\nVi = ReLU (Conv1×1(Ni, WV )) ,\n∀i ∈[1, C′];\n(5)\nwhere WQ, WK, WV\n∈R1×32×1×1 are the learnable\nweights and Q, K, V ∈RC′×1×W ′×H′ are the query, key\nand value tensors. Next, each feature map Nj is mapped to\nits output tensor Gj by computing a weighted sum of the\nvalues, where each weight (within parentheses in (6)) mea-\nsures the compatibility (or similarity) between the query and\nits corresponding key tensor using an inner-product:\nGi =\nC′\nX\nj=1\n \nexp (Qi · Kj)\n√dk. PC′\nk=1 exp (Qi · Kk)\n!\nVi,\n(6)\n\n\nwhere dk = W ′ × H′, and Gi ∈R1×C′×W ′×H′, ∀i. Fi-\nnally, we transform the original feature maps F by apply-\ning a Hadamard product between the feature mask G and F,\nthus, rendering the required feature maps transductive:\n˜F\nS = G ◦FS\nor\n˜F\nQ = G ◦FQ.\nHere, FS and FQ represent the feature maps corresponding\nto the support and query images, respectively. As a result of\noperating on this channel-pixel distribution across images in\na task, FS and FQ are rendered task-aware. Note that the\nquery tensor Q must not be confused with the query set Q\nof a task.\nAlgorithmic Overview and Training Strategy\nFigure 4: TRIDENT is comprised of two intertwined variational\nnetworks. Zg\ns is concatenated with the output of AttFEX, and used\nfor inferring Zg\nl , where g ∈{S, Q}. Next, both Zg\nl and Zg\ns are used\nto reconstruct images ˜X\ng while Zg\nl is used to extract ˜Y g.\nOverview of TRIDENT. The complete architecture of\nTRIDENT is illustrated in Fig. 4. The ConvEnc feature ex-\ntractor and the linear layers µφ2(.), σ2\nφ2(.) constitute the in-\nference network qφ2 of the semantic latent variable (bottom\nrow of Fig. 4). The AttFEX module, another ConvEnc,\nand linear layers µφ1(.) and σ2\nφ1(.) make up the infer-\nence network qφ1 of the label latent variable (top row of\nFig. 4). The proposed approach, TRIDENT, is described in\nAlgorithm 1. Note that TRIDENT is trained in a MAML\n(Finn, Abbeel, and Levine 2017) fashion, where depend-\ning on the inner or outer loop, the support or query set\n(g ∈{S, Q}) will be the reference, respectively. First,\nthe lower ConvEnc block extracts feature maps Xg\nCE =\nConvEnc(Xg). Xg\nCE’s are then ﬂattened and passed onto\nµφ2(.), σ2\nφ2(.), which respectively output the mean and vari-\nance vectors of the semantic latent distribution, as discussed\nin (1). This is done either for the entire support or the query\nimages Xg, where g ∈{S, Q} for a given task Ti. We\nthen sample a set of vectors Zg\ns (subscript s for seman-\ntic) from their corresponding Gaussian distributions using\nthe re-parameterization trick (line 1, Algorithm 1). Upon\npassing X = XS ∪XQ through the upper ConvEnc, the\nAttFEX module of qφ1 comes into play to create task-\ncognizant feature maps ˜F\ng for either S or Q (line 2). Zg\ns\ntogether with ˜F\ng are passed onto the linear layers µφ1(.),\nσ2\nφ1(.) to generate the mean and variance vectors of the label\nAlgorithm 1: TRIDENT\nRequire: XS, XQ, Y g, Xg\nCE, where g ∈{S, Q}\n1 Sample: Zg\ns ∼qφ2\n\u0000Zs | µφ2(Xg\nCE), diag\n\u0000σ2\nφ2(Xg\nCE)\n\u0001\u0001\n2 Compute task-cognizant embeddings:\n[˜F\nS, ˜F\nQ] = AttFEX(ConvEnc(X)); X = XS ∪XQ\n3 Concatenate Zg\ns and ˜Fg into [ ˜Fg, Zg\ns] and sample:\nZg\nl ∼qφ1\n\u0000Zl | µφ1([˜F\ng, Zg\ns]), diag(σ2\nφ1([˜F\ng, Zg\ns]))\n\u0001\n4 Reconstruct Xg using ˜X\ng = pθ2(X | Zg\nl , Zg\ns)\n5 Extract class-conditional probabilities using:\np\n\u0000 ˜Y g | Zg\nl\n\u0001\n= softmax\n\u0000pθ1(Y g | Zg\nl )\n\u0001\n6 Compute Lg = Lg\nR + Lg\nC using (3)\nReturn: Lg\nAlgorithm 2: End to End Meta-Training of TRIDENT\nRequire: Dtr, α, β, B\n1 Randomly initialise Ψ = (φ1, φ2, θ1, θ2)\n2 while not converged do\n3\nSample B tasks Ti = Si ∪Qi from Dtr\n4\nfor each task Ti do\n5\nfor number of adaptation steps do\n6\nCompute LSi(Ψ) = TRIDENT(Ti −{Y Qi})\n7\nEvaluate ∇(Ψ)LSi(Ψ)\n8\nΨ ←Ψ −α∇ΨLSi(Ψ)\n9\nend\n10\n(Ψ′)i = Ψ\n11\nend\n12\nCompute\nLQi(Ψ′\ni) = TRIDENT(Ti −{Y Si}); ∀i ∈[1, B]\n13\nMeta-update on Qi: Ψ ←Ψ −β∇Ψ\nPB\ni=1 LQi(Ψ′\ni)\n14 end\nlatent Gaussian distributions (line 3). After sampling the set\nof vectors Zg\nl (subscript l for label) from their correspond-\ning distributions, we use Zg\nl and Zg\ns to reconstruct the input\nimages ˜X\ng using the generative network pθ2 (line 4). Next,\nZg\nl ’s are input to the classiﬁer network pθ1 to generate the\nclass logits, which are normalized using a softmax(.),\nresulting in class-conditional probabilities p( ˜Y g | Zg\nl ) (line\n5). Finally (in line 6), using the outputs of all the compo-\nnents discussed earlier, we calculate the loss Lg as formu-\nlated in (2) and (3).\nTraining strategy. An important as-\npect of the training procedure of TRIDENT is that its set\nof parameters Ψ = (θ1, θ2, φ1, φ2) are meta-learnt by back-\npropagating through the adaptation procedure on the support\nset, as proposed in MAML (Finn, Abbeel, and Levine 2017)\nand illustrated here in Algorithm 2. This increases the sen-\nsitivity of the parameters Ψ towards the loss function for\nfast adaptation to unseen tasks and reduces generalization\nerrors on the query set Q. First, we randomly initialize the\nparameters Ψ (line 1, Algorithm 2) to compute the objective\nfunction over the support set LSi(Ψ) using equation (3) in\nthe main manuscript, and perform a number of gradient de-\n\n\nTable 1: Accuracies in (% ± std). The predominant methodology of the baselines: Ind.: inductive inference, TF: transductive feature\nextraction methods, TI: transductive inference methods. Conv: convolutional blocks, RN: ResNet backbone, †: extra data. Style: best and\nsecond best. TRIDENT employs a transductive feature extraction module (TF), and the simplest of backbones (Conv4).\nminiImagenet\ntieredImagenet\nmini→CUB\nMethods\nBackbone\nApproach\n5-way 1-shot\n5-way 5-shot\n5-way 1-shot\n5-way 5-shot\n5-way 1-shot\n5-way 5-shot\nMAML (Finn, Abbeel, and Levine 2017)\nConv4\nInd.\n48.70 ± 1.84\n63.11 ± 0.92\n51.67 ± 1.81\n70.30 ± 0.08\n34.01 ± 1.25\n48.83 ± 0.62\nABML (Ravi and Beatson 2019)\nConv4\nInd.\n40.88 ± 0.25\n58.19 ± 0.17\n-\n-\n31.51 ± 0.32\n47.80 ± 0.51\nOVE(PL) (Patacchiola et al. 2020)\nConv4\nInd.\n48.00 ± 0.24\n67.14 ± 0.23\n-\n-\n37.49 ± 0.11\n57.23 ± 0.31\nDKT+Cos (Patacchiola et al. 2020)\nConv4\nInd.\n48.64 ± 0.45\n62.85 ± 0.37\n-\n-\n40.22 ± 0.54\n55.65 ± 0.05\nBOIL (Oh et al. 2021)\nConv4\nInd.\n49.61 ± 0.16\n48.58 ± 0.27\n66.45 ± 0.37\n69.37 ± 0.12\n-\n-\nLFWT(Tseng et al. 2020)\nRN10\nTF+TI\n66.32 ± 0.80\n81.98 ± 0.55\n-\n-\n47.47 ± 0.75\n66.98 ± 0.68\nFRN(Wertheimer, Tang, and Hariharan 2021)\nRN12\nInd.\n66.45 ± 0.19\n82.83 ± 0.13\n71.16 ± 0.22\n86.01 ± 0.15\n54.11 ± 0.19\n77.09 ± 0.15\nDPGN(Yang et al. 2020)\nRN12\nTF+TI\n67.77\n84.6\n72.45\n87.24\n-\n-\nPAL(Ma et al. 2021)\nRN12\nTF+TI\n69.37 ± 0.64\n84.40 ± 0.44\n72.25 ± 0.72\n86.95 ± 0.47\n-\n-\nProto-Completion(Zhang et al. 2021a)\nRN12\nTF+TI\n73.13 ± 0.85\n82.06 ± 0.54\n81.04 ± 0.89\n87.42 ± 0.57\n-\n-\nTPMN(Wu et al. 2021)\nRN12\nTF+TI\n67.64 ± 0.63\n83.44 ± 0.43\n72.24 ± 0.70\n86.55 ± 0.63\n-\n-\nLIF-EMD(Li, Wang, and Hu 2021)\nRN12\nTF+TI\n68.94 ± 0.28\n85.07 ± 0.50\n73.76 ± 0.32\n87.83 ± 0.59\n-\n-\nTransd-CNAPS(Bateni et al. 2022)\nRN18\nTF+TI\n55.6 ± 0.9\n73.1 ± 0.7\n65.9 ± 1.0\n81.8 ± 0.7\n-\n-\nBaseline++(Chen et al. 2019)\nRN18\nTF\n51.87 ± 0.77\n75.68 ± 0.63\n-\n-\n42.85 ± 0.69\n62.04 ± 0.76\nFEAT(Ye et al. 2020)\nRN18\nTF\n66.78\n82.05\n70.80\n84.79\n50.67 ± 0.78\n71.08 ± 0.73\nSimpleShot(Wang et al. 2019)\nWRN\nInd.\n63.32\n80.28\n69.98\n85.45\n48.56\n65.63\nAssoc-Align(Afrasiyabi, Lalonde, and Gagn´e 2020)\nWRN\nTF\n65.92 ± 0.60\n82.85 ± 0.55\n74.40 ± 0.68\n86.61 ± 0.59\n47.25 ± 0.76\n72.37 ± 0.89\nReRank(SHEN et al. 2021)\nWRN\nTF+TI\n72.4±0.6\n80.2±0.4\n79.5±0.6\n84.8±0.4\n-\n-\nTIM-GD(Boudiaf et al. 2020)\nWRN\nTI\n77.8\n87.4\n82.1\n89.8\n-\n71\nLaplacianShot(Ziko et al. 2020)\nWRN\nTI\n74.9\n84.07\n80.22\n87.49\n55.46\n66.33\nS2M2(Mangla et al. 2020)\nWRN\nTF\n64.93 ± 0.18\n83.18 ± 0.11\n73.71 ± 0.22\n88.59 ± 0.14\n48.24 ± 0.84\n70.44 ± 0.75\nMetaQDA(Zhang et al. 2021b)\nWRN\nTF\n67.83 ± 0.64\n84.28 ± 0.69\n74.33 ± 0.65\n89.56 ± 0.79\n53.75 ± 0.72\n71.84 ± 0.66\nPT+MAP(Hu, Gripon, and Pateux 2021)\nWRN\nTF+TI\n82.92 ± 0.26\n88.82 ± 0.13\n85.67 ± 0.26\n90.45 ± 0.14\n62.49 ± 0.32\n76.51 ± 0.18\nPEMnE-BMS(Hu, Pateux, and Gripon 2022)\nWRN\nTF+TI\n83.35 ± 0.25\n89.53 ± 0.13\n86.07 ± 0.25\n91.09 ± 0.14\n63.90 ± 0.31\n79.15 ± 0.18\nTransd-CNAPS+FETI(Bateni et al. 2022)\nRN18†\nTF+TI\n79.9 ± 0.8\n91.50 ± 0.4\n73.8 ± 0.1\n87.7 ± 0.6\n-\n-\nTRIDENT(Ours)\nConv4\nTF\n86.11 ± 0.59\n95.95 ± 0.28\n86.97 ± 0.50\n96.57 ± 0.17\n84.61 ± 0.33\n80.74 ± 0.35\nscent steps on the parameters Ψ to adapt them to the support\nset (lines 5 to 9). This is called the inner-update and is done\nseparately for all the support sets corresponding to their B\ndifferent tasks (line 3). Once the inner-update is computed\nfor each of the B parameter sets, the loss is evaluated on the\nquery set LQi(Ψ′\ni) (line 12), following which a meta-update\nis conducted over all the corresponding query sets, which\ninvolves computing a gradient through a gradient procedure\nas described in (Finn, Abbeel, and Levine 2017) (line 13).\nExperimental Evaluation\nThe goal of this section is to address the following four\nquestions: (i) How well does TRIDENT perform when com-\npared against the state-of-the-art methods for few-shot clas-\nsiﬁcation? (ii) How reliable is TRIDENT in terms of the\nconﬁdence and uncertainty metrics? (iii) How well does\nTRIDENT perform in a cross-domain setting where there is\na domain shift between the training and testing datasets? (iv)\nDoes TRIDENT actually decouple latent variables?\nBenchmark Datasets. We evaluate TRIDENT on the three\nmost commonly adopted datasets: miniImagenet (Ravi and\nLarochelle 2017), tieredImagenet (Ren et al. 2018) and CUB\n(Welinder et al. 2010). miniImagenet and tieredImagenet are\nsubsets of ImageNet (Deng et al. 2009) utilized for few-shot\nclassiﬁcation. Further details on these datasets can be found\nin the Appendix.\nImplementational Details. We use PyTorch (Paszke et al.\n2019) and learn2learn (Arnold et al. 2020) for all our im-\nplementations. We use a commonly adopted Conv4 ar-\nchitecture (Ravi and Larochelle 2017; Finn, Abbeel, and\nLevine 2017; Patacchiola et al. 2020; Afrasiyabi, Lalonde,\nand Gagn´e 2020; Wang et al. 2019; Boudiaf et al. 2020) as\nConvEnc to obtain the generic feature maps. Following the\nstandard setting in the literature (Finn, Abbeel, and Levine\n2017; Ravi and Larochelle 2017), the Conv4 has four con-\nvolutional blocks where each block has a 3 × 3 convolu-\ntion layer with 32 feature maps, followed by a batch nor-\nmalization (BN) (Ioffe and Szegedy 2015) layer, a 2 × 2\nmax-pooling layer and a LeakyReLU(0.2) activation.\nThe generative network pθ1 for zl is a classiﬁer with two\nlinear layers and a LeakyReLU(0.2) activation in be-\ntween, while pθ2 for zs consists of four blocks of a 2-\nD upsampling layer, followed by a 3 × 3 convolution and\nLeakyReLU(0.2) activation. Both latent variables zl and\nzs have a dimensionality of 64. Following (Nichol, Achiam,\nand Schulman 2018b; Liu et al. 2019; Vaswani et al. 2017),\nimages are resized to 84 × 84 for all conﬁgurations and we\ntrain and report test accuracy of (5-way, 1 and 5-shot) set-\ntings with 10 query images per class for all datasets. Hyper-\nparameter settings can be found in the Appendix.\nEvaluation Results\nWe report test accuracies indicating 95% conﬁdence inter-\nvals over 600 tasks for miniImagenet, and 2000 tasks for\nboth tieredImagenet and CUB, as is customary across the\nliterature (Chen et al. 2019; Dhillon et al. 2020; Bateni\net al. 2022). We compare our performance against a wide\nvariety of state-of-the-art few-shot classiﬁcation methods\nsuch as: (i) metric-learning (Wang et al. 2019; Bateni et al.\n2020; Afrasiyabi, Lalonde, and Gagn´e 2020; Yang et al.\n2020), (ii) transductive feature-extraction based (Oreshkin,\nRodr´ıguez L´opez, and Lacoste 2018; Ye et al. 2020; Li\net al. 2019; Xu et al. 2021), (iii) optimization-based (Finn,\nAbbeel, and Levine 2017; Mishra et al. 2018; Oh et al.\n2021; Lee et al. 2019; Rusu et al. 2019), (iv) transductive\ninference-based (Bateni et al. 2022; Boudiaf et al. 2020;\n\n\nTable 2: Parameter count of TRIDENT against competitors.\nConv4\nµφ\nσφ\nAttFEX\nTRIDENT\nConv4\nRN18\nWRN\nqφ1\n28896\n51264\n51264\n6994\nqφ2\n28896\n51264\n51264\n-\npθ1+ pθ2\n2245 + 132009\n412,238\n190, 410\n12.4M\n36.482M\nZiko et al. 2020; Liu et al. 2019), and (v) Bayesian (Iakovl-\neva, Verbeek, and Alahari 2020; Zhang et al. 2019; Hu et al.\n2020; Patacchiola et al. 2020; Ravi and Beatson 2019) ap-\nproaches. Previous works (Liu et al. 2019), (Hou et al. 2019)\nhave demonstrated the superiority of transductive inference\nmethods over their inductive counterparts. In this light, we\ncompare against a larger number of transductive (18 base-\nlines) rather than inductive (7 baselines) methods for a fair\ncomparison.\nIt is important to note that TRIDENT is only a transduc-\ntive feature-extraction based method as we utilize the query\nset images to extract task-aware feature embeddings; it is\nnot a transductive inference based method since we per-\nform inference of class-labels over the entire domain of def-\ninition and not just for the selected query samples (Vap-\nnik 2006; Gammerman, Vovk, and Vapnik 1998). The re-\nsults on miniImagenet and tieredImagenet for both (5-way,\n1 and 5-shot) settings are summarized in Table 1. We ac-\ncentuate on the fact that we also compare against Transd-\nCNAPS+FETI (Bateni et al. 2022), where the authors pre-\ntrain the ResNet-18 backbone on the entire train split\nof Imagenet. We, however, avoid training on additional\ndatasets, in favor of fair comparison with the rest of liter-\nature. Regardless of the choice of backbone (simplest in our\ncase), TRIDENT sets a new state-of-the-art on miniImagenet\nand tieredImagenet for both (5-way, 1 and 5-shot) settings,\noffering up to 5% gain over the prior art. Recently, a more\nchallenging cross-domain setting has been proposed for few-\nshot classiﬁcation to assess its generalization capabilities to\nunseen datasets. The commonly adopted setting is where one\ntrains on miniImagenet and tests on CUB (Chen et al. 2019).\nThe results of this experiment are also presented in Table 1.\nWe compare against any existing baselines for which this\ncross-domain experiment has been conducted. As can be\nseen, and to the best of our knowledge, TRIDENT again sets\na new state-of-the-art by a signiﬁcant margin of 20% for (5-\nway, 1-shot) setting, and 1.5% for (5-way, 5-shot) setting.\nComputational Complexity. Most of the reported base-\nlines in Table 1 use stronger backbones such as ResNet12,\nResNet18 and WRN which contain 11.5, 12.4 and 36.4 mil-\nlions of parameters respectively. On the other hand, we use\nthree Conv4s along with two fully connected layers and an\nAttFEX module which accounts for 410,958 and 412,238\nparameters in the (5-way, 1-shot) and (5-way, 5-shot) sce-\nnarios, respectively. This is summarized in details in Table 2.\nEven though we are more parameter heavy than approaches\nthat use a single Conv4 as feature extractor, TRIDENT’s\ntotal parameters still lies in the same order of magnitude as\nthese approaches. In summary, when it comes to complexity\nin parameter space, we are considerably more efﬁcient than\nthe vast majority of the cited competitors.\nReliability Metrics. A complementary set of metrics are\ntypically used in probabilistic settings to measure the uncer-\ntainty and reliability of predictions. More speciﬁcally, ex-\npected calibration error (ECE) and maximum calibration er-\nror (MCE) respectively measure the expected and maximum\nTable 3: Calibration errors of TRIDENT. Style: best and\nsecond best.\nMetrics\nMAML\nPLATIPUS\nABPML\nABML\nBMAML\nVAMPIRE\nTRIDENT\nECE\n0.046\n0.032\n0.013\n0.026\n0.025\n0.008\n0.0036\n5-way,\n1-shot\nMCE\n0.073\n0.108\n0.037\n0.058\n0.092\n0.038\n0.029\nECE\n0.032\n-\n0.006\n-\n0.027\n-\n0.0015\n5-way,\n5-shot\nMCE\n0.044\n-\n0.030\n-\n0.049\n-\n0.018\nbinned difference between conﬁdence and accuracy (Guo\net al. 2017). This is illustrated in Table 3 where TRIDENT\noffers superior calibration on miniImagenet (5-way, 1 and\n5-shot) as compared to other probabilistic approaches, and\nMAML (Finn, Abbeel, and Levine 2017).\nAblation Study\nWe analyze the classiﬁcation performance of TRIDENT\nacross various paramaters and hyper-parameters, as is sum-\nmarized in Table 4. We use miniImagenet (5-way, 1-shot)\nsetting to carry out ablation study experiments. To cover\ndifferent design perspectives, we carry out ablation on: (i)\nMAML-style training parameters: meta-batch size B and\nnumber of inner adaption steps n, (ii) latent space dimen-\nsionality: zl and zs to assess the impact of their size, (iii)\nAttFEX features: number of features extracted by WM,\nWN. Looking at the results, TRIDENT’s performance is\ndirectly proportional to the number of tasks and inner-\nadaptation steps, as is previously demonstrated in (Antreas\nAntoniou, Harrison Edwards, and Amos J. Storkey 2019;\nFinn, Abbeel, and Levine 2017) for MAML based training.\nRegarding latent space dimensions, a correlation between a\nhigher dimension of zl and zs and a better performance can\nbe observed. Even though, the results show that increasing\nboth dimensions beyond 64 leads to performance degrada-\ntion. As such, (64, 64) seems to be the sweet spot. Finally,\non feature space dimensions of AttFEX, the performance\nimproves when WM > WN, and the best performance is\nachieved when the parameters are set to (64, 32). Notably,\nthe exact set of parameters return the best performance for\n(5-way, 5-shot) setting.\nDecoupling Analysis\nAs a qualitative demonstration, we visualize the label and\nsemantic latent means (µl and µs) of query images for a\nrandomly selected (5-way, 5-shot) task from miniImagenet,\nbefore and after the MAML meta-update procedure. The\nUMAP (McInnes, Healy, and Melville 2018) plots in Fig. 5\nillustrate signiﬁcant improvement in class-conditional sep-\naration of query samples for label latent space upon meta-\nadaption, whereas negligible improvement is visible on the\nsemantic latent space. This is a qualitative evidence that ZL\ncaptures more class-discriminating information as compared\nto ZS. To substantiate this quantitatively, the clustering ca-\npacity of these latent spaces is also measured by the Davies-\nBouldin score (DBI) (Davies and Bouldin 1979), where, the\nlower the DBI score, the better both the inter-cluster sep-\naration and intra-cluster “tightness”. Fig. 5 shows that the\nDBI score drops signiﬁcantly more after meta-adaptation in\nthe case of ZL as compared to ZS, indicating better cluster-\ning of features in the former than the latter. This aligns with\nthe proposed decoupling strategy of TRIDENT and corrob-\norates the validity of our proposition to put an emphasis on\n\n\nTable 4: Ablation study for miniImagenet (5-way, 1-shot) tasks. Accuracies in (% ± std.).\n(B, n)\n(5, 3)\n(5, 5)\n(10, 3)\n(10, 5)\n(20, 3)\n(20, 5)\n-\n67.43 ± 0.75\n69.21 ± 0.66\n74.6 ± 0.84\n80.82 ± 0.68\n86.11 ± 0.59\n(dim(zl),\ndim(zs))\n(32, 32)\n(32, 64)\n(32, 128)\n(64, 32)\n(64, 64)\n(64, 128)\n(128, 32)\n(128, 64)\n(128, 128)\n76.29 ± 0.72\n75.44 ± 0.81\n79.1 ± 0.57\n82.93 ± 0.8\n86.11 ± 0.59\n85.62 ± 0.52\n81.49 ± 0.65\n82.89 ± 0.48\n84.42 ± 0.59\n(dim(WM),\ndim(WN))\n(32, 32)\n(32, 64)\n(32, 128)\n(64, 32)\n(64, 64)\n(64, 128)\n(128, 32)\n(128, 64)\n(128, 128)\n78.4 ± 0.23\n77.89 ± 0.39\n79.55 ± 0.87\n86.11 ± 0.59\n84.87 ± 0.45\n82.11 ± 0.35\n84.67 ± 0.7\n85.8 ± 0.58\n83.92 ± 0.63\nFigure 5: Better class separation upon meta-update is conﬁrmed\nby lower DBI scores. Different colors/markers indicate classes.\nlabel latent information for the downstream few-shot task.\nConcluding Remarks\nWe introduce a novel variational inference network (coined\nas TRIDENT) that simultaneously infers decoupled latent\nvariables representing semantic and label information of\nan image. The proposed network is comprised of two in-\ntertwined variational sub-networks responsible for inferring\nthe semantic and label information separately, the latter be-\ning enhanced using an attention-based transductive feature\nextraction module (AttFEX). Our extensive experimental\nresults corroborate the efﬁcacy of this transductive decou-\npling strategy on a variety of few-shot classiﬁcation settings\ndemonstrating superior performance and setting a new state-\nof-the-art for the most commonly adopted dataset mini and\ntieredImagenet as well as for the recent challenging cross-\ndomain scenario of miniImagenet →CUB. As future work,\nwe plan to demonstrate the applicability of TRIDENT in\nsemi-supervised and unsupervised settings by including the\nlikelihood of unlabelled samples derived from the graphi-\ncal model. This would render TRIDENT as an all-inclusive\nholistic approach towards solving few-shot classiﬁcation.\nReferences\nAfrasiyabi, A.; Lalonde, J.-F.; and Gagn´e, C. 2020. Asso-\nciative alignment for few-shot image classiﬁcation. In Eu-\nropean Conference on Computer Vision, 18–35. Springer.\nAntreas Antoniou; Harrison Edwards; and Amos J. Storkey.\n2019. How to train your MAML. In ICLR (Poster). Open-\nReview.net.\nArnold, S. M. R.; Mahajan, P.; Datta, D.; Bunner, I.; and\nZarkias, K. S. 2020.\nlearn2learn: A Library for Meta-\nLearning Research.\nBateni, P.; Barber, J.; van de Meent, J.-W.; and Wood, F.\n2022. Enhancing Few-Shot Image Classiﬁcation With Un-\nlabelled Examples. In Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision (WACV),\n2796–2805.\nBateni, P.; Goyal, R.; Masrani, V.; Wood, F.; and Sigal, L.\n2020. Improved Few-Shot Visual Classiﬁcation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR).\nBoudiaf, M.; Ziko, I.; Rony, J.; Dolz, J.; Piantanida, P.; and\nBen Ayed, I. 2020.\nInformation Maximization for Few-\nShot Learning. In Larochelle, H.; Ranzato, M.; Hadsell, R.;\nBalcan, M. F.; and Lin, H., eds., Advances in Neural Infor-\nmation Processing Systems, volume 33, 2445–2457. Curran\nAssociates, Inc.\nBRIER, G. W. 1950. VERIFICATION OF FORECASTS\nEXPRESSED IN TERMS OF PROBABILITY.\nMonthly\nWeather Review, 78(1): 1 – 3.\nChen, W.-Y.; Liu, Y.-C.; Kira, Z.; Wang, Y.-C.; and Huang,\nJ.-B. 2019. A Closer Look at Few-shot Classiﬁcation. In\nInternational Conference on Learning Representations.\nDavies, D. L.; and Bouldin, D. W. 1979. A Cluster Separa-\ntion Measure. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, PAMI-1(2): 224–227.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, 248–255. Ieee.\nDhillon, G. S.; Chaudhari, P.; Ravichandran, A.; and Soatto,\nS. 2020. A Baseline for Few-Shot Image Classiﬁcation. In\nInternational Conference on Learning Representations.\nDupont, E. 2018. Learning Disentangled Joint Continuous\nand Discrete Representations. In Bengio, S.; Wallach, H.;\nLarochelle, H.; Grauman, K.; Cesa-Bianchi, N.; and Gar-\nnett, R., eds., Advances in Neural Information Processing\nSystems, volume 31. Curran Associates, Inc.\nEdwards, H.; and Storkey, A. J. 2017. Towards a Neural\nStatistician. ArXiv, abs/1606.02185.\nFinn, C.; Abbeel, P.; and Levine, S. 2017. Model-Agnostic\nMeta-Learning for Fast Adaptation of Deep Networks. In\n\n\nProceedings of the 34th International Conference on Ma-\nchine Learning, volume 70 of Proceedings of Machine\nLearning Research, 1126–1135.\nFinn, C.; Xu, K.; and Levine, S. 2018. Probabilistic Model-\nAgnostic Meta-Learning.\nIn Bengio, S.; Wallach, H.;\nLarochelle, H.; Grauman, K.; Cesa-Bianchi, N.; and Gar-\nnett, R., eds., Advances in Neural Information Processing\nSystems, volume 31. Curran Associates, Inc.\nGaly-Fajou, T.; Wenzel, F.; Donner, C.; and Opper, M.\n2020. Multi-Class Gaussian Process Classiﬁcation Made\nConjugate: Efﬁcient Inference via Data Augmentation. In\nAdams, R. P.; and Gogate, V., eds., Proceedings of The 35th\nUncertainty in Artiﬁcial Intelligence Conference, volume\n115 of Proceedings of Machine Learning Research, 755–\n765. PMLR.\nGammerman, A.; Vovk, V.; and Vapnik, V. 1998. Learning\nby transduction, vol UAI’98.\nGordon, J.; Bronskill, J.; Bauer, M.; Nowozin, S.; and\nTurner, R. 2019. Meta-Learning Probabilistic Inference for\nPrediction. In International Conference on Learning Rep-\nresentations.\nGuo, C.; Pleiss, G.; Sun, Y.; and Weinberger, K. Q. 2017.\nOn Calibration of Modern Neural Networks.\nIn Pro-\nceedings of the 34th International Conference on Machine\nLearning, volume 70 of Proceedings of Machine Learning\nResearch, 1321–1330. PMLR.\nHiggins, I.; Matthey, L.; Pal, A.; Burgess, C.; Glorot, X.;\nBotvinick, M.; Mohamed, S.; and Lerchner, A. 2017. beta-\nVAE: Learning Basic Visual Concepts with a Constrained\nVariational Framework.\nIn International Conference on\nLearning Representations.\nHou, R.; Chang, H.; MA, B.; Shan, S.; and Chen, X. 2019.\nCross Attention Network for Few-shot Classiﬁcation.\nIn\nWallach, H.; Larochelle, H.; Beygelzimer, A.; d'Alch´e-Buc,\nF.; Fox, E.; and Garnett, R., eds., Advances in Neural Infor-\nmation Processing Systems, volume 32. Curran Associates,\nInc.\nHu, S. X.; Moreno, P.; Xiao, Y.; Shen, X.; Obozinski,\nG.; Lawrence, N.; and Damianou, A. 2020.\nEmpirical\nBayes Transductive Meta-Learning with Synthetic Gradi-\nents. In International Conference on Learning Representa-\ntions (ICLR).\nHu, Y.; Gripon, V.; and Pateux, S. 2021. Leveraging the fea-\nture distribution in transfer-based few-shot learning. In In-\nternational Conference on Artiﬁcial Neural Networks, 487–\n499. Springer.\nHu, Y.; Pateux, S.; and Gripon, V. 2022. Squeezing back-\nbone feature distributions to the max for efﬁcient few-shot\nlearning. Algorithms, 15(5): 147.\nIakovleva, E.; Verbeek, J.; and Alahari, K. 2020.\nMeta-\nLearning with Shared Amortized Variational Inference. In\nProceedings of the 37th International Conference on Ma-\nchine Learning, Proceedings of Machine Learning Re-\nsearch. PMLR.\nIoffe, S.; and Szegedy, C. 2015. Batch Normalization: Ac-\ncelerating Deep Network Training by Reducing Internal\nCovariate Shift. In Proceedings of the 32nd International\nConference on Machine Learning, volume 37 of Proceed-\nings of Machine Learning Research, 448–456. PMLR.\nJoy, T.; Schmon, S.; Torr, P.; N, S.; and Rainforth, T. 2021.\nCapturing Label Characteristics in {VAE}s.\nIn Interna-\ntional Conference on Learning Representations.\nKingma, D. P.; and Ba, J. 2015.\nAdam: A Method for\nStochastic Optimization. In ICLR (Poster).\nKingma, D. P.; Mohamed, S.; Jimenez Rezende, D.; and\nWelling, M. 2014.\nSemi-supervised Learning with Deep\nGenerative Models.\nIn Advances in Neural Information\nProcessing Systems, volume 27.\nKingma, D. P.; and Welling, M. 2014. Auto-Encoding Vari-\national Bayes. arXiv:1312.6114.\nLee, K.; Maji, S.; Ravichandran, A.; and Soatto, S. 2019.\nMeta-Learning with Differentiable Convex Optimization.\nIn CVPR.\nLi, H.; Eigen, D.; Dodge, S.; Zeiler, M.; and Wang, X. 2019.\nFinding Task-Relevant Features for Few-Shot Learning by\nCategory Traversal. In CVPR.\nLi, J.; Wang, Z.; and Hu, X. 2021. Learning Intact Features\nby Erasing-Inpainting for Few-shot Classiﬁcation.\nPro-\nceedings of the AAAI Conference on Artiﬁcial Intelligence,\n35(9): 8401–8409.\nLiu, J.; Song, L.; and Qin, Y. 2020. Prototype Rectiﬁcation\nfor Few-Shot Learning. In Computer Vision - ECCV 2020\n- 16th European Conference, Glasgow, UK, August 23-28,\n2020, Proceedings, Part I.\nLiu, Y.; Lee, J.; Park, M.; Kim, S.; Yang, E.; Hwang, S.; and\nYang, Y. 2019. Learning to Propagate Labels: Transductive\nPropagation Network for Few-shot Learning. In Interna-\ntional Conference on Learning Representations.\nLong, M.; CAO, Z.; Wang, J.; and Jordan, M. I. 2018. Con-\nditional Adversarial Domain Adaptation. In Advances in\nNeural Information Processing Systems, volume 31. Curran\nAssociates, Inc.\nMa, J.; Xie, H.; Han, G.; Chang, S.-F.; Galstyan, A.;\nand Abd-Almageed, W. 2021.\nPartner-assisted learning\nfor few-shot image classiﬁcation.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\n10573–10582.\nMangla, P.; Kumari, N.; Sinha, A.; Singh, M.; Krishna-\nmurthy, B.; and Balasubramanian, V. N. 2020. Charting the\nright manifold: Manifold mixup for few-shot learning. In\nProceedings of the IEEE/CVF winter conference on appli-\ncations of computer vision, 2218–2227.\nMathieu, E.; Rainforth, T.; Siddharth, N.; and Teh, Y. W.\n2019.\nDisentangling Disentanglement in Variational Au-\ntoencoders. In Chaudhuri, K.; and Salakhutdinov, R., eds.,\nProceedings of the 36th International Conference on Ma-\nchine Learning, volume 97 of Proceedings of Machine\nLearning Research, 4402–4412. PMLR.\nMcInnes, L.; Healy, J.; and Melville, J. 2018. Umap: Uni-\nform manifold approximation and projection for dimension\nreduction. arXiv preprint arXiv:1802.03426.\nMishra, N.; Rohaninejad, M.; Chen, X.; and Abbeel, P.\n2018. A Simple Neural Attentive Meta-Learner. In Inter-\nnational Conference on Learning Representations.\nNguyen, C. C.; Do, T.; and Carneiro, G. 2019. Uncertainty\nin Model-Agnostic Meta-Learning using Variational Infer-\nence. CoRR.\nNichol, A.; Achiam, J.; and Schulman, J. 2018a. On First-\nOrder Meta-Learning Algorithms. arXiv:1803.02999.\nNichol, A.; Achiam, J.; and Schulman, J. 2018b. On First-\nOrder Meta-Learning Algorithms. CoRR, abs/1803.02999.\n\n\nOh, J.; Yoo, H.; Kim, C.; and Yun, S.-Y. 2021. BOIL: To-\nwards Representation Change for Few-shot Learning. In\nInternational Conference on Learning Representations.\nOreshkin, B.; Rodr´ıguez L´opez, P.; and Lacoste, A. 2018.\nTADAM: Task dependent adaptive metric for improved\nfew-shot learning. In Advances in Neural Information Pro-\ncessing Systems, volume 31.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga,\nL.; Desmaison, A.; Kopf, A.; Yang, E.; DeVito, Z.; Raison,\nM.; Tejani, A.; Chilamkurthy, S.; Steiner, B.; Fang, L.; Bai,\nJ.; and Chintala, S. 2019. PyTorch: An Imperative Style,\nHigh-Performance Deep Learning Library. In Advances in\nNeural Information Processing Systems 32.\nPatacchiola, M.; Turner, J.; Crowley, E. J.; O’Boyle, M.\nF. P.; and Storkey, A. J. 2020. Bayesian Meta-Learning for\nthe Few-Shot Setting via Deep Kernels. In NeurIPS.\nRajeswaran, A.; Finn, C.; Kakade, S. M.; and Levine, S.\n2019. Meta-Learning with Implicit Gradients. In Wallach,\nH.; Larochelle, H.; Beygelzimer, A.; d'Alch´e-Buc, F.; Fox,\nE.; and Garnett, R., eds., Advances in Neural Information\nProcessing Systems, volume 32. Curran Associates, Inc.\nRavi, S.; and Beatson, A. 2019. Amortized Bayesian Meta-\nLearning. In International Conference on Learning Repre-\nsentations.\nRavi, S.; and Larochelle, H. 2017. Optimization as a Model\nfor Few-Shot Learning. In ICLR.\nRen, M.; Ravi, S.; Triantaﬁllou, E.; Snell, J.; Swersky, K.;\nTenenbaum, J. B.; Larochelle, H.; and Zemel, R. S. 2018.\nMeta-Learning for Semi-Supervised Few-Shot Classiﬁca-\ntion. In International Conference on Learning Represen-\ntations.\nRequeima, J.; Gordon, J.; Bronskill, J.; Nowozin, S.; and\nTurner, R. E. 2019. Fast and Flexible Multi-Task Classi-\nﬁcation using Conditional Neural Adaptive Processes. In\nAdvances in Neural Information Processing Systems, vol-\nume 32.\nRusu, A. A.; Rao, D.; Sygnowski, J.; Vinyals, O.; Pascanu,\nR.; Osindero, S.; and Hadsell, R. 2019. Meta-Learning with\nLatent Embedding Optimization. In International Confer-\nence on Learning Representations.\nSatorras, V. G.; and Estrach, J. B. 2018. Few-Shot Learning\nwith Graph Neural Networks. In International Conference\non Learning Representations.\nSHEN, X.; Xiao, Y.; Hu, S. X.; Sbai, O.; and Aubry, M.\n2021. Re-ranking for image retrieval and transductive few-\nshot classiﬁcation. In Beygelzimer, A.; Dauphin, Y.; Liang,\nP.; and Vaughan, J. W., eds., Advances in Neural Informa-\ntion Processing Systems.\nSnell, J.; Swersky, K.; and Zemel, R. 2017. Prototypical\nNetworks for Few-shot Learning. In Advances in Neural\nInformation Processing Systems, volume 30.\nSnell, J.; and Zemel, R. 2021. Bayesian Few-Shot Classiﬁ-\ncation with One-vs-Each P´olya-Gamma Augmented Gaus-\nsian Processes. In International Conference on Learning\nRepresentations.\nSun, Z.; Wu, J.; Li, X.; Yang, W.; and Xue, J.-H. 2021.\nAmortized Bayesian Prototype Meta-learning: A New\nProbabilistic Meta-learning Approach to Few-shot Image\nClassiﬁcation.\nIn Proceedings of The 24th International\nConference on Artiﬁcial Intelligence and Statistics, Pro-\nceedings of Machine Learning Research.\nSung, F.; Yang, Y.; Zhang, L.; Xiang, T.; Torr, P. H.; and\nHospedales, T. M. 2018. Learning to Compare: Relation\nNetwork for Few-Shot Learning.\nIn Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR).\nTseng, H.-Y.; Lee, H.-Y.; Huang, J.-B.; and Yang, M.-\nH. 2020.\nCross-domain few-shot classiﬁcation via\nlearned feature-wise transformation.\narXiv preprint\narXiv:2001.08735.\nVapnik, V. N. 2006. Estimation of Dependences Based on\nEmpirical Data. Estimation of Dependences Based on Em-\npirical Data.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017.\nAttention is All you Need. In Advances in Neural Informa-\ntion Processing Systems, volume 30.\nVinyals, O.; Blundell, C.; Lillicrap, T.; kavukcuoglu, k.; and\nWierstra, D. 2016. Matching Networks for One Shot Learn-\ning. In Advances in Neural Information Processing Systems,\nvolume 29.\nWang, Y.; Chao, W.-L.; Weinberger, K. Q.; and van der\nMaaten, L. 2019. SimpleShot: Revisiting Nearest-Neighbor\nClassiﬁcation for Few-Shot Learning. arXiv:1911.04623.\nWelinder, P.; Branson, S.; Mita, T.; Wah, C.; Schroff, F.; Be-\nlongie, S.; and Perona, P. 2010. Caltech-UCSD Birds 200.\nTechnical Report CNS-TR-2010-001, California Institute of\nTechnology.\nWertheimer, D.; Tang, L.; and Hariharan, B. 2021. Few-\nShot Classiﬁcation With Feature Map Reconstruction Net-\nworks.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 8012–\n8021.\nWu, J.; Zhang, T.; Zhang, Y.; and Wu, F. 2021. Task-Aware\nPart Mining Network for Few-Shot Learning. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, 8433–8442.\nXu, W.; yifan xu; Wang, H.; and Tu, Z. 2021. Attentional\nConstellation Nets for Few-Shot Learning. In International\nConference on Learning Representations.\nYang, L.; Li, L.; Zhang, Z.; Zhou, X.; Zhou, E.; and Liu,\nY. 2020. DPGN: Distribution Propagation Graph Network\nfor Few-Shot Learning.\n2020 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 13387–\n13396.\nYe, H.-J.; Hu, H.; Zhan, D.-C.; and Sha, F. 2020.\nFew-\nShot Learning via Embedding Adaptation with Set-to-Set\nFunctions. In IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 8808–8817.\nYoon, J.; Kim, T.; Dia, O.; Kim, S.; Bengio, Y.; and Ahn,\nS. 2018.\nBayesian Model-Agnostic Meta-Learning.\nIn\nAdvances in Neural Information Processing Systems, vol-\nume 31.\nZhang, B.; Li, X.; Ye, Y.; Huang, Z.; and Zhang, L. 2021a.\nPrototype Completion With Primitive Knowledge for Few-\nShot Learning. In CVPR, 3754–3762.\nZhang, J.; Zhao, C.; Ni, B.; Xu, M.; and Yang, X. 2019.\nVariational Few-Shot Learning. In 2019 IEEE/CVF Inter-\nnational Conference on Computer Vision (ICCV), 1685–\n1694.\n\n\nZhang, X.; Meng, D.; Gouk, H.; and Hospedales, T. M.\n2021b. Shallow bayesian meta learning for real-world few-\nshot recognition. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, 651–660.\nZiko, I. M.; Dolz, J.; Granger, E.; and Ayed, I. B. 2020.\nLaplacian Regularized Few-Shot Learning.\nIn ICML,\n11660–11670.\n\n\nAppendix\nImpact of AttFEX\nIn order to study the impact of the transductive feature ex-\ntractor AttFEX, we exclude it during training and train the\nremaining architecture. Training proceeds exactly as men-\ntioned before in the manuscript. As can be seen in Ta-\nTable 5: Impact of AttFEX on classiﬁcation accuracies.\nminiImagenet\ntieredImagenet\n(5-way, 1-shot)\n(5-way, 5-shot)\n(5-way, 1-shot)\n(5-way, 5-shot)\nAttFEX OFF\n67.68 ± 0.55\n78.53 ± 0.21\n69.32 ± 0.76\n79.32 ± 0.76\nAttFEX ON\n86.11 ± 0.59\n95.95 ± 0.28\n86.97 ± 0.50\n96.57 ± 0.17\nble 5, the exclusion of AttFEX from TRIDENT results in\na substantial drop in classiﬁcation performance across both\ndatasets and task settings. Empirically, this further substan-\ntiates the importance of AttFEX’s ability to render the fea-\nture maps transductive/task-aware. As explained earlier in\nthe main manuscript, it is imperative to include y in the input\nto qφ1(.) for mathematical correctness of the variational in-\nference formulation. However, in order to utilize TRIDENT\nas a classiﬁcation and not a label reconstruction network, we\nchoose not to input y to qφ1(.), but rather do so indirectly by\ninducing a semblance of label characteristics in the features\nextracted from the images in a task. Thus, it is important to\nrealize that this ability of AttFEX to render feature maps\ntransductive is not just an adhoc performance enhancer, but\nrather an essential part of TRIDENT since it allows us to not\nviolate our generative and inference mechanics.\nAdditional Details of Datasets\nminiImagenet (Vinyals et al. 2016) is a subset of ImageNet\n(Deng et al. 2009) for few-shot classiﬁcation. It contains 100\nclasses with 600 samples each. We follow the predominantly\nadopted settings of (Ravi and Larochelle 2017; Chen et al.\n2019) where we split the entire dataset into 64 classes for\ntraining, 16 for validation and 20 for testing. tieredImagenet\nis a larger subset of ImageNet (Deng et al. 2009) with 608\nclasses and 779, 165 total images, which are grouped into\n34 higher-level nodes in the ImageNet human-curated hier-\narchy. This set of nodes is partitioned into 20, 6, and 8 dis-\njoint sets of training, validation, and testing nodes, and the\ncorresponding classes form the respective meta-sets. CUB\n(Welinder et al. 2010) dataset has a total of 200 classes, split\ninto training, validation and test sets following (Chen et al.\n2019). We use this dataset to simulate the effect of a domain\nshift where the model is ﬁrst trained on a (5-way, 1 or 5-\nshot) conﬁguration of miniImagenet and then tested on the\ntest classes of CUB, as used in (Chen et al. 2019; Boudiaf\net al. 2020; Ziko et al. 2020; Long et al. 2018).\nImplementational Details\nLet α1 and α2 respectively denote the scaling factors of the\nMSE and cross-entropy terms in our objective functions LR\nand LC, as already deﬁned in Subsection 4.2. The terms\nα and β respectively denote the learning rates of the in-\nner and meta updates whereas B and n respectively denote\nTable 6: Hyperparameter values when training TRIDENT.\nminiImagenet\ntieredImagenet\nH.P.\n5-way, 1-shot\n5-way, 5-shot\n5-way, 1-shot\n5-way, 5-shot\nα1\n1e-2\n1e-2\n1e-2\n1e-2\nα2\n100\n100\n150\n150\nα\n1e-3\n1e-3\n1.5e-3\n1.7e-3\nβ\n1e-4\n1e-4\n1.5e-4\n1.7e-4\nB\n20\n20\n20\n20\nn\n5\n5\n5\n5\nthe number of sampled tasks and adaptation steps of the in-\nner-update of our end-to-end training process, as described\nin Algorithm 2. The hyperparameter values (H.P.) used\nfor training TRIDENT on miniImagenet and tieredImagenet\nare shown in Table 6. We apply the same hyperparame-\nters for the cross-domain testing scenario of miniImagenet\n→CUB used for training TRIDENT on miniImagenet, for\nthe given (N-way, K-shot) conﬁguration. Hyperparameters\nare kept ﬁxed throughout training, validation and testing for\na given conﬁguration. Adam (Kingma and Ba 2015) opti-\nmizer is used for inner and meta-updates. Finally, the query,\nkey and value extraction networks fq(, ; WQ), fk(.; WK),\nfv(.; WV ) of the AttFEX module only use Conv1×1(.)\nand not the LeakyReLU(0.2) activation function for\n(5-way, 1-shot) tasks, irrespective of the dataset. We ob-\nserved that utilizing BatchNorm (Ioffe and Szegedy 2015)\nin the decoder of zs (pθ2) to train TRIDENT on (5-way, 5-\nshot) tasks of miniImagenet and on (5-way, 1-shot) tasks of\ntieredImagenet leads to better scores and improved stabil-\nity during training. We used the ReLU activation function\ninstead of LeakyReLU(0.2) to carry out training on (5-\nway, 1-shot) tasks of tieredImagenet. Meta-learning objec-\ntives can lead to unstable optimization processes in prac-\ntice, especially when coupled with stochastic sampling in\nlatent spaces, as also previously observed in (Antreas Anto-\nniou, Harrison Edwards, and Amos J. Storkey 2019; Rusu\net al. 2019). For ease of experimentation we clip the meta-\ngradient norm at an absolute value of 1. TRIDENT con-\nverges in 82, 000 and 22, 500 epochs for (5-way, 1-shot)\nand (5-way, 5-shot) tasks of miniImagenet, respectively and\ntakes 67, 500 and 48, 000 epochs for convergence on (5-way,\n1-shot) and (5-way, 5-shot) tasks of tieredImagenet, respec-\ntively. This translates to an average training time of 110\nhours on an 11GB NVIDIA 1080Ti GPU. Note that we did\nnot employ any data augmentation, feature averaging or any\nother data apart from the corresponding training subset Dtr,\nduring training and evaluation.\nAdditional Calibration Results\nTo further examine the reliability and calibration of our\nmethod, we assess the ECE, MCE (Guo et al. 2017) and\nBrier scores (BRIER 1950) of TRIDENT on the challenging\ncross-domain scenario of miniImagenet →CUB for (5-way,\n5-shot) tasks. This table can be treated as an extension to\nTable 3 since here, we compare the calibration metrics for\nan additional scenario. When compared against other base-\nlines that report these metrics on the aforementioned sce-\n\n\nTable 7: Style: best and second best.\nMethods\nECE\nMCE\nBrier\nFeature Transfer(Chen et al. 2019)\n0.275\n0.646\n0.772\nBaseline(Chen et al. 2019)\n0.315\n0.537\n0.716\nMatching Nets(Vinyals et al. 2016)\n0.030\n0.079\n0.630\nProto Nets(Snell, Swersky, and Zemel 2017)\n0.009\n0.025\n0.604\nRelation Net(Sung et al. 2018)\n0.234\n0.554\n0.730\nDKT+Cos(Patacchiola et al. 2020)\n0.236\n0.426\n0.670\nBMAML(Yoon et al. 2018)\n0.048\n0.077\n0.619\nBMAML+Chaser(Yoon et al. 2018)\n0.066\n0.260\n0.639\nLogSoftGP(ML)(Galy-Fajou et al. 2020)\n0.220\n0.513\n0.709\nLogSoftGP(PL)(Galy-Fajou et al. 2020)\n0.022\n0.042\n0.564\nOVE(ML)(Snell and Zemel 2021)\n0.049\n0.066\n0.576\nOVE(PL)(Snell and Zemel 2021)\n0.020\n0.032\n0.556\nTRIDENT(Ours)\n0.009\n0.02\n0.276\nnario, TRIDENT proves to be the most calibrated with the\nbest reliability scores. This is shown in Table 7.\n"
}