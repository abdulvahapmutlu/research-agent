{
  "filename": "1703.00837v2.pdf",
  "num_pages": 11,
  "pages": [
    "Meta Networks\nTsendsuren Munkhdalai 1 Hong Yu 1\nAbstract\nNeural networks have been successfully applied\nin applications with a large amount of labeled\ndata. However, the task of rapid generalization\non new concepts with small training data while\npreserving performances on previously learned\nones still presents a signiﬁcant challenge to neu-\nral network models.\nIn this work, we intro-\nduce a novel meta learning method, Meta Net-\nworks (MetaNet), that learns a meta-level knowl-\nedge across tasks and shifts its inductive bi-\nases via fast parameterization for rapid gener-\nalization.\nWhen evaluated on Omniglot and\nMini-ImageNet benchmarks, our MetaNet mod-\nels achieve a near human-level performance and\noutperform the baseline approaches by up to\n6% accuracy. We demonstrate several appealing\nproperties of MetaNet relating to generalization\nand continual learning.\n1. Introduction\nDeep neural networks have shown great success in sev-\neral application domains when a large amount of labeled\ndata is available for training. However, the availability of\nsuch large training data has generally been a prerequisite\nin a majority of learning tasks. Furthermore, the standard\ndeep neural networks lack the ability to continuous learn-\ning or incrementally learning new concepts on the ﬂy, with-\nout forgetting or corrupting previously learned patterns. In\ncontrast, humans can rapidly learn and generalize from a\nfew examples of the same concept. Humans are also very\ngood at incremental (i.e. continuous) learning. These abil-\nities have been mostly explained by the meta learning (i.e.\nlearning to learn) process in the brain (Harlow, 1949).\nPrevious work on meta learning has formulated the prob-\nlem as two-level learning, a slow learning of a meta-level\n1University\nof\nMassachusetts,\nMA,\nUSA.\nCor-\nrespondence\nto:\nTsendsuren\nMunkhdalai\n<tsend-\nsuren.munkhdalai@umassmed.edu>.\nProceedings of the 34 th International Conference on Machine\nLearning, Sydney, Australia, 2017. JMLR: W&CP. Copyright\n2017 by the author(s).\nMemory\nSlow weights\nFast weights\nMeta weights\nSlow weights\nFast weights\nInput\nMeta info\nFast \nparameterization\nFast \nparameterization\nMemory access\nOutput\nMeta \nlearner\nBase \nlearner\nFigure 1. Overall architecture of Meta Networks.\nmodel performing across tasks and a rapid learning of a\nbase-level model acting within each task (Mitchell et al.,\n1993; Vilalta & Drissi, 2002). The goal of a meta-level\nlearner is to acquire generic knowledge of different tasks.\nThe knowledge can then be transferred to the base-level\nlearner to provide generalization in the context of a single\ntask. The base and meta-level models can be framed in a\nsingle learner (Schmidhuber, 1987) or in separate learners\n(Bengio et al., 1990; Hochreiter et al., 2001).\nIn this work we introduce a meta learning model called\nMetaNet (for Meta Networks) that supports meta-level con-\ntinual learning by allowing neural networks to learn and to\ngeneralize a new task or concept from a single example on\nthe ﬂy. The overall architecture of MetaNet is shown in\nFigure 1. MetaNet consists of two main learning compo-\nnents, a base learner and a meta learner, and is equipped\nwith an external memory. Learning occurs at two levels\nin separate spaces (i.e. meta space and task space). The\nbase learner performs in the input task space whereas the\nmeta learner operates in a task-agnostic meta space. By\noperating in the abstract meta space, the meta learner sup-\nports continual learning and performs meta knowledge ac-\nquisition across different tasks. Towards this end, the base\nlearner ﬁrst analyzes the input task. The base learner then\nprovides the meta learner with a feedback in the form of\nhigher order meta information to explain its own status in\nthe current task space.\nBased on the meta information,\narXiv:1703.00837v2  [cs.LG]  8 Jun 2017\n",
    "Meta Networks\nthe meta learner rapidly parameterizes both itself and the\nbase learner so that the MetaNet model can recognize the\nnew concepts of the input task. Speciﬁcally, the training\nweights of MetaNet evolve at different time-scales: stan-\ndard slow weights are updated through a learning algo-\nrithm (i.e. REINFORCE), task-level fast weights are up-\ndated within the scope of each task, and example-level fast\nweights are updated for a speciﬁc input example. Finally\nMetaNet equipped with external memory allows for rapid\nlearning and generalization.\nUnder the MetaNet framework, it is important to deﬁne the\ntypes of the meta information which can be obtained from\nthe learners. While other representations of meta informa-\ntion are also applicable, we use loss gradients as meta in-\nformation. MetaNet has two types of loss functions with\ndistinct objectives: a representation (i.e. embedding) loss\ndeﬁned for the good representation learner criteria and a\nmain (task) loss used for the input task objective.\nWe extensively studied the performance and the charac-\nteristics of MetaNet on one-shot supervised learning (SL)\nproblems under several different settings. Our proposed\nmethod not only improves the state-of-the-art results on\nthe standard benchmarks, but also shows some interesting\nproperties related to generalization and continual learning.\n2. Related Work\nOur work connects different threads of research in order to\nmodel neural architectures for rapid learning and general-\nization. Rapid learning and generalization refers to a one-\nshot learning scenario where a learner is introduced to a\nsequence of tasks, where each task entails multi-class clas-\nsiﬁcation with a single or few labeled example per class.\nA key challenge in this setting is that the classes or con-\ncepts vary across the tasks. Due to this, one-shot learning\nproblems have been widely addressed by generative mod-\nels and metric learning methods. One notable success is\nreported by a probabilistic programming approach (Lake\net al., 2015). They used speciﬁc knowledge of how pen\nstrokes are composed to produce characters of different al-\nphabets. Koch (2015) applied Siamese Networks to per-\nform one-shot classiﬁcation. Recently, Vinyals et al. (2016)\nuniﬁed the training and testing of a one-shot learner under\nthe same procedure and developed an end-to-end differen-\ntiable nearest neighbor method for one-shot learning. San-\ntoro et al. (2016) proposed a memory-based approach and\ntrained Neural Turing Machines (Graves et al., 2014) for\none-shot learning, although the meta-learner and the one-\nshot learner in this work are not separable explicitly. The\ntraining procedure used by Santoro et al. (2016) adapted the\nwork of Hochreiter et al. (2001) in which they use LSTMs\nas the meta-level model. More recently an LSTM-based\none-shot optimizer was proposed (Ravi & Larochell, 2017).\nBy taking in the loss, the gradient and the parameters of the\nbase learner, the meta optimizer was trained to update the\nparameters for one-shot classiﬁcation.\nA related line of work focuses on building meta opti-\nmizers (Hochreiter et al., 2001; Maclaurin et al., 2015;\nAndrychowicz et al., 2016; Li & Malik, 2017). As the main\ninterest here is to train an optimization algorithm within\nthe meta learning framework, these efforts have mainly fo-\ncused on tasks with large datasets. In contrast, with the ab-\nsence of large datasets, our experimental setup emphasizes\nthe difﬁculties of optimizing a neural network with a large\nnumber of parameters to generalize with limited examples\nof a new concept. Our work proposes a novel rapid param-\neterization approach by employing meta information. By\nfollowing the success of the previous work (Mitchell et al.,\n1993; Younger et al., 1999; Andrychowicz et al., 2016;\nRavi & Larochell, 2017), we study the meta information\npresent in the loss gradient of neural nets. Fast weights\nand utilizing one neural network to generate parameters\nfor another neural network have previously been studied\nseparately. Hinton & Plaut (1987) suggested the usage of\nfast weights for rapid learning. Ba et al. (2016) recently\nused fast weights to replace soft attention mechanism. Fast\nweights have also been used to implement recurrent nets\n(Schmidhuber, 1992; 1993a) and self-referential networks\n(Schmidhuber, 1987; 1993b). These usages of fast weights\nare well motivated by the fact that synapses have dynamics\nat many different time-scales (Greengard, 2001).\nThe approach proposed by Gomez & Schmidhuber (2005)\nis more closely related to our work. They used recurrent\nnets to generate fast weights for a single-layer network\ncontroller. De Brabandere et al. (2016) used one network\nto generate slow ﬁlter weights for a convolutional neural\nnet. More recently David Ha & Le (2017) generated slow\nweights for recurrent nets.\nOur MetaNet generates fast\nweights at two time-scales by operating in meta space. To\nintegrate the fast weights with the slow weights, we pro-\npose a novel layer augmentation approach.\nFinally, we note that our MetaNet equipped with an ex-\nternal memory can be seen as a memory augmented neu-\nral network (MANN). MANNs have shown promising re-\nsults on a range of tasks starting from small programming\nproblems (Graves et al., 2014) to large-scale language tasks\n(Weston et al., 2015; Sukhbaatar et al., 2015; Munkhdalai\n& Yu, 2017).\n3. Meta Networks\nMetaNet learns to fast parameterize underlying neural net-\nworks for rapid generalizations by processing a higher or-\nder meta information, resulting in a ﬂexible AI model that\ncan adapt to a sequence of tasks with possibly distinct in-\n",
    "Meta Networks\nAlgorithm 1 MetaNet for one-shot supervised learning\nRequire: Support set {x′\ni, y′\ni}N\ni=1 and Training set {xi, yi}L\ni=1\nRequire: Base learner b, Dynamic representation learning func-\ntion u, Fast weight generation functions m and d, and Slow\nweights θ = {W, Q, Z, G}\nRequire: Layer augmentation scheme\n1: Sample T examples from support set\n2: for i = 1, T do\n3:\nLi ←lossemb(u(Q, x′\ni), y′\ni)\n4:\n∇i ←∇QLi\n5: end for\n6: Q∗= d(G, {∇}T\ni=1)\n7: for i = 1, N do\n8:\nLi ←losstask(b(W, x′\ni), y′\ni)\n9:\n∇i ←∇W Li\n10:\nW ∗\ni ←m(Z, ∇i)\n11:\nStore W ∗\ni in ith position of memory M\n12:\nr′\ni = u(Q, Q∗, x′\ni)\n13:\nStore r′\ni in ith position of index memory R\n14: end for\n15: Ltrain = 0\n16: for i = 1, L do\n17:\nri = u(Q, Q∗, xi)\n18:\nai = attention(R, ri)\n19:\nW ∗\ni = softmax(ai)⊤M\n20:\nLtrain\n←\nLtrain\n+ losstask(b(W, W ∗\ni , xi), yi)\n{Alternatively the base learner can take as input ri instead\nof xi}\n21: end for\n22: Update θ using ∇θLtrain\nput and output distributions. The model consists of two\nmain learning modules (Figure 1). The meta learner is re-\nsponsible for fast weight generation by operating across\ntasks while the base learner performs within each task by\ncapturing the task objective. The generated fast weights\nare integrated into both base learner and meta learner to\nshift the inductive bias of the learners. We propose a novel\nlayer augmentation method to integrate the standard slow\nweights and the task or example speciﬁc fast weights in a\nneural net.\nTo train MetaNet, we adapt a task formulation procedure by\nVinyals et al. (2016). We form a sequence of tasks, where\neach task consists of a support set {x′\ni, y′\ni}N\ni=1 and a train-\ning set {xi, yi}L\ni=1. The class labels are consistent for both\nsupport and training sets of the same task, but vary across\ndistinct tasks. Overall the training of MetaNet consists of\nthree main procedures: acquisition of meta information,\ngeneration of fast weights and optimization of slow weights,\nexecuted collectively by the base and the meta learner. The\ntraining of MetaNet is described in Algorithm 1.\nTo test the model for one-shot SL, we sample another se-\nquence of tasks from a test dataset with unseen classes.\nThen the model is deployed to classify test examples based\non its support set. We assume that we have class labels for\nthe support set during both training and testing. Note that\nFast weight layer\nSlow weight layer\n+\nFast weight layer\nSlow weight layer\nReLU\nReLU\n+\nSoftmax\nFast weight layer\nSlow weight layer\n+\nReLU\nReLU\nInput\nOutput\nFigure 2. A layer augmented MLP\nin one-shot learning setup, the support set contains only\nsingle example per class and thus it is cheap to obtain.\n3.1. Meta Learner\nThe meta learner consists of a dynamic representation\nlearning function u and fast weight generation functions\nm and d. The function u has a representation learning ob-\njective and constructs embeddings of inputs in each task\nspace by using task-level fast weights. The weight gen-\neration functions m and d are responsible for processing\nthe meta information and generating the example and task-\nlevel fast weights.\nMore speciﬁcally, the function m learns the mapping from\nthe loss gradient {∇i}N\ni=1, derived from the base learner b,\nto fast weights {W ∗\ni }N\ni=1:\nW ∗\ni = m(Z, ∇i)\n(1)\nwhere m is a neural network with parameter Z. The fast\nweights are then stored in a memory M = {W ∗\ni }N\ni=1. The\nmemory M is indexed with task dependent embeddings\nR = {r′\ni}N\ni=1 of the support examples {x′\ni}N\ni=1, obtained\nby the dynamic representation learning function u.\nThe representation learning function u is a neural net pa-\nrameterized by slow weights Q and task-level fast weights\nQ∗. It uses the representation loss lossemb to capture a rep-\nresentation learning objective and to obtain the gradients as\nmeta information. We generate the fast weights Q∗on a per\ntask basis as follows:\nLi = lossemb(u(Q, x′\ni), y′\ni)\n(2)\n∇i = ∇QLi\n(3)\nQ∗= d(G, {∇}T\ni=1)\n(4)\n",
    "Meta Networks\nwhere d denotes a neural net parameterized by G, that ac-\ncepts variable sized input. First, we sample T examples\n(T ≤N) {x′\ni, y′\ni}T\ni=1 from the support set and obtain the\nloss gradient as meta information. Then d observes the gra-\ndient corresponding to each sampled example and summa-\nrizes into the task speciﬁc parameters. We use LSTM for\nd although the order of inputs to d does not matter. Alter-\nnatively we can take summation or average of the gradients\nand use a MLP. However, in our preliminary experiment we\nobserved that the latter results in a poor convergence.\nOnce the fast weights are generated, the task dependent in-\nput representations {r′\ni}N\ni=1 are computed as:\nr′\ni = u(Q, Q∗, x′\ni)\n(5)\nwhere the parameters Q and Q∗are integrated using the\nlayer augmentation method described in Section 3.3.\nThe loss, lossemb does not need to be the same as the main\ntask loss losstask. However, it should be able to capture\na representation learning objective. We use cross-entropy\nloss when the support set has only a single example per\nclass. When there are more than one examples per class\navailable, contrastive loss (Chopra et al., 2005) is a natural\nchoice for lossemb since both positive and negative samples\ncan be formed. In this case, we randomly draw T number\nof pairs to observe the gradients and the loss is\nLi = lossemb(u(Q, x′\n1,i), u(Q, x′\n2,i), li)\n(6)\nwhere li is auxiliary label:\nli =\n(\n1,\nif y′\n1,i = y′\n2,i\n0,\notherwise\n(7)\nOnce the parameters are stored in the memory M and the\nmemory index R is constructed, the meta learner parame-\nterizes the base learner with the fast weights W ∗\ni . First it\nembeds the input xi in the task space by using the dynamic\nrepresentation learning network (i.e. Equation 5) and then\nreads the memory with soft attention:\nai = attention(R, ri)\n(8)\nW ∗\ni = norm(ai)⊤M\n(9)\nwhere attention calculates similarity between the memory\nindex and the input embedding and we use cosine similar-\nity as attention and norm is a normalization function, for\nwhich we use softmax.\n3.2. Base Learner\nThe base learner, denoted as b, is a function or a neural\nnet that estimates the main task objective via a task loss\nlosstask. However, unlike standard neural nets, b is param-\neterized by slow weights W and example-level fast weights\nW ∗. The slow weights are updated via a learning algorithm\nduring training whereas the fast weights are generated by\nthe meta learner for every input.\nThe base learner uses a representation of meta information\nobtained by using a support set, to provide the meta learner\nwith feedbacks about the new input task. The meta infor-\nmation is derived from the base learner in form of the loss\ngradient information:\nLi = losstask(b(W, x′\ni), y′\ni)\n(10)\n∇i = ∇W Li\n(11)\nHere Li is the loss for support examples {x′\ni, y′\ni}N\ni=1. N is\nthe number of support examples in the task set (typically\na single instance per class in the one-shot learning setup).\n∇i is the loss gradient with respect to parameters W and is\nour meta information. Note that the loss function losstask\nis generic and can take any form, such as a cumulative re-\nward in reinforcement learning. For our one-shot classiﬁ-\ncation setup we use cross-entropy loss. The meta learner\ntakes in the gradient information ∇i and generates the fast\nparameters W ∗as in Equation 1.\nAssuming that the fast weights W ∗\ni for input xi are deﬁned,\nthe base learner performs the one-shot classiﬁcation as:\nP(ˆyi|xi, W, W ∗\ni ) = b(W, W ∗\ni , xi)\n(12)\nwhere ˆyi is predicted output and {xi}L\ni=1 is an input drawn\nfrom the training set {xi, yi}L\ni=1 for the current task. Alter-\nnatively the base learner can take as input the task speciﬁc\nrepresentations {ri}L\ni=1 produced by the dynamic represen-\ntation learning network, effectively reducing the number of\nMetaNet parameters and leveraging shared representations.\nIn this case, the base learner is forced to operate in the dy-\nnamic task space constructed by u instead of building new\nrepresentations from the raw inputs {xi}L\ni=1.\nDuring training, given output labels {yi}L\ni=1, we minimize\nthe cross-entropy loss for one-shot SL. The training param-\neters of MetaNet θ consists of the slow weights W and Q\nand the meta weights Z and G (i.e. θ = {W, Q, Z, G})\nand jointly updated via a training algorithm such as back-\npropagation to minimize the task loss losstask (Equation\n12).\nIn a similar way, as deﬁned in the Equation 2-4, we can also\nparameterize the base learner with task-level fast weights.\nAn ablation experiment on different variation of MetaNet\nis reported in Section 4.\n3.3. Layer Augmentation\nA slow weight layer in the base learner is extended with\nits corresponding fast weights for rapid generalization. An\n",
    "Meta Networks\nTable 1. One-shot accuracy on Omniglot previous split\nModel\n5-way\n10-way\n15-way\n20-way\nPixel kNN (Kaiser et al., 2017)\n41.7\n-\n-\n26.7\nSiamese Net (Koch, 2015)\n97.3\n-\n-\n88.1\nMANN (Santoro et al., 2016)\n82.8\n-\n-\n-\nMatching Nets (Vinyals et al., 2016)\n98.1\n-\n-\n93.8\nNeural Statistician (Edwards & Storkey, 2017)\n98.1\n-\n-\n93.2\nSiamese Net with Memory (Kaiser et al., 2017)\n98.4\n-\n-\n95.0\nMetaNet-\n98.4\n98.32\n96.68\n96.13\nMetaNet\n98.95\n98.67\n97.11\n97.0\nMetaNet+\n98.45\n97.05\n96.48\n95.08\nexample of the layer augmentation approach applied to a\nMLP is shown in Figure 2. The input of an augmented\nlayer is ﬁrst transformed by both slow and fast weights and\nthen passed through a non-linearity (i.e. ReLU) resulting\nin two separate activation vectors. Finally the activation\nvectors are aggregated by an element-wise vector addition.\nFor the last softmax layer, we ﬁrst aggregate two trans-\nformed inputs and then normalize for classiﬁcation output.\nIntuitively, the fast and slow weights in the layer aug-\nmented neural net can be seen as feature detectors oper-\nating in two distinct numeric domains. The application of\nthe non-linearity maps them into the same domain, which\nis [0, ∞) in the case of ReLU so that the activations can be\naggregated and processed further. Our aggregation func-\ntion here is element-wise sum.\nAlthough it is possible to deﬁne the base learner with\nonly fast weights, in our preliminary experiment we found\nthat the integration of both slow and fast weights with the\nlayer augmentation approach is essential in convergence\nof MetaNet models. A MetaNet model relying on a base\nleaner with only fast weights were failed to converge and\nthe best performance of this model was reported to be as\nequal as that of a constant classiﬁer that assigns the same\nlabel to every input.\n4. Results\nWe carried out one-shot classiﬁcation experiments on three\ndatasets: Omniglot, Mini-ImageNet and MNIST. The Om-\nniglot dataset consists of images across 1623 classes with\nonly 20 images per class, from 50 different alphabets (Lake\net al., 2015). It also comes with a standard split of 30 train-\ning and 20 evaluation alphabets. Following (Santoro et al.,\n2016), we augmented the training set through 90, 180 and\n270 degrees rotations. The images are resized to 28 x 28\npixels for computational efﬁciency. For the experiment on\nMini-ImageNet data, we evaluated on the same class sub-\nset provided by Ravi & Larochell (2017). MNIST images\nwere used as out-of-domain data. The training details are\ndescribed in Appendix A.\n4.1. One-shot Learning Test\nIn this section we will report four groups of benchmark\nexperiments:\nOmniglot previous split, Mini-ImageNet,\nMNIST as out-of-domain data and Omniglot standard split.\n4.1.1. OMNIGLOT PREVIOUS SPLIT\nFollowing the previous setup Vinyals et al. (2016), we split\nthe Omniglot classes into 1200 and 423 classes for train-\ning and testing. We performed 5, 10, 15 and 20-way one-\nshot classiﬁcation and compared our performance against\nthe state-of-the-art results. We also studied three variations\nof MetaNet as an ablation experiment in order to show how\nfast parameterization affects the network dynamics.\nIn Table 1, we compared the performance of our models\nwith all published models (as baselines). The ﬁrst group\nof methods are the previously published models. The next\ngroup is MetaNet variations. MetaNet is the main archi-\ntecture described in Section 3. MetaNet- is a variant with-\nout task-level fast weights Q∗in the embedding function\nu whereas MetaNet+ has additional task-level weights for\nthe base learner in addition to W ∗. Our MetaNet model\nimproves the previous best results by 0.5% to 2% accu-\nracy. As the number of classes increases (from 5-way to\n20-way classiﬁcation), overall the performance of the one-\nshot learners decreases. MetaNet’s performance drop is\nrelatively small (around 2%) while the drop for the other\nmodels ranges from 3% to 15%. As a result, our model\nshows an absolute improvement of 2% on 20-way one-shot\ntask.\nComparing different MetaNet variations, the additional\ntask-level weights in the base learner (MetaNet+) did not\nseem to help and in fact had a negative effect on perfor-\nmance. MetaNet- however performed surprisingly well but\nstill falls behind the MetaNet model as it lacks the dynamic\nrepresentation learning function. This performance gap in-\ncreases when we test them in out-of-the domain setting\n(Appendix B).\n",
    "Meta Networks\nTable 2. One-shot accuracy on Mini-ImageNet test set\nModel\n5-way\nFine-tuning (Ravi & Larochell, 2017)\n28.86 ± 0.54\nkNN (Ravi & Larochell, 2017)\n41.08 ± 0.70\nMatching Nets (Vinyals et al., 2016)\n43.56 ± 0.84\nMetaLearner LSTM (Ravi & Larochell, 2017)\n43.44 ± 0.77\nMetaNet\n49.21 ± 0.96\n4.1.2. MINI-IMAGENET\nThe training, dev and testing sets of 64, 16, and 20 Im-\nageNet classes (with 600 examples per class) were pro-\nvided by Ravi & Larochell (2017). By following Ravi &\nLarochell (2017), we sampled 15 examples per class for\nevaluation.\nBy using the dev set, we set an evaluation\ncheckpoint where only if the model performance exceeds\nthe previous best result on random 400 trials produced from\nthe dev set, we apply the model to another 400 trials ran-\ndomly produced from the testing set and report the average\naccuracy.\nIn Table 2, we present the results of the 5-way one-shot\nevaluation. MetaNet improved the previous result by up to\n6% accuracy and obtained the best result.1\n4.1.3. OMNIGLOT STANDARD SPLIT\nOmniglot data comes with a standard split of 30 train-\ning alphabets with 964 classes and 20 evaluation alphabets\nwith 659 classes. We trained and tested only the standard\nMetaNet model in this setup. In order to best match the\nevaluation protocol of Lake et al. (2015), we form 400 tasks\n(trials) from the evaluation classes to test the model.\nIn Table 3, we listed the MetaNet results along with the pre-\nvious models and human performance. Our MetaNet out-\nperformed the human performance by a slight margin, but\nunderperformed the probabilistic programming approach.\nHowever, the performance gap is rather small between\nthese top three baselines. In addition while the probabilistic\nprogramming performs slightly better than MetaNet, our\nmodel does not rely on any extra prior knowledge about\nhow characters and strokes are composed. Comparing the\nresults on two Omniglot splits in Tables 1 and 3, MetaNet\nshowed decreasing performances on the standard split. The\nlater setup seems to be slightly difﬁcult as the number of\nclasses in the training set is less (1200 vs 964) and test\nclasses are bigger (423 vs 659).\n1Our code and data will be made available at: https://\nbitbucket.org/tsendeemts/metanet\n4.2. Generalization Test\nWe conducted a set of experiments to test the generalization\nof MetaNet from multiple aspects. The ﬁrst experiment\ntests whether a MetaNet model trained on an N-way one-\nshot task could generalize to another K-way task (where\nN ̸= K) without actually training on the second task.\nThe second experiment is to test if a meta learner trained\nfor rapid parameterization of a base learner btrain could\nparameterize another base learner beval during evaluation.\nThe last experimental setup examines whether MetaNet\nsupports meta-level continual learning.\n4.2.1. N-WAY TRAINING AND K-WAY TESTING\nIn this experiment, MetaNet is trained on N-way one-shot\nclassiﬁcation task and then tested on K-way one-shot tasks.\nThe number of training and test classes are varied (i.e. N ̸=\nK). To handle this, we inserted a softmax layer into the\nbase learner during evaluation and then augmented it with\nthe fast weights generated by the meta learner. If the meta\nlearner is generic enough, it should be able to parameterize\nthe new softmax layer on the ﬂy. The new layer weights\nremained ﬁxed since no parameter update was performed\nfor this layer. The K-way test tasks were formed from the\n423 unseen classes in the test set.\nThe MetaNet models were trained on one of 5, 10, 15 and\n20-way one-shot tasks and evaluated on the rest. In Table\n4, we summarized the results. As a comparison we also\nincluded some results from Table 1, which reports accu-\nracy of N-way train and test setting. The MetaNet model\ntrained on 5-way tasks obtained 93.07% of 20-way test ac-\ncuracy which is still a closer match to Matching Network\nand higher than Siamese Net trained 20-way tasks. An in-\nteresting ﬁnding is that when N is smaller than K, i.e. the\nmodel is trained on easier tasks than test ones, we observe\na decreasing performance. Conversely the models trained\non harder tasks (i.e. N > K) achieved increasing perfor-\nmances when tested on the easier tasks and the performance\nis even higher than the ones that were applied to the tasks\nwith the same level difﬁculty (i.e. N = K). For exam-\nple, the model skilled on 20-way classiﬁcation improved\nthe 5-way one-shot baseline by 0.6% showing a ceiling\nperformance in this setting. We also conducted a prelim-\ninary experiment on more extreme test-time classiﬁcation.\nMetaNet trained on 10-way task achieved around 65% on\n100-way one-shot classiﬁcation task.\nThis ﬂexibility in MetaNet is crucial because one-shot\nlearning usually involves an online concept identiﬁcation\nscenario. Furthermore we can empirically obtain a perfor-\nmance lower or upper bound. Particularly the test perfor-\nmance obtained on the tasks with the same level difﬁculty\nthat the model was skilled on can be used as a performance\nlower or an upper bound depending on a scenario under\n",
    "Meta Networks\nTable 3. One-shot accuracy on Omniglot standard split\nModel\n5-way\n10-way\n15-way\n20-way\nHuman performance (Lake et al., 2015)\n-\n-\n-\n95.5\nPixel kNN (Lake et al., 2013)\n-\n-\n-\n21.7\nAfﬁne model (Lake et al., 2013)\n-\n-\n-\n81.8\nDeep Boltzmann Machines (Lake et al., 2013)\n-\n-\n-\n62.0\nHierarchial Bayesian Program Learning (Lake et al., 2015)\n-\n-\n-\n96.7\nSiamese Net (Koch, 2015)\n-\n-\n-\n92.0\nMetaNet\n98.45\n97.32\n96.4\n95.92\nwhich the model will be deployed in the future. For ex-\nample, for the MetaNet model that will deployed under the\nN > K scenario, we can obtain the performance lower\nbound by testing it on the N = K tasks.\n4.2.2. RAPID PARAMETERIZATION OF FIXED WEIGHT\nBASE LEARNER\nWe replaced the entire base learner with a new CNN dur-\ning evaluation. The slow weights of this network remained\nﬁxed. The fast weights are generated by the meta learner\nthat is trained to parameterize the old base learner and used\nto augmented the ﬁxed slow weights.\nWe tested a small and a large CNN for the base learner.\nThe small CNN has 32 ﬁlters and the large CNN has 128\nﬁlters. In Figure 3, the test performances of these CNNs are\ncompared. The base learner (target CNN) optimized along\nwithin the model performed better than the ﬁxed weight\nCNNs. The performance difference between these mod-\nels is large in earlier training iterations. However, as the\nmeta learner sees more one-shot learning trials, the test ac-\ncuracies of the base learners converge. This results show\nthat MetaNet effectively learns to parameterize a neural net\nwith ﬁxed weights.\n4.2.3. META-LEVEL CONTINUAL LEARNING\nMetaNet operates in two spaces: input problem space and\nmeta (gradient) space. If the meta space is problem in-\ndependent, MetaNet should support meta-level continual\nlearning or life-long learning. This experiment tests this\nin the case of the loss gradient.\nTable 4. Accuracy of MetaNet trained on N-way and tested on K-\nway one-shot tasks\nTest\nTrain\n5-way\n10-way\n15-way\n20-way\n5-way\n98.95\n96.4\n93.6\n93.07\n10-way\n99.25\n96.87\n96.95\n96.21\n15-way\n99.35\n98.17\n97.11\n96.36\n20-way\n99.55\n98.87\n97.41\n97.0\n40\n60\n80\n100\n400\n6800\n13200\n19600\n26000\nAccuracy \nThe number of trials \nTarget CNN\nSmall CNN\nBig CNN\nFigure 3. Comparison of the test performances of the base learn-\ners on Omniglot 5-way classiﬁcation.\nFollowing the previous work on catastrophic forgetting in\nneural networks (Srivastava et al., 2013; Goodfellow et al.,\n2014; Kirkpatrick et al., 2016), we formulated two prob-\nlems in a sequential manner. We ﬁrst trained and tested\nthe model on the Omniglot sets and then we switched and\ncontinued training on the MNIST data. After training on\na number of MNIST one-shot tasks, we re-evaluated the\nmodel on the same Omniglot test set and compare per-\nformance. A decrease in performance here indicates that\nthe meta weights Z and G of the neural nets m and d are\nprone to catastrophic forgetting and the model therefore\ndoes not support continual learning. On the other hand,\nan increase in performance indicates that MetaNet supports\nreverse transfer learning and continual learning.\nWe allocated separate parameters for the weights W and Q\nwhen we switched the problems so the only meta weights\nwere updated. We used two three-layer MLPs with 64 hid-\nden units as the embedding function and the base learner.\nThe MNIST image and classes were augmented by ran-\ndomly permuting the pixels. We created 50 different ran-\ndom shufﬂes and thus the training set for the second one-\nshot problem consisted of 500 classes. We conducted mul-\ntiple runs and increased the MNIST training trials by mul-\ntiples of 400 (i.e. 400, 800, 1200...) in each run giving\n",
    "Meta Networks\n-2\n-1\n0\n1\n2\n400\n2800\n5200\n7600\nThe number of MNIST trials \nAcc difference\nFigure 4. The difference between the two Omniglot test accura-\ncies obtained before and after training on MNIST task.\nmore time for MetaNet to adapt its meta weights on the\nsecond problem so that it may forget the knowledge about\nOmniglot. Each run was repeated ﬁve times and we report\nthe average statistics. For every run, the network and the\noptimizer were reinitialized and the training started from\nscratch.\nIn Figure 4, we plotted the accuracy difference between two\nOmniglot test performances obtained before and after train-\ning on the MNIST task. The performance improvement (y-\naxis) after training on the MNIST tasks ranges from -1.7%\nto 1.24% depending on the training time (x-axis). The pos-\nitive values indicate that the training on the second problem\nautomatically improves the performance of the earlier task\nexhibiting the reverse transfer property. Therefore, we can\nconclude that MetaNet successfully performs reverse trans-\nfer. At the same time, it is skilled on MNIST one-shot clas-\nsiﬁcation. The MNIST training accuracy reaches over 72%\nafter 2400 MNIST trials. However, reverse transfer hap-\npens only up to a certain point in MNIST training (2400\ntrials). After that, the meta weights start to forget the Om-\nniglot information. As a result from 2800 trials onwards,\nthe Omniglot test accuracy drops. Nevertheless even after\n7600 MNIST trials, at which point the MNIST training ac-\ncuracy reached over 90%, the Omniglot performance drop\nwas only 1.7%.\n5. Discussion and Future Work\nOne-shot learning in combination with a meta learning\nframework can be a useful approach to address certain neu-\nral network drawbacks related to rapid generalization with\nsmall data and continual learning. We present a novel meta\nlearning method, MetaNet, that performs a generic knowl-\nedge acquisition in a meta space and shifts the parameters\nand inductive biases of underlying neural networks via fast\nparameterization for the rapid generalization.\nUnder the MetaNet framework, an important consideration\nis the type of higher order meta information that can be ex-\ntracted as a feedback from the model when operating on a\nnew task. One desirable property here is that the meta in-\nformation should be generic and problem independent. It\nshould also be expressive enough to explain the model set-\nting in the current task space. We explored the use of loss\ngradients as meta information in this work. As shown in\nthe results, using the gradients as meta information seems\nto be a promising direction. MetaNet obtains state-of-the\nart results on several one-shot SL benchmarks and leads to\na very ﬂexible AI model. For instance, in MetaNet we can\nalternate between different softmax layers on the ﬂy dur-\ning test. It supports continual learning up to a certain point.\nWe observed that neural nets with ﬁxed slow weights can\nperform well for new task inputs when augmented with the\nfast weights. When the slow weights are updated during\ntraining, it learns domain biases resulting in even better per-\nformance on identiﬁcation of new concepts within the same\ndomain. However, one could expect a higher performance\nfrom the ﬁxed weight network when aiming for one-shot\ngeneralization across distant domains.\nAn interesting future direction would be in exploring a new\ntype of meta information that is more robust and expres-\nsive, and in developing synaptic weights that are capable of\nmaintaining such higher order information. One could take\ninspiration from the meta learning process in the brain and\nask whether the brain operates on some kind of higher or-\nder information to generalize across tasks and acquire new\nskills.\nThe rapid parameterization approach presented here has\nbeen shown to be an effective alternative to the direct op-\ntimization methods that learn to update network param-\neters for one-shot generalization.\nHowever, a problem\nthis approach poses is the integration of slow and fast\nweights.\nAs a solution to this, we presented a simple\nlayer augmentation method. Although the layer augmenta-\ntion worked reasonably well, this method becomes difﬁcult\nwhen a neural net has many types of parameters operat-\ning in multiple different time-scales. For example, a single\nbase learner equipped with three types of weights (slow,\nexample-speciﬁc, and task-level weights) integrated under\nthe layer augmentation paradigm could not perform as well\nas a simpler one. Therefore, a potential extension would be\nto train MetaNet so it can discover its own augmentation\nschema for efﬁciency.\nMetaNet can readily be applied to parameterize policies\nin reinforcement learning and imitation learning, leading\nto an agent with one-shot and meta learning capabilities.\nMetaNet based on recurrent networks as underlying learn-\ners could lead to useful applications in sequence modeling\nand language understanding tasks.\n",
    "Meta Networks\nAcknowledgements\nWe would like to thank the anonymous reviewers and our\ncolleagues, Jesse Lingeman, Abhyuday Jagannatha and\nJohn Lalor for their insightful comments and suggestions\non improving the manuscript. This work was supported in\npart by the grant HL125089 from the National Institutes of\nHealth and by the grant 1I01HX001457-01 supported by\nthe Health Services Research & Development of the US\nDepartment of Veterans Affairs Investigator Initiated Re-\nsearch. Any opinions, ﬁndings and conclusions or recom-\nmendations expressed in this material are those of the au-\nthors and do not necessarily reﬂect those of the sponsor.\nReferences\nAndrychowicz, Marcin, Denil, Misha, Gomez, Sergio,\nHoffman, Matthew W, Pfau, David, Schaul, Tom, and\nde Freitas, Nando. Learning to learn by gradient descent\nby gradient descent. In Advances in Neural Information\nProcessing Systems, pp. 3981–3989, 2016.\nBa, Jimmy, Hinton, Geoffrey E, Mnih, Volodymyr, Leibo,\nJoel Z, and Ionescu, Catalin. Using fast weights to attend\nto the recent past. In Advances In Neural Information\nProcessing Systems, pp. 4331–4339, 2016.\nBengio, Yoshua, Bengio, Samy, and Cloutier, Jocelyn.\nLearning a synaptic learning rule.\nUniversit´e de\nMontr´eal, D´epartement d’informatique et de recherche\nop´erationnelle, 1990.\nChopra, Sumit, Hadsell, Raia, and LeCun, Yann. Learning\na similarity metric discriminatively, with application to\nface veriﬁcation. In Computer Vision and Pattern Recog-\nnition, 2005. CVPR 2005. IEEE Computer Society Con-\nference on, volume 1, pp. 539–546. IEEE, 2005.\nDavid Ha, Andrew Dai and Le, Quoc V. Hypernetworks.\nIn ICLR 2017, 2017.\nDe Brabandere, Bert, Jia, Xu, Tuytelaars, Tinne, and\nVan Gool, Luc. Dynamic ﬁlter networks. In Neural In-\nformation Processing Systems (NIPS), 2016.\nEdwards, Harrison and Storkey, Amos. Towards a neural\nstatistician. In ICLR 2017, 2017.\nGomez, Faustino and Schmidhuber, J¨urgen. Evolving mod-\nular fast-weight networks for control. In International\nConference on Artiﬁcial Neural Networks, pp. 383–389.\nSpringer, 2005.\nGoodfellow, Ian J, Mirza, Mehdi, Xiao, Da, Courville,\nAaron, and Bengio, Yoshua. An empirical investigation\nof catastrophic forgetting in gradient-based neural net-\nworks. In ICLR 2014, 2014.\nGraves, Alex, Wayne, Greg, and Danihelka, Ivo. Neural\nturing machines. arXiv preprint arXiv:1410.5401, 2014.\nGreengard, Paul. The neurobiology of slow synaptic trans-\nmission. Science, 294(5544):1024–1030, 2001.\nHarlow, Harry F. The formation of learning sets. Psycho-\nlogical review, 56(1):51, 1949.\nHinton, Geoffrey E and Plaut, David C. Using fast weights\nto deblur old memories.\nIn Proceedings of the ninth\nannual conference of the Cognitive Science Society, pp.\n177–186, 1987.\nHochreiter, Sepp, Younger, A Steven, and Conwell, Pe-\nter R. Learning to learn using gradient descent. In Inter-\nnational Conference on Artiﬁcial Neural Networks, pp.\n87–94. Springer, 2001.\nKaiser, Lukasz, Nachum, Oﬁr, Roy, Aurko, and Bengio,\nSamy. Learning to remember rare events. In ICLR 2017,\n2017.\nKirkpatrick, James, Pascanu, Razvan, Rabinowitz, Neil,\nVeness, Joel, Desjardins, Guillaume, Rusu, Andrei A,\nMilan, Kieran, Quan, John, Ramalho, Tiago, Grabska-\nBarwinska, Agnieszka, et al.\nOvercoming catas-\ntrophic forgetting in neural networks.\narXiv preprint\narXiv:1612.00796, 2016.\nKoch, Gregory.\nSiamese neural networks for one-shot\nimage recognition. PhD thesis, University of Toronto,\n2015.\nLake, Brenden M, Salakhutdinov, Ruslan R, and Tenen-\nbaum, Josh. One-shot learning by inverting a composi-\ntional causal process. In Advances in neural information\nprocessing systems, pp. 2526–2534, 2013.\nLake, Brenden M, Salakhutdinov, Ruslan, and Tenenbaum,\nJoshua B. Human-level concept learning through prob-\nabilistic program induction. Science, 350(6266):1332–\n1338, 2015.\nLi, Ke and Malik, Jitendra. Learning to optimize. In ICLR\n2017, 2017.\nMaclaurin, Dougal, Duvenaud, David, and Adams, Ryan.\nGradient-based hyperparameter optimization through re-\nversible learning. In International Conference on Ma-\nchine Learning, pp. 2113–2122, 2015.\nMitchell, Tom M, Thrun, Sebastian B, et al. Explanation-\nbased neural network learning for robot control.\nAd-\nvances in neural information processing systems, pp.\n287–287, 1993.\n",
    "Meta Networks\nMunkhdalai, Tsendsuren and Yu, Hong.\nNeural seman-\ntic encoders.\nIn Proceedings of the 15th Conference\nof the European Chapter of the Association for Compu-\ntational Linguistics: Volume 1, Long Papers, pp. 397–\n407, Valencia, Spain, April 2017. Association for Com-\nputational Linguistics. URL http://www.aclweb.\norg/anthology/E17-1038.\nRavi, Sachin and Larochell, Hugo. Optimization as a model\nfor few-shot learning. In ICLR 2017, 2017.\nSantoro, Adam, Bartunov, Sergey, Botvinick, Matthew,\nWierstra, Daan, and Lillicrap, Timothy. Meta-learning\nwith memory-augmented neural networks. In Proceed-\nings of The 33rd International Conference on Machine\nLearning, pp. 1842–1850, 2016.\nSchmidhuber, J.\nReducing the Ratio Between Learning\nComplexity and Number of Time Varying Variables\nin Fully Recurrent Nets, pp. 460–463.\nSpringer\nLondon,\nLondon,\n1993a.\nISBN\n978-1-4471-\n2063-6.\ndoi:\n10.1007/978-1-4471-2063-6 110.\nURL\nhttp://dx.doi.org/10.1007/\n978-1-4471-2063-6_110.\nSchmidhuber, J.\nA neural network that embeds its own\nmeta-levels. In IEEE International Conference on Neu-\nral Networks, pp. 407–412 vol.1, 1993b. doi: 10.1109/\nICNN.1993.298591.\nSchmidhuber, J¨urgen.\nEvolutionary principles in self-\nreferential learning. PhD thesis, Technical University\nof Munich, 1987.\nSchmidhuber, J¨urgen.\nLearning to control fast-weight\nmemories:\nAn alternative to dynamic recurrent net-\nworks. Neural Computation, 4(1):131–139, 1992.\nSrivastava, Rupesh K, Masci, Jonathan, Kazerounian,\nSohrob, Gomez, Faustino, and Schmidhuber, J¨urgen.\nCompete to compute. In Advances in neural informa-\ntion processing systems, pp. 2310–2318, 2013.\nSukhbaatar, Sainbayar, Weston, Jason, Fergus, Rob, et al.\nEnd-to-end memory networks. In Advances in neural\ninformation processing systems, pp. 2440–2448, 2015.\nVilalta, Ricardo and Drissi, Youssef. A perspective view\nand survey of meta-learning. Artiﬁcial Intelligence Re-\nview, 18(2):77–95, 2002.\nVinyals, Oriol, Blundell, Charles, Lillicrap, Tim, Wierstra,\nDaan, et al. Matching networks for one shot learning. In\nAdvances in Neural Information Processing Systems, pp.\n3630–3638, 2016.\nWeston, Jason, Chopra, Sumit, and Bordes, Antoine. Mem-\nory networks. In In Proceedings Of The International\nConference on Representation Learning (ICLR 2015),\nSan Diego, California, May 2015. In press.\nYounger, A Steven, Conwell, Peter R, and Cotter, Neil E.\nFixed-weight on-line learning.\nIEEE Transactions on\nNeural Networks, 10(2):272–283, 1999.\n",
    "Meta Networks\nA. Training Details\nTo train and test MetaNet on one-shot learning, we adapted\nthe training procedure introduced by Vinyals et al. (2016).\nFirst, we split the data into training and test sets consisting\nof two disjoint classes. We then formulate a series of tasks\n(trials) from the training set. Each task has a support set of\nN classes with one image per, resulting in an N-way one-\nshot classiﬁcation problem. In addition to the support set,\nwe also include L number of labeled examples in each task\nset to update the parameters θ during training. For testing,\nwe follow the same procedure to form a set of test tasks\nfrom the disjoint classes. However, now MetaNet assigns\nclass labels to L examples based only on the labeled sup-\nport set of each test task.\nFor the one-shot benchmarks on the Omniglot dataset, we\nused a CNN with 64 ﬁlters as the base learner b. This CNN\nhas 5 convolutional layers, each of which is a 3 x 3 con-\nvolution with 64 ﬁlters, followed by a ReLU non-linearity,\na 2 x 2 max-pooling layer, a fully connected (FC) layer,\nand a softmax layer. Another CNN with the same architec-\nture is used to deﬁne the dynamic representation learning\nfunction u, from which we take the output of the FC layer\nas the task dependent representation r. We trained a simi-\nlar CNNs architecture with 32 ﬁlters for the experiment on\nMini-ImageNet. However for computational efﬁciency as\nwell as to demonstrate the ﬂexibility of MetaNet, the last\nthree layers of these CNN models were augmented by fast\nweights. For the networks d and m, we used a single-layer\nLSTM with 20 hidden units and a three-layer MLP with 20\nhidden units and ReLU non-linearity. As in Andrychow-\nicz et al. (2016), the parameters G and Z of d and m are\nshared across the coordinates of the gradients ∇and the\ngradients are normalized using the same preprocessing rule\n(with p = 7). The MetaNet parameters θ are optimized\nwith ADAM. The initial learning rate was set to 10−3. The\nmodel parameters θ were randomly initialized from the uni-\nform distribution over [-0.1, 0.1).\nB. MNIST as Out-Of-Domain Data\nWe treated MNIST images as a separate domain data. Par-\nticularly a model is trained on the Omniglot training set and\nevaluated on the MNIST test set in 10-way one-shot learn-\ning setup. We hypothesize that models with a high dynamic\nshould perform well on this task.\nIn Figure 5, we plotted the results of this experiment.\nMetaNet- achieved 71.6% accuracy which was 0.6% and\n3.2% lower than the other variants with fast weights. This\nis not surprising since MetaNet without dynamic represen-\ntation learning function lacks an ability to adapt its pa-\nrameters to MNIST image representations. The standard\nMetaNet model achieved 74.8% and MetaNet+ obtained\n72.3%. Matching Net (Vinyals et al., 2016) reported 72.0%\naccuracy in this setup. Again we did not observe improve-\nment with MetaNet+ model here. The best result was re-\ncently reported by using a generative model, Neural Statis-\ntician, that extends variational autoencoder to summarize\ninput set (Edwards & Storkey, 2017).\n64\n66\n68\n70\n72\n74\n76\n78\n80\nAccuracy \nMNIST as out-of-domain data evaluation \nSiamese Net\nMatching Net\nMetaNet-\nMetaNet\nMetaNet+\nNeural Statistician\nFigure 5. MNIST 10-way one-shot classiﬁcation results.\n"
  ],
  "full_text": "Meta Networks\nTsendsuren Munkhdalai 1 Hong Yu 1\nAbstract\nNeural networks have been successfully applied\nin applications with a large amount of labeled\ndata. However, the task of rapid generalization\non new concepts with small training data while\npreserving performances on previously learned\nones still presents a signiﬁcant challenge to neu-\nral network models.\nIn this work, we intro-\nduce a novel meta learning method, Meta Net-\nworks (MetaNet), that learns a meta-level knowl-\nedge across tasks and shifts its inductive bi-\nases via fast parameterization for rapid gener-\nalization.\nWhen evaluated on Omniglot and\nMini-ImageNet benchmarks, our MetaNet mod-\nels achieve a near human-level performance and\noutperform the baseline approaches by up to\n6% accuracy. We demonstrate several appealing\nproperties of MetaNet relating to generalization\nand continual learning.\n1. Introduction\nDeep neural networks have shown great success in sev-\neral application domains when a large amount of labeled\ndata is available for training. However, the availability of\nsuch large training data has generally been a prerequisite\nin a majority of learning tasks. Furthermore, the standard\ndeep neural networks lack the ability to continuous learn-\ning or incrementally learning new concepts on the ﬂy, with-\nout forgetting or corrupting previously learned patterns. In\ncontrast, humans can rapidly learn and generalize from a\nfew examples of the same concept. Humans are also very\ngood at incremental (i.e. continuous) learning. These abil-\nities have been mostly explained by the meta learning (i.e.\nlearning to learn) process in the brain (Harlow, 1949).\nPrevious work on meta learning has formulated the prob-\nlem as two-level learning, a slow learning of a meta-level\n1University\nof\nMassachusetts,\nMA,\nUSA.\nCor-\nrespondence\nto:\nTsendsuren\nMunkhdalai\n<tsend-\nsuren.munkhdalai@umassmed.edu>.\nProceedings of the 34 th International Conference on Machine\nLearning, Sydney, Australia, 2017. JMLR: W&CP. Copyright\n2017 by the author(s).\nMemory\nSlow weights\nFast weights\nMeta weights\nSlow weights\nFast weights\nInput\nMeta info\nFast \nparameterization\nFast \nparameterization\nMemory access\nOutput\nMeta \nlearner\nBase \nlearner\nFigure 1. Overall architecture of Meta Networks.\nmodel performing across tasks and a rapid learning of a\nbase-level model acting within each task (Mitchell et al.,\n1993; Vilalta & Drissi, 2002). The goal of a meta-level\nlearner is to acquire generic knowledge of different tasks.\nThe knowledge can then be transferred to the base-level\nlearner to provide generalization in the context of a single\ntask. The base and meta-level models can be framed in a\nsingle learner (Schmidhuber, 1987) or in separate learners\n(Bengio et al., 1990; Hochreiter et al., 2001).\nIn this work we introduce a meta learning model called\nMetaNet (for Meta Networks) that supports meta-level con-\ntinual learning by allowing neural networks to learn and to\ngeneralize a new task or concept from a single example on\nthe ﬂy. The overall architecture of MetaNet is shown in\nFigure 1. MetaNet consists of two main learning compo-\nnents, a base learner and a meta learner, and is equipped\nwith an external memory. Learning occurs at two levels\nin separate spaces (i.e. meta space and task space). The\nbase learner performs in the input task space whereas the\nmeta learner operates in a task-agnostic meta space. By\noperating in the abstract meta space, the meta learner sup-\nports continual learning and performs meta knowledge ac-\nquisition across different tasks. Towards this end, the base\nlearner ﬁrst analyzes the input task. The base learner then\nprovides the meta learner with a feedback in the form of\nhigher order meta information to explain its own status in\nthe current task space.\nBased on the meta information,\narXiv:1703.00837v2  [cs.LG]  8 Jun 2017\n\n\nMeta Networks\nthe meta learner rapidly parameterizes both itself and the\nbase learner so that the MetaNet model can recognize the\nnew concepts of the input task. Speciﬁcally, the training\nweights of MetaNet evolve at different time-scales: stan-\ndard slow weights are updated through a learning algo-\nrithm (i.e. REINFORCE), task-level fast weights are up-\ndated within the scope of each task, and example-level fast\nweights are updated for a speciﬁc input example. Finally\nMetaNet equipped with external memory allows for rapid\nlearning and generalization.\nUnder the MetaNet framework, it is important to deﬁne the\ntypes of the meta information which can be obtained from\nthe learners. While other representations of meta informa-\ntion are also applicable, we use loss gradients as meta in-\nformation. MetaNet has two types of loss functions with\ndistinct objectives: a representation (i.e. embedding) loss\ndeﬁned for the good representation learner criteria and a\nmain (task) loss used for the input task objective.\nWe extensively studied the performance and the charac-\nteristics of MetaNet on one-shot supervised learning (SL)\nproblems under several different settings. Our proposed\nmethod not only improves the state-of-the-art results on\nthe standard benchmarks, but also shows some interesting\nproperties related to generalization and continual learning.\n2. Related Work\nOur work connects different threads of research in order to\nmodel neural architectures for rapid learning and general-\nization. Rapid learning and generalization refers to a one-\nshot learning scenario where a learner is introduced to a\nsequence of tasks, where each task entails multi-class clas-\nsiﬁcation with a single or few labeled example per class.\nA key challenge in this setting is that the classes or con-\ncepts vary across the tasks. Due to this, one-shot learning\nproblems have been widely addressed by generative mod-\nels and metric learning methods. One notable success is\nreported by a probabilistic programming approach (Lake\net al., 2015). They used speciﬁc knowledge of how pen\nstrokes are composed to produce characters of different al-\nphabets. Koch (2015) applied Siamese Networks to per-\nform one-shot classiﬁcation. Recently, Vinyals et al. (2016)\nuniﬁed the training and testing of a one-shot learner under\nthe same procedure and developed an end-to-end differen-\ntiable nearest neighbor method for one-shot learning. San-\ntoro et al. (2016) proposed a memory-based approach and\ntrained Neural Turing Machines (Graves et al., 2014) for\none-shot learning, although the meta-learner and the one-\nshot learner in this work are not separable explicitly. The\ntraining procedure used by Santoro et al. (2016) adapted the\nwork of Hochreiter et al. (2001) in which they use LSTMs\nas the meta-level model. More recently an LSTM-based\none-shot optimizer was proposed (Ravi & Larochell, 2017).\nBy taking in the loss, the gradient and the parameters of the\nbase learner, the meta optimizer was trained to update the\nparameters for one-shot classiﬁcation.\nA related line of work focuses on building meta opti-\nmizers (Hochreiter et al., 2001; Maclaurin et al., 2015;\nAndrychowicz et al., 2016; Li & Malik, 2017). As the main\ninterest here is to train an optimization algorithm within\nthe meta learning framework, these efforts have mainly fo-\ncused on tasks with large datasets. In contrast, with the ab-\nsence of large datasets, our experimental setup emphasizes\nthe difﬁculties of optimizing a neural network with a large\nnumber of parameters to generalize with limited examples\nof a new concept. Our work proposes a novel rapid param-\neterization approach by employing meta information. By\nfollowing the success of the previous work (Mitchell et al.,\n1993; Younger et al., 1999; Andrychowicz et al., 2016;\nRavi & Larochell, 2017), we study the meta information\npresent in the loss gradient of neural nets. Fast weights\nand utilizing one neural network to generate parameters\nfor another neural network have previously been studied\nseparately. Hinton & Plaut (1987) suggested the usage of\nfast weights for rapid learning. Ba et al. (2016) recently\nused fast weights to replace soft attention mechanism. Fast\nweights have also been used to implement recurrent nets\n(Schmidhuber, 1992; 1993a) and self-referential networks\n(Schmidhuber, 1987; 1993b). These usages of fast weights\nare well motivated by the fact that synapses have dynamics\nat many different time-scales (Greengard, 2001).\nThe approach proposed by Gomez & Schmidhuber (2005)\nis more closely related to our work. They used recurrent\nnets to generate fast weights for a single-layer network\ncontroller. De Brabandere et al. (2016) used one network\nto generate slow ﬁlter weights for a convolutional neural\nnet. More recently David Ha & Le (2017) generated slow\nweights for recurrent nets.\nOur MetaNet generates fast\nweights at two time-scales by operating in meta space. To\nintegrate the fast weights with the slow weights, we pro-\npose a novel layer augmentation approach.\nFinally, we note that our MetaNet equipped with an ex-\nternal memory can be seen as a memory augmented neu-\nral network (MANN). MANNs have shown promising re-\nsults on a range of tasks starting from small programming\nproblems (Graves et al., 2014) to large-scale language tasks\n(Weston et al., 2015; Sukhbaatar et al., 2015; Munkhdalai\n& Yu, 2017).\n3. Meta Networks\nMetaNet learns to fast parameterize underlying neural net-\nworks for rapid generalizations by processing a higher or-\nder meta information, resulting in a ﬂexible AI model that\ncan adapt to a sequence of tasks with possibly distinct in-\n\n\nMeta Networks\nAlgorithm 1 MetaNet for one-shot supervised learning\nRequire: Support set {x′\ni, y′\ni}N\ni=1 and Training set {xi, yi}L\ni=1\nRequire: Base learner b, Dynamic representation learning func-\ntion u, Fast weight generation functions m and d, and Slow\nweights θ = {W, Q, Z, G}\nRequire: Layer augmentation scheme\n1: Sample T examples from support set\n2: for i = 1, T do\n3:\nLi ←lossemb(u(Q, x′\ni), y′\ni)\n4:\n∇i ←∇QLi\n5: end for\n6: Q∗= d(G, {∇}T\ni=1)\n7: for i = 1, N do\n8:\nLi ←losstask(b(W, x′\ni), y′\ni)\n9:\n∇i ←∇W Li\n10:\nW ∗\ni ←m(Z, ∇i)\n11:\nStore W ∗\ni in ith position of memory M\n12:\nr′\ni = u(Q, Q∗, x′\ni)\n13:\nStore r′\ni in ith position of index memory R\n14: end for\n15: Ltrain = 0\n16: for i = 1, L do\n17:\nri = u(Q, Q∗, xi)\n18:\nai = attention(R, ri)\n19:\nW ∗\ni = softmax(ai)⊤M\n20:\nLtrain\n←\nLtrain\n+ losstask(b(W, W ∗\ni , xi), yi)\n{Alternatively the base learner can take as input ri instead\nof xi}\n21: end for\n22: Update θ using ∇θLtrain\nput and output distributions. The model consists of two\nmain learning modules (Figure 1). The meta learner is re-\nsponsible for fast weight generation by operating across\ntasks while the base learner performs within each task by\ncapturing the task objective. The generated fast weights\nare integrated into both base learner and meta learner to\nshift the inductive bias of the learners. We propose a novel\nlayer augmentation method to integrate the standard slow\nweights and the task or example speciﬁc fast weights in a\nneural net.\nTo train MetaNet, we adapt a task formulation procedure by\nVinyals et al. (2016). We form a sequence of tasks, where\neach task consists of a support set {x′\ni, y′\ni}N\ni=1 and a train-\ning set {xi, yi}L\ni=1. The class labels are consistent for both\nsupport and training sets of the same task, but vary across\ndistinct tasks. Overall the training of MetaNet consists of\nthree main procedures: acquisition of meta information,\ngeneration of fast weights and optimization of slow weights,\nexecuted collectively by the base and the meta learner. The\ntraining of MetaNet is described in Algorithm 1.\nTo test the model for one-shot SL, we sample another se-\nquence of tasks from a test dataset with unseen classes.\nThen the model is deployed to classify test examples based\non its support set. We assume that we have class labels for\nthe support set during both training and testing. Note that\nFast weight layer\nSlow weight layer\n+\nFast weight layer\nSlow weight layer\nReLU\nReLU\n+\nSoftmax\nFast weight layer\nSlow weight layer\n+\nReLU\nReLU\nInput\nOutput\nFigure 2. A layer augmented MLP\nin one-shot learning setup, the support set contains only\nsingle example per class and thus it is cheap to obtain.\n3.1. Meta Learner\nThe meta learner consists of a dynamic representation\nlearning function u and fast weight generation functions\nm and d. The function u has a representation learning ob-\njective and constructs embeddings of inputs in each task\nspace by using task-level fast weights. The weight gen-\neration functions m and d are responsible for processing\nthe meta information and generating the example and task-\nlevel fast weights.\nMore speciﬁcally, the function m learns the mapping from\nthe loss gradient {∇i}N\ni=1, derived from the base learner b,\nto fast weights {W ∗\ni }N\ni=1:\nW ∗\ni = m(Z, ∇i)\n(1)\nwhere m is a neural network with parameter Z. The fast\nweights are then stored in a memory M = {W ∗\ni }N\ni=1. The\nmemory M is indexed with task dependent embeddings\nR = {r′\ni}N\ni=1 of the support examples {x′\ni}N\ni=1, obtained\nby the dynamic representation learning function u.\nThe representation learning function u is a neural net pa-\nrameterized by slow weights Q and task-level fast weights\nQ∗. It uses the representation loss lossemb to capture a rep-\nresentation learning objective and to obtain the gradients as\nmeta information. We generate the fast weights Q∗on a per\ntask basis as follows:\nLi = lossemb(u(Q, x′\ni), y′\ni)\n(2)\n∇i = ∇QLi\n(3)\nQ∗= d(G, {∇}T\ni=1)\n(4)\n\n\nMeta Networks\nwhere d denotes a neural net parameterized by G, that ac-\ncepts variable sized input. First, we sample T examples\n(T ≤N) {x′\ni, y′\ni}T\ni=1 from the support set and obtain the\nloss gradient as meta information. Then d observes the gra-\ndient corresponding to each sampled example and summa-\nrizes into the task speciﬁc parameters. We use LSTM for\nd although the order of inputs to d does not matter. Alter-\nnatively we can take summation or average of the gradients\nand use a MLP. However, in our preliminary experiment we\nobserved that the latter results in a poor convergence.\nOnce the fast weights are generated, the task dependent in-\nput representations {r′\ni}N\ni=1 are computed as:\nr′\ni = u(Q, Q∗, x′\ni)\n(5)\nwhere the parameters Q and Q∗are integrated using the\nlayer augmentation method described in Section 3.3.\nThe loss, lossemb does not need to be the same as the main\ntask loss losstask. However, it should be able to capture\na representation learning objective. We use cross-entropy\nloss when the support set has only a single example per\nclass. When there are more than one examples per class\navailable, contrastive loss (Chopra et al., 2005) is a natural\nchoice for lossemb since both positive and negative samples\ncan be formed. In this case, we randomly draw T number\nof pairs to observe the gradients and the loss is\nLi = lossemb(u(Q, x′\n1,i), u(Q, x′\n2,i), li)\n(6)\nwhere li is auxiliary label:\nli =\n(\n1,\nif y′\n1,i = y′\n2,i\n0,\notherwise\n(7)\nOnce the parameters are stored in the memory M and the\nmemory index R is constructed, the meta learner parame-\nterizes the base learner with the fast weights W ∗\ni . First it\nembeds the input xi in the task space by using the dynamic\nrepresentation learning network (i.e. Equation 5) and then\nreads the memory with soft attention:\nai = attention(R, ri)\n(8)\nW ∗\ni = norm(ai)⊤M\n(9)\nwhere attention calculates similarity between the memory\nindex and the input embedding and we use cosine similar-\nity as attention and norm is a normalization function, for\nwhich we use softmax.\n3.2. Base Learner\nThe base learner, denoted as b, is a function or a neural\nnet that estimates the main task objective via a task loss\nlosstask. However, unlike standard neural nets, b is param-\neterized by slow weights W and example-level fast weights\nW ∗. The slow weights are updated via a learning algorithm\nduring training whereas the fast weights are generated by\nthe meta learner for every input.\nThe base learner uses a representation of meta information\nobtained by using a support set, to provide the meta learner\nwith feedbacks about the new input task. The meta infor-\nmation is derived from the base learner in form of the loss\ngradient information:\nLi = losstask(b(W, x′\ni), y′\ni)\n(10)\n∇i = ∇W Li\n(11)\nHere Li is the loss for support examples {x′\ni, y′\ni}N\ni=1. N is\nthe number of support examples in the task set (typically\na single instance per class in the one-shot learning setup).\n∇i is the loss gradient with respect to parameters W and is\nour meta information. Note that the loss function losstask\nis generic and can take any form, such as a cumulative re-\nward in reinforcement learning. For our one-shot classiﬁ-\ncation setup we use cross-entropy loss. The meta learner\ntakes in the gradient information ∇i and generates the fast\nparameters W ∗as in Equation 1.\nAssuming that the fast weights W ∗\ni for input xi are deﬁned,\nthe base learner performs the one-shot classiﬁcation as:\nP(ˆyi|xi, W, W ∗\ni ) = b(W, W ∗\ni , xi)\n(12)\nwhere ˆyi is predicted output and {xi}L\ni=1 is an input drawn\nfrom the training set {xi, yi}L\ni=1 for the current task. Alter-\nnatively the base learner can take as input the task speciﬁc\nrepresentations {ri}L\ni=1 produced by the dynamic represen-\ntation learning network, effectively reducing the number of\nMetaNet parameters and leveraging shared representations.\nIn this case, the base learner is forced to operate in the dy-\nnamic task space constructed by u instead of building new\nrepresentations from the raw inputs {xi}L\ni=1.\nDuring training, given output labels {yi}L\ni=1, we minimize\nthe cross-entropy loss for one-shot SL. The training param-\neters of MetaNet θ consists of the slow weights W and Q\nand the meta weights Z and G (i.e. θ = {W, Q, Z, G})\nand jointly updated via a training algorithm such as back-\npropagation to minimize the task loss losstask (Equation\n12).\nIn a similar way, as deﬁned in the Equation 2-4, we can also\nparameterize the base learner with task-level fast weights.\nAn ablation experiment on different variation of MetaNet\nis reported in Section 4.\n3.3. Layer Augmentation\nA slow weight layer in the base learner is extended with\nits corresponding fast weights for rapid generalization. An\n\n\nMeta Networks\nTable 1. One-shot accuracy on Omniglot previous split\nModel\n5-way\n10-way\n15-way\n20-way\nPixel kNN (Kaiser et al., 2017)\n41.7\n-\n-\n26.7\nSiamese Net (Koch, 2015)\n97.3\n-\n-\n88.1\nMANN (Santoro et al., 2016)\n82.8\n-\n-\n-\nMatching Nets (Vinyals et al., 2016)\n98.1\n-\n-\n93.8\nNeural Statistician (Edwards & Storkey, 2017)\n98.1\n-\n-\n93.2\nSiamese Net with Memory (Kaiser et al., 2017)\n98.4\n-\n-\n95.0\nMetaNet-\n98.4\n98.32\n96.68\n96.13\nMetaNet\n98.95\n98.67\n97.11\n97.0\nMetaNet+\n98.45\n97.05\n96.48\n95.08\nexample of the layer augmentation approach applied to a\nMLP is shown in Figure 2. The input of an augmented\nlayer is ﬁrst transformed by both slow and fast weights and\nthen passed through a non-linearity (i.e. ReLU) resulting\nin two separate activation vectors. Finally the activation\nvectors are aggregated by an element-wise vector addition.\nFor the last softmax layer, we ﬁrst aggregate two trans-\nformed inputs and then normalize for classiﬁcation output.\nIntuitively, the fast and slow weights in the layer aug-\nmented neural net can be seen as feature detectors oper-\nating in two distinct numeric domains. The application of\nthe non-linearity maps them into the same domain, which\nis [0, ∞) in the case of ReLU so that the activations can be\naggregated and processed further. Our aggregation func-\ntion here is element-wise sum.\nAlthough it is possible to deﬁne the base learner with\nonly fast weights, in our preliminary experiment we found\nthat the integration of both slow and fast weights with the\nlayer augmentation approach is essential in convergence\nof MetaNet models. A MetaNet model relying on a base\nleaner with only fast weights were failed to converge and\nthe best performance of this model was reported to be as\nequal as that of a constant classiﬁer that assigns the same\nlabel to every input.\n4. Results\nWe carried out one-shot classiﬁcation experiments on three\ndatasets: Omniglot, Mini-ImageNet and MNIST. The Om-\nniglot dataset consists of images across 1623 classes with\nonly 20 images per class, from 50 different alphabets (Lake\net al., 2015). It also comes with a standard split of 30 train-\ning and 20 evaluation alphabets. Following (Santoro et al.,\n2016), we augmented the training set through 90, 180 and\n270 degrees rotations. The images are resized to 28 x 28\npixels for computational efﬁciency. For the experiment on\nMini-ImageNet data, we evaluated on the same class sub-\nset provided by Ravi & Larochell (2017). MNIST images\nwere used as out-of-domain data. The training details are\ndescribed in Appendix A.\n4.1. One-shot Learning Test\nIn this section we will report four groups of benchmark\nexperiments:\nOmniglot previous split, Mini-ImageNet,\nMNIST as out-of-domain data and Omniglot standard split.\n4.1.1. OMNIGLOT PREVIOUS SPLIT\nFollowing the previous setup Vinyals et al. (2016), we split\nthe Omniglot classes into 1200 and 423 classes for train-\ning and testing. We performed 5, 10, 15 and 20-way one-\nshot classiﬁcation and compared our performance against\nthe state-of-the-art results. We also studied three variations\nof MetaNet as an ablation experiment in order to show how\nfast parameterization affects the network dynamics.\nIn Table 1, we compared the performance of our models\nwith all published models (as baselines). The ﬁrst group\nof methods are the previously published models. The next\ngroup is MetaNet variations. MetaNet is the main archi-\ntecture described in Section 3. MetaNet- is a variant with-\nout task-level fast weights Q∗in the embedding function\nu whereas MetaNet+ has additional task-level weights for\nthe base learner in addition to W ∗. Our MetaNet model\nimproves the previous best results by 0.5% to 2% accu-\nracy. As the number of classes increases (from 5-way to\n20-way classiﬁcation), overall the performance of the one-\nshot learners decreases. MetaNet’s performance drop is\nrelatively small (around 2%) while the drop for the other\nmodels ranges from 3% to 15%. As a result, our model\nshows an absolute improvement of 2% on 20-way one-shot\ntask.\nComparing different MetaNet variations, the additional\ntask-level weights in the base learner (MetaNet+) did not\nseem to help and in fact had a negative effect on perfor-\nmance. MetaNet- however performed surprisingly well but\nstill falls behind the MetaNet model as it lacks the dynamic\nrepresentation learning function. This performance gap in-\ncreases when we test them in out-of-the domain setting\n(Appendix B).\n\n\nMeta Networks\nTable 2. One-shot accuracy on Mini-ImageNet test set\nModel\n5-way\nFine-tuning (Ravi & Larochell, 2017)\n28.86 ± 0.54\nkNN (Ravi & Larochell, 2017)\n41.08 ± 0.70\nMatching Nets (Vinyals et al., 2016)\n43.56 ± 0.84\nMetaLearner LSTM (Ravi & Larochell, 2017)\n43.44 ± 0.77\nMetaNet\n49.21 ± 0.96\n4.1.2. MINI-IMAGENET\nThe training, dev and testing sets of 64, 16, and 20 Im-\nageNet classes (with 600 examples per class) were pro-\nvided by Ravi & Larochell (2017). By following Ravi &\nLarochell (2017), we sampled 15 examples per class for\nevaluation.\nBy using the dev set, we set an evaluation\ncheckpoint where only if the model performance exceeds\nthe previous best result on random 400 trials produced from\nthe dev set, we apply the model to another 400 trials ran-\ndomly produced from the testing set and report the average\naccuracy.\nIn Table 2, we present the results of the 5-way one-shot\nevaluation. MetaNet improved the previous result by up to\n6% accuracy and obtained the best result.1\n4.1.3. OMNIGLOT STANDARD SPLIT\nOmniglot data comes with a standard split of 30 train-\ning alphabets with 964 classes and 20 evaluation alphabets\nwith 659 classes. We trained and tested only the standard\nMetaNet model in this setup. In order to best match the\nevaluation protocol of Lake et al. (2015), we form 400 tasks\n(trials) from the evaluation classes to test the model.\nIn Table 3, we listed the MetaNet results along with the pre-\nvious models and human performance. Our MetaNet out-\nperformed the human performance by a slight margin, but\nunderperformed the probabilistic programming approach.\nHowever, the performance gap is rather small between\nthese top three baselines. In addition while the probabilistic\nprogramming performs slightly better than MetaNet, our\nmodel does not rely on any extra prior knowledge about\nhow characters and strokes are composed. Comparing the\nresults on two Omniglot splits in Tables 1 and 3, MetaNet\nshowed decreasing performances on the standard split. The\nlater setup seems to be slightly difﬁcult as the number of\nclasses in the training set is less (1200 vs 964) and test\nclasses are bigger (423 vs 659).\n1Our code and data will be made available at: https://\nbitbucket.org/tsendeemts/metanet\n4.2. Generalization Test\nWe conducted a set of experiments to test the generalization\nof MetaNet from multiple aspects. The ﬁrst experiment\ntests whether a MetaNet model trained on an N-way one-\nshot task could generalize to another K-way task (where\nN ̸= K) without actually training on the second task.\nThe second experiment is to test if a meta learner trained\nfor rapid parameterization of a base learner btrain could\nparameterize another base learner beval during evaluation.\nThe last experimental setup examines whether MetaNet\nsupports meta-level continual learning.\n4.2.1. N-WAY TRAINING AND K-WAY TESTING\nIn this experiment, MetaNet is trained on N-way one-shot\nclassiﬁcation task and then tested on K-way one-shot tasks.\nThe number of training and test classes are varied (i.e. N ̸=\nK). To handle this, we inserted a softmax layer into the\nbase learner during evaluation and then augmented it with\nthe fast weights generated by the meta learner. If the meta\nlearner is generic enough, it should be able to parameterize\nthe new softmax layer on the ﬂy. The new layer weights\nremained ﬁxed since no parameter update was performed\nfor this layer. The K-way test tasks were formed from the\n423 unseen classes in the test set.\nThe MetaNet models were trained on one of 5, 10, 15 and\n20-way one-shot tasks and evaluated on the rest. In Table\n4, we summarized the results. As a comparison we also\nincluded some results from Table 1, which reports accu-\nracy of N-way train and test setting. The MetaNet model\ntrained on 5-way tasks obtained 93.07% of 20-way test ac-\ncuracy which is still a closer match to Matching Network\nand higher than Siamese Net trained 20-way tasks. An in-\nteresting ﬁnding is that when N is smaller than K, i.e. the\nmodel is trained on easier tasks than test ones, we observe\na decreasing performance. Conversely the models trained\non harder tasks (i.e. N > K) achieved increasing perfor-\nmances when tested on the easier tasks and the performance\nis even higher than the ones that were applied to the tasks\nwith the same level difﬁculty (i.e. N = K). For exam-\nple, the model skilled on 20-way classiﬁcation improved\nthe 5-way one-shot baseline by 0.6% showing a ceiling\nperformance in this setting. We also conducted a prelim-\ninary experiment on more extreme test-time classiﬁcation.\nMetaNet trained on 10-way task achieved around 65% on\n100-way one-shot classiﬁcation task.\nThis ﬂexibility in MetaNet is crucial because one-shot\nlearning usually involves an online concept identiﬁcation\nscenario. Furthermore we can empirically obtain a perfor-\nmance lower or upper bound. Particularly the test perfor-\nmance obtained on the tasks with the same level difﬁculty\nthat the model was skilled on can be used as a performance\nlower or an upper bound depending on a scenario under\n\n\nMeta Networks\nTable 3. One-shot accuracy on Omniglot standard split\nModel\n5-way\n10-way\n15-way\n20-way\nHuman performance (Lake et al., 2015)\n-\n-\n-\n95.5\nPixel kNN (Lake et al., 2013)\n-\n-\n-\n21.7\nAfﬁne model (Lake et al., 2013)\n-\n-\n-\n81.8\nDeep Boltzmann Machines (Lake et al., 2013)\n-\n-\n-\n62.0\nHierarchial Bayesian Program Learning (Lake et al., 2015)\n-\n-\n-\n96.7\nSiamese Net (Koch, 2015)\n-\n-\n-\n92.0\nMetaNet\n98.45\n97.32\n96.4\n95.92\nwhich the model will be deployed in the future. For ex-\nample, for the MetaNet model that will deployed under the\nN > K scenario, we can obtain the performance lower\nbound by testing it on the N = K tasks.\n4.2.2. RAPID PARAMETERIZATION OF FIXED WEIGHT\nBASE LEARNER\nWe replaced the entire base learner with a new CNN dur-\ning evaluation. The slow weights of this network remained\nﬁxed. The fast weights are generated by the meta learner\nthat is trained to parameterize the old base learner and used\nto augmented the ﬁxed slow weights.\nWe tested a small and a large CNN for the base learner.\nThe small CNN has 32 ﬁlters and the large CNN has 128\nﬁlters. In Figure 3, the test performances of these CNNs are\ncompared. The base learner (target CNN) optimized along\nwithin the model performed better than the ﬁxed weight\nCNNs. The performance difference between these mod-\nels is large in earlier training iterations. However, as the\nmeta learner sees more one-shot learning trials, the test ac-\ncuracies of the base learners converge. This results show\nthat MetaNet effectively learns to parameterize a neural net\nwith ﬁxed weights.\n4.2.3. META-LEVEL CONTINUAL LEARNING\nMetaNet operates in two spaces: input problem space and\nmeta (gradient) space. If the meta space is problem in-\ndependent, MetaNet should support meta-level continual\nlearning or life-long learning. This experiment tests this\nin the case of the loss gradient.\nTable 4. Accuracy of MetaNet trained on N-way and tested on K-\nway one-shot tasks\nTest\nTrain\n5-way\n10-way\n15-way\n20-way\n5-way\n98.95\n96.4\n93.6\n93.07\n10-way\n99.25\n96.87\n96.95\n96.21\n15-way\n99.35\n98.17\n97.11\n96.36\n20-way\n99.55\n98.87\n97.41\n97.0\n40\n60\n80\n100\n400\n6800\n13200\n19600\n26000\nAccuracy \nThe number of trials \nTarget CNN\nSmall CNN\nBig CNN\nFigure 3. Comparison of the test performances of the base learn-\ners on Omniglot 5-way classiﬁcation.\nFollowing the previous work on catastrophic forgetting in\nneural networks (Srivastava et al., 2013; Goodfellow et al.,\n2014; Kirkpatrick et al., 2016), we formulated two prob-\nlems in a sequential manner. We ﬁrst trained and tested\nthe model on the Omniglot sets and then we switched and\ncontinued training on the MNIST data. After training on\na number of MNIST one-shot tasks, we re-evaluated the\nmodel on the same Omniglot test set and compare per-\nformance. A decrease in performance here indicates that\nthe meta weights Z and G of the neural nets m and d are\nprone to catastrophic forgetting and the model therefore\ndoes not support continual learning. On the other hand,\nan increase in performance indicates that MetaNet supports\nreverse transfer learning and continual learning.\nWe allocated separate parameters for the weights W and Q\nwhen we switched the problems so the only meta weights\nwere updated. We used two three-layer MLPs with 64 hid-\nden units as the embedding function and the base learner.\nThe MNIST image and classes were augmented by ran-\ndomly permuting the pixels. We created 50 different ran-\ndom shufﬂes and thus the training set for the second one-\nshot problem consisted of 500 classes. We conducted mul-\ntiple runs and increased the MNIST training trials by mul-\ntiples of 400 (i.e. 400, 800, 1200...) in each run giving\n\n\nMeta Networks\n-2\n-1\n0\n1\n2\n400\n2800\n5200\n7600\nThe number of MNIST trials \nAcc difference\nFigure 4. The difference between the two Omniglot test accura-\ncies obtained before and after training on MNIST task.\nmore time for MetaNet to adapt its meta weights on the\nsecond problem so that it may forget the knowledge about\nOmniglot. Each run was repeated ﬁve times and we report\nthe average statistics. For every run, the network and the\noptimizer were reinitialized and the training started from\nscratch.\nIn Figure 4, we plotted the accuracy difference between two\nOmniglot test performances obtained before and after train-\ning on the MNIST task. The performance improvement (y-\naxis) after training on the MNIST tasks ranges from -1.7%\nto 1.24% depending on the training time (x-axis). The pos-\nitive values indicate that the training on the second problem\nautomatically improves the performance of the earlier task\nexhibiting the reverse transfer property. Therefore, we can\nconclude that MetaNet successfully performs reverse trans-\nfer. At the same time, it is skilled on MNIST one-shot clas-\nsiﬁcation. The MNIST training accuracy reaches over 72%\nafter 2400 MNIST trials. However, reverse transfer hap-\npens only up to a certain point in MNIST training (2400\ntrials). After that, the meta weights start to forget the Om-\nniglot information. As a result from 2800 trials onwards,\nthe Omniglot test accuracy drops. Nevertheless even after\n7600 MNIST trials, at which point the MNIST training ac-\ncuracy reached over 90%, the Omniglot performance drop\nwas only 1.7%.\n5. Discussion and Future Work\nOne-shot learning in combination with a meta learning\nframework can be a useful approach to address certain neu-\nral network drawbacks related to rapid generalization with\nsmall data and continual learning. We present a novel meta\nlearning method, MetaNet, that performs a generic knowl-\nedge acquisition in a meta space and shifts the parameters\nand inductive biases of underlying neural networks via fast\nparameterization for the rapid generalization.\nUnder the MetaNet framework, an important consideration\nis the type of higher order meta information that can be ex-\ntracted as a feedback from the model when operating on a\nnew task. One desirable property here is that the meta in-\nformation should be generic and problem independent. It\nshould also be expressive enough to explain the model set-\nting in the current task space. We explored the use of loss\ngradients as meta information in this work. As shown in\nthe results, using the gradients as meta information seems\nto be a promising direction. MetaNet obtains state-of-the\nart results on several one-shot SL benchmarks and leads to\na very ﬂexible AI model. For instance, in MetaNet we can\nalternate between different softmax layers on the ﬂy dur-\ning test. It supports continual learning up to a certain point.\nWe observed that neural nets with ﬁxed slow weights can\nperform well for new task inputs when augmented with the\nfast weights. When the slow weights are updated during\ntraining, it learns domain biases resulting in even better per-\nformance on identiﬁcation of new concepts within the same\ndomain. However, one could expect a higher performance\nfrom the ﬁxed weight network when aiming for one-shot\ngeneralization across distant domains.\nAn interesting future direction would be in exploring a new\ntype of meta information that is more robust and expres-\nsive, and in developing synaptic weights that are capable of\nmaintaining such higher order information. One could take\ninspiration from the meta learning process in the brain and\nask whether the brain operates on some kind of higher or-\nder information to generalize across tasks and acquire new\nskills.\nThe rapid parameterization approach presented here has\nbeen shown to be an effective alternative to the direct op-\ntimization methods that learn to update network param-\neters for one-shot generalization.\nHowever, a problem\nthis approach poses is the integration of slow and fast\nweights.\nAs a solution to this, we presented a simple\nlayer augmentation method. Although the layer augmenta-\ntion worked reasonably well, this method becomes difﬁcult\nwhen a neural net has many types of parameters operat-\ning in multiple different time-scales. For example, a single\nbase learner equipped with three types of weights (slow,\nexample-speciﬁc, and task-level weights) integrated under\nthe layer augmentation paradigm could not perform as well\nas a simpler one. Therefore, a potential extension would be\nto train MetaNet so it can discover its own augmentation\nschema for efﬁciency.\nMetaNet can readily be applied to parameterize policies\nin reinforcement learning and imitation learning, leading\nto an agent with one-shot and meta learning capabilities.\nMetaNet based on recurrent networks as underlying learn-\ners could lead to useful applications in sequence modeling\nand language understanding tasks.\n\n\nMeta Networks\nAcknowledgements\nWe would like to thank the anonymous reviewers and our\ncolleagues, Jesse Lingeman, Abhyuday Jagannatha and\nJohn Lalor for their insightful comments and suggestions\non improving the manuscript. This work was supported in\npart by the grant HL125089 from the National Institutes of\nHealth and by the grant 1I01HX001457-01 supported by\nthe Health Services Research & Development of the US\nDepartment of Veterans Affairs Investigator Initiated Re-\nsearch. Any opinions, ﬁndings and conclusions or recom-\nmendations expressed in this material are those of the au-\nthors and do not necessarily reﬂect those of the sponsor.\nReferences\nAndrychowicz, Marcin, Denil, Misha, Gomez, Sergio,\nHoffman, Matthew W, Pfau, David, Schaul, Tom, and\nde Freitas, Nando. Learning to learn by gradient descent\nby gradient descent. In Advances in Neural Information\nProcessing Systems, pp. 3981–3989, 2016.\nBa, Jimmy, Hinton, Geoffrey E, Mnih, Volodymyr, Leibo,\nJoel Z, and Ionescu, Catalin. Using fast weights to attend\nto the recent past. In Advances In Neural Information\nProcessing Systems, pp. 4331–4339, 2016.\nBengio, Yoshua, Bengio, Samy, and Cloutier, Jocelyn.\nLearning a synaptic learning rule.\nUniversit´e de\nMontr´eal, D´epartement d’informatique et de recherche\nop´erationnelle, 1990.\nChopra, Sumit, Hadsell, Raia, and LeCun, Yann. Learning\na similarity metric discriminatively, with application to\nface veriﬁcation. In Computer Vision and Pattern Recog-\nnition, 2005. CVPR 2005. IEEE Computer Society Con-\nference on, volume 1, pp. 539–546. IEEE, 2005.\nDavid Ha, Andrew Dai and Le, Quoc V. Hypernetworks.\nIn ICLR 2017, 2017.\nDe Brabandere, Bert, Jia, Xu, Tuytelaars, Tinne, and\nVan Gool, Luc. Dynamic ﬁlter networks. In Neural In-\nformation Processing Systems (NIPS), 2016.\nEdwards, Harrison and Storkey, Amos. Towards a neural\nstatistician. In ICLR 2017, 2017.\nGomez, Faustino and Schmidhuber, J¨urgen. Evolving mod-\nular fast-weight networks for control. In International\nConference on Artiﬁcial Neural Networks, pp. 383–389.\nSpringer, 2005.\nGoodfellow, Ian J, Mirza, Mehdi, Xiao, Da, Courville,\nAaron, and Bengio, Yoshua. An empirical investigation\nof catastrophic forgetting in gradient-based neural net-\nworks. In ICLR 2014, 2014.\nGraves, Alex, Wayne, Greg, and Danihelka, Ivo. Neural\nturing machines. arXiv preprint arXiv:1410.5401, 2014.\nGreengard, Paul. The neurobiology of slow synaptic trans-\nmission. Science, 294(5544):1024–1030, 2001.\nHarlow, Harry F. The formation of learning sets. Psycho-\nlogical review, 56(1):51, 1949.\nHinton, Geoffrey E and Plaut, David C. Using fast weights\nto deblur old memories.\nIn Proceedings of the ninth\nannual conference of the Cognitive Science Society, pp.\n177–186, 1987.\nHochreiter, Sepp, Younger, A Steven, and Conwell, Pe-\nter R. Learning to learn using gradient descent. In Inter-\nnational Conference on Artiﬁcial Neural Networks, pp.\n87–94. Springer, 2001.\nKaiser, Lukasz, Nachum, Oﬁr, Roy, Aurko, and Bengio,\nSamy. Learning to remember rare events. In ICLR 2017,\n2017.\nKirkpatrick, James, Pascanu, Razvan, Rabinowitz, Neil,\nVeness, Joel, Desjardins, Guillaume, Rusu, Andrei A,\nMilan, Kieran, Quan, John, Ramalho, Tiago, Grabska-\nBarwinska, Agnieszka, et al.\nOvercoming catas-\ntrophic forgetting in neural networks.\narXiv preprint\narXiv:1612.00796, 2016.\nKoch, Gregory.\nSiamese neural networks for one-shot\nimage recognition. PhD thesis, University of Toronto,\n2015.\nLake, Brenden M, Salakhutdinov, Ruslan R, and Tenen-\nbaum, Josh. One-shot learning by inverting a composi-\ntional causal process. In Advances in neural information\nprocessing systems, pp. 2526–2534, 2013.\nLake, Brenden M, Salakhutdinov, Ruslan, and Tenenbaum,\nJoshua B. Human-level concept learning through prob-\nabilistic program induction. Science, 350(6266):1332–\n1338, 2015.\nLi, Ke and Malik, Jitendra. Learning to optimize. In ICLR\n2017, 2017.\nMaclaurin, Dougal, Duvenaud, David, and Adams, Ryan.\nGradient-based hyperparameter optimization through re-\nversible learning. In International Conference on Ma-\nchine Learning, pp. 2113–2122, 2015.\nMitchell, Tom M, Thrun, Sebastian B, et al. Explanation-\nbased neural network learning for robot control.\nAd-\nvances in neural information processing systems, pp.\n287–287, 1993.\n\n\nMeta Networks\nMunkhdalai, Tsendsuren and Yu, Hong.\nNeural seman-\ntic encoders.\nIn Proceedings of the 15th Conference\nof the European Chapter of the Association for Compu-\ntational Linguistics: Volume 1, Long Papers, pp. 397–\n407, Valencia, Spain, April 2017. Association for Com-\nputational Linguistics. URL http://www.aclweb.\norg/anthology/E17-1038.\nRavi, Sachin and Larochell, Hugo. Optimization as a model\nfor few-shot learning. In ICLR 2017, 2017.\nSantoro, Adam, Bartunov, Sergey, Botvinick, Matthew,\nWierstra, Daan, and Lillicrap, Timothy. Meta-learning\nwith memory-augmented neural networks. In Proceed-\nings of The 33rd International Conference on Machine\nLearning, pp. 1842–1850, 2016.\nSchmidhuber, J.\nReducing the Ratio Between Learning\nComplexity and Number of Time Varying Variables\nin Fully Recurrent Nets, pp. 460–463.\nSpringer\nLondon,\nLondon,\n1993a.\nISBN\n978-1-4471-\n2063-6.\ndoi:\n10.1007/978-1-4471-2063-6 110.\nURL\nhttp://dx.doi.org/10.1007/\n978-1-4471-2063-6_110.\nSchmidhuber, J.\nA neural network that embeds its own\nmeta-levels. In IEEE International Conference on Neu-\nral Networks, pp. 407–412 vol.1, 1993b. doi: 10.1109/\nICNN.1993.298591.\nSchmidhuber, J¨urgen.\nEvolutionary principles in self-\nreferential learning. PhD thesis, Technical University\nof Munich, 1987.\nSchmidhuber, J¨urgen.\nLearning to control fast-weight\nmemories:\nAn alternative to dynamic recurrent net-\nworks. Neural Computation, 4(1):131–139, 1992.\nSrivastava, Rupesh K, Masci, Jonathan, Kazerounian,\nSohrob, Gomez, Faustino, and Schmidhuber, J¨urgen.\nCompete to compute. In Advances in neural informa-\ntion processing systems, pp. 2310–2318, 2013.\nSukhbaatar, Sainbayar, Weston, Jason, Fergus, Rob, et al.\nEnd-to-end memory networks. In Advances in neural\ninformation processing systems, pp. 2440–2448, 2015.\nVilalta, Ricardo and Drissi, Youssef. A perspective view\nand survey of meta-learning. Artiﬁcial Intelligence Re-\nview, 18(2):77–95, 2002.\nVinyals, Oriol, Blundell, Charles, Lillicrap, Tim, Wierstra,\nDaan, et al. Matching networks for one shot learning. In\nAdvances in Neural Information Processing Systems, pp.\n3630–3638, 2016.\nWeston, Jason, Chopra, Sumit, and Bordes, Antoine. Mem-\nory networks. In In Proceedings Of The International\nConference on Representation Learning (ICLR 2015),\nSan Diego, California, May 2015. In press.\nYounger, A Steven, Conwell, Peter R, and Cotter, Neil E.\nFixed-weight on-line learning.\nIEEE Transactions on\nNeural Networks, 10(2):272–283, 1999.\n\n\nMeta Networks\nA. Training Details\nTo train and test MetaNet on one-shot learning, we adapted\nthe training procedure introduced by Vinyals et al. (2016).\nFirst, we split the data into training and test sets consisting\nof two disjoint classes. We then formulate a series of tasks\n(trials) from the training set. Each task has a support set of\nN classes with one image per, resulting in an N-way one-\nshot classiﬁcation problem. In addition to the support set,\nwe also include L number of labeled examples in each task\nset to update the parameters θ during training. For testing,\nwe follow the same procedure to form a set of test tasks\nfrom the disjoint classes. However, now MetaNet assigns\nclass labels to L examples based only on the labeled sup-\nport set of each test task.\nFor the one-shot benchmarks on the Omniglot dataset, we\nused a CNN with 64 ﬁlters as the base learner b. This CNN\nhas 5 convolutional layers, each of which is a 3 x 3 con-\nvolution with 64 ﬁlters, followed by a ReLU non-linearity,\na 2 x 2 max-pooling layer, a fully connected (FC) layer,\nand a softmax layer. Another CNN with the same architec-\nture is used to deﬁne the dynamic representation learning\nfunction u, from which we take the output of the FC layer\nas the task dependent representation r. We trained a simi-\nlar CNNs architecture with 32 ﬁlters for the experiment on\nMini-ImageNet. However for computational efﬁciency as\nwell as to demonstrate the ﬂexibility of MetaNet, the last\nthree layers of these CNN models were augmented by fast\nweights. For the networks d and m, we used a single-layer\nLSTM with 20 hidden units and a three-layer MLP with 20\nhidden units and ReLU non-linearity. As in Andrychow-\nicz et al. (2016), the parameters G and Z of d and m are\nshared across the coordinates of the gradients ∇and the\ngradients are normalized using the same preprocessing rule\n(with p = 7). The MetaNet parameters θ are optimized\nwith ADAM. The initial learning rate was set to 10−3. The\nmodel parameters θ were randomly initialized from the uni-\nform distribution over [-0.1, 0.1).\nB. MNIST as Out-Of-Domain Data\nWe treated MNIST images as a separate domain data. Par-\nticularly a model is trained on the Omniglot training set and\nevaluated on the MNIST test set in 10-way one-shot learn-\ning setup. We hypothesize that models with a high dynamic\nshould perform well on this task.\nIn Figure 5, we plotted the results of this experiment.\nMetaNet- achieved 71.6% accuracy which was 0.6% and\n3.2% lower than the other variants with fast weights. This\nis not surprising since MetaNet without dynamic represen-\ntation learning function lacks an ability to adapt its pa-\nrameters to MNIST image representations. The standard\nMetaNet model achieved 74.8% and MetaNet+ obtained\n72.3%. Matching Net (Vinyals et al., 2016) reported 72.0%\naccuracy in this setup. Again we did not observe improve-\nment with MetaNet+ model here. The best result was re-\ncently reported by using a generative model, Neural Statis-\ntician, that extends variational autoencoder to summarize\ninput set (Edwards & Storkey, 2017).\n64\n66\n68\n70\n72\n74\n76\n78\n80\nAccuracy \nMNIST as out-of-domain data evaluation \nSiamese Net\nMatching Net\nMetaNet-\nMetaNet\nMetaNet+\nNeural Statistician\nFigure 5. MNIST 10-way one-shot classiﬁcation results.\n"
}