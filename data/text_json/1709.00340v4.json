{
  "filename": "1709.00340v4.pdf",
  "num_pages": 12,
  "pages": [
    "1\nFine-grained Visual-textual Representation Learning\nXiangteng He and Yuxin Peng\nAbstract—Fine-grained visual categorization is to recognize\nhundreds of subcategories belonging to the same basic-level\ncategory, which is a highly challenging task due to the quite\nsubtle and local visual distinctions among similar subcategories.\nMost existing methods generally learn part detectors to discover\ndiscriminative regions for better categorization performance.\nHowever, not all parts are beneﬁcial and indispensable for visual\ncategorization, and the setting of part detector number heavily\nrelies on prior knowledge as well as experimental validation. As is\nknown to all, when we describe the object of an image via textual\ndescriptions, we mainly focus on the pivotal characteristics, and\nrarely pay attention to common characteristics as well as the\nbackground areas. This is an involuntary transfer from human\nvisual attention to textual attention, which leads to the fact that\ntextual attention tells us how many and which parts are dis-\ncriminative and signiﬁcant to categorization. So textual attention\ncould help us to discover visual attention in image. Inspired\nby this, we propose a ﬁne-grained visual-textual representation\nlearning (VTRL) approach, and its main contributions are: (1)\nFine-grained visual-textual pattern mining devotes to discovering\ndiscriminative visual-textual pairwise information for boosting\ncategorization performance through jointly modeling vision and\ntext with generative adversarial networks (GANs), which au-\ntomatically and adaptively discovers discriminative parts. (2)\nVisual-textual representation learning jointly combines visual and\ntextual information, which preserves the intra-modality and inter-\nmodality information to generate complementary ﬁne-grained\nrepresentation, as well as further improves categorization perfor-\nmance. Comprehensive experimental results on the widely-used\nCUB-200-2011 and Oxford Flowers-102 datasets demonstrate the\neffectiveness of our VTRL approach, which achieves the best\ncategorization accuracy compared with state-of-the-art methods.\nIndex Terms—Fine-grained visual categorization, ﬁne-grained\nvisual-textual pattern mining, visual-textual representation learn-\ning.\nI. INTRODUCTION\nF\nINE-GRAINED visual categorization aims to recognize\nsimilar subcategories in the same basic-level category. It\nis one of the most challenging and signiﬁcant open problems\nin multimedia and computer vision areas, which has achieved\ngreat progress as well as attracted extensive attention of\nacademia and industry in recent years. The progress incarnates\nin three aspects: (1) More ﬁne-grained domains have been\ncovered, such as animal species [1], [2], plant breeds [3],\n[4], car types [5] and aircraft models [6]. (2) Methodologies\nof ﬁne-grained visual categorization have achieved promising\nperformance in recent years [7]–[11], due to the application\nof deep neural networks (DNNs). (3) Some information tech-\nnology companies, such as Microsoft and Baidu, begin to\nThe authors are with the Institute of Computer Science and Technology,\nPeking University, Beijing 100871, China. Corresponding author: Yuxin Peng\n(e-mail: pengyuxin@pku.edu.cn).\nSmall variance among different subcategories\nArtic Tern\nCaspian Tern\nLarge variance in the same subcategory\nCommon Tern\nSalty Backed Gull\nSalty Backed Gull\nSalty Backed Gull\nFig. 1: Examples from CUB-200-2011 dataset [1]. Note that\nﬁne-grained visual categorization is a technically challenging\ntask even for humans to recognize these subcategories, due\nto small variances among different subcategories and large\nvariances in the same subcategory.\nturn ﬁne-grained visual categorization technologies into their\napplications1 2.\nFine-grained visual categorization lies in the continuum\nbetween basic-level visual categorization (e.g. object recogni-\ntion) and identiﬁcation of individuals (e.g. face recognition).\nIts main challenges can be summarized as the following\ntwo aspects: (1) Variances among similar subcategories are\nsubtle and local, because they belong to the same genus. (2)\nVariances in the same subcategory are large and diverse, due to\ndifferent poses and views, as well as for animals or plants also\nbecause of different living environments and growth periods.\nFor example, as shown in Fig. 1, the images of “Artic Tern”\nand “Caspian Tern” look similar in global appearance, but\nthe images of “Salty Backed Gull” look different in the pose,\nview and feather color. So it is hard for a person without\nprofessional knowledge to recognize them.\nThese subcategories can be distinguished by the subtle and\nlocal variances of the discriminative parts. It is crucial for\nﬁne-grained visual categorization to localize the object and its\ndiscriminative parts. Researchers generally adopt a two-stage\ncategorization pipeline: the ﬁrst stage is to localize the object\nor its discriminative parts, and the second is to extract their\nfeatures to categorize the subcategory. For example, Zhang\net al. [12] utilize R-CNN [13] with geometric constraints to\ndetect object and its parts ﬁrst, and then extract the features\nof the object and its parts, ﬁnally train one-versus-all linear\nSVMs for categorization. However, not all the parts are\nbeneﬁcial and indispensable for ﬁne-grained categorization.\n1https://www.microsoft.com/en-us/research/project/ﬂowerreco-cn/\n2http://image.baidu.com/?fr=shitu/\narXiv:1709.00340v4  [cs.CV]  20 Feb 2019\n",
    "2\nThe conclusive distinctions among subcategories generally\nlocate at a few speciﬁc parts, such as the red beak or the\nblack tail. So the categorization performance depends on the\nnumber of part detectors and whether the detected parts are\ndiscriminative or not. However, mainstream methods generally\nset the detector number due to their prior knowledge or the\nexperimental validation, which is highly empirical and limited.\nFor example, when the number of part detectors applied in the\nexperiments increase from eight to ﬁfteen, the performance of\nﬁne-grained categorization declines, as reported in [14]. Six\npart detectors are applied by Zhang et al. [15] to achieve the\nbest categorization accuracy. He and Peng [16] applies two\ndiscriminative parts for ﬁne-grained categorization. They are\nlimited in ﬂexibility, and hard to generalize.\nTherefore, it is signiﬁcant to automatically learn how many\nand which parts really make sense to ﬁne-grained visual\ncategorization. When human beings see two images of two\ndifferent subcategories, human visual attention mechanism\nplays an important role in focusing on the pivotal distinctions\nbetween them. Inspired by this, researchers begin to apply hu-\nman visual attention mechanism in their works, aiming to ﬁnd\nthe most discriminative characteristics for categorization. Xiao\net al. [17] propose a two-level attention model (TL Atten), in\nwhich object-level attention selects relevant image proposals to\na certain object, and part-level attention selects relevant image\nproposals to the discriminative parts of the object. Fu et al. [18]\npropose a recurrent attention convolutional neural network\n(RA-CNN) to recursively learn discriminative region attention\nand region-based feature representation. These works simulate\nhuman visual attention mechanism to ﬁnd discriminative parts\nfor categorization from visual information.\nAttention is the behavioral and cognitive process of se-\nlectively concentrating on a discrete aspect of information,\nwhether deemed subjective or objective, while ignoring other\nperceivable information [19]. As is known to all, when human\nbeings give the interpretation of the visual data by textual\ndescriptions, they tend to indicate how many and which\nparts are distinguishing from other subcategories. These words\ndescribing the part attributes are regarded as textual attention,\nwhich generally appears frequently in the textual descriptions.\nThis is an involuntary transfer from human visual attention to\ntextual attention. In this transfer process, common character-\nistics of object and background areas are ignored naturally.\nTextual attention can be obtained by discovering the frequent\nitem sets in the textual descriptions, which point out the\ndiscriminative parts of the subcategory. From Fig. 2, we can\nsee that the frequent item sets contain “red break”, which is\na discriminative characteristic that distinguishes “Heermann\nGull” from “Red Legged Kittiwake”.\nTherefore, how to exactly relate textual attention to vi-\nsual attention and mine the discriminative parts are pivotal\nto ﬁne-grained visual categorization. This paper proposes\na ﬁne-grained visual-textual representation learning (VTRL)\napproach, and its main contributions are:\n• Fine-grained visual-textual pattern mining devotes to\ndiscovering discriminative visual-textual parts for cat-\negorization by jointly modeling vision and text with\ngenerative adversarial networks (GANs). Different from\nSubcategory\nHeermann\nGull\n(1)A large bird with different shades of grey \nall over its body, white and black tail feathers, \nand a long sharp orange beak.\n(2)This bird is gray and black in color, with a \norange beak.\n(3)This bird has black outer retices and white \ninner retires and an orange beak.\n...\nVision\nText\nRed Legged \nKittiwake\n(1)This bird has a white head, breast and belly \nwith gray wings, red feet and thighs, and a red \nbeak.\n(2)This is a white bird with gray wings, red \nwebbed feet and a red beak.\n(3)This bird has a white head, nape, breast and \nbelly, with gray wings, the underside of which \nare black-tipped.\n...\nFig. 2: Examples of visual and textual attentions. The images\ncome from CUB-200-2011 dataset [1], and text are collected\nby Reed et al. [20] through Amazon Mechanical Turk (AMT)\nplatform.\nexisting methods, the localized discriminative parts in this\npaper could not only tell us how many and which parts\nare signiﬁcant for categorization, but also which attributes\nof parts are distinguishing from other subcategories. The\npart number is determined automatically and adaptively\nby textual attention.\n• Visual-textual representation learning is proposed to\ncombine visual and textual information. Visual stream\nfocuses on the locations of the discriminative parts,\nwhile textual stream focuses on the discrimination of the\nregions. It preserves the intra-modality and inter-modality\ninformation to generate complementary ﬁne-grained rep-\nresentation, as well as further improves categorization\naccuracy.\nOur previous conference paper CVL [11] proposes a two-\nstream model combining vision and language for learning\nthe ﬁne-grained representation. Vision stream learns deep\nrepresentations from visual information and language stream\nutilizes textual information to encode salient visual aspects\nfor distinguishing subcategories. The main differences between\nthe proposed VTRL approach and CVL can be summarized\nas the following three aspects: (1) Our VTRL approach\nemploys textual pattern mining to localize textual attention\nfor exploiting the human visual attention transferred into\ntextual information, which indicates how many and which\nparts are signiﬁcant and indispensable for categorization.\nWhile CVL directly utilizes the whole textual information,\ndoes not mine ﬁne-grained textual attention information. (2)\nOur VTRL approach employs visual pattern mining based on\ndiscovered textual patterns to localize discriminative parts, so\nthat discriminative parts and objects are both exploited to learn\nmulti-grained and multi-level representations for boosting ﬁne-\ngrained categorization. While CVL only exploits the objects,\nwhich ignores the complementary and semantic ﬁne-grained\nclues provided by the discriminative parts. (3) Our VTRL\napproach employs ﬁne-grained visual-textual pattern mining\nto discover the discriminative and signiﬁcant visual-textual\npairwise information via jointly modeling vision and text with\n",
    "3\nGANs, which mines the correlation between textual and visual\nattention. While CVL only combines vision and text, ignoring\nto exploit their visual and textual attention, as well as their\ncorrelation. Compared with state-of-the-art methods on two\nwidely-used ﬁne-grained visual categorization datasets, our\nVTRL approach achieves the best categorization accuracy.\nThe remainder of this paper is organized as follows: We\nbrieﬂy review the related works in Section II. In Section III our\nproposed VTRL approach is presented in detail. Then Section\nIV reports the experimental results and analyses. Finally,\nSection V concludes this paper.\nII. RELATED WORK\nIn this section, we brieﬂy review the related works of\nﬁne-grained visual categorization, frequent pattern mining and\nmulti-modal analysis.\nA. Fine-grained Visual Categorization\nSince the discriminative regions of image is crucial for\nﬁne-grained visual categorization, most existing methods [12],\n[17] ﬁrst localize the discriminative regions of image, such as\nthe object and its parts, and then extract their discriminative\nfeatures for ﬁne-grained categorization. Some methods directly\nuse the annotations of the object [21], [22] and parts [23],\n[24] to localize the discriminative regions. However, it is not\navailable to obtain the annotations in practical applications,\nsome researchers begin to use the annotations of the object\nand parts only in the training phase. Zhang et al. [25] propose\nthe Deformable Part-based Model (DPM) to localize the\ndiscriminative regions with the object and part annotations\nas the supervised information in the training phase. Further\nmore, PG Alignment [26] is proposed to train part detectors\nonly with object annotation, and localize the discriminative\nparts in an automatic manner in the testing phase.\nOnly using object annotation is still not promising in the\npractical applications. Recently, some works [17], [27], [28]\nare proposed to localize the discriminative regions in a weakly-\nsupervised manner, which means that neither object nor part\nannotations are used in both training and testing phases. Xiao\net al. [17] combine the object and part level attentions to select\nthe discriminative image proposals, which is the ﬁrst work to\nlocalize the discriminative regions without using object and\npart annotations. Yao et al. [27] also propose to combine the\ntwo complementary object-level and part-level visual descrip-\ntions for better performance. A neural activation constellation\n(NAC) part model [29] is proposed to train part detectors with\nconstellation model. He and Peng [16] integrate two spatial\nconstraints to select more discriminative proposals and achieve\nbetter categorization accuracy. The aforementioned methods\nmostly set the detector number due to the prior knowledge or\nexperimental validation, which is highly limited in ﬂexibility\nand difﬁcult for generalizing to the other domains. Therefore,\nwe attempt to automatically learn how many and which parts\nreally make sense to categorization via ﬁne-grained visual-\ntextual pattern mining.\nB. Frequent Pattern Mining\nFrequent patterns are itemsets, subsequences, or substruc-\ntures that appear in a data set with frequency no less than a\nuser-speciﬁed threshold [30]. For example, diaper and beer\nappear frequently together in sales data of a supermarket,\nwhich is a frequent pattern. Frequent pattern mining is ﬁrst\nproposed by Agrawal et al. [31] for market basket analysis.\nAgrawal and Strikant propose Apriori algorithm [32] to mine\nfrequent patterns in a large transaction database. For textual\nmining, frequent patterns may be sequential patterns, frequent\nitemsets, or multiple grams. While for visual mining, frequent\npatterns may be middle-level feature representation or high-\nlevel semantic representation. Han et al. [33] propose to mine\nvisual patterns using low-level features. Li et al. [34] propose\nto combine CNN features and association rule mining for dis-\ncovering visual patterns. Li et al. [35] propose a novel multi-\nmodal pattern mining method, which takes textual pattern and\nvisual pattern into consideration at the same time. In this\npaper, we ﬁrst utilize Apriori algorithm to discover the textual\npatterns, and then employ generative adversarial networks\n(GANs) to mine the relationships between part proposals\nand textual patterns for better categorization accuracy, which\ndiscovers visual and textual patterns at the same time as well\nas mines the intrinsic correlation between them.\nC. Multi-modal Analysis\nNowadays, multi-modal data, e.g. image, text, video and\naudio, has been widely available on the Internet. They con-\ntains different kinds of information, which are complemen-\ntary to help achieving comprehensive results in many real-\nworld applications. So it is signiﬁcant to learn multi-modal\nrepresentation for boosting the signal-modal tasks [36], [37].\nCanonical correlation analysis (CCA) [38] is proposed to learn\nlinear projection matrices, which project features of different\nmodalities into the common space and obtain the common\nrepresentation. It is widely used for modeling multi-modal\ndata [39]–[41]. Zhai et al. propose the joint representation\nlearning method (JRL) to learn projection matrices considering\nthe semantic and correlation information. Due to the advances\nof deep learning, deep learning based methods have been pro-\nposed to boost the performance of multi-modal representation\nlearning. Ngiam et al. [42] propose the bimodal autoencoders\n(Bimodal AE) to model multi-modal data via minimizing the\nreconstruction error, and learn a shared representation across\nmodalities.\nRecently, image and video captioning, which are types\nof multi-modal analysis, have achieved great progress. Long\nShort-Term Memory (LSTM) [43] and character-based con-\nvolutional networks [44] are widely used in image and video\ncaptioning. The architecture of Convolutional and Recurrent\nNetworks (CNN-RNN) is widely used in image and video\ncaptioning, and achieves great performance. In this paper, we\napply the extension of Convolutional and Recurrent Networks\n(CNN-RNN) to learn a visual semantic embedding. In this\npaper, we bring the multi-modal representation learning into\nﬁne-grained visual categorization to jointly modeling vision\nand text for boosting the performance.\n",
    "4\nbrown bird with a yellow pointed beak \nConvolutional \nencoding\nSequential \nencoding\nTextual stream\nVisual stream\nCNN\nVisual \nscore\nTextual\nscore\nC\nO\nN\nV\nW1\nW2\nWn\nGAP\n...\n...\nC\nO\nN\nV\nInputs\nOutputs\nBlack Tern\nCactus Wren\nDownyWoodpecker\nLaysan Albatross\nPart proposals\nLocalized objects\nFine-grained \nVisual-textual \nPattern Mining\nWhite head Black wings\nDiscriminative \nvisual-textual parts\nFig. 3: Overview of our VTRL approach.\nIII. OUR VTRL APPROACH\nA. Overview of Our VTRL Approach\nOur approach is based on a very promising and interesting\nintuition: textual descriptions can point out the discrimina-\ntive characteristics of images, and provide complementary\ninformation with visual information. Therefore, we propose\na ﬁne-grained visual-textual representation learning (VTRL)\napproach, which takes the advantages of visual and textual\ninformation jointly as well as exploits the intrinsic correlation\nbetween them. Fig. 3 shows our VTRL approach. First, we\nconduct ﬁne-grained visual-textual pattern mining to discover\nthe discriminative visual-textual parts as shown in Fig. 4. Then,\nwe localize the object region of image to boost the visual\nanalysis. Finally, we propose a visual-textual representation\nlearning approach to jointly model visual and textual streams\nfor better categorization accuracy.\nB. Fine-grained Visual-textual Pattern Mining\nSince human visual attention is described into the form of\ntextual descriptions, we ﬁrst conduct textual pattern mining to\ndiscover the textual attention, which indicates the distinguish-\ning part attributes from other subcategories, such as the shape,\nsize and color of the part. Then, we conduct visual pattern\nmining to localize the discriminative parts corresponding to\nthe textual patterns discovered by textual pattern mining. The\noverview of our ﬁne-grained visual-textual pattern mining\napproach is shown in Fig. 4. In the following paragraphs,\nwe describe the ﬁne-grained visual-textual pattern mining\napproach from three aspects: 1) deﬁnition of pattern mining, 2)\ntextual pattern mining and 3) visual pattern mining via GANs.\n1) Deﬁnition of Pattern Mining: We ﬁrst introduce the basic\ndeﬁnitions for pattern mining. Assume that there is a set of\nn items, which is denoted as X = {x1, x2, ..., xn}, and the\ntransaction T is a subset of X, i.e. T ⊆X. We also deﬁne\nBlack wings\nWhite head\nText \ntransaction\nAssociation \nrule mining\nTextual pattern 1\nTextual pattern 2\n{0,1}\nText encoder\nPart proposal\nDiscriminator network\nthis bird has a white head, \nthe bill is long and curved, \nwith a white belly and \nblack wings.\nthis bird is white in color \nwith a black beak, and \nwhite eye rings.\nthis large bird has long bill, \na white breast, belly & head \nand a black back & wings\nthis bird is nearly all white \nwith gray tarsus and feet.\nbird has grey and white \nbeak with grey rectrices and \nthe rest of the bird is white.\nthis large bird has a white \nhead and belly, white wings \nwith black on the ends of \nthe feathers, and a white \ntail.\nthis bird has wings that are \nblack and white and has a \nlong bill\nTextual pattern mining\nVisual pattern mining\nFig. 4: Overview of our ﬁne-grained visual-textual pattern min-\ning approach. {0, 1} denotes the output of the discriminator in\nGANs, which indicates whether the input part proposal meets\nthe input textual pattern.\na transaction database D = {T1, T2, ..., TK} that contains K\ntransactions. Our goal is to discover a particular subset T ∗of\ntransactions database X, which can predict the presence of\nsome target item y ∈Ty, and T ∗⊂Ty as well as y ∩T ∗= ∅.\nT ∗refers to frequent itemset in pattern mining literature. The\nsupport of T ∗denotes how often T ∗appears in D and its\ndeﬁnition is as follow:\nsupp(T ∗) = |{Ty|T ∗⊆Ty, Ty ∈D}|\nK\n(1)\nAn association rule T ∗→y deﬁnes a relationship between\nT ∗and a certain item y. Therefore, we aim to ﬁnd patterns\nthat appear in a transaction there is a high likelihood that y.\nWe deﬁne the conﬁdence as follow:\nconf(T ∗→y) = supp(T ∗∪y)\nsupp(T ∗)\n(2)\n2) Textual Pattern Mining: In this paper, we devote to\ndiscovering textual patterns, which contain the human visual\nattention information. First, we remove stop words and punc-\ntuations from each textual description. Then we select the\nwords, which appear in at least 10 textual descriptions in\nour dataset. Build a vocabulary with these selected words,\nwhich is used for generating transactions. It is noted that\nthere are no duplicate words in the vocabulary. In order to\ngenerate transaction for each textual description, we map\neach word back to its corresponding word in the vocabulary,\nthen include that corresponding word index in the transaction.\nAfter obtaining the transactions, we perform association rule\nmining to ﬁnd the words that frequently appear in textual\ndescriptions, which also means that these words can represent\nthe characteristics of this subcategory. Speciﬁcally, we utilize\nthe Apriori algorithm [32] to ﬁnd a set of patterns P through\n",
    "5\nassociation rule mining. Each pattern p ∈P must satisfy the\nfollowing criteria:\nsupp(p) > suppmin\n(3)\nconf(p →c) > confmin\n(4)\nwhere suppmin and confmin are thresholds for the support\nvalue and conﬁdence value respectively, and c means the\nimage-level subcategory label. After association rule mining,\neach discovered pattern p contains a set of words.\nWe want to ﬁnd some patterns that point out the discrimi-\nnative parts of the image, which have the semantic meaning.\nTherefore, we conduct distance constraint on association rule\nmining as follow:\ndis(wi, wj) < dismin\n(5)\nwhere wi and wj mean the i-th and j-th words in the same\ntextual description, and dis(·) means the interval between the\ni-th and j-th words. The distance function ensures that the\ndiscovered patterns have the semantic meaning. The actual\nthreshold in distance function is set to 4 in the experiments,\nwhich is set by the cross-validation method following [12].\nFinally, we discover a set of patterns P, i.e. textual attention\nin the textual descriptions, which contains the information of\nhuman visual attention.\n3) Visual Pattern Mining via GANs: After obtaining the\ntextual attention, we devote to mining the relationship between\nvisual and textual attention, i.e. localize the discriminative\nparts of images via the guidance of textual attention. Due to\nthe great progress made by generative adversarial networks\n(GANs), which can generate images based on textual infor-\nmation. In this paper, we employ GANs to break through the\ngap between visual and textual information, and localize the\ndiscriminative parts corresponding to the discovered textual\npatterns. Speciﬁcally, the network architecture follows GAN-\nCLS [45]. The original training images and their annotated\ntextual descriptions are used to train the GAN-CLS model.\nWe take the alternating strategy to update the generator and\ndiscriminator networks, and use ADAM solver [46] to train\nthe model. The training settings, such as learning rate and\nmomentum, are conﬁgured following GAN-CLS [45].\nIt is noted that part proposals and textual patterns are not\nused to train the GAN-CLS model, as it is unavailable to\nobtain their matching labels. Reed et al. [20] point out that\nthe text embedding based on textual descriptions covers the\nvisual attributes, i.e. textual patterns, such as shape, size and\ncolor of the part. GAN-CLS follows [20] to obtain a visually-\ndiscriminative vector representation of text descriptions, by\nusing deep convolutional and recurrent text encoders that learn\na correspondence function with images. Even using images\nand textual descriptions in the training phase, GAN-CLS can\nstill learn the correlation between the part proposals and\ntextual patterns. As described in GAN-CLS, the generator\nhas learned to generate plausible images, and also learned\nto align them with the conditioning information, and likewise\nthe discriminator must learn to evaluate whether samples from\ngenerator meet this conditioning constraint. So we ﬁrst train\nGAN-CLS on the datasets in our paper, and then apply the\ndiscriminator in GAN-CLS to select the corresponding part\nproposals for the speciﬁc textual patterns, where we take one\npart proposal as the sample input, and one textual pattern as the\nconditioning constraint input. The selected part proposals con-\ntain discriminative information that helps to distinguish similar\nsubcategories. In the following paragraphs, we introduce the\nvisual pattern mining approach in details.\nFirst, for each image we perform bottom-up process to\ngenerate part proposals. In this paper, we utilize selective\nsearch method [47] to generate 1000 part proposals for each\nimage. Then we take the part proposals and discovered textual\npatterns as the inputs of discriminator network, to relate the\ndiscovered textual patterns with the corresponding part pro-\nposals. For each part proposal, discriminator network outputs\na score vector that refers to the degree of correlations between\npart proposal and textual patterns. We select the part proposal\nwith highest score for each textual pattern, which is one of\nthe most discriminative parts for categorization. They will be\nutilized as the inputs of visual-textual representation learning.\nC. Object Localization\nFor better categorization performance, we apply an au-\ntomatic object localization method based on CAM [48]to\nlocalize the object in a weakly-supervised manner, which\nmeans that neither object nor part annotations are used in both\ntraining and testing phases. Through CAM, we can generate\na subcategory activation map Mc for each subcategory c, in\nwhich the spatial value is calculated as follow:\nMc(x, y) =\nX\nk\nwc\nkfk(x, y)\n(6)\nwhere fk(x, y) denotes the activation of unit k in the last con-\nvolutional layer at spatial location (x, y), and wc\nk is the weight\ncorresponding to subcategory c for unit k. The subcategory\nlabel information is not available in testing phase, so we set\nsubcategory c by the predicted subcategory. After obtaining the\nactivation map for each image, we conduct OTSU algorithm\n[49] to binarize the image and take the bounding box that\ncovers the largest connected area as the localization of object.\nThe localized object is utilized as the inputs of visual-textual\nrepresentation learning along with the localized discriminative\nparts via ﬁne-grained visual-textual pattern mining. Examples\nof object localization results are shown in Fig. 5. It is noted\nthat we use a variant of VGGNet [50] as CAM following\n[48]. In order to get a higher spatial resolution, the layers\nafter conv5 3 are removed, resulting in a mapping resolution\nof 14 × 14. Besides, a convolutional layer of size 3 × 3, stride\n1, pad 1 with 1024 neurons is added, followed by a global\naverage pooling layer and a softmax layer.\nD. Visual-textual Representation Learning\nSince visual content and textual descriptions provide com-\nplementary information, we jointly model them with a two-\nstream model for learning visual-textual representations to\nboost the categorization performance. The two-stream model\nconsists of: 1) visual stream and 2) textual stream.\n",
    "6\nFig. 5: Examples of object localization results in this paper.\nThe red rectangles indicate the ground truth object annotations,\ni.e. bounding boxes of objects, and the yellow rectangles\nindicate the object regions localized by our approach.\n1) Visual Stream: We apply CNN model, e.g. VGGNet [50]\nin our experiments, as the visual categorization function f. The\nCNN model is pre-trained on the ImageNet 1 K dataset [51],\nand then ﬁne-tuned on the ﬁne-grained visual categorization\ndataset.\nFor a given image I, we ﬁrst conduct object localization\nand ﬁne-grained visual-textual pattern mining respectively to\nobtain the object b and its n discriminative parts Pa =\n{Pa1, Pa2, ..., Pan}. Then the object and discriminative parts\nare cropped from the original image, and saved as images\nIb and IP a = {IP a1, IP a2, ..., IP an}. We feed the original\nimage I and its object image Ib as well as its part images\nIP a = {IP a1, IP a2, ..., IP an} to the CNN model to obtain the\npredicted visual scores. For the part images, we calculate their\nmean value as the ﬁnal part prediction. Finally, we calculate\nthe weighted mean of original prediction, object prediction and\npart prediction as the ﬁnal visual prediction.\n2) Textual Stream: In textual stream, we aim to measure\nthe similarity between visual and textual information. We ﬁrst\napply the deep structured joint embedding method [20] to\njointly embed vision (i.e. images) and text (i.e. natural lan-\nguage descriptions for images), which learns a compatibility\nfunction of vision and text.\nWe\ndeﬁne\nthe\ntraining\ndata\nas\nD\n=\n(vn, tn, yn), n = 1, ..., N, where v ∈V and t ∈T denote the\nvision and text, and y ∈Y denotes their subcategory labels.\nThen we apply the empirical risk to learn the visual and\ntextual classiﬁer functions fv : V →Y and ft : T →Y as\nfollows:\n1\nN\nN\nX\nn=1\n∆(yn, fv(vn)) + ∆(yn, ft(tn))\n(7)\nwhere ∆: y × y →R is the 0-1 loss and\nfv(v) = arg max\ny∈Y Et∼T (y)[F(v, t)]\n(8)\nft(t) = arg max\ny∈Y Ev∼V (y)[F(v, t)]\n(9)\nThe compatibility function F : V ×Y →R is deﬁned as the\ninner product of features from the learnable encoder functions\nas follows:\nF(v, t) = θ(v)T φ(t)\n(10)\nwhere θ(v) is the visual encoder, and φ(t) is the textual\nencoder. The visual and textual encoders are implemented\nby GoogleNet [52] and Convolutional Recurrent Net (CNN-\nRNN) [20] respectively in our approach. The CNN-RNN\nmodel consist of a mid-level temporal CNN hidden layer and\na recurrent network. The outputs of the hidden unit over the\ntextual sequence is averaged as the textural features. Then the\ntextual predicted score is deﬁned as a linear accumulation of\nevidence for compatibility with the image which needs to be\nrecognized.\nE. Training Process\nIn this subsection, we summarize our training process.\nWe train three models for original images, objects and parts\nrespectively. Their detailed training processes are shown in\nAlgorithm 1.\nAlgorithm 1 Training Process\nInput: The training images I and their corresponding textual\ndescriptions T.\nOutput: The model M.\n1: Set M = {Mori, Mobject, Mpart}\n2: Use I to ﬁne-tune the CNN model, which is pre-trained\non ImageNet, obtaining the model Mori\n3: Conduct object localization as described in Section III-C,\nto get the object regions b of I\n4: Crop b from I and save as images Ib\n5: Use Ib to ﬁne-tune Mori, obtaining the model Mobject\n6: Follow [45] to train GAN-CLS using minibatch SGD with\nI and T as pairwise constraints\n7: Conduct selective search [47] on each image to get part\nproposals S\n8: Conduct textual pattern mining to obtain the discriminative\ntextual patterns P for each subcategory\n9: for k = 1, ..., n; j = 1, ..., d do\n10:\nTake k-th part proposal Sk and j-th textual pattern Pj\nas the input of the generator G of GAN-CLS\n11:\nPerform a feed-forward pass, and output the correlation\nscore of Sk and Pj\n12:\nFor Pj we select one part proposal with the highest\ncorrelation score\n13: end for\n14: Use the selected part proposals to ﬁne-tune Mobject,\nobtaining the model Mpart\n15: return M.\n",
    "7\nF. Final Prediction\nFor a given image I, we obtain the visual predicted score\nfrom the view of the visual information, and obtain the\ntextual predicted score via measuring the visual and textual\ninformation with the shared compatibility function. Due to\nthe fact that joint learning of visual and textual information\npreserves the intra-modality and inter-modality information to\ngenerate complementary information, we fuse the visual and\ntextual predicted results as the ﬁnal prediction via the follow\nequation:\nf(I) = fv(v) + β ∗ft(t)\n(11)\nwhere fv(v) and ft(t) are the visual and textual predicted\nscores as mentioned above. β is selected by the cross-\nvalidation method following [12], and its value is 2 in our\nexperiments on the two ﬁne-grained datasets.\nIV. EXPERIMENTS\nA. Datasets\nThis subsection presents two ﬁne-grained visual catego-\nrization datasets adopted in the experiments, including CUB-\n200-2011 and Oxford Flowers-102 datasets, and their detailed\ninformation is described as follows:\n• CUB-200-2011. It is the most widely-used dataset for\nﬁne-grained visual categorization task. The visual infor-\nmation comes from the original dataset of CUB-200-2011\n[1]. It contains 11,788 images of 200 subcategories be-\nlonging to birds, 5,994 for training and 5,794 for testing.\nEach image has detailed annotations: 1 subcategory label,\n15 part locations, 312 binary attributes and 1 bounding\nbox. The textual information comes from Reed et al. [20].\nThey expand the CUB-200-2011 dataset by collecting\nﬁne-grained natural language descriptions. Ten single-\nsentence descriptions are collected for each image, as\nshown in Fig. 6. The natural language descriptions are\ncollected through the Amazon Mechanical Turk (AMT)\nplatform, and are required at least 10 words, without any\ninformation of subcategories and actions.\n• Oxford Flowers-102. Same with CUB-200-2011 dataset,\ntextual information comes from Reed et al. [20], and\nvisual information comes from the original dataset of\nOxford Flowers-102 [4], as shown in Fig. 6. It has 8,189\nimages of 102 subcategories belonging to ﬂowers, 1,020\nfor training, 1,020 for validation and 6,149 for testing.\nEach subcategory consists of between 40 and 258 images.\nB. Evaluation Metric\nAccuracy is adopted to comprehensively evaluate the cat-\negorization performances of our VTRL approach as well as\ncompared state-of-the-art methods, which is widely used in\nﬁne-grained visual categorization [8], [12], and its deﬁnition\nis as follow:\nAccuracy = Ra\nR\n(12)\nwhere R denotes the number of images in testing set, and Ra\ndenotes the number of images that are correctly classiﬁed.\nC. Implementation Details\nFine-grained Visual-textual Pattern Mining. First, we cal-\nculate the frequency of each word in the textual descriptions\nfor each subcategory, and select the top-10 words as keywords,\nand then discover textual frequent patterns via Apriori algo-\nrithm [32]. It is noted that we conduct textual pattern mining\nfor each subcategory respectively rather than all subcategories\ntogether, which guarantees that the frequent textual patterns\ntend to be the descriptions of the discriminative parts, such\nas “white head”, “black wings” and “long bill”. Second, we\nconduct selective search [47] on each image to generate part\nproposals. Finally, we employ discriminator network to relate\ntextual patterns to part proposals, then select the proposal with\nhighest score as the discriminative part for each textual pattern.\nFor each subcategory, the number of parts is set automatically\nand adaptively based on the discovered textual patterns. Fig.\n7 shows some matching examples between textual pattern and\nvisual pattern, which are the discriminative characteristics of\nthe subcategory, such as “long&brown neck”, “yellow belly”\nand “black beak”.\nVisual-textual Representation learning. For textual stream,\nwe apply CNN-RNN [20] as the text encoder to learn a\ncorrespondence function with images. In the training phase,\nwe follow Reed et al. [20]. For visual stream, we apply\nthe widely-used model 19-layer VGGNet [50] with batch\nnormalization. The model is ﬁrst pre-trained on ImageNet\n1K dataset, and then ﬁne-tuned on the ﬁne-grained visual\ncategorization dataset. Inspired by the strategy adopted by\nXiao et al [17], we utilize the pre-trained CNN model as a\nﬁlter net to select proposals relevant to the object from the\ngenerated image proposals by selective search method. We\nfurther ﬁne-tune the pre-trained model with the selected image\nproposals.\nD. Comparisons with state-of-the-art methods\nIn this subsection, we present the experimental results of our\nproposed approach as well as all the compared state-of-the-art\nmethods, as shown in Tables I and II, which demonstrate the\neffectiveness of our proposed VTRL approach. As shown in\nTable I, our proposed VTRL approach improves the catego-\nrization accuracy from 85.65% to 86.31% on CUB-200-2011\ndataset. We divide the compared methods into three groups due\nto the usage of object and part annotations in these methods.\n• Neither object nor part annotations are used. Nowadays,\nresearchers focus on how to get better categorization ac-\ncuracy under the weakly-supervised setting, which means\nneither object nor part annotations are used. Most of these\nmethods utilize the attention property of convolutional\nneural layers to localize the discriminative parts of object\nfor better accuracy, such as Fused CN-Nets [28], RA-\nCNN [18], PNA [8], TSC [16] and TL Atten [17]. They\nsimulate human visual attention mechanism only from\nvisual information. In our approach, we exploit visual\nand textual attention simultaneously as well as mine\nthe complementary information between them, which\nmake our proposed approach more effective and obtain a\n0.66% higher accuracy than the best performing result\n",
    "8\nSubcategory\nHeermann\nGull\n(1)A large bird with different shades of grey \nall over its body, white and black tail feathers, \nand a long sharp orange beak.\n(2)This bird is gray and black in color, with a \norange beak.\n(3)This bird has black outer retices and white \ninner retires and an orange beak.\n...\nVision\nText\nRed Legged \nKittiwake\n(1)This bird has a white head, breast and belly \nwith gray wings, red feet and thighs, and a red \nbeak.\n(2)This is a white bird with gray wings, red \nwebbed feet and a red beak.\n(3)Long bird with an orange beak and white \nfeathers with grey colored wings.\n...\nBohemian\nWaxwing\n(1)This bird is light gray with a light orange \npatch on its under-tail covets, neck and crown, \nand a black malar stripe and nape.\n(2)This is a grey bird with a red and yellow \ntail and a red face.\n(3)This bird has wings that are gray and black \nand has a red crown\n...\n(1)This flower is white and yellow in color, \nwith petals that are heart shaped.\n(2)This white color flower has the simple row \nof heart shaped petals shaded with orange \ncolor at the end.\n(3)This flower has thick heart shaped white \npetals and a very yellow star shaped center.\n...\nVision\nText\n(1)The flower has petals that are white with \nyellow centers.\n(2)This flower has large, flat white petals that \nconnect to each other and have a yellow \ncenter.\n(3)This flower is white and yellow in color, \nwith petals that are connected to each other.\n...\n(1)This flower has a yellow center surrounded \nby several large, overlapping white petals \nwith ruffled edges.\n(2)This flower is white and yellow in color, \nwith petals that are ruffled on the edges.\n( 3 ) T his pretty little flower has large white \npetals and a yellow center\n...\nSubcategory\nPrimula\nSilverbush\nTree Poppy\nCUB-200-2011\nOxford Flowers-102\nFig. 6: Some examples of vision and text in CUB-200-2011 dataset and Oxford Flowers-102 dataset.\nwhite head\nblack wings\nlong&brown neck\nyellow belly\nwhite wings\nblack beak\nFig. 7: Examples of the matching between textual patterns and visual patterns in our ﬁne-grained visual-textual pattern mining\napproach.\nof Fused CN-Nets [28]. We also compare with our\nprevious conference work, i.e. CVL [11]. We can see\nthat our VTRL approach brings improvements than CVL\nby 0.76% and 0.67% respectively on CUB-200-2011\nand Oxford Flowers-102 datasets. It is mainly because\nthat the VTRL approach exploits the textual attention to\nlocalize discriminative regions, while CVL directly uses\nthe whole textual descriptions and does not consider the\ndiscriminative regions in the images.\n• Only one of object and part annotations is used. These\nmethods utilize object annotation (i.e. bounding box) to\ntrain an object detector or learn part detectors, which are\nto learn more representative features for categorization.\nIn our approach, we utilize CAM [48] to automatically\nlocalize the object region of image, which avoids using\nobject annotation. The result of object localization can\nbe seen in Fig. 2. Even using object annotation, these\nmethods achieve lower accuracies than our proposed\nVTRL approach.\n• Both object and part annotations are used. In order\nto obtain better categorization accuracy, some methods\nutilize both object and part annotations at training phase\nas well as testing phase. However, these annotations are\nheavy labor-consuming. In our approach, we get object\nregion and discriminative parts automatically via object\nlocalization and ﬁne-grained visual-textual pattern mining\nrespectively without using any annotations. We promote\nthe categorization performance through discovering the\n",
    "9\nTABLE I: Comparisons with state-of-the-art methods on CUB-200-2011, sorted by amount of annotation used. “Bbox” and\n“Parts”indicate the object and part annotations (i.e. bounding box and parts locations) provided by the dataset.\nMethod\nTrain Annotation\nTest Annotation\nAccuracy (%)\nBbox\nParts\nBbox\nParts\nOur VTRL Approach\n86.31\nFused CN-Nets [28]\n85.65\nCVL [11]\n85.55\nRA-CNN [18]\n85.30\nPNA [8]\n84.70\nTSC [16]\n84.69\nFOAF [53]\n84.63\nLow-rank Bilinear [54]\n84.21\nSpatial Transformer [55]\n84.10\nBilinear-CNN [56]\n84.10\nMulti-grained [57]\n81.70\nAutoBD [27]\n81.60\nNAC [29]\n81.01\nPIR [58]\n79.34\nTL Atten [17]\n77.90\nMIL [59]\n77.40\nVGG-BGLm [60]\n75.90\nDense Graph Mining [61]\n60.19\nCoarse-to-Fine [62]\n√\n82.50\nPG Alignment [26]\n√\n√\n82.80\nTriplet-A (64) [63]\n√\n√\n80.70\nWebly-supervised [64]\n√\n√\n78.60\nPN-CNN [65]\n√\n√\n75.70\nPart-based R-CNN [12]\n√\n√\n73.50\nSPDA-CNN [66]\n√\n√\n√\n85.14\nDeep LAC [67]\n√\n√\n√\n84.10\nPBC [68]\n√\n√\n√\n83.70\nSPDA-CNN [69]\n√\n√\n√\n81.01\nPS-CNN [14]\n√\n√\n√\n76.20\nPN-CNN [65]\n√\n√\n√\n√\n85.40\nTABLE II: Comparisons with state-of-the-art methods on\nOxford Flowers-102.\nMethod\nAccuracy (%)\nOur VTRL Approach\n96.89\nCVL [11]\n96.21\nPBC [68]\n96.10\nNAC [29]\n95.34\nRIIR [70]\n94.01\nDeep Optimized [71]\n91.30\nSDR [71]\n90.50\nMML [72]\n89.45\nCNN Feature [73]\n86.80\nGeneralized Max Pooling [74]\n84.60\nEfﬁcient Object Detection [3]\n80.66\ndiscriminative and representative object and its parts.\nBesides, categorization results on Oxford Flowers-102\ndataset are shown in Table II, and also have the similar trend as\nCUB-200-2011 dataset, while our proposed VTRL approach\nstill keeps the best.\nE. Effects of Components in Our VTRL Approach\nIn this subsection, we conduct two baseline experiments\nto verify the separate contribution of each component in our\nproposed VTRL approach. Tables III to V show the accuracies\nof our proposed VTRL approach as well as the baseline\napproaches on CUB-200-2011 dataset at the following two\naspects.\n1) Effects of Fine-grained Visual-textual Pattern Mining\nand Object Localization: In our VTRL approach, ﬁne-grained\nTABLE III: Effects of ﬁne-grained pattern mining and object\nlocalization for visual stream.\nMethod\nAccuracy (%)\nVTRL-visual\n85.54\nVTRL-visual(w/o object)\n83.21\nVTRL-visual(w/o parts)\n84.79\nVTRL-visual(w/o object&parts)\n80.82\nTABLE IV: Effects of different components of our proposed\napproach on CUB-200-2011.\nMethod\nAccuracy (%)\nOur VTRL Approach\n86.31\nVTRL(w/o object)\n85.17\nVTRL(w/o parts)\n85.83\nVTRL(w/o object&parts)\n84.05\nvisual-textual pattern mining and object localization gener-\nate discriminative parts and object for promoting the cate-\ngorization accuracy. They make sense to the visual stream\nand then further impact whole approach. Tables III and IV\nshow the effects of ﬁne-grained visual-textual pattern mining\nand object localization to visual stream and our proposed\nVTRL approach respectively. In the tables, “object” means\nthat object localization is conducted, and “parts” means that\nﬁne-grained visual-textual pattern mining is employed. We can\nobserve that considering object localization can achieve better\ncategorization accuracy than considering ﬁne-grained visual-\ntextual pattern mining. This is because that objects contain\n",
    "10\nTABLE V: Effects of different components of our proposed\napproach on CUB-200-2011.\nMethod\nAccuracy (%)\nOur VTRL Approach\n86.31\nVTRL-textual\n81.81\nVTRL-visual\n85.54\nVTRL(only original image)\n80.82\nCo-attention [75]\n73.90\nSubcategory\nSooty\nAlbatross\n(1)This bird has wings that are grey and has a black bill.\n(2)This bird is gray in color, with a large curved beak.\n(3)This bird is white and brown in color, and has a black beak.\nVision\nText Rank List(Top3)\nCalifornia\nGull\n(1)This bird has large feet, a short yellow bill, and a black and white body.\n(2)This bird has wings that are grey and has a white belly and yellow bill.\n(3)This bird has a yellow beak as well as a white belly.\nCerulean\nWarbler\n(1)A little bird with a short, grey bill, blue crown, nape, white breast.\n(2)The bird has a white abdomen, black breast and white throat, blue specks.\n(3)This bird is blue and white in color with a black beak, and black eye rings.\nFig. 8: Some results of the textual stream.\nthe global and local features simultaneously, while discrim-\ninative parts focus subtle and local characteristics. However,\njointly considering object localization and ﬁne-grained visual-\ntextual pattern mining can further improve the categorization\naccuracy.\nFine-grained visual-textual pattern mining aims to select the\npart proposals that corresponding to the discovered textual\npatterns. The relations between part proposals and textual\npatterns ensure the discrimination and representativeness of\nselected parts. Some examples of discovered visual-textual\npatterns are shown in Fig. 7.\n2) Effectiveness of Visual-textual Representation Learning:\nWe also present the baseline experiment to verify the effec-\ntiveness of visual-textual representation learning. The results\nare shown in Table V, where “VTRL-textual” means textual\nstream, “VTRL-visual” means visual stream and “VTRL(only\noriginal image” means only a ﬁne-tuned CNN model is used.\nWe can observe that categorization result of textual stream is\npromising. From the ﬁrst line of each row in Fig. 8, we can ﬁnd\nthat textual description with the highest score always points\nout the discriminative characteristics of the object, as the\nred words shows. Combining visual and textual information\ncan further achieve more accurate categorization result, which\ndemonstrates that the two types of information are comple-\nmentary: visual information focuses on the global and local\nfeatures, and textual information further points the importance\nof these features. Fig. 9 shows some example results where the\ntextual and visual streams are complementary. Visual stream\nis effective for dealing with those images, which have few\ndiscriminative characteristics. Humans can only describe them\nin a rough way, but cannot describe them in detail, which leads\nthat the textual information carries less useful information to\ndistinguish it from other subcategories. Examples are shown\nas the right two images. Textual stream is effective for dealing\nwith those images, whose foreground and background are\nthis small bird has a dark \ngrey brown head and \nwings and a light green \nbreast and belly, the bill \nis bright orange\nsmall brown and yellow \nbird with brown wings \nand orange head, short \nbeak\na large bird, totally black, \nwith shaggy black throat \nfeathers\na bird with a small black \npointed bill and entirely \ngray feathers covering its \nbody\nFig. 9: Some example results where the textual and visual\nstreams are complementary. The left two images are rightly\ncategorized by textual stream, but wrongly categorized by\nvisual stream. The right two images are just the opposite.\nhard to be distinguished by visual stream. But they can be\ndescribed in details by text, which carries the information\nof the discriminative characteristics and be helpful for the\ncategorization. Examples are shown as the left two images.\nBesides, we also compare our VTRL approach with method\nbased on both textual and visual attention, such as Co-attention\n[75]. It only achieves the accuracy of 73.90%, which is lower\nthan our VTRL approach. It is mainly because that our VTRL\napproach discovers the ﬁne-grained visual-textual patterns,\nwhich are key hints to the ﬁne-grained categorization.\nFrom the above baseline results, the separate contribution\nof each component in our proposed VTRL approach can\nbe veriﬁed. First, object localization and ﬁne-grained pat-\ntern mining discover the discriminative and representative\ninformation of image via visual-textual attention. Second, the\ncomplementarity between visual and textual information is\nfully captured by visual-textual representation learning.\nV. CONCLUSIONS\nIn this paper, the ﬁne-grained visual-textual representation\nlearning approach has been proposed. Based on textual atten-\ntion, we employ ﬁne-grained visual-textual pattern mining to\ndiscover discriminative information for categorization through\njointly modeling vision and text with GANs. Then, visual-\ntextual representation learning jointly considers visual and\ntextual information, which preserves the intra-modality and\ninter-modality information to generate complementary ﬁne-\ngrained representation, and further improve categorization\nperformance. Experimental results on two widely-used ﬁne-\ngrained visual categorization datasets demonstrate the superi-\nority of our approach compared with state-of-the-art methods.\nAs for the future work, we will focus on the following two\naspects: First, we will attempt to extend the current two-stream\nframework into an end-to-end framework for simplifying the\nprocess. Second, we will exploit exact and effective methods\non relating textual attention and visual attention for more\naccurate discriminative parts localization as well as better\ncategorization performance.\nREFERENCES\n[1] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge\nBelongie. The caltech-ucsd birds-200-2011 dataset. 2011.\n",
    "11\n[2] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei\nLi. Novel dataset for ﬁne-grained image categorization: Stanford dogs.\nIn CVPR Workshop on Fine-Grained Visual Categorization (FGVC),\nvolume 2, 2011.\n[3] Anelia Angelova and Shenghuo Zhu.\nEfﬁcient object detection and\nsegmentation for ﬁne-grained recognition.\nIn IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), pages 811–818,\n2013.\n[4] M-E. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over\na large number of classes. In Indian Conference on Computer Vision,\nGraphics and Image Processing, Dec 2008.\n[5] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object\nrepresentations for ﬁne-grained categorization. In IEEE International\nConference on Computer Vision Workshops, pages 554–561, 2013.\n[6] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and\nAndrea Vedaldi.\nFine-grained visual classiﬁcation of aircraft.\narXiv\npreprint arXiv:1306.5151, 2013.\n[7] Chen Huang, Zhihai He, Guitao Cao, and Wenming Cao. Task-driven\nprogressive part localization for ﬁne-grained object recognition. IEEE\nTransactions on Multimedia (TMM), 18(12):2372–2383, 2016.\n[8] Xiaopeng Zhang, Hongkai Xiong, Wengang Zhou, Weiyao Lin, and\nQi Tian. Picking neural activations for ﬁne-grained recognition. IEEE\nTransactions on Multimedia (TMM), 2017.\n[9] Yan Wang, Sheng Li, and Alex C Kot. Deepbag: Recognizing handbag\nmodels. IEEE Transactions on Multimedia (TMM), 17(11):2072–2083,\n2015.\n[10] Yan Wang, Sheng Li, and Alex C Kot. On branded handbag recognition.\nIEEE Transactions on Multimedia (TMM), 18(9):1869–1881, 2016.\n[11] Xiangteng He and Yuxin Peng. Fine-grained image classiﬁcation via\ncombining vision and language. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), July 2017.\n[12] Ning Zhang, Jeff Donahue, Ross Girshick, and Trevor Darrell. Part-\nbased r-cnns for ﬁne-grained category detection. In European Confer-\nence on Computer Vision (ECCV), pages 834–849, 2014.\n[13] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich\nfeature hierarchies for accurate object detection and semantic segmenta-\ntion. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 580–587, 2014.\n[14] Shaoli Huang, Zhe Xu, Dacheng Tao, and Ya Zhang. Part-stacked cnn\nfor ﬁne-grained visual categorization. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages 1173–1182, 2016.\n[15] Xiaopeng Zhang, Hongkai Xiong, Wengang Zhou, Weiyao Lin, and\nQi Tian. Picking deep ﬁlter responses for ﬁne-grained image recognition.\nIn IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 1134–1142, 2016.\n[16] Xiangteng He and Yuxin Peng. Weakly supervised learning of part selec-\ntion model with spatial constraints for ﬁne-grained image classiﬁcation.\nIn AAAI Conference on Artiﬁcial Intelligence (AAAI), pages 4075–4081,\n2017.\n[17] Tianjun Xiao, Yichong Xu, Kuiyuan Yang, Jiaxing Zhang, Yuxin Peng,\nand Zheng Zhang. The application of two-level attention models in deep\nconvolutional neural network for ﬁne-grained image classiﬁcation. In\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR),\npages 842–850, 2015.\n[18] Jianlong Fu, Heliang Zheng, and Tao Mei. Look closer to see better:\nRecurrent attention convolutional neural network for ﬁne-grained image\nrecognition.\nIn IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), July 2017.\n[19] John Robert Anderson. Cognitive psychology and its implications. WH\nFreeman/Times Books/Henry Holt & Co, 1985.\n[20] Scott Reed, Zeynep Akata, Honglak Lee, and Bernt Schiele. Learning\ndeep representations of ﬁne-grained visual descriptions.\nIn IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), pages\n49–58, 2016.\n[21] Yuning Chai, Victor Lempitsky, and Andrew Zisserman.\nSymbiotic\nsegmentation and part localization for ﬁne-grained categorization. In\nIEEE International Conference on Computer Vision (ICCV), pages 321–\n328, 2013.\n[22] Shulin Yang, Liefeng Bo, Jue Wang, and Linda G Shapiro. Unsupervised\ntemplate learning for ﬁne-grained object recognition. In Advances in\nNeural Information Processing Systems (NIPS), pages 3122–3130, 2012.\n[23] Thomas Berg and Peter Belhumeur.\nPoof: Part-based one-vs.-one\nfeatures for ﬁne-grained categorization, face veriﬁcation, and attribute\nestimation.\nIn IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 955–962, 2013.\n[24] Lingxi Xie, Qi Tian, Richang Hong, Shuicheng Yan, and Bo Zhang.\nHierarchical part matching for ﬁne-grained visual categorization.\nIn\nIEEE International Conference on Computer Vision (ICCV), pages\n1641–1648, 2013.\n[25] Ning Zhang, Ryan Farrell, Forrest Iandola, and Trevor Darrell.\nDe-\nformable part descriptors for ﬁne-grained recognition and attribute\nprediction.\nIn IEEE International Conference on Computer Vision\n(ICCV), pages 729–736, 2013.\n[26] Jonathan Krause, Hailin Jin, Jianchao Yang, and Li Fei-Fei. Fine-grained\nrecognition without part annotations. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages 5546–5555, 2015.\n[27] Hantao Yao, Shiliang Zhang, Chenggang Yan, Yongdong Zhang, Jintao\nLi, and Qi Tian. Autobd: Automated bi-level description for scalable\nﬁne-grained visual categorization. IEEE Transactions on Image Pro-\ncessing (TIP), 27(1):10–23, 2018.\n[28] Hantao Yao, Shiliang Zhang, Yongdong Zhang, Jintao Li, and Qi Tian.\nOne-shot ﬁne-grained instance retrieval. In ACM on Multimedia Con-\nference (ACM MM), pages 342–350. ACM, 2017.\n[29] Marcel Simon and Erik Rodner. Neural activation constellations: Un-\nsupervised part model discovery with convolutional networks. In IEEE\nInternational Conference on Computer Vision (ICCV), pages 1143–1151,\n2015.\n[30] Jiawei Han, Hong Cheng, Dong Xin, and Xifeng Yan.\nFrequent\npattern mining: current status and future directions. Data Mining and\nKnowledge Discovery, 15(1):55–86, 2007.\n[31] Rakesh Agrawal, Tomasz Imieli´nski, and Arun Swami. Mining associ-\nation rules between sets of items in large databases. In ACM SIGMOD\nRecord, volume 22, pages 207–216. ACM, 1993.\n[32] Rakesh Agrawal, Ramakrishnan Srikant, et al.\nFast algorithms for\nmining association rules. In International Conference on Very Large\nData Bases (VLDB), volume 1215, pages 487–499, 1994.\n[33] Jiawei Han, Jian Pei, and Yiwen Yin. Mining frequent patterns without\ncandidate generation. In ACM SIGMOD Record, volume 29, pages 1–12.\nACM, 2000.\n[34] Yao Li, Lingqiao Liu, Chunhua Shen, and Anton van den Hengel. Mid-\nlevel deep pattern mining. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 971–980, 2015.\n[35] Hongzhi Li, Joseph G Ellis, Heng Ji, and Shih-Fu Chang. Event speciﬁc\nmultimodal pattern mining for knowledge base construction. In ACM\non Multimedia Conference (ACM MM), pages 821–830. ACM, 2016.\n[36] Richang Hong, Jinhui Tang, Hung-Khoon Tan, Chong-Wah Ngo,\nShuicheng Yan, and Tat-Seng Chua.\nBeyond search: Event-driven\nsummarization for web videos.\nACM Transactions on Multimedia\nComputing, Communications, and Applications (TOMM), 7(4):35, 2011.\n[37] Richang Hong, Lei Li, Junjie Cai, Dapeng Tao, Meng Wang, and\nQi Tian.\nCoherent semantic-visual indexing for large-scale image\nretrieval in the cloud. IEEE Transactions on Image Processing (TIP),\n26(9):4128–4138, 2017.\n[38] Harold Hotelling. Relations between two sets of variates. Biometrika,\n28(3/4):321–377, 1936.\n[39] Herv´e Bredin and G´erard Chollet.\nAudio-visual speech synchrony\nmeasure for talking-face identity veriﬁcation.\nIn IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP),\nvolume 2, pages II–233, 2007.\n[40] David R Hardoon, Sandor Szedmak, and John Shawe-Taylor. Canonical\ncorrelation analysis: An overview with application to learning methods.\nNeural Computation, 16(12):2639–2664, 2004.\n[41] Benjamin Klein, Guy Lev, Gil Sadeh, and Lior Wolf. Associating neural\nword embeddings with deep image representations using ﬁsher vectors.\nIn IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 4437–4446, 2015.\n[42] Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak\nLee, and Andrew Y Ng. Multimodal deep learning. In International\nConference on Machine Learning (ICML), pages 689–696, 2011.\n[43] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory.\nNeural Computation, 9(8):1735–1780, 1997.\n[44] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolu-\ntional networks for text classiﬁcation. In Advances in Neural Information\nProcessing Systems (NIPS), pages 649–657, 2015.\n[45] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt\nSchiele, and Honglak Lee. Generative adversarial text to image synthe-\nsis. In International Conference on Machine Learning (ICML), pages\n1060–1069, 2016.\n[46] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014.\n[47] Jasper RR Uijlings, Koen EA van de Sande, Theo Gevers, and\nArnold WM Smeulders. Selective search for object recognition. In-\nternational Journal of Computer Vision (IJCV), 104(2):154–171, 2013.\n",
    "12\n[48] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio\nTorralba. Learning deep features for discriminative localization. In IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), pages\n2921–2929, 2016.\n[49] Nobuyuki Otsu.\nA threshold selection method from gray-level his-\ntograms. IEEE Transactions on Systems, Man, and Cybernetics, 9(1):62–\n66, 1979.\n[50] Karen Simonyan and Andrew Zisserman.\nVery deep convolu-\ntional networks for large-scale image recognition.\narXiv preprint\narXiv:1409.1556, 2014.\n[51] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-\nFei.\nImagenet: A large-scale hierarchical image database.\nIn IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), pages\n248–255, 2009.\n[52] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\nDragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew\nRabinovich. Going deeper with convolutions. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), pages 1–9, 2015.\n[53] Xiaopeng Zhang, Hongkai Xiong, Wengang Zhou, and Qi Tian. Fused\none-vs-all features with semantic alignments for ﬁne-grained visual cat-\negorization. IEEE Transactions on Image Processing (TIP), 25(2):878–\n892, 2016.\n[54] Shu Kong and Charless Fowlkes. Low-rank bilinear pooling for ﬁne-\ngrained classiﬁcation.\nIn IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), July 2017.\n[55] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.\nSpatial\ntransformer networks. In Advances in Neural Information Processing\nSystems (NIPS), pages 2017–2025, 2015.\n[56] Tsung-Yu Lin, Aruni RoyChowdhury, and Subhransu Maji.\nBilinear\ncnn models for ﬁne-grained visual recognition. In IEEE International\nConference on Computer Vision (ICCV), pages 1449–1457, 2015.\n[57] Dequan Wang, Zhiqiang Shen, Jie Shao, Wei Zhang, Xiangyang Xue,\nand Zheng Zhang.\nMultiple granularity descriptors for ﬁne-grained\ncategorization. In International Conference on Computer Vision (ICCV),\npages 2399–2406, 2015.\n[58] Yu Zhang, Xiu-Shen Wei, Jianxin Wu, Jianfei Cai, Jiangbo Lu, Viet-Anh\nNguyen, and Minh N Do. Weakly supervised ﬁne-grained categorization\nwith part-based image representation.\nIEEE Transactions on Image\nProcessing (TIP), 25(4):1713–1725, 2016.\n[59] Zhe Xu, Dacheng Tao, Shaoli Huang, and Ya Zhang. Friend or foe:\nFine-grained categorization with weak supervision. IEEE Transactions\non Image Processing (TIP), 26(1):135–146, 2017.\n[60] Feng Zhou and Yuanqing Lin.\nFine-grained image classiﬁcation by\nexploring bipartite-graph labels. arXiv preprint arXiv:1512.02665, 2015.\n[61] Luming Zhang, Yang Yang, Meng Wang, Richang Hong, Liqiang Nie,\nand Xuelong Li. Detecting densely distributed graph patterns for ﬁne-\ngrained image categorization. IEEE Transactions on Image Processing\n(TIP), 25(2):553–565, 2016.\n[62] Hantao Yao, Shiliang Zhang, Yongdong Zhang, Jintao Li, and Qi Tian.\nCoarse-to-ﬁne description for ﬁne-grained visual categorization. IEEE\nTransactions on Image Processing (TIP), 25(10):4858–4872, 2016.\n[63] Yin Cui, Feng Zhou, Yuanqing Lin, and Serge Belongie. Fine-grained\ncategorization and dataset bootstrapping using deep metric learning with\nhumans in the loop. arXiv preprint arXiv:1512.05227, 2015.\n[64] Zhe Xu, Shaoli Huang, Ya Zhang, and Dacheng Tao. Webly-supervised\nﬁne-grained visual categorization via deep domain adaptation.\nIEEE\nTransactions on Pattern Analysis and Machine Intelligence (TPAMI),\n2016.\n[65] Steve Branson, Grant Van Horn, Serge Belongie, and Pietro Perona.\nBird species categorization using pose normalized deep convolutional\nnets. arXiv preprint arXiv:1406.2952, 2014.\n[66] Han Zhang, Tao Xu, Mohamed Elhoseiny, Xiaolei Huang, Shaoting\nZhang, Ahmed Elgammal, and Dimitris Metaxas. Spda-cnn: Unifying\nsemantic part detection and abstraction for ﬁne-grained recognition.\npages 1143–1152, 2016.\n[67] Di Lin, Xiaoyong Shen, Cewu Lu, and Jiaya Jia.\nDeep lac: Deep\nlocalization, alignment and classiﬁcation for ﬁne-grained recognition. In\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR),\npages 1666–1674, 2015.\n[68] Chao Huang, Hongliang Li, Yurui Xie, Qingbo Wu, and Bing Luo.\nPbc: Polygon-based classiﬁer for ﬁne-grained categorization.\nIEEE\nTransactions on Multimedia (TMM), 19(4):673–684, 2017.\n[69] Han Zhang, Tao Xu, Mohamed Elhoseiny, Xiaolei Huang, Shaoting\nZhang, Ahmed Elgammal, and Dimitris Metaxas. Spda-cnn: Unifying\nsemantic part detection and abstraction for ﬁne-grained recognition. In\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR),\npages 1143–1152, 2016.\n[70] Lingxi Xie, Jingdong Wang, Weiyao Lin, Bo Zhang, and Qi Tian.\nTowards reversal-invariant image representation. International Journal\nof Computer Vision (IJCV), 123(2):226–250, 2017.\n[71] Hossein Azizpour, Ali Sharif Razavian, Josephine Sullivan, Atsuto Maki,\nand Stefan Carlsson. From generic to speciﬁc deep representations for\nvisual recognition. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition Workshops, pages 36–45, 2015.\n[72] Qi Qian, Rong Jin, Shenghuo Zhu, and Yuanqing Lin. Fine-grained vi-\nsual categorization via multi-stage metric learning. In IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages 3716–3724,\n2015.\n[73] Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan\nCarlsson. Cnn features off-the-shelf: an astounding baseline for recogni-\ntion. In IEEE Conference on Computer Vision and Pattern Recognition\nWorkshops, pages 806–813, 2014.\n[74] Naila Murray and Florent Perronnin. Generalized max pooling. In IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), pages\n2473–2480, 2014.\n[75] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical\nquestion-image co-attention for visual question answering. In Advances\nIn Neural Information Processing Systems, pages 289–297, 2016.\n"
  ],
  "full_text": "1\nFine-grained Visual-textual Representation Learning\nXiangteng He and Yuxin Peng\nAbstract—Fine-grained visual categorization is to recognize\nhundreds of subcategories belonging to the same basic-level\ncategory, which is a highly challenging task due to the quite\nsubtle and local visual distinctions among similar subcategories.\nMost existing methods generally learn part detectors to discover\ndiscriminative regions for better categorization performance.\nHowever, not all parts are beneﬁcial and indispensable for visual\ncategorization, and the setting of part detector number heavily\nrelies on prior knowledge as well as experimental validation. As is\nknown to all, when we describe the object of an image via textual\ndescriptions, we mainly focus on the pivotal characteristics, and\nrarely pay attention to common characteristics as well as the\nbackground areas. This is an involuntary transfer from human\nvisual attention to textual attention, which leads to the fact that\ntextual attention tells us how many and which parts are dis-\ncriminative and signiﬁcant to categorization. So textual attention\ncould help us to discover visual attention in image. Inspired\nby this, we propose a ﬁne-grained visual-textual representation\nlearning (VTRL) approach, and its main contributions are: (1)\nFine-grained visual-textual pattern mining devotes to discovering\ndiscriminative visual-textual pairwise information for boosting\ncategorization performance through jointly modeling vision and\ntext with generative adversarial networks (GANs), which au-\ntomatically and adaptively discovers discriminative parts. (2)\nVisual-textual representation learning jointly combines visual and\ntextual information, which preserves the intra-modality and inter-\nmodality information to generate complementary ﬁne-grained\nrepresentation, as well as further improves categorization perfor-\nmance. Comprehensive experimental results on the widely-used\nCUB-200-2011 and Oxford Flowers-102 datasets demonstrate the\neffectiveness of our VTRL approach, which achieves the best\ncategorization accuracy compared with state-of-the-art methods.\nIndex Terms—Fine-grained visual categorization, ﬁne-grained\nvisual-textual pattern mining, visual-textual representation learn-\ning.\nI. INTRODUCTION\nF\nINE-GRAINED visual categorization aims to recognize\nsimilar subcategories in the same basic-level category. It\nis one of the most challenging and signiﬁcant open problems\nin multimedia and computer vision areas, which has achieved\ngreat progress as well as attracted extensive attention of\nacademia and industry in recent years. The progress incarnates\nin three aspects: (1) More ﬁne-grained domains have been\ncovered, such as animal species [1], [2], plant breeds [3],\n[4], car types [5] and aircraft models [6]. (2) Methodologies\nof ﬁne-grained visual categorization have achieved promising\nperformance in recent years [7]–[11], due to the application\nof deep neural networks (DNNs). (3) Some information tech-\nnology companies, such as Microsoft and Baidu, begin to\nThe authors are with the Institute of Computer Science and Technology,\nPeking University, Beijing 100871, China. Corresponding author: Yuxin Peng\n(e-mail: pengyuxin@pku.edu.cn).\nSmall variance among different subcategories\nArtic Tern\nCaspian Tern\nLarge variance in the same subcategory\nCommon Tern\nSalty Backed Gull\nSalty Backed Gull\nSalty Backed Gull\nFig. 1: Examples from CUB-200-2011 dataset [1]. Note that\nﬁne-grained visual categorization is a technically challenging\ntask even for humans to recognize these subcategories, due\nto small variances among different subcategories and large\nvariances in the same subcategory.\nturn ﬁne-grained visual categorization technologies into their\napplications1 2.\nFine-grained visual categorization lies in the continuum\nbetween basic-level visual categorization (e.g. object recogni-\ntion) and identiﬁcation of individuals (e.g. face recognition).\nIts main challenges can be summarized as the following\ntwo aspects: (1) Variances among similar subcategories are\nsubtle and local, because they belong to the same genus. (2)\nVariances in the same subcategory are large and diverse, due to\ndifferent poses and views, as well as for animals or plants also\nbecause of different living environments and growth periods.\nFor example, as shown in Fig. 1, the images of “Artic Tern”\nand “Caspian Tern” look similar in global appearance, but\nthe images of “Salty Backed Gull” look different in the pose,\nview and feather color. So it is hard for a person without\nprofessional knowledge to recognize them.\nThese subcategories can be distinguished by the subtle and\nlocal variances of the discriminative parts. It is crucial for\nﬁne-grained visual categorization to localize the object and its\ndiscriminative parts. Researchers generally adopt a two-stage\ncategorization pipeline: the ﬁrst stage is to localize the object\nor its discriminative parts, and the second is to extract their\nfeatures to categorize the subcategory. For example, Zhang\net al. [12] utilize R-CNN [13] with geometric constraints to\ndetect object and its parts ﬁrst, and then extract the features\nof the object and its parts, ﬁnally train one-versus-all linear\nSVMs for categorization. However, not all the parts are\nbeneﬁcial and indispensable for ﬁne-grained categorization.\n1https://www.microsoft.com/en-us/research/project/ﬂowerreco-cn/\n2http://image.baidu.com/?fr=shitu/\narXiv:1709.00340v4  [cs.CV]  20 Feb 2019\n\n\n2\nThe conclusive distinctions among subcategories generally\nlocate at a few speciﬁc parts, such as the red beak or the\nblack tail. So the categorization performance depends on the\nnumber of part detectors and whether the detected parts are\ndiscriminative or not. However, mainstream methods generally\nset the detector number due to their prior knowledge or the\nexperimental validation, which is highly empirical and limited.\nFor example, when the number of part detectors applied in the\nexperiments increase from eight to ﬁfteen, the performance of\nﬁne-grained categorization declines, as reported in [14]. Six\npart detectors are applied by Zhang et al. [15] to achieve the\nbest categorization accuracy. He and Peng [16] applies two\ndiscriminative parts for ﬁne-grained categorization. They are\nlimited in ﬂexibility, and hard to generalize.\nTherefore, it is signiﬁcant to automatically learn how many\nand which parts really make sense to ﬁne-grained visual\ncategorization. When human beings see two images of two\ndifferent subcategories, human visual attention mechanism\nplays an important role in focusing on the pivotal distinctions\nbetween them. Inspired by this, researchers begin to apply hu-\nman visual attention mechanism in their works, aiming to ﬁnd\nthe most discriminative characteristics for categorization. Xiao\net al. [17] propose a two-level attention model (TL Atten), in\nwhich object-level attention selects relevant image proposals to\na certain object, and part-level attention selects relevant image\nproposals to the discriminative parts of the object. Fu et al. [18]\npropose a recurrent attention convolutional neural network\n(RA-CNN) to recursively learn discriminative region attention\nand region-based feature representation. These works simulate\nhuman visual attention mechanism to ﬁnd discriminative parts\nfor categorization from visual information.\nAttention is the behavioral and cognitive process of se-\nlectively concentrating on a discrete aspect of information,\nwhether deemed subjective or objective, while ignoring other\nperceivable information [19]. As is known to all, when human\nbeings give the interpretation of the visual data by textual\ndescriptions, they tend to indicate how many and which\nparts are distinguishing from other subcategories. These words\ndescribing the part attributes are regarded as textual attention,\nwhich generally appears frequently in the textual descriptions.\nThis is an involuntary transfer from human visual attention to\ntextual attention. In this transfer process, common character-\nistics of object and background areas are ignored naturally.\nTextual attention can be obtained by discovering the frequent\nitem sets in the textual descriptions, which point out the\ndiscriminative parts of the subcategory. From Fig. 2, we can\nsee that the frequent item sets contain “red break”, which is\na discriminative characteristic that distinguishes “Heermann\nGull” from “Red Legged Kittiwake”.\nTherefore, how to exactly relate textual attention to vi-\nsual attention and mine the discriminative parts are pivotal\nto ﬁne-grained visual categorization. This paper proposes\na ﬁne-grained visual-textual representation learning (VTRL)\napproach, and its main contributions are:\n• Fine-grained visual-textual pattern mining devotes to\ndiscovering discriminative visual-textual parts for cat-\negorization by jointly modeling vision and text with\ngenerative adversarial networks (GANs). Different from\nSubcategory\nHeermann\nGull\n(1)A large bird with different shades of grey \nall over its body, white and black tail feathers, \nand a long sharp orange beak.\n(2)This bird is gray and black in color, with a \norange beak.\n(3)This bird has black outer retices and white \ninner retires and an orange beak.\n...\nVision\nText\nRed Legged \nKittiwake\n(1)This bird has a white head, breast and belly \nwith gray wings, red feet and thighs, and a red \nbeak.\n(2)This is a white bird with gray wings, red \nwebbed feet and a red beak.\n(3)This bird has a white head, nape, breast and \nbelly, with gray wings, the underside of which \nare black-tipped.\n...\nFig. 2: Examples of visual and textual attentions. The images\ncome from CUB-200-2011 dataset [1], and text are collected\nby Reed et al. [20] through Amazon Mechanical Turk (AMT)\nplatform.\nexisting methods, the localized discriminative parts in this\npaper could not only tell us how many and which parts\nare signiﬁcant for categorization, but also which attributes\nof parts are distinguishing from other subcategories. The\npart number is determined automatically and adaptively\nby textual attention.\n• Visual-textual representation learning is proposed to\ncombine visual and textual information. Visual stream\nfocuses on the locations of the discriminative parts,\nwhile textual stream focuses on the discrimination of the\nregions. It preserves the intra-modality and inter-modality\ninformation to generate complementary ﬁne-grained rep-\nresentation, as well as further improves categorization\naccuracy.\nOur previous conference paper CVL [11] proposes a two-\nstream model combining vision and language for learning\nthe ﬁne-grained representation. Vision stream learns deep\nrepresentations from visual information and language stream\nutilizes textual information to encode salient visual aspects\nfor distinguishing subcategories. The main differences between\nthe proposed VTRL approach and CVL can be summarized\nas the following three aspects: (1) Our VTRL approach\nemploys textual pattern mining to localize textual attention\nfor exploiting the human visual attention transferred into\ntextual information, which indicates how many and which\nparts are signiﬁcant and indispensable for categorization.\nWhile CVL directly utilizes the whole textual information,\ndoes not mine ﬁne-grained textual attention information. (2)\nOur VTRL approach employs visual pattern mining based on\ndiscovered textual patterns to localize discriminative parts, so\nthat discriminative parts and objects are both exploited to learn\nmulti-grained and multi-level representations for boosting ﬁne-\ngrained categorization. While CVL only exploits the objects,\nwhich ignores the complementary and semantic ﬁne-grained\nclues provided by the discriminative parts. (3) Our VTRL\napproach employs ﬁne-grained visual-textual pattern mining\nto discover the discriminative and signiﬁcant visual-textual\npairwise information via jointly modeling vision and text with\n\n\n3\nGANs, which mines the correlation between textual and visual\nattention. While CVL only combines vision and text, ignoring\nto exploit their visual and textual attention, as well as their\ncorrelation. Compared with state-of-the-art methods on two\nwidely-used ﬁne-grained visual categorization datasets, our\nVTRL approach achieves the best categorization accuracy.\nThe remainder of this paper is organized as follows: We\nbrieﬂy review the related works in Section II. In Section III our\nproposed VTRL approach is presented in detail. Then Section\nIV reports the experimental results and analyses. Finally,\nSection V concludes this paper.\nII. RELATED WORK\nIn this section, we brieﬂy review the related works of\nﬁne-grained visual categorization, frequent pattern mining and\nmulti-modal analysis.\nA. Fine-grained Visual Categorization\nSince the discriminative regions of image is crucial for\nﬁne-grained visual categorization, most existing methods [12],\n[17] ﬁrst localize the discriminative regions of image, such as\nthe object and its parts, and then extract their discriminative\nfeatures for ﬁne-grained categorization. Some methods directly\nuse the annotations of the object [21], [22] and parts [23],\n[24] to localize the discriminative regions. However, it is not\navailable to obtain the annotations in practical applications,\nsome researchers begin to use the annotations of the object\nand parts only in the training phase. Zhang et al. [25] propose\nthe Deformable Part-based Model (DPM) to localize the\ndiscriminative regions with the object and part annotations\nas the supervised information in the training phase. Further\nmore, PG Alignment [26] is proposed to train part detectors\nonly with object annotation, and localize the discriminative\nparts in an automatic manner in the testing phase.\nOnly using object annotation is still not promising in the\npractical applications. Recently, some works [17], [27], [28]\nare proposed to localize the discriminative regions in a weakly-\nsupervised manner, which means that neither object nor part\nannotations are used in both training and testing phases. Xiao\net al. [17] combine the object and part level attentions to select\nthe discriminative image proposals, which is the ﬁrst work to\nlocalize the discriminative regions without using object and\npart annotations. Yao et al. [27] also propose to combine the\ntwo complementary object-level and part-level visual descrip-\ntions for better performance. A neural activation constellation\n(NAC) part model [29] is proposed to train part detectors with\nconstellation model. He and Peng [16] integrate two spatial\nconstraints to select more discriminative proposals and achieve\nbetter categorization accuracy. The aforementioned methods\nmostly set the detector number due to the prior knowledge or\nexperimental validation, which is highly limited in ﬂexibility\nand difﬁcult for generalizing to the other domains. Therefore,\nwe attempt to automatically learn how many and which parts\nreally make sense to categorization via ﬁne-grained visual-\ntextual pattern mining.\nB. Frequent Pattern Mining\nFrequent patterns are itemsets, subsequences, or substruc-\ntures that appear in a data set with frequency no less than a\nuser-speciﬁed threshold [30]. For example, diaper and beer\nappear frequently together in sales data of a supermarket,\nwhich is a frequent pattern. Frequent pattern mining is ﬁrst\nproposed by Agrawal et al. [31] for market basket analysis.\nAgrawal and Strikant propose Apriori algorithm [32] to mine\nfrequent patterns in a large transaction database. For textual\nmining, frequent patterns may be sequential patterns, frequent\nitemsets, or multiple grams. While for visual mining, frequent\npatterns may be middle-level feature representation or high-\nlevel semantic representation. Han et al. [33] propose to mine\nvisual patterns using low-level features. Li et al. [34] propose\nto combine CNN features and association rule mining for dis-\ncovering visual patterns. Li et al. [35] propose a novel multi-\nmodal pattern mining method, which takes textual pattern and\nvisual pattern into consideration at the same time. In this\npaper, we ﬁrst utilize Apriori algorithm to discover the textual\npatterns, and then employ generative adversarial networks\n(GANs) to mine the relationships between part proposals\nand textual patterns for better categorization accuracy, which\ndiscovers visual and textual patterns at the same time as well\nas mines the intrinsic correlation between them.\nC. Multi-modal Analysis\nNowadays, multi-modal data, e.g. image, text, video and\naudio, has been widely available on the Internet. They con-\ntains different kinds of information, which are complemen-\ntary to help achieving comprehensive results in many real-\nworld applications. So it is signiﬁcant to learn multi-modal\nrepresentation for boosting the signal-modal tasks [36], [37].\nCanonical correlation analysis (CCA) [38] is proposed to learn\nlinear projection matrices, which project features of different\nmodalities into the common space and obtain the common\nrepresentation. It is widely used for modeling multi-modal\ndata [39]–[41]. Zhai et al. propose the joint representation\nlearning method (JRL) to learn projection matrices considering\nthe semantic and correlation information. Due to the advances\nof deep learning, deep learning based methods have been pro-\nposed to boost the performance of multi-modal representation\nlearning. Ngiam et al. [42] propose the bimodal autoencoders\n(Bimodal AE) to model multi-modal data via minimizing the\nreconstruction error, and learn a shared representation across\nmodalities.\nRecently, image and video captioning, which are types\nof multi-modal analysis, have achieved great progress. Long\nShort-Term Memory (LSTM) [43] and character-based con-\nvolutional networks [44] are widely used in image and video\ncaptioning. The architecture of Convolutional and Recurrent\nNetworks (CNN-RNN) is widely used in image and video\ncaptioning, and achieves great performance. In this paper, we\napply the extension of Convolutional and Recurrent Networks\n(CNN-RNN) to learn a visual semantic embedding. In this\npaper, we bring the multi-modal representation learning into\nﬁne-grained visual categorization to jointly modeling vision\nand text for boosting the performance.\n\n\n4\nbrown bird with a yellow pointed beak \nConvolutional \nencoding\nSequential \nencoding\nTextual stream\nVisual stream\nCNN\nVisual \nscore\nTextual\nscore\nC\nO\nN\nV\nW1\nW2\nWn\nGAP\n...\n...\nC\nO\nN\nV\nInputs\nOutputs\nBlack Tern\nCactus Wren\nDownyWoodpecker\nLaysan Albatross\nPart proposals\nLocalized objects\nFine-grained \nVisual-textual \nPattern Mining\nWhite head Black wings\nDiscriminative \nvisual-textual parts\nFig. 3: Overview of our VTRL approach.\nIII. OUR VTRL APPROACH\nA. Overview of Our VTRL Approach\nOur approach is based on a very promising and interesting\nintuition: textual descriptions can point out the discrimina-\ntive characteristics of images, and provide complementary\ninformation with visual information. Therefore, we propose\na ﬁne-grained visual-textual representation learning (VTRL)\napproach, which takes the advantages of visual and textual\ninformation jointly as well as exploits the intrinsic correlation\nbetween them. Fig. 3 shows our VTRL approach. First, we\nconduct ﬁne-grained visual-textual pattern mining to discover\nthe discriminative visual-textual parts as shown in Fig. 4. Then,\nwe localize the object region of image to boost the visual\nanalysis. Finally, we propose a visual-textual representation\nlearning approach to jointly model visual and textual streams\nfor better categorization accuracy.\nB. Fine-grained Visual-textual Pattern Mining\nSince human visual attention is described into the form of\ntextual descriptions, we ﬁrst conduct textual pattern mining to\ndiscover the textual attention, which indicates the distinguish-\ning part attributes from other subcategories, such as the shape,\nsize and color of the part. Then, we conduct visual pattern\nmining to localize the discriminative parts corresponding to\nthe textual patterns discovered by textual pattern mining. The\noverview of our ﬁne-grained visual-textual pattern mining\napproach is shown in Fig. 4. In the following paragraphs,\nwe describe the ﬁne-grained visual-textual pattern mining\napproach from three aspects: 1) deﬁnition of pattern mining, 2)\ntextual pattern mining and 3) visual pattern mining via GANs.\n1) Deﬁnition of Pattern Mining: We ﬁrst introduce the basic\ndeﬁnitions for pattern mining. Assume that there is a set of\nn items, which is denoted as X = {x1, x2, ..., xn}, and the\ntransaction T is a subset of X, i.e. T ⊆X. We also deﬁne\nBlack wings\nWhite head\nText \ntransaction\nAssociation \nrule mining\nTextual pattern 1\nTextual pattern 2\n{0,1}\nText encoder\nPart proposal\nDiscriminator network\nthis bird has a white head, \nthe bill is long and curved, \nwith a white belly and \nblack wings.\nthis bird is white in color \nwith a black beak, and \nwhite eye rings.\nthis large bird has long bill, \na white breast, belly & head \nand a black back & wings\nthis bird is nearly all white \nwith gray tarsus and feet.\nbird has grey and white \nbeak with grey rectrices and \nthe rest of the bird is white.\nthis large bird has a white \nhead and belly, white wings \nwith black on the ends of \nthe feathers, and a white \ntail.\nthis bird has wings that are \nblack and white and has a \nlong bill\nTextual pattern mining\nVisual pattern mining\nFig. 4: Overview of our ﬁne-grained visual-textual pattern min-\ning approach. {0, 1} denotes the output of the discriminator in\nGANs, which indicates whether the input part proposal meets\nthe input textual pattern.\na transaction database D = {T1, T2, ..., TK} that contains K\ntransactions. Our goal is to discover a particular subset T ∗of\ntransactions database X, which can predict the presence of\nsome target item y ∈Ty, and T ∗⊂Ty as well as y ∩T ∗= ∅.\nT ∗refers to frequent itemset in pattern mining literature. The\nsupport of T ∗denotes how often T ∗appears in D and its\ndeﬁnition is as follow:\nsupp(T ∗) = |{Ty|T ∗⊆Ty, Ty ∈D}|\nK\n(1)\nAn association rule T ∗→y deﬁnes a relationship between\nT ∗and a certain item y. Therefore, we aim to ﬁnd patterns\nthat appear in a transaction there is a high likelihood that y.\nWe deﬁne the conﬁdence as follow:\nconf(T ∗→y) = supp(T ∗∪y)\nsupp(T ∗)\n(2)\n2) Textual Pattern Mining: In this paper, we devote to\ndiscovering textual patterns, which contain the human visual\nattention information. First, we remove stop words and punc-\ntuations from each textual description. Then we select the\nwords, which appear in at least 10 textual descriptions in\nour dataset. Build a vocabulary with these selected words,\nwhich is used for generating transactions. It is noted that\nthere are no duplicate words in the vocabulary. In order to\ngenerate transaction for each textual description, we map\neach word back to its corresponding word in the vocabulary,\nthen include that corresponding word index in the transaction.\nAfter obtaining the transactions, we perform association rule\nmining to ﬁnd the words that frequently appear in textual\ndescriptions, which also means that these words can represent\nthe characteristics of this subcategory. Speciﬁcally, we utilize\nthe Apriori algorithm [32] to ﬁnd a set of patterns P through\n\n\n5\nassociation rule mining. Each pattern p ∈P must satisfy the\nfollowing criteria:\nsupp(p) > suppmin\n(3)\nconf(p →c) > confmin\n(4)\nwhere suppmin and confmin are thresholds for the support\nvalue and conﬁdence value respectively, and c means the\nimage-level subcategory label. After association rule mining,\neach discovered pattern p contains a set of words.\nWe want to ﬁnd some patterns that point out the discrimi-\nnative parts of the image, which have the semantic meaning.\nTherefore, we conduct distance constraint on association rule\nmining as follow:\ndis(wi, wj) < dismin\n(5)\nwhere wi and wj mean the i-th and j-th words in the same\ntextual description, and dis(·) means the interval between the\ni-th and j-th words. The distance function ensures that the\ndiscovered patterns have the semantic meaning. The actual\nthreshold in distance function is set to 4 in the experiments,\nwhich is set by the cross-validation method following [12].\nFinally, we discover a set of patterns P, i.e. textual attention\nin the textual descriptions, which contains the information of\nhuman visual attention.\n3) Visual Pattern Mining via GANs: After obtaining the\ntextual attention, we devote to mining the relationship between\nvisual and textual attention, i.e. localize the discriminative\nparts of images via the guidance of textual attention. Due to\nthe great progress made by generative adversarial networks\n(GANs), which can generate images based on textual infor-\nmation. In this paper, we employ GANs to break through the\ngap between visual and textual information, and localize the\ndiscriminative parts corresponding to the discovered textual\npatterns. Speciﬁcally, the network architecture follows GAN-\nCLS [45]. The original training images and their annotated\ntextual descriptions are used to train the GAN-CLS model.\nWe take the alternating strategy to update the generator and\ndiscriminator networks, and use ADAM solver [46] to train\nthe model. The training settings, such as learning rate and\nmomentum, are conﬁgured following GAN-CLS [45].\nIt is noted that part proposals and textual patterns are not\nused to train the GAN-CLS model, as it is unavailable to\nobtain their matching labels. Reed et al. [20] point out that\nthe text embedding based on textual descriptions covers the\nvisual attributes, i.e. textual patterns, such as shape, size and\ncolor of the part. GAN-CLS follows [20] to obtain a visually-\ndiscriminative vector representation of text descriptions, by\nusing deep convolutional and recurrent text encoders that learn\na correspondence function with images. Even using images\nand textual descriptions in the training phase, GAN-CLS can\nstill learn the correlation between the part proposals and\ntextual patterns. As described in GAN-CLS, the generator\nhas learned to generate plausible images, and also learned\nto align them with the conditioning information, and likewise\nthe discriminator must learn to evaluate whether samples from\ngenerator meet this conditioning constraint. So we ﬁrst train\nGAN-CLS on the datasets in our paper, and then apply the\ndiscriminator in GAN-CLS to select the corresponding part\nproposals for the speciﬁc textual patterns, where we take one\npart proposal as the sample input, and one textual pattern as the\nconditioning constraint input. The selected part proposals con-\ntain discriminative information that helps to distinguish similar\nsubcategories. In the following paragraphs, we introduce the\nvisual pattern mining approach in details.\nFirst, for each image we perform bottom-up process to\ngenerate part proposals. In this paper, we utilize selective\nsearch method [47] to generate 1000 part proposals for each\nimage. Then we take the part proposals and discovered textual\npatterns as the inputs of discriminator network, to relate the\ndiscovered textual patterns with the corresponding part pro-\nposals. For each part proposal, discriminator network outputs\na score vector that refers to the degree of correlations between\npart proposal and textual patterns. We select the part proposal\nwith highest score for each textual pattern, which is one of\nthe most discriminative parts for categorization. They will be\nutilized as the inputs of visual-textual representation learning.\nC. Object Localization\nFor better categorization performance, we apply an au-\ntomatic object localization method based on CAM [48]to\nlocalize the object in a weakly-supervised manner, which\nmeans that neither object nor part annotations are used in both\ntraining and testing phases. Through CAM, we can generate\na subcategory activation map Mc for each subcategory c, in\nwhich the spatial value is calculated as follow:\nMc(x, y) =\nX\nk\nwc\nkfk(x, y)\n(6)\nwhere fk(x, y) denotes the activation of unit k in the last con-\nvolutional layer at spatial location (x, y), and wc\nk is the weight\ncorresponding to subcategory c for unit k. The subcategory\nlabel information is not available in testing phase, so we set\nsubcategory c by the predicted subcategory. After obtaining the\nactivation map for each image, we conduct OTSU algorithm\n[49] to binarize the image and take the bounding box that\ncovers the largest connected area as the localization of object.\nThe localized object is utilized as the inputs of visual-textual\nrepresentation learning along with the localized discriminative\nparts via ﬁne-grained visual-textual pattern mining. Examples\nof object localization results are shown in Fig. 5. It is noted\nthat we use a variant of VGGNet [50] as CAM following\n[48]. In order to get a higher spatial resolution, the layers\nafter conv5 3 are removed, resulting in a mapping resolution\nof 14 × 14. Besides, a convolutional layer of size 3 × 3, stride\n1, pad 1 with 1024 neurons is added, followed by a global\naverage pooling layer and a softmax layer.\nD. Visual-textual Representation Learning\nSince visual content and textual descriptions provide com-\nplementary information, we jointly model them with a two-\nstream model for learning visual-textual representations to\nboost the categorization performance. The two-stream model\nconsists of: 1) visual stream and 2) textual stream.\n\n\n6\nFig. 5: Examples of object localization results in this paper.\nThe red rectangles indicate the ground truth object annotations,\ni.e. bounding boxes of objects, and the yellow rectangles\nindicate the object regions localized by our approach.\n1) Visual Stream: We apply CNN model, e.g. VGGNet [50]\nin our experiments, as the visual categorization function f. The\nCNN model is pre-trained on the ImageNet 1 K dataset [51],\nand then ﬁne-tuned on the ﬁne-grained visual categorization\ndataset.\nFor a given image I, we ﬁrst conduct object localization\nand ﬁne-grained visual-textual pattern mining respectively to\nobtain the object b and its n discriminative parts Pa =\n{Pa1, Pa2, ..., Pan}. Then the object and discriminative parts\nare cropped from the original image, and saved as images\nIb and IP a = {IP a1, IP a2, ..., IP an}. We feed the original\nimage I and its object image Ib as well as its part images\nIP a = {IP a1, IP a2, ..., IP an} to the CNN model to obtain the\npredicted visual scores. For the part images, we calculate their\nmean value as the ﬁnal part prediction. Finally, we calculate\nthe weighted mean of original prediction, object prediction and\npart prediction as the ﬁnal visual prediction.\n2) Textual Stream: In textual stream, we aim to measure\nthe similarity between visual and textual information. We ﬁrst\napply the deep structured joint embedding method [20] to\njointly embed vision (i.e. images) and text (i.e. natural lan-\nguage descriptions for images), which learns a compatibility\nfunction of vision and text.\nWe\ndeﬁne\nthe\ntraining\ndata\nas\nD\n=\n(vn, tn, yn), n = 1, ..., N, where v ∈V and t ∈T denote the\nvision and text, and y ∈Y denotes their subcategory labels.\nThen we apply the empirical risk to learn the visual and\ntextual classiﬁer functions fv : V →Y and ft : T →Y as\nfollows:\n1\nN\nN\nX\nn=1\n∆(yn, fv(vn)) + ∆(yn, ft(tn))\n(7)\nwhere ∆: y × y →R is the 0-1 loss and\nfv(v) = arg max\ny∈Y Et∼T (y)[F(v, t)]\n(8)\nft(t) = arg max\ny∈Y Ev∼V (y)[F(v, t)]\n(9)\nThe compatibility function F : V ×Y →R is deﬁned as the\ninner product of features from the learnable encoder functions\nas follows:\nF(v, t) = θ(v)T φ(t)\n(10)\nwhere θ(v) is the visual encoder, and φ(t) is the textual\nencoder. The visual and textual encoders are implemented\nby GoogleNet [52] and Convolutional Recurrent Net (CNN-\nRNN) [20] respectively in our approach. The CNN-RNN\nmodel consist of a mid-level temporal CNN hidden layer and\na recurrent network. The outputs of the hidden unit over the\ntextual sequence is averaged as the textural features. Then the\ntextual predicted score is deﬁned as a linear accumulation of\nevidence for compatibility with the image which needs to be\nrecognized.\nE. Training Process\nIn this subsection, we summarize our training process.\nWe train three models for original images, objects and parts\nrespectively. Their detailed training processes are shown in\nAlgorithm 1.\nAlgorithm 1 Training Process\nInput: The training images I and their corresponding textual\ndescriptions T.\nOutput: The model M.\n1: Set M = {Mori, Mobject, Mpart}\n2: Use I to ﬁne-tune the CNN model, which is pre-trained\non ImageNet, obtaining the model Mori\n3: Conduct object localization as described in Section III-C,\nto get the object regions b of I\n4: Crop b from I and save as images Ib\n5: Use Ib to ﬁne-tune Mori, obtaining the model Mobject\n6: Follow [45] to train GAN-CLS using minibatch SGD with\nI and T as pairwise constraints\n7: Conduct selective search [47] on each image to get part\nproposals S\n8: Conduct textual pattern mining to obtain the discriminative\ntextual patterns P for each subcategory\n9: for k = 1, ..., n; j = 1, ..., d do\n10:\nTake k-th part proposal Sk and j-th textual pattern Pj\nas the input of the generator G of GAN-CLS\n11:\nPerform a feed-forward pass, and output the correlation\nscore of Sk and Pj\n12:\nFor Pj we select one part proposal with the highest\ncorrelation score\n13: end for\n14: Use the selected part proposals to ﬁne-tune Mobject,\nobtaining the model Mpart\n15: return M.\n\n\n7\nF. Final Prediction\nFor a given image I, we obtain the visual predicted score\nfrom the view of the visual information, and obtain the\ntextual predicted score via measuring the visual and textual\ninformation with the shared compatibility function. Due to\nthe fact that joint learning of visual and textual information\npreserves the intra-modality and inter-modality information to\ngenerate complementary information, we fuse the visual and\ntextual predicted results as the ﬁnal prediction via the follow\nequation:\nf(I) = fv(v) + β ∗ft(t)\n(11)\nwhere fv(v) and ft(t) are the visual and textual predicted\nscores as mentioned above. β is selected by the cross-\nvalidation method following [12], and its value is 2 in our\nexperiments on the two ﬁne-grained datasets.\nIV. EXPERIMENTS\nA. Datasets\nThis subsection presents two ﬁne-grained visual catego-\nrization datasets adopted in the experiments, including CUB-\n200-2011 and Oxford Flowers-102 datasets, and their detailed\ninformation is described as follows:\n• CUB-200-2011. It is the most widely-used dataset for\nﬁne-grained visual categorization task. The visual infor-\nmation comes from the original dataset of CUB-200-2011\n[1]. It contains 11,788 images of 200 subcategories be-\nlonging to birds, 5,994 for training and 5,794 for testing.\nEach image has detailed annotations: 1 subcategory label,\n15 part locations, 312 binary attributes and 1 bounding\nbox. The textual information comes from Reed et al. [20].\nThey expand the CUB-200-2011 dataset by collecting\nﬁne-grained natural language descriptions. Ten single-\nsentence descriptions are collected for each image, as\nshown in Fig. 6. The natural language descriptions are\ncollected through the Amazon Mechanical Turk (AMT)\nplatform, and are required at least 10 words, without any\ninformation of subcategories and actions.\n• Oxford Flowers-102. Same with CUB-200-2011 dataset,\ntextual information comes from Reed et al. [20], and\nvisual information comes from the original dataset of\nOxford Flowers-102 [4], as shown in Fig. 6. It has 8,189\nimages of 102 subcategories belonging to ﬂowers, 1,020\nfor training, 1,020 for validation and 6,149 for testing.\nEach subcategory consists of between 40 and 258 images.\nB. Evaluation Metric\nAccuracy is adopted to comprehensively evaluate the cat-\negorization performances of our VTRL approach as well as\ncompared state-of-the-art methods, which is widely used in\nﬁne-grained visual categorization [8], [12], and its deﬁnition\nis as follow:\nAccuracy = Ra\nR\n(12)\nwhere R denotes the number of images in testing set, and Ra\ndenotes the number of images that are correctly classiﬁed.\nC. Implementation Details\nFine-grained Visual-textual Pattern Mining. First, we cal-\nculate the frequency of each word in the textual descriptions\nfor each subcategory, and select the top-10 words as keywords,\nand then discover textual frequent patterns via Apriori algo-\nrithm [32]. It is noted that we conduct textual pattern mining\nfor each subcategory respectively rather than all subcategories\ntogether, which guarantees that the frequent textual patterns\ntend to be the descriptions of the discriminative parts, such\nas “white head”, “black wings” and “long bill”. Second, we\nconduct selective search [47] on each image to generate part\nproposals. Finally, we employ discriminator network to relate\ntextual patterns to part proposals, then select the proposal with\nhighest score as the discriminative part for each textual pattern.\nFor each subcategory, the number of parts is set automatically\nand adaptively based on the discovered textual patterns. Fig.\n7 shows some matching examples between textual pattern and\nvisual pattern, which are the discriminative characteristics of\nthe subcategory, such as “long&brown neck”, “yellow belly”\nand “black beak”.\nVisual-textual Representation learning. For textual stream,\nwe apply CNN-RNN [20] as the text encoder to learn a\ncorrespondence function with images. In the training phase,\nwe follow Reed et al. [20]. For visual stream, we apply\nthe widely-used model 19-layer VGGNet [50] with batch\nnormalization. The model is ﬁrst pre-trained on ImageNet\n1K dataset, and then ﬁne-tuned on the ﬁne-grained visual\ncategorization dataset. Inspired by the strategy adopted by\nXiao et al [17], we utilize the pre-trained CNN model as a\nﬁlter net to select proposals relevant to the object from the\ngenerated image proposals by selective search method. We\nfurther ﬁne-tune the pre-trained model with the selected image\nproposals.\nD. Comparisons with state-of-the-art methods\nIn this subsection, we present the experimental results of our\nproposed approach as well as all the compared state-of-the-art\nmethods, as shown in Tables I and II, which demonstrate the\neffectiveness of our proposed VTRL approach. As shown in\nTable I, our proposed VTRL approach improves the catego-\nrization accuracy from 85.65% to 86.31% on CUB-200-2011\ndataset. We divide the compared methods into three groups due\nto the usage of object and part annotations in these methods.\n• Neither object nor part annotations are used. Nowadays,\nresearchers focus on how to get better categorization ac-\ncuracy under the weakly-supervised setting, which means\nneither object nor part annotations are used. Most of these\nmethods utilize the attention property of convolutional\nneural layers to localize the discriminative parts of object\nfor better accuracy, such as Fused CN-Nets [28], RA-\nCNN [18], PNA [8], TSC [16] and TL Atten [17]. They\nsimulate human visual attention mechanism only from\nvisual information. In our approach, we exploit visual\nand textual attention simultaneously as well as mine\nthe complementary information between them, which\nmake our proposed approach more effective and obtain a\n0.66% higher accuracy than the best performing result\n\n\n8\nSubcategory\nHeermann\nGull\n(1)A large bird with different shades of grey \nall over its body, white and black tail feathers, \nand a long sharp orange beak.\n(2)This bird is gray and black in color, with a \norange beak.\n(3)This bird has black outer retices and white \ninner retires and an orange beak.\n...\nVision\nText\nRed Legged \nKittiwake\n(1)This bird has a white head, breast and belly \nwith gray wings, red feet and thighs, and a red \nbeak.\n(2)This is a white bird with gray wings, red \nwebbed feet and a red beak.\n(3)Long bird with an orange beak and white \nfeathers with grey colored wings.\n...\nBohemian\nWaxwing\n(1)This bird is light gray with a light orange \npatch on its under-tail covets, neck and crown, \nand a black malar stripe and nape.\n(2)This is a grey bird with a red and yellow \ntail and a red face.\n(3)This bird has wings that are gray and black \nand has a red crown\n...\n(1)This flower is white and yellow in color, \nwith petals that are heart shaped.\n(2)This white color flower has the simple row \nof heart shaped petals shaded with orange \ncolor at the end.\n(3)This flower has thick heart shaped white \npetals and a very yellow star shaped center.\n...\nVision\nText\n(1)The flower has petals that are white with \nyellow centers.\n(2)This flower has large, flat white petals that \nconnect to each other and have a yellow \ncenter.\n(3)This flower is white and yellow in color, \nwith petals that are connected to each other.\n...\n(1)This flower has a yellow center surrounded \nby several large, overlapping white petals \nwith ruffled edges.\n(2)This flower is white and yellow in color, \nwith petals that are ruffled on the edges.\n( 3 ) T his pretty little flower has large white \npetals and a yellow center\n...\nSubcategory\nPrimula\nSilverbush\nTree Poppy\nCUB-200-2011\nOxford Flowers-102\nFig. 6: Some examples of vision and text in CUB-200-2011 dataset and Oxford Flowers-102 dataset.\nwhite head\nblack wings\nlong&brown neck\nyellow belly\nwhite wings\nblack beak\nFig. 7: Examples of the matching between textual patterns and visual patterns in our ﬁne-grained visual-textual pattern mining\napproach.\nof Fused CN-Nets [28]. We also compare with our\nprevious conference work, i.e. CVL [11]. We can see\nthat our VTRL approach brings improvements than CVL\nby 0.76% and 0.67% respectively on CUB-200-2011\nand Oxford Flowers-102 datasets. It is mainly because\nthat the VTRL approach exploits the textual attention to\nlocalize discriminative regions, while CVL directly uses\nthe whole textual descriptions and does not consider the\ndiscriminative regions in the images.\n• Only one of object and part annotations is used. These\nmethods utilize object annotation (i.e. bounding box) to\ntrain an object detector or learn part detectors, which are\nto learn more representative features for categorization.\nIn our approach, we utilize CAM [48] to automatically\nlocalize the object region of image, which avoids using\nobject annotation. The result of object localization can\nbe seen in Fig. 2. Even using object annotation, these\nmethods achieve lower accuracies than our proposed\nVTRL approach.\n• Both object and part annotations are used. In order\nto obtain better categorization accuracy, some methods\nutilize both object and part annotations at training phase\nas well as testing phase. However, these annotations are\nheavy labor-consuming. In our approach, we get object\nregion and discriminative parts automatically via object\nlocalization and ﬁne-grained visual-textual pattern mining\nrespectively without using any annotations. We promote\nthe categorization performance through discovering the\n\n\n9\nTABLE I: Comparisons with state-of-the-art methods on CUB-200-2011, sorted by amount of annotation used. “Bbox” and\n“Parts”indicate the object and part annotations (i.e. bounding box and parts locations) provided by the dataset.\nMethod\nTrain Annotation\nTest Annotation\nAccuracy (%)\nBbox\nParts\nBbox\nParts\nOur VTRL Approach\n86.31\nFused CN-Nets [28]\n85.65\nCVL [11]\n85.55\nRA-CNN [18]\n85.30\nPNA [8]\n84.70\nTSC [16]\n84.69\nFOAF [53]\n84.63\nLow-rank Bilinear [54]\n84.21\nSpatial Transformer [55]\n84.10\nBilinear-CNN [56]\n84.10\nMulti-grained [57]\n81.70\nAutoBD [27]\n81.60\nNAC [29]\n81.01\nPIR [58]\n79.34\nTL Atten [17]\n77.90\nMIL [59]\n77.40\nVGG-BGLm [60]\n75.90\nDense Graph Mining [61]\n60.19\nCoarse-to-Fine [62]\n√\n82.50\nPG Alignment [26]\n√\n√\n82.80\nTriplet-A (64) [63]\n√\n√\n80.70\nWebly-supervised [64]\n√\n√\n78.60\nPN-CNN [65]\n√\n√\n75.70\nPart-based R-CNN [12]\n√\n√\n73.50\nSPDA-CNN [66]\n√\n√\n√\n85.14\nDeep LAC [67]\n√\n√\n√\n84.10\nPBC [68]\n√\n√\n√\n83.70\nSPDA-CNN [69]\n√\n√\n√\n81.01\nPS-CNN [14]\n√\n√\n√\n76.20\nPN-CNN [65]\n√\n√\n√\n√\n85.40\nTABLE II: Comparisons with state-of-the-art methods on\nOxford Flowers-102.\nMethod\nAccuracy (%)\nOur VTRL Approach\n96.89\nCVL [11]\n96.21\nPBC [68]\n96.10\nNAC [29]\n95.34\nRIIR [70]\n94.01\nDeep Optimized [71]\n91.30\nSDR [71]\n90.50\nMML [72]\n89.45\nCNN Feature [73]\n86.80\nGeneralized Max Pooling [74]\n84.60\nEfﬁcient Object Detection [3]\n80.66\ndiscriminative and representative object and its parts.\nBesides, categorization results on Oxford Flowers-102\ndataset are shown in Table II, and also have the similar trend as\nCUB-200-2011 dataset, while our proposed VTRL approach\nstill keeps the best.\nE. Effects of Components in Our VTRL Approach\nIn this subsection, we conduct two baseline experiments\nto verify the separate contribution of each component in our\nproposed VTRL approach. Tables III to V show the accuracies\nof our proposed VTRL approach as well as the baseline\napproaches on CUB-200-2011 dataset at the following two\naspects.\n1) Effects of Fine-grained Visual-textual Pattern Mining\nand Object Localization: In our VTRL approach, ﬁne-grained\nTABLE III: Effects of ﬁne-grained pattern mining and object\nlocalization for visual stream.\nMethod\nAccuracy (%)\nVTRL-visual\n85.54\nVTRL-visual(w/o object)\n83.21\nVTRL-visual(w/o parts)\n84.79\nVTRL-visual(w/o object&parts)\n80.82\nTABLE IV: Effects of different components of our proposed\napproach on CUB-200-2011.\nMethod\nAccuracy (%)\nOur VTRL Approach\n86.31\nVTRL(w/o object)\n85.17\nVTRL(w/o parts)\n85.83\nVTRL(w/o object&parts)\n84.05\nvisual-textual pattern mining and object localization gener-\nate discriminative parts and object for promoting the cate-\ngorization accuracy. They make sense to the visual stream\nand then further impact whole approach. Tables III and IV\nshow the effects of ﬁne-grained visual-textual pattern mining\nand object localization to visual stream and our proposed\nVTRL approach respectively. In the tables, “object” means\nthat object localization is conducted, and “parts” means that\nﬁne-grained visual-textual pattern mining is employed. We can\nobserve that considering object localization can achieve better\ncategorization accuracy than considering ﬁne-grained visual-\ntextual pattern mining. This is because that objects contain\n\n\n10\nTABLE V: Effects of different components of our proposed\napproach on CUB-200-2011.\nMethod\nAccuracy (%)\nOur VTRL Approach\n86.31\nVTRL-textual\n81.81\nVTRL-visual\n85.54\nVTRL(only original image)\n80.82\nCo-attention [75]\n73.90\nSubcategory\nSooty\nAlbatross\n(1)This bird has wings that are grey and has a black bill.\n(2)This bird is gray in color, with a large curved beak.\n(3)This bird is white and brown in color, and has a black beak.\nVision\nText Rank List(Top3)\nCalifornia\nGull\n(1)This bird has large feet, a short yellow bill, and a black and white body.\n(2)This bird has wings that are grey and has a white belly and yellow bill.\n(3)This bird has a yellow beak as well as a white belly.\nCerulean\nWarbler\n(1)A little bird with a short, grey bill, blue crown, nape, white breast.\n(2)The bird has a white abdomen, black breast and white throat, blue specks.\n(3)This bird is blue and white in color with a black beak, and black eye rings.\nFig. 8: Some results of the textual stream.\nthe global and local features simultaneously, while discrim-\ninative parts focus subtle and local characteristics. However,\njointly considering object localization and ﬁne-grained visual-\ntextual pattern mining can further improve the categorization\naccuracy.\nFine-grained visual-textual pattern mining aims to select the\npart proposals that corresponding to the discovered textual\npatterns. The relations between part proposals and textual\npatterns ensure the discrimination and representativeness of\nselected parts. Some examples of discovered visual-textual\npatterns are shown in Fig. 7.\n2) Effectiveness of Visual-textual Representation Learning:\nWe also present the baseline experiment to verify the effec-\ntiveness of visual-textual representation learning. The results\nare shown in Table V, where “VTRL-textual” means textual\nstream, “VTRL-visual” means visual stream and “VTRL(only\noriginal image” means only a ﬁne-tuned CNN model is used.\nWe can observe that categorization result of textual stream is\npromising. From the ﬁrst line of each row in Fig. 8, we can ﬁnd\nthat textual description with the highest score always points\nout the discriminative characteristics of the object, as the\nred words shows. Combining visual and textual information\ncan further achieve more accurate categorization result, which\ndemonstrates that the two types of information are comple-\nmentary: visual information focuses on the global and local\nfeatures, and textual information further points the importance\nof these features. Fig. 9 shows some example results where the\ntextual and visual streams are complementary. Visual stream\nis effective for dealing with those images, which have few\ndiscriminative characteristics. Humans can only describe them\nin a rough way, but cannot describe them in detail, which leads\nthat the textual information carries less useful information to\ndistinguish it from other subcategories. Examples are shown\nas the right two images. Textual stream is effective for dealing\nwith those images, whose foreground and background are\nthis small bird has a dark \ngrey brown head and \nwings and a light green \nbreast and belly, the bill \nis bright orange\nsmall brown and yellow \nbird with brown wings \nand orange head, short \nbeak\na large bird, totally black, \nwith shaggy black throat \nfeathers\na bird with a small black \npointed bill and entirely \ngray feathers covering its \nbody\nFig. 9: Some example results where the textual and visual\nstreams are complementary. The left two images are rightly\ncategorized by textual stream, but wrongly categorized by\nvisual stream. The right two images are just the opposite.\nhard to be distinguished by visual stream. But they can be\ndescribed in details by text, which carries the information\nof the discriminative characteristics and be helpful for the\ncategorization. Examples are shown as the left two images.\nBesides, we also compare our VTRL approach with method\nbased on both textual and visual attention, such as Co-attention\n[75]. It only achieves the accuracy of 73.90%, which is lower\nthan our VTRL approach. It is mainly because that our VTRL\napproach discovers the ﬁne-grained visual-textual patterns,\nwhich are key hints to the ﬁne-grained categorization.\nFrom the above baseline results, the separate contribution\nof each component in our proposed VTRL approach can\nbe veriﬁed. First, object localization and ﬁne-grained pat-\ntern mining discover the discriminative and representative\ninformation of image via visual-textual attention. Second, the\ncomplementarity between visual and textual information is\nfully captured by visual-textual representation learning.\nV. CONCLUSIONS\nIn this paper, the ﬁne-grained visual-textual representation\nlearning approach has been proposed. Based on textual atten-\ntion, we employ ﬁne-grained visual-textual pattern mining to\ndiscover discriminative information for categorization through\njointly modeling vision and text with GANs. Then, visual-\ntextual representation learning jointly considers visual and\ntextual information, which preserves the intra-modality and\ninter-modality information to generate complementary ﬁne-\ngrained representation, and further improve categorization\nperformance. Experimental results on two widely-used ﬁne-\ngrained visual categorization datasets demonstrate the superi-\nority of our approach compared with state-of-the-art methods.\nAs for the future work, we will focus on the following two\naspects: First, we will attempt to extend the current two-stream\nframework into an end-to-end framework for simplifying the\nprocess. Second, we will exploit exact and effective methods\non relating textual attention and visual attention for more\naccurate discriminative parts localization as well as better\ncategorization performance.\nREFERENCES\n[1] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge\nBelongie. The caltech-ucsd birds-200-2011 dataset. 2011.\n\n\n11\n[2] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei\nLi. Novel dataset for ﬁne-grained image categorization: Stanford dogs.\nIn CVPR Workshop on Fine-Grained Visual Categorization (FGVC),\nvolume 2, 2011.\n[3] Anelia Angelova and Shenghuo Zhu.\nEfﬁcient object detection and\nsegmentation for ﬁne-grained recognition.\nIn IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), pages 811–818,\n2013.\n[4] M-E. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over\na large number of classes. In Indian Conference on Computer Vision,\nGraphics and Image Processing, Dec 2008.\n[5] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object\nrepresentations for ﬁne-grained categorization. In IEEE International\nConference on Computer Vision Workshops, pages 554–561, 2013.\n[6] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and\nAndrea Vedaldi.\nFine-grained visual classiﬁcation of aircraft.\narXiv\npreprint arXiv:1306.5151, 2013.\n[7] Chen Huang, Zhihai He, Guitao Cao, and Wenming Cao. Task-driven\nprogressive part localization for ﬁne-grained object recognition. IEEE\nTransactions on Multimedia (TMM), 18(12):2372–2383, 2016.\n[8] Xiaopeng Zhang, Hongkai Xiong, Wengang Zhou, Weiyao Lin, and\nQi Tian. Picking neural activations for ﬁne-grained recognition. IEEE\nTransactions on Multimedia (TMM), 2017.\n[9] Yan Wang, Sheng Li, and Alex C Kot. Deepbag: Recognizing handbag\nmodels. IEEE Transactions on Multimedia (TMM), 17(11):2072–2083,\n2015.\n[10] Yan Wang, Sheng Li, and Alex C Kot. On branded handbag recognition.\nIEEE Transactions on Multimedia (TMM), 18(9):1869–1881, 2016.\n[11] Xiangteng He and Yuxin Peng. Fine-grained image classiﬁcation via\ncombining vision and language. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), July 2017.\n[12] Ning Zhang, Jeff Donahue, Ross Girshick, and Trevor Darrell. Part-\nbased r-cnns for ﬁne-grained category detection. In European Confer-\nence on Computer Vision (ECCV), pages 834–849, 2014.\n[13] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich\nfeature hierarchies for accurate object detection and semantic segmenta-\ntion. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 580–587, 2014.\n[14] Shaoli Huang, Zhe Xu, Dacheng Tao, and Ya Zhang. Part-stacked cnn\nfor ﬁne-grained visual categorization. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages 1173–1182, 2016.\n[15] Xiaopeng Zhang, Hongkai Xiong, Wengang Zhou, Weiyao Lin, and\nQi Tian. Picking deep ﬁlter responses for ﬁne-grained image recognition.\nIn IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 1134–1142, 2016.\n[16] Xiangteng He and Yuxin Peng. Weakly supervised learning of part selec-\ntion model with spatial constraints for ﬁne-grained image classiﬁcation.\nIn AAAI Conference on Artiﬁcial Intelligence (AAAI), pages 4075–4081,\n2017.\n[17] Tianjun Xiao, Yichong Xu, Kuiyuan Yang, Jiaxing Zhang, Yuxin Peng,\nand Zheng Zhang. The application of two-level attention models in deep\nconvolutional neural network for ﬁne-grained image classiﬁcation. In\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR),\npages 842–850, 2015.\n[18] Jianlong Fu, Heliang Zheng, and Tao Mei. Look closer to see better:\nRecurrent attention convolutional neural network for ﬁne-grained image\nrecognition.\nIn IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), July 2017.\n[19] John Robert Anderson. Cognitive psychology and its implications. WH\nFreeman/Times Books/Henry Holt & Co, 1985.\n[20] Scott Reed, Zeynep Akata, Honglak Lee, and Bernt Schiele. Learning\ndeep representations of ﬁne-grained visual descriptions.\nIn IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), pages\n49–58, 2016.\n[21] Yuning Chai, Victor Lempitsky, and Andrew Zisserman.\nSymbiotic\nsegmentation and part localization for ﬁne-grained categorization. In\nIEEE International Conference on Computer Vision (ICCV), pages 321–\n328, 2013.\n[22] Shulin Yang, Liefeng Bo, Jue Wang, and Linda G Shapiro. Unsupervised\ntemplate learning for ﬁne-grained object recognition. In Advances in\nNeural Information Processing Systems (NIPS), pages 3122–3130, 2012.\n[23] Thomas Berg and Peter Belhumeur.\nPoof: Part-based one-vs.-one\nfeatures for ﬁne-grained categorization, face veriﬁcation, and attribute\nestimation.\nIn IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 955–962, 2013.\n[24] Lingxi Xie, Qi Tian, Richang Hong, Shuicheng Yan, and Bo Zhang.\nHierarchical part matching for ﬁne-grained visual categorization.\nIn\nIEEE International Conference on Computer Vision (ICCV), pages\n1641–1648, 2013.\n[25] Ning Zhang, Ryan Farrell, Forrest Iandola, and Trevor Darrell.\nDe-\nformable part descriptors for ﬁne-grained recognition and attribute\nprediction.\nIn IEEE International Conference on Computer Vision\n(ICCV), pages 729–736, 2013.\n[26] Jonathan Krause, Hailin Jin, Jianchao Yang, and Li Fei-Fei. Fine-grained\nrecognition without part annotations. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages 5546–5555, 2015.\n[27] Hantao Yao, Shiliang Zhang, Chenggang Yan, Yongdong Zhang, Jintao\nLi, and Qi Tian. Autobd: Automated bi-level description for scalable\nﬁne-grained visual categorization. IEEE Transactions on Image Pro-\ncessing (TIP), 27(1):10–23, 2018.\n[28] Hantao Yao, Shiliang Zhang, Yongdong Zhang, Jintao Li, and Qi Tian.\nOne-shot ﬁne-grained instance retrieval. In ACM on Multimedia Con-\nference (ACM MM), pages 342–350. ACM, 2017.\n[29] Marcel Simon and Erik Rodner. Neural activation constellations: Un-\nsupervised part model discovery with convolutional networks. In IEEE\nInternational Conference on Computer Vision (ICCV), pages 1143–1151,\n2015.\n[30] Jiawei Han, Hong Cheng, Dong Xin, and Xifeng Yan.\nFrequent\npattern mining: current status and future directions. Data Mining and\nKnowledge Discovery, 15(1):55–86, 2007.\n[31] Rakesh Agrawal, Tomasz Imieli´nski, and Arun Swami. Mining associ-\nation rules between sets of items in large databases. In ACM SIGMOD\nRecord, volume 22, pages 207–216. ACM, 1993.\n[32] Rakesh Agrawal, Ramakrishnan Srikant, et al.\nFast algorithms for\nmining association rules. In International Conference on Very Large\nData Bases (VLDB), volume 1215, pages 487–499, 1994.\n[33] Jiawei Han, Jian Pei, and Yiwen Yin. Mining frequent patterns without\ncandidate generation. In ACM SIGMOD Record, volume 29, pages 1–12.\nACM, 2000.\n[34] Yao Li, Lingqiao Liu, Chunhua Shen, and Anton van den Hengel. Mid-\nlevel deep pattern mining. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 971–980, 2015.\n[35] Hongzhi Li, Joseph G Ellis, Heng Ji, and Shih-Fu Chang. Event speciﬁc\nmultimodal pattern mining for knowledge base construction. In ACM\non Multimedia Conference (ACM MM), pages 821–830. ACM, 2016.\n[36] Richang Hong, Jinhui Tang, Hung-Khoon Tan, Chong-Wah Ngo,\nShuicheng Yan, and Tat-Seng Chua.\nBeyond search: Event-driven\nsummarization for web videos.\nACM Transactions on Multimedia\nComputing, Communications, and Applications (TOMM), 7(4):35, 2011.\n[37] Richang Hong, Lei Li, Junjie Cai, Dapeng Tao, Meng Wang, and\nQi Tian.\nCoherent semantic-visual indexing for large-scale image\nretrieval in the cloud. IEEE Transactions on Image Processing (TIP),\n26(9):4128–4138, 2017.\n[38] Harold Hotelling. Relations between two sets of variates. Biometrika,\n28(3/4):321–377, 1936.\n[39] Herv´e Bredin and G´erard Chollet.\nAudio-visual speech synchrony\nmeasure for talking-face identity veriﬁcation.\nIn IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP),\nvolume 2, pages II–233, 2007.\n[40] David R Hardoon, Sandor Szedmak, and John Shawe-Taylor. Canonical\ncorrelation analysis: An overview with application to learning methods.\nNeural Computation, 16(12):2639–2664, 2004.\n[41] Benjamin Klein, Guy Lev, Gil Sadeh, and Lior Wolf. Associating neural\nword embeddings with deep image representations using ﬁsher vectors.\nIn IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 4437–4446, 2015.\n[42] Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak\nLee, and Andrew Y Ng. Multimodal deep learning. In International\nConference on Machine Learning (ICML), pages 689–696, 2011.\n[43] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory.\nNeural Computation, 9(8):1735–1780, 1997.\n[44] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolu-\ntional networks for text classiﬁcation. In Advances in Neural Information\nProcessing Systems (NIPS), pages 649–657, 2015.\n[45] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt\nSchiele, and Honglak Lee. Generative adversarial text to image synthe-\nsis. In International Conference on Machine Learning (ICML), pages\n1060–1069, 2016.\n[46] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014.\n[47] Jasper RR Uijlings, Koen EA van de Sande, Theo Gevers, and\nArnold WM Smeulders. Selective search for object recognition. In-\nternational Journal of Computer Vision (IJCV), 104(2):154–171, 2013.\n\n\n12\n[48] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio\nTorralba. Learning deep features for discriminative localization. In IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), pages\n2921–2929, 2016.\n[49] Nobuyuki Otsu.\nA threshold selection method from gray-level his-\ntograms. IEEE Transactions on Systems, Man, and Cybernetics, 9(1):62–\n66, 1979.\n[50] Karen Simonyan and Andrew Zisserman.\nVery deep convolu-\ntional networks for large-scale image recognition.\narXiv preprint\narXiv:1409.1556, 2014.\n[51] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-\nFei.\nImagenet: A large-scale hierarchical image database.\nIn IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), pages\n248–255, 2009.\n[52] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\nDragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew\nRabinovich. Going deeper with convolutions. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), pages 1–9, 2015.\n[53] Xiaopeng Zhang, Hongkai Xiong, Wengang Zhou, and Qi Tian. Fused\none-vs-all features with semantic alignments for ﬁne-grained visual cat-\negorization. IEEE Transactions on Image Processing (TIP), 25(2):878–\n892, 2016.\n[54] Shu Kong and Charless Fowlkes. Low-rank bilinear pooling for ﬁne-\ngrained classiﬁcation.\nIn IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), July 2017.\n[55] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.\nSpatial\ntransformer networks. In Advances in Neural Information Processing\nSystems (NIPS), pages 2017–2025, 2015.\n[56] Tsung-Yu Lin, Aruni RoyChowdhury, and Subhransu Maji.\nBilinear\ncnn models for ﬁne-grained visual recognition. In IEEE International\nConference on Computer Vision (ICCV), pages 1449–1457, 2015.\n[57] Dequan Wang, Zhiqiang Shen, Jie Shao, Wei Zhang, Xiangyang Xue,\nand Zheng Zhang.\nMultiple granularity descriptors for ﬁne-grained\ncategorization. In International Conference on Computer Vision (ICCV),\npages 2399–2406, 2015.\n[58] Yu Zhang, Xiu-Shen Wei, Jianxin Wu, Jianfei Cai, Jiangbo Lu, Viet-Anh\nNguyen, and Minh N Do. Weakly supervised ﬁne-grained categorization\nwith part-based image representation.\nIEEE Transactions on Image\nProcessing (TIP), 25(4):1713–1725, 2016.\n[59] Zhe Xu, Dacheng Tao, Shaoli Huang, and Ya Zhang. Friend or foe:\nFine-grained categorization with weak supervision. IEEE Transactions\non Image Processing (TIP), 26(1):135–146, 2017.\n[60] Feng Zhou and Yuanqing Lin.\nFine-grained image classiﬁcation by\nexploring bipartite-graph labels. arXiv preprint arXiv:1512.02665, 2015.\n[61] Luming Zhang, Yang Yang, Meng Wang, Richang Hong, Liqiang Nie,\nand Xuelong Li. Detecting densely distributed graph patterns for ﬁne-\ngrained image categorization. IEEE Transactions on Image Processing\n(TIP), 25(2):553–565, 2016.\n[62] Hantao Yao, Shiliang Zhang, Yongdong Zhang, Jintao Li, and Qi Tian.\nCoarse-to-ﬁne description for ﬁne-grained visual categorization. IEEE\nTransactions on Image Processing (TIP), 25(10):4858–4872, 2016.\n[63] Yin Cui, Feng Zhou, Yuanqing Lin, and Serge Belongie. Fine-grained\ncategorization and dataset bootstrapping using deep metric learning with\nhumans in the loop. arXiv preprint arXiv:1512.05227, 2015.\n[64] Zhe Xu, Shaoli Huang, Ya Zhang, and Dacheng Tao. Webly-supervised\nﬁne-grained visual categorization via deep domain adaptation.\nIEEE\nTransactions on Pattern Analysis and Machine Intelligence (TPAMI),\n2016.\n[65] Steve Branson, Grant Van Horn, Serge Belongie, and Pietro Perona.\nBird species categorization using pose normalized deep convolutional\nnets. arXiv preprint arXiv:1406.2952, 2014.\n[66] Han Zhang, Tao Xu, Mohamed Elhoseiny, Xiaolei Huang, Shaoting\nZhang, Ahmed Elgammal, and Dimitris Metaxas. Spda-cnn: Unifying\nsemantic part detection and abstraction for ﬁne-grained recognition.\npages 1143–1152, 2016.\n[67] Di Lin, Xiaoyong Shen, Cewu Lu, and Jiaya Jia.\nDeep lac: Deep\nlocalization, alignment and classiﬁcation for ﬁne-grained recognition. In\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR),\npages 1666–1674, 2015.\n[68] Chao Huang, Hongliang Li, Yurui Xie, Qingbo Wu, and Bing Luo.\nPbc: Polygon-based classiﬁer for ﬁne-grained categorization.\nIEEE\nTransactions on Multimedia (TMM), 19(4):673–684, 2017.\n[69] Han Zhang, Tao Xu, Mohamed Elhoseiny, Xiaolei Huang, Shaoting\nZhang, Ahmed Elgammal, and Dimitris Metaxas. Spda-cnn: Unifying\nsemantic part detection and abstraction for ﬁne-grained recognition. In\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR),\npages 1143–1152, 2016.\n[70] Lingxi Xie, Jingdong Wang, Weiyao Lin, Bo Zhang, and Qi Tian.\nTowards reversal-invariant image representation. International Journal\nof Computer Vision (IJCV), 123(2):226–250, 2017.\n[71] Hossein Azizpour, Ali Sharif Razavian, Josephine Sullivan, Atsuto Maki,\nand Stefan Carlsson. From generic to speciﬁc deep representations for\nvisual recognition. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition Workshops, pages 36–45, 2015.\n[72] Qi Qian, Rong Jin, Shenghuo Zhu, and Yuanqing Lin. Fine-grained vi-\nsual categorization via multi-stage metric learning. In IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages 3716–3724,\n2015.\n[73] Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan\nCarlsson. Cnn features off-the-shelf: an astounding baseline for recogni-\ntion. In IEEE Conference on Computer Vision and Pattern Recognition\nWorkshops, pages 806–813, 2014.\n[74] Naila Murray and Florent Perronnin. Generalized max pooling. In IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), pages\n2473–2480, 2014.\n[75] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical\nquestion-image co-attention for visual question answering. In Advances\nIn Neural Information Processing Systems, pages 289–297, 2016.\n"
}