{
  "filename": "1911.10807v1.pdf",
  "num_pages": 12,
  "pages": [
    "arXiv:1911.10807v1  [cs.CV]  25 Nov 2019\nFast and Generalized Adaptation for Few-Shot Learning\nLiang Song, Jinlu Liu, Yongqiang Qin\nAInnovation Technology Co., Ltd.\nsongliang, liujinlu, qinyongqiang@ainnovation.com\nAbstract\nThe ability of fast generalizing to novel tasks from a few\nexamples is critical in dealing with few-shot learning prob-\nlems. However, deep learning models severely suffer from\noverﬁtting in extreme low data regime. In this paper, we\npropose Adaptable Cosine Classiﬁer (ACC) and Amphib-\nian to achieve fast and generalized adaptation for few-shot\nlearning. The ACC realizes the ﬂexible retraining of a deep\nnetwork on small data without overﬁtting. The Amphibian\nlearns a good weight initialization in the parameter space\nwhere optimal solutions for the tasks of the same class clus-\nter tightly. It enables rapid adaptation to novel tasks with\nfew gradient updates. We conduct comprehensive experi-\nments on four few-shot datasets and achieve state-of-the-art\nperformance in all cases. Notably, we achieve the accuracy\nof 87.75% on 5-shot miniImageNet which approximately\noutperforms existing methods by 10%. We also conduct ex-\nperiment on cross-domain few-shot tasks and provide the\nbest results.\n1. Introduction\nDeep Neural Networks have shown great power in vi-\nsual learning tasks such as image classiﬁcation [11, 15, 29],\nobject detection [9, 24, 10], and semantic segmentation\n[18, 26, 2]. However, training these models requires a large\namount of labeled data. In many cases, the model perfor-\nmance sharply drops if the labeled data is scarce. Recent\nresearches pay attention to rapidly learn a model in a data-\nefﬁcient way. The goal is enabling a model to fast adapt to a\nnew task without the need for hundreds of training data. In\nthe low-data regime, fast adaptation to new tasks and avoid-\ning the overﬁtting problem are challenging. These issues\nmotivate the study of few-shot learning.\nFew-shot learning [34, 6, 7] aims to efﬁciently learn a\nmodel that can recognize novel classes when the training\nexamples are extremely limited. It is commonly formalized\nin a meta-learning way where a task is called N-way K-\nshot task if it consists of N classes with K labeled examples\nper class. In few-shot learning, we learn a model on differ-\nTask 1-1\nTask 1-2\nTask 2-1\nTask 2-2\nInner Loop 1\nInner Loop 2\nOuter Loop\nFigure 1. Nested training loops of our proposed Amphibian. In the\ninner loop, parameters are updated using multiple tasks belonging\nto the same classes. In the outer loop, each step is performed with\ndifferent classes.\nent tasks from a large labeled dataset of base classes and\naim to fast adapt the model to unseen tasks of novel classes.\nMost existing methods show limited performance when fac-\ning two major challenges in few-shot learning: rapid adap-\ntation and excellent generalization. Various methods are ex-\nplored to attack the challenges which can be divided into\ntwo categories: metric learning based methods [7, 34] and\nmeta learning based methods [6, 20]. 1) Metric learning\nbased methods target to learn to compare target examples\nand few labeled examples by a distance metric in the em-\nbedding space.\nThese methods usually have weak gen-\neralization ability. Without retraining, the models trained\non the base classes poorly generalize to the novel classes.\nWhile with retraining, the models are prone to overﬁt on\nthe few labeled data. Even so, some metric based learn-\ning approaches have an advantage of learning discrimina-\ntive features. 2) Meta learning based methods aim to learn\na basic model on abundant training tasks and the model can\nquickly converge on new tasks with a few gradient updates\nor ﬁne-tuning steps. A common solution is to ﬁnd a good\ninitialization but some methods learn a biased initialization\nwhich prevents the model from generalizing well to unseen\ntasks.\nIn this paper, our goal is to achieve fast and generalized\nadaptation for few-shot learning. As shown in Figure 2,\nour method consists of two stages: pre-training and adapta-\ntion. We propose an Adaptable Cosine Classiﬁer for dis-\ncriminative feature learning. To further improve generaliza-\ntion, we propose a meta learning method called Amphibian\nwhich ensures the rapid and generalized adaptation. The\nAdaptable Cosine Classiﬁer consists of a feature extractor\n1\n",
    "Task 1\n...\nTask \nFeature\nExtractor\nLearnable\nClassi\u0001cation\nWeight\nInner Loop\nOuter Loop\nTasks from \nsame classes\nTasks from \ndifferent classes\nLoss\nOne task\nBackward\nPre-training with Amphibian\nClassi\u0000cation\nWeight\nLoss\nBackward\nAdaptable\nCosine Classi\u0002er\nSupport Set\nof the Task\nQuery Set\nof the Task\nFeature\nExtractor\nAdapted\nFeature\nExtractor\nScore\nAdapting with Adaptable Cosine Classi\u0003er\nAdaptable\nCosine Classi\u0004er\nClassi\n\u0005cation\nWeight\nFigure 2. Framework of our proposed method. Our proposed method involves two stages pre-training and adaptation. The module in\nlight blue is trainable in each stage. In the pre-training phase, we aim to learn a good initialization with our proposed Amphibian. In the\nadaptation phase, we adapt the feature extractor to speciﬁc tasks using our proposed Adaptable Cosine Classiﬁer.\nand a cosine classiﬁer. It is trained in the ﬁrst stage to learn a\nfeature embedding space where examples in the same class\nare close to each other and far away from examples in other\ndifferent classes. We train the network on the supervised\nclassiﬁcation task in the ﬁrst stage. In the second stage, we\njust adapt the feature extractor on the novel classes. And we\nuse the mean vectors of labeled examples to parametrize the\ncosine classiﬁer since retraining the classiﬁer on few exam-\nples takes risks of overﬁtting.\nTo better train the model, we propose a meta learning\nmethod called Amphibian which ﬁnds a good weight ini-\ntialization for fast and generalized adaptation.\nAn ideal\nweight initialization enables the model to learn from a task\nthrough one gradient update and achieve the best results on\nnew tasks. To ﬁnd a good weight initialization as much as\npossible, we aim to maximize the performance on a new\ntask through one or few gradient updates in current task\nduring the training process. We train the Adaptable Co-\nsine Classiﬁer using Amphibian strategy in the ﬁrst stage.\nThus, we obtain a good weight initialization from the base\nclasses which realizes rapid adaptation and convergence on\nthe novel classes in the second stage.\nIn a word, our method achieves discriminative feature\nlearning and generalization, which takes advantages of met-\nric learning based methods and meta learning based meth-\nods respectively. Our contributions are threefold:\n1. We propose the Adaptable Cosine Classiﬁer which\nhas great generalization ability surpassing the exist-\ning metric learning based methods. Adaptable Cosine\nClassiﬁer is ﬂexible to be retrained and can fast adapt\nto novel tasks.\n2. We propose a meta learning method called Amphibian\nto train a good weight initialization such that it enables\nrapid and generalized adaptation to new tasks with a\nsmall number of gradient updates.\n3. Our experiments show that our method achieves the\nstate-of-the-art and outperforms existing methods by\n4%-10% on two few-shot benchmarks: miniImageNet\nand tieredImageNet. We also evaluate our method in\ncross-domain few-shot classiﬁcation and produce the\nsuperior results on miniImageNet →CUB-200-2011\ntasks.\n2. Related Work\nMany methods have been published to solve the few-\nshot classiﬁcation problem [7, 16, 6, 20, 30, 33, 34, 28, 32].\nIn this section, we brieﬂy introduce some approaches\nrelevant to our work.\nMetric learning based approach\nMetric learning based approaches [34, 30, 33, 7] learn\na projection function that can map examples from the\nimage space to the feature space. And the features preserve\nthe class neighborhood structure in the feature space so\nthat we can recognize them easily.\nInstead of using a\nfully-connected layer, MatchingNet [34] and ProtoNet [30]\nuse the nearest-neighbor method in the feature space with\nthe Euclidean metric.\nAnother commonly used distance\nmetric is the cosine similarity [7].\nThere are also two\ncommonly used methods to calculate the prototypes, using\nlearned prototypes and using the support set to calculate\nprototypes in each task.\nUnlike the above methods,\nRelationNet [33] learns a deep non-linear distance metric\nto replace Euclidean metric or cosine similarity. By this\nmethod, it aims to learn the best metric that can adapt\nto different tasks. All the above methods are based on a\nfundamental hypothesis that the learned feature extractor\ncan be generalized to novel classes.\nBut there is still a\nsigniﬁcant performance gap between the base classes and\nthe novel classes. It indicates that the mapping function\ncan not ﬁt in the novel classes well. So we propose the\nAdaptable Cosine Classiﬁer method that can adapt the\nmetric learning based model to novel tasks to eliminate the\ngap.\n",
    "Meta learning based approach\nMeta learning based approaches [28, 6, 20, 32] typically\ninvolve two phases: pre-training and adapting. Due to the\nparticularity of the few-shot classiﬁcation, the key is how\nto perform generalized adaptation in the adapting stage.\nRavi et al. [28] concentrates on ﬁnding an optimizer that\nis better than SGD [25]. The similarity between gradient\ndescent methods and long short-term memory [13] inspires\nthem to propose the LSTM-based network.\nMAML [6]\nis another typical meta learning based method. It aims to\nlearn a good weight initialization which is close is to all\ntasks. So that with a limited number of labeled examples\nthe model can ﬁnd the task speciﬁc optimal in one gradient\nupdate step. Alex Nichol et al. propose Reptile [20] which\ncan be treated as extended MAML with k gradient update\nsteps. Furthermore, they theoretically analyze the reason\nwhy the MAML liked method works. MTL [32] focuses\non preventing meta overﬁtting by reducing the number\nof learnable parameters.\nThey propose Scale-and-Shift\nparameters to adapt to novel tasks without updating all\nparameters. In this paper, we propose Amphibian to learn\na good weight initialization from which we can perform\nbetter generalized adaptation in limited examples.\n3. Methodology\nIn this section, we ﬁrst give the problem formulation in\nsection 3.1. Secondly, we introduce in section 3.2 the pro-\nposed approach to adapt a metric based method to novel\ntasks. Then we discuss how to train an initialization of the\nfeature extractor by our proposed Amphibian that can fast\nadapt to all tasks in section 3.3. Finally, we provide a theo-\nretical explanation of why Amphibian works in section 3.4.\n3.1. Problem Formulation\nFirstly, we introduce a generic notion of a learning task.\nFormally, each task τ = {Ds, Dq, Ctask, L(Dq|φ)} in-\nvolves two sets: support set Ds and query set Dq respec-\ntively. The examples of Ds and Dq both belong to classes\nCtask. The support set Ds consists of N × K examples\n(N is the number of classes Ctask, and K is the number\nof examples per class) and corresponding labels. And the\nnumber of labeled examples K is a small number like 1 or\n5. The query set Dq involves some unlabeled examples be-\nlong to Ctask. The objective is using the limited labeled\nsupport set to learn a model Fφ that can recognize all the\nexamples from the query set Dq. It can be formulated as\nminimizing the cost function L(Dq|φ). Such a task is a so-\ncalled N-way K-shot few-shot classiﬁcation task.\nIn the few-shot classiﬁcation problem, we are given\ntwo datasets: base set Dbase and novel set Dnovel respec-\ntively. The base set is composed of examples x and the\ncorresponding labels y, Dbase = {(x1, y1), ..., (xn, yn)}.\nAll the examples from the base set belong to class Cbase.\nThe novel set is similar with the base set Dnovel\n=\n{(x′\n1, y′\n1), ..., (x′\nn, y′\nn)}, and all the examples belong to\nclass Cnovel. And Cnovel is disjoint with Cbase. The base\nset is available for us to learn a good model. And evaluate\nthe model on the novel set with few-shot tasks. The objec-\ntive is:\nmin\nφ Eτ[L(Dq|A(Ds, φ))]\n(1)\nwhere A(·) denotes the adaptation procedure.\nAnd\nA(Ds, φ) denotes the updated parameters on Ds.\n3.2. Adaptable Cosine Classiﬁer\nIn general, there is a feature extractor Fφ(·) and a clas-\nsiﬁer Z(·; W) in a deep neural network for classiﬁcation. φ\nis the parameter of the feature extractor Fφ, and W is the\nclassiﬁcation weight of the classiﬁer Z(·; W). The feature\nextractor Fφ maps the data x from the image space to the\nembedding space. Then the classiﬁer Z(·; W) uses the fea-\nture embedding Fφ(x) to estimate the classiﬁcation scores.\nSince there are only few training examples for novel\nclasses in few-shot classiﬁcation, learning a linear classiﬁer\nwith good generalization ability is very difﬁcult. In order to\novercome this critical problem, the Cosine Classiﬁer (CC)\nis proposed [7] which uses cosine similarity instead of dot-\nproduct when computing classiﬁcation scores. The cosine\nsimilarity operator has an advantage in preserving the class\nneighborhood structure in the embedding space. The CC\ncan be formalized as:\nZk(Fφ(x); W) =\neγcos(Fφ(x),Wk)\nP∥C∥\ni=1 eγcos(Fφ(x),Wi)\n(2)\nwhere Zk(·) denotes the classiﬁcation score of the k-th\nclass, Wi is the i-th classiﬁcation weight, γ denotes the\nscale of the softmax operator, and ∥C∥is the number of\nclasses.\nAt the pre-training stage, we train the model on the ex-\namples from base classes Cbase and the objective is to min-\nimize the negative log-likelihood loss:\nL(x|φ, Wb) =\nE\nx,y∈Dbase\n[−logZy(Fφ(x); Wb)]\n(3)\nwhere Wb denotes the classiﬁcation weight vectors of the\nbase categories and y is the true label.\nSince the feature extractor Fφ is trained only on the base\nclasses, it can not extract good features of the novel classes\nwithout adaptation. However, it takes a risk of overﬁtting\nto retrain the CC on the few examples to learn classiﬁcation\nweights of the novel classes. To overcome the difﬁculty,\n",
    "we propose Adaptable Cosine Classiﬁer (ACC) to adapt the\nfeature extractor to new tasks. As discussed before, the crit-\nical challenge for few-shot classiﬁcation problem is to learn\nthe best classiﬁcation weight of the novel classes without\noverﬁtting on the few training examples. So we give the\ndeﬁnition of the best classiﬁcation weight:\nmin\nWj\nX\ni\nDistance(Wj, Fφ(xi))\n(4)\nwhere Wj is the classiﬁcation weight of the j-th class, and\nxi is the example belonging to class Cj. To be consistent\nwith the cosine operator used at the pre-training stage, we\nadopt −cos(·, ·) as the distance metric in Equation 4. The\noptimization of Equation 4 has the closed formed solution:\nWj =\nP\ni Fφ(xi)/Kj\n∥P\ni Fφ(xi)/Kj∥2\n(5)\nwhere Kj denotes the number of examples belonging to cat-\negory Cj.\nAt the inference stage, we adapt our feature extractor to\nthe novel classes and evaluate on the query set. Since we\ndo not have the true distribution of novel classes, we use the\nsupport set to estimate the true distribution. The objective\nis to minimize the negative log-likelihood loss:\nL(x|φ, Wn) =\nE\nx,y∈Dsupport\n[−logZy(Fφ(x); Wn)]\n(6)\nwhere Wn is calculated directly from Equation 6.\nIn general, to ﬁne-tune CC we need to re-initialize the\nclassiﬁcation weights and retrain the model from scratch.\nHowever, the model tend to overﬁtting with the limited\ntraining data. By our proposed ACC, we compute the best\nclassiﬁcation weights directly. It gives the model the abil-\nity of fast and generalized adaptation to novel tasks. The\nadapted feature extractor can extract more discriminative\nfeatures which are conducive to classiﬁcation. As expected,\nexperimental results also indicate that the adaptation can\nbring us signiﬁcant performance improvement.\n3.3. Amphibian\nTo further improve the ability of adaptation, we pro-\npose a simple yet effective method named Amphibian. Like\nMAML [6] and Reptile [20], Amphibian learns an initial-\nization for the parameters of a model. From this initializa-\ntion, the model can perform fast and generalized adaptation\nto different tasks.\nWhen adapting to a new task τs, we are given a labeled\nsupport set Ds, and an unlabeled query set Dq. Examples\nof both sets are sampled from Ctask. Our objective is to\nupdate the initialization φ of the model one step using the\nsupport set Ds then the updated model is capable of recog-\nnize the examples from the query set Dq. This objective can\nAlgorithm 1 Amphibian\nRequire: α, β: step size hyper-parameters\nInitialize parameters φ of a model\nfor iteration = 1, 2, . . . do\nsample N categories Ctaskfrom Cbase\nInitialize θ0 = φ\nfor i = 1, 2, . . . , m do\nsample K examples per category from Dbase\nthe N × K examples compose a task τi\nCompute L(τi; θi−1)\nUpdate θi ←θi−1 −β∇L(τi; θi−1)\nend for\nUpdate φ ←φ + α(θm −φ)\nend for\nbe formalized as:\nmin\nφ Ex∈Dq[L(x|φ′)]\n(7)\nwhere φ′ denotes the updated parameters using one gradi-\nent descent update on task τs, φ′ = φ −β∇φL(τs|φ). β is\na hyper-parameter. Since the support set Ds and the query\nset Dq is independent. In order to minimize the expecta-\ntion of the loss on Dq, we need to minimize the loss on all\nexamples belonging to Ctask:\nmin\nφ Ex∈Dq[L(x|φ′)] = min\nφ E{x|y∈Ctask}[L(x|φ′)]\n(8)\nEquation 8 can be further formalized as:\nmin\nφ Ex∈Dq[L(x|φ′)] = min\nφ Eτi∈Ttask[L(τi|φ′)]\n(9)\nwhere Ttask denotes all the possible tasks whose exam-\nples belong to Ctask, and τi denotes one task sampled from\nTtask.\nMeanwhile, we aim to perform well in the current task\nτs so that the model can extract task-independent features.\nThe integral objective can be formalized as:\nmin\nφ Eτi∈Ttask[L(τi, φ′)] + ηL(τs|φ)\n(10)\nwhere η is a hyper-parameter, controlling the weight of the\nabove two objectives. Since we can not obtain all the tasks,\nwe use one task τi to estimate Eτ∈Ttask[L(τ, φ′)]. Thus, the\ngradient update process is:\nφ ←φ −α∇φ(L(τi|φ′) + ηL(τs|φ))\n(11)\nwhere α is a hyper-parameter. Note that the cost function\nis computed using the updated parameters φ′, yet the train-\ning parameters are φ. So optimizing parameters φ requires\nthe second order gradient. Higher order gradient requires\n",
    "lots of computational overhead. To efﬁciently solve this op-\ntimization problem, we exploit a ﬁrst-order approximation\n[6]:\nφ ←φ −α(∇φ′L(τi|φ′) + η∇φL(τs|φ))\n(12)\nFurthermore, we can generalize the above optimization\nprocedure to m tasks. Given m tasks [τ1, τ2, ..., τm] belong-\ning to the same categories Ctask, the training procedure can\nbe formalized as:\nφ ←φ −α\nm\nX\ni=1\nηi∇θiL(τi|θi)\n(13)\nθi ←θi−1 −β∇L(τi|θi−1)\n(14)\nθ0 is initialized from φ, and θi denotes the parameters after\nthe i-th inner update.\nThe integral optimization procedure involves two nested\nloops called inner loop and outer loop respectively. The\ninner loop in Equation 14 is the update procedure of param-\neters θ with inner loop learning rate β, and all the tasks τi\nbelong to Ttask. The outer loop is composed of multiple\ninner loops with different classes Ctask. The parameter φ\nis updated once in each outer step as shown in Equation 13.\nAnd α denotes the outer loop learning rate. In pre-training\nstage, we train the feature extractor in Algorithm 1 on the\nbase set Dbase.\n3.4. Theoretical Analysis\nWe ﬁrst use Taylor Expansion [20] to approximate the\noptimization process of MAML [6], Reptile [20], and Am-\nphibian. To simplify the problem, we take two inner steps\nas an example. In one outer step Ttask, we sample two\ntasks: τ1, τ2 belong to the same categories: Ctask. Let\nL(τ|φ) denote the cost function on task τ with parame-\nter φ, and fφ denotes the feature extractor.\nWe deﬁne\nL′\nφ(τ|φ′) =\n∂\n∂φL(τ|φ′) and L′′\nφ(τ|φ′) =\n∂2\n∂φ2 L(τ|φ′)\nIn MAML, we use the ﬁrst order approximate version.\nThe gradients of MAML and Reptile can be formalized as\n[20]:\ngMAML = L′\nφ(τ1|φ) −βL′′\nφ(τ1|φ)L′\nφ(τ1|φ)\n(15)\ngReptile = 2L′\nφ(τ1|φ) −βL′′\nφ(τ1|φ)L′\nφ(τ1|φ)\n(16)\nThe derivation of our Amphibian is provided in appendix\nand the ﬁnal formulation is:\ngAmphibian = 2L′\nφ(τ1|φ) −βL′′\nφ(τ2|φ)L′\nφ(τ1|φ)\n(17)\nThe difference between Amphibian and Reptile is re-\nﬂected in the second order term.\nFigure 3. Schematic illustration of Reptile and Amphibian in the\nparameter space. W∗\ni denotes the manifold of the optimal solu-\ntion for task τi. τ1 and τ2 belong to classes Ctask1. τ3 and τ4\nbelong to classes Ctask2. Ctask1 is disjoint with Ctask2. Let τ1\nis the support set and τ2 is the query set. Although Reptile ﬁnds\na weight initialization φReptile that is close to the manifold of op-\ntimal solution for all tasks, it can not preserve the class neighbor\nstructure. When adapting to τ1, the parameter is actually getting\naway from the manifold of the optimal solution for τ2. Our pro-\nposed Amphibian ﬁnds a weight initialization φAmphibian that is\nclose to all manifold of optimal solution. At the same time, the\noptimal solutions for the tasks belonging to the same classes are\nclose to each other. This characteristic is deﬁned as class neigh-\nborhood structure in the parameter space. When adapting to τ1,\nthe parameter is closer to the manifold of optimal solution for τ2.\nWe further analyze them in the parameter space to show\nthe difference of Reptile and Amphibian. When adapting,\nour objective is to update the model one step using the seen\ntask τs, and perform well in all the unseen tasks τu belong-\ning to the same classes. The objective can be formalized\nas:\nmin\nφ E[L(τu|φ′)]\n(18)\nwhere φ′ denotes the updated parameter, φ′ = φ−βL′\nφ(τs).\nEquation 18 can be decomposed into two parts:\nmin\nφ E[L(τu|φ′)] = min\nφ E[(L(τu|φ′) −L(τu|φ)) + L(τu|φ)]\n(19)\nWe can estimate the ﬁrst term by Taylor Expansion:\nL(τu|φ′) =L(τu|φ)+\nL′\nφ(τu|φ)(φ′ −φ) + O(δ2)\n(20)\nSubstituting Equation 20 into Equation 19, we get:\nmin\nφ E[L′\nφ(τu|φ)(φ′ −φ) + L(τu|φ)]\n(21)\nConsidering φ′ = φ −βL′\nφ(τs), we get:\nmin\nφ E[−βL′\nφ(τu|φ)L′\nφ(τs|φ) + L(τu|φ)]\n(22)\nThe ﬁrst term can be further simpliﬁed as:\n−ηCos(L′\nφ(τu|φ), L′\nφ(τs|φ))\n(23)\n",
    "Let ˆφi = L′\nφ(τi|φ), we get:\nmin\nφ E[−ηCos(ˆφs, ˆφu) + L(τu|φ)]\n(24)\nwhere ˆφi denotes the relative position of the optimal solu-\ntion φi for the task τi to the weight initialization. By cal-\nculate the expectation of Equation 24, the objective can be\nformalized as:\nmin\nφ Eτs[Eτu[−ηCos(ˆφs, ˆφu)]] + Eτs[L(τs|φ)]\n(25)\nwhere the ﬁrst term is exactly the class neighborhood struc-\nture in the parameter space. And the second term denotes\nminimizing the empirical risk. As shown in Figure 3, we\nargue that preserving the class neighborhood structure in\nparameter space is more effective than minimizing the Eu-\nclidean distance like Reptile.\n4. Experimental results\n4.1. Datasets\nThe miniImageNet [34] consists of 100 classes of Im-\nageNet [4] and each class has 600 images of size 84 × 84.\nWe follow the standard split proposed in [22]: the whole\ndataset is divided into three subsets which take 64, 16, and\n20 classes for training, validation, and test.\nThe tieredImageNet [23] has 608 classes which are ran-\ndomly chosen from the ImageNet [4]. As proposed in [23],\nthe 608 classes are split into training, validation, test sub-\nsets which contain 351, 97, and 160 classes respectively. It\nis further split into 34 high-level semantic categories includ-\ning 20, 6, 8 classes for training, validation and test. In total,\nthere are 779,165 images with a size of 84 × 84.\nThe CIFAR-FS includes 100 classes which is derived\nfrom CIFAR-100 dataset [1] and each class has 600 images\nof size 32 × 32. The whole dataset is divided into three\nsubsets: 64 training classes, 16 validation classes and 20\ntest classes.\nThe FC100 is a subset of CIFAR-100 dataset [21] which\nis composed of 100 classes. We follow the standard split\nproposed in [16]. The 100 classes are split into 60, 20,\nand 20 classes for training, validation and test. It is fur-\nther divided into 20 higher level semantic classes including\n12 training, 4 validation and 4 test classes. In total, there\nare 60,000 images of size 32 × 32.\nThe CUB-200-2011 [35] is a ﬁne-grained dataset of\nbirds. It contains 200 species and 11,788 images. We fol-\nlowed the commonly used evaluation protocol proposed by\n[12]. The dataset has 100 training classes, 50 validation\nclasses and 50 test classes. Each image is resized to 84×84.\n4.2. Implementation Details\nNetwork Architectures To ensure a fair comparison\nwith existing methods, we adopt two commonly used back-\nbones as our feature extractor: Conv-128 [7] and WRN-28\n[36]. Conv-128 is composed of 4 convolutional modules\nwith 3 × 3 convolutions, each followed by a BatchNorm\n[14], a ReLU nonlinearity [19], and a 2 × 2 max-pooling\nunit. With input images of size 84 × 84, the output feature\nmap has size 128 × 5 × 5 and then be ﬂattened into a ﬁ-\nnal 3200-dimension feature vector. WRN-28 is a 28-layer\nWide Residual Network [36] with a width factor 10. It con-\nsists of 3 blocks, and each block has 4 BasicBlocks [11]. A\nBasicBlock is composed of 2 convolutional modules with\n3 × 3 convolutions, each followed by a BatchNorm [14],\na ReLU nonlinearity [19] and a short-cut connection [11].\nThere is a dropout unit [31] at the end of the each block.\nThe size of input image is 84 × 84 and the output feature is\n640-dimension after the global average pooling in the last\nblock.\nTraining and Evaluation Firstly, we random sample m\nbatches as m tasks in each outer step. Then, we apply SGD\non these m tasks and get the inner loop parameter θm. The\nreal parameter φ of our model Fφ(·) is updated by θm. In\nour experiment, m is set to 5. We use random rotation and\nrandom crop for data augmentation at the training stage. For\neach novel task, we ﬁrstly adapt our feature extractor to the\nsupport set and evaluate on the query set. The query set\ncontains 15 samples per class which is consistent with ex-\nisting works. The accuracy is averaged from 600 episodes\nwith 95% conﬁdence interval. Detailed setups in our exper-\niments are provided in appendix.\n4.3. Comparison with State-of-the-arts\nminiImageNet and tieredImageNet Table 1 summa-\nrizes the results on miniImageNet [34] and tieredImageNet\n[23] where our approach outperforms existing state-of-the-\nart methods with a signiﬁcant improvement.\nCompared\nwith MTL [32], our method shows good generalization abil-\nity. MTL only updates a Scale-and-Shift parameter on novel\ntasks to prevent overﬁtting while we update the whole fea-\nture extractor. It takes more risks of overﬁtting when up-\ndating far more parameters to adapt to novel tasks. We still\noutperform MTL by 12% on 5-way 5-shot miniImageNet\nclassiﬁcation. From the last two rows in Table 1, we note\nthat our proposed Amphibian can further improve the per-\nformance which is consistent with our theoretical analysis.\nCIFAR-FS and FC100 Results are reported in Ta-\nble 2 which illustrates our state-of-the-art performance on\nCIFAR-FS and FC100. Especially in 5-shot, our method\nachieves the accuracy of 89.3% in CIFAR-FS and 66.9% in\nFC100. We outperforms MetaOptNet-SVM [16] by 5.1%\nand 11.1% respectively. FC100 is a harder dataset with a\nlarge gap between base and novel classes. In such a tough\ndataset, we still achieve state-of-the-art performance which\nshows our capability of fast and generalized adaptation.\nminiImageNet −→CUB-200-2011 To further highlight\n",
    "Table 1. Few-shot classiﬁcation accuracy (%) on miniImageNet and tiredImageNet. Results are collected from [16].\nAlgorithm\nBackbone\nminiImageNet\ntieredImageNet\n1-shot\n5-shot\n1-shot\n5-shot\nMeta Learning\nMeta-LSTM [28]\n64-64-64-64\n43.44 ± 0.77 60.60 ± 0.71\n-\n-\nMAML [6]\n32-32-32-32\n48.70 ± 1.84 63.11 ± 0.92 51.67 ± 1.81 70.30 ± 1.75\nReptile [20]\n32-32-32-32\n49.97 ± 0.32 65.99 ± 0.58\n-\n-\nTADAM [21]\nResNet-12\n58.50 ± 0.30 76.70 ± 0.30\n-\n-\nMTL [32]\nResNet-12\n61.20 ± 1.80 75.50 ± 0.80\n-\n-\nMetaOptNet-SVM [16]\nResNet-12\n62.64 ± 0.61 78.63 ± 0.46 65.99 ± 0.72 81.56 ± 0.53\nwDAE-GNN [8]\nWRN-28\n62.96 ± 0.15 78.85 ± 0.10 68.18 ± 0.16 83.09 ± 0.12\nFine-tuning [5]\nWRN-28\n57.73 ± 0.62 78.17 ± 0.49 66.58 ± 0.70 85.55 ± 0.48\nMetric Learning\nMatchingNet [34]\n64-64-64-64\n43.56 ± 0.84 55.31 ± 0.73\n-\n-\nProtoNet [30]\n64-64-64-64\n49.42 ± 0.78 68.20 ± 0.66 53.31 ± 0.89 72.69 ± 0.74\nRelationNet [33]\n64-96-128-256 50.44 ± 0.82 65.32 ± 0.70 54.48 ± 0.93 71.32 ± 0.78\nDynamic Few-shot [7]\n64-64-128-128 56.20 ± 0.86 73.00 ± 0.64\n-\n-\nBaseline++ [3]\n64-64-64-64\n48.24 ± 0.75 66.43 ± 0.63\n-\n-\nDN4 [17]\n64-64-64-64\n51.24 ± 0.74 71.02 ± 0.64\n-\n-\nLEO [27]\nWRN-28\n61.76 ± 0.08 77.59 ± 0.12 66.33 ± 0.05 81.44 ± 0.09\nOurs\nACC\nWRN-28\n62.08 ± 0.54 85.41 ± 0.62 66.23 ± 0.65 83.42 ± 0.85\nACC + Amphibian\nWRN-28\n64.21 ± 0.62 87.75 ± 0.73 68.77 ± 0.69 86.75 ± 0.79\nTable 2. Few-shot classiﬁcation accuracy (%) on CIFAR-FS and FC100. Results are collected from [16].\nAlgorithm\nBackbone\nCIFAR-FS\nFC100\n1-shot\n5-shot\n1-shot\n5-shot\nMeta Learning\nMAML [6]\n32-32-32-32\n58.9 ± 1.9\n71.5 ± 1.0\n-\n-\nR2D2 [1]\nConv-512\n65.3 ± 0.2\n79.4 ± 0.1\n-\n-\nTADAM [21]\nResNet-12\n-\n-\n40.1 ± 0.4\n56.1 ± 0.4\nMetaOptNet-SVM [16]\nResNet-12\n72.0 ± 0.7\n84.2 ± 0.5\n41.1 ± 0.6\n55.5 ± 0.6\nMetric Learning\nProtoNet [30]\n64-64-64-64\n55.5 ± 0.7\n72.0 ± 0.6\n35.3 ± 0.6\n48.6 ± 0.6\nRelationNet [33]\n64-96-128-256\n55.0 ± 1.0\n69.3 ± 0.8\n-\n-\nOurs\nACC\nWRN-28\n72.6 ± 0.5\n88.4 ± 0.6\n40.2 ± 0.5\n62.7 ± 0.6\nACC + Amphibian\nWRN-28\n73.1 ± 0.5\n89.3 ± 0.9\n41.6 ± 0.4\n66.9 ± 0.5\nTable 3. Results on cross-domain few-shot learning: miniIma-\ngeNet −→CUB-200-2011. Results are collected from [3].\nAlgorithm\n1-shot\n5-shot\nMatchingNet [34]\n-\n53.07\nProtoNet [30]\n-\n62.02\nMAML [6]\n-\n51.34\nRelationNet [33]\n-\n57.71\nACC\n35.19\n74.99\nACC + Amphibian\n39.98\n77.34\nthe adaptation ability of our method, we conduct extensive\nexperiments in cross-domain few-shot learning: miniIma-\ngeNet −→CUB-200-2011. The results are shown in Table\n3. We train the model on the base classes in miniImageNet\nand evaluate it on the novel classes in CUB-200-2011. The\ncross-domain few-shot learning is a more tough challenge\ndue to the huge gap between the two datasets. Our pro-\nposed method outperforms other methods by a large mar-\ngin achieving the state-of-the-art result. Note the results of\nother methods is evaluated with image size of 224 × 224,\nwhile we evaluate with image size of 84 × 84. Smaller in-\nput images contain less information. So we actually test our\nmethod in a more difﬁcult setting. However, our proposed\nACC+Amphibian still outperforms existing methods with a\nsigniﬁcant margin. It illustrates that the ACC+Amphibian is\nable to fast adapt to novel tasks even under a large domain\ngap.\n4.4. Ablation Study\nBackbone Comparison Table 4 shows the results on\nthe 5-way miniImageNet and tiredImageNet using different\nbackbone as the feature extractor. At test time, CC directly\nuses mean vectors as classiﬁcation weights without adapt-\ning the feature extractor to novel classes. Our proposed\nACC makes adaptation to novel classes which outperforms\nCC on all tasks. With deeper backbone, the effect of adapta-\ntion becomes more signiﬁcant. For instance, ACC exceeds\nCC by 0.27% on 5-shot miniImageNet with Conv-128 and\nnotably, it exceeds CC by 7.8% with WRN-28. As we know,\n",
    "Table 4. Results on miniImageNet and tiredImageNet with different backbones. The results of CC are reported by our implementation.\nAlgorithm\nBackbone\nminiImageNet\ntieredImageNet\n1-shot\n5-shot\n1-shot\n5-shot\nCC\nConv-128\n52.42\n71.05\n55.56\n72.87\nACC\nConv-128\n52.81\n71.32\n56.06\n73.98\nACC + Amphibian\nConv-128\n53.36\n74.75\n56.88\n75.08\nCC\nWRN-28\n60.32\n77.61\n64.59\n81.47\nACC\nWRN-28\n62.08\n85.41\n66.23\n83.42\nACC + Amphibian\nWRN-28\n64.21\n87.75\n68.77\n86.75\nFigure 4. Accuracy comparison of adaptation and ﬁne-tuning in\ndifferent steps. The results are reported on 5-way miniImageNet\nclassiﬁcation tasks. Best viewed in color.\nTable 5. Comparison with MAML and Reptile under the same\nexperiment settings on miniImageNet. Our proposed Amphibian\noutperforms MAML and Reptile by a signiﬁcant margin. Results\nare collected from [20].\nAlgorithm\n1-shot\n5-shot\nMAML + Transduction\n48.70\n63.11\nFOMAML + Transduction\n48.07\n63.15\nReptile\n47.07\n62.74\nReptile + Transduction\n49.97\n65.99\nAmphibian\n47.95\n67.58\nAmphibian + Transduction\n50.58\n68.48\nﬁne-tuning a deep network on few data is easily to overﬁt\nand that is why MTL [32] merely ﬁne-tunes a small scalar\nparameter. However, we adapt the whole feature extractor\non small data without overﬁtting even if the backbone goes\ndeeper. The results suggest that our proposed method can\nadapt deep networks in few-shot scenario without overﬁt-\nting.\nImpact of Adaptation We show the accuracy compar-\nison of adaptation and ﬁne-tuning in Figure 4. CC+ﬁne-\ntuning refers to retraining a cosine classiﬁer for novel\nclasses at test time, which is depicted by the dashed lines\nin Figure 4. It is clear to see that with our adaptation (see\nsolid lines), the model fast adapts to the novel classes in a\nfew steps and achieves remarkable performance. After 2\nsteps, ACC achieves the highest accuracy of 62.08% in 1-\nshot. And after 6 steps, it achieves the accuracy of 85.41%\nin 5-shot. From the high performance, we highlight that\nour method has strong ability of fast adaptation to speciﬁc\ntasks without overﬁtting. The ﬁne-tuning method performs\npoorly in few-shot scenario. It converges slowly and suffers\nfrom severe overﬁtting in small dataset.\nImpact of Amphibian In Table 4 and Figure 4, the result\ncomparison of ACC and ACC+Amphibian shows the im-\nprovement brought by Amphibian. It ﬁnds a better weight\ninitialization of the feature extractor, which increases the\naccuracy by 3.33% in 5-shot tieredImageNet with WRN-28.\nFurthermore, we compare our Amphibian with MAML and\nReptile in Table 5 to show the superiority of our method.\nFor fair comparison, we use the same CNN architectures\nand data preprocessing as adopted in [6]. The optimizer\nis reset in every task to prevent information leakage when\ntesting. As indicated in Reptile [20], [6] uses transduction\nsetting for evaluating thus, we further provide the results un-\nder transduction setting. It shows that Amphibian surpasses\nMAML and Reptile in all cases. In 5-way 5-shot, Amphib-\nian outperforms Reptile by a signiﬁcant margin of 4.84%.\nThe results verify the theoretical analysis in section 3.4 that\nour method is more valid than Reptile to ﬁnd a better weight\ninitialization.\n5. Conclusions\nWe propose Adaptable Cosine Classiﬁer and Amphibian\nto achieve both fast and generalized adaptation for few-shot\nlearning. Adaptable Cosine Classiﬁer provides the ability\nof fast adapting a metric based model on few data without\noverﬁtting. And Amphibian further improves the perfor-\nmance by learning a weight initialization that can perform\ngreat generalization to novel tasks. The theoretical expla-\nnation is provided to explain the rationale of our proposed\nAmphibian. It shows that Amphibian can learn a parame-\nter space where optimal solutions for the tasks belongs to\nthe same class are close to each other. We achieve state-\nof-the-art performance on four few-shot benchmarks. Fur-\nthermore, our method outperforms existing works on the\n",
    "cross-domain few-shot problem.\nReferences\n[1] Luca Bertinetto, Joao F. Henriques, Philip Torr, and An-\ndrea Vedaldi. Meta-learning with differentiable closed-form\nsolvers. In International Conference on Learning Represen-\ntations, 2019. 6, 7\n[2] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L Yuille. Deeplab: Semantic image\nsegmentation with deep convolutional nets, atrous convolu-\ntion, and fully connected crfs. IEEE transactions on pattern\nanalysis and machine intelligence, 40(4):834–848, 2017. 1\n[3] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Wang,\nand Jia-Bin Huang. A closer look at few-shot classiﬁcation.\nIn International Conference on Learning Representations,\n2019. 7\n[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 248–255. Ieee,\n2009. 6\n[5] Guneet S Dhillon, Pratik Chaudhari, Avinash Ravichandran,\nand Stefano Soatto. A baseline for few-shot image classiﬁ-\ncation. arXiv preprint arXiv:1909.02729, 2019. 7\n[6] Chelsea Finn, Pieter Abbeel, and Sergey Levine.\nModel-\nagnostic meta-learning for fast adaptation of deep networks.\nIn Proceedings of the International Conference on Machine\nLearning, pages 1126–1135. JMLR. org, 2017. 1, 2, 3, 4, 5,\n7, 8\n[7] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot\nvisual learning without forgetting.\nIn Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 4367–4375, 2018. 1, 2, 3, 6, 7\n[8] Spyros Gidaris and Nikos Komodakis. Generating classiﬁ-\ncation weights with gnn denoising autoencoders for few-shot\nlearning. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 21–30, 2019. 7\n[9] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE Inter-\nnational Conference on Computer Vision, pages 1440–1448,\n2015. 1\n[10] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE International\nConference on Computer Vision, pages 2961–2969, 2017. 1\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770–778, 2016. 1, 6\n[12] Nathan Hilliard, Lawrence Phillips, Scott Howland, Art¨em\nYankov, Courtney D Corley, and Nathan O Hodas. Few-shot\nlearning with metric-agnostic conditional embeddings. arXiv\npreprint arXiv:1802.04376, 2018. 6\n[13] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term\nmemory. Neural computation, 9(8):1735–1780, 1997. 3\n[14] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. In Proceedings of the International Conference\non Machine Learning, pages 448–456, 2015. 6\n[15] Yann LeCun, L´eon Bottou, Yoshua Bengio, Patrick Haffner,\net al. Gradient-based learning applied to document recog-\nnition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n1\n[16] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and\nStefano Soatto. Meta-learning with differentiable convex op-\ntimization. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 10657–10665,\n2019. 2, 6, 7\n[17] Wenbin Li, Lei Wang, Jinglin Xu, Jing Huo, Yang Gao, and\nJiebo Luo. Revisiting local descriptor based image-to-class\nmeasure for few-shot learning. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\npages 7260–7268, 2019. 7\n[18] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 3431–3440, 2015. 1\n[19] Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units\nimprove restricted boltzmann machines. In Proceedings of\nthe International Conference on Machine Learning, pages\n807–814, 2010. 6\n[20] Alex Nichol, Joshua Achiam, and John Schulman.\nOn\nﬁrst-order\nmeta-learning\nalgorithms.\narXiv\npreprint\narXiv:1803.02999, 2018. 1, 2, 3, 4, 5, 7, 8\n[21] Boris Oreshkin, Pau Rodr´ıguez L´opez, and Alexandre La-\ncoste. Tadam: Task dependent adaptive metric for improved\nfew-shot learning. In Advances in Neural Information Pro-\ncessing Systems, pages 721–731, 2018. 6, 7\n[22] Sachin Ravi and Hugo Larochelle. Optimization as a model\nfor few-shot learning. In International Conference on Learn-\ning Representations, 2017. 6\n[23] Mengye Ren, Sachin Ravi, Eleni Triantaﬁllou, Jake Snell,\nKevin Swersky, Josh B. Tenenbaum, Hugo Larochelle, and\nRichard S. Zemel. Meta-learning for semi-supervised few-\nshot classiﬁcation. In International Conference on Learning\nRepresentations, 2018. 6\n[24] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. In Advances in Neural Information Pro-\ncessing Systems, pages 91–99, 2015. 1\n[25] Herbert Robbins and Sutton Monro. A stochastic approxima-\ntion method. The Annals of Mathematical Statistics, pages\n400–407, 1951. 3\n[26] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In International Conference on Medical image com-\nputing and computer-assisted intervention, pages 234–241.\nSpringer, 2015. 1\n[27] Andrei Rusu, Dushyant Rao, Jakub Sygnowski, Oriol\nVinyals, Razvan Pascanu, Simon Osindero, and Raia Had-\nsell.\nMeta-learning with latent embedding optimization.\nIn International Conference on Learning Representations,\n2019. 7\n[28] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan\nWierstra, and Timothy Lillicrap.\nMeta-learning with\nmemory-augmented neural networks.\nIn Proceedings of\n",
    "the International Conference on Machine Learning, pages\n1842–1850, 2016. 2, 3, 7\n[29] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. In In-\nternational Conference on Learning Representations, 2015.\n1\n[30] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypi-\ncal networks for few-shot learning. In Advances in Neural\nInformation Processing Systems, pages 4077–4087, 2017. 2,\n7\n[31] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya\nSutskever, and Ruslan Salakhutdinov. Dropout: a simple way\nto prevent neural networks from overﬁtting. The journal of\nmachine learning research, 15(1):1929–1958, 2014. 6\n[32] Qianru Sun, Yaoyao Liu, Tat-Seng Chua, and Bernt Schiele.\nMeta-transfer learning for few-shot learning.\nIn Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 403–412, 2019. 2, 3, 6, 7, 8\n[33] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS\nTorr, and Timothy M Hospedales. Learning to compare: Re-\nlation network for few-shot learning. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 1199–1208, 2018. 2, 7\n[34] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan\nWierstra, et al. Matching networks for one shot learning. In\nAdvances in Neural Information Processing Systems, pages\n3630–3638, 2016. 1, 2, 6, 7\n[35] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-\nona, and Serge Belongie. The caltech-ucsd birds-200-2011\ndataset. Advances in Water Resources, 2011. 6\n[36] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-\nworks. In British Machine Vision Conference, 2016. 6\n",
    "A. Theoretical Analysis\nA.1. Gradient Formula\nIn this section, we give the derivation of Eq. (17) in the\npaper. We deﬁne L′\nφ(τ|φ) =\n∂\n∂φL(τ|φ) and L′′\nφ(τ|φ) =\n∂2\n∂φ2 L(τ|φ) where τ is a task and φ is the learnable param-\neter of our feature extractor. We take two inner steps as an\nexample to derive the formulation of our proposed Amphib-\nian. φ0 is initialized by φ: φ0 = φ. The updated parameter\nafter two SGD steps:\nφ1 = φ0 −βL′\nφ0(τ1|φ0)\n(26)\nφ2 = φ0 −βL′\nφ0(τ1|φ0) −βL′\nφ1(τ2|φ1)\n(27)\nIn this manuscript, β is the inner loop learning rate, which\nis consistent with the description in our paper. Then, we get\nthe formulation of L′\nφ1(τ2|φ1):\nL′\nφ1(τ2|φ1) = L′\nφ0(τ2|φ0) + L′′\nφ0(τ2|φ0)(φ1 −φ0) + O(δ2)\n= L′\nφ0(τ2|φ0) −βL′′\nφ0(τ2|φ0)L′\nφ0(τ1|φ0) + O(δ2)\n(28)\nWe provide the gradient formula of our Amphibian as:\ngAmphibian = L′\nφ0(τ1|φ0) + L′\nφ0(τ2|φ1)\n= L′\nφ0(τ1|φ0) + L′\nφ0(τ2|φ0)\n−βL′′\nφ0(τ2|φ0)L′\nφ0(τ1|φ0) + O(δ2)\n(29)\nA.2. Analysis in the Parameter Space\nIn this section, we give more detailed analysis for clear\ncomparison with other method. As mentioned in the paper,\nReptile is an extended version of MAML. Thus we only\ngive the objective of Reptile for illustration.\nReptile targets to update on task τs one step and achieves\nthe best performance on τs. The objective can be formulated\nas:\nmin\nφ E[L(τs|φ′)]\n(30)\nφ′ = φ −βL′\nφ(τs|φ)\n(31)\nEq. 30 can be decomposed into two parts:\nmin\nφ E[L(τs|φ′)] = min\nφ E[(L(τs|φ′) −L(τs|φ)) + L(τs|φ)]\n(32)\nWe can estimate the ﬁrst term by Taylor Expansion:\nL(τs|φ′) =L(τs|φ)+\nL′\nφ(τs|φ)(φ′ −φ) + O(δ2)\n(33)\nSubstituting Eq. 33 into Eq. 32, we get:\nmin\nφ E[L′\nφ(τs|φ)(φ′ −φ) + L(τs|φ)]\n(34)\n(a) Accuracy with different adaptation steps.\n(b) Accuracy with different ﬁne-tuning steps.\nFigure 5. Accuracy comparison with adaptation and ﬁne-tuning on\nminiImageNet.\nWe take Eq. 31 into Eq. 34 then the objective of Reptile is:\nmin\nφ E[−βL′\nφ(τs|φ)L′\nφ(τs|φ) + L(τs|φ)]\n(35)\nThe ﬁrst term can be further simpliﬁed as:\n−β∥L′\nφ(τs|φ)∥2\n2\n(36)\nThe overall objective of Reptile can be formulated as:\nmin\nφ E[−β∥L′\nφ(τs|φ)∥2\n2 + L(τs|φ)]\n(37)\nWe give the objective of our Amphibian which is the\nsame as the 22-th equation of the paper:\nmin\nφ E[−βL′\nφ(τu|φ)L′\nφ(τs|φ) + L(τu|φ)]\n(38)\n",
    "Table 6. Hyper-parameters of ImageNet-derivatives: miniImageNet and tieredImageNet and CIFAR-derivatives: CIFAR-FS and FC100.\nParameter\nImageNet-derivatives\nCIFAR-derivatives\nTrain\nEpoch number\n40\n40\nInner batch size\n64\n256\nInner iteration m\n5\n5\nOuter step size α\n0.5\n0.5\nOuter iterations per epoch\n1000\n1000\nSGD learning rate β (1-20 epoch)\n10−1\n10−1\nSGD learning rate β (21-40 epoch)\n6 × 10−3\n6 × 10−3\nSGD momentum\n0.9\n0.9\nSGD weight decay\n5 × 10−4\n5 × 10−4\nDropout rate\n0.3\n0.3\nEvaluation\nAdam learning rate\n10−4\n10−4\nThe ﬁrst term is derived as:\n−βL′\nφ(τu|φ)L′\nφ(τs|φ) =\n−β∥L′\nφ(τu|φ)∥2 · ∥L′\nφ(τs|φ)∥2 · Cos(L′\nφ(τu|φ), L′\nφ(τs|φ))\n(39)\nThus the overall objective of our Amphibian is:\nmin\nφ E[−ηCos(L′\nφ(τu|φ), L′\nφ(τs|φ)) + L(τu|φ)]\n(40)\nwhere\nη = β∥L′\nφ(τu|φ)∥2 · ∥L′\nφ(τs|φ)∥2\n(41)\nFrom the derivation of Reptile and Amphibian, we can\nsee that Reptile targets to ﬁnd a point where the L2 norm of\nthe gradient is maximum. At this point, models can achieve\nfast gradient update. Notably, our Amphibian considers not\nonly the L2 norm of the gradient but also the gradient direc-\ntion. It shows that our method can achieve fast and general-\nized adaptation in the parameter space. The formulation in\nthe paper is simpliﬁed for illustration.\nB. Setup\nWe display the detailed setup in Table 6. The hyper-\nparameters of different datasets are almost same. It demon-\nstrates that our method can perform well on different\ndatasets without much hyper-parameter tuning.\nC. Comparison with Fine-tuning and Adapta-\ntion\nThe accuracy comparison displayed in the paper only\ngives the ﬁne-tuning accuracy within 10 steps. For clear\nillustration, we give the complete accuracy curve of ﬁne-\ntuning as shown in Figure 5(b). Fine-tuning is slower and\ntakes much more steps than adaptation (see Figure 5(a)).\nWe can see that CC+ﬁne-tuning just achieves the 5-shot ac-\ncuracy of 70% which is much lower than our ACC. ACC\ncan be viewed as CC+adaptation which takes 6 adaptation\nsteps to achieve the best 5-shot accuracy of 85.41%.\nIt\ndemonstrates the ability of fast and generalized adaptation\nof our method.\n"
  ],
  "full_text": "arXiv:1911.10807v1  [cs.CV]  25 Nov 2019\nFast and Generalized Adaptation for Few-Shot Learning\nLiang Song, Jinlu Liu, Yongqiang Qin\nAInnovation Technology Co., Ltd.\nsongliang, liujinlu, qinyongqiang@ainnovation.com\nAbstract\nThe ability of fast generalizing to novel tasks from a few\nexamples is critical in dealing with few-shot learning prob-\nlems. However, deep learning models severely suffer from\noverﬁtting in extreme low data regime. In this paper, we\npropose Adaptable Cosine Classiﬁer (ACC) and Amphib-\nian to achieve fast and generalized adaptation for few-shot\nlearning. The ACC realizes the ﬂexible retraining of a deep\nnetwork on small data without overﬁtting. The Amphibian\nlearns a good weight initialization in the parameter space\nwhere optimal solutions for the tasks of the same class clus-\nter tightly. It enables rapid adaptation to novel tasks with\nfew gradient updates. We conduct comprehensive experi-\nments on four few-shot datasets and achieve state-of-the-art\nperformance in all cases. Notably, we achieve the accuracy\nof 87.75% on 5-shot miniImageNet which approximately\noutperforms existing methods by 10%. We also conduct ex-\nperiment on cross-domain few-shot tasks and provide the\nbest results.\n1. Introduction\nDeep Neural Networks have shown great power in vi-\nsual learning tasks such as image classiﬁcation [11, 15, 29],\nobject detection [9, 24, 10], and semantic segmentation\n[18, 26, 2]. However, training these models requires a large\namount of labeled data. In many cases, the model perfor-\nmance sharply drops if the labeled data is scarce. Recent\nresearches pay attention to rapidly learn a model in a data-\nefﬁcient way. The goal is enabling a model to fast adapt to a\nnew task without the need for hundreds of training data. In\nthe low-data regime, fast adaptation to new tasks and avoid-\ning the overﬁtting problem are challenging. These issues\nmotivate the study of few-shot learning.\nFew-shot learning [34, 6, 7] aims to efﬁciently learn a\nmodel that can recognize novel classes when the training\nexamples are extremely limited. It is commonly formalized\nin a meta-learning way where a task is called N-way K-\nshot task if it consists of N classes with K labeled examples\nper class. In few-shot learning, we learn a model on differ-\nTask 1-1\nTask 1-2\nTask 2-1\nTask 2-2\nInner Loop 1\nInner Loop 2\nOuter Loop\nFigure 1. Nested training loops of our proposed Amphibian. In the\ninner loop, parameters are updated using multiple tasks belonging\nto the same classes. In the outer loop, each step is performed with\ndifferent classes.\nent tasks from a large labeled dataset of base classes and\naim to fast adapt the model to unseen tasks of novel classes.\nMost existing methods show limited performance when fac-\ning two major challenges in few-shot learning: rapid adap-\ntation and excellent generalization. Various methods are ex-\nplored to attack the challenges which can be divided into\ntwo categories: metric learning based methods [7, 34] and\nmeta learning based methods [6, 20]. 1) Metric learning\nbased methods target to learn to compare target examples\nand few labeled examples by a distance metric in the em-\nbedding space.\nThese methods usually have weak gen-\neralization ability. Without retraining, the models trained\non the base classes poorly generalize to the novel classes.\nWhile with retraining, the models are prone to overﬁt on\nthe few labeled data. Even so, some metric based learn-\ning approaches have an advantage of learning discrimina-\ntive features. 2) Meta learning based methods aim to learn\na basic model on abundant training tasks and the model can\nquickly converge on new tasks with a few gradient updates\nor ﬁne-tuning steps. A common solution is to ﬁnd a good\ninitialization but some methods learn a biased initialization\nwhich prevents the model from generalizing well to unseen\ntasks.\nIn this paper, our goal is to achieve fast and generalized\nadaptation for few-shot learning. As shown in Figure 2,\nour method consists of two stages: pre-training and adapta-\ntion. We propose an Adaptable Cosine Classiﬁer for dis-\ncriminative feature learning. To further improve generaliza-\ntion, we propose a meta learning method called Amphibian\nwhich ensures the rapid and generalized adaptation. The\nAdaptable Cosine Classiﬁer consists of a feature extractor\n1\n\n\nTask 1\n...\nTask \nFeature\nExtractor\nLearnable\nClassi\u0001cation\nWeight\nInner Loop\nOuter Loop\nTasks from \nsame classes\nTasks from \ndifferent classes\nLoss\nOne task\nBackward\nPre-training with Amphibian\nClassi\u0000cation\nWeight\nLoss\nBackward\nAdaptable\nCosine Classi\u0002er\nSupport Set\nof the Task\nQuery Set\nof the Task\nFeature\nExtractor\nAdapted\nFeature\nExtractor\nScore\nAdapting with Adaptable Cosine Classi\u0003er\nAdaptable\nCosine Classi\u0004er\nClassi\n\u0005cation\nWeight\nFigure 2. Framework of our proposed method. Our proposed method involves two stages pre-training and adaptation. The module in\nlight blue is trainable in each stage. In the pre-training phase, we aim to learn a good initialization with our proposed Amphibian. In the\nadaptation phase, we adapt the feature extractor to speciﬁc tasks using our proposed Adaptable Cosine Classiﬁer.\nand a cosine classiﬁer. It is trained in the ﬁrst stage to learn a\nfeature embedding space where examples in the same class\nare close to each other and far away from examples in other\ndifferent classes. We train the network on the supervised\nclassiﬁcation task in the ﬁrst stage. In the second stage, we\njust adapt the feature extractor on the novel classes. And we\nuse the mean vectors of labeled examples to parametrize the\ncosine classiﬁer since retraining the classiﬁer on few exam-\nples takes risks of overﬁtting.\nTo better train the model, we propose a meta learning\nmethod called Amphibian which ﬁnds a good weight ini-\ntialization for fast and generalized adaptation.\nAn ideal\nweight initialization enables the model to learn from a task\nthrough one gradient update and achieve the best results on\nnew tasks. To ﬁnd a good weight initialization as much as\npossible, we aim to maximize the performance on a new\ntask through one or few gradient updates in current task\nduring the training process. We train the Adaptable Co-\nsine Classiﬁer using Amphibian strategy in the ﬁrst stage.\nThus, we obtain a good weight initialization from the base\nclasses which realizes rapid adaptation and convergence on\nthe novel classes in the second stage.\nIn a word, our method achieves discriminative feature\nlearning and generalization, which takes advantages of met-\nric learning based methods and meta learning based meth-\nods respectively. Our contributions are threefold:\n1. We propose the Adaptable Cosine Classiﬁer which\nhas great generalization ability surpassing the exist-\ning metric learning based methods. Adaptable Cosine\nClassiﬁer is ﬂexible to be retrained and can fast adapt\nto novel tasks.\n2. We propose a meta learning method called Amphibian\nto train a good weight initialization such that it enables\nrapid and generalized adaptation to new tasks with a\nsmall number of gradient updates.\n3. Our experiments show that our method achieves the\nstate-of-the-art and outperforms existing methods by\n4%-10% on two few-shot benchmarks: miniImageNet\nand tieredImageNet. We also evaluate our method in\ncross-domain few-shot classiﬁcation and produce the\nsuperior results on miniImageNet →CUB-200-2011\ntasks.\n2. Related Work\nMany methods have been published to solve the few-\nshot classiﬁcation problem [7, 16, 6, 20, 30, 33, 34, 28, 32].\nIn this section, we brieﬂy introduce some approaches\nrelevant to our work.\nMetric learning based approach\nMetric learning based approaches [34, 30, 33, 7] learn\na projection function that can map examples from the\nimage space to the feature space. And the features preserve\nthe class neighborhood structure in the feature space so\nthat we can recognize them easily.\nInstead of using a\nfully-connected layer, MatchingNet [34] and ProtoNet [30]\nuse the nearest-neighbor method in the feature space with\nthe Euclidean metric.\nAnother commonly used distance\nmetric is the cosine similarity [7].\nThere are also two\ncommonly used methods to calculate the prototypes, using\nlearned prototypes and using the support set to calculate\nprototypes in each task.\nUnlike the above methods,\nRelationNet [33] learns a deep non-linear distance metric\nto replace Euclidean metric or cosine similarity. By this\nmethod, it aims to learn the best metric that can adapt\nto different tasks. All the above methods are based on a\nfundamental hypothesis that the learned feature extractor\ncan be generalized to novel classes.\nBut there is still a\nsigniﬁcant performance gap between the base classes and\nthe novel classes. It indicates that the mapping function\ncan not ﬁt in the novel classes well. So we propose the\nAdaptable Cosine Classiﬁer method that can adapt the\nmetric learning based model to novel tasks to eliminate the\ngap.\n\n\nMeta learning based approach\nMeta learning based approaches [28, 6, 20, 32] typically\ninvolve two phases: pre-training and adapting. Due to the\nparticularity of the few-shot classiﬁcation, the key is how\nto perform generalized adaptation in the adapting stage.\nRavi et al. [28] concentrates on ﬁnding an optimizer that\nis better than SGD [25]. The similarity between gradient\ndescent methods and long short-term memory [13] inspires\nthem to propose the LSTM-based network.\nMAML [6]\nis another typical meta learning based method. It aims to\nlearn a good weight initialization which is close is to all\ntasks. So that with a limited number of labeled examples\nthe model can ﬁnd the task speciﬁc optimal in one gradient\nupdate step. Alex Nichol et al. propose Reptile [20] which\ncan be treated as extended MAML with k gradient update\nsteps. Furthermore, they theoretically analyze the reason\nwhy the MAML liked method works. MTL [32] focuses\non preventing meta overﬁtting by reducing the number\nof learnable parameters.\nThey propose Scale-and-Shift\nparameters to adapt to novel tasks without updating all\nparameters. In this paper, we propose Amphibian to learn\na good weight initialization from which we can perform\nbetter generalized adaptation in limited examples.\n3. Methodology\nIn this section, we ﬁrst give the problem formulation in\nsection 3.1. Secondly, we introduce in section 3.2 the pro-\nposed approach to adapt a metric based method to novel\ntasks. Then we discuss how to train an initialization of the\nfeature extractor by our proposed Amphibian that can fast\nadapt to all tasks in section 3.3. Finally, we provide a theo-\nretical explanation of why Amphibian works in section 3.4.\n3.1. Problem Formulation\nFirstly, we introduce a generic notion of a learning task.\nFormally, each task τ = {Ds, Dq, Ctask, L(Dq|φ)} in-\nvolves two sets: support set Ds and query set Dq respec-\ntively. The examples of Ds and Dq both belong to classes\nCtask. The support set Ds consists of N × K examples\n(N is the number of classes Ctask, and K is the number\nof examples per class) and corresponding labels. And the\nnumber of labeled examples K is a small number like 1 or\n5. The query set Dq involves some unlabeled examples be-\nlong to Ctask. The objective is using the limited labeled\nsupport set to learn a model Fφ that can recognize all the\nexamples from the query set Dq. It can be formulated as\nminimizing the cost function L(Dq|φ). Such a task is a so-\ncalled N-way K-shot few-shot classiﬁcation task.\nIn the few-shot classiﬁcation problem, we are given\ntwo datasets: base set Dbase and novel set Dnovel respec-\ntively. The base set is composed of examples x and the\ncorresponding labels y, Dbase = {(x1, y1), ..., (xn, yn)}.\nAll the examples from the base set belong to class Cbase.\nThe novel set is similar with the base set Dnovel\n=\n{(x′\n1, y′\n1), ..., (x′\nn, y′\nn)}, and all the examples belong to\nclass Cnovel. And Cnovel is disjoint with Cbase. The base\nset is available for us to learn a good model. And evaluate\nthe model on the novel set with few-shot tasks. The objec-\ntive is:\nmin\nφ Eτ[L(Dq|A(Ds, φ))]\n(1)\nwhere A(·) denotes the adaptation procedure.\nAnd\nA(Ds, φ) denotes the updated parameters on Ds.\n3.2. Adaptable Cosine Classiﬁer\nIn general, there is a feature extractor Fφ(·) and a clas-\nsiﬁer Z(·; W) in a deep neural network for classiﬁcation. φ\nis the parameter of the feature extractor Fφ, and W is the\nclassiﬁcation weight of the classiﬁer Z(·; W). The feature\nextractor Fφ maps the data x from the image space to the\nembedding space. Then the classiﬁer Z(·; W) uses the fea-\nture embedding Fφ(x) to estimate the classiﬁcation scores.\nSince there are only few training examples for novel\nclasses in few-shot classiﬁcation, learning a linear classiﬁer\nwith good generalization ability is very difﬁcult. In order to\novercome this critical problem, the Cosine Classiﬁer (CC)\nis proposed [7] which uses cosine similarity instead of dot-\nproduct when computing classiﬁcation scores. The cosine\nsimilarity operator has an advantage in preserving the class\nneighborhood structure in the embedding space. The CC\ncan be formalized as:\nZk(Fφ(x); W) =\neγcos(Fφ(x),Wk)\nP∥C∥\ni=1 eγcos(Fφ(x),Wi)\n(2)\nwhere Zk(·) denotes the classiﬁcation score of the k-th\nclass, Wi is the i-th classiﬁcation weight, γ denotes the\nscale of the softmax operator, and ∥C∥is the number of\nclasses.\nAt the pre-training stage, we train the model on the ex-\namples from base classes Cbase and the objective is to min-\nimize the negative log-likelihood loss:\nL(x|φ, Wb) =\nE\nx,y∈Dbase\n[−logZy(Fφ(x); Wb)]\n(3)\nwhere Wb denotes the classiﬁcation weight vectors of the\nbase categories and y is the true label.\nSince the feature extractor Fφ is trained only on the base\nclasses, it can not extract good features of the novel classes\nwithout adaptation. However, it takes a risk of overﬁtting\nto retrain the CC on the few examples to learn classiﬁcation\nweights of the novel classes. To overcome the difﬁculty,\n\n\nwe propose Adaptable Cosine Classiﬁer (ACC) to adapt the\nfeature extractor to new tasks. As discussed before, the crit-\nical challenge for few-shot classiﬁcation problem is to learn\nthe best classiﬁcation weight of the novel classes without\noverﬁtting on the few training examples. So we give the\ndeﬁnition of the best classiﬁcation weight:\nmin\nWj\nX\ni\nDistance(Wj, Fφ(xi))\n(4)\nwhere Wj is the classiﬁcation weight of the j-th class, and\nxi is the example belonging to class Cj. To be consistent\nwith the cosine operator used at the pre-training stage, we\nadopt −cos(·, ·) as the distance metric in Equation 4. The\noptimization of Equation 4 has the closed formed solution:\nWj =\nP\ni Fφ(xi)/Kj\n∥P\ni Fφ(xi)/Kj∥2\n(5)\nwhere Kj denotes the number of examples belonging to cat-\negory Cj.\nAt the inference stage, we adapt our feature extractor to\nthe novel classes and evaluate on the query set. Since we\ndo not have the true distribution of novel classes, we use the\nsupport set to estimate the true distribution. The objective\nis to minimize the negative log-likelihood loss:\nL(x|φ, Wn) =\nE\nx,y∈Dsupport\n[−logZy(Fφ(x); Wn)]\n(6)\nwhere Wn is calculated directly from Equation 6.\nIn general, to ﬁne-tune CC we need to re-initialize the\nclassiﬁcation weights and retrain the model from scratch.\nHowever, the model tend to overﬁtting with the limited\ntraining data. By our proposed ACC, we compute the best\nclassiﬁcation weights directly. It gives the model the abil-\nity of fast and generalized adaptation to novel tasks. The\nadapted feature extractor can extract more discriminative\nfeatures which are conducive to classiﬁcation. As expected,\nexperimental results also indicate that the adaptation can\nbring us signiﬁcant performance improvement.\n3.3. Amphibian\nTo further improve the ability of adaptation, we pro-\npose a simple yet effective method named Amphibian. Like\nMAML [6] and Reptile [20], Amphibian learns an initial-\nization for the parameters of a model. From this initializa-\ntion, the model can perform fast and generalized adaptation\nto different tasks.\nWhen adapting to a new task τs, we are given a labeled\nsupport set Ds, and an unlabeled query set Dq. Examples\nof both sets are sampled from Ctask. Our objective is to\nupdate the initialization φ of the model one step using the\nsupport set Ds then the updated model is capable of recog-\nnize the examples from the query set Dq. This objective can\nAlgorithm 1 Amphibian\nRequire: α, β: step size hyper-parameters\nInitialize parameters φ of a model\nfor iteration = 1, 2, . . . do\nsample N categories Ctaskfrom Cbase\nInitialize θ0 = φ\nfor i = 1, 2, . . . , m do\nsample K examples per category from Dbase\nthe N × K examples compose a task τi\nCompute L(τi; θi−1)\nUpdate θi ←θi−1 −β∇L(τi; θi−1)\nend for\nUpdate φ ←φ + α(θm −φ)\nend for\nbe formalized as:\nmin\nφ Ex∈Dq[L(x|φ′)]\n(7)\nwhere φ′ denotes the updated parameters using one gradi-\nent descent update on task τs, φ′ = φ −β∇φL(τs|φ). β is\na hyper-parameter. Since the support set Ds and the query\nset Dq is independent. In order to minimize the expecta-\ntion of the loss on Dq, we need to minimize the loss on all\nexamples belonging to Ctask:\nmin\nφ Ex∈Dq[L(x|φ′)] = min\nφ E{x|y∈Ctask}[L(x|φ′)]\n(8)\nEquation 8 can be further formalized as:\nmin\nφ Ex∈Dq[L(x|φ′)] = min\nφ Eτi∈Ttask[L(τi|φ′)]\n(9)\nwhere Ttask denotes all the possible tasks whose exam-\nples belong to Ctask, and τi denotes one task sampled from\nTtask.\nMeanwhile, we aim to perform well in the current task\nτs so that the model can extract task-independent features.\nThe integral objective can be formalized as:\nmin\nφ Eτi∈Ttask[L(τi, φ′)] + ηL(τs|φ)\n(10)\nwhere η is a hyper-parameter, controlling the weight of the\nabove two objectives. Since we can not obtain all the tasks,\nwe use one task τi to estimate Eτ∈Ttask[L(τ, φ′)]. Thus, the\ngradient update process is:\nφ ←φ −α∇φ(L(τi|φ′) + ηL(τs|φ))\n(11)\nwhere α is a hyper-parameter. Note that the cost function\nis computed using the updated parameters φ′, yet the train-\ning parameters are φ. So optimizing parameters φ requires\nthe second order gradient. Higher order gradient requires\n\n\nlots of computational overhead. To efﬁciently solve this op-\ntimization problem, we exploit a ﬁrst-order approximation\n[6]:\nφ ←φ −α(∇φ′L(τi|φ′) + η∇φL(τs|φ))\n(12)\nFurthermore, we can generalize the above optimization\nprocedure to m tasks. Given m tasks [τ1, τ2, ..., τm] belong-\ning to the same categories Ctask, the training procedure can\nbe formalized as:\nφ ←φ −α\nm\nX\ni=1\nηi∇θiL(τi|θi)\n(13)\nθi ←θi−1 −β∇L(τi|θi−1)\n(14)\nθ0 is initialized from φ, and θi denotes the parameters after\nthe i-th inner update.\nThe integral optimization procedure involves two nested\nloops called inner loop and outer loop respectively. The\ninner loop in Equation 14 is the update procedure of param-\neters θ with inner loop learning rate β, and all the tasks τi\nbelong to Ttask. The outer loop is composed of multiple\ninner loops with different classes Ctask. The parameter φ\nis updated once in each outer step as shown in Equation 13.\nAnd α denotes the outer loop learning rate. In pre-training\nstage, we train the feature extractor in Algorithm 1 on the\nbase set Dbase.\n3.4. Theoretical Analysis\nWe ﬁrst use Taylor Expansion [20] to approximate the\noptimization process of MAML [6], Reptile [20], and Am-\nphibian. To simplify the problem, we take two inner steps\nas an example. In one outer step Ttask, we sample two\ntasks: τ1, τ2 belong to the same categories: Ctask. Let\nL(τ|φ) denote the cost function on task τ with parame-\nter φ, and fφ denotes the feature extractor.\nWe deﬁne\nL′\nφ(τ|φ′) =\n∂\n∂φL(τ|φ′) and L′′\nφ(τ|φ′) =\n∂2\n∂φ2 L(τ|φ′)\nIn MAML, we use the ﬁrst order approximate version.\nThe gradients of MAML and Reptile can be formalized as\n[20]:\ngMAML = L′\nφ(τ1|φ) −βL′′\nφ(τ1|φ)L′\nφ(τ1|φ)\n(15)\ngReptile = 2L′\nφ(τ1|φ) −βL′′\nφ(τ1|φ)L′\nφ(τ1|φ)\n(16)\nThe derivation of our Amphibian is provided in appendix\nand the ﬁnal formulation is:\ngAmphibian = 2L′\nφ(τ1|φ) −βL′′\nφ(τ2|φ)L′\nφ(τ1|φ)\n(17)\nThe difference between Amphibian and Reptile is re-\nﬂected in the second order term.\nFigure 3. Schematic illustration of Reptile and Amphibian in the\nparameter space. W∗\ni denotes the manifold of the optimal solu-\ntion for task τi. τ1 and τ2 belong to classes Ctask1. τ3 and τ4\nbelong to classes Ctask2. Ctask1 is disjoint with Ctask2. Let τ1\nis the support set and τ2 is the query set. Although Reptile ﬁnds\na weight initialization φReptile that is close to the manifold of op-\ntimal solution for all tasks, it can not preserve the class neighbor\nstructure. When adapting to τ1, the parameter is actually getting\naway from the manifold of the optimal solution for τ2. Our pro-\nposed Amphibian ﬁnds a weight initialization φAmphibian that is\nclose to all manifold of optimal solution. At the same time, the\noptimal solutions for the tasks belonging to the same classes are\nclose to each other. This characteristic is deﬁned as class neigh-\nborhood structure in the parameter space. When adapting to τ1,\nthe parameter is closer to the manifold of optimal solution for τ2.\nWe further analyze them in the parameter space to show\nthe difference of Reptile and Amphibian. When adapting,\nour objective is to update the model one step using the seen\ntask τs, and perform well in all the unseen tasks τu belong-\ning to the same classes. The objective can be formalized\nas:\nmin\nφ E[L(τu|φ′)]\n(18)\nwhere φ′ denotes the updated parameter, φ′ = φ−βL′\nφ(τs).\nEquation 18 can be decomposed into two parts:\nmin\nφ E[L(τu|φ′)] = min\nφ E[(L(τu|φ′) −L(τu|φ)) + L(τu|φ)]\n(19)\nWe can estimate the ﬁrst term by Taylor Expansion:\nL(τu|φ′) =L(τu|φ)+\nL′\nφ(τu|φ)(φ′ −φ) + O(δ2)\n(20)\nSubstituting Equation 20 into Equation 19, we get:\nmin\nφ E[L′\nφ(τu|φ)(φ′ −φ) + L(τu|φ)]\n(21)\nConsidering φ′ = φ −βL′\nφ(τs), we get:\nmin\nφ E[−βL′\nφ(τu|φ)L′\nφ(τs|φ) + L(τu|φ)]\n(22)\nThe ﬁrst term can be further simpliﬁed as:\n−ηCos(L′\nφ(τu|φ), L′\nφ(τs|φ))\n(23)\n\n\nLet ˆφi = L′\nφ(τi|φ), we get:\nmin\nφ E[−ηCos(ˆφs, ˆφu) + L(τu|φ)]\n(24)\nwhere ˆφi denotes the relative position of the optimal solu-\ntion φi for the task τi to the weight initialization. By cal-\nculate the expectation of Equation 24, the objective can be\nformalized as:\nmin\nφ Eτs[Eτu[−ηCos(ˆφs, ˆφu)]] + Eτs[L(τs|φ)]\n(25)\nwhere the ﬁrst term is exactly the class neighborhood struc-\nture in the parameter space. And the second term denotes\nminimizing the empirical risk. As shown in Figure 3, we\nargue that preserving the class neighborhood structure in\nparameter space is more effective than minimizing the Eu-\nclidean distance like Reptile.\n4. Experimental results\n4.1. Datasets\nThe miniImageNet [34] consists of 100 classes of Im-\nageNet [4] and each class has 600 images of size 84 × 84.\nWe follow the standard split proposed in [22]: the whole\ndataset is divided into three subsets which take 64, 16, and\n20 classes for training, validation, and test.\nThe tieredImageNet [23] has 608 classes which are ran-\ndomly chosen from the ImageNet [4]. As proposed in [23],\nthe 608 classes are split into training, validation, test sub-\nsets which contain 351, 97, and 160 classes respectively. It\nis further split into 34 high-level semantic categories includ-\ning 20, 6, 8 classes for training, validation and test. In total,\nthere are 779,165 images with a size of 84 × 84.\nThe CIFAR-FS includes 100 classes which is derived\nfrom CIFAR-100 dataset [1] and each class has 600 images\nof size 32 × 32. The whole dataset is divided into three\nsubsets: 64 training classes, 16 validation classes and 20\ntest classes.\nThe FC100 is a subset of CIFAR-100 dataset [21] which\nis composed of 100 classes. We follow the standard split\nproposed in [16]. The 100 classes are split into 60, 20,\nand 20 classes for training, validation and test. It is fur-\nther divided into 20 higher level semantic classes including\n12 training, 4 validation and 4 test classes. In total, there\nare 60,000 images of size 32 × 32.\nThe CUB-200-2011 [35] is a ﬁne-grained dataset of\nbirds. It contains 200 species and 11,788 images. We fol-\nlowed the commonly used evaluation protocol proposed by\n[12]. The dataset has 100 training classes, 50 validation\nclasses and 50 test classes. Each image is resized to 84×84.\n4.2. Implementation Details\nNetwork Architectures To ensure a fair comparison\nwith existing methods, we adopt two commonly used back-\nbones as our feature extractor: Conv-128 [7] and WRN-28\n[36]. Conv-128 is composed of 4 convolutional modules\nwith 3 × 3 convolutions, each followed by a BatchNorm\n[14], a ReLU nonlinearity [19], and a 2 × 2 max-pooling\nunit. With input images of size 84 × 84, the output feature\nmap has size 128 × 5 × 5 and then be ﬂattened into a ﬁ-\nnal 3200-dimension feature vector. WRN-28 is a 28-layer\nWide Residual Network [36] with a width factor 10. It con-\nsists of 3 blocks, and each block has 4 BasicBlocks [11]. A\nBasicBlock is composed of 2 convolutional modules with\n3 × 3 convolutions, each followed by a BatchNorm [14],\na ReLU nonlinearity [19] and a short-cut connection [11].\nThere is a dropout unit [31] at the end of the each block.\nThe size of input image is 84 × 84 and the output feature is\n640-dimension after the global average pooling in the last\nblock.\nTraining and Evaluation Firstly, we random sample m\nbatches as m tasks in each outer step. Then, we apply SGD\non these m tasks and get the inner loop parameter θm. The\nreal parameter φ of our model Fφ(·) is updated by θm. In\nour experiment, m is set to 5. We use random rotation and\nrandom crop for data augmentation at the training stage. For\neach novel task, we ﬁrstly adapt our feature extractor to the\nsupport set and evaluate on the query set. The query set\ncontains 15 samples per class which is consistent with ex-\nisting works. The accuracy is averaged from 600 episodes\nwith 95% conﬁdence interval. Detailed setups in our exper-\niments are provided in appendix.\n4.3. Comparison with State-of-the-arts\nminiImageNet and tieredImageNet Table 1 summa-\nrizes the results on miniImageNet [34] and tieredImageNet\n[23] where our approach outperforms existing state-of-the-\nart methods with a signiﬁcant improvement.\nCompared\nwith MTL [32], our method shows good generalization abil-\nity. MTL only updates a Scale-and-Shift parameter on novel\ntasks to prevent overﬁtting while we update the whole fea-\nture extractor. It takes more risks of overﬁtting when up-\ndating far more parameters to adapt to novel tasks. We still\noutperform MTL by 12% on 5-way 5-shot miniImageNet\nclassiﬁcation. From the last two rows in Table 1, we note\nthat our proposed Amphibian can further improve the per-\nformance which is consistent with our theoretical analysis.\nCIFAR-FS and FC100 Results are reported in Ta-\nble 2 which illustrates our state-of-the-art performance on\nCIFAR-FS and FC100. Especially in 5-shot, our method\nachieves the accuracy of 89.3% in CIFAR-FS and 66.9% in\nFC100. We outperforms MetaOptNet-SVM [16] by 5.1%\nand 11.1% respectively. FC100 is a harder dataset with a\nlarge gap between base and novel classes. In such a tough\ndataset, we still achieve state-of-the-art performance which\nshows our capability of fast and generalized adaptation.\nminiImageNet −→CUB-200-2011 To further highlight\n\n\nTable 1. Few-shot classiﬁcation accuracy (%) on miniImageNet and tiredImageNet. Results are collected from [16].\nAlgorithm\nBackbone\nminiImageNet\ntieredImageNet\n1-shot\n5-shot\n1-shot\n5-shot\nMeta Learning\nMeta-LSTM [28]\n64-64-64-64\n43.44 ± 0.77 60.60 ± 0.71\n-\n-\nMAML [6]\n32-32-32-32\n48.70 ± 1.84 63.11 ± 0.92 51.67 ± 1.81 70.30 ± 1.75\nReptile [20]\n32-32-32-32\n49.97 ± 0.32 65.99 ± 0.58\n-\n-\nTADAM [21]\nResNet-12\n58.50 ± 0.30 76.70 ± 0.30\n-\n-\nMTL [32]\nResNet-12\n61.20 ± 1.80 75.50 ± 0.80\n-\n-\nMetaOptNet-SVM [16]\nResNet-12\n62.64 ± 0.61 78.63 ± 0.46 65.99 ± 0.72 81.56 ± 0.53\nwDAE-GNN [8]\nWRN-28\n62.96 ± 0.15 78.85 ± 0.10 68.18 ± 0.16 83.09 ± 0.12\nFine-tuning [5]\nWRN-28\n57.73 ± 0.62 78.17 ± 0.49 66.58 ± 0.70 85.55 ± 0.48\nMetric Learning\nMatchingNet [34]\n64-64-64-64\n43.56 ± 0.84 55.31 ± 0.73\n-\n-\nProtoNet [30]\n64-64-64-64\n49.42 ± 0.78 68.20 ± 0.66 53.31 ± 0.89 72.69 ± 0.74\nRelationNet [33]\n64-96-128-256 50.44 ± 0.82 65.32 ± 0.70 54.48 ± 0.93 71.32 ± 0.78\nDynamic Few-shot [7]\n64-64-128-128 56.20 ± 0.86 73.00 ± 0.64\n-\n-\nBaseline++ [3]\n64-64-64-64\n48.24 ± 0.75 66.43 ± 0.63\n-\n-\nDN4 [17]\n64-64-64-64\n51.24 ± 0.74 71.02 ± 0.64\n-\n-\nLEO [27]\nWRN-28\n61.76 ± 0.08 77.59 ± 0.12 66.33 ± 0.05 81.44 ± 0.09\nOurs\nACC\nWRN-28\n62.08 ± 0.54 85.41 ± 0.62 66.23 ± 0.65 83.42 ± 0.85\nACC + Amphibian\nWRN-28\n64.21 ± 0.62 87.75 ± 0.73 68.77 ± 0.69 86.75 ± 0.79\nTable 2. Few-shot classiﬁcation accuracy (%) on CIFAR-FS and FC100. Results are collected from [16].\nAlgorithm\nBackbone\nCIFAR-FS\nFC100\n1-shot\n5-shot\n1-shot\n5-shot\nMeta Learning\nMAML [6]\n32-32-32-32\n58.9 ± 1.9\n71.5 ± 1.0\n-\n-\nR2D2 [1]\nConv-512\n65.3 ± 0.2\n79.4 ± 0.1\n-\n-\nTADAM [21]\nResNet-12\n-\n-\n40.1 ± 0.4\n56.1 ± 0.4\nMetaOptNet-SVM [16]\nResNet-12\n72.0 ± 0.7\n84.2 ± 0.5\n41.1 ± 0.6\n55.5 ± 0.6\nMetric Learning\nProtoNet [30]\n64-64-64-64\n55.5 ± 0.7\n72.0 ± 0.6\n35.3 ± 0.6\n48.6 ± 0.6\nRelationNet [33]\n64-96-128-256\n55.0 ± 1.0\n69.3 ± 0.8\n-\n-\nOurs\nACC\nWRN-28\n72.6 ± 0.5\n88.4 ± 0.6\n40.2 ± 0.5\n62.7 ± 0.6\nACC + Amphibian\nWRN-28\n73.1 ± 0.5\n89.3 ± 0.9\n41.6 ± 0.4\n66.9 ± 0.5\nTable 3. Results on cross-domain few-shot learning: miniIma-\ngeNet −→CUB-200-2011. Results are collected from [3].\nAlgorithm\n1-shot\n5-shot\nMatchingNet [34]\n-\n53.07\nProtoNet [30]\n-\n62.02\nMAML [6]\n-\n51.34\nRelationNet [33]\n-\n57.71\nACC\n35.19\n74.99\nACC + Amphibian\n39.98\n77.34\nthe adaptation ability of our method, we conduct extensive\nexperiments in cross-domain few-shot learning: miniIma-\ngeNet −→CUB-200-2011. The results are shown in Table\n3. We train the model on the base classes in miniImageNet\nand evaluate it on the novel classes in CUB-200-2011. The\ncross-domain few-shot learning is a more tough challenge\ndue to the huge gap between the two datasets. Our pro-\nposed method outperforms other methods by a large mar-\ngin achieving the state-of-the-art result. Note the results of\nother methods is evaluated with image size of 224 × 224,\nwhile we evaluate with image size of 84 × 84. Smaller in-\nput images contain less information. So we actually test our\nmethod in a more difﬁcult setting. However, our proposed\nACC+Amphibian still outperforms existing methods with a\nsigniﬁcant margin. It illustrates that the ACC+Amphibian is\nable to fast adapt to novel tasks even under a large domain\ngap.\n4.4. Ablation Study\nBackbone Comparison Table 4 shows the results on\nthe 5-way miniImageNet and tiredImageNet using different\nbackbone as the feature extractor. At test time, CC directly\nuses mean vectors as classiﬁcation weights without adapt-\ning the feature extractor to novel classes. Our proposed\nACC makes adaptation to novel classes which outperforms\nCC on all tasks. With deeper backbone, the effect of adapta-\ntion becomes more signiﬁcant. For instance, ACC exceeds\nCC by 0.27% on 5-shot miniImageNet with Conv-128 and\nnotably, it exceeds CC by 7.8% with WRN-28. As we know,\n\n\nTable 4. Results on miniImageNet and tiredImageNet with different backbones. The results of CC are reported by our implementation.\nAlgorithm\nBackbone\nminiImageNet\ntieredImageNet\n1-shot\n5-shot\n1-shot\n5-shot\nCC\nConv-128\n52.42\n71.05\n55.56\n72.87\nACC\nConv-128\n52.81\n71.32\n56.06\n73.98\nACC + Amphibian\nConv-128\n53.36\n74.75\n56.88\n75.08\nCC\nWRN-28\n60.32\n77.61\n64.59\n81.47\nACC\nWRN-28\n62.08\n85.41\n66.23\n83.42\nACC + Amphibian\nWRN-28\n64.21\n87.75\n68.77\n86.75\nFigure 4. Accuracy comparison of adaptation and ﬁne-tuning in\ndifferent steps. The results are reported on 5-way miniImageNet\nclassiﬁcation tasks. Best viewed in color.\nTable 5. Comparison with MAML and Reptile under the same\nexperiment settings on miniImageNet. Our proposed Amphibian\noutperforms MAML and Reptile by a signiﬁcant margin. Results\nare collected from [20].\nAlgorithm\n1-shot\n5-shot\nMAML + Transduction\n48.70\n63.11\nFOMAML + Transduction\n48.07\n63.15\nReptile\n47.07\n62.74\nReptile + Transduction\n49.97\n65.99\nAmphibian\n47.95\n67.58\nAmphibian + Transduction\n50.58\n68.48\nﬁne-tuning a deep network on few data is easily to overﬁt\nand that is why MTL [32] merely ﬁne-tunes a small scalar\nparameter. However, we adapt the whole feature extractor\non small data without overﬁtting even if the backbone goes\ndeeper. The results suggest that our proposed method can\nadapt deep networks in few-shot scenario without overﬁt-\nting.\nImpact of Adaptation We show the accuracy compar-\nison of adaptation and ﬁne-tuning in Figure 4. CC+ﬁne-\ntuning refers to retraining a cosine classiﬁer for novel\nclasses at test time, which is depicted by the dashed lines\nin Figure 4. It is clear to see that with our adaptation (see\nsolid lines), the model fast adapts to the novel classes in a\nfew steps and achieves remarkable performance. After 2\nsteps, ACC achieves the highest accuracy of 62.08% in 1-\nshot. And after 6 steps, it achieves the accuracy of 85.41%\nin 5-shot. From the high performance, we highlight that\nour method has strong ability of fast adaptation to speciﬁc\ntasks without overﬁtting. The ﬁne-tuning method performs\npoorly in few-shot scenario. It converges slowly and suffers\nfrom severe overﬁtting in small dataset.\nImpact of Amphibian In Table 4 and Figure 4, the result\ncomparison of ACC and ACC+Amphibian shows the im-\nprovement brought by Amphibian. It ﬁnds a better weight\ninitialization of the feature extractor, which increases the\naccuracy by 3.33% in 5-shot tieredImageNet with WRN-28.\nFurthermore, we compare our Amphibian with MAML and\nReptile in Table 5 to show the superiority of our method.\nFor fair comparison, we use the same CNN architectures\nand data preprocessing as adopted in [6]. The optimizer\nis reset in every task to prevent information leakage when\ntesting. As indicated in Reptile [20], [6] uses transduction\nsetting for evaluating thus, we further provide the results un-\nder transduction setting. It shows that Amphibian surpasses\nMAML and Reptile in all cases. In 5-way 5-shot, Amphib-\nian outperforms Reptile by a signiﬁcant margin of 4.84%.\nThe results verify the theoretical analysis in section 3.4 that\nour method is more valid than Reptile to ﬁnd a better weight\ninitialization.\n5. Conclusions\nWe propose Adaptable Cosine Classiﬁer and Amphibian\nto achieve both fast and generalized adaptation for few-shot\nlearning. Adaptable Cosine Classiﬁer provides the ability\nof fast adapting a metric based model on few data without\noverﬁtting. And Amphibian further improves the perfor-\nmance by learning a weight initialization that can perform\ngreat generalization to novel tasks. The theoretical expla-\nnation is provided to explain the rationale of our proposed\nAmphibian. It shows that Amphibian can learn a parame-\nter space where optimal solutions for the tasks belongs to\nthe same class are close to each other. We achieve state-\nof-the-art performance on four few-shot benchmarks. Fur-\nthermore, our method outperforms existing works on the\n\n\ncross-domain few-shot problem.\nReferences\n[1] Luca Bertinetto, Joao F. Henriques, Philip Torr, and An-\ndrea Vedaldi. Meta-learning with differentiable closed-form\nsolvers. In International Conference on Learning Represen-\ntations, 2019. 6, 7\n[2] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L Yuille. Deeplab: Semantic image\nsegmentation with deep convolutional nets, atrous convolu-\ntion, and fully connected crfs. IEEE transactions on pattern\nanalysis and machine intelligence, 40(4):834–848, 2017. 1\n[3] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Wang,\nand Jia-Bin Huang. A closer look at few-shot classiﬁcation.\nIn International Conference on Learning Representations,\n2019. 7\n[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 248–255. Ieee,\n2009. 6\n[5] Guneet S Dhillon, Pratik Chaudhari, Avinash Ravichandran,\nand Stefano Soatto. A baseline for few-shot image classiﬁ-\ncation. arXiv preprint arXiv:1909.02729, 2019. 7\n[6] Chelsea Finn, Pieter Abbeel, and Sergey Levine.\nModel-\nagnostic meta-learning for fast adaptation of deep networks.\nIn Proceedings of the International Conference on Machine\nLearning, pages 1126–1135. JMLR. org, 2017. 1, 2, 3, 4, 5,\n7, 8\n[7] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot\nvisual learning without forgetting.\nIn Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 4367–4375, 2018. 1, 2, 3, 6, 7\n[8] Spyros Gidaris and Nikos Komodakis. Generating classiﬁ-\ncation weights with gnn denoising autoencoders for few-shot\nlearning. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 21–30, 2019. 7\n[9] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE Inter-\nnational Conference on Computer Vision, pages 1440–1448,\n2015. 1\n[10] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE International\nConference on Computer Vision, pages 2961–2969, 2017. 1\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770–778, 2016. 1, 6\n[12] Nathan Hilliard, Lawrence Phillips, Scott Howland, Art¨em\nYankov, Courtney D Corley, and Nathan O Hodas. Few-shot\nlearning with metric-agnostic conditional embeddings. arXiv\npreprint arXiv:1802.04376, 2018. 6\n[13] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term\nmemory. Neural computation, 9(8):1735–1780, 1997. 3\n[14] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. In Proceedings of the International Conference\non Machine Learning, pages 448–456, 2015. 6\n[15] Yann LeCun, L´eon Bottou, Yoshua Bengio, Patrick Haffner,\net al. Gradient-based learning applied to document recog-\nnition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n1\n[16] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and\nStefano Soatto. Meta-learning with differentiable convex op-\ntimization. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 10657–10665,\n2019. 2, 6, 7\n[17] Wenbin Li, Lei Wang, Jinglin Xu, Jing Huo, Yang Gao, and\nJiebo Luo. Revisiting local descriptor based image-to-class\nmeasure for few-shot learning. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\npages 7260–7268, 2019. 7\n[18] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 3431–3440, 2015. 1\n[19] Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units\nimprove restricted boltzmann machines. In Proceedings of\nthe International Conference on Machine Learning, pages\n807–814, 2010. 6\n[20] Alex Nichol, Joshua Achiam, and John Schulman.\nOn\nﬁrst-order\nmeta-learning\nalgorithms.\narXiv\npreprint\narXiv:1803.02999, 2018. 1, 2, 3, 4, 5, 7, 8\n[21] Boris Oreshkin, Pau Rodr´ıguez L´opez, and Alexandre La-\ncoste. Tadam: Task dependent adaptive metric for improved\nfew-shot learning. In Advances in Neural Information Pro-\ncessing Systems, pages 721–731, 2018. 6, 7\n[22] Sachin Ravi and Hugo Larochelle. Optimization as a model\nfor few-shot learning. In International Conference on Learn-\ning Representations, 2017. 6\n[23] Mengye Ren, Sachin Ravi, Eleni Triantaﬁllou, Jake Snell,\nKevin Swersky, Josh B. Tenenbaum, Hugo Larochelle, and\nRichard S. Zemel. Meta-learning for semi-supervised few-\nshot classiﬁcation. In International Conference on Learning\nRepresentations, 2018. 6\n[24] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. In Advances in Neural Information Pro-\ncessing Systems, pages 91–99, 2015. 1\n[25] Herbert Robbins and Sutton Monro. A stochastic approxima-\ntion method. The Annals of Mathematical Statistics, pages\n400–407, 1951. 3\n[26] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In International Conference on Medical image com-\nputing and computer-assisted intervention, pages 234–241.\nSpringer, 2015. 1\n[27] Andrei Rusu, Dushyant Rao, Jakub Sygnowski, Oriol\nVinyals, Razvan Pascanu, Simon Osindero, and Raia Had-\nsell.\nMeta-learning with latent embedding optimization.\nIn International Conference on Learning Representations,\n2019. 7\n[28] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan\nWierstra, and Timothy Lillicrap.\nMeta-learning with\nmemory-augmented neural networks.\nIn Proceedings of\n\n\nthe International Conference on Machine Learning, pages\n1842–1850, 2016. 2, 3, 7\n[29] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. In In-\nternational Conference on Learning Representations, 2015.\n1\n[30] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypi-\ncal networks for few-shot learning. In Advances in Neural\nInformation Processing Systems, pages 4077–4087, 2017. 2,\n7\n[31] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya\nSutskever, and Ruslan Salakhutdinov. Dropout: a simple way\nto prevent neural networks from overﬁtting. The journal of\nmachine learning research, 15(1):1929–1958, 2014. 6\n[32] Qianru Sun, Yaoyao Liu, Tat-Seng Chua, and Bernt Schiele.\nMeta-transfer learning for few-shot learning.\nIn Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 403–412, 2019. 2, 3, 6, 7, 8\n[33] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS\nTorr, and Timothy M Hospedales. Learning to compare: Re-\nlation network for few-shot learning. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 1199–1208, 2018. 2, 7\n[34] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan\nWierstra, et al. Matching networks for one shot learning. In\nAdvances in Neural Information Processing Systems, pages\n3630–3638, 2016. 1, 2, 6, 7\n[35] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-\nona, and Serge Belongie. The caltech-ucsd birds-200-2011\ndataset. Advances in Water Resources, 2011. 6\n[36] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-\nworks. In British Machine Vision Conference, 2016. 6\n\n\nA. Theoretical Analysis\nA.1. Gradient Formula\nIn this section, we give the derivation of Eq. (17) in the\npaper. We deﬁne L′\nφ(τ|φ) =\n∂\n∂φL(τ|φ) and L′′\nφ(τ|φ) =\n∂2\n∂φ2 L(τ|φ) where τ is a task and φ is the learnable param-\neter of our feature extractor. We take two inner steps as an\nexample to derive the formulation of our proposed Amphib-\nian. φ0 is initialized by φ: φ0 = φ. The updated parameter\nafter two SGD steps:\nφ1 = φ0 −βL′\nφ0(τ1|φ0)\n(26)\nφ2 = φ0 −βL′\nφ0(τ1|φ0) −βL′\nφ1(τ2|φ1)\n(27)\nIn this manuscript, β is the inner loop learning rate, which\nis consistent with the description in our paper. Then, we get\nthe formulation of L′\nφ1(τ2|φ1):\nL′\nφ1(τ2|φ1) = L′\nφ0(τ2|φ0) + L′′\nφ0(τ2|φ0)(φ1 −φ0) + O(δ2)\n= L′\nφ0(τ2|φ0) −βL′′\nφ0(τ2|φ0)L′\nφ0(τ1|φ0) + O(δ2)\n(28)\nWe provide the gradient formula of our Amphibian as:\ngAmphibian = L′\nφ0(τ1|φ0) + L′\nφ0(τ2|φ1)\n= L′\nφ0(τ1|φ0) + L′\nφ0(τ2|φ0)\n−βL′′\nφ0(τ2|φ0)L′\nφ0(τ1|φ0) + O(δ2)\n(29)\nA.2. Analysis in the Parameter Space\nIn this section, we give more detailed analysis for clear\ncomparison with other method. As mentioned in the paper,\nReptile is an extended version of MAML. Thus we only\ngive the objective of Reptile for illustration.\nReptile targets to update on task τs one step and achieves\nthe best performance on τs. The objective can be formulated\nas:\nmin\nφ E[L(τs|φ′)]\n(30)\nφ′ = φ −βL′\nφ(τs|φ)\n(31)\nEq. 30 can be decomposed into two parts:\nmin\nφ E[L(τs|φ′)] = min\nφ E[(L(τs|φ′) −L(τs|φ)) + L(τs|φ)]\n(32)\nWe can estimate the ﬁrst term by Taylor Expansion:\nL(τs|φ′) =L(τs|φ)+\nL′\nφ(τs|φ)(φ′ −φ) + O(δ2)\n(33)\nSubstituting Eq. 33 into Eq. 32, we get:\nmin\nφ E[L′\nφ(τs|φ)(φ′ −φ) + L(τs|φ)]\n(34)\n(a) Accuracy with different adaptation steps.\n(b) Accuracy with different ﬁne-tuning steps.\nFigure 5. Accuracy comparison with adaptation and ﬁne-tuning on\nminiImageNet.\nWe take Eq. 31 into Eq. 34 then the objective of Reptile is:\nmin\nφ E[−βL′\nφ(τs|φ)L′\nφ(τs|φ) + L(τs|φ)]\n(35)\nThe ﬁrst term can be further simpliﬁed as:\n−β∥L′\nφ(τs|φ)∥2\n2\n(36)\nThe overall objective of Reptile can be formulated as:\nmin\nφ E[−β∥L′\nφ(τs|φ)∥2\n2 + L(τs|φ)]\n(37)\nWe give the objective of our Amphibian which is the\nsame as the 22-th equation of the paper:\nmin\nφ E[−βL′\nφ(τu|φ)L′\nφ(τs|φ) + L(τu|φ)]\n(38)\n\n\nTable 6. Hyper-parameters of ImageNet-derivatives: miniImageNet and tieredImageNet and CIFAR-derivatives: CIFAR-FS and FC100.\nParameter\nImageNet-derivatives\nCIFAR-derivatives\nTrain\nEpoch number\n40\n40\nInner batch size\n64\n256\nInner iteration m\n5\n5\nOuter step size α\n0.5\n0.5\nOuter iterations per epoch\n1000\n1000\nSGD learning rate β (1-20 epoch)\n10−1\n10−1\nSGD learning rate β (21-40 epoch)\n6 × 10−3\n6 × 10−3\nSGD momentum\n0.9\n0.9\nSGD weight decay\n5 × 10−4\n5 × 10−4\nDropout rate\n0.3\n0.3\nEvaluation\nAdam learning rate\n10−4\n10−4\nThe ﬁrst term is derived as:\n−βL′\nφ(τu|φ)L′\nφ(τs|φ) =\n−β∥L′\nφ(τu|φ)∥2 · ∥L′\nφ(τs|φ)∥2 · Cos(L′\nφ(τu|φ), L′\nφ(τs|φ))\n(39)\nThus the overall objective of our Amphibian is:\nmin\nφ E[−ηCos(L′\nφ(τu|φ), L′\nφ(τs|φ)) + L(τu|φ)]\n(40)\nwhere\nη = β∥L′\nφ(τu|φ)∥2 · ∥L′\nφ(τs|φ)∥2\n(41)\nFrom the derivation of Reptile and Amphibian, we can\nsee that Reptile targets to ﬁnd a point where the L2 norm of\nthe gradient is maximum. At this point, models can achieve\nfast gradient update. Notably, our Amphibian considers not\nonly the L2 norm of the gradient but also the gradient direc-\ntion. It shows that our method can achieve fast and general-\nized adaptation in the parameter space. The formulation in\nthe paper is simpliﬁed for illustration.\nB. Setup\nWe display the detailed setup in Table 6. The hyper-\nparameters of different datasets are almost same. It demon-\nstrates that our method can perform well on different\ndatasets without much hyper-parameter tuning.\nC. Comparison with Fine-tuning and Adapta-\ntion\nThe accuracy comparison displayed in the paper only\ngives the ﬁne-tuning accuracy within 10 steps. For clear\nillustration, we give the complete accuracy curve of ﬁne-\ntuning as shown in Figure 5(b). Fine-tuning is slower and\ntakes much more steps than adaptation (see Figure 5(a)).\nWe can see that CC+ﬁne-tuning just achieves the 5-shot ac-\ncuracy of 70% which is much lower than our ACC. ACC\ncan be viewed as CC+adaptation which takes 6 adaptation\nsteps to achieve the best 5-shot accuracy of 85.41%.\nIt\ndemonstrates the ability of fast and generalized adaptation\nof our method.\n"
}