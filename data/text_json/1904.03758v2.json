{
  "filename": "1904.03758v2.pdf",
  "num_pages": 9,
  "pages": [
    "Meta-Learning with Differentiable Convex Optimization\nKwonjoon Lee2\nSubhransu Maji1,3\nAvinash Ravichandran1\nStefano Soatto1,4\n1Amazon Web Services\n2UC San Diego\n3UMass Amherst\n4UCLA\nkwl042@ucsd.edu\n{smmaji,ravinash,soattos}@amazon.com\nAbstract\nMany meta-learning approaches for few-shot learning\nrely on simple base learners such as nearest-neighbor clas-\nsiï¬ers. However, even in the few-shot regime, discrimina-\ntively trained linear predictors can offer better generaliza-\ntion. We propose to use these predictors as base learners to\nlearn representations for few-shot learning and show they\noffer better tradeoffs between feature size and performance\nacross a range of few-shot recognition benchmarks. Our\nobjective is to learn feature embeddings that generalize well\nunder a linear classiï¬cation rule for novel categories. To\nefï¬ciently solve the objective, we exploit two properties of\nlinear classiï¬ers: implicit differentiation of the optimality\nconditions of the convex problem and the dual formulation\nof the optimization problem. This allows us to use high-\ndimensional embeddings with improved generalization at a\nmodest increase in computational overhead. Our approach,\nnamed MetaOptNet, achieves state-of-the-art performance\non miniImageNet, tieredImageNet, CIFAR-FS, and FC100\nfew-shot learning benchmarks. Our code is available on-\nline1.\n1. Introduction\nThe ability to learn from a few examples is a hallmark\nof human intelligence, yet it remains a challenge for mod-\nern machine learning systems. This problem has received\nsigniï¬cant attention from the machine learning community\nrecently where few-shot learning is cast as a meta-learning\nproblem (e.g., [22, 8, 33, 28]). The goal is to minimize gen-\neralization error across a distribution of tasks with few train-\ning examples. Typically, these approaches are composed of\nan embedding model that maps the input domain into a fea-\nture space and a base learner that maps the feature space\nto task variables. The meta-learning objective is to learn\nan embedding model such that the base learner generalizes\nwell across tasks.\nWhile many choices for base learners exist, nearest-\nneighbor classiï¬ers and their variants (e.g., [28, 33]) are\n1https://github.com/kjunelee/MetaOptNet\npopular as the classiï¬cation rule is simple and the approach\nscales well in the low-data regime. However, discrimina-\ntively trained linear classiï¬ers often outperform nearest-\nneighbor classiï¬ers (e.g., [4, 16]) in the low-data regime\nas they can exploit the negative examples which are often\nmore abundant to learn better class boundaries. Moreover,\nthey can effectively use high dimensional feature embed-\ndings as model capacity can be controlled by appropriate\nregularization such as weight sparsity or norm.\nHence, in this paper, we investigate linear classiï¬ers as\nthe base learner for a meta-learning based approach for few-\nshot learning. The approach is illustrated in Figure 1 where\na linear support vector machine (SVM) is used to learn a\nclassiï¬er given a set of labeled training examples and the\ngeneralization error is computed on a novel set of examples\nfrom the same task. The key challenge is computational\nsince the meta-learning objective of minimizing the gener-\nalization error across tasks requires training a linear classi-\nï¬er in the inner loop of optimization (see Section 3). How-\never, the objective of linear models is convex and can be\nsolved efï¬ciently. We observe that two additional properties\narising from the convex nature that allows efï¬cient meta-\nlearning: implicit differentiation of the optimization [2, 11]\nand the low-rank nature of the classiï¬er in the few-shot set-\nting. The ï¬rst property allows the use of off-the-shelf con-\nvex optimizers to estimate the optima and implicitly differ-\nentiate the optimality or Karush-Kuhn-Tucker (KKT) con-\nditions to train embedding model.\nThe second property\nmeans that the number of optimization variables in the dual\nformation is far smaller than the feature dimension for few-\nshot learning.\nTo this end, we have incorporated a differentiable\nquadratic programming (QP) solver [1] which allows end-\nto-end learning of the embedding model with various linear\nclassiï¬ers, e.g., multiclass support vector machines (SVMs)\n[5] or linear regression, for few-shot classiï¬cation tasks.\nMaking use of these properties, we show that our method\nis practical and offers substantial gains over nearest neigh-\nbor classiï¬ers at a modest increase in computational costs\n(see Table 3). Our method achieves state-of-the-art perfor-\nmance on 5-way 1-shot and 5-shot classiï¬cation for popu-\narXiv:1904.03758v2  [cs.CV]  23 Apr 2019\n",
    "!\"\n!\"\nâ„’ğ‘šğ‘’ğ‘¡ğ‘\nEmbeddings of \nTraining Examples\nWeights of\nLinear Classifier\nScore (logit)\nfor Each Class\nTraining Examples\nTest Examples\nSVM\nLoss\nFigure 1. Overview of our approach. Schematic illustration of our method MetaOptNet on an 1-shot 3-way classiï¬cation task. The\nmeta-training objective is to learn the parameters Ï† of a feature embedding model fÏ† that generalizes well across tasks when used with\nregularized linear classiï¬ers (e.g., SVMs). A task is a tuple of a few-shot training set and a test set (see Section 3 for details).\nlar few-shot benchmarks including miniImageNet [33, 22],\ntieredImageNet [23], CIFAR-FS [3], and FC100 [20].\n2. Related Work\nMeta-learning studies what aspects of the learner (com-\nmonly referred to as bias or prior) effect generalization\nacross a distribution of tasks [26, 31, 32]. Meta-learning ap-\nproaches for few-shot learning can be broadly categorized\nthese approaches into three groups. Gradient-based meth-\nods [22, 8] use gradient descent to adapt the embedding\nmodel parameters (e.g., all layers of a deep network) given\ntraining examples. Nearest-neighbor methods [33, 28] learn\na distance-based prediction rule over the embeddings. For\nexample, prototypical networks [28] represent each class by\nthe mean embedding of the examples, and the classiï¬cation\nrule is based on the distance to the nearest class mean. An-\nother example is matching networks [33] that learns a ker-\nnel density estimate of the class densities using the embed-\ndings over training data (the model can also be interpreted\nas a form of attention over training examples). Model-based\nmethods [18, 19] learn a parameterized predictor to estimate\nmodel parameters, e.g., a recurrent network that predicts pa-\nrameters analogous to a few steps of gradient descent in pa-\nrameter space. While gradient-based methods are general,\nthey are prone to overï¬tting as the embedding dimension\ngrows [18, 25]. Nearest-neighbor approaches offer simplic-\nity and scale well in the few-shot setting. However, nearest-\nneighbor methods have no mechanisms for feature selection\nand are not very robust to noisy features.\nOur work is related to techniques for backpropagation\nthough optimization procedures. Domke [6] presented a\ngeneric method based on unrolling gradient-descent for a\nï¬xed number of steps and automatic differentiation to com-\npute gradients. However, the trace of the optimizer (i.e.,\nthe intermediate values) needs to be stored in order to com-\npute the gradients which can be prohibitive for large prob-\nlems. The storage overhead issue was considered in more\ndetail by Maclaurin et al. [15] where they studied low pre-\ncision representations of the optimization trace of deep net-\nworks. If the argmin of the optimization can be found an-\nalytically, such as in unconstrained quadratic minimization\nproblems, then it is also possible to compute the gradients\nanalytically. This has been applied for learning in low-level\nvision problems [30, 27]. A concurrent and closely related\nwork [3] uses this idea to learn few-shot models using ridge-\nregression base learners which have closed-form solutions.\nWe refer readers to Gould et al. [11] which provides an ex-\ncellent survey of techniques for differentiating argmin and\nargmax problems.\nOur approach advocates the use of linear classiï¬ers\nwhich can be formulated as convex learning problems. In\nparticular, the objective is a quadratic program (QP) which\ncan be efï¬ciently solved to obtain its global optima using\ngradient-based techniques. Moreover, the solution to con-\nvex problems can be characterized by their Karush-Kuhn-\nTucker (KKT) conditions which allow us to backpropagate\nthrough the learner using the implicit function theorem [12].\nSpeciï¬cally, we use the formulation of Amos and Kolter [1]\nwhich provides efï¬cient GPU routines for computing solu-\ntions to QPs and their gradients. While they applied this\nframework to learn representations for constraint satisfac-\ntion problems, it is also well-suited for few-shot learning as\nthe problem sizes that arise are typically small.\nWhile our experiments focus on linear classiï¬ers with\nhinge loss and â„“2 regularization, our framework can be used\nwith other loss functions and non-linear kernels. For exam-\nple, the ridge regression learner used in [3] can be imple-\nmented within our framework allowing a direct comparison.\n",
    "3. Meta-learning with Convex Base Learners\nWe ï¬rst derive the meta-learning framework for few-shot\nlearning following prior work (e.g., [28, 22, 8]) and then\ndiscuss how convex base learners, such as linear SVMs, can\nbe incorporated.\n3.1. Problem formulation\nGiven the training set Dtrain = {(xt, yt)}T\nt=1, the goal\nof the base learner A is to estimate parameters Î¸ of the pre-\ndictor y = f(x; Î¸) so that it generalizes well to the unseen\ntest set Dtest = {(xt, yt)}Q\nt=1. It is often assumed that the\ntraining and test set are sampled from the same distribution\nand the domain is mapped to a feature space using an em-\nbedding model fÏ† parameterized by Ï†. For optimization-\nbased learners, the parameters are obtained by minimizing\nthe empirical loss over training data along with a regular-\nization that encourages simpler models. This can be written\nas:\nÎ¸ = A(Dtrain; Ï†) = arg min\nÎ¸\nLbase(Dtrain; Î¸, Ï†) + R(Î¸)\n(1)\nwhere Lbase is a loss function, such as the negative log-\nlikelihood of labels, and R(Î¸) is a regularization term. Reg-\nularization plays an important role in generalization when\ntraining data is limited.\nMeta-learning approaches for few-shot learning aim to\nminimize the generalization error across a distribution of\ntasks sampled from a task distribution.\nConcretely, this\ncan be thought of as learning over a collection of tasks:\nT\n= {(Dtrain\ni\n, Dtest\ni\n)}I\ni=1, often referred to as a meta-\ntraining set. The tuple (Dtrain\ni\n, Dtest\ni\n) describes a training\nand a test dataset, or a task. The objective is to learn an\nembedding model Ï† that minimizes generalization (or test)\nerror across tasks given a base learner A. Formally, the\nlearning objective is:\nmin\nÏ† ET\n\u0002\nLmeta(Dtest; Î¸, Ï†), where Î¸ = A(Dtrain; Ï†)\n\u0003\n.\n(2)\nFigure 1 illustrates the training and testing for a single\ntask. Once the embedding model fÏ† is learned, its general-\nization is estimated on a set of held-out tasks (often referred\nto as a meta-test set) S = {(Dtrain\nj\n, Dtest\nj\n)}J\nj=1 computed\nas:\nES\n\u0002\nLmeta(Dtest; Î¸, Ï†), where Î¸ = A(Dtrain; Ï†)\n\u0003\n.\n(3)\nFollowing prior work [22, 8], we call the stages of estimat-\ning the expectation in Equation 2 and 3 as meta-training and\nmeta-testing respectively. During meta-training, we keep an\nadditional held-out meta-validation set to choose the hyper-\nparameters of the meta-learner and pick the best embedding\nmodel.\n3.2. Episodic sampling of tasks\nStandard few-shot learning benchmarks such as miniIm-\nageNet [22] evaluate models in K-way, N-shot classiï¬ca-\ntion tasks. Here K denotes the number of classes, and N\ndenotes the number of training examples per class. Few-\nshot learning techniques are evaluated for small values of\nN, typically N âˆˆ{1, 5}. In practice, these datasets do not\nexplicitly contain tuples (Dtrain\ni\n, Dtest\ni\n), but each task for\nmeta-learning is constructed â€œon the ï¬‚yâ€ during the meta-\ntraining stage, commonly described as an episode.\nFor example, in prior work [33, 22], a task (or episode)\nTi = (Dtrain\ni\n, Dtest\ni\n) is sampled as follows. The overall\nset of categories is Ctrain. For each episode, categories Ci\ncontaining K categories from the Ctrain are ï¬rst sampled\n(with replacement); then training (support) set Dtrain\ni\n=\n{(xn, yn) | n = 1, . . . , N Ã— K, yn âˆˆCi} consisting of N\nimages per category is sampled; and ï¬nally, the test (query)\nset Dtest\ni\n= {(xn, yn) | n = 1, . . . , Q Ã— K, yn âˆˆCi}\nconsisting of Q images per category is sampled.\nWe emphasize that we need to sample without replace-\nment, i.e., Dtrain\ni\nâˆ©Dtest\ni\n= Ã˜, to optimize the gener-\nalization error.\nIn the same manner, meta-validation set\nand meta-test set are constructed on the ï¬‚y from Cval and\nCtest, respectively.\nIn order to measure the embedding\nmodelâ€™s generalization to unseen categories, Ctrain, Cval,\nand Ctest are chosen to be mutually disjoint.\n3.3. Convex base learners\nThe choice of the base learner A has a signiï¬cant im-\npact on Equation 2. The base learner that computes Î¸ =\nA(Dtrain; Ï†) has to be efï¬cient since the expectation has to\nbe computed over a distribution of tasks. Moreover, to esti-\nmate parameters Ï† of the embedding model the gradients of\nthe task test error Lmeta(Dtest; Î¸, Ï†) with respect to Ï† have\nto be efï¬ciently computed. This has motivated simple base\nlearners such as nearest class mean [28] for which the pa-\nrameters of the base learner Î¸ are easy to compute and the\nobjective is differentiable.\nWe consider base learners based on multi-class linear\nclassiï¬ers (e.g., support vector machines (SVMs) [5, 34],\nlogistic regression, and ridge regression) where the base-\nlearnerâ€™s objective is convex. For example, a K-class linear\nSVM can be written as Î¸ = {wk}K\nk=1. The Crammer and\nSinger [5] formulation of the multi-class SVM is:\nÎ¸ = A(Dtrain; Ï†) = arg min\n{wk}\nmin\n{Î¾i}\n1\n2\nX\nk\n||wk||2\n2 + C\nX\nn\nÎ¾n\nsubject to\nwyn Â· fÏ†(xn) âˆ’wk Â· fÏ†(xn) â‰¥1 âˆ’Î´yn,k âˆ’Î¾n, âˆ€n, k\n(4)\nwhere Dtrain = {(xn, yn)}, C is the regularization param-\neter and Î´Â·,Â· is the Kronecker delta function.\n",
    "Gradients of the SVM objective.\nFrom Figure 1, we see\nthat in order to make our system end-to-end trainable, we\nrequire that the solution of the SVM solver should be dif-\nferentiable with respect to its input, i.e., we should be able\nto compute {\nâˆ‚Î¸\nâˆ‚fÏ†(xn)}NÃ—K\nn=1 . The objective of SVM is con-\nvex and has a unique optimum. This allows for the use of\nimplicit function theorem (e.g., [12, 7, 2]) on the optimality\n(KKT) conditions to obtain the necessary gradients. For the\nsake of completeness, we derive the form of the theorem for\nconvex optimization problems as stated in [2]. Consider the\nfollowing convex optimization problem:\nminimize\nf0(Î¸, z)\nsubject to\nf(Î¸, z) âª¯0\nh(Î¸, z) = 0.\n(5)\nwhere the vector Î¸ âˆˆRd is the optimization variable of the\nproblem, the vector z âˆˆRe is the input parameter of the\noptimization problem, which is {fÏ†(xn)} in our case. We\ncan optimize the objective by solving for the saddle point\n(ËœÎ¸, ËœÎ», ËœÎ½) of the following Lagrangian:\nL(Î¸, Î», Î½, z) = f0(Î¸, z) + Î»T f(Î¸, z) + Î½T h(Î¸, z).\n(6)\nIn other words, we can obtain the optimum of the objective\nfunction by solving g(ËœÎ¸, ËœÎ», ËœÎ½, z) = 0 where\ng(Î¸, Î», Î½, z) =\nï£®\nï£°\nâˆ‡Î¸L(Î¸, Î», Î½, z)\ndiag(Î»)f(Î¸, z)\nh(Î¸, z)\nï£¹\nï£».\n(7)\nGiven a function f(x) : Rn â†’Rm, denote Dxf(x) as\nits Jacobian âˆˆRmÃ—n.\nTheorem 1 (From Barratt [2]) Suppose g(ËœÎ¸, ËœÎ», ËœÎ½, z) = 0.\nThen, when all derivatives exist,\nDz ËœÎ¸ = âˆ’DÎ¸g(ËœÎ¸, ËœÎ», ËœÎ½, z)âˆ’1Dzg(ËœÎ¸, ËœÎ», ËœÎ½, z).\n(8)\nThis result is obtained by applying the implicit function\ntheorem to the KKT conditions. Thus, once we compute the\noptimal solution ËœÎ¸, we can obtain a closed-form expression\nfor the gradient of ËœÎ¸ with respect to the input data. This\nobviates the need for backpropagating through the entire\noptimization trajectory since the solution does not depend\non the trajectory or initialization due to its uniqueness. This\nalso saves memory, an advantage that convex problems have\nover generic optimization problems.\nTime complexity.\nThe forward pass (i.e., computation of\nEquation 4) using our approach requires the solution to the\nQP solver whose complexity scales as O(d3) where d is\nthe number of optimization variables. This time is domi-\nnated by factorizing the KKT matrix required for primal-\ndual interior point method. Backward pass requires the so-\nlution to Equation 8 in Theorem 1, whose complexity is\nO(d2) given the factorization already computed in the for-\nward pass. Both forward pass and backward pass can be\nexpensive when the dimension of embedding fÏ† is large.\nDual formulation.\nThe dual formulation of the objective\nin Equation 4 allows us to address the poor dependence on\nthe embedding dimension and can be written as follows. Let\nwk(Î±k) =\nX\nn\nÎ±k\nnfÏ†(xn)\nâˆ€k.\n(9)\nWe can instead optimize in the dual space:\nmax\n{Î±k}\nh\nâˆ’1\n2\nX\nk\n||wk(Î±k)||2\n2 +\nX\nn\nÎ±yn\nn\ni\nsubject to\nÎ±yn\nn â‰¤C,\nÎ±k\nn â‰¤0\nâˆ€k Ì¸= yn,\nX\nk\nÎ±k\nn = 0\nâˆ€n.\n(10)\nThis results in a quadratic program (QP) over the dual\nvariables {Î±k}K\nk=1. We note that the size of the optimiza-\ntion variable is the number of training examples times the\nnumber of classes. This is often much smaller than the size\nof the feature dimension for few-shot learning. We solve\nthe dual QP of Equation 10 using [1] which implements a\ndifferentiable GPU-based QP solver. In practice (as seen\nin Table 3) the time taken by the QP solver is comparable\nto the time taken to compute features using the ResNet-12\narchitectures so the overall speed per iteration is not signif-\nicantly different from those based on simple base learners\nsuch as nearest class prototype (mean) used in Prototypical\nNetworks [28].\nConcurrent to our work, Bertinetto et al. [3] employed\nridge regression as the base learner which has a closed-form\nsolution. Although ridge regression may not be best suited\nfor classiï¬cation problems, their work showed that training\nmodels by minimizing squared error with respect to one-hot\nlabels works well in practice. The resulting optimization for\nridge regression is also a QP and can be implemented within\nour framework as:\nmax\n{Î±k}\nh\nâˆ’1\n2\nX\nk\n||wk(Î±k)||2\n2 âˆ’Î»\n2\nX\nk\n||Î±k||2\n2 +\nX\nn\nÎ±yn\nn\ni\n(11)\nwhere wk is deï¬ned as Equation 9. A comparison of lin-\near SVM and ridge regression in Section 4 shows a slight\nadvantage of the linear SVM formation.\n3.4. Meta-learning objective\nTo measure the performance of the model we evaluate\nthe negative log-likelihood of the test data sampled from\n",
    "the same task. Hence, we can re-express the meta-learning\nobjective of Equation 2 as:\nLmeta(Dtest; Î¸, Ï†, Î³) =\nX\n(x,y)âˆˆDtest\n[âˆ’Î³wy Â· fÏ†(x) + log\nX\nk\nexp(Î³wk Â· fÏ†(x))]\n(12)\nwhere Î¸ = A(Dtrain; Ï†) = {wk}K\nk=1 and Î³ is a learnable\nscale parameter. Prior work in few-shot learning [20, 3, 10]\nsuggest that adjusting the prediction score by a learnable\nscale parameter Î³ provides better performance under near-\nest class mean and ridge regression base learners.\nWe empirically ï¬nd that inserting Î³ is beneï¬cial for the\nmeta-learning with SVM base learner as well. While other\nchoices of test loss, such as hinge loss, are possible, log-\nlikelihood worked the best in our experiments.\n4. Experiments\nWe ï¬rst describe the network architecture and optimiza-\ntion details used in our experiments (Section 4.1). We then\npresent results on standard few-shot classiï¬cation bench-\nmarks including derivatives of ImageNet (Section 4.2) and\nCIFAR (Section 4.3), followed by a detailed analysis of the\nimpact of various base learners on accuracy and speed us-\ning the same embedding network and training setup (Sec-\ntion 4.4-4.6).\n4.1. Implementation details\nMeta-learning setup. We use a ResNet-12 network follow-\ning [20, 18] in our experiments. Let Rk denote a residual\nblock that consists of three {3Ã—3 convolution with k ï¬lters,\nbatch normalization, Leaky ReLU(0.1)}; let MP denote a\n2Ã—2 max pooling. We use DropBlock regularization [9],\na form of structured Dropout.\nLet DB(k, b) denote\na DropBlock layer with keep rate=k and block size=b.\nThe network architecture for ImageNet derivatives is:\nR64-MP-DB(0.9,1)-R160-MP-DB(0.9,1)-R320-\nMP-DB(0.9,5)-R640-MP-DB(0.9,5),\nwhile\nthe\nnetwork architecture used for CIFAR derivatives is:\nR64-MP-DB(0.9,1)-R160-MP-DB(0.9,1)-R320-\nMP-DB(0.9,2)-R640-MP-DB(0.9,2).\nWe do not\napply a global average pooling after the last residual block.\nAs an optimizer, we use SGD with Nesterov momen-\ntum of 0.9 and weight decay of 0.0005. Each mini-batch\nconsists of 8 episodes. The model was meta-trained for 60\nepochs, with each epoch consisting of 1000 episodes. The\nlearning rate was initially set to 0.1, and then changed to\n0.006, 0.0012, and 0.00024 at epochs 20, 40 and 50, re-\nspectively, following the practice of [10].\nDuring meta-training, we adopt horizontal ï¬‚ip, random\ncrop, and color (brightness, contrast, and saturation) jitter\ndata augmentation as in [10, 21]. For experiments on mini-\nImageNet with ResNet-12, we use label smoothing with\nÏµ = 0.1. Unlike [28] where they used higher way clas-\nsiï¬cation for meta-training than meta-testing, we use a 5-\nway classiï¬cation in both stages following recent works\n[10, 20]. Each class contains 6 test (query) samples dur-\ning meta-training and 15 test samples during meta-testing.\nOur meta-trained model was chosen based on 5-way 5-shot\ntest accuracy on the meta-validation set.\nMeta-training shot. For prototypical networks, we match\nthe meta-training shot to meta-testing shot following the\nusual practice [28, 10]. For SVM and ridge regression, we\nobserve that keeping meta-training shot higher than meta-\ntesting shot leads to better test accuracies as shown in Fig-\nure 2. Hence, during meta-training, we set training shot to\n15 for miniImageNet with ResNet-12; 5 for miniImageNet\nwith 4-layer CNN (in Table 3); 10 for tieredImageNet; 5 for\nCIFAR-FS; and 15 for FC100.\nBase-learner setup. For linear classiï¬er training, we use\nthe quadratic programming (QP) solver OptNet [1]. Regu-\nlarization parameter C of SVM was set to 0.1. Regulariza-\ntion parameter Î» of ridge regression was set to 50.0. For the\nnearest class mean (prototypical networks), we use squared\nEuclidean distance normalized with respect to the feature\ndimension.\nEarly stopping. Although we can run the optimizer un-\ntil convergence, in practice we found that running the QP\nsolver for a ï¬xed number of iterations (just three) works\nwell in practice. Early stopping acts an additional regular-\nizer and even leads to a slightly better performance.\n4.2. Experiments on ImageNet derivatives\nThe miniImageNet dataset [33] is a standard benchmark\nfor few-shot image classiï¬cation benchmark, consisting of\n100 randomly chosen classes from ILSVRC-2012 [24].\nThese classes are randomly split into 64, 16 and 20 classes\nfor meta-training, meta-validation, and meta-testing respec-\ntively. Each class contains 600 images of size 84Ã—84. Since\nthe class splits were not released in the original publica-\ntion [33], we use the commonly-used split proposed in [22].\nThe tieredImageNet benchmark [23] is a larger subset\nof ILSVRC-2012 [24], composed of 608 classes grouped\ninto 34 high-level categories. These are divided into 20 cat-\negories for meta-training, 6 categories for meta-validation,\nand 8 categories for meta-testing. This corresponds to 351,\n97 and 160 classes for meta-training, meta-validation, and\nmeta-testing respectively. This dataset aims to minimize the\nsemantic similarity between the splits. All images are of\nsize 84 Ã— 84.\nResults. Table 1 summarizes the results on the 5-way mini-\nImageNet and tieredImageNet. Our method achieves state-\nof-the-art performance on 5-way miniImageNet and tiered-\nImageNet benchmarks. Note that LEO [25] make use of\nencoder and relation network in addition to the WRN-28-10\nbackbone network to produce sample-dependent initializa-\n",
    "Table 1. Comparison to prior work on miniImageNet and tieredImageNet. Average few-shot classiï¬cation accuracies (%) with 95%\nconï¬dence intervals on miniImageNet and tieredImageNet meta-test splits. a-b-c-d denotes a 4-layer convolutional network with a, b, c,\nand d ï¬lters in each layer. âˆ—Results from [22]. â€ Used the union of meta-training set and meta-validation set to meta-train the meta-learner.\nâ€œRRâ€ stands for ridge regression.\nminiImageNet 5-way\ntieredImageNet 5-way\nmodel\nbackbone\n1-shot\n5-shot\n1-shot\n5-shot\nMeta-Learning LSTMâˆ—[22]\n64-64-64-64\n43.44 Â± 0.77\n60.60 Â± 0.71\n-\n-\nMatching Networksâˆ—[33]\n64-64-64-64\n43.56 Â± 0.84\n55.31 Â± 0.73\n-\n-\nMAML [8]\n32-32-32-32\n48.70 Â± 1.84\n63.11 Â± 0.92\n51.67 Â± 1.81\n70.30 Â± 1.75\nPrototypical Networksâˆ—â€  [28]\n64-64-64-64\n49.42 Â± 0.78\n68.20 Â± 0.66\n53.31 Â± 0.89\n72.69 Â± 0.74\nRelation Networksâˆ—[29]\n64-96-128-256\n50.44 Â± 0.82\n65.32 Â± 0.70\n54.48 Â± 0.93\n71.32 Â± 0.78\nR2D2 [3]\n96-192-384-512\n51.2 Â± 0.6\n68.8 Â± 0.1\n-\n-\nTransductive Prop Nets [14]\n64-64-64-64\n55.51 Â± 0.86\n69.86 Â± 0.65\n59.91 Â± 0.94\n73.30 Â± 0.75\nSNAIL [18]\nResNet-12\n55.71 Â± 0.99\n68.88 Â± 0.92\n-\n-\nDynamic Few-shot [10]\n64-64-128-128\n56.20 Â± 0.86\n73.00 Â± 0.64\n-\n-\nAdaResNet [19]\nResNet-12\n56.88 Â± 0.62\n71.94 Â± 0.57\n-\n-\nTADAM [20]\nResNet-12\n58.50 Â± 0.30\n76.70 Â± 0.30\n-\n-\nActivation to Parameterâ€  [21]\nWRN-28-10\n59.60 Â± 0.41\n73.74 Â± 0.19\n-\n-\nLEOâ€  [25]\nWRN-28-10\n61.76 Â± 0.08\n77.59 Â± 0.12\n66.33 Â± 0.05\n81.44 Â± 0.09\nMetaOptNet-RR (ours)\nResNet-12\n61.41 Â± 0.61\n77.88 Â± 0.46\n65.36 Â± 0.71\n81.34 Â± 0.52\nMetaOptNet-SVM (ours)\nResNet-12\n62.64 Â± 0.61\n78.63 Â± 0.46\n65.99 Â± 0.72\n81.56 Â± 0.53\nMetaOptNet-SVM-trainval (ours)â€ \nResNet-12\n64.09 Â± 0.62\n80.00 Â± 0.45\n65.81 Â± 0.74\n81.75 Â± 0.53\ntion of gradient descent. TADAM [20] employs a task em-\nbedding network (TEN) block for each convolutional layer\nâ€“ which predicts element-wise scale and shift vectors.\nWe also note that [25, 21] pretrain the WRN-28-10 fea-\nture extractor [36] to jointly classify all 64 classes in mini-\nImageNet meta-training set; then freeze the network during\nthe meta-training. [20] make use of a similar strategy of\nusing standard classiï¬cation: they co-train the feature em-\nbedding on few-shot classiï¬cation task (5-way) and stan-\ndard classiï¬cation task (64-way). In contrast, our system is\nmeta-trained end-to-end, explicitly training the feature ex-\ntractor to work well on few-shot learning tasks with regular-\nized linear classiï¬ers. This strategy allows us to clearly see\nthe effect of meta-learning. Our method is arguably simpler\nand achieves strong performance.\n4.3. Experiments on CIFAR derivatives\nThe CIFAR-FS dataset [3] is a recently proposed few-\nshot image classiï¬cation benchmark, consisting of all 100\nclasses from CIFAR-100 [13]. The classes are randomly\nsplit into 64, 16 and 20 for meta-training, meta-validation,\nand meta-testing respectively. Each class contains 600 im-\nages of size 32 Ã— 32.\nThe FC100 dataset [20] is another dataset derived from\nCIFAR-100 [13], containing 100 classes which are grouped\ninto 20 superclasses. These classes are partitioned into 60\nclasses from 12 superclasses for meta-training, 20 classes\nfrom 4 superclasses for meta-validation, and 20 classes\nfrom 4 superclasses for meta-testing. The goal is to min-\nimize semantic overlap between classes similar to the goal\n1\n5\n10\n15\nMeta-training shot\n55\n60\n65\n70\n75\n80\nAccuracy (%)\nminiImageNet 5-way\nMetaOptNetÂ­SVMÂ­1Â­shot\nMetaOptNetÂ­SVMÂ­5Â­shot\nPrototypicalÂ NetworksÂ­1Â­shot\nPrototypicalÂ NetworksÂ­5Â­shot\n1\n5\n10\n15\nMeta-training shot\n60\n65\n70\n75\n80\nAccuracy (%)\ntieredImageNet 5-way\n1\n5\n10\n15\nMeta-training shot\n70\n72\n74\n76\n78\n80\n82\n84\nAccuracy (%)\nCIFAR-FS 5-way\n1\n5\n10\n15\nMeta-training shot\n37.5\n40.0\n42.5\n45.0\n47.5\n50.0\n52.5\n55.0\nAccuracy (%)\nFC100 5-way\nFigure 2. Test accuracies (%) on meta-test sets with varying\nmeta-training shot. Shaded region denotes 95% conï¬dence in-\nterval. In general, the performance of MetaOptNet-SVM on both\n1-shot and 5-shot regimes increases with increasing meta-training\nshot.\nof tieredImageNet. Each class contains 600 images of size\n32 Ã— 32.\nResults.\nTable 2 summarizes the results on the 5-way\nclassiï¬cation tasks where our method MetaOptNet-SVM\nachieves the state-of-the-art performance. On the harder\nFC100 dataset, the gap between various base learners is\nmore signiï¬cant, which highlights the advantage of com-\nplex base learners in the few-shot learning setting.\n",
    "Table 2. Comparison to prior work on CIFAR-FS and FC100. Average few-shot classiï¬cation accuracies (%) with 95% conï¬dence\nintervals on CIFAR-FS and FC100. a-b-c-d denotes a 4-layer convolutional network with a, b, c, and d ï¬lters in each layer. âˆ—CIFAR-FS\nresults from [3]. â€ FC100 result from [20]. Â¶Used the union of meta-training set and meta-validation set to meta-train the meta-learner.\nâ€œRRâ€ stands for ridge regression.\nCIFAR-FS 5-way\nFC100 5-way\nmodel\nbackbone\n1-shot\n5-shot\n1-shot\n5-shot\nMAMLâˆ—[8]\n32-32-32-32\n58.9 Â± 1.9\n71.5 Â± 1.0\n-\n-\nPrototypical Networksâˆ—â€  [28]\n64-64-64-64\n55.5 Â± 0.7\n72.0 Â± 0.6\n35.3 Â± 0.6\n48.6 Â± 0.6\nRelation Networksâˆ—[29]\n64-96-128-256\n55.0 Â± 1.0\n69.3 Â± 0.8\n-\n-\nR2D2 [3]\n96-192-384-512\n65.3 Â± 0.2\n79.4 Â± 0.1\n-\n-\nTADAM [20]\nResNet-12\n-\n-\n40.1 Â± 0.4\n56.1 Â± 0.4\nProtoNets (our backbone) [28]\nResNet-12\n72.2 Â± 0.7\n83.5 Â± 0.5\n37.5 Â± 0.6\n52.5 Â± 0.6\nMetaOptNet-RR (ours)\nResNet-12\n72.6 Â± 0.7\n84.3 Â± 0.5\n40.5 Â± 0.6\n55.3 Â± 0.6\nMetaOptNet-SVM (ours)\nResNet-12\n72.0 Â± 0.7\n84.2 Â± 0.5\n41.1 Â± 0.6\n55.5 Â± 0.6\nMetaOptNet-SVM-trainval (ours)Â¶\nResNet-12\n72.8 Â± 0.7\n85.0 Â± 0.5\n47.2 Â± 0.6\n62.5 Â± 0.6\nTable 3. Effect of the base learner and embedding network architecture. Average few-shot classiï¬cation accuracy (%) and forward\ninference time (ms) per episode on miniImageNet and tieredImageNet with varying base learner and backbone architecture. The former\ngroup of results used the standard 4-layer convolutional network with 64 ï¬lters per layer used in [33, 28], whereas the latter used a 12-layer\nResNet without the global average pooling. â€œRRâ€ stands for ridge regression.\nminiImageNet 5-way\ntieredImageNet 5-way\n1-shot\n5-shot\n1-shot\n5-shot\nmodel\nacc. (%) time (ms) acc. (%) time (ms)\nacc. (%) time (ms) acc. (%) time (ms)\n4-layer conv (feature dimension=1600)\nPrototypical Networks [17, 28]\n53.47Â±0.63\n6Â±0.01\n70.68Â±0.49\n7Â±0.02\n54.28Â±0.67\n6Â±0.03\n71.42Â±0.61\n7Â±0.02\nMetaOptNet-RR (ours)\n53.23Â±0.59\n20Â±0.03\n69.51Â±0.48\n27Â±0.05\n54.63Â±0.67\n21Â±0.05\n72.11Â±0.59\n28Â±0.06\nMetaOptNet-SVM (ours)\n52.87Â±0.57\n28Â±0.02\n68.76Â±0.48\n37Â±0.05\n54.71Â±0.67\n28Â±0.07\n71.79Â±0.59\n38Â±0.08\nResNet-12 (feature dimension=16000)\nPrototypical Networks [17, 28]\n59.25Â±0.64\n60Â±17\n75.60Â±0.48\n66Â±17\n61.74Â±0.77\n61Â±17\n80.00Â±0.55\n66Â±18\nMetaOptNet-RR (ours)\n61.41Â±0.61\n68Â±17\n77.88Â±0.46\n75Â±17\n65.36Â±0.71\n69Â±17\n81.34Â±0.52\n77Â±17\nMetaOptNet-SVM (ours)\n62.64Â±0.61\n78Â±17\n78.63Â±0.46\n89Â±17\n65.99Â±0.72\n78Â±17\n81.56Â±0.53\n90Â±17\n4.4. Comparisons between base learners\nTable 3 shows the results where we vary the base learner\nfor two different embedding architectures. When we use\na standard 4-layer convolutional network where the feature\ndimension is low (1600), we do not observe a substantial\nbeneï¬t of adopting discriminative classiï¬ers for few-shot\nlearning. Indeed, nearest class mean classiï¬er [17] is proven\nto work well under a low-dimensional feature as shown\nin Prototypical Networks [28].\nHowever, when the em-\nbedding dimensional is much higher (16000), SVMs yield\nbetter few-shot accuracy than other base learners. Thus,\nregularized linear classiï¬ers provide robustness when high-\ndimensional features are available.\nThe added beneï¬ts come at a modest increase in com-\nputational cost. For ResNet-12, compared to nearest class\nmean classiï¬er, the additional overhead is around 13% for\nthe ridge regression base learner and around 30-50% for\nthe SVM base learner. As seen in from Figure 2, the per-\nformance of our model on both 1-shot and 5-shot regimes\ngenerally increases with increasing meta-training shot. This\nmakes the approach more practical as we can meta-train the\nembedding once with a high shot for all meta-testing shots.\nAs noted in the FC100 experiment, SVM base learner\nseems to be beneï¬cial when the semantic overlap between\ntest and train is smaller. We hypothesize that the class em-\nbeddings are more signiï¬cantly more compact for training\ndata than test data (e.g., see [35]); hence ï¬‚exibility in the\nbase learner allows robustness to noisy embeddings and im-\nproves generalization.\n",
    "1\n2\n3\nIterations\n61.0\n61.5\n62.0\n62.5\n63.0\nAccuracy (%)\nminiImageNet 5-way 1-shot\nMetaOptNetÂ­SVM\nMetaOptNetÂ­RR\n1\n2\n3\nIterations\n77.50\n77.75\n78.00\n78.25\n78.50\n78.75\n79.00\nAccuracy (%)\nminiImageNet 5-way 5-shot\nMetaOptNetÂ­SVM\nMetaOptNetÂ­RR\nFigure 3. Test accuracies (%) on miniImageNet meta-test set\nwith varying iterations of QP solver. The error bar denotes 95%\nconï¬dence interval. Ridge regression base learner (MetaOptNet-\nRR) converges in 1 iteration; SVM base learner (MetaOptNet-\nSVM) was run for 3 iterations.\n4.5. Reducing meta-overï¬tting\nAugmenting meta-training set. Despite sampling tasks, at\nthe end of meta-training MetaOptNet-SVM with ResNet-\n12 achieves nearly 100% test accuracy on all the meta-\ntraining datasets except the tieredImageNet. To alleviate\noverï¬tting, similarly to [25, 21], we use the union of the\nmeta-training and meta-validation sets to meta-train the em-\nbedding, keeping the hyperparameters, such as the number\nof epochs, identical to the previous setting. In particular,\nwe terminate the meta-training after 21 epochs for mini-\nImageNet, 52 epochs for tieredImageNet, 21 epochs for\nCIFAR-FS, and 21 epochs for FC100. Tables 1 and 2 show\nthe results with the augmented meta-training sets, denoted\nas MetaOptNet-SVM-trainval. On minImageNet, CIFAR-\nFS, and FC100 datasets, we observe improvements in test\naccuracies.\nOn tieredImageNet dataset, the difference is\nnegligible. We suspect that this is because our system has\nnot yet entered the regime of overï¬tting (In fact, we ob-\nserve âˆ¼94% test accuracy on tieredImageNet meta-training\nset). Our results suggest that meta-learning embedding with\nmore meta-training â€œclassesâ€ helps reduce overï¬tting to the\nmeta-training set.\nVarious regularization techniques. Table 4 shows the ef-\nfect of regularization methods on MetaOptNet-SVM with\nResNet-12. We note that early works on few-shot learning\n[28, 8] did not employ any of these techniques. We observe\nthat without the use of regularization, the performance of\nResNet-12 reduces to the one of the 4-layer convolutional\nnetwork with 64 ï¬lters per layer shown in Table 3. This\nshows the importance of regularization for meta-learners.\nWe expect that performances of few-shot learning systems\nwould be further improved by introducing novel regulariza-\ntion methods.\n4.6. Efï¬ciency of dual optimization\nTo see whether the dual optimization is indeed effective\nand efï¬cient, we measure accuracies on meta-test set with\nvarying iteration of the QP solver. Each iteration of QP\nsolver [1] involves computing updates for primal and dual\nvariables via LU decomposition of KKT matrix. The results\nData\nAug.\nWeight\nDecay\nDrop\nBlock\nLabel\nSmt.\nLarger\nData\n1-shot 5-shot\n51.13\n70.88\nâœ“\n55.80\n75.76\nâœ“\n56.65\n73.72\nâœ“\nâœ“\n60.33\n76.61\nâœ“\nâœ“\nâœ“\n61.11\n77.40\nâœ“\nâœ“\nâœ“\nâœ“\n62.64\n78.63\nâœ“\nâœ“\nâœ“\nâœ“\nâœ“\n64.09\n80.00\nTable 4. Ablation study.\nVarious regularization techniques\nimproves test accuracy regularization techniques improves test\naccuracy (%) on 5-way miniImageNet benchmark.\nWe use\nMetaOptNet-SVM with ResNet-12 for results. â€˜Data Aug.â€™, â€˜La-\nbel Smt.â€™, and â€˜Larger Dataâ€™ stand for data augmentation, label\nsmoothing on the meta-learning objective, and merged dataset of\nmeta-training split and meta-test split, respectively.\nare shown in Figure 3. The QP solver reaches the optima of\nridge regression objective in just one iteration. Alternatively\none can use its closed-form solution as used in [3]. Also, we\nobserve that for 1-shot tasks, the QP SVM solver reaches\noptimal accuracies in 1 iteration, although we observed that\nthe KKT conditions are not exactly satisï¬ed yet. For 5-shot\ntasks, even if we run QP SVM solver for 1 iteration, we\nachieve better accuracies than other base learners. When the\niteration of SVM solver is limited to 1 iteration, 1 episode\ntakes 69 Â± 17 ms for an 1-shot task, and 80 Â± 17 ms for a 5-\nshot task, which is on par with the computational cost of the\nridge regression solver (Table 3). These experiments show\nthat solving dual objectives for SVM and ridge regression\nis very effective under few-shot settings.\n5. Conclusion\nIn this paper, we presented a meta-learning approach\nwith convex base learners for few-shot learning. The dual\nformulation and KKT conditions can be exploited to en-\nable computational and memory efï¬cient meta-learning that\nis especially well-suited for few-shot learning problems.\nLinear classiï¬ers offer better generalization than nearest-\nneighbor classiï¬ers at a modest increase in computational\ncosts (as seen in Table 3). Our experiments suggest that\nregularized linear models allow signiï¬cantly higher embed-\nding dimensions with reduced overï¬tting. For future work,\nwe aim to explore other convex base-learners such as kernel\nSVMs. This would allow the ability to incrementally in-\ncrease model capacity as more training data becomes avail-\nable for a task.\nAcknowledgements. The authors thank Yifan Xu, Jimmy\nYan, Weijian Xu, Justin Lazarow, and Vijay Mahadevan for\nvaluable discussions. Also, we appreciate the anonymous\nreviewers for their helpful and constructive comments and\nsuggestions. Finally, we would like to thank Chuyi Sun for\nhelp with Figure 1.\n",
    "References\n[1] Brandon Amos and J. Zico Kolter. OptNet: Differentiable\noptimization as a layer in neural networks. In ICML, 2017.\n1, 2, 4, 5, 8\n[2] Shane Barratt.\nOn the Differentiability of the Solution to\nConvex Optimization Problems.\narXiv:1804.05098, 2018.\n1, 4\n[3] Luca Bertinetto, JoËœao F. Henriques, Philip H. S. Torr, and\nAndrea Vedaldi. Meta-learning with differentiable closed-\nform solvers. In ICLR, 2019. 2, 4, 5, 6, 7, 8\n[4] Rich Caruana, Nikos Karampatziakis, and Ainur Yesse-\nnalina.\nAn empirical evaluation of supervised learning in\nhigh dimensions. In ICML, 2008. 1\n[5] Koby Crammer and Yoram Singer. On the algorithmic im-\nplementation of multiclass kernel-based vector machines. J.\nMach. Learn. Res., 2:265â€“292, Mar. 2002. 1, 3\n[6] Justin Domke.\nGeneric methods for optimization-based\nmodeling. In AISTATS, 2012. 2\n[7] Asen L. Dontchev and R. Tyrrell Rockafellar. Implicit func-\ntions and solution mappings. Springer Monogr. Math., 2009.\n4\n[8] Chelsea Finn, Pieter Abbeel, and Sergey Levine.\nModel-\nagnostic meta-learning for fast adaptation of deep networks.\nIn ICML, 2017. 1, 2, 3, 6, 7, 8\n[9] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V. Le. Dropblock:\nA regularization method for convolutional networks.\nIn\nNeurIPS, 2018. 5\n[10] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot\nvisual learning without forgetting. In CVPR, 2018. 5, 6\n[11] Stephen Gould, Basura Fernando, Anoop Cherian, Peter\nAnderson, Rodrigo Santa Cruz, and Edison Guo.\nOn\ndifferentiating parameterized argmin and argmax problems\nwith application to bi-level optimization.\narXiv preprint\narXiv:1607.05447, 2016. 1, 2\n[12] Steven G. Krantz and Harold R. Parks. The implicit function\ntheorem: history, theory, and applications. Springer Science\n& Business Media, 2012. 2, 4\n[13] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-\n100 (canadian institute for advanced research). 6\n[14] Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, and Yi\nYang. Transductive propagation network for few-shot learn-\ning. In ICLR, 2019. 6\n[15] Dougal Maclaurin, David Duvenaud, and Ryan Adams.\nGradient-based hyperparameter optimization through re-\nversible learning. In ICML, 2015. 2\n[16] Tomasz Malisiewicz, Abhinav Gupta, and Alexei A. Efros.\nEnsemble of exemplar-svms for object detection and beyond.\nIn ICCV, 2011. 1\n[17] Thomas Mensink, Jakob Verbeek, Florent Perronnin, and\nGabriella Csurka. Distance-based image classiï¬cation: Gen-\neralizing to new classes at near-zero cost. IEEE Trans. Pat-\ntern Anal. Mach. Intell., 35(11):2624â€“2637, Nov. 2013. 7\n[18] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter\nAbbeel. A simple neural attentive meta-learner. In ICLR,\n2018. 2, 5, 6\n[19] Tsendsuren Munkhdalai, Xingdi Yuan, Soroush Mehri, and\nAdam Trischler. Rapid adaptation with conditionally shifted\nneurons. In ICML, 2018. 2, 6\n[20] Boris N. Oreshkin, Pau RodrÂ´Ä±guez, and Alexandre Lacoste.\nTadam: Task dependent adaptive metric for improved few-\nshot learning. In NeurIPS, 2018. 2, 5, 6, 7\n[21] Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan L. Yuille.\nFew-shot image recognition by predicting parameters from\nactivations. In CVPR, 2018. 5, 6, 8\n[22] Sachin Ravi and Hugo Larochelle. Optimization as a model\nfor few-shot learning. In ICLR, 2017. 1, 2, 3, 5, 6\n[23] Mengye Ren, Sachin Ravi, Eleni Triantaï¬llou, Jake Snell,\nKevin Swersky, Josh B. Tenenbaum, Hugo Larochelle, and\nRichard S. Zemel. Meta-learning for semi-supervised few-\nshot classiï¬cation. In ICLR, 2018. 2, 5\n[24] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and\nLi Fei-Fei. Imagenet large scale visual recognition challenge.\nInt. J. Comput. Vision, 115(3):211â€“252, Dec. 2015. 5\n[25] Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol\nVinyals, Razvan Pascanu, Simon Osindero, and Raia Had-\nsell. Meta-learning with latent embedding optimization. In\nICLR, 2019. 2, 5, 6, 8\n[26] Jurgen Schmidhuber.\nEvolutionary principles in self-\nreferential learning. on learning now to learn: The meta-\nmeta-meta...-hook. Diploma thesis, Technische Universitat\nMunchen, Germany, 14 May 1987. 2\n[27] Uwe Schmidt and Stefan Roth. Shrinkage ï¬elds for effective\nimage restoration. In CVPR, 2014. 2\n[28] Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototyp-\nical networks for few-shot learning. In NIPS, 2017. 1, 2, 3,\n4, 5, 6, 7, 8\n[29] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip\nH. S. Torr, and Timothy M. Hospedales. Learning to com-\npare: Relation network for few-shot learning.\nIn CVPR,\n2018. 6, 7\n[30] Marshall F. Tappen, Ce Liu, Edward H. Adelson, and\nWilliam T. Freeman. Learning gaussian conditional random\nï¬elds for low-level vision. In CVPR, 2007. 2\n[31] Sebastian Thrun. Lifelong Learning Algorithms, pages 181â€“\n209. Springer US, Boston, MA, 1998. 2\n[32] Ricardo Vilalta and Youssef Drissi.\nA perspective view\nand survey of meta-learning. Artiï¬cial Intelligence Review,\n18(2):77â€“95, Jun 2002. 2\n[33] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray\nKavukcuoglu, and Daan Wierstra. Matching networks for\none shot learning. In NIPS, 2016. 1, 2, 3, 5, 6, 7\n[34] Jason Weston and Chris Watkins. Support Vector Machines\nfor Multiclass Pattern Recognition. In European Symposium\nOn Artiï¬cial Neural Networks, 1999. 3\n[35] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.\nHow transferable are features in deep neural networks? In\nNIPS, 2014. 7\n[36] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-\nworks. In BMVC, 2016. 6\n"
  ],
  "full_text": "Meta-Learning with Differentiable Convex Optimization\nKwonjoon Lee2\nSubhransu Maji1,3\nAvinash Ravichandran1\nStefano Soatto1,4\n1Amazon Web Services\n2UC San Diego\n3UMass Amherst\n4UCLA\nkwl042@ucsd.edu\n{smmaji,ravinash,soattos}@amazon.com\nAbstract\nMany meta-learning approaches for few-shot learning\nrely on simple base learners such as nearest-neighbor clas-\nsiï¬ers. However, even in the few-shot regime, discrimina-\ntively trained linear predictors can offer better generaliza-\ntion. We propose to use these predictors as base learners to\nlearn representations for few-shot learning and show they\noffer better tradeoffs between feature size and performance\nacross a range of few-shot recognition benchmarks. Our\nobjective is to learn feature embeddings that generalize well\nunder a linear classiï¬cation rule for novel categories. To\nefï¬ciently solve the objective, we exploit two properties of\nlinear classiï¬ers: implicit differentiation of the optimality\nconditions of the convex problem and the dual formulation\nof the optimization problem. This allows us to use high-\ndimensional embeddings with improved generalization at a\nmodest increase in computational overhead. Our approach,\nnamed MetaOptNet, achieves state-of-the-art performance\non miniImageNet, tieredImageNet, CIFAR-FS, and FC100\nfew-shot learning benchmarks. Our code is available on-\nline1.\n1. Introduction\nThe ability to learn from a few examples is a hallmark\nof human intelligence, yet it remains a challenge for mod-\nern machine learning systems. This problem has received\nsigniï¬cant attention from the machine learning community\nrecently where few-shot learning is cast as a meta-learning\nproblem (e.g., [22, 8, 33, 28]). The goal is to minimize gen-\neralization error across a distribution of tasks with few train-\ning examples. Typically, these approaches are composed of\nan embedding model that maps the input domain into a fea-\nture space and a base learner that maps the feature space\nto task variables. The meta-learning objective is to learn\nan embedding model such that the base learner generalizes\nwell across tasks.\nWhile many choices for base learners exist, nearest-\nneighbor classiï¬ers and their variants (e.g., [28, 33]) are\n1https://github.com/kjunelee/MetaOptNet\npopular as the classiï¬cation rule is simple and the approach\nscales well in the low-data regime. However, discrimina-\ntively trained linear classiï¬ers often outperform nearest-\nneighbor classiï¬ers (e.g., [4, 16]) in the low-data regime\nas they can exploit the negative examples which are often\nmore abundant to learn better class boundaries. Moreover,\nthey can effectively use high dimensional feature embed-\ndings as model capacity can be controlled by appropriate\nregularization such as weight sparsity or norm.\nHence, in this paper, we investigate linear classiï¬ers as\nthe base learner for a meta-learning based approach for few-\nshot learning. The approach is illustrated in Figure 1 where\na linear support vector machine (SVM) is used to learn a\nclassiï¬er given a set of labeled training examples and the\ngeneralization error is computed on a novel set of examples\nfrom the same task. The key challenge is computational\nsince the meta-learning objective of minimizing the gener-\nalization error across tasks requires training a linear classi-\nï¬er in the inner loop of optimization (see Section 3). How-\never, the objective of linear models is convex and can be\nsolved efï¬ciently. We observe that two additional properties\narising from the convex nature that allows efï¬cient meta-\nlearning: implicit differentiation of the optimization [2, 11]\nand the low-rank nature of the classiï¬er in the few-shot set-\nting. The ï¬rst property allows the use of off-the-shelf con-\nvex optimizers to estimate the optima and implicitly differ-\nentiate the optimality or Karush-Kuhn-Tucker (KKT) con-\nditions to train embedding model.\nThe second property\nmeans that the number of optimization variables in the dual\nformation is far smaller than the feature dimension for few-\nshot learning.\nTo this end, we have incorporated a differentiable\nquadratic programming (QP) solver [1] which allows end-\nto-end learning of the embedding model with various linear\nclassiï¬ers, e.g., multiclass support vector machines (SVMs)\n[5] or linear regression, for few-shot classiï¬cation tasks.\nMaking use of these properties, we show that our method\nis practical and offers substantial gains over nearest neigh-\nbor classiï¬ers at a modest increase in computational costs\n(see Table 3). Our method achieves state-of-the-art perfor-\nmance on 5-way 1-shot and 5-shot classiï¬cation for popu-\narXiv:1904.03758v2  [cs.CV]  23 Apr 2019\n\n\n!\"\n!\"\nâ„’ğ‘šğ‘’ğ‘¡ğ‘\nEmbeddings of \nTraining Examples\nWeights of\nLinear Classifier\nScore (logit)\nfor Each Class\nTraining Examples\nTest Examples\nSVM\nLoss\nFigure 1. Overview of our approach. Schematic illustration of our method MetaOptNet on an 1-shot 3-way classiï¬cation task. The\nmeta-training objective is to learn the parameters Ï† of a feature embedding model fÏ† that generalizes well across tasks when used with\nregularized linear classiï¬ers (e.g., SVMs). A task is a tuple of a few-shot training set and a test set (see Section 3 for details).\nlar few-shot benchmarks including miniImageNet [33, 22],\ntieredImageNet [23], CIFAR-FS [3], and FC100 [20].\n2. Related Work\nMeta-learning studies what aspects of the learner (com-\nmonly referred to as bias or prior) effect generalization\nacross a distribution of tasks [26, 31, 32]. Meta-learning ap-\nproaches for few-shot learning can be broadly categorized\nthese approaches into three groups. Gradient-based meth-\nods [22, 8] use gradient descent to adapt the embedding\nmodel parameters (e.g., all layers of a deep network) given\ntraining examples. Nearest-neighbor methods [33, 28] learn\na distance-based prediction rule over the embeddings. For\nexample, prototypical networks [28] represent each class by\nthe mean embedding of the examples, and the classiï¬cation\nrule is based on the distance to the nearest class mean. An-\nother example is matching networks [33] that learns a ker-\nnel density estimate of the class densities using the embed-\ndings over training data (the model can also be interpreted\nas a form of attention over training examples). Model-based\nmethods [18, 19] learn a parameterized predictor to estimate\nmodel parameters, e.g., a recurrent network that predicts pa-\nrameters analogous to a few steps of gradient descent in pa-\nrameter space. While gradient-based methods are general,\nthey are prone to overï¬tting as the embedding dimension\ngrows [18, 25]. Nearest-neighbor approaches offer simplic-\nity and scale well in the few-shot setting. However, nearest-\nneighbor methods have no mechanisms for feature selection\nand are not very robust to noisy features.\nOur work is related to techniques for backpropagation\nthough optimization procedures. Domke [6] presented a\ngeneric method based on unrolling gradient-descent for a\nï¬xed number of steps and automatic differentiation to com-\npute gradients. However, the trace of the optimizer (i.e.,\nthe intermediate values) needs to be stored in order to com-\npute the gradients which can be prohibitive for large prob-\nlems. The storage overhead issue was considered in more\ndetail by Maclaurin et al. [15] where they studied low pre-\ncision representations of the optimization trace of deep net-\nworks. If the argmin of the optimization can be found an-\nalytically, such as in unconstrained quadratic minimization\nproblems, then it is also possible to compute the gradients\nanalytically. This has been applied for learning in low-level\nvision problems [30, 27]. A concurrent and closely related\nwork [3] uses this idea to learn few-shot models using ridge-\nregression base learners which have closed-form solutions.\nWe refer readers to Gould et al. [11] which provides an ex-\ncellent survey of techniques for differentiating argmin and\nargmax problems.\nOur approach advocates the use of linear classiï¬ers\nwhich can be formulated as convex learning problems. In\nparticular, the objective is a quadratic program (QP) which\ncan be efï¬ciently solved to obtain its global optima using\ngradient-based techniques. Moreover, the solution to con-\nvex problems can be characterized by their Karush-Kuhn-\nTucker (KKT) conditions which allow us to backpropagate\nthrough the learner using the implicit function theorem [12].\nSpeciï¬cally, we use the formulation of Amos and Kolter [1]\nwhich provides efï¬cient GPU routines for computing solu-\ntions to QPs and their gradients. While they applied this\nframework to learn representations for constraint satisfac-\ntion problems, it is also well-suited for few-shot learning as\nthe problem sizes that arise are typically small.\nWhile our experiments focus on linear classiï¬ers with\nhinge loss and â„“2 regularization, our framework can be used\nwith other loss functions and non-linear kernels. For exam-\nple, the ridge regression learner used in [3] can be imple-\nmented within our framework allowing a direct comparison.\n\n\n3. Meta-learning with Convex Base Learners\nWe ï¬rst derive the meta-learning framework for few-shot\nlearning following prior work (e.g., [28, 22, 8]) and then\ndiscuss how convex base learners, such as linear SVMs, can\nbe incorporated.\n3.1. Problem formulation\nGiven the training set Dtrain = {(xt, yt)}T\nt=1, the goal\nof the base learner A is to estimate parameters Î¸ of the pre-\ndictor y = f(x; Î¸) so that it generalizes well to the unseen\ntest set Dtest = {(xt, yt)}Q\nt=1. It is often assumed that the\ntraining and test set are sampled from the same distribution\nand the domain is mapped to a feature space using an em-\nbedding model fÏ† parameterized by Ï†. For optimization-\nbased learners, the parameters are obtained by minimizing\nthe empirical loss over training data along with a regular-\nization that encourages simpler models. This can be written\nas:\nÎ¸ = A(Dtrain; Ï†) = arg min\nÎ¸\nLbase(Dtrain; Î¸, Ï†) + R(Î¸)\n(1)\nwhere Lbase is a loss function, such as the negative log-\nlikelihood of labels, and R(Î¸) is a regularization term. Reg-\nularization plays an important role in generalization when\ntraining data is limited.\nMeta-learning approaches for few-shot learning aim to\nminimize the generalization error across a distribution of\ntasks sampled from a task distribution.\nConcretely, this\ncan be thought of as learning over a collection of tasks:\nT\n= {(Dtrain\ni\n, Dtest\ni\n)}I\ni=1, often referred to as a meta-\ntraining set. The tuple (Dtrain\ni\n, Dtest\ni\n) describes a training\nand a test dataset, or a task. The objective is to learn an\nembedding model Ï† that minimizes generalization (or test)\nerror across tasks given a base learner A. Formally, the\nlearning objective is:\nmin\nÏ† ET\n\u0002\nLmeta(Dtest; Î¸, Ï†), where Î¸ = A(Dtrain; Ï†)\n\u0003\n.\n(2)\nFigure 1 illustrates the training and testing for a single\ntask. Once the embedding model fÏ† is learned, its general-\nization is estimated on a set of held-out tasks (often referred\nto as a meta-test set) S = {(Dtrain\nj\n, Dtest\nj\n)}J\nj=1 computed\nas:\nES\n\u0002\nLmeta(Dtest; Î¸, Ï†), where Î¸ = A(Dtrain; Ï†)\n\u0003\n.\n(3)\nFollowing prior work [22, 8], we call the stages of estimat-\ning the expectation in Equation 2 and 3 as meta-training and\nmeta-testing respectively. During meta-training, we keep an\nadditional held-out meta-validation set to choose the hyper-\nparameters of the meta-learner and pick the best embedding\nmodel.\n3.2. Episodic sampling of tasks\nStandard few-shot learning benchmarks such as miniIm-\nageNet [22] evaluate models in K-way, N-shot classiï¬ca-\ntion tasks. Here K denotes the number of classes, and N\ndenotes the number of training examples per class. Few-\nshot learning techniques are evaluated for small values of\nN, typically N âˆˆ{1, 5}. In practice, these datasets do not\nexplicitly contain tuples (Dtrain\ni\n, Dtest\ni\n), but each task for\nmeta-learning is constructed â€œon the ï¬‚yâ€ during the meta-\ntraining stage, commonly described as an episode.\nFor example, in prior work [33, 22], a task (or episode)\nTi = (Dtrain\ni\n, Dtest\ni\n) is sampled as follows. The overall\nset of categories is Ctrain. For each episode, categories Ci\ncontaining K categories from the Ctrain are ï¬rst sampled\n(with replacement); then training (support) set Dtrain\ni\n=\n{(xn, yn) | n = 1, . . . , N Ã— K, yn âˆˆCi} consisting of N\nimages per category is sampled; and ï¬nally, the test (query)\nset Dtest\ni\n= {(xn, yn) | n = 1, . . . , Q Ã— K, yn âˆˆCi}\nconsisting of Q images per category is sampled.\nWe emphasize that we need to sample without replace-\nment, i.e., Dtrain\ni\nâˆ©Dtest\ni\n= Ã˜, to optimize the gener-\nalization error.\nIn the same manner, meta-validation set\nand meta-test set are constructed on the ï¬‚y from Cval and\nCtest, respectively.\nIn order to measure the embedding\nmodelâ€™s generalization to unseen categories, Ctrain, Cval,\nand Ctest are chosen to be mutually disjoint.\n3.3. Convex base learners\nThe choice of the base learner A has a signiï¬cant im-\npact on Equation 2. The base learner that computes Î¸ =\nA(Dtrain; Ï†) has to be efï¬cient since the expectation has to\nbe computed over a distribution of tasks. Moreover, to esti-\nmate parameters Ï† of the embedding model the gradients of\nthe task test error Lmeta(Dtest; Î¸, Ï†) with respect to Ï† have\nto be efï¬ciently computed. This has motivated simple base\nlearners such as nearest class mean [28] for which the pa-\nrameters of the base learner Î¸ are easy to compute and the\nobjective is differentiable.\nWe consider base learners based on multi-class linear\nclassiï¬ers (e.g., support vector machines (SVMs) [5, 34],\nlogistic regression, and ridge regression) where the base-\nlearnerâ€™s objective is convex. For example, a K-class linear\nSVM can be written as Î¸ = {wk}K\nk=1. The Crammer and\nSinger [5] formulation of the multi-class SVM is:\nÎ¸ = A(Dtrain; Ï†) = arg min\n{wk}\nmin\n{Î¾i}\n1\n2\nX\nk\n||wk||2\n2 + C\nX\nn\nÎ¾n\nsubject to\nwyn Â· fÏ†(xn) âˆ’wk Â· fÏ†(xn) â‰¥1 âˆ’Î´yn,k âˆ’Î¾n, âˆ€n, k\n(4)\nwhere Dtrain = {(xn, yn)}, C is the regularization param-\neter and Î´Â·,Â· is the Kronecker delta function.\n\n\nGradients of the SVM objective.\nFrom Figure 1, we see\nthat in order to make our system end-to-end trainable, we\nrequire that the solution of the SVM solver should be dif-\nferentiable with respect to its input, i.e., we should be able\nto compute {\nâˆ‚Î¸\nâˆ‚fÏ†(xn)}NÃ—K\nn=1 . The objective of SVM is con-\nvex and has a unique optimum. This allows for the use of\nimplicit function theorem (e.g., [12, 7, 2]) on the optimality\n(KKT) conditions to obtain the necessary gradients. For the\nsake of completeness, we derive the form of the theorem for\nconvex optimization problems as stated in [2]. Consider the\nfollowing convex optimization problem:\nminimize\nf0(Î¸, z)\nsubject to\nf(Î¸, z) âª¯0\nh(Î¸, z) = 0.\n(5)\nwhere the vector Î¸ âˆˆRd is the optimization variable of the\nproblem, the vector z âˆˆRe is the input parameter of the\noptimization problem, which is {fÏ†(xn)} in our case. We\ncan optimize the objective by solving for the saddle point\n(ËœÎ¸, ËœÎ», ËœÎ½) of the following Lagrangian:\nL(Î¸, Î», Î½, z) = f0(Î¸, z) + Î»T f(Î¸, z) + Î½T h(Î¸, z).\n(6)\nIn other words, we can obtain the optimum of the objective\nfunction by solving g(ËœÎ¸, ËœÎ», ËœÎ½, z) = 0 where\ng(Î¸, Î», Î½, z) =\nï£®\nï£°\nâˆ‡Î¸L(Î¸, Î», Î½, z)\ndiag(Î»)f(Î¸, z)\nh(Î¸, z)\nï£¹\nï£».\n(7)\nGiven a function f(x) : Rn â†’Rm, denote Dxf(x) as\nits Jacobian âˆˆRmÃ—n.\nTheorem 1 (From Barratt [2]) Suppose g(ËœÎ¸, ËœÎ», ËœÎ½, z) = 0.\nThen, when all derivatives exist,\nDz ËœÎ¸ = âˆ’DÎ¸g(ËœÎ¸, ËœÎ», ËœÎ½, z)âˆ’1Dzg(ËœÎ¸, ËœÎ», ËœÎ½, z).\n(8)\nThis result is obtained by applying the implicit function\ntheorem to the KKT conditions. Thus, once we compute the\noptimal solution ËœÎ¸, we can obtain a closed-form expression\nfor the gradient of ËœÎ¸ with respect to the input data. This\nobviates the need for backpropagating through the entire\noptimization trajectory since the solution does not depend\non the trajectory or initialization due to its uniqueness. This\nalso saves memory, an advantage that convex problems have\nover generic optimization problems.\nTime complexity.\nThe forward pass (i.e., computation of\nEquation 4) using our approach requires the solution to the\nQP solver whose complexity scales as O(d3) where d is\nthe number of optimization variables. This time is domi-\nnated by factorizing the KKT matrix required for primal-\ndual interior point method. Backward pass requires the so-\nlution to Equation 8 in Theorem 1, whose complexity is\nO(d2) given the factorization already computed in the for-\nward pass. Both forward pass and backward pass can be\nexpensive when the dimension of embedding fÏ† is large.\nDual formulation.\nThe dual formulation of the objective\nin Equation 4 allows us to address the poor dependence on\nthe embedding dimension and can be written as follows. Let\nwk(Î±k) =\nX\nn\nÎ±k\nnfÏ†(xn)\nâˆ€k.\n(9)\nWe can instead optimize in the dual space:\nmax\n{Î±k}\nh\nâˆ’1\n2\nX\nk\n||wk(Î±k)||2\n2 +\nX\nn\nÎ±yn\nn\ni\nsubject to\nÎ±yn\nn â‰¤C,\nÎ±k\nn â‰¤0\nâˆ€k Ì¸= yn,\nX\nk\nÎ±k\nn = 0\nâˆ€n.\n(10)\nThis results in a quadratic program (QP) over the dual\nvariables {Î±k}K\nk=1. We note that the size of the optimiza-\ntion variable is the number of training examples times the\nnumber of classes. This is often much smaller than the size\nof the feature dimension for few-shot learning. We solve\nthe dual QP of Equation 10 using [1] which implements a\ndifferentiable GPU-based QP solver. In practice (as seen\nin Table 3) the time taken by the QP solver is comparable\nto the time taken to compute features using the ResNet-12\narchitectures so the overall speed per iteration is not signif-\nicantly different from those based on simple base learners\nsuch as nearest class prototype (mean) used in Prototypical\nNetworks [28].\nConcurrent to our work, Bertinetto et al. [3] employed\nridge regression as the base learner which has a closed-form\nsolution. Although ridge regression may not be best suited\nfor classiï¬cation problems, their work showed that training\nmodels by minimizing squared error with respect to one-hot\nlabels works well in practice. The resulting optimization for\nridge regression is also a QP and can be implemented within\nour framework as:\nmax\n{Î±k}\nh\nâˆ’1\n2\nX\nk\n||wk(Î±k)||2\n2 âˆ’Î»\n2\nX\nk\n||Î±k||2\n2 +\nX\nn\nÎ±yn\nn\ni\n(11)\nwhere wk is deï¬ned as Equation 9. A comparison of lin-\near SVM and ridge regression in Section 4 shows a slight\nadvantage of the linear SVM formation.\n3.4. Meta-learning objective\nTo measure the performance of the model we evaluate\nthe negative log-likelihood of the test data sampled from\n\n\nthe same task. Hence, we can re-express the meta-learning\nobjective of Equation 2 as:\nLmeta(Dtest; Î¸, Ï†, Î³) =\nX\n(x,y)âˆˆDtest\n[âˆ’Î³wy Â· fÏ†(x) + log\nX\nk\nexp(Î³wk Â· fÏ†(x))]\n(12)\nwhere Î¸ = A(Dtrain; Ï†) = {wk}K\nk=1 and Î³ is a learnable\nscale parameter. Prior work in few-shot learning [20, 3, 10]\nsuggest that adjusting the prediction score by a learnable\nscale parameter Î³ provides better performance under near-\nest class mean and ridge regression base learners.\nWe empirically ï¬nd that inserting Î³ is beneï¬cial for the\nmeta-learning with SVM base learner as well. While other\nchoices of test loss, such as hinge loss, are possible, log-\nlikelihood worked the best in our experiments.\n4. Experiments\nWe ï¬rst describe the network architecture and optimiza-\ntion details used in our experiments (Section 4.1). We then\npresent results on standard few-shot classiï¬cation bench-\nmarks including derivatives of ImageNet (Section 4.2) and\nCIFAR (Section 4.3), followed by a detailed analysis of the\nimpact of various base learners on accuracy and speed us-\ning the same embedding network and training setup (Sec-\ntion 4.4-4.6).\n4.1. Implementation details\nMeta-learning setup. We use a ResNet-12 network follow-\ning [20, 18] in our experiments. Let Rk denote a residual\nblock that consists of three {3Ã—3 convolution with k ï¬lters,\nbatch normalization, Leaky ReLU(0.1)}; let MP denote a\n2Ã—2 max pooling. We use DropBlock regularization [9],\na form of structured Dropout.\nLet DB(k, b) denote\na DropBlock layer with keep rate=k and block size=b.\nThe network architecture for ImageNet derivatives is:\nR64-MP-DB(0.9,1)-R160-MP-DB(0.9,1)-R320-\nMP-DB(0.9,5)-R640-MP-DB(0.9,5),\nwhile\nthe\nnetwork architecture used for CIFAR derivatives is:\nR64-MP-DB(0.9,1)-R160-MP-DB(0.9,1)-R320-\nMP-DB(0.9,2)-R640-MP-DB(0.9,2).\nWe do not\napply a global average pooling after the last residual block.\nAs an optimizer, we use SGD with Nesterov momen-\ntum of 0.9 and weight decay of 0.0005. Each mini-batch\nconsists of 8 episodes. The model was meta-trained for 60\nepochs, with each epoch consisting of 1000 episodes. The\nlearning rate was initially set to 0.1, and then changed to\n0.006, 0.0012, and 0.00024 at epochs 20, 40 and 50, re-\nspectively, following the practice of [10].\nDuring meta-training, we adopt horizontal ï¬‚ip, random\ncrop, and color (brightness, contrast, and saturation) jitter\ndata augmentation as in [10, 21]. For experiments on mini-\nImageNet with ResNet-12, we use label smoothing with\nÏµ = 0.1. Unlike [28] where they used higher way clas-\nsiï¬cation for meta-training than meta-testing, we use a 5-\nway classiï¬cation in both stages following recent works\n[10, 20]. Each class contains 6 test (query) samples dur-\ning meta-training and 15 test samples during meta-testing.\nOur meta-trained model was chosen based on 5-way 5-shot\ntest accuracy on the meta-validation set.\nMeta-training shot. For prototypical networks, we match\nthe meta-training shot to meta-testing shot following the\nusual practice [28, 10]. For SVM and ridge regression, we\nobserve that keeping meta-training shot higher than meta-\ntesting shot leads to better test accuracies as shown in Fig-\nure 2. Hence, during meta-training, we set training shot to\n15 for miniImageNet with ResNet-12; 5 for miniImageNet\nwith 4-layer CNN (in Table 3); 10 for tieredImageNet; 5 for\nCIFAR-FS; and 15 for FC100.\nBase-learner setup. For linear classiï¬er training, we use\nthe quadratic programming (QP) solver OptNet [1]. Regu-\nlarization parameter C of SVM was set to 0.1. Regulariza-\ntion parameter Î» of ridge regression was set to 50.0. For the\nnearest class mean (prototypical networks), we use squared\nEuclidean distance normalized with respect to the feature\ndimension.\nEarly stopping. Although we can run the optimizer un-\ntil convergence, in practice we found that running the QP\nsolver for a ï¬xed number of iterations (just three) works\nwell in practice. Early stopping acts an additional regular-\nizer and even leads to a slightly better performance.\n4.2. Experiments on ImageNet derivatives\nThe miniImageNet dataset [33] is a standard benchmark\nfor few-shot image classiï¬cation benchmark, consisting of\n100 randomly chosen classes from ILSVRC-2012 [24].\nThese classes are randomly split into 64, 16 and 20 classes\nfor meta-training, meta-validation, and meta-testing respec-\ntively. Each class contains 600 images of size 84Ã—84. Since\nthe class splits were not released in the original publica-\ntion [33], we use the commonly-used split proposed in [22].\nThe tieredImageNet benchmark [23] is a larger subset\nof ILSVRC-2012 [24], composed of 608 classes grouped\ninto 34 high-level categories. These are divided into 20 cat-\negories for meta-training, 6 categories for meta-validation,\nand 8 categories for meta-testing. This corresponds to 351,\n97 and 160 classes for meta-training, meta-validation, and\nmeta-testing respectively. This dataset aims to minimize the\nsemantic similarity between the splits. All images are of\nsize 84 Ã— 84.\nResults. Table 1 summarizes the results on the 5-way mini-\nImageNet and tieredImageNet. Our method achieves state-\nof-the-art performance on 5-way miniImageNet and tiered-\nImageNet benchmarks. Note that LEO [25] make use of\nencoder and relation network in addition to the WRN-28-10\nbackbone network to produce sample-dependent initializa-\n\n\nTable 1. Comparison to prior work on miniImageNet and tieredImageNet. Average few-shot classiï¬cation accuracies (%) with 95%\nconï¬dence intervals on miniImageNet and tieredImageNet meta-test splits. a-b-c-d denotes a 4-layer convolutional network with a, b, c,\nand d ï¬lters in each layer. âˆ—Results from [22]. â€ Used the union of meta-training set and meta-validation set to meta-train the meta-learner.\nâ€œRRâ€ stands for ridge regression.\nminiImageNet 5-way\ntieredImageNet 5-way\nmodel\nbackbone\n1-shot\n5-shot\n1-shot\n5-shot\nMeta-Learning LSTMâˆ—[22]\n64-64-64-64\n43.44 Â± 0.77\n60.60 Â± 0.71\n-\n-\nMatching Networksâˆ—[33]\n64-64-64-64\n43.56 Â± 0.84\n55.31 Â± 0.73\n-\n-\nMAML [8]\n32-32-32-32\n48.70 Â± 1.84\n63.11 Â± 0.92\n51.67 Â± 1.81\n70.30 Â± 1.75\nPrototypical Networksâˆ—â€  [28]\n64-64-64-64\n49.42 Â± 0.78\n68.20 Â± 0.66\n53.31 Â± 0.89\n72.69 Â± 0.74\nRelation Networksâˆ—[29]\n64-96-128-256\n50.44 Â± 0.82\n65.32 Â± 0.70\n54.48 Â± 0.93\n71.32 Â± 0.78\nR2D2 [3]\n96-192-384-512\n51.2 Â± 0.6\n68.8 Â± 0.1\n-\n-\nTransductive Prop Nets [14]\n64-64-64-64\n55.51 Â± 0.86\n69.86 Â± 0.65\n59.91 Â± 0.94\n73.30 Â± 0.75\nSNAIL [18]\nResNet-12\n55.71 Â± 0.99\n68.88 Â± 0.92\n-\n-\nDynamic Few-shot [10]\n64-64-128-128\n56.20 Â± 0.86\n73.00 Â± 0.64\n-\n-\nAdaResNet [19]\nResNet-12\n56.88 Â± 0.62\n71.94 Â± 0.57\n-\n-\nTADAM [20]\nResNet-12\n58.50 Â± 0.30\n76.70 Â± 0.30\n-\n-\nActivation to Parameterâ€  [21]\nWRN-28-10\n59.60 Â± 0.41\n73.74 Â± 0.19\n-\n-\nLEOâ€  [25]\nWRN-28-10\n61.76 Â± 0.08\n77.59 Â± 0.12\n66.33 Â± 0.05\n81.44 Â± 0.09\nMetaOptNet-RR (ours)\nResNet-12\n61.41 Â± 0.61\n77.88 Â± 0.46\n65.36 Â± 0.71\n81.34 Â± 0.52\nMetaOptNet-SVM (ours)\nResNet-12\n62.64 Â± 0.61\n78.63 Â± 0.46\n65.99 Â± 0.72\n81.56 Â± 0.53\nMetaOptNet-SVM-trainval (ours)â€ \nResNet-12\n64.09 Â± 0.62\n80.00 Â± 0.45\n65.81 Â± 0.74\n81.75 Â± 0.53\ntion of gradient descent. TADAM [20] employs a task em-\nbedding network (TEN) block for each convolutional layer\nâ€“ which predicts element-wise scale and shift vectors.\nWe also note that [25, 21] pretrain the WRN-28-10 fea-\nture extractor [36] to jointly classify all 64 classes in mini-\nImageNet meta-training set; then freeze the network during\nthe meta-training. [20] make use of a similar strategy of\nusing standard classiï¬cation: they co-train the feature em-\nbedding on few-shot classiï¬cation task (5-way) and stan-\ndard classiï¬cation task (64-way). In contrast, our system is\nmeta-trained end-to-end, explicitly training the feature ex-\ntractor to work well on few-shot learning tasks with regular-\nized linear classiï¬ers. This strategy allows us to clearly see\nthe effect of meta-learning. Our method is arguably simpler\nand achieves strong performance.\n4.3. Experiments on CIFAR derivatives\nThe CIFAR-FS dataset [3] is a recently proposed few-\nshot image classiï¬cation benchmark, consisting of all 100\nclasses from CIFAR-100 [13]. The classes are randomly\nsplit into 64, 16 and 20 for meta-training, meta-validation,\nand meta-testing respectively. Each class contains 600 im-\nages of size 32 Ã— 32.\nThe FC100 dataset [20] is another dataset derived from\nCIFAR-100 [13], containing 100 classes which are grouped\ninto 20 superclasses. These classes are partitioned into 60\nclasses from 12 superclasses for meta-training, 20 classes\nfrom 4 superclasses for meta-validation, and 20 classes\nfrom 4 superclasses for meta-testing. The goal is to min-\nimize semantic overlap between classes similar to the goal\n1\n5\n10\n15\nMeta-training shot\n55\n60\n65\n70\n75\n80\nAccuracy (%)\nminiImageNet 5-way\nMetaOptNetÂ­SVMÂ­1Â­shot\nMetaOptNetÂ­SVMÂ­5Â­shot\nPrototypicalÂ NetworksÂ­1Â­shot\nPrototypicalÂ NetworksÂ­5Â­shot\n1\n5\n10\n15\nMeta-training shot\n60\n65\n70\n75\n80\nAccuracy (%)\ntieredImageNet 5-way\n1\n5\n10\n15\nMeta-training shot\n70\n72\n74\n76\n78\n80\n82\n84\nAccuracy (%)\nCIFAR-FS 5-way\n1\n5\n10\n15\nMeta-training shot\n37.5\n40.0\n42.5\n45.0\n47.5\n50.0\n52.5\n55.0\nAccuracy (%)\nFC100 5-way\nFigure 2. Test accuracies (%) on meta-test sets with varying\nmeta-training shot. Shaded region denotes 95% conï¬dence in-\nterval. In general, the performance of MetaOptNet-SVM on both\n1-shot and 5-shot regimes increases with increasing meta-training\nshot.\nof tieredImageNet. Each class contains 600 images of size\n32 Ã— 32.\nResults.\nTable 2 summarizes the results on the 5-way\nclassiï¬cation tasks where our method MetaOptNet-SVM\nachieves the state-of-the-art performance. On the harder\nFC100 dataset, the gap between various base learners is\nmore signiï¬cant, which highlights the advantage of com-\nplex base learners in the few-shot learning setting.\n\n\nTable 2. Comparison to prior work on CIFAR-FS and FC100. Average few-shot classiï¬cation accuracies (%) with 95% conï¬dence\nintervals on CIFAR-FS and FC100. a-b-c-d denotes a 4-layer convolutional network with a, b, c, and d ï¬lters in each layer. âˆ—CIFAR-FS\nresults from [3]. â€ FC100 result from [20]. Â¶Used the union of meta-training set and meta-validation set to meta-train the meta-learner.\nâ€œRRâ€ stands for ridge regression.\nCIFAR-FS 5-way\nFC100 5-way\nmodel\nbackbone\n1-shot\n5-shot\n1-shot\n5-shot\nMAMLâˆ—[8]\n32-32-32-32\n58.9 Â± 1.9\n71.5 Â± 1.0\n-\n-\nPrototypical Networksâˆ—â€  [28]\n64-64-64-64\n55.5 Â± 0.7\n72.0 Â± 0.6\n35.3 Â± 0.6\n48.6 Â± 0.6\nRelation Networksâˆ—[29]\n64-96-128-256\n55.0 Â± 1.0\n69.3 Â± 0.8\n-\n-\nR2D2 [3]\n96-192-384-512\n65.3 Â± 0.2\n79.4 Â± 0.1\n-\n-\nTADAM [20]\nResNet-12\n-\n-\n40.1 Â± 0.4\n56.1 Â± 0.4\nProtoNets (our backbone) [28]\nResNet-12\n72.2 Â± 0.7\n83.5 Â± 0.5\n37.5 Â± 0.6\n52.5 Â± 0.6\nMetaOptNet-RR (ours)\nResNet-12\n72.6 Â± 0.7\n84.3 Â± 0.5\n40.5 Â± 0.6\n55.3 Â± 0.6\nMetaOptNet-SVM (ours)\nResNet-12\n72.0 Â± 0.7\n84.2 Â± 0.5\n41.1 Â± 0.6\n55.5 Â± 0.6\nMetaOptNet-SVM-trainval (ours)Â¶\nResNet-12\n72.8 Â± 0.7\n85.0 Â± 0.5\n47.2 Â± 0.6\n62.5 Â± 0.6\nTable 3. Effect of the base learner and embedding network architecture. Average few-shot classiï¬cation accuracy (%) and forward\ninference time (ms) per episode on miniImageNet and tieredImageNet with varying base learner and backbone architecture. The former\ngroup of results used the standard 4-layer convolutional network with 64 ï¬lters per layer used in [33, 28], whereas the latter used a 12-layer\nResNet without the global average pooling. â€œRRâ€ stands for ridge regression.\nminiImageNet 5-way\ntieredImageNet 5-way\n1-shot\n5-shot\n1-shot\n5-shot\nmodel\nacc. (%) time (ms) acc. (%) time (ms)\nacc. (%) time (ms) acc. (%) time (ms)\n4-layer conv (feature dimension=1600)\nPrototypical Networks [17, 28]\n53.47Â±0.63\n6Â±0.01\n70.68Â±0.49\n7Â±0.02\n54.28Â±0.67\n6Â±0.03\n71.42Â±0.61\n7Â±0.02\nMetaOptNet-RR (ours)\n53.23Â±0.59\n20Â±0.03\n69.51Â±0.48\n27Â±0.05\n54.63Â±0.67\n21Â±0.05\n72.11Â±0.59\n28Â±0.06\nMetaOptNet-SVM (ours)\n52.87Â±0.57\n28Â±0.02\n68.76Â±0.48\n37Â±0.05\n54.71Â±0.67\n28Â±0.07\n71.79Â±0.59\n38Â±0.08\nResNet-12 (feature dimension=16000)\nPrototypical Networks [17, 28]\n59.25Â±0.64\n60Â±17\n75.60Â±0.48\n66Â±17\n61.74Â±0.77\n61Â±17\n80.00Â±0.55\n66Â±18\nMetaOptNet-RR (ours)\n61.41Â±0.61\n68Â±17\n77.88Â±0.46\n75Â±17\n65.36Â±0.71\n69Â±17\n81.34Â±0.52\n77Â±17\nMetaOptNet-SVM (ours)\n62.64Â±0.61\n78Â±17\n78.63Â±0.46\n89Â±17\n65.99Â±0.72\n78Â±17\n81.56Â±0.53\n90Â±17\n4.4. Comparisons between base learners\nTable 3 shows the results where we vary the base learner\nfor two different embedding architectures. When we use\na standard 4-layer convolutional network where the feature\ndimension is low (1600), we do not observe a substantial\nbeneï¬t of adopting discriminative classiï¬ers for few-shot\nlearning. Indeed, nearest class mean classiï¬er [17] is proven\nto work well under a low-dimensional feature as shown\nin Prototypical Networks [28].\nHowever, when the em-\nbedding dimensional is much higher (16000), SVMs yield\nbetter few-shot accuracy than other base learners. Thus,\nregularized linear classiï¬ers provide robustness when high-\ndimensional features are available.\nThe added beneï¬ts come at a modest increase in com-\nputational cost. For ResNet-12, compared to nearest class\nmean classiï¬er, the additional overhead is around 13% for\nthe ridge regression base learner and around 30-50% for\nthe SVM base learner. As seen in from Figure 2, the per-\nformance of our model on both 1-shot and 5-shot regimes\ngenerally increases with increasing meta-training shot. This\nmakes the approach more practical as we can meta-train the\nembedding once with a high shot for all meta-testing shots.\nAs noted in the FC100 experiment, SVM base learner\nseems to be beneï¬cial when the semantic overlap between\ntest and train is smaller. We hypothesize that the class em-\nbeddings are more signiï¬cantly more compact for training\ndata than test data (e.g., see [35]); hence ï¬‚exibility in the\nbase learner allows robustness to noisy embeddings and im-\nproves generalization.\n\n\n1\n2\n3\nIterations\n61.0\n61.5\n62.0\n62.5\n63.0\nAccuracy (%)\nminiImageNet 5-way 1-shot\nMetaOptNetÂ­SVM\nMetaOptNetÂ­RR\n1\n2\n3\nIterations\n77.50\n77.75\n78.00\n78.25\n78.50\n78.75\n79.00\nAccuracy (%)\nminiImageNet 5-way 5-shot\nMetaOptNetÂ­SVM\nMetaOptNetÂ­RR\nFigure 3. Test accuracies (%) on miniImageNet meta-test set\nwith varying iterations of QP solver. The error bar denotes 95%\nconï¬dence interval. Ridge regression base learner (MetaOptNet-\nRR) converges in 1 iteration; SVM base learner (MetaOptNet-\nSVM) was run for 3 iterations.\n4.5. Reducing meta-overï¬tting\nAugmenting meta-training set. Despite sampling tasks, at\nthe end of meta-training MetaOptNet-SVM with ResNet-\n12 achieves nearly 100% test accuracy on all the meta-\ntraining datasets except the tieredImageNet. To alleviate\noverï¬tting, similarly to [25, 21], we use the union of the\nmeta-training and meta-validation sets to meta-train the em-\nbedding, keeping the hyperparameters, such as the number\nof epochs, identical to the previous setting. In particular,\nwe terminate the meta-training after 21 epochs for mini-\nImageNet, 52 epochs for tieredImageNet, 21 epochs for\nCIFAR-FS, and 21 epochs for FC100. Tables 1 and 2 show\nthe results with the augmented meta-training sets, denoted\nas MetaOptNet-SVM-trainval. On minImageNet, CIFAR-\nFS, and FC100 datasets, we observe improvements in test\naccuracies.\nOn tieredImageNet dataset, the difference is\nnegligible. We suspect that this is because our system has\nnot yet entered the regime of overï¬tting (In fact, we ob-\nserve âˆ¼94% test accuracy on tieredImageNet meta-training\nset). Our results suggest that meta-learning embedding with\nmore meta-training â€œclassesâ€ helps reduce overï¬tting to the\nmeta-training set.\nVarious regularization techniques. Table 4 shows the ef-\nfect of regularization methods on MetaOptNet-SVM with\nResNet-12. We note that early works on few-shot learning\n[28, 8] did not employ any of these techniques. We observe\nthat without the use of regularization, the performance of\nResNet-12 reduces to the one of the 4-layer convolutional\nnetwork with 64 ï¬lters per layer shown in Table 3. This\nshows the importance of regularization for meta-learners.\nWe expect that performances of few-shot learning systems\nwould be further improved by introducing novel regulariza-\ntion methods.\n4.6. Efï¬ciency of dual optimization\nTo see whether the dual optimization is indeed effective\nand efï¬cient, we measure accuracies on meta-test set with\nvarying iteration of the QP solver. Each iteration of QP\nsolver [1] involves computing updates for primal and dual\nvariables via LU decomposition of KKT matrix. The results\nData\nAug.\nWeight\nDecay\nDrop\nBlock\nLabel\nSmt.\nLarger\nData\n1-shot 5-shot\n51.13\n70.88\nâœ“\n55.80\n75.76\nâœ“\n56.65\n73.72\nâœ“\nâœ“\n60.33\n76.61\nâœ“\nâœ“\nâœ“\n61.11\n77.40\nâœ“\nâœ“\nâœ“\nâœ“\n62.64\n78.63\nâœ“\nâœ“\nâœ“\nâœ“\nâœ“\n64.09\n80.00\nTable 4. Ablation study.\nVarious regularization techniques\nimproves test accuracy regularization techniques improves test\naccuracy (%) on 5-way miniImageNet benchmark.\nWe use\nMetaOptNet-SVM with ResNet-12 for results. â€˜Data Aug.â€™, â€˜La-\nbel Smt.â€™, and â€˜Larger Dataâ€™ stand for data augmentation, label\nsmoothing on the meta-learning objective, and merged dataset of\nmeta-training split and meta-test split, respectively.\nare shown in Figure 3. The QP solver reaches the optima of\nridge regression objective in just one iteration. Alternatively\none can use its closed-form solution as used in [3]. Also, we\nobserve that for 1-shot tasks, the QP SVM solver reaches\noptimal accuracies in 1 iteration, although we observed that\nthe KKT conditions are not exactly satisï¬ed yet. For 5-shot\ntasks, even if we run QP SVM solver for 1 iteration, we\nachieve better accuracies than other base learners. When the\niteration of SVM solver is limited to 1 iteration, 1 episode\ntakes 69 Â± 17 ms for an 1-shot task, and 80 Â± 17 ms for a 5-\nshot task, which is on par with the computational cost of the\nridge regression solver (Table 3). These experiments show\nthat solving dual objectives for SVM and ridge regression\nis very effective under few-shot settings.\n5. Conclusion\nIn this paper, we presented a meta-learning approach\nwith convex base learners for few-shot learning. The dual\nformulation and KKT conditions can be exploited to en-\nable computational and memory efï¬cient meta-learning that\nis especially well-suited for few-shot learning problems.\nLinear classiï¬ers offer better generalization than nearest-\nneighbor classiï¬ers at a modest increase in computational\ncosts (as seen in Table 3). Our experiments suggest that\nregularized linear models allow signiï¬cantly higher embed-\nding dimensions with reduced overï¬tting. For future work,\nwe aim to explore other convex base-learners such as kernel\nSVMs. This would allow the ability to incrementally in-\ncrease model capacity as more training data becomes avail-\nable for a task.\nAcknowledgements. The authors thank Yifan Xu, Jimmy\nYan, Weijian Xu, Justin Lazarow, and Vijay Mahadevan for\nvaluable discussions. Also, we appreciate the anonymous\nreviewers for their helpful and constructive comments and\nsuggestions. Finally, we would like to thank Chuyi Sun for\nhelp with Figure 1.\n\n\nReferences\n[1] Brandon Amos and J. Zico Kolter. OptNet: Differentiable\noptimization as a layer in neural networks. In ICML, 2017.\n1, 2, 4, 5, 8\n[2] Shane Barratt.\nOn the Differentiability of the Solution to\nConvex Optimization Problems.\narXiv:1804.05098, 2018.\n1, 4\n[3] Luca Bertinetto, JoËœao F. Henriques, Philip H. S. Torr, and\nAndrea Vedaldi. Meta-learning with differentiable closed-\nform solvers. In ICLR, 2019. 2, 4, 5, 6, 7, 8\n[4] Rich Caruana, Nikos Karampatziakis, and Ainur Yesse-\nnalina.\nAn empirical evaluation of supervised learning in\nhigh dimensions. In ICML, 2008. 1\n[5] Koby Crammer and Yoram Singer. On the algorithmic im-\nplementation of multiclass kernel-based vector machines. J.\nMach. Learn. Res., 2:265â€“292, Mar. 2002. 1, 3\n[6] Justin Domke.\nGeneric methods for optimization-based\nmodeling. In AISTATS, 2012. 2\n[7] Asen L. Dontchev and R. Tyrrell Rockafellar. Implicit func-\ntions and solution mappings. Springer Monogr. Math., 2009.\n4\n[8] Chelsea Finn, Pieter Abbeel, and Sergey Levine.\nModel-\nagnostic meta-learning for fast adaptation of deep networks.\nIn ICML, 2017. 1, 2, 3, 6, 7, 8\n[9] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V. Le. Dropblock:\nA regularization method for convolutional networks.\nIn\nNeurIPS, 2018. 5\n[10] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot\nvisual learning without forgetting. In CVPR, 2018. 5, 6\n[11] Stephen Gould, Basura Fernando, Anoop Cherian, Peter\nAnderson, Rodrigo Santa Cruz, and Edison Guo.\nOn\ndifferentiating parameterized argmin and argmax problems\nwith application to bi-level optimization.\narXiv preprint\narXiv:1607.05447, 2016. 1, 2\n[12] Steven G. Krantz and Harold R. Parks. The implicit function\ntheorem: history, theory, and applications. Springer Science\n& Business Media, 2012. 2, 4\n[13] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-\n100 (canadian institute for advanced research). 6\n[14] Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, and Yi\nYang. Transductive propagation network for few-shot learn-\ning. In ICLR, 2019. 6\n[15] Dougal Maclaurin, David Duvenaud, and Ryan Adams.\nGradient-based hyperparameter optimization through re-\nversible learning. In ICML, 2015. 2\n[16] Tomasz Malisiewicz, Abhinav Gupta, and Alexei A. Efros.\nEnsemble of exemplar-svms for object detection and beyond.\nIn ICCV, 2011. 1\n[17] Thomas Mensink, Jakob Verbeek, Florent Perronnin, and\nGabriella Csurka. Distance-based image classiï¬cation: Gen-\neralizing to new classes at near-zero cost. IEEE Trans. Pat-\ntern Anal. Mach. Intell., 35(11):2624â€“2637, Nov. 2013. 7\n[18] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter\nAbbeel. A simple neural attentive meta-learner. In ICLR,\n2018. 2, 5, 6\n[19] Tsendsuren Munkhdalai, Xingdi Yuan, Soroush Mehri, and\nAdam Trischler. Rapid adaptation with conditionally shifted\nneurons. In ICML, 2018. 2, 6\n[20] Boris N. Oreshkin, Pau RodrÂ´Ä±guez, and Alexandre Lacoste.\nTadam: Task dependent adaptive metric for improved few-\nshot learning. In NeurIPS, 2018. 2, 5, 6, 7\n[21] Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan L. Yuille.\nFew-shot image recognition by predicting parameters from\nactivations. In CVPR, 2018. 5, 6, 8\n[22] Sachin Ravi and Hugo Larochelle. Optimization as a model\nfor few-shot learning. In ICLR, 2017. 1, 2, 3, 5, 6\n[23] Mengye Ren, Sachin Ravi, Eleni Triantaï¬llou, Jake Snell,\nKevin Swersky, Josh B. Tenenbaum, Hugo Larochelle, and\nRichard S. Zemel. Meta-learning for semi-supervised few-\nshot classiï¬cation. In ICLR, 2018. 2, 5\n[24] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and\nLi Fei-Fei. Imagenet large scale visual recognition challenge.\nInt. J. Comput. Vision, 115(3):211â€“252, Dec. 2015. 5\n[25] Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol\nVinyals, Razvan Pascanu, Simon Osindero, and Raia Had-\nsell. Meta-learning with latent embedding optimization. In\nICLR, 2019. 2, 5, 6, 8\n[26] Jurgen Schmidhuber.\nEvolutionary principles in self-\nreferential learning. on learning now to learn: The meta-\nmeta-meta...-hook. Diploma thesis, Technische Universitat\nMunchen, Germany, 14 May 1987. 2\n[27] Uwe Schmidt and Stefan Roth. Shrinkage ï¬elds for effective\nimage restoration. In CVPR, 2014. 2\n[28] Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototyp-\nical networks for few-shot learning. In NIPS, 2017. 1, 2, 3,\n4, 5, 6, 7, 8\n[29] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip\nH. S. Torr, and Timothy M. Hospedales. Learning to com-\npare: Relation network for few-shot learning.\nIn CVPR,\n2018. 6, 7\n[30] Marshall F. Tappen, Ce Liu, Edward H. Adelson, and\nWilliam T. Freeman. Learning gaussian conditional random\nï¬elds for low-level vision. In CVPR, 2007. 2\n[31] Sebastian Thrun. Lifelong Learning Algorithms, pages 181â€“\n209. Springer US, Boston, MA, 1998. 2\n[32] Ricardo Vilalta and Youssef Drissi.\nA perspective view\nand survey of meta-learning. Artiï¬cial Intelligence Review,\n18(2):77â€“95, Jun 2002. 2\n[33] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray\nKavukcuoglu, and Daan Wierstra. Matching networks for\none shot learning. In NIPS, 2016. 1, 2, 3, 5, 6, 7\n[34] Jason Weston and Chris Watkins. Support Vector Machines\nfor Multiclass Pattern Recognition. In European Symposium\nOn Artiï¬cial Neural Networks, 1999. 3\n[35] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.\nHow transferable are features in deep neural networks? In\nNIPS, 2014. 7\n[36] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-\nworks. In BMVC, 2016. 6\n"
}