{
  "filename": "1711.06025v2.pdf",
  "num_pages": 10,
  "pages": [
    "Learning to Compare: Relation Network for Few-Shot Learning\nFlood Sung\nYongxin Yang3\nLi Zhang2\nTao Xiang1\nPhilip H.S. Torr2\nTimothy M. Hospedales3\n1Queen Mary University of London\n2University of Oxford\n3The University of Edinburgh\nfloodsung@gmail.com\nt.xiang@qmul.ac.uk\n{lz, phst}@robots.ox.ac.uk\n{yongxin.yang, t.hospedales}@ed.ac.uk\nAbstract\nWe present a conceptually simple, ﬂexible, and general\nframework for few-shot learning, where a classiﬁer must\nlearn to recognise new classes given only few examples from\neach. Our method, called the Relation Network (RN), is\ntrained end-to-end from scratch. During meta-learning, it\nlearns to learn a deep distance metric to compare a small\nnumber of images within episodes, each of which is de-\nsigned to simulate the few-shot setting. Once trained, a RN\nis able to classify images of new classes by computing rela-\ntion scores between query images and the few examples of\neach new class without further updating the network. Be-\nsides providing improved performance on few-shot learn-\ning, our framework is easily extended to zero-shot learning.\nExtensive experiments on ﬁve benchmarks demonstrate that\nour simple approach provides a uniﬁed and effective ap-\nproach for both of these two tasks.\n1. Introduction\nDeep learning models have achieved great success in vi-\nsual recognition tasks [22, 15, 35]. However, these super-\nvised learning models need large amounts of labelled data\nand many iterations to train their large number of parame-\nters. This severely limits their scalability to new classes due\nto annotation cost, but more fundamentally limits their ap-\nplicability to newly emerging (eg. new consumer devices)\nor rare (eg. rare animals) categories where numerous anno-\ntated images may simply never exist. In contrast, humans\nare very good at recognising objects with very little direct\nsupervision, or none at all i.e., few-shot [23, 9] or zero-shot\n[24] learning. For example, children have no problem gen-\neralising the concept of “zebra” from a single picture in\na book, or hearing its description as looking like a stripy\nhorse. Motivated by the failure of conventional deep learn-\ning methods to work well on one or few examples per class,\nand inspired by the few- and zero-shot learning ability of\nhumans, there has been a recent resurgence of interest in\nmachine one/few-shot [8, 39, 32, 18, 20, 10, 27, 36, 29] and\nzero-shot [11, 3, 24, 45, 25, 31] learning.\nFew-shot learning aims to recognise novel visual cate-\ngories from very few labelled examples. The availability\nof only one or very few examples challenges the standard\n‘ﬁne-tuning’ practice in deep learning [10]. Data augmen-\ntation and regularisation techniques can alleviate overﬁt-\nting in such a limited-data regime, but they do not solve\nit. Therefore contemporary approaches to few-shot learning\noften decompose training into an auxiliary meta learning\nphase where transferrable knowledge is learned in the form\nof good initial conditions [10], embeddings [36, 39] or opti-\nmisation strategies [29]. The target few-shot learning prob-\nlem is then learned by ﬁne-tuning [10] with the learned op-\ntimisation strategy [29] or computed in a feed-forward pass\n[36, 39, 4, 32] without updating network weights. Zero-shot\nlearning also suffers from a related challenge. Recognisers\nare trained by a single example in the form of a class de-\nscription (c.f., single exemplar image in one-shot), making\ndata insufﬁciency for gradient-based learning a challenge.\nWhile promising, most existing few-shot learning ap-\nproaches either require complex inference mechanisms [23,\n9], complex recurrent neural network (RNN) architectures\n[39, 32], or ﬁne-tuning the target problem [10, 29]. Our\napproach is most related to others that aim to train an effec-\ntive metric for one-shot learning [39, 36, 20]. Where they\nfocus on the learning of the transferrable embedding and\npre-deﬁne a ﬁxed metric (e.g., as Euclidean [36]), we fur-\nther aim to learn a transferrable deep metric for comparing\nthe relation between images (few-shot learning), or between\nimages and class descriptions (zero-shot learning). By ex-\npressing the inductive bias of a deeper solution (multiple\nnon-linear learned stages at both embedding and relation\nmodules), we make it easier to learn a generalisable solu-\ntion to the problem.\nSpeciﬁcally, we propose a two-branch Relation Network\n(RN) that performs few-shot recognition by learning to\ncompare query images against few-shot labeled sample im-\nages. First an embedding module generates representations\nof the query and training images. Then these embeddings\nare compared by a relation module that determines if they\narXiv:1711.06025v2  [cs.CV]  27 Mar 2018\n",
    "are from matching categories or not. Deﬁning an episode-\nbased strategy inspired by [39, 36], the embedding and re-\nlation modules are meta-learned end-to-end to support few-\nshot learning. This can be seen as extending the strategy\nof [39, 36] to include a learnable non-linear comparator,\ninstead of a ﬁxed linear comparator.\nOur approach out-\nperforms prior approaches, while being simpler (no RNNs\n[39, 32, 29]) and faster (no ﬁne-tuning [29, 10]). Our pro-\nposed strategy also directly generalises to zero-shot learn-\ning. In this case the sample branch embeds a single-shot\ncategory description rather than a single exemplar training\nimage, and the relation module learns to compare query im-\nage and category description embeddings.\nOverall our contribution is to provide a clean framework\nthat elegantly encompasses both few and zero-shot learn-\ning. Our evaluation on four benchmarks show that it pro-\nvides compelling performance across the board while being\nsimpler and faster than the alternatives.\n2. Related Work\nThe study of one or few-shot object recognition has been\nof interest for some time [9].\nEarlier work on few-shot\nlearning tended to involve generative models with complex\niterative inference strategies [9, 23]. With the success of\ndiscriminative deep learning-based approaches in the data-\nrich many-shot setting [22, 15, 35], there has been a surge\nof interest in generalising such deep learning approaches to\nthe few-shot learning setting. Many of these approaches use\na meta-learning or learning-to-learn strategy in the sense\nthat they extract some transferrable knowledge from a set\nof auxiliary tasks (meta-learning, learning-to-learn), which\nthen helps them to learn the target few-shot problem well\nwithout suffering from the overﬁtting that might be ex-\npected when applying deep models to sparse data problems.\nLearning to Fine-Tune\nThe successful MAML approach\n[10] aimed to meta-learn an initial condition (set of neural\nnetwork weights) that is good for ﬁne-tuning on few-shot\nproblems.\nThe strategy here is to search for the weight\nconﬁguration of a given neural network such that it can\nbe effectively ﬁne-tuned on a sparse data problem within\na few gradient-descent update steps. Many distinct target\nproblems are sampled from a multiple task training set; the\nbase neural network model is then ﬁne-tuned to solve each\nof them, and the success at each target problem after ﬁne-\ntuning drives updates in the base model – thus driving the\nproduction of an easy to ﬁne-tune initial condition. The\nfew-shot optimisation approach [29] goes further in meta-\nlearning not only a good initial condition but an LSTM-\nbased optimizer that is trained to be speciﬁcally effective for\nﬁne-tuning. However both of these approaches suffer from\nthe need to ﬁne-tune on the target problem. In contrast, our\napproach solves target problems in an entirely feed-forward\nmanner with no model updates required, making it more\nconvenient for low-latency or low-power applications.\nRNN Memory Based\nAnother category of approaches\nleverage recurrent neural networks with memories [27, 32].\nHere the idea is typically that an RNN iterates over an ex-\namples of given problem and accumulates the knowledge\nrequired to solve that problem in its hidden activations, or\nexternal memory. New examples can be classiﬁed, for ex-\nample by comparing them to historic information stored in\nthe memory. So ‘learning’ a single target problem can oc-\ncur in unrolling the RNN, while learning-to-learn means\ntraining the weights of the RNN by learning many distinct\nproblems. While appealing, these architectures face issues\nin ensuring that they reliably store all the, potentially long\nterm, historical information of relevance without forgetting.\nIn our approach we avoid the complexity of recurrent net-\nworks, and the issues involved in ensuring the adequacy of\ntheir memory. Instead our learning-to-learn approach is de-\nﬁned entirely with simple and fast feed forward CNNs.\nEmbedding and Metric Learning Approaches\nThe\nprior approaches entail some complexity when learning the\ntarget few-shot problem.\nAnother category of approach\naims to learn a set of projection functions that take query\nand sample images from the target problem and classify\nthem in a feed forward manner [39, 36, 4]. One approach\nis to parameterise the weights of a feed-forward classiﬁer\nin terms of the sample set [4]. The meta-learning here is\nto train the auxiliary parameterisation net that learns how\nto paramaterise a given feed-forward classiﬁcation problem\nin terms of a few-shot sample set. Metric-learning based\napproaches aim to learn a set of projection functions such\nthat when represented in this embedding, images are easy\nto recognise using simple nearest neighbour or linear classi-\nﬁers [39, 36, 20]. In this case the meta-learned transferrable\nknowledge are the projection functions and the target prob-\nlem is a simple feed-forward computation.\nThe most related methodologies to ours are the proto-\ntypical networks of [36] and the siamese networks of [20].\nThese approaches focus on learning embeddings that trans-\nform the data such that it can be recognised with a ﬁxed\nnearest-neighbour [36] or linear [20, 36] classiﬁer. In con-\ntrast, our framework further deﬁnes a relation classiﬁer\nCNN, in the style of [33, 44, 14] (While [33] focuses on\nreasoning about relation between two objects in a same im-\nage which is to address a different problem.). Compared\nto [20, 36], this can be seen as providing a learnable rather\nthan ﬁxed metric, or non-linear rather than linear classiﬁer.\nCompared to [20] we beneﬁt from an episodic training strat-\negy with an end-to-end manner from scratch, and compared\nto [32] we avoid the complexity of set-to-set RNN embed-\nding of the sample-set, and simply rely on pooling [33].\nZero-Shot Learning\nOur approach is designed for few-\n",
    "shot learning, but elegantly spans the space into zero-shot\nlearning (ZSL) by modifying the sample branch to input a\nsingle category description rather than single training im-\nage. When applied to ZSL our architecture is related to\nmethods that learn to align images and category embed-\ndings and perform recognition by predicting if an image\nand category embedding pair match [11, 3, 43, 46]. Sim-\nilarly to the case with the prior metric-based few-shot ap-\nproaches, most of these apply a ﬁxed manually deﬁned sim-\nilarity metric or linear classiﬁer after combining the image\nand category embedding. In contrast, we again beneﬁt from\na deeper end-to-end architecture including a learned non-\nlinear metric in the form of our learned convolutional re-\nlation network; as well as from an episode-based training\nstrategy.\n3. Methodology\n3.1. Problem Deﬁnition\nWe consider the task of few-shot classiﬁer learning. For-\nmally, we have three datasets: a training set, a support set,\nand a testing set. The support set and testing set share the\nsame label space, but the training set has its own label space\nthat is disjoint with support/testing set. If the support set\ncontains K labelled examples for each of C unique classes,\nthe target few-shot problem is called C-way K-shot.\nWith the support set only, we can in principle train a clas-\nsiﬁer to assign a class label ˆy to each sample ˆx in the test\nset. However, due to the lack of labelled samples in the sup-\nport set, the performance of such a classiﬁer is usually not\nsatisfactory. Therefore we aim to perform meta-learning on\nthe training set, in order to extract transferrable knowledge\nthat will allow us to perform better few-shot learning on the\nsupport set and thus classify the test set more successfully.\nAn effective way to exploit the training set is to mimic\nthe few-shot learning setting via episode based training, as\nproposed in [39]. In each training iteration, an episode is\nformed by randomly selecting C classes from the training\nset with K labelled samples from each of the C classes to\nact as the sample set S = {(xi, yi)}m\ni=1 (m = K × C), as\nwell as a fraction of the remainder of those C classes’ sam-\nples to serve as the query set Q = {(xj, yj)}n\nj=1. This sam-\nple/query set split is designed to simulate the support/test set\nthat will be encountered at test time. A model trained from\nsample/query set can be further ﬁne-tuned using the support\nset, if desired. In this work we adopt such an episode-based\ntraining strategy. In our few-shot experiments (see Section\n4.1) we consider one-shot (K = 1, Figure 1) and ﬁve-shot\n(K = 5) settings. We also address the K = 0 zero-shot\nlearning case as explained in Section 3.3.\n3.2. Model\nOne-Shot\nOur Relation Network (RN) consists of two\nmodules: an embedding module fϕ and a relation module\ngφ, as illustrated in Figure 1. Samples xj in the query set Q,\nand samples xi in the sample set S are fed through the em-\nbedding module fϕ, which produces feature maps fϕ(xi)\nand fϕ(xj). The feature maps fϕ(xi) and fϕ(xj) are com-\nbined with operator C(fϕ(xi), fϕ(xj)). In this work we as-\nsume C(·, ·) to be concatenation of feature maps in depth,\nalthough other choices are possible.\nThe combined feature map of the sample and query are\nfed into the relation module gφ, which eventually produces\na scalar in range of 0 to 1 representing the similarity be-\ntween xi and xj, which is called relation score. Thus, in\nthe C-way one-shot setting, we generate C relation scores\nri,j for the relation between one query input xj and training\nsample set examples xi,\nri,j = gφ(C(fϕ(xi), fϕ(xj))),\ni = 1, 2, . . . , C\n(1)\nK-shot\nFor K-shot where K > 1, we element-wise sum\nover the embedding module outputs of all samples from\neach training class to form this class’ feature map. This\npooled class-level feature map is combined with the query\nimage feature map as above. Thus, the number of relation\nscores for one query is always C in both one-shot or few-\nshot setting.\nObjective function\nWe use mean square error (MSE)\nloss (Eq. (2)) to train our model, regressing the relation\nscore ri,j to the ground truth: matched pairs have similarity\n1 and the mismatched pair have similarity 0.\nϕ, φ ←argmin\nϕ,φ\nm\nX\ni=1\nn\nX\nj=1\n(ri,j −1(yi == yj))2\n(2)\nThe choice of MSE is somewhat non-standard.\nOur\nproblem may seem to be a classiﬁcation problem with a la-\nbel space {0, 1}. However conceptually we are predicting\nrelation scores, which can be considered a regression prob-\nlem despite that for ground-truth we can only automatically\ngenerate {0, 1} targets.\n3.3. Zero-shot Learning\nZero-shot learning is analogous to one-shot learning in\nthat one datum is given to deﬁne each class to recognise.\nHowever instead of being given a support set with one-shot\nimage for each of C training classes, it contains a semantic\nclass embedding vector vc for each. Modifying our frame-\nwork to deal with the zero-shot case is straightforward: as\na different modality of semantic vectors is used for the sup-\nport set (e.g. attribute vectors instead of images), we use a\n",
    "𝑓\"\n𝑔$\nOne-hot \nvector\nRelation \nscore\nFeature maps concatenation\nrelation module\nembedding module\nFigure 1: Relation Network architecture for a 5-way 1-shot problem with one query example.\nsecond heterogeneous embedding module fϕ2 besides the\nembedding module fϕ1 used for the image query set. Then\nthe relation net gφ is applied as before. Therefore, the rela-\ntion score for each query input xj will be:\nri,j = gφ(C(fϕ1(vc), fϕ2(xj))),\ni = 1, 2, . . . , C\n(3)\nThe objective function for zero-shot learning is the same\nas that for few-shot learning.\n3.4. Network Architecture\nAs most few-shot learning models utilise four convolu-\ntional blocks for embedding module [39, 36], we follow the\nsame architecture setting for fair comparison, see Figure 2.\nMore concretely, each convolutional block contains a 64-\nﬁlter 3 × 3 convolution, a batch normalisation and a ReLU\nnonlinearity layer respectively. The ﬁrst two blocks also\ncontain a 2 × 2 max-pooling layer while the latter two do\nnot. We do so because we need the output feature maps\nfor further convolutional layers in the relation module. The\nrelation module consists of two convolutional blocks and\ntwo fully-connected layers. Each of convolutional block\nis a 3 × 3 convolution with 64 ﬁlters followed by batch\nnormalisation, ReLU non-linearity and 2 × 2 max-pooling.\nThe output size of last max pooling layer is H = 64 and\nH = 64 ∗3 ∗3 = 576 for Omniglot and miniImageNet\nrespectively. The two fully-connected layers are 8 and 1\ndimensional, respectively. All fully-connected layers are\nReLU except the output layer is Sigmoid in order to gen-\nerate relation scores in a reasonable range for all versions\nof our network architecture.\nThe zero-shot learning architecture is shown in Figure 3.\nIn this architecture, the DNN subnet is an existing network\n(e.g., Inception or ResNet) pretrained on ImageNet.\n4. Experiments\nWe evaluate our approach on two related tasks: few-shot\nclassiﬁcation on Omniglot and miniImagenet, and zero-\nshot classiﬁcation on Animals with Attributes (AwA) and\nCaltech-UCSD Birds-200-2011 (CUB). All the experiments\nare implemented based on PyTorch [1].\n4.1. Few-shot Recognition\nSettings\nFew-shot learning in all experiments uses\nAdam [19] with initial learning rate 10−3 , annealed by half\nfor every 100,000 episodes. All our models are end-to-end\ntrained from scratch with no additional dataset.\nBaselines\nWe compare against various state of the art\nbaselines for few-shot recognition, including neural statisti-\ncian [8], Matching Nets with and without ﬁne-tuning [39],\nMANN [32], Siamese Nets with Memory [18], Convolu-\ntional Siamese Nets [20], MAML [10], Meta Nets [27], Pro-\ntotypical Nets [36] and Meta-Learner LSTM [29].\n4.1.1\nOmniglot\nDataset\nOmniglot [23] contains 1623 characters (classes)\nfrom 50 different alphabets. Each class contains 20 samples\ndrawn by different people. Following [32, 39, 36], we aug-\nment new classes through 90◦, 180◦and 270◦rotations of\nexisting data and use 1200 original classes plus rotations for\ntraining and remaining 423 classes plus rotations for testing.\nAll input images are resized to 28 × 28.\nTraining\nBesides the K sample images, the 5-way 1-\nshot contains 19 query images, the 5-way 5-shot has 15\nquery images, the 20-way 1-shot has 10 query images and\nthe 20-way 5-shot has 5 query images for each of the C\nsampled classes in each training episode. This means for\n",
    "feature concatenation\n2X2 max-pool\nReLU\nbatch norm\n3X3 conv, 64 filters\n(a) Convolutional Block\n2X2 max-pool\nConvolutional Block\n2X2 max-pool\n2X2 max-pool\nFC ReLU, HX8\nrelation score\nFC Sigmoid, 8X1\n(b) RN for few-shot learning\nConvolutional Block\nConvolutional Block\nConvolutional Block\nConvolutional Block\nConvolutional Block\nFigure 2: Relation Network architecture for few-shot learning (b)\nwhich is composed of elements including convolutional block (a).\nexample that there are 19 × 5 + 1 × 5 = 100 images in\none training episode/mini-batch for the 5-way 1-shot exper-\niments.\nResults\nFollowing [36], we computed few-shot classiﬁ-\ncation accuracies on Omniglot by averaging over 1000 ran-\ndomly generated episodes from the testing set. For the 1-\nshot and 5-shot experiments, we batch one and ﬁve query\nimages per class respectively for evaluation during testing.\nThe results are shown in Table 1. We achieved state-of-the-\nart performance under all experiments setting with higher\naveraged accuracies and lower standard deviations, except\n5-way 5-shot where our model is 0.1% lower in accuracy\nthan [10]. This is despite that many alternatives have sig-\nniﬁcantly more complicated machinery [27, 8], or ﬁne-tune\non the target problem [10, 39], while we do not.\n4.1.2\nminiImageNet\nDataset\nThe miniImagenet dataset, originally proposed\nby [39], consists of 60,000 colour images with 100 classes,\neach having 600 examples. We followed the split intro-\nduced by [29], with 64, 16, and 20 classes for training, val-\nidation and testing, respectively. The 16 validation classes\nis used for monitoring generalisation performance only.\nTraining\nFollowing the standard setting adopted by most\nexisting few-shot learning work, we conducted 5 way 1-shot\nand 5-shot classiﬁcation. Beside the K sample images, the\nFC1, ReLU\nweight decay\nfeature concatenation\nDNN\nFC2, ReLU\nweight decay\nrelation score\nFC3 ReLU\nFC4 Sigmoid\nFigure 3: Relation Network architecture for zero-shot learning.\n5-way 1-shot contains 15 query images, and the 5-way 5-\nshot has 10 query images for each of the C sampled classes\nin each training episode. This means for example that there\nare 15×5+1×5 = 80 images in one training episode/mini-\nbatch for 5-way 1-shot experiments. We resize input images\nto 84 × 84. Our model is trained end-to-end from scratch,\nwith random initialisation, and no additional training set.\nResults\nFollowing [36], we batch 15 query images per\nclass in each episode for evaluation in both 1-shot and 5-\nshot scenarios and the few-shot classiﬁcation accuracies\nare computed by averaging over 600 randomly generated\nepisodes from the test set.\nFrom Table 2, we can see that our model achieved state-\nof-the-art performance on 5-way 1-shot settings and com-\npetitive results on 5-way 5-shot. However, the 1-shot result\nreported by prototypical networks [36] reqired to be trained\non 30-way 15 queries per training episode, and 5-shot re-\nsult was trained on 20-way 15 queries per training episode.\nWhen trained with 5-way 15 query per training episode,\n[36] only got 46.14 ± 0.77% for 1-shot evaluation, clearly\nweaker than ours. In contrast, all our models are trained\non 5-way, 1 query for 1-shot and 5 queries for 5-shot per\ntraining episode, with much less training queries than [36].\n4.2. Zero-shot Recognition\nDatasets and settings\nWe follow two ZSL settings: the\nold setting and the new GBU setting provided by [42] for\ntraining/test splits. Under the old setting, adopted by most\nexisting ZSL works before [42], some of the test classes\nalso appear in the ImageNet 1K classes, which have been\nused to pretrain the image embedding network, thus vio-\nlating the zero-shot assumption. In contrast, the new GBU\nsetting ensures that none of the test classes of the datasets\nappear in the ImageNet 1K classes. Under both settings, the\n",
    "Model\nFine Tune\n5-way Acc.\n20-way Acc.\n1-shot\n5-shot\n1-shot\n5-shot\nMANN [32]\nN\n82.8%\n94.9%\n-\n-\nCONVOLUTIONAL SIAMESE NETS [20]\nN\n96.7%\n98.4%\n88.0%\n96.5%\nCONVOLUTIONAL SIAMESE NETS [20]\nY\n97.3%\n98.4%\n88.1%\n97.0%\nMATCHING NETS [39]\nN\n98.1%\n98.9%\n93.8%\n98.5%\nMATCHING NETS [39]\nY\n97.9%\n98.7%\n93.5%\n98.7%\nSIAMESE NETS WITH MEMORY [18]\nN\n98.4%\n99.6%\n95.0%\n98.6%\nNEURAL STATISTICIAN [8]\nN\n98.1%\n99.5%\n93.2%\n98.1%\nMETA NETS [27]\nN\n99.0%\n-\n97.0%\n-\nPROTOTYPICAL NETS [36]\nN\n98.8%\n99.7%\n96.0%\n98.9%\nMAML [10]\nY\n98.7 ± 0.4%\n99.9 ± 0.1%\n95.8 ± 0.3%\n98.9 ± 0.2%\nRELATION NET\nN\n99.6 ± 0.2%\n99.8± 0.1%\n97.6 ± 0.2%\n99.1± 0.1%\nTable 1: Omniglot few-shot classiﬁcation. Results are accuracies averaged over 1000 test episodes and with 95% conﬁdence intervals\nwhere reported. The best-performing method is highlighted, along with others whose conﬁdence intervals overlap. ‘-’: not reported.\nModel\nFT\n5-way Acc.\n1-shot\n5-shot\nMATCHING NETS [39]\nN\n43.56 ± 0.84%\n55.31 ± 0.73%\nMETA NETS [27]\nN\n49.21 ± 0.96%\n-\nMETA-LEARN LSTM [29]\nN\n43.44 ± 0.77%\n60.60 ± 0.71%\nMAML [10]\nY\n48.70 ± 1.84%\n63.11 ± 0.92%\nPROTOTYPICAL NETS [36]\nN\n49.42 ± 0.78%\n68.20 ± 0.66%\nRELATION NET\nN\n50.44 ± 0.82%\n65.32 ± 0.70%\nTable 2: Few-shot classiﬁcation accuracies on miniImagenet. All\naccuracy results are averaged over 600 test episodes and are re-\nported with 95% conﬁdence intervals, same as [36]. For each task,\nthe best-performing method is highlighted, along with any others\nwhose conﬁdence intervals overlap. ‘-’: not reported.\ntest set can comprise only the unseen class samples (conven-\ntional test set setting) or a mixture of seen and unseen class\nsamples. The latter, termed generalised zero-shot learning\n(GZSL), is more realistic in practice.\nTwo widely used ZSL benchmarks are selected for the\nold setting: AwA (Animals with Attributes) [24] consists\nof 30,745 images of 50 classes of animals. It has a ﬁxed\nsplit for evaluation with 40 training classes and 10 test\nclasses. CUB (Caltech-UCSD Birds-200-2011) [40] con-\ntains 11,788 images of 200 bird species with 150 seen\nclasses and 50 disjoint unseen classes. Three datasets [42]\nare selected for GBU setting: AwA1, AwA2 and CUB. The\nnewly released AwA2 [42] consists of 37,322 images of 50\nclasses which is an extension of AwA while AwA1 is same\nas AwA but under the GBU setting.\nSemantic representation\nFor AwA, we use the contin-\nuous 85-dimension class-level attribute vector from [24],\nwhich has been used by all recent works. For CUB, a con-\ntinuous 312-dimension class-level attribute vector is used.\nImplementation details\nTwo different embedding mod-\nules are used for the two input modalities in zero-shot\nlearning.\nUnless otherwise speciﬁed, we use Inception-\nV2 [38, 17] as the query image embedding DNN in the\nold and conventional setting and ResNet101 [16] for the\nGBU and generalised setting, taking the top pooling units\nas image embedding with dimension D = 1024 and 2048\nrespectively. This DNN is pre-trained on ILSVRC 2012\n1K classiﬁcation without ﬁne-tuning, as in recent deep ZSL\nworks [25, 30, 45]. A MLP network is used for embed-\nding semantic attribute vectors. The size of hidden layer\nFC1 (Figure 3) is set to 1024 and 1200 for AwA and CUB\nrespectively, and the output size FC2 is set to the same di-\nmension as the image embedding for both datasets. For the\nrelation module, the image and semantic embeddings are\nconcatenated before being fed into MLPs with hidden layer\nFC3 size 400 and 1200 for AwA and CUB, respectively.\nWe add weight decay (L2 regularisation) in FC1 & 2 as\nthere is a hubness problem [45] in cross-modal mapping for\nZSL which can be best solved by mapping the semantic fea-\nture vector to the visual feature space with regularisation.\nAfter that, FC3 & 4 (relation module) are used to compute\nthe relation between the semantic representation (in the vi-\nsual feature space) and the visual representation. Since the\nhubness problem does not existing in this step, no L2 regu-\nlarisation/weight decay is needed. All the ZSL models are\ntrained with weight decay 10−5 in the embedding network.\nThe learning rate is initialised to 10−5 with Adam [19] and\nthen annealed by half every 200,000 iterations.\nResults under the old setting\nThe conventional evalua-\ntion for ZSL followed by the majority of prior work is to\nassume that the test data all comes from unseen classes. We\nevaluate this setting ﬁrst. We compare 15 alternative ap-\nproaches in Table 3. With only the attribute vector used as\nthe sample class embedding, our model achieves competi-\ntive result on AwA and state-of-the-art performance on the\nmore challenging CUB dataset, outperforming the most re-\nlated alternative prototypical networks [36] by a big margin.\nNote that only inductive methods are considered. Some re-\n",
    "Model\nF\nSS\nAwA\nCUB\n10-way 0-shot\n50-way 0-shot\nSJE [3]\nFG\nA\n66.7\n50.1\nESZSL [31]\nFG\nA\n76.3\n47.2\nSSE-RELU [46]\nFV\nA\n76.3\n30.4\nJLSE [47]\nFV\nA\n80.5\n42.1\nSYNC-STRUCT [6]\nFG\nA\n72.9\n54.5\nSEC-ML [5]\nFV\nA\n77.3\n43.3\nPROTO. NETS [36]\nFG\nA\n-\n54.6\nDEVISE [11]\nNG\nA/W\n56.7/50.4\n33.5\nSOCHER et al. [37]\nNG\nA/W\n60.8/50.3\n39.6\nMTMDL [43]\nNG\nA/W\n63.7/55.3\n32.3\nBA et al. [25]\nNG\nA/W\n69.3/58.7\n34.0\nDS-SJE [30]\nNG\nA/D\n-\n50.4/ 56.8\nSAE [21]\nNG\nA\n84.7\n61.4\nDEM [45]\nNG\nA/W\n86.7/78.8\n58.3\nRELATION NET\nNG\nA\n84.5\n62.0\nTable 3: Zero-shot classiﬁcation accuracy (%) comparison on AwA and\nCUB (hit@1 accuracy over all samples) under the old and conventional\nsetting. SS: semantic space; A: attribute space; W: semantic word vector\nspace; D: sentence description (only available for CUB). F: how the vi-\nsual feature space is computed; For non-deep models: FO if overfeat [34]\nis used; FG for GoogLeNet [38]; and FV for VGG net [35]. For neu-\nral network based methods, all use Inception-V2 (GoogLeNet with batch\nnormalisation) [38, 17] as the DNN image imbedding subnet, indicated as\nNG.\ncent methods [48, 12, 13] are tranductive in that they use all\ntest data at once for model training, which gives them a big\nadvantage at the cost of making a very strong assumption\nthat may not be met in practical applications, so we do not\ncompare with them here.\nResults under the GBU setting\nWe follow the evalua-\ntion setting of [42]. We compare our model with 11 alterna-\ntive ZSL models in Table 4. The 10 shallow models results\nare from [42] and the result of the state-of-the-art method\nDEM [45] is from the authors’ GitHub page1. We can see\nthat on AwA2 and CUB, Our model is particularly strong\nunder the more realistic GZSL setting measured using the\nharmonic mean (H) metric. While on AwA1, our method is\nonly outperformed by DEM [45].\n5. Why does Relation Network Work?\n5.1. Relationship to existing models\nRelated prior few-shot work uses ﬁxed pre-speciﬁed dis-\ntance metrics such as Euclidean or cosine distance to per-\nform classiﬁcation [39, 36]. These studies can be seen as\ndistance metric learning, but where all the learning occurs in\nthe feature embedding, and a ﬁxed metric is used given the\nlearned embedding. Also related are conventional metric\nlearning approaches [26, 7] that focus on learning a shallow\n(linear) Mahalanobis metric for a ﬁxed feature representa-\n1https://github.com/lzrobots/\nDeepEmbeddingModel_ZSL\n(a) Ground Truth\n(b) Relation Network\n(c) Metric Learning\n(d) Metric + Embedding\nFigure 4: An example relation learnable by Relation Network and\nnot by non-linear embedding + metric learning.\ntion. In contrast to prior work’s ﬁxed metric or ﬁxed fea-\ntures and shallow learned metric, Relation Network can be\nseen as both learning a deep embedding and learning a deep\nnon-linear metric (similarity function)2. These are mutually\ntuned end-to-end to support each other in few short learn-\ning.\nWhy might this be particularly useful? By using a ﬂex-\nible function approximator to learn similarity, we learn a\ngood metric in a data driven way and do not have to man-\nually choose the right metric (Euclidean, cosine, Maha-\nlanobis). Fixed metrics like [39, 36] assume that features\nare solely compared element-wise, and the most related [36]\nassumes linear separability after the embedding. These are\nthus critically dependent on the efﬁcacy of the learned em-\nbedding network, and hence limited by the extent to which\nthe embedding networks generate inadequately discrimina-\ntive representations. In contrast, by deep learning a non-\nlinear similarity metric jointly with the embedding, Relation\nNetwork can better identify matching/mismatching pairs.\n5.2. Visualisation\nTo illustrate the previous point about adequacy of learned\ninput embeddings, we show a synthetic example where ex-\nisting approaches deﬁnitely fail and our Relation Network\ncan succeed due to using a deep relation module. Assuming\n2D query and sample input embeddings to a relation mod-\nule, Fig. 4(a) shows the space of 2D sample inputs for a\nﬁxed 2D query input. Each sample input (pixel) is colored\naccording to whether it matches the ﬁxed query or not. This\n2Our architecture does not guarantee the self-similarity and symmetry\nproperties of a formal similarity function. But empirically we ﬁnd these\nproperties hold numerically for a trained Relation Network.\n",
    "AwA1\nAwA2\nCUB\nZSL\nGZSL\nZSL\nGZSL\nZSL\nGZSL\nModel\nT1\nu\ns\nH\nT1\nu\ns\nH\nT1\nu\ns\nH\nDAP [24]\n44.1\n0.0\n88.7\n0.0\n46.1\n0.0\n84.7\n0.0\n40.0\n1.7\n67.9\n3.3\nCONSE [28]\n45.6\n0.4\n88.6\n0.8\n44.5\n0.5\n90.6\n1.0\n34.3\n1.6\n72.2\n3.1\nSSE [46]\n60.1\n7.0\n80.5\n12.9\n61.0\n8.1\n82.5\n14.8\n43.9\n8.5\n46.9\n14.4\nDEVISE [11]\n54.2\n13.4\n68.7\n22.4\n59.7\n17.1\n74.7\n27.8\n52.0\n23.8\n53.0\n32.8\nSJE [3]\n65.6\n11.3\n74.6\n19.6\n61.9\n8.0\n73.9\n14.4\n53.9\n23.5\n59.2\n33.6\nLATEM [41]\n55.1\n7.3\n71.7\n13.3\n55.8\n11.5\n77.3\n20.0\n49.3\n15.2\n57.3\n24.0\nESZSL [31]\n58.2\n6.6\n75.6\n12.1\n58.6\n5.9\n77.8\n11.0\n53.9\n12.6\n63.8\n21.0\nALE [2]\n59.9\n16.8\n76.1\n27.5\n62.5\n14.0\n81.8\n23.9\n54.9\n23.7\n62.8\n34.4\nSYNC [6]\n54.0\n8.9\n87.3\n16.2\n46.6\n10.0\n90.5\n18.0\n55.6\n11.5\n70.9\n19.8\nSAE [21]\n53.0\n1.8\n77.1\n3.5\n54.1\n1.1\n82.2\n2.2\n33.3\n7.8\n57.9\n29.2\nDEM [45]\n68.4\n32.8\n84.7\n47.3\n67.1\n30.5\n86.4\n45.1\n51.7\n19.6\n54.0\n13.6\nRELATION NET\n68.2\n31.4\n91.3\n46.7\n64.2\n30.0\n93.4\n45.3\n55.6\n38.1\n61.1\n47.0\nTable 4: Comparative results under the GBU setting. Under the conventional ZSL setting, the performance is evaluated using per-class\naverage Top-1 (T1) accuracy (%), and under GZSL, it is measured using u = T1 on unseen classes, s = T1 on seen classes, and H =\nharmonic mean.\nrepresents a case where the output of the embedding mod-\nules is not discriminative enough for trivial (Euclidean NN)\ncomparison between query and sample set. In Fig. 4(c) we\nattempt to learn matching via a Mahalanobis metric learn-\ning relation module, and we can see the result is inadequate.\nIn Fig. 4(d) we learn a further 2-hidden layer MLP embed-\nding of query and sample inputs as well as the subsequent\nMahalanobis metric, which is also not adequate. Only by\nlearning the full deep relation module for similarity can we\nsolve this problem in Fig. 4(b).\nIn a real problem the difﬁculty of comparing embeddings\nmay not be this extreme, but it can still be challenging. We\nqualitatively illustrate the challenge of matching two exam-\nple Omniglot query images (embeddings projected to 2D,\nFigure 5(left)) by showing an analogous plot of real sample\nimages colored by match (cyan) or mismatch (magenta) to\ntwo example queries (yellow). Under standard assumptions\n[39, 36, 26, 7] the cyan matching samples should be near-\nest neighbours to the yellow query image with some metric\n(Euclidean, Cosine, Mahalanobis). But we can see that the\nmatch relation is more complex than this. In Figure 5(right),\nwe instead plot the same two example queries in terms of a\n2D PCA representation of each query-sample pair, as repre-\nsented by the relation module’s penultimate layer. We can\nsee that the relation network has mapped the data into a\nspace where the (mis)matched pairs are linearly separable.\n6. Conclusion\nWe proposed a simple method called the Relation Net-\nwork for few-shot and zero-shot learning. Relation network\nlearns an embedding and a deep non-linear distance metric\nfor comparing query and sample items. Training the net-\nwork end-to-end with episodic training tunes the embedding\nand distance metric for effective few-shot learning. This ap-\nFigure 5: Example Omniglot few-shot problem visualisations.\nLeft: Matched (cyan) and mismatched (magenta) sample embed-\ndings for a given query (yellow) are not straightforward to dif-\nferentiate. Right: Matched (yellow) and mismatched (magenta)\nrelation module pair representations are linearly separable.\nproach is far simpler and more efﬁcient than recent few-shot\nmeta-learning approaches, and produces state-of-the-art re-\nsults. It further proves effective at both conventional and\ngeneralised zero-shot settings.\nAcknowledgements\nThis work was supported by the\nERC grant ERC-2012-AdG 321162-HELIOS, EPSRC\ngrant\nSeebibyte\nEP/M013774/1,\nEPSRC/MURI\ngrant\nEP/N019474/1, EPSRC grant EP/R026173/1, and the\nEuropean Union’s Horizon 2020 research and innovation\nprogram (grant agreement no.\n640891).\nWe grate-\nfully acknowledge the support of NVIDIA Corporation\nwith the donation of the Titan Xp GPU and the ES-\nPRC funded Tier 2 facility, JADE used for this research.\n",
    "References\n[1] Pytorch.\nhttps://github.com/pytorch/\npytorch. 4\n[2] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label-\nembedding for image classiﬁcation. TPAMI, 2016. 8\n[3] Z. Akata, S. Reed, D. Walter, H. Lee, and B. Schiele. Eval-\nuation of output embeddings for ﬁne-grained image classiﬁ-\ncation. In CVPR, 2015. 1, 3, 7, 8\n[4] L. Bertinetto, J. F. Henriques, J. Valmadre, P. H. S. Torr,\nand A. Vedaldi. Learning feed-forward one-shot learners.\nIn NIPS, 2016. 1, 2\n[5] M. Bucher, S. Herbin, and F. Jurie. Improving semantic em-\nbedding consistency by metric learning for zero-shot classif-\nﬁcation. In ECCV, 2016. 7\n[6] S. Changpinyo, W.-L. Chao, B. Gong, and F. Sha. Synthe-\nsized classiﬁers for zero-shot learning. In CVPR, 2016. 7,\n8\n[7] D. Chen, X. Cao, L. Wang, F. Wen, and J. Sun. Bayesian face\nrevisited: A joint formulation. In ECCV. Springer Berlin\nHeidelberg, 2012. 7, 8\n[8] H. Edwards and A. Storkey. Towards a neural statistician.\nICLR, 2017. 1, 4, 5, 6\n[9] L. Fei-Fei, R. Fergus, and P. Perona. One-shot learning of\nobject categories. TPAMI, 2006. 1, 2\n[10] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-\nlearning for fast adaptation of deep networks.\nIn ICML,\n2017. 1, 2, 4, 5, 6\n[11] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean,\nT. Mikolov, et al. Devise: A deep visual-semantic embed-\nding model. In NIPS, 2013. 1, 3, 7, 8\n[12] Y. Fu, T. M. Hospedales, T. Xiang, Z. Fu, and S. Gong.\nTransductive multi-view embedding for zero-shot recogni-\ntion and annotation. In ECCV, 2014. 7\n[13] Y. Fu and L. Sigal. Semi-supervised vocabulary-informed\nlearning. In CVPR, 2016. 7\n[14] X. Han, T. Leung, Y. Jia, R. Sukthankar, and A. C. Berg.\nMatchnet: Unifying feature and metric learning for patch-\nbased matching. In CVPR, 2015. 2\n[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In CVPR, 2016. 1, 2\n[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In CVPR, 2016. 6\n[17] S. Ioffe and C. Szegedy. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift. In\nICML, 2015. 6, 7\n[18] Ł. Kaiser, O. Nachum, A. Roy, and S. Bengio. Learning to\nremember rare events. ICLR, 2017. 1, 4, 6\n[19] D. Kingma and J. Ba. Adam: A method for stochastic opti-\nmization. In ICLR, 2015. 4, 6\n[20] G. Koch, R. Zemel, and R. Salakhutdinov. Siamese neural\nnetworks for one-shot image recognition. In ICML Work-\nshop, 2015. 1, 2, 4, 6\n[21] E. Kodirov, T. Xiang, and S. Gong. Semantic autoencoder\nfor zero-shot learning. In CVPR, 2017. 7, 8\n[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImagenet\nclassiﬁcation with deep convolutional neural networks. In\nNIPS, 2012. 1, 2\n[23] B. Lake, R. Salakhutdinov, J. Gross, and J. Tenenbaum. One\nshot learning of simple visual concepts. In CogSci, 2011. 1,\n2, 4\n[24] C. H. Lampert, H. Nickisch, and S. Harmeling. Attribute-\nbased classiﬁcation for zero-shot visual object categoriza-\ntion. PAMI, 2014. 1, 6, 8\n[25] J. Lei Ba, K. Swersky, S. Fidler, and R. Salakhutdinov. Pre-\ndicting deep zero-shot convolutional neural networks using\ntextual descriptions. In ICCV, 2015. 1, 6, 7\n[26] T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka. Metric\nlearning for large scale image classiﬁcation: Generalizing to\nnew classes at near-zero cost. In ECCV, 2012. 7, 8\n[27] T. Munkhdalai and H. Yu. Meta networks. In ICML, 2017.\n1, 2, 4, 5, 6\n[28] M. Norouzi, T. Mikolov, S. Bengio, Y. Singer, J. Shlens,\nA. Frome, G. S. Corrado, and J. Dean. Zero-shot learning\nby convex combination of semantic embeddings. In ICLR,\n2014. 8\n[29] S. Ravi and H. Larochelle. Optimization as a model for few-\nshot learning. In ICLR, 2017. 1, 2, 4, 5, 6\n[30] S. Reed, Z. Akata, B. Schiele, and H. Lee. Learning deep\nrepresentations of ﬁne-grained visual descriptions. In CVPR,\n2016. 6, 7\n[31] B. Romera-Paredes and P. Torr. An embarrassingly simple\napproach to zero-shot learning. In ICML, 2015. 1, 7, 8\n[32] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and\nT. Lillicrap. Meta-learning with memory-augmented neural\nnetworks. In ICML, 2016. 1, 2, 4, 6\n[33] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski,\nR. Pascanu, P. Battaglia, and T. Lillicrap. A simple neural\nnetwork module for relational reasoning. In NIPS, 2017. 2\n[34] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\nand Y. LeCun. Overfeat: Integrated recognition, localization\nand detection using convolutional networks. arXiv preprint\narXiv:1312.6229, 2013. 7\n[35] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. ICLR, 2015. 1,\n2, 7\n[36] J. Snell, K. Swersky, and R. S. Zemel. Prototypical networks\nfor few-shot learning. In NIPS, 2017. 1, 2, 4, 5, 6, 7, 8\n[37] R. Socher, M. Ganjoo, C. D. Manning, and A. Ng. Zero-shot\nlearning through cross-modal transfer. In NIPS, 2013. 7\n[38] C. Szegedy,\nW. Liu,\nY. Jia,\nP. Sermanet,\nS. Reed,\nD. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.\nGoing deeper with convolutions. In CVPR, 2015. 6, 7\n[39] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al.\nMatching networks for one shot learning. In NIPS, 2016.\n1, 2, 3, 4, 5, 6, 7, 8\n[40] C. Wah, S. Branson, P. Perona, and S. Belongie. Multiclass\nrecognition and part localization with humans in the loop. In\nICCV, 2011. 6\n[41] Y. Xian, Z. Akata, G. Sharma, Q. Nguyen, M. Hein, and\nB. Schiele. Latent embeddings for zero-shot classiﬁcation.\nIn CVPR, 2016. 8\n[42] Y. Xian, C. H. Lampert, B. Schiele, and Z. Akata. Zero-\nshot learning-a comprehensive evaluation of the good, the\nbad and the ugly. arXiv preprint arXiv:1707.00600, 2017. 5,\n6, 7\n",
    "[43] Y. Yang and T. M. Hospedales.\nA uniﬁed perspective on\nmulti-domain and multi-task learning. In ICLR, 2015. 3, 7\n[44] S. Zagoruyko and N. Komodakis. Learning to compare im-\nage patches via convolutional neural networks. In CVPR,\n2015. 2\n[45] L. Zhang, T. Xiang, and S. Gong. Learning a deep embed-\nding model for zero-shot learning. In CVPR, 2017. 1, 6, 7,\n8\n[46] Z. Zhang and V. Saligrama. Zero-shot learning via semantic\nsimilarity embedding. In ICCV, 2015. 3, 7, 8\n[47] Z. Zhang and V. Saligrama. Zero-shot learning via joint la-\ntent similarity embedding. In CVPR, 2016. 7\n[48] Z. Zhang and V. Saligrama. Zero-shot recognition via struc-\ntured prediction. In ECCV, 2016. 7\n"
  ],
  "full_text": "Learning to Compare: Relation Network for Few-Shot Learning\nFlood Sung\nYongxin Yang3\nLi Zhang2\nTao Xiang1\nPhilip H.S. Torr2\nTimothy M. Hospedales3\n1Queen Mary University of London\n2University of Oxford\n3The University of Edinburgh\nfloodsung@gmail.com\nt.xiang@qmul.ac.uk\n{lz, phst}@robots.ox.ac.uk\n{yongxin.yang, t.hospedales}@ed.ac.uk\nAbstract\nWe present a conceptually simple, ﬂexible, and general\nframework for few-shot learning, where a classiﬁer must\nlearn to recognise new classes given only few examples from\neach. Our method, called the Relation Network (RN), is\ntrained end-to-end from scratch. During meta-learning, it\nlearns to learn a deep distance metric to compare a small\nnumber of images within episodes, each of which is de-\nsigned to simulate the few-shot setting. Once trained, a RN\nis able to classify images of new classes by computing rela-\ntion scores between query images and the few examples of\neach new class without further updating the network. Be-\nsides providing improved performance on few-shot learn-\ning, our framework is easily extended to zero-shot learning.\nExtensive experiments on ﬁve benchmarks demonstrate that\nour simple approach provides a uniﬁed and effective ap-\nproach for both of these two tasks.\n1. Introduction\nDeep learning models have achieved great success in vi-\nsual recognition tasks [22, 15, 35]. However, these super-\nvised learning models need large amounts of labelled data\nand many iterations to train their large number of parame-\nters. This severely limits their scalability to new classes due\nto annotation cost, but more fundamentally limits their ap-\nplicability to newly emerging (eg. new consumer devices)\nor rare (eg. rare animals) categories where numerous anno-\ntated images may simply never exist. In contrast, humans\nare very good at recognising objects with very little direct\nsupervision, or none at all i.e., few-shot [23, 9] or zero-shot\n[24] learning. For example, children have no problem gen-\neralising the concept of “zebra” from a single picture in\na book, or hearing its description as looking like a stripy\nhorse. Motivated by the failure of conventional deep learn-\ning methods to work well on one or few examples per class,\nand inspired by the few- and zero-shot learning ability of\nhumans, there has been a recent resurgence of interest in\nmachine one/few-shot [8, 39, 32, 18, 20, 10, 27, 36, 29] and\nzero-shot [11, 3, 24, 45, 25, 31] learning.\nFew-shot learning aims to recognise novel visual cate-\ngories from very few labelled examples. The availability\nof only one or very few examples challenges the standard\n‘ﬁne-tuning’ practice in deep learning [10]. Data augmen-\ntation and regularisation techniques can alleviate overﬁt-\nting in such a limited-data regime, but they do not solve\nit. Therefore contemporary approaches to few-shot learning\noften decompose training into an auxiliary meta learning\nphase where transferrable knowledge is learned in the form\nof good initial conditions [10], embeddings [36, 39] or opti-\nmisation strategies [29]. The target few-shot learning prob-\nlem is then learned by ﬁne-tuning [10] with the learned op-\ntimisation strategy [29] or computed in a feed-forward pass\n[36, 39, 4, 32] without updating network weights. Zero-shot\nlearning also suffers from a related challenge. Recognisers\nare trained by a single example in the form of a class de-\nscription (c.f., single exemplar image in one-shot), making\ndata insufﬁciency for gradient-based learning a challenge.\nWhile promising, most existing few-shot learning ap-\nproaches either require complex inference mechanisms [23,\n9], complex recurrent neural network (RNN) architectures\n[39, 32], or ﬁne-tuning the target problem [10, 29]. Our\napproach is most related to others that aim to train an effec-\ntive metric for one-shot learning [39, 36, 20]. Where they\nfocus on the learning of the transferrable embedding and\npre-deﬁne a ﬁxed metric (e.g., as Euclidean [36]), we fur-\nther aim to learn a transferrable deep metric for comparing\nthe relation between images (few-shot learning), or between\nimages and class descriptions (zero-shot learning). By ex-\npressing the inductive bias of a deeper solution (multiple\nnon-linear learned stages at both embedding and relation\nmodules), we make it easier to learn a generalisable solu-\ntion to the problem.\nSpeciﬁcally, we propose a two-branch Relation Network\n(RN) that performs few-shot recognition by learning to\ncompare query images against few-shot labeled sample im-\nages. First an embedding module generates representations\nof the query and training images. Then these embeddings\nare compared by a relation module that determines if they\narXiv:1711.06025v2  [cs.CV]  27 Mar 2018\n\n\nare from matching categories or not. Deﬁning an episode-\nbased strategy inspired by [39, 36], the embedding and re-\nlation modules are meta-learned end-to-end to support few-\nshot learning. This can be seen as extending the strategy\nof [39, 36] to include a learnable non-linear comparator,\ninstead of a ﬁxed linear comparator.\nOur approach out-\nperforms prior approaches, while being simpler (no RNNs\n[39, 32, 29]) and faster (no ﬁne-tuning [29, 10]). Our pro-\nposed strategy also directly generalises to zero-shot learn-\ning. In this case the sample branch embeds a single-shot\ncategory description rather than a single exemplar training\nimage, and the relation module learns to compare query im-\nage and category description embeddings.\nOverall our contribution is to provide a clean framework\nthat elegantly encompasses both few and zero-shot learn-\ning. Our evaluation on four benchmarks show that it pro-\nvides compelling performance across the board while being\nsimpler and faster than the alternatives.\n2. Related Work\nThe study of one or few-shot object recognition has been\nof interest for some time [9].\nEarlier work on few-shot\nlearning tended to involve generative models with complex\niterative inference strategies [9, 23]. With the success of\ndiscriminative deep learning-based approaches in the data-\nrich many-shot setting [22, 15, 35], there has been a surge\nof interest in generalising such deep learning approaches to\nthe few-shot learning setting. Many of these approaches use\na meta-learning or learning-to-learn strategy in the sense\nthat they extract some transferrable knowledge from a set\nof auxiliary tasks (meta-learning, learning-to-learn), which\nthen helps them to learn the target few-shot problem well\nwithout suffering from the overﬁtting that might be ex-\npected when applying deep models to sparse data problems.\nLearning to Fine-Tune\nThe successful MAML approach\n[10] aimed to meta-learn an initial condition (set of neural\nnetwork weights) that is good for ﬁne-tuning on few-shot\nproblems.\nThe strategy here is to search for the weight\nconﬁguration of a given neural network such that it can\nbe effectively ﬁne-tuned on a sparse data problem within\na few gradient-descent update steps. Many distinct target\nproblems are sampled from a multiple task training set; the\nbase neural network model is then ﬁne-tuned to solve each\nof them, and the success at each target problem after ﬁne-\ntuning drives updates in the base model – thus driving the\nproduction of an easy to ﬁne-tune initial condition. The\nfew-shot optimisation approach [29] goes further in meta-\nlearning not only a good initial condition but an LSTM-\nbased optimizer that is trained to be speciﬁcally effective for\nﬁne-tuning. However both of these approaches suffer from\nthe need to ﬁne-tune on the target problem. In contrast, our\napproach solves target problems in an entirely feed-forward\nmanner with no model updates required, making it more\nconvenient for low-latency or low-power applications.\nRNN Memory Based\nAnother category of approaches\nleverage recurrent neural networks with memories [27, 32].\nHere the idea is typically that an RNN iterates over an ex-\namples of given problem and accumulates the knowledge\nrequired to solve that problem in its hidden activations, or\nexternal memory. New examples can be classiﬁed, for ex-\nample by comparing them to historic information stored in\nthe memory. So ‘learning’ a single target problem can oc-\ncur in unrolling the RNN, while learning-to-learn means\ntraining the weights of the RNN by learning many distinct\nproblems. While appealing, these architectures face issues\nin ensuring that they reliably store all the, potentially long\nterm, historical information of relevance without forgetting.\nIn our approach we avoid the complexity of recurrent net-\nworks, and the issues involved in ensuring the adequacy of\ntheir memory. Instead our learning-to-learn approach is de-\nﬁned entirely with simple and fast feed forward CNNs.\nEmbedding and Metric Learning Approaches\nThe\nprior approaches entail some complexity when learning the\ntarget few-shot problem.\nAnother category of approach\naims to learn a set of projection functions that take query\nand sample images from the target problem and classify\nthem in a feed forward manner [39, 36, 4]. One approach\nis to parameterise the weights of a feed-forward classiﬁer\nin terms of the sample set [4]. The meta-learning here is\nto train the auxiliary parameterisation net that learns how\nto paramaterise a given feed-forward classiﬁcation problem\nin terms of a few-shot sample set. Metric-learning based\napproaches aim to learn a set of projection functions such\nthat when represented in this embedding, images are easy\nto recognise using simple nearest neighbour or linear classi-\nﬁers [39, 36, 20]. In this case the meta-learned transferrable\nknowledge are the projection functions and the target prob-\nlem is a simple feed-forward computation.\nThe most related methodologies to ours are the proto-\ntypical networks of [36] and the siamese networks of [20].\nThese approaches focus on learning embeddings that trans-\nform the data such that it can be recognised with a ﬁxed\nnearest-neighbour [36] or linear [20, 36] classiﬁer. In con-\ntrast, our framework further deﬁnes a relation classiﬁer\nCNN, in the style of [33, 44, 14] (While [33] focuses on\nreasoning about relation between two objects in a same im-\nage which is to address a different problem.). Compared\nto [20, 36], this can be seen as providing a learnable rather\nthan ﬁxed metric, or non-linear rather than linear classiﬁer.\nCompared to [20] we beneﬁt from an episodic training strat-\negy with an end-to-end manner from scratch, and compared\nto [32] we avoid the complexity of set-to-set RNN embed-\nding of the sample-set, and simply rely on pooling [33].\nZero-Shot Learning\nOur approach is designed for few-\n\n\nshot learning, but elegantly spans the space into zero-shot\nlearning (ZSL) by modifying the sample branch to input a\nsingle category description rather than single training im-\nage. When applied to ZSL our architecture is related to\nmethods that learn to align images and category embed-\ndings and perform recognition by predicting if an image\nand category embedding pair match [11, 3, 43, 46]. Sim-\nilarly to the case with the prior metric-based few-shot ap-\nproaches, most of these apply a ﬁxed manually deﬁned sim-\nilarity metric or linear classiﬁer after combining the image\nand category embedding. In contrast, we again beneﬁt from\na deeper end-to-end architecture including a learned non-\nlinear metric in the form of our learned convolutional re-\nlation network; as well as from an episode-based training\nstrategy.\n3. Methodology\n3.1. Problem Deﬁnition\nWe consider the task of few-shot classiﬁer learning. For-\nmally, we have three datasets: a training set, a support set,\nand a testing set. The support set and testing set share the\nsame label space, but the training set has its own label space\nthat is disjoint with support/testing set. If the support set\ncontains K labelled examples for each of C unique classes,\nthe target few-shot problem is called C-way K-shot.\nWith the support set only, we can in principle train a clas-\nsiﬁer to assign a class label ˆy to each sample ˆx in the test\nset. However, due to the lack of labelled samples in the sup-\nport set, the performance of such a classiﬁer is usually not\nsatisfactory. Therefore we aim to perform meta-learning on\nthe training set, in order to extract transferrable knowledge\nthat will allow us to perform better few-shot learning on the\nsupport set and thus classify the test set more successfully.\nAn effective way to exploit the training set is to mimic\nthe few-shot learning setting via episode based training, as\nproposed in [39]. In each training iteration, an episode is\nformed by randomly selecting C classes from the training\nset with K labelled samples from each of the C classes to\nact as the sample set S = {(xi, yi)}m\ni=1 (m = K × C), as\nwell as a fraction of the remainder of those C classes’ sam-\nples to serve as the query set Q = {(xj, yj)}n\nj=1. This sam-\nple/query set split is designed to simulate the support/test set\nthat will be encountered at test time. A model trained from\nsample/query set can be further ﬁne-tuned using the support\nset, if desired. In this work we adopt such an episode-based\ntraining strategy. In our few-shot experiments (see Section\n4.1) we consider one-shot (K = 1, Figure 1) and ﬁve-shot\n(K = 5) settings. We also address the K = 0 zero-shot\nlearning case as explained in Section 3.3.\n3.2. Model\nOne-Shot\nOur Relation Network (RN) consists of two\nmodules: an embedding module fϕ and a relation module\ngφ, as illustrated in Figure 1. Samples xj in the query set Q,\nand samples xi in the sample set S are fed through the em-\nbedding module fϕ, which produces feature maps fϕ(xi)\nand fϕ(xj). The feature maps fϕ(xi) and fϕ(xj) are com-\nbined with operator C(fϕ(xi), fϕ(xj)). In this work we as-\nsume C(·, ·) to be concatenation of feature maps in depth,\nalthough other choices are possible.\nThe combined feature map of the sample and query are\nfed into the relation module gφ, which eventually produces\na scalar in range of 0 to 1 representing the similarity be-\ntween xi and xj, which is called relation score. Thus, in\nthe C-way one-shot setting, we generate C relation scores\nri,j for the relation between one query input xj and training\nsample set examples xi,\nri,j = gφ(C(fϕ(xi), fϕ(xj))),\ni = 1, 2, . . . , C\n(1)\nK-shot\nFor K-shot where K > 1, we element-wise sum\nover the embedding module outputs of all samples from\neach training class to form this class’ feature map. This\npooled class-level feature map is combined with the query\nimage feature map as above. Thus, the number of relation\nscores for one query is always C in both one-shot or few-\nshot setting.\nObjective function\nWe use mean square error (MSE)\nloss (Eq. (2)) to train our model, regressing the relation\nscore ri,j to the ground truth: matched pairs have similarity\n1 and the mismatched pair have similarity 0.\nϕ, φ ←argmin\nϕ,φ\nm\nX\ni=1\nn\nX\nj=1\n(ri,j −1(yi == yj))2\n(2)\nThe choice of MSE is somewhat non-standard.\nOur\nproblem may seem to be a classiﬁcation problem with a la-\nbel space {0, 1}. However conceptually we are predicting\nrelation scores, which can be considered a regression prob-\nlem despite that for ground-truth we can only automatically\ngenerate {0, 1} targets.\n3.3. Zero-shot Learning\nZero-shot learning is analogous to one-shot learning in\nthat one datum is given to deﬁne each class to recognise.\nHowever instead of being given a support set with one-shot\nimage for each of C training classes, it contains a semantic\nclass embedding vector vc for each. Modifying our frame-\nwork to deal with the zero-shot case is straightforward: as\na different modality of semantic vectors is used for the sup-\nport set (e.g. attribute vectors instead of images), we use a\n\n\n𝑓\"\n𝑔$\nOne-hot \nvector\nRelation \nscore\nFeature maps concatenation\nrelation module\nembedding module\nFigure 1: Relation Network architecture for a 5-way 1-shot problem with one query example.\nsecond heterogeneous embedding module fϕ2 besides the\nembedding module fϕ1 used for the image query set. Then\nthe relation net gφ is applied as before. Therefore, the rela-\ntion score for each query input xj will be:\nri,j = gφ(C(fϕ1(vc), fϕ2(xj))),\ni = 1, 2, . . . , C\n(3)\nThe objective function for zero-shot learning is the same\nas that for few-shot learning.\n3.4. Network Architecture\nAs most few-shot learning models utilise four convolu-\ntional blocks for embedding module [39, 36], we follow the\nsame architecture setting for fair comparison, see Figure 2.\nMore concretely, each convolutional block contains a 64-\nﬁlter 3 × 3 convolution, a batch normalisation and a ReLU\nnonlinearity layer respectively. The ﬁrst two blocks also\ncontain a 2 × 2 max-pooling layer while the latter two do\nnot. We do so because we need the output feature maps\nfor further convolutional layers in the relation module. The\nrelation module consists of two convolutional blocks and\ntwo fully-connected layers. Each of convolutional block\nis a 3 × 3 convolution with 64 ﬁlters followed by batch\nnormalisation, ReLU non-linearity and 2 × 2 max-pooling.\nThe output size of last max pooling layer is H = 64 and\nH = 64 ∗3 ∗3 = 576 for Omniglot and miniImageNet\nrespectively. The two fully-connected layers are 8 and 1\ndimensional, respectively. All fully-connected layers are\nReLU except the output layer is Sigmoid in order to gen-\nerate relation scores in a reasonable range for all versions\nof our network architecture.\nThe zero-shot learning architecture is shown in Figure 3.\nIn this architecture, the DNN subnet is an existing network\n(e.g., Inception or ResNet) pretrained on ImageNet.\n4. Experiments\nWe evaluate our approach on two related tasks: few-shot\nclassiﬁcation on Omniglot and miniImagenet, and zero-\nshot classiﬁcation on Animals with Attributes (AwA) and\nCaltech-UCSD Birds-200-2011 (CUB). All the experiments\nare implemented based on PyTorch [1].\n4.1. Few-shot Recognition\nSettings\nFew-shot learning in all experiments uses\nAdam [19] with initial learning rate 10−3 , annealed by half\nfor every 100,000 episodes. All our models are end-to-end\ntrained from scratch with no additional dataset.\nBaselines\nWe compare against various state of the art\nbaselines for few-shot recognition, including neural statisti-\ncian [8], Matching Nets with and without ﬁne-tuning [39],\nMANN [32], Siamese Nets with Memory [18], Convolu-\ntional Siamese Nets [20], MAML [10], Meta Nets [27], Pro-\ntotypical Nets [36] and Meta-Learner LSTM [29].\n4.1.1\nOmniglot\nDataset\nOmniglot [23] contains 1623 characters (classes)\nfrom 50 different alphabets. Each class contains 20 samples\ndrawn by different people. Following [32, 39, 36], we aug-\nment new classes through 90◦, 180◦and 270◦rotations of\nexisting data and use 1200 original classes plus rotations for\ntraining and remaining 423 classes plus rotations for testing.\nAll input images are resized to 28 × 28.\nTraining\nBesides the K sample images, the 5-way 1-\nshot contains 19 query images, the 5-way 5-shot has 15\nquery images, the 20-way 1-shot has 10 query images and\nthe 20-way 5-shot has 5 query images for each of the C\nsampled classes in each training episode. This means for\n\n\nfeature concatenation\n2X2 max-pool\nReLU\nbatch norm\n3X3 conv, 64 filters\n(a) Convolutional Block\n2X2 max-pool\nConvolutional Block\n2X2 max-pool\n2X2 max-pool\nFC ReLU, HX8\nrelation score\nFC Sigmoid, 8X1\n(b) RN for few-shot learning\nConvolutional Block\nConvolutional Block\nConvolutional Block\nConvolutional Block\nConvolutional Block\nFigure 2: Relation Network architecture for few-shot learning (b)\nwhich is composed of elements including convolutional block (a).\nexample that there are 19 × 5 + 1 × 5 = 100 images in\none training episode/mini-batch for the 5-way 1-shot exper-\niments.\nResults\nFollowing [36], we computed few-shot classiﬁ-\ncation accuracies on Omniglot by averaging over 1000 ran-\ndomly generated episodes from the testing set. For the 1-\nshot and 5-shot experiments, we batch one and ﬁve query\nimages per class respectively for evaluation during testing.\nThe results are shown in Table 1. We achieved state-of-the-\nart performance under all experiments setting with higher\naveraged accuracies and lower standard deviations, except\n5-way 5-shot where our model is 0.1% lower in accuracy\nthan [10]. This is despite that many alternatives have sig-\nniﬁcantly more complicated machinery [27, 8], or ﬁne-tune\non the target problem [10, 39], while we do not.\n4.1.2\nminiImageNet\nDataset\nThe miniImagenet dataset, originally proposed\nby [39], consists of 60,000 colour images with 100 classes,\neach having 600 examples. We followed the split intro-\nduced by [29], with 64, 16, and 20 classes for training, val-\nidation and testing, respectively. The 16 validation classes\nis used for monitoring generalisation performance only.\nTraining\nFollowing the standard setting adopted by most\nexisting few-shot learning work, we conducted 5 way 1-shot\nand 5-shot classiﬁcation. Beside the K sample images, the\nFC1, ReLU\nweight decay\nfeature concatenation\nDNN\nFC2, ReLU\nweight decay\nrelation score\nFC3 ReLU\nFC4 Sigmoid\nFigure 3: Relation Network architecture for zero-shot learning.\n5-way 1-shot contains 15 query images, and the 5-way 5-\nshot has 10 query images for each of the C sampled classes\nin each training episode. This means for example that there\nare 15×5+1×5 = 80 images in one training episode/mini-\nbatch for 5-way 1-shot experiments. We resize input images\nto 84 × 84. Our model is trained end-to-end from scratch,\nwith random initialisation, and no additional training set.\nResults\nFollowing [36], we batch 15 query images per\nclass in each episode for evaluation in both 1-shot and 5-\nshot scenarios and the few-shot classiﬁcation accuracies\nare computed by averaging over 600 randomly generated\nepisodes from the test set.\nFrom Table 2, we can see that our model achieved state-\nof-the-art performance on 5-way 1-shot settings and com-\npetitive results on 5-way 5-shot. However, the 1-shot result\nreported by prototypical networks [36] reqired to be trained\non 30-way 15 queries per training episode, and 5-shot re-\nsult was trained on 20-way 15 queries per training episode.\nWhen trained with 5-way 15 query per training episode,\n[36] only got 46.14 ± 0.77% for 1-shot evaluation, clearly\nweaker than ours. In contrast, all our models are trained\non 5-way, 1 query for 1-shot and 5 queries for 5-shot per\ntraining episode, with much less training queries than [36].\n4.2. Zero-shot Recognition\nDatasets and settings\nWe follow two ZSL settings: the\nold setting and the new GBU setting provided by [42] for\ntraining/test splits. Under the old setting, adopted by most\nexisting ZSL works before [42], some of the test classes\nalso appear in the ImageNet 1K classes, which have been\nused to pretrain the image embedding network, thus vio-\nlating the zero-shot assumption. In contrast, the new GBU\nsetting ensures that none of the test classes of the datasets\nappear in the ImageNet 1K classes. Under both settings, the\n\n\nModel\nFine Tune\n5-way Acc.\n20-way Acc.\n1-shot\n5-shot\n1-shot\n5-shot\nMANN [32]\nN\n82.8%\n94.9%\n-\n-\nCONVOLUTIONAL SIAMESE NETS [20]\nN\n96.7%\n98.4%\n88.0%\n96.5%\nCONVOLUTIONAL SIAMESE NETS [20]\nY\n97.3%\n98.4%\n88.1%\n97.0%\nMATCHING NETS [39]\nN\n98.1%\n98.9%\n93.8%\n98.5%\nMATCHING NETS [39]\nY\n97.9%\n98.7%\n93.5%\n98.7%\nSIAMESE NETS WITH MEMORY [18]\nN\n98.4%\n99.6%\n95.0%\n98.6%\nNEURAL STATISTICIAN [8]\nN\n98.1%\n99.5%\n93.2%\n98.1%\nMETA NETS [27]\nN\n99.0%\n-\n97.0%\n-\nPROTOTYPICAL NETS [36]\nN\n98.8%\n99.7%\n96.0%\n98.9%\nMAML [10]\nY\n98.7 ± 0.4%\n99.9 ± 0.1%\n95.8 ± 0.3%\n98.9 ± 0.2%\nRELATION NET\nN\n99.6 ± 0.2%\n99.8± 0.1%\n97.6 ± 0.2%\n99.1± 0.1%\nTable 1: Omniglot few-shot classiﬁcation. Results are accuracies averaged over 1000 test episodes and with 95% conﬁdence intervals\nwhere reported. The best-performing method is highlighted, along with others whose conﬁdence intervals overlap. ‘-’: not reported.\nModel\nFT\n5-way Acc.\n1-shot\n5-shot\nMATCHING NETS [39]\nN\n43.56 ± 0.84%\n55.31 ± 0.73%\nMETA NETS [27]\nN\n49.21 ± 0.96%\n-\nMETA-LEARN LSTM [29]\nN\n43.44 ± 0.77%\n60.60 ± 0.71%\nMAML [10]\nY\n48.70 ± 1.84%\n63.11 ± 0.92%\nPROTOTYPICAL NETS [36]\nN\n49.42 ± 0.78%\n68.20 ± 0.66%\nRELATION NET\nN\n50.44 ± 0.82%\n65.32 ± 0.70%\nTable 2: Few-shot classiﬁcation accuracies on miniImagenet. All\naccuracy results are averaged over 600 test episodes and are re-\nported with 95% conﬁdence intervals, same as [36]. For each task,\nthe best-performing method is highlighted, along with any others\nwhose conﬁdence intervals overlap. ‘-’: not reported.\ntest set can comprise only the unseen class samples (conven-\ntional test set setting) or a mixture of seen and unseen class\nsamples. The latter, termed generalised zero-shot learning\n(GZSL), is more realistic in practice.\nTwo widely used ZSL benchmarks are selected for the\nold setting: AwA (Animals with Attributes) [24] consists\nof 30,745 images of 50 classes of animals. It has a ﬁxed\nsplit for evaluation with 40 training classes and 10 test\nclasses. CUB (Caltech-UCSD Birds-200-2011) [40] con-\ntains 11,788 images of 200 bird species with 150 seen\nclasses and 50 disjoint unseen classes. Three datasets [42]\nare selected for GBU setting: AwA1, AwA2 and CUB. The\nnewly released AwA2 [42] consists of 37,322 images of 50\nclasses which is an extension of AwA while AwA1 is same\nas AwA but under the GBU setting.\nSemantic representation\nFor AwA, we use the contin-\nuous 85-dimension class-level attribute vector from [24],\nwhich has been used by all recent works. For CUB, a con-\ntinuous 312-dimension class-level attribute vector is used.\nImplementation details\nTwo different embedding mod-\nules are used for the two input modalities in zero-shot\nlearning.\nUnless otherwise speciﬁed, we use Inception-\nV2 [38, 17] as the query image embedding DNN in the\nold and conventional setting and ResNet101 [16] for the\nGBU and generalised setting, taking the top pooling units\nas image embedding with dimension D = 1024 and 2048\nrespectively. This DNN is pre-trained on ILSVRC 2012\n1K classiﬁcation without ﬁne-tuning, as in recent deep ZSL\nworks [25, 30, 45]. A MLP network is used for embed-\nding semantic attribute vectors. The size of hidden layer\nFC1 (Figure 3) is set to 1024 and 1200 for AwA and CUB\nrespectively, and the output size FC2 is set to the same di-\nmension as the image embedding for both datasets. For the\nrelation module, the image and semantic embeddings are\nconcatenated before being fed into MLPs with hidden layer\nFC3 size 400 and 1200 for AwA and CUB, respectively.\nWe add weight decay (L2 regularisation) in FC1 & 2 as\nthere is a hubness problem [45] in cross-modal mapping for\nZSL which can be best solved by mapping the semantic fea-\nture vector to the visual feature space with regularisation.\nAfter that, FC3 & 4 (relation module) are used to compute\nthe relation between the semantic representation (in the vi-\nsual feature space) and the visual representation. Since the\nhubness problem does not existing in this step, no L2 regu-\nlarisation/weight decay is needed. All the ZSL models are\ntrained with weight decay 10−5 in the embedding network.\nThe learning rate is initialised to 10−5 with Adam [19] and\nthen annealed by half every 200,000 iterations.\nResults under the old setting\nThe conventional evalua-\ntion for ZSL followed by the majority of prior work is to\nassume that the test data all comes from unseen classes. We\nevaluate this setting ﬁrst. We compare 15 alternative ap-\nproaches in Table 3. With only the attribute vector used as\nthe sample class embedding, our model achieves competi-\ntive result on AwA and state-of-the-art performance on the\nmore challenging CUB dataset, outperforming the most re-\nlated alternative prototypical networks [36] by a big margin.\nNote that only inductive methods are considered. Some re-\n\n\nModel\nF\nSS\nAwA\nCUB\n10-way 0-shot\n50-way 0-shot\nSJE [3]\nFG\nA\n66.7\n50.1\nESZSL [31]\nFG\nA\n76.3\n47.2\nSSE-RELU [46]\nFV\nA\n76.3\n30.4\nJLSE [47]\nFV\nA\n80.5\n42.1\nSYNC-STRUCT [6]\nFG\nA\n72.9\n54.5\nSEC-ML [5]\nFV\nA\n77.3\n43.3\nPROTO. NETS [36]\nFG\nA\n-\n54.6\nDEVISE [11]\nNG\nA/W\n56.7/50.4\n33.5\nSOCHER et al. [37]\nNG\nA/W\n60.8/50.3\n39.6\nMTMDL [43]\nNG\nA/W\n63.7/55.3\n32.3\nBA et al. [25]\nNG\nA/W\n69.3/58.7\n34.0\nDS-SJE [30]\nNG\nA/D\n-\n50.4/ 56.8\nSAE [21]\nNG\nA\n84.7\n61.4\nDEM [45]\nNG\nA/W\n86.7/78.8\n58.3\nRELATION NET\nNG\nA\n84.5\n62.0\nTable 3: Zero-shot classiﬁcation accuracy (%) comparison on AwA and\nCUB (hit@1 accuracy over all samples) under the old and conventional\nsetting. SS: semantic space; A: attribute space; W: semantic word vector\nspace; D: sentence description (only available for CUB). F: how the vi-\nsual feature space is computed; For non-deep models: FO if overfeat [34]\nis used; FG for GoogLeNet [38]; and FV for VGG net [35]. For neu-\nral network based methods, all use Inception-V2 (GoogLeNet with batch\nnormalisation) [38, 17] as the DNN image imbedding subnet, indicated as\nNG.\ncent methods [48, 12, 13] are tranductive in that they use all\ntest data at once for model training, which gives them a big\nadvantage at the cost of making a very strong assumption\nthat may not be met in practical applications, so we do not\ncompare with them here.\nResults under the GBU setting\nWe follow the evalua-\ntion setting of [42]. We compare our model with 11 alterna-\ntive ZSL models in Table 4. The 10 shallow models results\nare from [42] and the result of the state-of-the-art method\nDEM [45] is from the authors’ GitHub page1. We can see\nthat on AwA2 and CUB, Our model is particularly strong\nunder the more realistic GZSL setting measured using the\nharmonic mean (H) metric. While on AwA1, our method is\nonly outperformed by DEM [45].\n5. Why does Relation Network Work?\n5.1. Relationship to existing models\nRelated prior few-shot work uses ﬁxed pre-speciﬁed dis-\ntance metrics such as Euclidean or cosine distance to per-\nform classiﬁcation [39, 36]. These studies can be seen as\ndistance metric learning, but where all the learning occurs in\nthe feature embedding, and a ﬁxed metric is used given the\nlearned embedding. Also related are conventional metric\nlearning approaches [26, 7] that focus on learning a shallow\n(linear) Mahalanobis metric for a ﬁxed feature representa-\n1https://github.com/lzrobots/\nDeepEmbeddingModel_ZSL\n(a) Ground Truth\n(b) Relation Network\n(c) Metric Learning\n(d) Metric + Embedding\nFigure 4: An example relation learnable by Relation Network and\nnot by non-linear embedding + metric learning.\ntion. In contrast to prior work’s ﬁxed metric or ﬁxed fea-\ntures and shallow learned metric, Relation Network can be\nseen as both learning a deep embedding and learning a deep\nnon-linear metric (similarity function)2. These are mutually\ntuned end-to-end to support each other in few short learn-\ning.\nWhy might this be particularly useful? By using a ﬂex-\nible function approximator to learn similarity, we learn a\ngood metric in a data driven way and do not have to man-\nually choose the right metric (Euclidean, cosine, Maha-\nlanobis). Fixed metrics like [39, 36] assume that features\nare solely compared element-wise, and the most related [36]\nassumes linear separability after the embedding. These are\nthus critically dependent on the efﬁcacy of the learned em-\nbedding network, and hence limited by the extent to which\nthe embedding networks generate inadequately discrimina-\ntive representations. In contrast, by deep learning a non-\nlinear similarity metric jointly with the embedding, Relation\nNetwork can better identify matching/mismatching pairs.\n5.2. Visualisation\nTo illustrate the previous point about adequacy of learned\ninput embeddings, we show a synthetic example where ex-\nisting approaches deﬁnitely fail and our Relation Network\ncan succeed due to using a deep relation module. Assuming\n2D query and sample input embeddings to a relation mod-\nule, Fig. 4(a) shows the space of 2D sample inputs for a\nﬁxed 2D query input. Each sample input (pixel) is colored\naccording to whether it matches the ﬁxed query or not. This\n2Our architecture does not guarantee the self-similarity and symmetry\nproperties of a formal similarity function. But empirically we ﬁnd these\nproperties hold numerically for a trained Relation Network.\n\n\nAwA1\nAwA2\nCUB\nZSL\nGZSL\nZSL\nGZSL\nZSL\nGZSL\nModel\nT1\nu\ns\nH\nT1\nu\ns\nH\nT1\nu\ns\nH\nDAP [24]\n44.1\n0.0\n88.7\n0.0\n46.1\n0.0\n84.7\n0.0\n40.0\n1.7\n67.9\n3.3\nCONSE [28]\n45.6\n0.4\n88.6\n0.8\n44.5\n0.5\n90.6\n1.0\n34.3\n1.6\n72.2\n3.1\nSSE [46]\n60.1\n7.0\n80.5\n12.9\n61.0\n8.1\n82.5\n14.8\n43.9\n8.5\n46.9\n14.4\nDEVISE [11]\n54.2\n13.4\n68.7\n22.4\n59.7\n17.1\n74.7\n27.8\n52.0\n23.8\n53.0\n32.8\nSJE [3]\n65.6\n11.3\n74.6\n19.6\n61.9\n8.0\n73.9\n14.4\n53.9\n23.5\n59.2\n33.6\nLATEM [41]\n55.1\n7.3\n71.7\n13.3\n55.8\n11.5\n77.3\n20.0\n49.3\n15.2\n57.3\n24.0\nESZSL [31]\n58.2\n6.6\n75.6\n12.1\n58.6\n5.9\n77.8\n11.0\n53.9\n12.6\n63.8\n21.0\nALE [2]\n59.9\n16.8\n76.1\n27.5\n62.5\n14.0\n81.8\n23.9\n54.9\n23.7\n62.8\n34.4\nSYNC [6]\n54.0\n8.9\n87.3\n16.2\n46.6\n10.0\n90.5\n18.0\n55.6\n11.5\n70.9\n19.8\nSAE [21]\n53.0\n1.8\n77.1\n3.5\n54.1\n1.1\n82.2\n2.2\n33.3\n7.8\n57.9\n29.2\nDEM [45]\n68.4\n32.8\n84.7\n47.3\n67.1\n30.5\n86.4\n45.1\n51.7\n19.6\n54.0\n13.6\nRELATION NET\n68.2\n31.4\n91.3\n46.7\n64.2\n30.0\n93.4\n45.3\n55.6\n38.1\n61.1\n47.0\nTable 4: Comparative results under the GBU setting. Under the conventional ZSL setting, the performance is evaluated using per-class\naverage Top-1 (T1) accuracy (%), and under GZSL, it is measured using u = T1 on unseen classes, s = T1 on seen classes, and H =\nharmonic mean.\nrepresents a case where the output of the embedding mod-\nules is not discriminative enough for trivial (Euclidean NN)\ncomparison between query and sample set. In Fig. 4(c) we\nattempt to learn matching via a Mahalanobis metric learn-\ning relation module, and we can see the result is inadequate.\nIn Fig. 4(d) we learn a further 2-hidden layer MLP embed-\nding of query and sample inputs as well as the subsequent\nMahalanobis metric, which is also not adequate. Only by\nlearning the full deep relation module for similarity can we\nsolve this problem in Fig. 4(b).\nIn a real problem the difﬁculty of comparing embeddings\nmay not be this extreme, but it can still be challenging. We\nqualitatively illustrate the challenge of matching two exam-\nple Omniglot query images (embeddings projected to 2D,\nFigure 5(left)) by showing an analogous plot of real sample\nimages colored by match (cyan) or mismatch (magenta) to\ntwo example queries (yellow). Under standard assumptions\n[39, 36, 26, 7] the cyan matching samples should be near-\nest neighbours to the yellow query image with some metric\n(Euclidean, Cosine, Mahalanobis). But we can see that the\nmatch relation is more complex than this. In Figure 5(right),\nwe instead plot the same two example queries in terms of a\n2D PCA representation of each query-sample pair, as repre-\nsented by the relation module’s penultimate layer. We can\nsee that the relation network has mapped the data into a\nspace where the (mis)matched pairs are linearly separable.\n6. Conclusion\nWe proposed a simple method called the Relation Net-\nwork for few-shot and zero-shot learning. Relation network\nlearns an embedding and a deep non-linear distance metric\nfor comparing query and sample items. Training the net-\nwork end-to-end with episodic training tunes the embedding\nand distance metric for effective few-shot learning. This ap-\nFigure 5: Example Omniglot few-shot problem visualisations.\nLeft: Matched (cyan) and mismatched (magenta) sample embed-\ndings for a given query (yellow) are not straightforward to dif-\nferentiate. Right: Matched (yellow) and mismatched (magenta)\nrelation module pair representations are linearly separable.\nproach is far simpler and more efﬁcient than recent few-shot\nmeta-learning approaches, and produces state-of-the-art re-\nsults. It further proves effective at both conventional and\ngeneralised zero-shot settings.\nAcknowledgements\nThis work was supported by the\nERC grant ERC-2012-AdG 321162-HELIOS, EPSRC\ngrant\nSeebibyte\nEP/M013774/1,\nEPSRC/MURI\ngrant\nEP/N019474/1, EPSRC grant EP/R026173/1, and the\nEuropean Union’s Horizon 2020 research and innovation\nprogram (grant agreement no.\n640891).\nWe grate-\nfully acknowledge the support of NVIDIA Corporation\nwith the donation of the Titan Xp GPU and the ES-\nPRC funded Tier 2 facility, JADE used for this research.\n\n\nReferences\n[1] Pytorch.\nhttps://github.com/pytorch/\npytorch. 4\n[2] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label-\nembedding for image classiﬁcation. TPAMI, 2016. 8\n[3] Z. Akata, S. Reed, D. Walter, H. Lee, and B. Schiele. Eval-\nuation of output embeddings for ﬁne-grained image classiﬁ-\ncation. In CVPR, 2015. 1, 3, 7, 8\n[4] L. Bertinetto, J. F. Henriques, J. Valmadre, P. H. S. Torr,\nand A. Vedaldi. Learning feed-forward one-shot learners.\nIn NIPS, 2016. 1, 2\n[5] M. Bucher, S. Herbin, and F. Jurie. Improving semantic em-\nbedding consistency by metric learning for zero-shot classif-\nﬁcation. In ECCV, 2016. 7\n[6] S. Changpinyo, W.-L. Chao, B. Gong, and F. Sha. Synthe-\nsized classiﬁers for zero-shot learning. In CVPR, 2016. 7,\n8\n[7] D. Chen, X. Cao, L. Wang, F. Wen, and J. Sun. Bayesian face\nrevisited: A joint formulation. In ECCV. Springer Berlin\nHeidelberg, 2012. 7, 8\n[8] H. Edwards and A. Storkey. Towards a neural statistician.\nICLR, 2017. 1, 4, 5, 6\n[9] L. Fei-Fei, R. Fergus, and P. Perona. One-shot learning of\nobject categories. TPAMI, 2006. 1, 2\n[10] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-\nlearning for fast adaptation of deep networks.\nIn ICML,\n2017. 1, 2, 4, 5, 6\n[11] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean,\nT. Mikolov, et al. Devise: A deep visual-semantic embed-\nding model. In NIPS, 2013. 1, 3, 7, 8\n[12] Y. Fu, T. M. Hospedales, T. Xiang, Z. Fu, and S. Gong.\nTransductive multi-view embedding for zero-shot recogni-\ntion and annotation. In ECCV, 2014. 7\n[13] Y. Fu and L. Sigal. Semi-supervised vocabulary-informed\nlearning. In CVPR, 2016. 7\n[14] X. Han, T. Leung, Y. Jia, R. Sukthankar, and A. C. Berg.\nMatchnet: Unifying feature and metric learning for patch-\nbased matching. In CVPR, 2015. 2\n[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In CVPR, 2016. 1, 2\n[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In CVPR, 2016. 6\n[17] S. Ioffe and C. Szegedy. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift. In\nICML, 2015. 6, 7\n[18] Ł. Kaiser, O. Nachum, A. Roy, and S. Bengio. Learning to\nremember rare events. ICLR, 2017. 1, 4, 6\n[19] D. Kingma and J. Ba. Adam: A method for stochastic opti-\nmization. In ICLR, 2015. 4, 6\n[20] G. Koch, R. Zemel, and R. Salakhutdinov. Siamese neural\nnetworks for one-shot image recognition. In ICML Work-\nshop, 2015. 1, 2, 4, 6\n[21] E. Kodirov, T. Xiang, and S. Gong. Semantic autoencoder\nfor zero-shot learning. In CVPR, 2017. 7, 8\n[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImagenet\nclassiﬁcation with deep convolutional neural networks. In\nNIPS, 2012. 1, 2\n[23] B. Lake, R. Salakhutdinov, J. Gross, and J. Tenenbaum. One\nshot learning of simple visual concepts. In CogSci, 2011. 1,\n2, 4\n[24] C. H. Lampert, H. Nickisch, and S. Harmeling. Attribute-\nbased classiﬁcation for zero-shot visual object categoriza-\ntion. PAMI, 2014. 1, 6, 8\n[25] J. Lei Ba, K. Swersky, S. Fidler, and R. Salakhutdinov. Pre-\ndicting deep zero-shot convolutional neural networks using\ntextual descriptions. In ICCV, 2015. 1, 6, 7\n[26] T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka. Metric\nlearning for large scale image classiﬁcation: Generalizing to\nnew classes at near-zero cost. In ECCV, 2012. 7, 8\n[27] T. Munkhdalai and H. Yu. Meta networks. In ICML, 2017.\n1, 2, 4, 5, 6\n[28] M. Norouzi, T. Mikolov, S. Bengio, Y. Singer, J. Shlens,\nA. Frome, G. S. Corrado, and J. Dean. Zero-shot learning\nby convex combination of semantic embeddings. In ICLR,\n2014. 8\n[29] S. Ravi and H. Larochelle. Optimization as a model for few-\nshot learning. In ICLR, 2017. 1, 2, 4, 5, 6\n[30] S. Reed, Z. Akata, B. Schiele, and H. Lee. Learning deep\nrepresentations of ﬁne-grained visual descriptions. In CVPR,\n2016. 6, 7\n[31] B. Romera-Paredes and P. Torr. An embarrassingly simple\napproach to zero-shot learning. In ICML, 2015. 1, 7, 8\n[32] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and\nT. Lillicrap. Meta-learning with memory-augmented neural\nnetworks. In ICML, 2016. 1, 2, 4, 6\n[33] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski,\nR. Pascanu, P. Battaglia, and T. Lillicrap. A simple neural\nnetwork module for relational reasoning. In NIPS, 2017. 2\n[34] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\nand Y. LeCun. Overfeat: Integrated recognition, localization\nand detection using convolutional networks. arXiv preprint\narXiv:1312.6229, 2013. 7\n[35] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. ICLR, 2015. 1,\n2, 7\n[36] J. Snell, K. Swersky, and R. S. Zemel. Prototypical networks\nfor few-shot learning. In NIPS, 2017. 1, 2, 4, 5, 6, 7, 8\n[37] R. Socher, M. Ganjoo, C. D. Manning, and A. Ng. Zero-shot\nlearning through cross-modal transfer. In NIPS, 2013. 7\n[38] C. Szegedy,\nW. Liu,\nY. Jia,\nP. Sermanet,\nS. Reed,\nD. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.\nGoing deeper with convolutions. In CVPR, 2015. 6, 7\n[39] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al.\nMatching networks for one shot learning. In NIPS, 2016.\n1, 2, 3, 4, 5, 6, 7, 8\n[40] C. Wah, S. Branson, P. Perona, and S. Belongie. Multiclass\nrecognition and part localization with humans in the loop. In\nICCV, 2011. 6\n[41] Y. Xian, Z. Akata, G. Sharma, Q. Nguyen, M. Hein, and\nB. Schiele. Latent embeddings for zero-shot classiﬁcation.\nIn CVPR, 2016. 8\n[42] Y. Xian, C. H. Lampert, B. Schiele, and Z. Akata. Zero-\nshot learning-a comprehensive evaluation of the good, the\nbad and the ugly. arXiv preprint arXiv:1707.00600, 2017. 5,\n6, 7\n\n\n[43] Y. Yang and T. M. Hospedales.\nA uniﬁed perspective on\nmulti-domain and multi-task learning. In ICLR, 2015. 3, 7\n[44] S. Zagoruyko and N. Komodakis. Learning to compare im-\nage patches via convolutional neural networks. In CVPR,\n2015. 2\n[45] L. Zhang, T. Xiang, and S. Gong. Learning a deep embed-\nding model for zero-shot learning. In CVPR, 2017. 1, 6, 7,\n8\n[46] Z. Zhang and V. Saligrama. Zero-shot learning via semantic\nsimilarity embedding. In ICCV, 2015. 3, 7, 8\n[47] Z. Zhang and V. Saligrama. Zero-shot learning via joint la-\ntent similarity embedding. In CVPR, 2016. 7\n[48] Z. Zhang and V. Saligrama. Zero-shot recognition via struc-\ntured prediction. In ECCV, 2016. 7\n"
}