{
  "filename": "41598_2022_Article_6718.pdf",
  "num_pages": 11,
  "pages": [
    "1\nVol.:(0123456789)\nScientific Reports |         (2022) 12:2924  \n| https://doi.org/10.1038/s41598-022-06718-2\nwww.nature.com/scientificreports\nAutomated human cell \nclassification in sparse datasets \nusing few‑shot learning\nReece Walsh1*, Mohamed H. Abdelpakey1, Mohamed S. Shehata1 & Mostafa M. Mohamed2\nClassifying and analyzing human cells is a lengthy procedure, often involving a trained professional. In \nan attempt to expedite this process, an active area of research involves automating cell classification \nthrough use of deep learning-based techniques. In practice, a large amount of data is required to \naccurately train these deep learning models. However, due to the sparse human cell datasets currently \navailable, the performance of these models is typically low. This study investigates the feasibility of \nusing few-shot learning-based techniques to mitigate the data requirements for accurate training. \nThe study is comprised of three parts: First, current state-of-the-art few-shot learning techniques are \nevaluated on human cell classification. The selected techniques are trained on a non-medical dataset \nand then tested on two out-of-domain, human cell datasets. The results indicate that, overall, the test \naccuracy of state-of-the-art techniques decreased by at least 30% when transitioning from a non-\nmedical dataset to a medical dataset. Reptile and EPNet were the top performing techniques tested \non the BCCD dataset and HEp-2 dataset respectively. Second, this study evaluates the potential \nbenefits, if any, to varying the backbone architecture and training schemes in current state-of-the-art \nfew-shot learning techniques when used in human cell classification. To this end, the best technique \nidentified in the first part of this study, EPNet, is used for experimentation. In particular, the study \nused 6 different network backbones, 5 data augmentation methodologies, and 2 model training \nschemes. Even with these additions, the overall test accuracy of EPNet decreased from 88.66% on \nnon-medical datasets to 44.13% at best on the medical datasets. Third, this study presents future \ndirections for using few-shot learning in human cell classification. In general, few-shot learning in its \ncurrent state performs poorly on human cell classification. The study proves that attempts to modify \nexisting network architectures are not effective and concludes that future research effort should be \nfocused on improving robustness towards out-of-domain testing using optimization-based or self-\nsupervised few-shot learning techniques.\nVisual analysis of human cells has long served as a steadfast diagnostic tool for a variety of potential ailments. \nExamples of these procedures include blood smear tests used for diagnosis of blood conditions or skin biopsies \nused for discovery of epidermal diseases. Analysis of human cells, however, can be a time consuming task, requir-\ning the attention of a trained professional for significant portions of time. Automated cell counters, or machines \nof a similar likeness, have alleviated some of the less complex, monotonous tasks. Automated classification of \ncomplex cell structures, though, remains a difficult goal due to large variations in cell shape, differing cell-image \ncapturing methodologies, and variance in cell staining protocols. Experienced professionals, however, are capable \nof overcoming these obstacles and identifying a wide variety of human cell types in adverse visual conditions. \nThis raises the question if similar flexible understanding of cells can be instilled within a given model.\nWork within the field of artificial intelligence (AI) has historically struggled to achieve performance similar \nto human perception. Some of the earliest work, such as Strachey’s Draughts ­program1, pushed the capabili-\nties of technology at the time while attempting to employ a learning mechanism to teach the machine about a \ngiven task. Over the years, Reasoning-as-Search2, Expert ­Systems3 and other techniques have been proposed as \nparadigms for enabling intelligent processing in computer programs. Today, however, Backpropagation-based \n­techniques4 are largely favoured, with the majority of AI research employing variations of Stochastic Gradient \nDescent (SGD) as the method from which a particular model learns. This trend, however, is not universal in \nnature, with some domains, such as time series analysis, persisting with use of alternative classification meth-\nods. Recent examples include Bai et al.’s work applying ensemble ­learning5 and Yan et al.’s work on time series \nOPEN\n1Department of Computer Science, Mathematics, Physics and Statistics, University of British Columbia, Kelowna, \nCanada. 2Department of Computer Science, Helwan University, Helwan, Egypt. *email: reece.walsh@ubc.ca\n",
    "2\nVol:.(1234567890)\nScientific Reports |         (2022) 12:2924  | \nhttps://doi.org/10.1038/s41598-022-06718-2\nwww.nature.com/scientificreports/\nsimilarity ­measurement6. Additionally, research into efficient, alternative optimization strategies is also an active \narea of publication. Recent metaheuristic algorithms, such as the I-PKL-CS ­algorithm7, the Dynamic Learning \nEvolution ­algorithm8, Elephant Herding ­Optimization9 (EHO), the Opposition-based Krill Herd ­algorithm10, and \nEHO using dynamic topology and biogeography-based ­optimization11, have demonstrated efficient capabilities \nwhen optimizing towards a given solution, as explored by Li et al.12 in a recent ­survey7,8,12.\nRecent research employing SGD has enabled highly accurate models in certain sub-fields, such as computer \nvision, through use of Backpropagation-enabled Convolutional Neural Networks (CNNs). The first successful \napplication came with AlexNet’s13 breakthrough performance on the ImageNet Large Scale Visual Recognition \nCompetition in 2012. Since then, numerous CNN architectures have been proposed, with notable contributions \nto the field including ­VGGNet14, ­ResNet15, Inception ­V316, and DBN for image ­processing17. Medical image-based \nclassification has specifically benefited from more performant computer vision techniques. Success has been \nfound with use of SGD-based CNNs on a range of image-based medical domains. In recent literature, for example, \nZhang et al.18 propose improved diagnosis of atrophic gastritis through application of ­DenseNet19 and Wang et al. \npropose ­MCNet20 for use in automated lesion segmentation using endoscopy images of the gastrointestinal tract.\nAchieving superhuman performance with today’s models, however, comes with a steep requirement for data. \nThe ILSVRC ImageNet ­dataset21, for instance, contains over 14,000,000 images with roughly 21,000 image classes \nrepresenting everyday things or objects. This dataset size is required in order to enable a performant under-\nstanding of each class. Additionally, the quality of a given dataset can be an issue, with class bias, class balance, \nand data quality all potential performance detractors, if neglected during model training. Even if these dataset \nconsiderations are put aside, modern approaches to AI can typically take multiple days to train on a challeng-\ning dataset. These limitations become particularly stifling when additional classes are considered for use with a \nmodel. Adding a new class to ImageNet requires roughly 600 new images to prevent class imbalance within the \ndataset. For scenarios involving common objects, such as those in ImageNet, obtaining 600 new images can be a \nfairly simple process. This changes, however, if images of the new class are difficult to obtain or existing datasets \nare incredibly shallow, resulting in class imbalance.\nIn an effort to solve the aforementioned issues, research into creating adaptable models for use on sparse \ndatasets has seen active development in recent years. These efforts can generally be categorized into transfer \nlearning and few-shot learning. This study focuses on the use of few-shot learning and its application to human \ncell classification.\nThe general goal of few-shot learning involves accurately performing a task on new data, given only a \nsparse amount of training data. Work in this field using CNNs largely began with Koch et al. proposed Sia-\nmese ­Network22, which demonstrated understanding of a new class given only a single \"shot\" or ground truth \nimage. Vinyals et al. furthered the field a year after with their proposed Matching ­Network23 and additionally \ncontributed the mini-ImageNet dataset, which is used for testing few-shot learning techniques today. Both of \nthe previously mentioned approaches, however, encouraged quick recognition of new data through processes \nexternal to the model itself. Finn et al. proposed Model-Agnostic Meta-Learning24 (MAML) instead approaches \nfew-shot learning as a process by which a model’s weights are directly manipulated. MAML’s optimization-based \napproach enabled a significant jump in few-shot learning performance, with test accuracy improving by over 6% \non mini-Imagenet when compared to Vinyals et al.’s Matching Network. Building off of MAML’s success, Nichol \net al. proposed ­Reptile25, a similar, optimization-based technique, which used a refined weight update strategy \nto achieve a 2% accuracy increase over MAML on mini-ImageNet.\nIn the past 4 years since MAML, state-of-the-art few-shot learning techniques have shifted towards application \nof semi-supervised learning and transductive strategies for better performance. Current networks have improved \nsignificantly on mini-ImageNet, with the recently proposed Simple CNAPS + ­FETI26 demonstrating 90.3%, an \nimprovement of over 27.2% over MAML. Taking into consideration the recent progress within the field of few-\nshot learning, this study investigates whether few-shot learning techniques can be effectively applied to human \ncell classification in situations involving sparse datasets.\nTo summarize, the three main contributions are as follows: \n1.\t This study investigates the use of few-shot learning in human cell classification. Figure 1 provides an illus-\ntrated example of the proposed process. To the best of the author’s knowledge, this study is the first of its \nkind and will provide valuable insights to researchers in this field.\n2.\t This study evaluates the potential benefits, if any, to varying the backbone architecture and training schemes \nin current state-of-the-art few-shot learning techniques when used in human cell classification.\n3.\t This study presents future direction for research in this area based upon the findings from this study.\nThe remainder of this paper is organized as follows: “Methods” section details the few-shot learning tech-\nniques applied and the experimental regiment by which we apply them. “Results” section presents results from \nthe aforementioned experiments. “Discussion” section explores our findings in further detail. “Conclusions and \nfuture work” section details the conclusions drawn from this study and establishes direction for future work \nperformed in this area.\nMethods\nIn the first part of this study, we train nine few-shot learning techniques on mini-ImageNet and evaluate their \nperformance on two selected human cell datasets. This experimental setup allows for the model to train on a \nnon-medical, balanced dataset and test few-shot performance on sparse medical datasets. The techniques used \nin this study were selected as the top nine from a set of notable, state-of-the-art techniques with code publicly \navailable. Figure 2 provides an illustrated overview of the techniques investigated over time.\n",
    "3\nVol.:(0123456789)\nScientific Reports |         (2022) 12:2924  | \nhttps://doi.org/10.1038/s41598-022-06718-2\nwww.nature.com/scientificreports/\nThe second part of this study evaluates the potential benefits, if any, to varying the backbone architecture \nand training schemes in current state-of-the-art few-shot learning techniques. EPNet was selected as the experi-\nmental model due to its competitive few-shot learning performance on both medical datasets and efficient \nimplementation.\nAll training and testing in this paper was performed using an NVIDIA Tesla V100 (with 32 GB of VRAM), \nPyTorch v1.8, and Python 3.8.\nPart 1: investigating existing few‑shot learning techniques. \nMetric‑based few‑shot learning.  Some \nof the earliest work within the field of few-shot learning leveraged metric-based analysis in order to generate \na similarity score between two given samples. At a very general level, application of K-Nearest Neighbors to a \ngiven dataset can be a thought of as a rudimentary, metric-based few-shot learning model. Metric-based models \nconsider input data similarly, clustering unlabelled data (known as the query set) based on information from a \npreviously seen set of labelled data (known as the support set).\nSnell et al.’s Prototypical ­Networks27 serve as notable, performant example of a recent metric-based few-shot \nlearning approach. The methodology proposed establishes use of an embedding function to map a given query \nset and support set to an embedding space. The mean of each class within the support is taken and defined as a \nprototype vector. The squared euclidean distance between a query embedding and all prototype vectors is used \nto generate the final distribution over classes for a given query point.\nFigure 1.   The process proposed for training and testing the nine selected few-shot learning techniques on out-\nof-domain data.\nFigure 2.   A temporal overview of notable few-shot learning techniques proposed within the past 5 years. \n\"Optimization-based\" few-shot learning techniques refer to those proposing changes to optimization processes \nemployed by a network. \"Metric-based\" few-shot learning techniques refer to those proposing a metric from \nwhich a similarity score between a set of samples can be obtained from. \"Augmented Metric-based\" few-shot \nlearning techniques refer to those proposing an augmentation (such as application of a self-supervised or \ntransductive process) to a metric-based few-shot learning technique.\n",
    "4\nVol:.(1234567890)\nScientific Reports |         (2022) 12:2924  | \nhttps://doi.org/10.1038/s41598-022-06718-2\nwww.nature.com/scientificreports/\nOptimization‑based few‑shot learning.  In contrast to metric-based strategies, approaches leveraging optimiza-\ntion-based few-shot learning propose no external metrics by which the model depends on. Instead, a model-\nagnostic approach is taken by defining a general-purpose optimization algorithm compatible with all models \nleveraging Stochastic Gradient Descent-based methods for learning. By applying this algorithm, all potential \nclasses are optimized, rather than continuous optimization towards a single dataset.\nTo enable further exploration and understanding of this strategy, we define a generic model as fθ with param-\neters θ , a generic dataset, D , a learning rate α , and a generic loss function, L . A \"task\" Ti is sampled from a dataset \nD as a grouping of classes. With the defined variables, we can update by a single Stochastic Gradient Descent \niteration using the following equation:\nIn doing so, however, we only compute the loss for a single batch within a single task. Optimization-based \napproaches, such as MAML and Reptile, promote accuracy across all given tasks, rather than a single task. To \nachieve generalization towards new tasks, MAML (and additionally Reptile) proposes an adaptation process \nwhich involves taking multiple gradient descent iterations for each task. The loss of each task is analyzed, enabling \ndiscovery of the optimal θ∗ which optimizes towards all tasks. In essence, Eq (1) is used to take small, iterative \ngradient descent steps for each task, discovering how optimization occurs. Once all tasks have been iterated over, \nθ∗ can be found, enabling us to take a large step in an overall optimal direction. Figure 3 illustrates optimizing \nacross three given tasks. With the above equation, we would have only taken a single step along either ∇L1 , ∇L2 , \nor ∇L3.\nTransductive and self‑supervised approaches to few‑shot learning.  Recent state-of-the-art studies within the \nfield of few-shot learning have demonstrated use of transductive techniques, self-supervised learning, and extra, \nunlabelled data in order to enable accurate performance. Rodriguez et al’s ­EPNet28 follows a transductive few-\nshot learning approach to enable quick uptake of new classes. In contrast, where optimization-based approaches \nleveraged a modified gradient descent algorithm, EPNet maps the support and query sets to an embedding space \nwherein all points are considered simultaneously. During this phase, labels are propagated from the support set \nto similar, unlabelled query set points. Figure 4 illustrates the process of propagation for a given set of points. \nBateni et al.’s proposed Simple ­CNAPS26 follows a similar metric-based clustering, however, a Mahalanobis dis-\ntance is used for comparison between points, rather than propagation of labels. PT+MAP29 and ­LaplacianShot30 \nfunction similarly, however, both propose alternative strategies for distance metrics when considering query and \n(1)\nθ\n′\ni = θ −α∇θLTi(fθ)\nFigure 3.   The optimization-based process for optimizing towards three tasks illustrated.\nFigure 4.   An illustrated example of transductive few-shot learning. (A) Grey circles represent unlabelled points \n(the query set) and coloured circles represent labelled points (the support set). (B) All unlabelled points are \nlabelled based on their position within the labelled data.\n",
    "5\nVol.:(0123456789)\nScientific Reports |         (2022) 12:2924  | \nhttps://doi.org/10.1038/s41598-022-06718-2\nwww.nature.com/scientificreports/\nsupport points. ­AmdimNet31 and ­S2M232, alternatively, leverage self-supervised techniques in order to generate \na stronger embedding-space mapping for input data.\nDataset selection and few‑shot data sampling.  All experiments performed in this study used the mini-ImageNet \nfew-shot ­dataset23 for training purposes. The BCCD White Blood Cell ­Dataset33 and a contrast-adjusted variant \nof the HEp-2 Cell ­dataset34 were used for out-of-domain few-shot testing. The HEp-2 dataset was additionally \nused for training. The aforementioned datasets were sampled from following the procedure defined by Vinyals \net al.23 for few-shot datasets.\nSampling data from the training and testing few-shot datasets was performed in an identical manner. Batches \nsampled from a dataset are defined as \"episodes\" for a given model. These episodes are composed of two data \nsections: a support set and a query set. The support set is comprised of labelled examples meant to teach a given \nmodel about the episode’s classes. Labelled classes, specifically, are sampled in terms of ways and shots. The \nnumber of ways in an episode defines how many classes are sampled, while the number of shots defines how \nmany of each class is sampled. The query set, in contrast, is composed of unlabelled data and serves as what \ncan be considered a miniature \"test set\" for the episode to benchmark how well a given model learned from the \nsupport set. Typically, 10 images per way are sampled from the respective class. All experiments performed in \nthis study use 5-way 5-shot sampling strategies for testing and training. Additionally, due to the sampling-based \nfew-shot datasets lacking a clear end, we define one epoch as being equivalent to sampling 600 episodes from \na given dataset.\nPart 2: evaluating modifications. \nData augmentations and regularization.  We apply data augmenta-\ntion and regularization techniques used in recent classification approaches in an attempt to prevent overfit-\nting on mini-ImageNet and encourage a more general embedding space. Hyperparameters associated with the \nemployed techniques applied in two ways. First, the hyperparameter is set to a static value and the model is left \nto train. Second, we follow the approach detailed in the recent ­EfficientNetV235 training scheme and gradually \nincrease (or decrease) a hyperparameter’s value over a number of epochs.\nRandAugment36.  A series of N random data augmentations (shear, colour jitter, etc) are applied to a batch of \nimages at a magnitude of M. N was set at a static value of 2 (an optimal value defined by Cubuk et al.) for all \nexperiments and M was tested at ranges [5–25], [5–15], [5–10], and a static value of 5.\nMixup37.  During the creation of an episode, sampled images are blended with another random image. The \nblending amount for one image is defined by the alpha hyperparameter. The blending amount for the alternate \nimage is defined as 1 −alpha . Alpha was tested at range [0–0.25], [0–0.1], and at a static value of 0.1.\nLabel ­smoothing38.  One-hot encoded vectors output from a classification model are run through label smooth-\ning before the final loss calculation is performed. In doing so, the one-hot encoded vector is replaced with a \nsmoothed, uniform distribution over the number of classes in the vector. Label smoothing contains a single \nhyperparameter that controls the level of smoothing applied to a given one-hot vector. The hyperparameter was \ntested at a static value of 0.1 (as defined by Muller et al.).\nExponential moving average of model ­weights39.  An exponential moving average is retained of a given model’s \nweights while training is underway. Averaged weights have been shown to perform better in some scenarios \nwhen applied to classification-based tasks.\nArchitectural improvements.  Internal changes to a given model were investigated during experimentation. \nHu et al.’s Squeeze and ­Excitation40 was evaluated due to its flexible implementation and notable performance \nimprovement. This architectural improvement, however, was noted to work best with Residual Networks and, as \nsuch, we only evaluate this architectural change using the ResNet-12 backbone. A hyperparameter R is exposed \nwith use of Squeeze and Excitation, enabling variation in the computational cost of the Squeeze and Excitation \nblock it pertains to. We tested this value at 0.1, 0.25, and 2.0.\nTraining methods.  We adopt two training strategies during experimentation. The first, and more prevalent, \nstrategy involves a single, long training cycle, typically defined as 100 epochs long on mini-ImageNet. At the end \nof this cycle, we expect the model’s loss to have converged. A plateau-based learning rate scheduler is used to \nmanage the learning rate throughout the training process. The second training strategy we employ, involves mir-\nroring the EfficientV2 training setup as part of the few-shot training process. Instead of a single, long cycle, we \nsubstitute with multiple, shorter training cycles (all training the same model), which we define as 80 epochs long. \nFour cycles are performed in total, which results in 320 epochs of training. Over the course of each consecutive \ncycle, the model is exposed to images that gradually become larger, all the while regularization is increased to \nprevent overfitting. An exponential decay learning rate schedule is used over each cycle, ensuring consistent \nperformance.\nNetwork backbone evaluation.  Within the field of few-shot learning, the majority of proposed methodologies \nhave historically leveraged one of three network backbones: a 4CONV network proposed as part of Vinyals’ \nMatching ­Network23, a Wide Residual ­Network41 with depth 28 and width 10, or a Residual ­Network15 with 12 \nlayers. To investigate application of other network backbones, we employ use of three networks: EfficientNetV2-\n",
    "6\nVol:.(1234567890)\nScientific Reports |         (2022) 12:2924  | \nhttps://doi.org/10.1038/s41598-022-06718-2\nwww.nature.com/scientificreports/\nSmall35, ResNet-18, and ­DenseNet42. EfficientNetV2 was selected for its state-of-the-art performance on mod-\nern, image classification datasets. ResNet-18 and DenseNet were selected for their state-of-the-art performance \nin previously proposed few-shot learning techniques.\nResults\nPart 1: investigating existing few‑shot learning techniques. \nModel evaluation.  To aid in compar-\ning each model, Table 1 provides a detailed overview of the respective technical attributes. The column headings \nwithin the table are expanded upon as follows.\nTechnique.  The type or style of few-shot learning applied.\nBackbone.  The backbone network applied by the few-shot learning approach. WRN28-10 refers to the \nWideResNet28-10 model. CONV4 refers to the convolutional model proposed by Vinyals et al. in MAML.\nPreprocessing.  Whether or not input to the network requires preprocessing by a feature extractor network.\nExtra training data.  Whether or not extra unlabelled training data was used to boost performance. Unlabelled \ntraining data typically relates to the support set at hand (for example, additional unlabelled images of cats are \nused if the cat class is in the support set).\nBaseline benchmarks.  Nine few-shot learning techniques were trained and benchmarked using the mini-Ima-\ngeNet training, validation, and testing dataset splits. Training was performed for 100 epochs using a plateau-\nbased learning rate scheduler with a patience of 10. From our observations, 100 epochs were sufficient to reach \nconvergence in all model permutations. Convergence was typically reached around the 60-80 epoch. Other \ntraining settings (optimizer choice, hyperparameter values, etc) were reproduced as defined by author’s of each \nrespective technique. A summary of these settings can be found in Table 2. The test accuracy on mini-ImageNet \nreported for all models was reproduced within margin-of-error. Out-of-domain few-shot testing was performed \nusing BCCD and HEp-2 as a 5-way 5-shot experiment. Performance demonstrated by all models on the HEp-2 \ndataset was within expectations, however, the accuracy exhibited after testing on BCCD suggested potential \nproblems. The issues were identified as problems stemming from input image size. Mini-ImageNet images have \nTable 1.   An overview of the differing details between the models trained and tested.\nModel name\nTechnique\nBackbone\nPreprocessing\nExtra training data\nModel evaluation table\nAmdimNet31\nSelf-supervised Metric\nAmdimNet\nNo\nYes\nEPNet28\nTransductive Metric\nWRN28-10\nNo\nYes\nSimpleCNAPS26\nMetric\nResNet18\nNo\nYes\nPT+MAP29\nMetric\nWRN28-10\nYes\nNo\nLaplacianShot30\nMetric\nWRN28-10\nNo\nNo\nS2M2R32\nSelf-supervised Metric\nWRN28-10\nYes\nNo\nReptile25\nOptimization\nCONV4\nNo\nNo\nMAML24\nOptimization\nCONV4\nNo\nNo\nProtoNet27\nMetric\nCONV4\nNo\nNo\nTable 2.   Parameter details specific to each technique.\nModel\nOptimizer\nMomentum\nWeight decay\nBatch size\nTechnique implementation details\nAmdimNet31\nAdam\n–\n–\n100\nEPNet28\nSGD\n0.9\n0.0005\n128\nSimpleCNAPS26\nAdam\n–\n–\n256\nPT+MAP29\nAdam\n–\n–\n16\nLaplacianShot30\nSGD\n0.9\n0.0001\n128\nS2M2R32\nAdam\n–\n–\n16\nReptile25\nAdam\n–\n–\n5\nMAML24\nAdam\n–\n–\n32\nProtoNet27\nAdam\n–\n–\n5\n",
    "7\nVol.:(0123456789)\nScientific Reports |         (2022) 12:2924  | \nhttps://doi.org/10.1038/s41598-022-06718-2\nwww.nature.com/scientificreports/\na resolution of 84px by 84px while BCCD images have a resolution of 224px by 224px. During initial testing \nexperiments, all input images were resized to 84px by 84px through anti-aliased, local mean downsampling. The \nfeatured white blood cell in a BCCD image that is being classified is not typically a prominent feature within an \ninput image, thus, by resizing, a significant portion of detail was lost, degrading few-shot performance. Global \npooling layers were added to all models, enabling arbitrary input image size and significantly improving BCCD \nfew-shot performance. Table 3 contains all results from the baseline tests run.\nIn an effort to further explore cell image-based few-shot performance, in-domain training and testing was \nperformed using HEp-2 as the training dataset and BCCD as the testing dataset. HEp-2 was selected as the train-\ning dataset due to the larger number of classes present (6 classes) versus BCCD (5 classes). In-domain training \nand testing was performed in the same manner as out-of-domain testing. The top performing techniques from \nout-of-domain testing (Reptile on BCCD and EPNet on HEp-2) were used. Table 4 details the results obtained \nfrom the in-domain tests run.\nPart 2: evaluating modifications. \nBackbone variations.  Three differing styles of network backbone \nwere evaluated in an attempt to further increase few-shot performance on EPNet. We solely train and test on mi-\nni-ImageNet in this instance since high accuracy on mini-ImageNet results in high accuracy on out-of-domain \ndatasets. To evaluate each backbone, EPNet’s original WideResNet backbone was replaced, trained, and tested \nwith EfficientNetV2, ResNet-18, and DenseNet. All selected backbone replacements, however, failed to match or \nsurpass the original WideResNet backbone. This result could likely be due to the relative complexity some of the \nselected backbones exhibited. Table 5 contains a detailed breakdown of the experimental results. DenseNet had \nalready demonstrated application in a recent few-shot learning approach, thus, the closest result being attributed \nto this network is no surprise.\nModel additions.  Various model additions were added to EPNet and benchmarked using mini-ImageNet. All \nadditions were trained for 100 epochs with a plateau-based learning rate schedule. Table 6 contains the full list \nof addition evaluation results. Generally, all proposed additions had a negative impact on EPNet during train-\ning. Some additions decreased accuracy by a couple percent while others drained accuracy by a large amount.\nDiscussion\nAnalyzing and classifying human cells (such as in blood smears or skin biopsies) is an intensive task requiring \nspecialized equipment and oversight from a trained professional. With recent progress in computer vision per-\nformance, however, automated image-based analysis of human cells has been an active area of research. Modern \ndeep learning-based approaches have specifically enabled superhuman performance in a wide array of fields. \nApplication of deep learning to medical scenarios, however, has typically stagnated due to dataset size require-\nments. A potential solution to these issues lies within the field of few-shot learning, an area of research concerned \nwith building performant networks using sparse amounts of data. Recent few-shot learning-based approaches \nhave demonstrated increasingly accurate performance on complex dataset, such as mini-ImageNet. In this study, \nwe investigated whether few-shot learning-based techniques could mitigate the data requirements necessary for \nTable 3.   Test accuracy results from baseline experiments run against the mini-ImageNet test set, BCCD, and \nHEp-2. Testing using the BCCD dataset was performed using additional global pooling layers. The highest \naccuracy relative to each dataset is in bold.\nModel\nMini-ImageNet\nBCCD\nHEp-2\nInitial dataset test performance\nAmdimNet31\n89.75 ± 0.12\n48.35 ± 0.18\n54.32 ± 0.21\nEPNet28\n88.66 ± 0.24\n47.39 ± 0.22\n55.12 ± 0.13\nSimpleCNAPS26\n90.11 ± 0.17\n47.06 ± 0.72\n53.15 ± 0.84\nPT+MAP29\n88.02 ± 0.13\n42.94 ± 0.17\n54.73 ± 0.22\nLaplacianShot30\n82.27 ± 0.15\n34.75 ± 0.13\n44.69 ± 0.17\nS2M2R32\n82.81 ± 0.31\n44.15 ±0.23\n54.41 ± 0.27\nReptile25\n65.62 ± 0.28\n50.91 ± 0.12\n51.76 ± 0.13\nMAML24\n64.62 ± 0.19\n42.81 ± 0.21\n45.21 ± 0.24\nProtoNet27\n67.88 ± 0.12\n46.89 ± 0.13\n50.70 ± 0.17\nTable 4.   Test accuracy results from in-domain training on HEp-2 and testing on BCCD.\nModel\nBCCD\nIn-domain performance\nEPNet28\n45.31 ± 0.21\nReptile25\n40.24 ± 0.23\n",
    "8\nVol:.(1234567890)\nScientific Reports |         (2022) 12:2924  | \nhttps://doi.org/10.1038/s41598-022-06718-2\nwww.nature.com/scientificreports/\nperformant deep learning-based cell classification. An optimal scenario, in this regard, would involve a selected \nfew-shot approach training on a non-medical dataset and accurately testing on a sparse medical dataset. Suc-\ncessful application of a few-shot technique to sparse medical data would drastically expedite existing workflows, \npotentially allowing automation of tasks typically allocated to trained professionals.\nTo facilitate this study, we selected mini-ImageNet23, a popular benchmark for few-shot learning techniques, \nas the dataset by which we would train on. For human cell-based evaluation, we selected the BCCD ­Dataset33 \n(BCCD) and the HEp-2 ­Dataset34 as the testing datasets. Our experimental process involved training few-shot \napproaches on mini-ImageNet and testing the resulting models on the BCCD dataset and the HEp-2 dataset. In \ndoing so, we benchmarked embeddings learned from a non-medical dataset on human cell-based classification. \nWe selected 9 notable, few-shot learning models proposed over the past 5 for use in our experiment. Each model \nwas implemented using the authors’ code (if available) and trained from scratch. Before testing, each model’s \nperformance on mini-ImageNet was verified against the original reported results (within margin-of-error).\nAfter completing experimentation, a decrease in accuracy of at least 30% was noted when transitioning from \nthe training dataset to an out-of-domain human cell dataset. In a rather shocking result, however, Reptile, a \nrelatively old technique, out-performed all newer few-shot learning approaches on the BCCD dataset and per-\nformed competitively on the HEp-2 dataset. MAML, a similar technique, also performed competitively on the \nout-of-domain testing datasets, beating a few newer approaches as well. These results potentially indicate that \nrelatively high performance on mini-ImageNet (and other few-shot benchmarks) does not necessarily guarantee \nproportional performance on out-of-domain tests. Reptile and MAML’s optimization-based strategy for fast \nadaption to new classes could also lead to further performance in out-of-domain tests.\nOverall, performance degradation on the selected medical datasets can largely be attributed to difficulties \ntransitioning from a non-medical domain to a medical domain. Severe out-of-domain accuracy decreases in \nfew-shot learning are corroborated by Bateni et al. in their experimentation with ­SimpleCNAPS26. Decreases as \nlarge as 20% were noted for out-of-domain images within the same dataset. This decrease, however, is a signifi-\ncant issue when rigorous standards for medical practice are taken into account. Models aiming for deployment \nin medicinal scenarios typically demonstrate high accuracy in their field of application. Even with proven and \naccurate capabilities, results produced by a model in a medical setting are still rigorously reviewed. Introducing \none of the current few-shot learning approaches investigated in this study could potentially lead to incorrect \noutput or, at worst, misdiagnosis for a patient.\nTable 6.   Test accuracy results using different model additions within EPNet. Each model addition was \nindependently trained on the mini-ImageNet training set and tested on the mini-ImageNet test set, BCCD, and \nHEp-2.\nModel addition\nMini-ImageNet\nBCCD (%)\nHEp-2\nModel addition performance\nNo additions\n88.7%\n47.4\n55.12%\nRandAugment (Magnitude = 5–15)\n75.8%\n34.6\n42.1%\nRandAugment (Magnitude = 5–10)\n69.3%\n27.8\n35.6%\nRandAugment (Magnitude = 5)\n70.1%\n28.9\n36.1%\nSqueeze and excitation (Reduction = 0.10)\n68.7%\n27.6\n35.2\nSqueeze and excitation (Reduction = 0.25)\n68.2%\n26.2\n34.9%\nSqueeze and excitation (Reduction = 2)\n63.9%\n22.4\n30.5%\nMixup (Alpha = 0.10)\n76.2%\n34.7\n42.8%\nLabel smoothing (A = 0.10)\n65.3\n23.2\n31.4%\nExponential moving average\n78.6%\n38.2\n44.1%\nTable 5.   Test accuracy results from using different backbone variations in EPNet on and testing on mini-\nImageNet, BCCD, and HEp-2. Each backbone was trained on mini-ImageNet’s training set before testing.\nBackbone (%)\nMini-ImageNet (%)\nBCCD\nHEp-2 (%)\nBackbone performance\nWideResNet28-10 (Original Backbone)\n88.7\n47.4\n55.1\nEfficientNetV2 (Default Width)\n59.8\n18.3\n26.3\nEfficientNetV2 (0.5 Width)\n67.3\n25.7\n33.3\nEfficientNetV2 (0.75 Width)\n69.2\n28.0\n35.7\nEfficientNetV2 (2.75 Width)\n70.8\n29.5\n37.1\nResNet-18\n68.2\n26.8\n34.7\nDenseNet\n78.8\n37.4\n45.0\n",
    "9\nVol.:(0123456789)\nScientific Reports |         (2022) 12:2924  | \nhttps://doi.org/10.1038/s41598-022-06718-2\nwww.nature.com/scientificreports/\nIn an attempt to boost few-shot performance, a variety of architectural revisions, data augmentation \napproaches, and training schemes were experimented with and benchmarked using mini-ImageNet. EPNet \nwas selected for this experimentation due to its ease of implementation and performance. Through this pro-\ncess, we discovered that recent measures taken to improve classification networks are ineffective on few-shot \nlearning-based networks. For example, EPNet’s backbone network was swapped and trained across a selection \nof performant, state-of-the-art classification backbones. ­EfficientNetV235, a network that recently achieved state-\nof-the-art accuracy on ImageNet, results in an accuracy decrease of at least 12% when combined with EPNet. \nSimilar, performance-boosting classification techniques, such as data regularization, resulted in performance \nregressions. After an exhaustive exploration of the aforementioned techniques, we concluded that the employed \nfew-shot learning technique should be the main focus for performance-based changes.\nThe methods applied in this work largely focus on a single training dataset and two, cell-based out-of-domain \ntesting datasets. Other, more rigorous few-shot learning evaluation frameworks, such as Triantafillou et al.’s \nMeta-Dataset43, employ use of multi-dataset strategies to gain a clearer understanding of a model’s performance. \nFurthermore, the few-shot learning techniques used in this study are selected within a limited window of time \n(5 years).\nConclusions and future work\nIn this study, we investigate the use of few-shot learning in human cell classification. During the performed train-\ning and testing, a variety of backbone architectures and training schemes were benchmarked for any potential \nbenefit. Although all tested techniques performed well when classifying unseen training data, significant perfor-\nmance decreases were observed when transitioning to either of the two human cell classification testing datasets. \nWith this in mind, we believe that few-shot learning techniques are still limited in the scope of problems they \ncan solve. Support for new techniques less \"brittle to [the] narrow domains they were trained on\" was recently \nhighlighted by Turing award winners Bengio, LeCun, and ­Hinton44. As such, we posit that a stronger emphasis \non out-of-domain robustness should be one of the main directions for future few-shot learning research.\nIn conclusion, few-shot learning methodologies are not yet capable of accurately performing out-of-domain \nclassification at a level accurate enough for human cell identification. We test this conclusion across a selection of \nnotable few-shot learning models proposed within the last 5 years. After training on mini-ImageNet and testing \non the BCCD and HEp-2 datasets, performance was found to drop by at least 30% after transitioning from the \nnon-medical dataset to the selected medical datasets. With this in mind, application of current few-shot learning \nmethodology to medical scenarios is, at this time, insufficient.\nTo facilitate better out-of-domain performance in few-shot learning, new areas of exploration are necessary. \nMAML and Reptile’s surprisingly competitive out-of-domain performance underscores a need to reconsider \nolder few-shot learning techniques. With this in mind, future few-shot learning research should reevaluate \noptimization strategies or focus on more flexible few-shot distance metrics. Metaheuristic algorithms, such as \nMonarch Butterfly ­Optimization45, the Earthworm Optimization ­Algorithm46, Elephant Herding ­Optimization47, \nthe Moth Search ­algorithm48, the Slime Mould ­algorithm49, and Harris Hawks ­optimization50, serve as possible \ndirections for improvement in optimization-based techniques. In future work, we plan on revisiting this area of \nresearch and investigating a wider variety of few-shot learning approaches across a more comprehensive set of \ndatasets (in and out-of-domain).\nReceived: 14 July 2021; Accepted: 31 January 2022\nReferences\n\t 1.\t Link, D. Programming enter: Christopher strachey’s draughts program. Comput. Resurrection. Bull. Comput. Conserv. Soc. 60, \n23–31 (2012).\n\t 2.\t McCorduck, P. & Cfe, C. Machines Who Think: A Personal Inquiry into the History and Prospects of Artificial Intelligence (CRC \nPress, 2004).\n\t 3.\t Jackson, P. Introduction to Expert Systems (Addison-Wesley Longman Publishing Co. Inc, 1998).\n\t 4.\t Rumelhart, D. E., Hinton, G. E. & Williams, R. J. Learning representations by back-propagating errors. Nature 323, 533–536 (1986).\n\t 5.\t Bai, B., Li, G., Wang, S., Wu, Z. & Yan, W. Time series classification based on multi-feature dictionary representation and ensemble \nlearning. Exp. Syst. Appl. 169, 114162 (2021).\n\t 6.\t Yan, W., Li, G., Wu, Z., Wang, S. & Yu, P. S. Extracting diverse-shapelets for early classification on time series. World Wide Web 23, \n3055–3081 (2020).\n\t 7.\t Li, J., Li, Y.-X., Tian, S.-S. & Xia, J.-L. An improved cuckoo search algorithm with self-adaptive knowledge learning. Neural Comput. \nAppl. 32, 11967–11997 (2020).\n\t 8.\t Li, G., Wang, G.-G., Dong, J., Yeh, W.-C. & Li, K. Dlea: A dynamic learning evolution algorithm for many-objective optimization. \nInf. Sci. 574, 567–589 (2021).\n\t 9.\t Li, W., Wang, G.-G. & Alavi, A. H. Learning-based elephant herding optimization algorithm for solving numerical optimization \nproblems. Knowl.-Based Syst. 195, 105675 (2020).\n\t10.\t Wang, G.-G., Deb, S., Gandomi, A. H. & Alavi, A. H. Opposition-based krill herd algorithm with cauchy mutation and position \nclamping. Neurocomputing 177, 147–157 (2016).\n\t11.\t Li, W. & Wang, G.-G. Elephant herding optimization using dynamic topology and biogeography-based optimization based on \nlearning for numerical optimization. Eng. Comput. 1–29 (2021).\n\t12.\t Li, W., Wang, G.-G. & Gandomi, A. H. A survey of learning-based intelligent optimization algorithms. Arch. Comput. Methods \nEng. 28, 1–19 (2021).\n\t13.\t Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep convolutional neural networks. Commun. ACM 60, \n84–90 (2017).\n\t14.\t Simonyan, K. & Zisserman, A. Very deep convolutional networks for large-scale image recognition. Preprint arXiv:​1409.​1556 \n(2014).\n",
    "10\nVol:.(1234567890)\nScientific Reports |         (2022) 12:2924  | \nhttps://doi.org/10.1038/s41598-022-06718-2\nwww.nature.com/scientificreports/\n\t15.\t He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer \nVision and Pattern Recognition, 770–778 (2016).\n\t16.\t Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J. & Wojna, Z. Rethinking the inception architecture for computer vision. In Proceed‑\nings of the IEEE Conference on Computer Vision and Pattern Recognition, 2818–2826 (2016).\n\t17.\t Ying, C., Huang, Z. & Ying, C. Accelerating the image processing by the optimization strategy for deep learning algorithm dbn. \nEURASIP J. Wirel. Commun. Netw. 2018, 1–8 (2018).\n\t18.\t Zhang, J., Yu, J., Fu, S. & Tian, X. Adoption value of deep learning and serological indicators in the screening of atrophic gastritis \nbased on artificial intelligence. J. Supercomput. 77, 1–20 (2021).\n\t19.\t Iandola, F. et al. Densenet: Implementing efficient convnet descriptor pyramids. Preprint arXiv:​1404.​1869 (2014).\n\t20.\t Wang, S. et al. Multi-scale context-guided deep network for automated lesion segmentation with endoscopy images of gastroin-\ntestinal tract. IEEE J. Biomed. Health Inform. 25, 514–525 (2020).\n\t21.\t Russakovsky, O. et al. Imagenet large scale visual recognition challenge. Int. J. Comput. Vis. 115, 211–252 (2015).\n\t22.\t Koch, G., Zemel, R. & Salakhutdinov, R. Siamese neural networks for one-shot image recognition. In ICML Deep Learning Work‑\nshop, vol. 2 (Lille, 2015).\n\t23.\t Vinyals, O. et al. Matching networks for one shot learning. Adv. Neural Inf. Process. Syst. 29, 3630–3638 (2016).\n\t24.\t Finn, C., Abbeel, P. & Levine, S. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference \non Machine Learning, 1126–1135 (PMLR, 2017).\n\t25.\t Nichol, A., Achiam, J. & Schulman, J. On first-order meta-learning algorithms. Preprint arXiv:​1803.​02999 (2018).\n\t26.\t Bateni, P., Goyal, R., Masrani, V., Wood, F. & Sigal, L. Improved few-shot visual classification. In Proceedings of the IEEE/CVF \nConference on Computer Vision and Pattern Recognition (CVPR) (2020).\n\t27.\t Snell, J., Swersky, K. & Zemel, R. S. Prototypical networks for few-shot learning. Preprint arXiv:​1703.​05175 (2017).\n\t28.\t Rodríguez, P., Laradji, I., Drouin, A. & Lacoste, A. Embedding propagation: Smoother manifold for few-shot classification. In \nEuropean Conference on Computer Vision, 121–138 (Springer, 2020).\n\t29.\t Hu, Y., Gripon, V. & Pateux, S. Leveraging the feature distribution in transfer-based few-shot learning. Preprint arXiv:​2006.​03806 \n(2020).\n\t30.\t Ziko, I., Dolz, J., Granger, E. & Ayed, I. B. Laplacian regularized few-shot learning. In International Conference on Machine Learn‑\ning, 11660–11670 (PMLR, 2020).\n\t31.\t Chen, D. et al. Self-supervised learning for few-shot image classification. In ICASSP 2021-2021 IEEE International Conference on \nAcoustics, Speech and Signal Processing (ICASSP), 1745–1749 (IEEE, 2021).\n\t32.\t Mangla, P. et al. Charting the right manifold: Manifold mixup for few-shot learning. In Proceedings of the IEEE/CVF Winter Con‑\nference on Applications of Computer Vision, 2218–2227 (2020).\n\t33.\t Bccd dataset. https://​github.​com/​Sheng​gan/​BCCD_​Datas​et (2019).\n\t34.\t Larsen, A. B. L., Vestergaard, J. S. & Larsen, R. Hep-2 cell classification using shape index histograms with donut-shaped spatial \npooling. IEEE Trans. Med. Imaging 33, 1573–1580 (2014).\n\t35.\t Tan, M. & Le, Q. V. Efficientnetv2: Smaller models and faster training. Preprint arXiv:​2104.​00298 (2021).\n\t36.\t Cubuk, E. D., Zoph, B., Shlens, J. & Le, Q. V. Randaugment: Practical automated data augmentation with a reduced search space. \nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 702–703 (2020).\n\t37.\t Zhang, H., Cisse, M., Dauphin, Y. N. & Lopez-Paz, D. mixup: Beyond empirical risk minimization. Preprint arXiv:​1710.​09412 \n(2017).\n\t38.\t Müller, R., Kornblith, S. & Hinton, G. When does label smoothing help? Preprint arXiv:​1906.​02629 (2019).\n\t39.\t Kingma, D. P. & Ba, J. Adam: A method for stochastic optimization. Preprint arXiv:​1412.​6980 (2014).\n\t40.\t Hu, J., Shen, L. & Sun, G. Squeeze-and-excitation networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern \nRecognition, 7132–7141 (2018).\n\t41.\t Zagoruyko, S. & Komodakis, N. Wide residual networks. Preprint arXiv:​1605.​07146 (2016).\n\t42.\t Lichtenstein, M., Sattigeri, P., Feris, R., Giryes, R. & Karlinsky, L. Tafssl: Task-adaptive feature sub-space learning for few-shot \nclassification. In European Conference on Computer Vision, 522–539 (Springer, 2020).\n\t43.\t Triantafillou, E. et al. Meta-dataset: A dataset of datasets for learning to learn from few examples. Preprint arXiv:​1903.​03096 (2019).\n\t44.\t Bengio, Y., Lecun, Y. & Hinton, G. Deep learning for ai. Commun. ACM 64, 58–65. https://​doi.​org/​10.​1145/​34482​50 (2021).\n\t45.\t Wang, G.-G., Deb, S. & Cui, Z. Monarch butterfly optimization. Neural Comput. Appl. 31, 1995–2014 (2019).\n\t46.\t Wang, G.-G., Deb, S. & Coelho, L. D. S. Earthworm optimisation algorithm: A bio-inspired metaheuristic algorithm for global \noptimisation problems. Int. J. Bio-inspired Comput. 12, 1–22 (2018).\n\t47.\t Wang, G.-G., Deb, S. & Coelho, L. d. S. Elephant herding optimization. In 2015 3rd International Symposium on Computational \nand Business Intelligence (ISCBI), 1–5 (IEEE, 2015).\n\t48.\t Wang, G.-G. Moth search algorithm: A bio-inspired metaheuristic algorithm for global optimization problems. Memetic Comput. \n10, 151–164 (2018).\n\t49.\t Li, S., Chen, H., Wang, M., Heidari, A. A. & Mirjalili, S. Slime mould algorithm: A new method for stochastic optimization. Fut. \nGen. Comput. Syst. 111, 300–323 (2020).\n\t50.\t Heidari, A. A. et al. Harris hawks optimization: Algorithm and applications. Fut. Gen. Comput. Syst. 97, 849–872 (2019).\nAcknowledgements\nFunding for this project was provided through the MITACS Accelerate Grant (IT20377).\nAuthor contributions\nR.W., M.H.A., M.S.S., and M.M.M. conceived the experiments, R.W. conducted the experiments, R.W., M.H.A., \nM.S.S., and M.M.M. analysed the results. All authors reviewed the manuscript.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to R.W.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note  Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\n",
    "11\nVol.:(0123456789)\nScientific Reports |         (2022) 12:2924  | \nhttps://doi.org/10.1038/s41598-022-06718-2\nwww.nature.com/scientificreports/\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http://​creat​iveco​mmons.​org/​licen​ses/​by/4.​0/.\n© The Author(s) 2022\n"
  ],
  "full_text": "1\nVol.:(0123456789)\nScientific Reports |         (2022) 12:2924  \n| https://doi.org/10.1038/s41598-022-06718-2\nwww.nature.com/scientificreports\nAutomated human cell \nclassification in sparse datasets \nusing few‑shot learning\nReece Walsh1*, Mohamed H. Abdelpakey1, Mohamed S. Shehata1 & Mostafa M. Mohamed2\nClassifying and analyzing human cells is a lengthy procedure, often involving a trained professional. In \nan attempt to expedite this process, an active area of research involves automating cell classification \nthrough use of deep learning-based techniques. In practice, a large amount of data is required to \naccurately train these deep learning models. However, due to the sparse human cell datasets currently \navailable, the performance of these models is typically low. This study investigates the feasibility of \nusing few-shot learning-based techniques to mitigate the data requirements for accurate training. \nThe study is comprised of three parts: First, current state-of-the-art few-shot learning techniques are \nevaluated on human cell classification. The selected techniques are trained on a non-medical dataset \nand then tested on two out-of-domain, human cell datasets. The results indicate that, overall, the test \naccuracy of state-of-the-art techniques decreased by at least 30% when transitioning from a non-\nmedical dataset to a medical dataset. Reptile and EPNet were the top performing techniques tested \non the BCCD dataset and HEp-2 dataset respectively. Second, this study evaluates the potential \nbenefits, if any, to varying the backbone architecture and training schemes in current state-of-the-art \nfew-shot learning techniques when used in human cell classification. To this end, the best technique \nidentified in the first part of this study, EPNet, is used for experimentation. In particular, the study \nused 6 different network backbones, 5 data augmentation methodologies, and 2 model training \nschemes. Even with these additions, the overall test accuracy of EPNet decreased from 88.66% on \nnon-medical datasets to 44.13% at best on the medical datasets. Third, this study presents future \ndirections for using few-shot learning in human cell classification. In general, few-shot learning in its \ncurrent state performs poorly on human cell classification. The study proves that attempts to modify \nexisting network architectures are not effective and concludes that future research effort should be \nfocused on improving robustness towards out-of-domain testing using optimization-based or self-\nsupervised few-shot learning techniques.\nVisual analysis of human cells has long served as a steadfast diagnostic tool for a variety of potential ailments. \nExamples of these procedures include blood smear tests used for diagnosis of blood conditions or skin biopsies \nused for discovery of epidermal diseases. Analysis of human cells, however, can be a time consuming task, requir-\ning the attention of a trained professional for significant portions of time. Automated cell counters, or machines \nof a similar likeness, have alleviated some of the less complex, monotonous tasks. Automated classification of \ncomplex cell structures, though, remains a difficult goal due to large variations in cell shape, differing cell-image \ncapturing methodologies, and variance in cell staining protocols. Experienced professionals, however, are capable \nof overcoming these obstacles and identifying a wide variety of human cell types in adverse visual conditions. \nThis raises the question if similar flexible understanding of cells can be instilled within a given model.\nWork within the field of artificial intelligence (AI) has historically struggled to achieve performance similar \nto human perception. Some of the earliest work, such as Strachey’s Draughts ­program1, pushed the capabili-\nties of technology at the time while attempting to employ a learning mechanism to teach the machine about a \ngiven task. Over the years, Reasoning-as-Search2, Expert ­Systems3 and other techniques have been proposed as \nparadigms for enabling intelligent processing in computer programs. Today, however, Backpropagation-based \n­techniques4 are largely favoured, with the majority of AI research employing variations of Stochastic Gradient \nDescent (SGD) as the method from which a particular model learns. This trend, however, is not universal in \nnature, with some domains, such as time series analysis, persisting with use of alternative classification meth-\nods. Recent examples include Bai et al.’s work applying ensemble ­learning5 and Yan et al.’s work on time series \nOPEN\n1Department of Computer Science, Mathematics, Physics and Statistics, University of British Columbia, Kelowna, \nCanada. 2Department of Computer Science, Helwan University, Helwan, Egypt. *email: reece.walsh@ubc.ca\n\n\n2\nVol:.(1234567890)\nScientific Reports |         (2022) 12:2924  | \nhttps://doi.org/10.1038/s41598-022-06718-2\nwww.nature.com/scientificreports/\nsimilarity ­measurement6. Additionally, research into efficient, alternative optimization strategies is also an active \narea of publication. Recent metaheuristic algorithms, such as the I-PKL-CS ­algorithm7, the Dynamic Learning \nEvolution ­algorithm8, Elephant Herding ­Optimization9 (EHO), the Opposition-based Krill Herd ­algorithm10, and \nEHO using dynamic topology and biogeography-based ­optimization11, have demonstrated efficient capabilities \nwhen optimizing towards a given solution, as explored by Li et al.12 in a recent ­survey7,8,12.\nRecent research employing SGD has enabled highly accurate models in certain sub-fields, such as computer \nvision, through use of Backpropagation-enabled Convolutional Neural Networks (CNNs). The first successful \napplication came with AlexNet’s13 breakthrough performance on the ImageNet Large Scale Visual Recognition \nCompetition in 2012. Since then, numerous CNN architectures have been proposed, with notable contributions \nto the field including ­VGGNet14, ­ResNet15, Inception ­V316, and DBN for image ­processing17. Medical image-based \nclassification has specifically benefited from more performant computer vision techniques. Success has been \nfound with use of SGD-based CNNs on a range of image-based medical domains. In recent literature, for example, \nZhang et al.18 propose improved diagnosis of atrophic gastritis through application of ­DenseNet19 and Wang et al. \npropose ­MCNet20 for use in automated lesion segmentation using endoscopy images of the gastrointestinal tract.\nAchieving superhuman performance with today’s models, however, comes with a steep requirement for data. \nThe ILSVRC ImageNet ­dataset21, for instance, contains over 14,000,000 images with roughly 21,000 image classes \nrepresenting everyday things or objects. This dataset size is required in order to enable a performant under-\nstanding of each class. Additionally, the quality of a given dataset can be an issue, with class bias, class balance, \nand data quality all potential performance detractors, if neglected during model training. Even if these dataset \nconsiderations are put aside, modern approaches to AI can typically take multiple days to train on a challeng-\ning dataset. These limitations become particularly stifling when additional classes are considered for use with a \nmodel. Adding a new class to ImageNet requires roughly 600 new images to prevent class imbalance within the \ndataset. For scenarios involving common objects, such as those in ImageNet, obtaining 600 new images can be a \nfairly simple process. This changes, however, if images of the new class are difficult to obtain or existing datasets \nare incredibly shallow, resulting in class imbalance.\nIn an effort to solve the aforementioned issues, research into creating adaptable models for use on sparse \ndatasets has seen active development in recent years. These efforts can generally be categorized into transfer \nlearning and few-shot learning. This study focuses on the use of few-shot learning and its application to human \ncell classification.\nThe general goal of few-shot learning involves accurately performing a task on new data, given only a \nsparse amount of training data. Work in this field using CNNs largely began with Koch et al. proposed Sia-\nmese ­Network22, which demonstrated understanding of a new class given only a single \"shot\" or ground truth \nimage. Vinyals et al. furthered the field a year after with their proposed Matching ­Network23 and additionally \ncontributed the mini-ImageNet dataset, which is used for testing few-shot learning techniques today. Both of \nthe previously mentioned approaches, however, encouraged quick recognition of new data through processes \nexternal to the model itself. Finn et al. proposed Model-Agnostic Meta-Learning24 (MAML) instead approaches \nfew-shot learning as a process by which a model’s weights are directly manipulated. MAML’s optimization-based \napproach enabled a significant jump in few-shot learning performance, with test accuracy improving by over 6% \non mini-Imagenet when compared to Vinyals et al.’s Matching Network. Building off of MAML’s success, Nichol \net al. proposed ­Reptile25, a similar, optimization-based technique, which used a refined weight update strategy \nto achieve a 2% accuracy increase over MAML on mini-ImageNet.\nIn the past 4 years since MAML, state-of-the-art few-shot learning techniques have shifted towards application \nof semi-supervised learning and transductive strategies for better performance. Current networks have improved \nsignificantly on mini-ImageNet, with the recently proposed Simple CNAPS + ­FETI26 demonstrating 90.3%, an \nimprovement of over 27.2% over MAML. Taking into consideration the recent progress within the field of few-\nshot learning, this study investigates whether few-shot learning techniques can be effectively applied to human \ncell classification in situations involving sparse datasets.\nTo summarize, the three main contributions are as follows: \n1.\t This study investigates the use of few-shot learning in human cell classification. Figure 1 provides an illus-\ntrated example of the proposed process. To the best of the author’s knowledge, this study is the first of its \nkind and will provide valuable insights to researchers in this field.\n2.\t This study evaluates the potential benefits, if any, to varying the backbone architecture and training schemes \nin current state-of-the-art few-shot learning techniques when used in human cell classification.\n3.\t This study presents future direction for research in this area based upon the findings from this study.\nThe remainder of this paper is organized as follows: “Methods” section details the few-shot learning tech-\nniques applied and the experimental regiment by which we apply them. “Results” section presents results from \nthe aforementioned experiments. “Discussion” section explores our findings in further detail. “Conclusions and \nfuture work” section details the conclusions drawn from this study and establishes direction for future work \nperformed in this area.\nMethods\nIn the first part of this study, we train nine few-shot learning techniques on mini-ImageNet and evaluate their \nperformance on two selected human cell datasets. This experimental setup allows for the model to train on a \nnon-medical, balanced dataset and test few-shot performance on sparse medical datasets. The techniques used \nin this study were selected as the top nine from a set of notable, state-of-the-art techniques with code publicly \navailable. Figure 2 provides an illustrated overview of the techniques investigated over time.\n\n\n3\nVol.:(0123456789)\nScientific Reports |         (2022) 12:2924  | \nhttps://doi.org/10.1038/s41598-022-06718-2\nwww.nature.com/scientificreports/\nThe second part of this study evaluates the potential benefits, if any, to varying the backbone architecture \nand training schemes in current state-of-the-art few-shot learning techniques. EPNet was selected as the experi-\nmental model due to its competitive few-shot learning performance on both medical datasets and efficient \nimplementation.\nAll training and testing in this paper was performed using an NVIDIA Tesla V100 (with 32 GB of VRAM), \nPyTorch v1.8, and Python 3.8.\nPart 1: investigating existing few‑shot learning techniques. \nMetric‑based few‑shot learning.  Some \nof the earliest work within the field of few-shot learning leveraged metric-based analysis in order to generate \na similarity score between two given samples. At a very general level, application of K-Nearest Neighbors to a \ngiven dataset can be a thought of as a rudimentary, metric-based few-shot learning model. Metric-based models \nconsider input data similarly, clustering unlabelled data (known as the query set) based on information from a \npreviously seen set of labelled data (known as the support set).\nSnell et al.’s Prototypical ­Networks27 serve as notable, performant example of a recent metric-based few-shot \nlearning approach. The methodology proposed establishes use of an embedding function to map a given query \nset and support set to an embedding space. The mean of each class within the support is taken and defined as a \nprototype vector. The squared euclidean distance between a query embedding and all prototype vectors is used \nto generate the final distribution over classes for a given query point.\nFigure 1.   The process proposed for training and testing the nine selected few-shot learning techniques on out-\nof-domain data.\nFigure 2.   A temporal overview of notable few-shot learning techniques proposed within the past 5 years. \n\"Optimization-based\" few-shot learning techniques refer to those proposing changes to optimization processes \nemployed by a network. \"Metric-based\" few-shot learning techniques refer to those proposing a metric from \nwhich a similarity score between a set of samples can be obtained from. \"Augmented Metric-based\" few-shot \nlearning techniques refer to those proposing an augmentation (such as application of a self-supervised or \ntransductive process) to a metric-based few-shot learning technique.\n\n\n4\nVol:.(1234567890)\nScientific Reports |         (2022) 12:2924  | \nhttps://doi.org/10.1038/s41598-022-06718-2\nwww.nature.com/scientificreports/\nOptimization‑based few‑shot learning.  In contrast to metric-based strategies, approaches leveraging optimiza-\ntion-based few-shot learning propose no external metrics by which the model depends on. Instead, a model-\nagnostic approach is taken by defining a general-purpose optimization algorithm compatible with all models \nleveraging Stochastic Gradient Descent-based methods for learning. By applying this algorithm, all potential \nclasses are optimized, rather than continuous optimization towards a single dataset.\nTo enable further exploration and understanding of this strategy, we define a generic model as fθ with param-\neters θ , a generic dataset, D , a learning rate α , and a generic loss function, L . A \"task\" Ti is sampled from a dataset \nD as a grouping of classes. With the defined variables, we can update by a single Stochastic Gradient Descent \niteration using the following equation:\nIn doing so, however, we only compute the loss for a single batch within a single task. Optimization-based \napproaches, such as MAML and Reptile, promote accuracy across all given tasks, rather than a single task. To \nachieve generalization towards new tasks, MAML (and additionally Reptile) proposes an adaptation process \nwhich involves taking multiple gradient descent iterations for each task. The loss of each task is analyzed, enabling \ndiscovery of the optimal θ∗ which optimizes towards all tasks. In essence, Eq (1) is used to take small, iterative \ngradient descent steps for each task, discovering how optimization occurs. Once all tasks have been iterated over, \nθ∗ can be found, enabling us to take a large step in an overall optimal direction. Figure 3 illustrates optimizing \nacross three given tasks. With the above equation, we would have only taken a single step along either ∇L1 , ∇L2 , \nor ∇L3.\nTransductive and self‑supervised approaches to few‑shot learning.  Recent state-of-the-art studies within the \nfield of few-shot learning have demonstrated use of transductive techniques, self-supervised learning, and extra, \nunlabelled data in order to enable accurate performance. Rodriguez et al’s ­EPNet28 follows a transductive few-\nshot learning approach to enable quick uptake of new classes. In contrast, where optimization-based approaches \nleveraged a modified gradient descent algorithm, EPNet maps the support and query sets to an embedding space \nwherein all points are considered simultaneously. During this phase, labels are propagated from the support set \nto similar, unlabelled query set points. Figure 4 illustrates the process of propagation for a given set of points. \nBateni et al.’s proposed Simple ­CNAPS26 follows a similar metric-based clustering, however, a Mahalanobis dis-\ntance is used for comparison between points, rather than propagation of labels. PT+MAP29 and ­LaplacianShot30 \nfunction similarly, however, both propose alternative strategies for distance metrics when considering query and \n(1)\nθ\n′\ni = θ −α∇θLTi(fθ)\nFigure 3.   The optimization-based process for optimizing towards three tasks illustrated.\nFigure 4.   An illustrated example of transductive few-shot learning. (A) Grey circles represent unlabelled points \n(the query set) and coloured circles represent labelled points (the support set). (B) All unlabelled points are \nlabelled based on their position within the labelled data.\n\n\n5\nVol.:(0123456789)\nScientific Reports |         (2022) 12:2924  | \nhttps://doi.org/10.1038/s41598-022-06718-2\nwww.nature.com/scientificreports/\nsupport points. ­AmdimNet31 and ­S2M232, alternatively, leverage self-supervised techniques in order to generate \na stronger embedding-space mapping for input data.\nDataset selection and few‑shot data sampling.  All experiments performed in this study used the mini-ImageNet \nfew-shot ­dataset23 for training purposes. The BCCD White Blood Cell ­Dataset33 and a contrast-adjusted variant \nof the HEp-2 Cell ­dataset34 were used for out-of-domain few-shot testing. The HEp-2 dataset was additionally \nused for training. The aforementioned datasets were sampled from following the procedure defined by Vinyals \net al.23 for few-shot datasets.\nSampling data from the training and testing few-shot datasets was performed in an identical manner. Batches \nsampled from a dataset are defined as \"episodes\" for a given model. These episodes are composed of two data \nsections: a support set and a query set. The support set is comprised of labelled examples meant to teach a given \nmodel about the episode’s classes. Labelled classes, specifically, are sampled in terms of ways and shots. The \nnumber of ways in an episode defines how many classes are sampled, while the number of shots defines how \nmany of each class is sampled. The query set, in contrast, is composed of unlabelled data and serves as what \ncan be considered a miniature \"test set\" for the episode to benchmark how well a given model learned from the \nsupport set. Typically, 10 images per way are sampled from the respective class. All experiments performed in \nthis study use 5-way 5-shot sampling strategies for testing and training. Additionally, due to the sampling-based \nfew-shot datasets lacking a clear end, we define one epoch as being equivalent to sampling 600 episodes from \na given dataset.\nPart 2: evaluating modifications. \nData augmentations and regularization.  We apply data augmenta-\ntion and regularization techniques used in recent classification approaches in an attempt to prevent overfit-\nting on mini-ImageNet and encourage a more general embedding space. Hyperparameters associated with the \nemployed techniques applied in two ways. First, the hyperparameter is set to a static value and the model is left \nto train. Second, we follow the approach detailed in the recent ­EfficientNetV235 training scheme and gradually \nincrease (or decrease) a hyperparameter’s value over a number of epochs.\nRandAugment36.  A series of N random data augmentations (shear, colour jitter, etc) are applied to a batch of \nimages at a magnitude of M. N was set at a static value of 2 (an optimal value defined by Cubuk et al.) for all \nexperiments and M was tested at ranges [5–25], [5–15], [5–10], and a static value of 5.\nMixup37.  During the creation of an episode, sampled images are blended with another random image. The \nblending amount for one image is defined by the alpha hyperparameter. The blending amount for the alternate \nimage is defined as 1 −alpha . Alpha was tested at range [0–0.25], [0–0.1], and at a static value of 0.1.\nLabel ­smoothing38.  One-hot encoded vectors output from a classification model are run through label smooth-\ning before the final loss calculation is performed. In doing so, the one-hot encoded vector is replaced with a \nsmoothed, uniform distribution over the number of classes in the vector. Label smoothing contains a single \nhyperparameter that controls the level of smoothing applied to a given one-hot vector. The hyperparameter was \ntested at a static value of 0.1 (as defined by Muller et al.).\nExponential moving average of model ­weights39.  An exponential moving average is retained of a given model’s \nweights while training is underway. Averaged weights have been shown to perform better in some scenarios \nwhen applied to classification-based tasks.\nArchitectural improvements.  Internal changes to a given model were investigated during experimentation. \nHu et al.’s Squeeze and ­Excitation40 was evaluated due to its flexible implementation and notable performance \nimprovement. This architectural improvement, however, was noted to work best with Residual Networks and, as \nsuch, we only evaluate this architectural change using the ResNet-12 backbone. A hyperparameter R is exposed \nwith use of Squeeze and Excitation, enabling variation in the computational cost of the Squeeze and Excitation \nblock it pertains to. We tested this value at 0.1, 0.25, and 2.0.\nTraining methods.  We adopt two training strategies during experimentation. The first, and more prevalent, \nstrategy involves a single, long training cycle, typically defined as 100 epochs long on mini-ImageNet. At the end \nof this cycle, we expect the model’s loss to have converged. A plateau-based learning rate scheduler is used to \nmanage the learning rate throughout the training process. The second training strategy we employ, involves mir-\nroring the EfficientV2 training setup as part of the few-shot training process. Instead of a single, long cycle, we \nsubstitute with multiple, shorter training cycles (all training the same model), which we define as 80 epochs long. \nFour cycles are performed in total, which results in 320 epochs of training. Over the course of each consecutive \ncycle, the model is exposed to images that gradually become larger, all the while regularization is increased to \nprevent overfitting. An exponential decay learning rate schedule is used over each cycle, ensuring consistent \nperformance.\nNetwork backbone evaluation.  Within the field of few-shot learning, the majority of proposed methodologies \nhave historically leveraged one of three network backbones: a 4CONV network proposed as part of Vinyals’ \nMatching ­Network23, a Wide Residual ­Network41 with depth 28 and width 10, or a Residual ­Network15 with 12 \nlayers. To investigate application of other network backbones, we employ use of three networks: EfficientNetV2-\n\n\n6\nVol:.(1234567890)\nScientific Reports |         (2022) 12:2924  | \nhttps://doi.org/10.1038/s41598-022-06718-2\nwww.nature.com/scientificreports/\nSmall35, ResNet-18, and ­DenseNet42. EfficientNetV2 was selected for its state-of-the-art performance on mod-\nern, image classification datasets. ResNet-18 and DenseNet were selected for their state-of-the-art performance \nin previously proposed few-shot learning techniques.\nResults\nPart 1: investigating existing few‑shot learning techniques. \nModel evaluation.  To aid in compar-\ning each model, Table 1 provides a detailed overview of the respective technical attributes. The column headings \nwithin the table are expanded upon as follows.\nTechnique.  The type or style of few-shot learning applied.\nBackbone.  The backbone network applied by the few-shot learning approach. WRN28-10 refers to the \nWideResNet28-10 model. CONV4 refers to the convolutional model proposed by Vinyals et al. in MAML.\nPreprocessing.  Whether or not input to the network requires preprocessing by a feature extractor network.\nExtra training data.  Whether or not extra unlabelled training data was used to boost performance. Unlabelled \ntraining data typically relates to the support set at hand (for example, additional unlabelled images of cats are \nused if the cat class is in the support set).\nBaseline benchmarks.  Nine few-shot learning techniques were trained and benchmarked using the mini-Ima-\ngeNet training, validation, and testing dataset splits. Training was performed for 100 epochs using a plateau-\nbased learning rate scheduler with a patience of 10. From our observations, 100 epochs were sufficient to reach \nconvergence in all model permutations. Convergence was typically reached around the 60-80 epoch. Other \ntraining settings (optimizer choice, hyperparameter values, etc) were reproduced as defined by author’s of each \nrespective technique. A summary of these settings can be found in Table 2. The test accuracy on mini-ImageNet \nreported for all models was reproduced within margin-of-error. Out-of-domain few-shot testing was performed \nusing BCCD and HEp-2 as a 5-way 5-shot experiment. Performance demonstrated by all models on the HEp-2 \ndataset was within expectations, however, the accuracy exhibited after testing on BCCD suggested potential \nproblems. The issues were identified as problems stemming from input image size. Mini-ImageNet images have \nTable 1.   An overview of the differing details between the models trained and tested.\nModel name\nTechnique\nBackbone\nPreprocessing\nExtra training data\nModel evaluation table\nAmdimNet31\nSelf-supervised Metric\nAmdimNet\nNo\nYes\nEPNet28\nTransductive Metric\nWRN28-10\nNo\nYes\nSimpleCNAPS26\nMetric\nResNet18\nNo\nYes\nPT+MAP29\nMetric\nWRN28-10\nYes\nNo\nLaplacianShot30\nMetric\nWRN28-10\nNo\nNo\nS2M2R32\nSelf-supervised Metric\nWRN28-10\nYes\nNo\nReptile25\nOptimization\nCONV4\nNo\nNo\nMAML24\nOptimization\nCONV4\nNo\nNo\nProtoNet27\nMetric\nCONV4\nNo\nNo\nTable 2.   Parameter details specific to each technique.\nModel\nOptimizer\nMomentum\nWeight decay\nBatch size\nTechnique implementation details\nAmdimNet31\nAdam\n–\n–\n100\nEPNet28\nSGD\n0.9\n0.0005\n128\nSimpleCNAPS26\nAdam\n–\n–\n256\nPT+MAP29\nAdam\n–\n–\n16\nLaplacianShot30\nSGD\n0.9\n0.0001\n128\nS2M2R32\nAdam\n–\n–\n16\nReptile25\nAdam\n–\n–\n5\nMAML24\nAdam\n–\n–\n32\nProtoNet27\nAdam\n–\n–\n5\n\n\n7\nVol.:(0123456789)\nScientific Reports |         (2022) 12:2924  | \nhttps://doi.org/10.1038/s41598-022-06718-2\nwww.nature.com/scientificreports/\na resolution of 84px by 84px while BCCD images have a resolution of 224px by 224px. During initial testing \nexperiments, all input images were resized to 84px by 84px through anti-aliased, local mean downsampling. The \nfeatured white blood cell in a BCCD image that is being classified is not typically a prominent feature within an \ninput image, thus, by resizing, a significant portion of detail was lost, degrading few-shot performance. Global \npooling layers were added to all models, enabling arbitrary input image size and significantly improving BCCD \nfew-shot performance. Table 3 contains all results from the baseline tests run.\nIn an effort to further explore cell image-based few-shot performance, in-domain training and testing was \nperformed using HEp-2 as the training dataset and BCCD as the testing dataset. HEp-2 was selected as the train-\ning dataset due to the larger number of classes present (6 classes) versus BCCD (5 classes). In-domain training \nand testing was performed in the same manner as out-of-domain testing. The top performing techniques from \nout-of-domain testing (Reptile on BCCD and EPNet on HEp-2) were used. Table 4 details the results obtained \nfrom the in-domain tests run.\nPart 2: evaluating modifications. \nBackbone variations.  Three differing styles of network backbone \nwere evaluated in an attempt to further increase few-shot performance on EPNet. We solely train and test on mi-\nni-ImageNet in this instance since high accuracy on mini-ImageNet results in high accuracy on out-of-domain \ndatasets. To evaluate each backbone, EPNet’s original WideResNet backbone was replaced, trained, and tested \nwith EfficientNetV2, ResNet-18, and DenseNet. All selected backbone replacements, however, failed to match or \nsurpass the original WideResNet backbone. This result could likely be due to the relative complexity some of the \nselected backbones exhibited. Table 5 contains a detailed breakdown of the experimental results. DenseNet had \nalready demonstrated application in a recent few-shot learning approach, thus, the closest result being attributed \nto this network is no surprise.\nModel additions.  Various model additions were added to EPNet and benchmarked using mini-ImageNet. All \nadditions were trained for 100 epochs with a plateau-based learning rate schedule. Table 6 contains the full list \nof addition evaluation results. Generally, all proposed additions had a negative impact on EPNet during train-\ning. Some additions decreased accuracy by a couple percent while others drained accuracy by a large amount.\nDiscussion\nAnalyzing and classifying human cells (such as in blood smears or skin biopsies) is an intensive task requiring \nspecialized equipment and oversight from a trained professional. With recent progress in computer vision per-\nformance, however, automated image-based analysis of human cells has been an active area of research. Modern \ndeep learning-based approaches have specifically enabled superhuman performance in a wide array of fields. \nApplication of deep learning to medical scenarios, however, has typically stagnated due to dataset size require-\nments. A potential solution to these issues lies within the field of few-shot learning, an area of research concerned \nwith building performant networks using sparse amounts of data. Recent few-shot learning-based approaches \nhave demonstrated increasingly accurate performance on complex dataset, such as mini-ImageNet. In this study, \nwe investigated whether few-shot learning-based techniques could mitigate the data requirements necessary for \nTable 3.   Test accuracy results from baseline experiments run against the mini-ImageNet test set, BCCD, and \nHEp-2. Testing using the BCCD dataset was performed using additional global pooling layers. The highest \naccuracy relative to each dataset is in bold.\nModel\nMini-ImageNet\nBCCD\nHEp-2\nInitial dataset test performance\nAmdimNet31\n89.75 ± 0.12\n48.35 ± 0.18\n54.32 ± 0.21\nEPNet28\n88.66 ± 0.24\n47.39 ± 0.22\n55.12 ± 0.13\nSimpleCNAPS26\n90.11 ± 0.17\n47.06 ± 0.72\n53.15 ± 0.84\nPT+MAP29\n88.02 ± 0.13\n42.94 ± 0.17\n54.73 ± 0.22\nLaplacianShot30\n82.27 ± 0.15\n34.75 ± 0.13\n44.69 ± 0.17\nS2M2R32\n82.81 ± 0.31\n44.15 ±0.23\n54.41 ± 0.27\nReptile25\n65.62 ± 0.28\n50.91 ± 0.12\n51.76 ± 0.13\nMAML24\n64.62 ± 0.19\n42.81 ± 0.21\n45.21 ± 0.24\nProtoNet27\n67.88 ± 0.12\n46.89 ± 0.13\n50.70 ± 0.17\nTable 4.   Test accuracy results from in-domain training on HEp-2 and testing on BCCD.\nModel\nBCCD\nIn-domain performance\nEPNet28\n45.31 ± 0.21\nReptile25\n40.24 ± 0.23\n\n\n8\nVol:.(1234567890)\nScientific Reports |         (2022) 12:2924  | \nhttps://doi.org/10.1038/s41598-022-06718-2\nwww.nature.com/scientificreports/\nperformant deep learning-based cell classification. An optimal scenario, in this regard, would involve a selected \nfew-shot approach training on a non-medical dataset and accurately testing on a sparse medical dataset. Suc-\ncessful application of a few-shot technique to sparse medical data would drastically expedite existing workflows, \npotentially allowing automation of tasks typically allocated to trained professionals.\nTo facilitate this study, we selected mini-ImageNet23, a popular benchmark for few-shot learning techniques, \nas the dataset by which we would train on. For human cell-based evaluation, we selected the BCCD ­Dataset33 \n(BCCD) and the HEp-2 ­Dataset34 as the testing datasets. Our experimental process involved training few-shot \napproaches on mini-ImageNet and testing the resulting models on the BCCD dataset and the HEp-2 dataset. In \ndoing so, we benchmarked embeddings learned from a non-medical dataset on human cell-based classification. \nWe selected 9 notable, few-shot learning models proposed over the past 5 for use in our experiment. Each model \nwas implemented using the authors’ code (if available) and trained from scratch. Before testing, each model’s \nperformance on mini-ImageNet was verified against the original reported results (within margin-of-error).\nAfter completing experimentation, a decrease in accuracy of at least 30% was noted when transitioning from \nthe training dataset to an out-of-domain human cell dataset. In a rather shocking result, however, Reptile, a \nrelatively old technique, out-performed all newer few-shot learning approaches on the BCCD dataset and per-\nformed competitively on the HEp-2 dataset. MAML, a similar technique, also performed competitively on the \nout-of-domain testing datasets, beating a few newer approaches as well. These results potentially indicate that \nrelatively high performance on mini-ImageNet (and other few-shot benchmarks) does not necessarily guarantee \nproportional performance on out-of-domain tests. Reptile and MAML’s optimization-based strategy for fast \nadaption to new classes could also lead to further performance in out-of-domain tests.\nOverall, performance degradation on the selected medical datasets can largely be attributed to difficulties \ntransitioning from a non-medical domain to a medical domain. Severe out-of-domain accuracy decreases in \nfew-shot learning are corroborated by Bateni et al. in their experimentation with ­SimpleCNAPS26. Decreases as \nlarge as 20% were noted for out-of-domain images within the same dataset. This decrease, however, is a signifi-\ncant issue when rigorous standards for medical practice are taken into account. Models aiming for deployment \nin medicinal scenarios typically demonstrate high accuracy in their field of application. Even with proven and \naccurate capabilities, results produced by a model in a medical setting are still rigorously reviewed. Introducing \none of the current few-shot learning approaches investigated in this study could potentially lead to incorrect \noutput or, at worst, misdiagnosis for a patient.\nTable 6.   Test accuracy results using different model additions within EPNet. Each model addition was \nindependently trained on the mini-ImageNet training set and tested on the mini-ImageNet test set, BCCD, and \nHEp-2.\nModel addition\nMini-ImageNet\nBCCD (%)\nHEp-2\nModel addition performance\nNo additions\n88.7%\n47.4\n55.12%\nRandAugment (Magnitude = 5–15)\n75.8%\n34.6\n42.1%\nRandAugment (Magnitude = 5–10)\n69.3%\n27.8\n35.6%\nRandAugment (Magnitude = 5)\n70.1%\n28.9\n36.1%\nSqueeze and excitation (Reduction = 0.10)\n68.7%\n27.6\n35.2\nSqueeze and excitation (Reduction = 0.25)\n68.2%\n26.2\n34.9%\nSqueeze and excitation (Reduction = 2)\n63.9%\n22.4\n30.5%\nMixup (Alpha = 0.10)\n76.2%\n34.7\n42.8%\nLabel smoothing (A = 0.10)\n65.3\n23.2\n31.4%\nExponential moving average\n78.6%\n38.2\n44.1%\nTable 5.   Test accuracy results from using different backbone variations in EPNet on and testing on mini-\nImageNet, BCCD, and HEp-2. Each backbone was trained on mini-ImageNet’s training set before testing.\nBackbone (%)\nMini-ImageNet (%)\nBCCD\nHEp-2 (%)\nBackbone performance\nWideResNet28-10 (Original Backbone)\n88.7\n47.4\n55.1\nEfficientNetV2 (Default Width)\n59.8\n18.3\n26.3\nEfficientNetV2 (0.5 Width)\n67.3\n25.7\n33.3\nEfficientNetV2 (0.75 Width)\n69.2\n28.0\n35.7\nEfficientNetV2 (2.75 Width)\n70.8\n29.5\n37.1\nResNet-18\n68.2\n26.8\n34.7\nDenseNet\n78.8\n37.4\n45.0\n\n\n9\nVol.:(0123456789)\nScientific Reports |         (2022) 12:2924  | \nhttps://doi.org/10.1038/s41598-022-06718-2\nwww.nature.com/scientificreports/\nIn an attempt to boost few-shot performance, a variety of architectural revisions, data augmentation \napproaches, and training schemes were experimented with and benchmarked using mini-ImageNet. EPNet \nwas selected for this experimentation due to its ease of implementation and performance. Through this pro-\ncess, we discovered that recent measures taken to improve classification networks are ineffective on few-shot \nlearning-based networks. For example, EPNet’s backbone network was swapped and trained across a selection \nof performant, state-of-the-art classification backbones. ­EfficientNetV235, a network that recently achieved state-\nof-the-art accuracy on ImageNet, results in an accuracy decrease of at least 12% when combined with EPNet. \nSimilar, performance-boosting classification techniques, such as data regularization, resulted in performance \nregressions. After an exhaustive exploration of the aforementioned techniques, we concluded that the employed \nfew-shot learning technique should be the main focus for performance-based changes.\nThe methods applied in this work largely focus on a single training dataset and two, cell-based out-of-domain \ntesting datasets. Other, more rigorous few-shot learning evaluation frameworks, such as Triantafillou et al.’s \nMeta-Dataset43, employ use of multi-dataset strategies to gain a clearer understanding of a model’s performance. \nFurthermore, the few-shot learning techniques used in this study are selected within a limited window of time \n(5 years).\nConclusions and future work\nIn this study, we investigate the use of few-shot learning in human cell classification. During the performed train-\ning and testing, a variety of backbone architectures and training schemes were benchmarked for any potential \nbenefit. Although all tested techniques performed well when classifying unseen training data, significant perfor-\nmance decreases were observed when transitioning to either of the two human cell classification testing datasets. \nWith this in mind, we believe that few-shot learning techniques are still limited in the scope of problems they \ncan solve. Support for new techniques less \"brittle to [the] narrow domains they were trained on\" was recently \nhighlighted by Turing award winners Bengio, LeCun, and ­Hinton44. As such, we posit that a stronger emphasis \non out-of-domain robustness should be one of the main directions for future few-shot learning research.\nIn conclusion, few-shot learning methodologies are not yet capable of accurately performing out-of-domain \nclassification at a level accurate enough for human cell identification. We test this conclusion across a selection of \nnotable few-shot learning models proposed within the last 5 years. After training on mini-ImageNet and testing \non the BCCD and HEp-2 datasets, performance was found to drop by at least 30% after transitioning from the \nnon-medical dataset to the selected medical datasets. With this in mind, application of current few-shot learning \nmethodology to medical scenarios is, at this time, insufficient.\nTo facilitate better out-of-domain performance in few-shot learning, new areas of exploration are necessary. \nMAML and Reptile’s surprisingly competitive out-of-domain performance underscores a need to reconsider \nolder few-shot learning techniques. With this in mind, future few-shot learning research should reevaluate \noptimization strategies or focus on more flexible few-shot distance metrics. Metaheuristic algorithms, such as \nMonarch Butterfly ­Optimization45, the Earthworm Optimization ­Algorithm46, Elephant Herding ­Optimization47, \nthe Moth Search ­algorithm48, the Slime Mould ­algorithm49, and Harris Hawks ­optimization50, serve as possible \ndirections for improvement in optimization-based techniques. In future work, we plan on revisiting this area of \nresearch and investigating a wider variety of few-shot learning approaches across a more comprehensive set of \ndatasets (in and out-of-domain).\nReceived: 14 July 2021; Accepted: 31 January 2022\nReferences\n\t 1.\t Link, D. Programming enter: Christopher strachey’s draughts program. Comput. Resurrection. Bull. Comput. Conserv. Soc. 60, \n23–31 (2012).\n\t 2.\t McCorduck, P. & Cfe, C. Machines Who Think: A Personal Inquiry into the History and Prospects of Artificial Intelligence (CRC \nPress, 2004).\n\t 3.\t Jackson, P. Introduction to Expert Systems (Addison-Wesley Longman Publishing Co. Inc, 1998).\n\t 4.\t Rumelhart, D. E., Hinton, G. E. & Williams, R. J. Learning representations by back-propagating errors. Nature 323, 533–536 (1986).\n\t 5.\t Bai, B., Li, G., Wang, S., Wu, Z. & Yan, W. Time series classification based on multi-feature dictionary representation and ensemble \nlearning. Exp. Syst. Appl. 169, 114162 (2021).\n\t 6.\t Yan, W., Li, G., Wu, Z., Wang, S. & Yu, P. S. Extracting diverse-shapelets for early classification on time series. World Wide Web 23, \n3055–3081 (2020).\n\t 7.\t Li, J., Li, Y.-X., Tian, S.-S. & Xia, J.-L. An improved cuckoo search algorithm with self-adaptive knowledge learning. Neural Comput. \nAppl. 32, 11967–11997 (2020).\n\t 8.\t Li, G., Wang, G.-G., Dong, J., Yeh, W.-C. & Li, K. Dlea: A dynamic learning evolution algorithm for many-objective optimization. \nInf. Sci. 574, 567–589 (2021).\n\t 9.\t Li, W., Wang, G.-G. & Alavi, A. H. Learning-based elephant herding optimization algorithm for solving numerical optimization \nproblems. Knowl.-Based Syst. 195, 105675 (2020).\n\t10.\t Wang, G.-G., Deb, S., Gandomi, A. H. & Alavi, A. H. Opposition-based krill herd algorithm with cauchy mutation and position \nclamping. Neurocomputing 177, 147–157 (2016).\n\t11.\t Li, W. & Wang, G.-G. Elephant herding optimization using dynamic topology and biogeography-based optimization based on \nlearning for numerical optimization. Eng. Comput. 1–29 (2021).\n\t12.\t Li, W., Wang, G.-G. & Gandomi, A. H. A survey of learning-based intelligent optimization algorithms. Arch. Comput. Methods \nEng. 28, 1–19 (2021).\n\t13.\t Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep convolutional neural networks. Commun. ACM 60, \n84–90 (2017).\n\t14.\t Simonyan, K. & Zisserman, A. Very deep convolutional networks for large-scale image recognition. Preprint arXiv:​1409.​1556 \n(2014).\n\n\n10\nVol:.(1234567890)\nScientific Reports |         (2022) 12:2924  | \nhttps://doi.org/10.1038/s41598-022-06718-2\nwww.nature.com/scientificreports/\n\t15.\t He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer \nVision and Pattern Recognition, 770–778 (2016).\n\t16.\t Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J. & Wojna, Z. Rethinking the inception architecture for computer vision. In Proceed‑\nings of the IEEE Conference on Computer Vision and Pattern Recognition, 2818–2826 (2016).\n\t17.\t Ying, C., Huang, Z. & Ying, C. Accelerating the image processing by the optimization strategy for deep learning algorithm dbn. \nEURASIP J. Wirel. Commun. Netw. 2018, 1–8 (2018).\n\t18.\t Zhang, J., Yu, J., Fu, S. & Tian, X. Adoption value of deep learning and serological indicators in the screening of atrophic gastritis \nbased on artificial intelligence. J. Supercomput. 77, 1–20 (2021).\n\t19.\t Iandola, F. et al. Densenet: Implementing efficient convnet descriptor pyramids. Preprint arXiv:​1404.​1869 (2014).\n\t20.\t Wang, S. et al. Multi-scale context-guided deep network for automated lesion segmentation with endoscopy images of gastroin-\ntestinal tract. IEEE J. Biomed. Health Inform. 25, 514–525 (2020).\n\t21.\t Russakovsky, O. et al. Imagenet large scale visual recognition challenge. Int. J. Comput. Vis. 115, 211–252 (2015).\n\t22.\t Koch, G., Zemel, R. & Salakhutdinov, R. Siamese neural networks for one-shot image recognition. In ICML Deep Learning Work‑\nshop, vol. 2 (Lille, 2015).\n\t23.\t Vinyals, O. et al. Matching networks for one shot learning. Adv. Neural Inf. Process. Syst. 29, 3630–3638 (2016).\n\t24.\t Finn, C., Abbeel, P. & Levine, S. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference \non Machine Learning, 1126–1135 (PMLR, 2017).\n\t25.\t Nichol, A., Achiam, J. & Schulman, J. On first-order meta-learning algorithms. Preprint arXiv:​1803.​02999 (2018).\n\t26.\t Bateni, P., Goyal, R., Masrani, V., Wood, F. & Sigal, L. Improved few-shot visual classification. In Proceedings of the IEEE/CVF \nConference on Computer Vision and Pattern Recognition (CVPR) (2020).\n\t27.\t Snell, J., Swersky, K. & Zemel, R. S. Prototypical networks for few-shot learning. Preprint arXiv:​1703.​05175 (2017).\n\t28.\t Rodríguez, P., Laradji, I., Drouin, A. & Lacoste, A. Embedding propagation: Smoother manifold for few-shot classification. In \nEuropean Conference on Computer Vision, 121–138 (Springer, 2020).\n\t29.\t Hu, Y., Gripon, V. & Pateux, S. Leveraging the feature distribution in transfer-based few-shot learning. Preprint arXiv:​2006.​03806 \n(2020).\n\t30.\t Ziko, I., Dolz, J., Granger, E. & Ayed, I. B. Laplacian regularized few-shot learning. In International Conference on Machine Learn‑\ning, 11660–11670 (PMLR, 2020).\n\t31.\t Chen, D. et al. Self-supervised learning for few-shot image classification. In ICASSP 2021-2021 IEEE International Conference on \nAcoustics, Speech and Signal Processing (ICASSP), 1745–1749 (IEEE, 2021).\n\t32.\t Mangla, P. et al. Charting the right manifold: Manifold mixup for few-shot learning. In Proceedings of the IEEE/CVF Winter Con‑\nference on Applications of Computer Vision, 2218–2227 (2020).\n\t33.\t Bccd dataset. https://​github.​com/​Sheng​gan/​BCCD_​Datas​et (2019).\n\t34.\t Larsen, A. B. L., Vestergaard, J. S. & Larsen, R. Hep-2 cell classification using shape index histograms with donut-shaped spatial \npooling. IEEE Trans. Med. Imaging 33, 1573–1580 (2014).\n\t35.\t Tan, M. & Le, Q. V. Efficientnetv2: Smaller models and faster training. Preprint arXiv:​2104.​00298 (2021).\n\t36.\t Cubuk, E. D., Zoph, B., Shlens, J. & Le, Q. V. Randaugment: Practical automated data augmentation with a reduced search space. \nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 702–703 (2020).\n\t37.\t Zhang, H., Cisse, M., Dauphin, Y. N. & Lopez-Paz, D. mixup: Beyond empirical risk minimization. Preprint arXiv:​1710.​09412 \n(2017).\n\t38.\t Müller, R., Kornblith, S. & Hinton, G. When does label smoothing help? Preprint arXiv:​1906.​02629 (2019).\n\t39.\t Kingma, D. P. & Ba, J. Adam: A method for stochastic optimization. Preprint arXiv:​1412.​6980 (2014).\n\t40.\t Hu, J., Shen, L. & Sun, G. Squeeze-and-excitation networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern \nRecognition, 7132–7141 (2018).\n\t41.\t Zagoruyko, S. & Komodakis, N. Wide residual networks. Preprint arXiv:​1605.​07146 (2016).\n\t42.\t Lichtenstein, M., Sattigeri, P., Feris, R., Giryes, R. & Karlinsky, L. Tafssl: Task-adaptive feature sub-space learning for few-shot \nclassification. In European Conference on Computer Vision, 522–539 (Springer, 2020).\n\t43.\t Triantafillou, E. et al. Meta-dataset: A dataset of datasets for learning to learn from few examples. Preprint arXiv:​1903.​03096 (2019).\n\t44.\t Bengio, Y., Lecun, Y. & Hinton, G. Deep learning for ai. Commun. ACM 64, 58–65. https://​doi.​org/​10.​1145/​34482​50 (2021).\n\t45.\t Wang, G.-G., Deb, S. & Cui, Z. Monarch butterfly optimization. Neural Comput. Appl. 31, 1995–2014 (2019).\n\t46.\t Wang, G.-G., Deb, S. & Coelho, L. D. S. Earthworm optimisation algorithm: A bio-inspired metaheuristic algorithm for global \noptimisation problems. Int. J. Bio-inspired Comput. 12, 1–22 (2018).\n\t47.\t Wang, G.-G., Deb, S. & Coelho, L. d. S. Elephant herding optimization. In 2015 3rd International Symposium on Computational \nand Business Intelligence (ISCBI), 1–5 (IEEE, 2015).\n\t48.\t Wang, G.-G. Moth search algorithm: A bio-inspired metaheuristic algorithm for global optimization problems. Memetic Comput. \n10, 151–164 (2018).\n\t49.\t Li, S., Chen, H., Wang, M., Heidari, A. A. & Mirjalili, S. Slime mould algorithm: A new method for stochastic optimization. Fut. \nGen. Comput. Syst. 111, 300–323 (2020).\n\t50.\t Heidari, A. A. et al. Harris hawks optimization: Algorithm and applications. Fut. Gen. Comput. Syst. 97, 849–872 (2019).\nAcknowledgements\nFunding for this project was provided through the MITACS Accelerate Grant (IT20377).\nAuthor contributions\nR.W., M.H.A., M.S.S., and M.M.M. conceived the experiments, R.W. conducted the experiments, R.W., M.H.A., \nM.S.S., and M.M.M. analysed the results. All authors reviewed the manuscript.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to R.W.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note  Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\n\n\n11\nVol.:(0123456789)\nScientific Reports |         (2022) 12:2924  | \nhttps://doi.org/10.1038/s41598-022-06718-2\nwww.nature.com/scientificreports/\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http://​creat​iveco​mmons.​org/​licen​ses/​by/4.​0/.\n© The Author(s) 2022\n"
}