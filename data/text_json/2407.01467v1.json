{
  "filename": "2407.01467v1.pdf",
  "num_pages": 15,
  "pages": [
    "The Balanced-Pairwise-Affinities Feature Transform\nDaniel Shalam 1 Simon Korman 1\nAbstract\nThe Balanced-Pairwise-Affinities (BPA) feature\ntransform is designed to upgrade the features of a\nset of input items to facilitate downstream match-\ning or grouping related tasks. The transformed\nset encodes a rich representation of high order\nrelations between the input features. A particu-\nlar min-cost-max-flow fractional matching prob-\nlem, whose entropy regularized version can be\napproximated by an optimal transport (OT) op-\ntimization, leads to a transform which is effi-\ncient, differentiable, equivariant, parameterless\nand probabilistically interpretable.\nWhile the\nSinkhorn OT solver has been adapted extensively\nin many contexts, we use it differently by min-\nimizing the cost between a set of features to it-\nself and using the transport plan’s rows as the\nnew representation. Empirically, the transform is\nhighly effective and flexible in its use and con-\nsistently improves networks it is inserted into,\nin a variety of tasks and training schemes. We\ndemonstrate state-of-the-art results in few-shot\nclassification, unsupervised image clustering and\nperson re-identification.\nCode is available at\ngithub.com/DanielShalam/BPA .\n1. Introduction\nIn this work, we reassess the functionality of features in set-\ninput problems, in which a task is defined over a set of items.\nProminent examples of this setting are few-shot classifica-\ntion (Ravi & Larochelle, 2017), clustering (Van Gansbeke\net al., 2020), feature matching (Korman & Avidan, 2015)\nand person re-identification (Ye et al., 2021), to name but a\nfew. In such tasks, features computed at test time are mainly\ncompared relative to one another, and less so to the features\nseen at training time. For such tasks, the practice of learning\na generic feature extractor during training and applying it at\ntest time is sub-optimal.\nIn set-input problems, such as few-shot classification, an\n1Department of Computer Science, University of Haifa, Israel.\nCorrespondence to: Daniel Shalam <dani360@gmail.com>.\ninstance of the task is in the form of a set of n items (e.g.\nimages) {xi}n\ni=1. A generic neural-network pipeline (Fig. 1\nLeft) typically uses a feature embedding (extractor) F, that\nis applied independently to each input item, to obtain a set\nof features V ={vi}n\ni=1={F(xi)}n\ni=1, prior to downstream\ntask-specific processing G (e.g. a clustering head or classi-\nfier). The features V can be of high quality (concise, unique,\ndescriptive), but are limited in representation since they are\nextracted based on knowledge acquired for similar examples\nat train time, with no context of the test time instance they\nare part of, which is critical in set-input tasks.\nWe rather consider the more general framework (Fig. 1\nRight), in which the per-item independently extracted fea-\nture collection V is passed to an attention-mechanism type\ncomputation, in which some transform jointly processes the\nentire set of instance features, re-embedding each feature in\nlight of the joint statistics of the entire instance.\nThe main idea of BPA is very intuitive and is demonstrated\non a toy example in Fig. 2. The embedding of each feature\nwill encode the distribution of its affinities to the rest of the\nset items. Specifically, items in the embedded space will be\nclose if and only if they share a similar such distribution, i.e.\n’agree’ on the way they ’see’ the entire set. In fact, the trans-\nform largely discards the item-specific feature information,\nresulting in a purely relative normalized representation that\nresults in a highly efficient embedding with many attractive\nproperties.\nThe proposed transformation can be computed very effi-\nciently, with negligible runtime within the hosting network,\nand can be easily used in different contexts, as can be seen\nin the pseudo-code snippets we provide in Sections A and\nC of the Appendix. The embedding itself is given by rows\nof an optimal-transport (OT) plan matrix, which is the solu-\ntion to a regularized min-cost-max-flow fractional matching\nproblem that is defined over the pairwise (self)-affinities\nmatrix of the features in the set.\nTechnically, it involves the computation of pairwise dis-\ntances and several normalization iterations of a Sinkhorn\n(Cuturi, 2013) algorithm, baring apparent similarities to\nmany related methods based on either Spectral Clustering\n(Ng et al., 2001) that normalize the same affinity matrix),\nattention-mechanisms (Vaswani et al., 2017) that learn fea-\ntures based on a self-affinities matrix perhaps even normal-\n1\narXiv:2407.01467v1  [cs.LG]  25 Jun 2024\n",
    "The Balanced-Pairwise-Affinities Feature Transform\nFigure 1. Generic designs of networks that act on sets of items. These cover relevant architectures, e.g. for few-shot-classification and\nclustering. Left: A generic network for processing a set of input items typically follows the depicted structure: (i) Each item separately\ngoes through a common feature extractor F. (ii) The set of extracted features is the input to a downstream task processing module G. ;\nRight: A more general structure in which the extracted features undergo a joint processing by a transform T. Our BPA transform (as well\nas other attention mechanisms) is of this type and its high-level design (within the ‘green’ module) is detailed in Fig. 2.\nized (Sander et al., 2022) and other matching (Sarlin et al.,\n2020) or classification (Hu et al., 2020) algorithms where\noptimal-transport plans are computed between source items\nand target items or class centers. However, the most im-\nportant difference and our main novel observation is that\nthe self fractional matching itself (which can be viewed as\na balanced affinity matrix) can serve as a powerful embed-\nding, since the distances in this space (between assignment\nvectors) have explicit interpretations that we explore, which\nare highly beneficial to general grouping based algorithms\nthat are applied to such set-input tasks.\nContribution\nWe propose a parameter-less optimal-transport based feature\ntransform, termed BPA, which can be used as a drop-in addi-\ntion that converts a generic feature extraction scheme to one\nthat is well suited to set-input tasks (e.g. from Figure 1 Left\nto Right). It is analyzed and shown to have the following\nattractive set of qualities. (i) efficiency: having real-time\ninference; (ii) differentiability: allowing end-to-end training\nof the entire ‘embedding-transform-inference’ pipeline of\nFig. 1 Right; (iii) equivariance: ensuring that the embed-\nding works coherently under any order of the input items;\n(iv) probabilistic interpretation: each embedded feature\nwill encode its distribution of affinities to all other features,\nby conforming to a doubly-stochastic constraint; (iv) valu-\nable metrics for the item set: Distances between embedded\nvectors will include both direct and indirect (third-party)\nsimilarity information between input features.\nEmpirically, we show BPA’s flexibility and ease of applica-\ntion to a wide variety of tasks, by incorporating it in leading\nmethods of each type. We test different configurations, such\nas whether the hosting network is pre-trained or re-trained\nwith BPA inside, across different backbones, whether trans-\nductive or inductive. Few-shot-classification is our main ap-\nplication with extensive experimentation on standard bench-\nmarks, testing on unsupervised-image-clustering shows the\npotential of BPA in the unsupervised domain and the person-\nre-identification experiments show how BPA deals with non-\ncurated large-scale tasks. In all three applications, over the\ndifferent setups and datasets, BPA consistently improves its\nhosting methods, achieving new state-of-the-art results.\n2. Relation to Prior Work\n2.1. Related Techniques\nSet-to-Set (or Set-to-Feature) Functions have been devel-\noped to act jointly on a set of items (typically features) and\noutput an updated set (or a single feature), which are used\nfor downstream inference tasks. Deep-Sets (Zaheer et al.,\n2017) formalized fundamental requirements from architec-\ntures that process sets. Point-Net (Qi et al., 2017) presented\nan influential design for learning local and global features on\n3D point-clouds, while Maron et al. (2020) study the design\nof equi/in-variant layers. Unlike BPA, the joint processing\nin these methods is limited, amounting to weight-sharing\nbetween separate processes and joint aggregations.\nAttention Mechanisms. The introduction of Relational\nNetworks (Santoro et al., 2017) and Transformers (Vaswani\net al., 2017) with their initial applications in vision mod-\nels (Ramachandran et al., 2019) have lead to the huge impact\nof Vision Transformers (ViTs) (Dosovitskiy et al., 2020) in\nmany vision tasks (Khan et al., 2021). While BPA can be\nseen as a self-attention module, it is very different, first,\nsince it is parameterless, and hence can work at test-time on\na pre-trained network. In addition, is can provide an explicit\nprobabilistic global interpretation of the instance data.\nSpectral Methods have been widely used as simple trans-\nforms applied on data that needs to undergo grouping or\nsearch based operations, jointly processing the set of fea-\ntures, resulting in a compact and perhaps discriminative\nrepresentation. PCA (Pearson, 1901) provides a joint di-\nmension reduction, which maximally preserves data vari-\nance, but does not necessarily improve feature affinities\nfor downstream tasks. Spectral Clustering (SC) (Shi &\nMalik, 2000; Ng et al., 2001) is the leading non-learnable\nclustering method in use in the field. If we ignore its final\n2\n",
    "The Balanced-Pairwise-Affinities Feature Transform\nFigure 2. The BPA transform: illustrated on a toy 7 image 3-class MNIST example.\nclustering stage, SC consists of forming a pairwise affinity\nmatrix which is normalized (Zass & Shashua, 2006) before\nextracting its leading eigenvectors, which form the final\nembedding. BPA is also based on normalizing an affinity\nmatrix, but uses this matrix’s rows as embedded features\nand avoids any further spectral decompositions, which are\ncostly and difficult to differentiate through.\nOptimal Transport (OT) problems are directly related to\nmeasuring distances between distributions or sets of features.\nCuturi (2013) popularized the Sinkhorn algorithm which is\na simple, differentiable and fast approximation of entropy-\nregularized OT, which has since been used extensively, for\nclustering (Lee et al., 2019; Asano et al., 2020), few-show-\nclassification (Huang et al., 2019; Ziko et al., 2020; Hu\net al., 2020; Zhang et al., 2021; Chen & Wang, 2021; Zhu\n& Koniusz, 2022), matching (Wang et al., 2019; Fey et al.,\n2020; Sarlin et al., 2020), representation learning (Caron\net al., 2020; Asano et al., 2020), retrieval (Xie et al., 2020),\nperson re-identification (Wang et al., 2022), style-transfer\n(Kolkin et al., 2019) and attention (Sander et al., 2022).\nOur approach also builds on some attractive properties of the\nSinkhorn solver. While our usage of Sinkhorn is extremely\nsimple (see Algorithm 1), it is fundamentally different from\nall other OT usages we are aware of, since: (i) We com-\npute the transport-plan between a set of features and itself -\nnot between feature-sets and label/class-prototypes (Huang\net al., 2019; Ziko et al., 2020; Hu et al., 2020; Zhang et al.,\n2021; Chen & Wang, 2021; Zhu & Koniusz, 2022; Lee et al.,\n2019; Asano et al., 2020; Xie et al., 2020; Wang et al., 2022;\nKolkin et al., 2019), or between two different feature-sets\n(Wang et al., 2019; Fey et al., 2020; Sarlin et al., 2020;\nSander et al., 2022); (ii) While others use the transport-plan\nto obtain distances or associations between features and\nfeatures/classes, we use its own rows as new feature vectors\nfor downstream tasks.\n2.2. Instance-Specific Applications\nFew-Shot Classification (FSC) is a branch of few-shot\nlearning in which a classifier learns to recognize previously\nunseen classes given a limited number of labeled examples.\nIn the meta-learning approach, the training data is split into\ntasks (or episodes) mimicking the test time tasks to which\nthe learner is required to generalize. MAML (Finn et al.,\n2017) “learns to fine-tune” by learning a network initializa-\ntion from which it can quickly adapt to novel classes. In\nProtoNet (Snell et al., 2017), a learner is meta-trained to\npredict query feature classes, based on distances from sup-\nport class-prototypes in the embedding space. The trainable\nversion of BPA can be viewed as a meta-learning algorithm.\nSubsequent works (Chen et al., 2018; Dhillon et al., 2020)\nadvocate using larger and more expressive backbones, em-\nploying transductive inference, which fully exploits the data\nat inference, including unlabeled images. BPA is transduc-\ntive, but does not make assumptions on (nor needs to know)\nthe number of classes (ways) or items per class (shots), as it\nexecutes a general probabilistic grouping action.\nRecently, attention mechanisms were shown to be effective\nfor FSC (Kang et al., 2021; Zhang et al., 2020; Ye et al.,\n2020) and a number of works (Ziko et al., 2020; Huang\net al., 2019; Hu et al., 2020; Zhang et al., 2021; Chen &\nWang, 2021) have adopted Sinkhorn (Cuturi, 2013) as a\nparameterless unsupervised classifier that computes match-\nings between query embeddings and class centers. Sill-Net\n(Zhang et al., 2021) that augments training samples with\nillumination features and PTMap-SF (Chen & Wang, 2021)\nthat proposes DCT-based feature embedding, are both based\non PTMap (Hu et al., 2020). The state-of-the-art PMF (Hu\net al., 2022), proposed a 3 stage pipeline of pre-training on\nexternal data, meta-training with labelled tasks, and fine-\ntuning on unseen tasks. BPA can be incorporated into these\nmethods, immediately after their feature extraction stage.\nUnsupervised Image Clustering (UIC) is the task of\ngrouping related images, without any label information,\ninto representative clusters. Naturally, the ability to measure\nthe similarities among samples is a crucial aspect of UIC.\nRecent methods have achieved tremendous progress in this\ntask, towards closing the gap with supervised counterparts.\nThe leading approaches directly learn to map images to la-\nbels, by constraining the training of an unsupervised classi-\nfication model with different types of indirect loss functions.\nProminent works in this area include DAC (Chang et al.,\n3\n",
    "The Balanced-Pairwise-Affinities Feature Transform\n2017), which recasts the clustering problem into a binary\npairwise-classification framework and SCAN (Van Gans-\nbeke et al., 2020) which builds on a pre-trained encoder\nthat provides nearest-neighbor based constraints for training\na classifier. The recent state-of-the-art SPICE (Niu et al.,\n2022), is a pseudo-labeling based method, which divides\nthe clustering network into a feature model for measuring\nthe instance-level similarity and a clustering head for identi-\nfying the cluster-level discrepancy.\nPerson Re-Identification (Re-ID) is the task identifying a\ncertain person (identity) between multiple detected pedes-\ntrian images, from different non-overlapping cameras. It\nis challenging due to the scale of the problem and large\nvariation in pose, background and illumination.\nSee Ye et al. (2021) for an excellent comprehensive sur-\nvey on the topic. Among the most popular methods are\nOSNet (Zhou et al., 2019) that developed an efficient small-\nscale network with high performance and DropBlock (Top-\nDB-Net) (Quispe & Pedrini, 2020) which achieved state-of-\nthe-art results by dropping a region block in the feature map\nfor attentive learning. The Re-ID task is typically larger\nscale - querying thousands of identities against a target of\ntens of thousands. Also, the data is much more real-world\ncompared to the carefully curated FSC sets.\n3. The BPA Transform\n3.1. Derivation\nAssume we are given a task instance which consists of an\ninference problem over a set of n items {xi}n\ni=1, where\neach of the items belongs to a space of input items Ω⊆RD.\nThe inference task can be modeled as fθ({xi}n\ni=1), using a\nlearned function fθ, which acts on the set of input items and\nis parameterized by a set of parameters θ. Typically, such\nfunctions combine an initial feature extraction stage that is\napplied independently to each input item, with a subsequent\nstage of (separate or joint) processing of the feature vectors\n(see Fig. 1 Left or Right, respectively).\nThat is, the function fθ takes the form fθ({xi}n\ni=1) =\nGψ({Fϕ(xi)}n\ni=1), where Fϕ is the feature extractor (or\nembedding network) and Gψ is the task inference function,\nparameterized by ϕ and ψ respectively, where θ = ϕ ∪ψ.\nThe feature embedding F : RD →Rd, usually in the form\nof a neural-network (with d ≪D), could be either pre-\ntrained, or trained in the context of the task function f,\nalong with the inference function G.\nFor an input {xi}n\ni=1, let us define the set of embedded\nfeatures {vi}n\ni=1 = {F(xi)}n\ni=1. In the following, we con-\nsider these sets of input vectors and features as real-valued\nrow-stacked matrices X ∈Rn×D and V ∈Rn×d.\nWe suggest a novel re-embedding of the feature set V, using\na transform, that we denote by T, in order to obtain a new\nset of features W = T(V), where W ∈Rn×n. The new\nfeature set W has an explicit probabilistic interpretation,\nwhich is specifically suited for tasks related to classification,\nmatching or grouping of items in the input set X. In par-\nticular, W will be a symmetric, doubly-stochastic matrix\n(non-negative, with rows and columns that sum to 1), where\nthe entry wij (for i ̸= j) encodes the belief that items xi\nand xj belong to the same class or cluster.\nThe proposed transform T : Rn×d →Rn×n (see Fig. 2)\nacts on the original feature set V as follows. It begins by\ncomputing the squared Euclidean pairwise distances matrix\nD, namely, dij = ||vi −vj||2, which can be computed\nefficiently as dij = 2(1 −cos(vi, vj)) = 2(1 −vi · vT\nj ),\nwhen the rows of V are unit normalized. Or in a compact\nform, D = 2(1 −S), where 1 is the all ones n × n matrix\nand S = V · VT is the cosine affinity matrix of V.\nW will be computed as the optimal transport (OT) plan\nmatrix between the n-dimensional all-ones vector 1n and\nitself, under the self cost matrix D∞, which is the distance\nmatrix D with a very (infinitely) large scalar replacing each\nof the entries on its diagonal (which were all zero), that\nenforces the affinities of each feature to distribute among\nthe others. Explicitly, let D∞= D + αI, where α is a very\n(infinitely) large constant and I is the n × n identity matrix.\nW is defined to be the doubly-stochastic matrix that is the\nminimizer of the functional\nW = arg min\nW∈Bn\n⟨D∞, W⟩\n(1)\nwhere Bn is the set (known as the Birkhoff polytope) of\nn × n doubly-stochastic matrices and ⟨·, ·⟩stands for the\nFrobenius (standard) dot-product.\nThis objective can be minimized using simplex or interior\npoint methods with complexity Θ(n3 log n). In practice,\nwe use the highly efficient Sinkhorn-Knopp method (Cu-\nturi, 2013), which is an iterative scheme that optimizes an\nentropy-regularized version of the problem, where each iter-\nation takes Θ(n2). Namely:\nW = arg min\nW∈Bn\n⟨D∞, W⟩−1\nλh(W)\n(2)\nwhere h(W) = −P\ni,j wij log(wij) is the Shannon en-\ntropy of W and λ is the entropy regularization parameter.\nThe transport-plan matrix W that is the minimizer of Equa-\ntion (2) will become the result of our transform, after ’restor-\ning’ perfect affinities on the diagonal (replacing the diagonal\nentries from 0s to 1s) by W = W + I, where I is the n × n\nidentity matrix. Our final set of features is T(V) = W and\neach of its rows is the re-embedding of each of the corre-\nsponding features (rows) in V. The BPA transform is given\n4\n",
    "The Balanced-Pairwise-Affinities Feature Transform\nFigure 3. The min-cost max-flow perspective: Costs are shown.\nin Algorithm 1 in the appendix, in PyTorch-style pseudo-\ncode. Note that W is symmetric as a result of the symmetry\nof D and its own double-stochasticity. We next explain its\nprobabilistic interpretation.\n3.2. Probabilistic interpretation\nThe optimization problem in Equation (1) can be written\nmore explicitly as follows:\nmin\nW ⟨D∞, W⟩\ns.t.\nW · 1n = WT · 1n = 1n\n(3)\nwhich can be seen to be the same as:\nmin\nW ⟨D, W⟩\ns.t.\nW · 1n = WT · 1n = 1n\nwii = 0\nfor\ni = 1, . . . n\n(4)\nsince the use of the infinite weights on the diagonal of D∞\nis equivalent to using the original D with a constraint of\nzeros along the diagonal of W.\nThe optimization problem in Equation (4) is in fact a frac-\ntional matching instance between the set of n original fea-\ntures and itself. It can be posed as a bipartite-graph min-cost\nmax-flow instance (The problem of finding a min cost flow\nout of all max-flow solutions), as depicted in Fig. 3. The\ngraph has n nodes on each side, representing the original\nfeatures {vi}n\ni=1 (the rows of V). Across the two sides, the\ncost of the edge (vi, vj) is the distance dij and the edges\nof the type (vi, vi) have a cost of infinity (or can simply be\nremoved). Each ‘left’ node is connected to a ’source’ node\nS by an edge of cost 0 and similarly each ’right’ node is\nconnected to a ‘target’ (sink) node T. All edges in the graph\nhave a capacity of 1 and the goal is to find an optimal frac-\ntional self matching, by finding a min-cost max-flow from\nsource to sink. Note that the max-flow can easily be seen to\nbe n, but a min-cost flow is sought among max-flows.\nIn this set-to-itself matching view, each vector vi is fraction-\nally matched to the set of all other vectors V −{vi} based on\nthe pairwise distances, but importantly taking into account\nthe fractional matches of the rest of the vectors in order to\nsatisfy the double-stochasticity constraint. The construction\nconstrains the max flow to have a total outgoing flow of\n1 from each ‘left’ node and a total incoming flow of 1 to\neach ‘right’ node. Therefore, the ith transformed feature\nFigure 4. The (symmetric) embedding matrix W and the abso-\nlute difference between its ith and jth rows.\nwi (ith row of W) is a distribution (non-negative entries,\nsumming to 1), where wii = 0 and wij is the relative belief\nthat features i and j belong to the same ‘class’.\n3.3. Properties\nWe can now point out some important properties of the\nproposed embedding, given by the rows of the matrix W.\nSome of these properties can be observed in the toy 3-class\nMNIST digit example, illustrated in Fig. 2.\nInterpretability of distances in the embedded space: An\nimportant property of our embedding is that each embed-\nded feature encodes its distribution of affinities to all other\nfeatures. In particular, the comparison of embedded vectors\nwi and wj (of items i and j in a set) includes both direct\nand indirect information about the similarity between the\nfeatures. Refer to Figure 4 for a detailed explanation of\nthis property. If we look at the different coordinates k of\nthe absolute difference vector a = |wi −wj|, BPA cap-\ntures (i) direct affinity: For k which is either i or j, it holds\nthat ak = 1 −wij = 1 −wji 1. This amount measures how\nhigh (close to 1) is the mutual belief of features i and j about\none another. (ii) indirect (3rd-party) affinity: For k /∈{i, j},\nwe have ak = |wik −wjk|, which is a comparison of the\nbeliefs of features i and j regarding the (third-party) feature\nk. The double-stochasticity of the transformed feature-set\nensures that the compared vectors are similarly scaled (as\ndistributions, plus 1 on the diagonal) and the symmetry\nfurther enforces the equal relative affinity between pairs.\nAs an example, observe the output features 4 and 5 in Fig. 2,\nthat re-embed the ’green’ features of the digit ’7’ images.\nAs desired, these embedding are close in the target 7D space.\nThe closeness is driven by both their closeness in the original\nspace (coordinates 4 and 5) as well as the agreement on\nspecific large differences from other images. This property\nis responsible for better separation between classes in the\ntarget domain, which leads to improved performance on\ntasks like classification, clustering or retrieval.\n1Note: (i) wii = wjj = 1 ; (ii) wij = wji from symmetry of\nW ; (iii) all elements of W are ≤1 hence the | · | can be dropped ;\n5\n",
    "The Balanced-Pairwise-Affinities Feature Transform\nParameterless-ness, Differentiability and Equivariance:\nThese three properties are inherited from the Sinkhorn OT\nsolver. The transform is parameterless, giving it the flexi-\nbility to be used in other pipelines, directly over different\nkinds of embeddings, without the harsh requirement of re-\ntraining the entire pipeline. Retraining is certainly possible,\nand beneficial in many situations, but not mandatory, as our\nexperiments work quite well without it. Also, due to the\ndifferentiability of the Sinkhorn algorithm (Cuturi, 2013),\nback-propagating through BPA can be done naturally, hence\nit is possible to (re-)train the hosting network to adapt to\nBPA, if desired. The embedding works coherently with\nrespect to any change of order of the input items (features).\nThis can be shown by construction, since min-cost max-flow\nsolvers as well as the Sinkhorn OT solver are equivariant\nwith respect to permutations of their inputs.\nUsage flexibility: Recall that BPA is applied on sets of\nfeatures, typically computed by some embedding network\nand its output features are passed to downstream network\ncomponents. Since BPA is parameterless, it can be simply\ninserted to any trained hosting network and since it is differ-\nentiable, it is possible to train the hosting network with BPA\ninside it. We therefore denote by BPAp the basic drop-in\nusage of BPA, inserted into a pretrained network. This is the\neasiest and most flexible way to use BPA, nevertheless show-\ning consistent benefits in the different tested applications.\nWe denote by BPAt the usage where the hosting network\nis trained with BPA within. It allows to adapt the hosting\nnetwork’s parameters to the presence of the transform, with\nthe potential of further improving performance.\nTransductive or Inductive: Note that BPA is a transductive\nmethod in the sense that it needs to jointly process the data,\nbut in doing so, unlike many transductive methods, it does\nnot make any limiting assumptions about the input structure,\nsuch as knowing the number of classes, or items per class.\nIn any case, we consider the BPAp and BPAt variants to be\ntransductive, regardless of the nature of the hosting network.\nNevertheless, being transductive is possibly restrictive for\ncertain tasks, for which test-time inputs might be received\none-by-one. Therefore, we suggest a third usage type, BPAi,\nwhere the hosting network is trained with BPA inside (just\nlike in BPAt), but BPA is not applied at inference (simply\nnot inserted), hence the hosting network remains inductive\nif it was so in the first place.\nDimensionality: BPA has the unique property that the\ndimension of its embedded feature depends on (equals)\nthe number of features in the set. Given a batch of n d-\ndimensional features V ∈Rn×d, it outputs a batch of n\nn-dimensional features W = BPA(V ) ∈Rn×n. On one\nhand, this is a desired property, since it is natural that the\nfeature dimensionality (and capacity) depends on the com-\nplexity of the task, which typically grows with the number\nTable 1. Feature-dimension control strategies: Accuracy on\n5-way 1-shot MiniImagenet. * marks the dimension of original\n640d pre-trained resnet-12 features. # marks the size of a batch that\nincludes a single 5-way 1-shot 15-query task (80 = 5 · (1 + 15)),\nwhich is the output dimension of vanilla BPA. Best and second\nbest results, per dimension, are in Bold and italics.\ninput to ProtoNet / dim.\n5\n10\n20\n40\n80#\n640∗\nV (orignal)\n-\n-\n-\n-\n-\n64.6\nPCA(V )\n66.2\n65.7\n64.4\n64.1\n64.3\n-\nSC(V )\n66.8\n58.2\n46.2\n38.3\n25.5\n-\nBPAp(V )\n-\n-\n-\n-\n71.2\n-\nBPAt(V )\n-\n-\n-\n-\n72.1\n-\nBPAp Attn(V )\n-\n-\n-\n-\n-\n69.1\nBPAt Attn(V )\n-\n-\n-\n-\n-\n70.0\nBPAp Attn(SC(V ))\n69.1\n69.1\n68.1\n68.5\n69.2\n-\nBPAp Attn(PCA(V ))\n67.1\n67.8\n67.5\n67.6\n67.8\n-\nof features (Think of the inter-relations which are more\ncomplex to model). On the other hand, it might impose\na problem in situations at which the downstream calcula-\ntion that follows expects a specific feature dimension, for\nexample with a pre-trained non-convolutional layer.\nIn order to make BPA usable in such cases, we propose\nan attention-like variant, BPA Attn, in which the normal-\nized BPA matrix is used to balance the input features with-\nout changing their dimension, by simple multiplication, i.e.\nBPA Attn(V ) = BPA(V ) · V . This variant allows to main-\ntain the original feature dimension d, or even a smaller\ndimension if desired, by applying dimension reduction on\nthe original set of features prior to applying BPA Attn.\nIn Table 1, we examine few-shot classification accuracy on\nMiniImagenet (Vinyals et al., 2016) with downstream classi-\nfication by ProtoNet (Snell et al., 2017). Each classification\ninstance consists of 80 images, encoded to 640-dimensional\nfeatures by a pre-trained resnet-12 network. ProtoNet works\non either: (i) the original feature set V (ii) its dimension\nreduced versions, calculated by either PCA or Spectral-\nClustering (SC) (iii) vanilla BPA (iv) BPA Attn on original\nor reduced features. As can be observed, the best accuracies\nare achieved by vanilla BPA, but the attention provided by\nBPA is able to stabilize performance across the entire range\nof dimensions.\nHyper-parameters and ablations: BPA has two hyper-\nparameters that were chosen through cross-validation and\nkept fixed for each application over all datasets. The number\nof Sinkhorn iterations for computing the optimal transport\nplan was fixed to 5 and entropy regularization parameter λ\n(Eq. (3.1)) was set to 0.1 for UIC and FSC and to 0.25 for\nReID. In Appendix B we ablate these hyper-parameters as\nwell as the scalability of BPA in terms of set-input size (Fig.\n5) on few-shot-classification, and in Appendix D, we study\nits robustness to noise and feature dimensionality (Fig. 10)\nby a controlled synthetic clustering experiment.\n6\n",
    "The Balanced-Pairwise-Affinities Feature Transform\nTable 2. Few-Shot Classification (FSC) accuracy on MiniIma-\ngenet. Results are ordered by backbone (resnet-12, wrn-28-10, ViT\nsmall/base), each listing baseline methods and BPA variants. BPA\nimprovements (colored percentages) are in comparison with each\nrespective baseline hosting method (obtained by division). Bold\nand italics highlight best and second best results per backbone.\nT/I denotes transductive/inductive methods. (&) from Ziko et al.\n(2020); ($) from original paper; (#) our implementation;\nmethod\nT/I\nnetwork\n5-way 1-shot\n5-way 5-shot\nProtoNet(#)\nI\nResNet\n62.39\n80.33\nDeepEMD($)\nI\nResNet\n65.91\n82.41\nFEAT($)\nI\nResNet\n66.78\n82.05\nRENet($)\nI\nResNet\n67.60\n82.58\nProtoNet-BPAp\nT\nResNet\n67.34 (+7.9%)\n81.84 (+1.6%)\nProtoNet-BPAi\nI\nResNet\n64.36 (+3.1%)\n81.82 (+1.8%)\nProtoNet-BPAt\nT\nResNet\n67.90 (+8.8%)\n83.09 (+3.2%)\nProtoNet(&)\nI\nWRN\n62.60\n79.97\nPTMap($)\nT\nWRN\n82.92\n88.80\nSillNet($)\nT\nWRN\n82.99\n89.14\nPTMap-SF($)\nT\nWRN\n84.81\n90.62\nPTMap-BPAp\nT\nWRN\n83.19 (+0.3%)\n89.56 (+0.9%)\nPTMap-BPAt\nT\nWRN\n84.18 (+1.5%)\n90.51 (+1.9%)\nSillNet-BPAp\nT\nWRN\n83.35 (+0.4%)\n89.65 (+0.6%)\nPTMap-SF-BPAp\nT\nWRN\n85.59 (+0.9%)\n91.34 (+0.8%)\nPMF($)\nI\nViT-s\n93.10\n98.00\nPMF-BPAp\nT\nViT-s\n94.49 (+1.4%)\n97.68 (-0.3%)\nPMF-BPAi\nI\nViT-s\n92.70 (-0.4%)\n98.00 (+0.0%)\nPMF-BPAp\nT\nViT-s\n95.30 (+2.3%)\n97.90 (-0.1%)\nPMF($)\nI\nViT-b\n95.30\n98.40\nPMF-BPAp\nT\nViT-b\n95.90 (+0.6%)\n98.30 (-0.1%)\nPMF-BPAi\nI\nViT-b\n95.20 (-0.1%)\n98.70 (+0.3%)\nPMF-BPAt\nT\nViT-b\n96.3 (+1.0%)\n98.5 (+0.1%)\n4. Results\nIn this section, we experiment with BPA on three applica-\ntions: Few-Shot Classification (Sec. 4.1), Unsupervised\nImage Clustering (Sec. 4.2) and Person Re-Identification\n(Sec. 4.3). In each, we achieve state-of-the-art results, by\nmerely using current state-of-the-art methods as hosting\nnetworks of the BPA transform. Perhaps more importantly,\nwe demonstrate the flexibility and simplicity of applying\nBPA in these setups, with improvements in the entire range\nof testing, including different hosting methods, different\nfeature embeddings of different complexity backbones and\nwhether retraining the hosting network or just dropping-in\nBPA and performing standard inference. To show the sim-\nplicity of inserting BPA into hosting algorithms, we provide\npseudocodes for each of the experiments in Appendix C.\n4.1. Few-Shot Classification (FSC)\nOur main experiment is a comprehensive evaluation on the\nstandard few-shot classification benchmarks MiniImagenet\n(Vinyals et al., 2016) and CIFAR-FS (Bertinetto et al., 2019),\nwith detailed results in Tables 2 and 3 respectively. We\nevaluate the performance of the proposed BPA, applying\nit to a variety of FSC methods including the recent state-\nof-the-art (PTMap (Hu et al., 2020), SillNet (Zhang et al.,\n2021), PTMap-SF (Chen & Wang, 2021) and PMF (Hu\nTable 3. Few-Shot Classification (FSC) accuracy on CIFAR-FS.\nmethod\nT/I\nnetwork\n5-way 1-shot\n5-way 5-shot\nPTMap($)\nT\nWRN\n87.69\n90.68\nSillNet($)\nT\nWRN\n87.73\n91.09\nPTMap-SF($)\nT\nWRN\n89.39\n92.08\nPTMap-BPAp\nT\nWRN\n87.37 (-0.4%)\n91.12 (+0.5%)\nSillNet-BPAp\nT\nWRN\n87.30 (-0.5%)\n91.40 (+0.3%)\nPTMap-SF-BPAp\nT\nWRN\n89.94 (+0.6%)\n92.83 (+0.8%)\nPMF($)\nI\nViT-s\n81.1\n92.5\nPMF-BPAp\nT\nViT-s\n84.7 (+4.4%)\n92.8 (+0.3%)\nPMF-BPAi\nI\nViT-s\n84.80 (+4.5%)\n93.40 (+0.9%)\nPMF-BPAt\nT\nViT-s\n88.90 (+9.6%)\n93.80 (+1.4%)\nPMF($)\nI\nViT-b\n84.30\n92.20\nPMF-BPAp\nT\nViT-b\n88.2 (+4.6%)\n94 (+1.9%)\nPMF-BPAi\nI\nViT-b\n87.10 (+3.3%)\n94.70 (+2.7%)\nPMF-BPAt\nT\nViT-b\n91.00 (+7.9%)\n95.00 (+3.0%)\net al., 2022)) as well as to conventional methods like the\npopular ProtoNet (Snell et al., 2017). While in the Mini-\nImagenet evaluation we include a wide range of methods\nand backbones, in the CIFAR-FS evaluation we focus on the\nstate-of-the-art methods and configurations.\nFor each evaluated ’hosting’ method, we incorporate BPA\ninto the pipeline as follows. Given an FSC instance, we\ntransform the entire set of method-specific feature repre-\nsentations using BPA, in order to better capture relative\ninformation. The rest of the pipeline is resumed, allowing\nfor both inference and training. Note that BPA flexibly fits\ninto the FSC task, with no required knowledge or assump-\ntions regarding the setting (# of ways, shots or queries).\nThe basic ‘drop-in’ BPAp consistently, and in many cases\nalso significantly, improves the hosting method performance,\nincluding state-of-the-art, across all benchmarks and back-\nbones with accuracy improvement of around 3.5% and 1.5%\non the 1- and 5- shot tasks. This improvement without re-\ntraining the embedding backbone shows BPA’s effectiveness\nin capturing meaningful relationships between features in a\nvery general sense. When re-training the hosting network\nwith BPA inside, in an end-to-end fashion, BPAt provides\nfurther improvements, in almost every method, with aver-\nages of 5% and 3% on the 1- and 5- shot tasks.\nWhile most of the leading methods are transductive, our\ninductive version, BPAi, can be seen to steadily improve on\ninductive methods like ProtoNet and PMF, without intro-\nducing transductive inference. This further emphasizes the\ngenerality and applicability of our method.\n4.2. Unsupervised Image Clustering (UIC)\nNext, we evaluate BPA in the unsupervised domain, using\nthe unsupervised image clustering task, with the additional\nchallenge of capturing the relation between features that\nwere learned without labels. To do so, we adopt SPICE (Niu\net al., 2022), a recent method that has shown phenomenal\nsuccess in the field. In SPICE, training is divided into\n3 phases: (i) unsupervised representation learning (using\n7\n",
    "The Balanced-Pairwise-Affinities Feature Transform\nTable 4. Unsupervised Image Clustering (UIC) results on STL-\n10 (Coates et al., 2011), CIFAR-100-20 (Krizhevsky & Hinton,\n2009) and CIFAR-100-20 (Krizhevsky & Hinton, 2009).\nbenchmark\nSTL-10\nCIFAR-10\nCIFAR-100-20\nnetwork\nACC NMI ARI ACC NMI ARI ACC NMI ARI\nk-means\n0.192 0.125 0.061 0.229 0.087 0.049 0.130 0.084 0.028\nDAC\n0.470 0.366 0.257 0.522 0.396 0.306 0.238 0.185 0.088\nDSEC\n0.482 0.403 0.286 0.478 0.438 0.340 0.255 0.212 0.110\nIDFD\n0.756 0.643 0.575 0.815 0.711 0.663 0.425 0.426 0.264\nSPICEs\n0.908 0.817 0.812 0.838 0.734 0.705 0.468 0.448 0.294\nSPICE\n0.938 0.872 0.870 0.926 0.865 0.852 0.538 0.567 0.387\nSPICEs-BPAt 0.912 0.823 0.821 0.880 0.784 0.769 0.494 0.477 0.334\nSPICE-BPAt\n0.943 0.880 0.879 0.933 0.870 0.866 0.550 0.560 0.402\nMoCo (He et al., 2019) over a resnet-34 backbone); (ii)\nclustering-head training, with result termed SPICEs; and\n(iii) a joint training phase (using FixMatch (Sohn et al.,\n2020) over a wrn backbone), result termed SPICE.\nWe insert BPA into phase (ii), clustering-head training, as\nfollows. Given a batch of representations, SPICE assigns\nclass pseudo-labels to the nearest neighbors of the most\nprobable samples (k samples with the highest probability\nper class). In the original work, SPICE uses the dot-product\nof the MoCo features to find the neighbors. Instead, we\ntransform each batch of MoCo features using BPA and\nuse the same dot-product on the resulting informative BPA\nfeatures to find a more reliable set of neighbors. We experi-\nment on 3 standard datasets, STL-10 (Coates et al., 2011),\nCIFAR-10 and CIFAR-100-20 (Krizhevsky & Hinton, 2009),\nwhile keeping all original SPICE implementation hyper-\nparameters unchanged. We report both SPICEs and SPICE\nresults, as in the original work (Niu et al., 2022).\nTable 4 summarizes the experiment, in terms of clustering\nAccuracy (ACC), Normalized Mutual Information (NMI),\nand Adjusted Rand Index (ARI). It is done for the two stages\nof SPICE, with and without BPA, along with several other\nbaselines. The results show a significant improvement of\nSPICEs-BPAt over SPICEs (just by applying BPA to the\nlearned features), with an average increase of 5% in NMI\nand 8% in ARI. The advantage brought by the insertion\nof BPA carries on to the joint-processing stage (BPAt over\nSPICEs), though with a smaller average increase of 0.1%\nin NMI and 2.2% in ARI, leading to new state-of-the-art\nresults on these datasets. These results demonstrate the\nrelevance of BPA to unsupervised feature learning setups\nand its possible potential to other applications in this area.\n4.3. Person Re-Identification (Re-ID)\nWe explore the application of BPA to large-scale instances\nand datasets by considering the person re-identification task\n(Ye et al., 2021). Given a set of query images and a large\nset of gallery images, the task is to rank the similarities of\neach query against the entire gallery. This is typically done\nby learning specialized image features that are compared\nTable 5. Image Re-Identification (Re-ID) results on CUHK03\n(Li et al., 2014) and Market-1501 (Zheng et al., 2015).\nbenchmark\nCUHK03-det\nCUHK03-lab\nMarket-1501\nnetwork\nmAP\nRank-1\nmAP\nRank-1\nmAP\nRank-1\nMHN\n65.4\n71.7\n72.4\n77.2\n85.0\n95.1\nSONA\n76.3\n79.1\n79.2\n81.8\n88.6\n95.6\nOSNet\n67.8\n72.3\n–\n–\n84.9\n94.8\nPyramid\n74.8\n78.9\n76.9\n81.8\n88.2\n95.7\nTDB\n72.9\n75.7\n75.6\n77.7\n85.7\n94.3\nTDBRK\n87.1\n87.1\n89.1\n89.0\n94.0\n95.3\nTDB-BPAp\n77.9\n80.4\n80.4\n82.6\n88.1\n94.4\nTDBRK-BPAp\n87.9\n88.0\n89.5\n89.8\n94.0\n95.0\nby Euclidean distances. BPA is used to replace such pre-\ncomputed image features, by a well balanced representation\nwith strong relative information, that is jointly computed\nover the union of query and gallery features. BPA is applied\non pre-trained TopDBNet (Quispe & Pedrini, 2020) resnet-\n50 features and tested on the large-scale ReID benchmarks\nCUHK03 (Li et al., 2014) (both ’detected’ and ‘labeled’) as\nwell as the Market-1501 (Zheng et al., 2015) set, reporting\nmAP (mean Average Precision) and Rank-1 metrics.\nIn Table 5, TDB and TDBRK are shorthands for using\nTopDBNet features, before and after re-ranking (Zhong\net al., 2017). There is a consistent benefit in applying BPA\nto these state-of-the-art features, prior to the distance com-\nputations, with a significant average increase of over 5% in\nmAP and 4% in Rank-1 prior to re-ranking and a modest\nincrease of 0.5% in both measures after ranking. These re-\nsults demonstrate that BPA can handle large-scale instances\n(with thousands of features) and successfully improve per-\nformance measures in such retrieval oriented tasks.\n5. Conclusions, Limitations and Future Work\nWe presented a novel feature-embedding approach for set-\ninput grouping-related tasks such as clustering, classifica-\ntion and retrieval. The proposed BPA feature-set transform\nis non-parametric, differentiable, efficient, easy to use and\nis shown to capture complex relations between the set-input\nitems. Applying BPA to the tasks of few-shot-classification,\nunsupervised-image-clustering and person-re-identification,\nwhether by insertion into a pre-trained network or by re-\ntraining the hosting network, has shown across-the-board\nimprovements, setting new state-of-the-art results.\nIn future work, we plan to address current limitations and\nexplore potential extensions. BPA is currently limited to pro-\nducing features that represent relative information, within\nthe set-items. It could possibly be applied to tokens (e.g.\npatches) of a single item (e.g. image), similar to trans-\nformers, perhaps dropping the equivariance property and\nutilizing spatial encoding, to improve non-relative represen-\ntations. In addition, it could be useful for guiding contrastive\nself-supervised learning, where embeddings are trained by\nrelative information of augmented views.\n8\n",
    "The Balanced-Pairwise-Affinities Feature Transform\nReferences\nAsano, Y., Rupprecht, C., and Vedaldi, A. Self-labelling via\nsimultaneous clustering and representation learning. In\nInternational Conference on Learning Representations\n(ICLR), 2020.\nBertinetto, L., Henriques, J. F., Torr, P., and Vedaldi, A.\nMeta-learning with differentiable closed-form solvers. In\nInternational Conference on Learning Representations\n(ICLR), 2019.\nCaron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P.,\nand Joulin, A. Unsupervised learning of visual features\nby contrasting cluster assignments. Advances in neural\ninformation processing systems (NeurIPS), 2020.\nChang, J., Wang, L., Meng, G., Xiang, S., and Pan, C. Deep\nadaptive image clustering. In Proceedings of the IEEE\nInternational Conference on Computer Vision (ICCV),\n2017.\nChen, W.-Y., Liu, Y.-C., Kira, Z., Wang, Y.-C. F., and Huang,\nJ.-B. A closer look at few-shot classification. In Interna-\ntional Conference on Learning Representations (ICLR),\n2018.\nChen, X. and Wang, G. Few-shot learning by integrating\nspatial and frequency representation. arXiv:2105.05348,\n2021.\nCoates, A., Ng, A., and Lee, H. An analysis of single-\nlayer networks in unsupervised feature learning. In Pro-\nceedings of the fourteenth international conference on\nartificial intelligence and statistics, 2011.\nCuturi, M. Sinkhorn distances: Lightspeed computation\nof optimal transport. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2013.\nDhillon, G. S., Chaudhari, P., Ravichandran, A., and Soatto,\nS. A baseline for few-shot image classification. In Inter-\nnational Conference on Learning Representations (ICLR),\n2020.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,\nM., Heigold, G., Gelly, S., et al. An image is worth\n16x16 words: Transformers for image recognition at scale.\narXiv:2010.11929, 2020.\nFey, M., Lenssen, J. E., Morris, C., Masci, J., and\nKriege, N. M.\nDeep graph matching consensus.\narXiv:2001.09621, 2020.\nFinn, C., Abbeel, P., and Levine, S. Model-agnostic meta-\nlearning for fast adaptation of deep networks. In Interna-\ntional Conference on Machine Learning (ICML), 2017.\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo-\nmentum contrast for unsupervised visual representation\nlearning. arXiv:1911.05722, 2019.\nHu, S. X., Li, D., St¨uhmer, J., Kim, M., and Hospedales,\nT. M. Pushing the limits of simple pipelines for few-shot\nlearning: External data and fine-tuning make a difference.\nIn Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition (CVPR), 2022.\nHu, Y., Gripon, V., and Pateux, S. Leveraging the fea-\nture distribution in transfer-based few-shot learning. In\narXiv:2006.03806, 2020.\nHuang, G., Larochelle, H., and Lacoste-Julien, S.\nAre\nfew-shot learning benchmarks too simple? solving them\nwithout task supervision at test-time. arXiv:1902.08605,\n2019.\nKang, D., Kwon, H., Min, J., and Cho, M. Relational\nembedding for few-shot classification. In Proceedings\nof the IEEE/CVF International Conference on Computer\nVision (ICCV), 2021.\nKhan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan,\nF. S., and Shah, M. Transformers in vision: A survey.\narXiv:2101.01169, 2021.\nKolkin, N., Salavon, J., and Shakhnarovich, G. Style trans-\nfer by relaxed optimal transport and self-similarity. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2019.\nKorman, S. and Avidan, S. Coherency sensitive hashing.\nIEEE Transactions on Pattern Analysis and Machine In-\ntelligence (PAMI), 2015.\nKrizhevsky, A. and Hinton, G. Learning multiple layers of\nfeatures from tiny images. 2009.\nKuhn, H. W. The hungarian method for the assignment\nproblem. Naval Research Logistics Quarterly, 2, 1955.\nLee, J., Lee, Y., Kim, J., Kosiorek, A., Choi, S., and Teh,\nY. W. Set transformer: A framework for attention-based\npermutation-invariant neural networks. In International\nConference on Machine Learning (ICML), 2019.\nLi, W., Zhao, R., Xiao, T., and Wang, X. Deepreid: Deep\nfilter pairing neural network for person re-identification.\nIn Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2014.\nMaron, H., Litany, O., Chechik, G., and Fetaya, E. On\nlearning sets of symmetric elements. In International\nConference on Machine Learning (ICML), 2020.\n9\n",
    "The Balanced-Pairwise-Affinities Feature Transform\nNg, A., Jordan, M., and Weiss, Y. On spectral clustering:\nAnalysis and an algorithm. Advances in neural informa-\ntion processing systems (NeurIPS), 2001.\nNiu, C., Shan, H., and Wang, G. Spice: Semantic pseudo-\nlabeling for image clustering. IEEE Transactions on\nImage Processing (TIP), 2022.\nPearson, K. On lines and planes of closest fit to systems\nof points in space. The London, Edinburgh, and Dublin\nphilosophical magazine and journal of science, 1901.\nQi, C. R., Su, H., Mo, K., and Guibas, L. J. Pointnet:\nDeep learning on point sets for 3d classification and seg-\nmentation. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2017.\nQuispe, R. and Pedrini, H. Top-db-net: Top dropblock\nfor activation enhancement in person re-identification.\nInternational Conference on Pattern Recognition (ICPR),\n2020.\nRamachandran, P., Parmar, N., Vaswani, A., Bello, I., Lev-\nskaya, A., and Shlens, J. Stand-alone self-attention in\nvision models. Advances in Neural Information Process-\ning Systems (NeurIPS), 2019.\nRavi, S. and Larochelle, H. Optimization as a model for few-\nshot learning. In International Conference on Learning\nRepresentations (ICLR), 2017.\nSander, M. E., Ablin, P., Blondel, M., and Peyr´e, G. Sink-\nformers: Transformers with doubly stochastic attention.\nIn International conference on artificial intelligence and\nstatistics (AISTATS). PMLR, 2022.\nSantoro, A., Raposo, D., Barrett, D. G., Malinowski, M.,\nPascanu, R., Battaglia, P., and Lillicrap, T. A simple\nneural network module for relational reasoning. Advances\nin Neural Information Processing Systems (NeurIPS),\n2017.\nSarlin, P.-E., DeTone, D., Malisiewicz, T., and Rabinovich,\nA. Superglue: Learning feature matching with graph\nneural networks. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), 2020.\nShi, J. and Malik, J. Normalized cuts and image segmenta-\ntion. IEEE Transactions on pattern analysis and machine\nintelligence (PAMI), 2000.\nSnell, J., Swersky, K., and Zemel, R. Prototypical networks\nfor few-shot learning. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2017.\nSohn, K., Berthelot, D., Li, C.-L., Zhang, Z., Carlini, N.,\nCubuk, E. D., Kurakin, A., Zhang, H., and Raffel, C.\nFixmatch: Simplifying semi-supervised learning with\nconsistency and confidence. arXiv:2001.07685, 2020.\nVan Gansbeke, W., Vandenhende, S., Georgoulis, S., Proes-\nmans, M., and Van Gool, L. Scan: Learning to classify\nimages without labels. In European Conference on Com-\nputer Vision (ECCV), 2020.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-\ntion is all you need. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2017.\nVinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K.,\nand Wierstra, D. Matching networks for one shot learning.\nIn Proceedings of the 30th International Conference on\nNeural Information Processing Systems (NeurIPS), 2016.\nWang, J., Zhang, Z., Chen, M., Zhang, Y., Wang, C., Sheng,\nB., Qu, Y., and Xie, Y.\nOptimal transport for label-\nefficient visible-infrared person re-identification. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV), 2022.\nWang, R., Yan, J., and Yang, X. Learning combinatorial\nembedding networks for deep graph matching. In Pro-\nceedings of the IEEE/CVF international conference on\ncomputer vision (ICCV), 2019.\nXie, Y., Dai, H., Chen, M., Dai, B., Zhao, T., Zha, H.,\nWei, W., and Pfister, T. Differentiable top-k with optimal\ntransport. Advances in Neural Information Processing\nSystems (NeurIPS), 2020.\nYe, H.-J., Hu, H., Zhan, D.-C., and Sha, F. Few-shot learning\nvia embedding adaptation with set-to-set functions. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2020.\nYe, M., Shen, J., Lin, G., Xiang, T., Shao, L., and Hoi,\nS. C. Deep learning for person re-identification: A survey\nand outlook. IEEE Transactions on Pattern Analysis and\nMachine Intelligence (PAMI), 2021.\nZaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B.,\nSalakhutdinov, R. R., and Smola, A. J.\nDeep sets.\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\nZass, R. and Shashua, A. Doubly stochastic normalization\nfor spectral clustering. Advances in neural information\nprocessing systems (NeurIPS), 2006.\nZhang, C., Cai, Y., Lin, G., and Shen, C. Deepemd: Few-\nshot image classification with differentiable earth mover’s\ndistance and structured classifiers. In IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), 2020.\n10\n",
    "The Balanced-Pairwise-Affinities Feature Transform\nZhang, H., Cao, Z., Yan, Z., and Zhang, C. Sill-net: Feature\naugmentation with separated illumination representation.\narXiv:2102.03539, 2021.\nZheng, L., Shen, L., Tian, L., Wang, S., Wang, J., and Tian,\nQ. Scalable person re-identification: A benchmark. In\n2015 IEEE International Conference on Computer Vision\n(ICCV), 2015.\nZhong, Z., Zheng, L., Cao, D., and Li, S.\nRe-ranking\nperson re-identification with k-reciprocal encoding. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2017.\nZhou, K., Yang, Y., Cavallaro, A., and Xiang, T. Omni-\nscale feature learning for person re-identification. In\nProceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV), 2019.\nZhu, H. and Koniusz, P. Ease: Unsupervised discriminant\nsubspace learning for transductive few-shot learning. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2022.\nZiko, I. M., Dolz, J., Granger, E., and Ayed, I. B. Laplacian\nregularized few-shot learning. In International Confer-\nence on Machine Learning (ICML), 2020.\nAppendix\nThe Appendix includes the following sections:\nA. PyTorch-style BPA Implementation\nB. Ablation Studies\nC. BPA Insertion into Hosting Algorithms\nD. Clustering on the Sphere - a Case Study\nA. PyTorch-style BPA Implementation\nWe provide in Algorithm 1 a PyTorch Style implementation\nthat fully aligns with the description in the paper as well as\nwith our actual implementation that was used to execute all\nof the experiments. In Appendix C we further demonstrate\nthe ”insertions” of BPA into hosting methods, for each of\nour three main applications.\nNotice mainly that: (i) The transform can easily be dropped-\nin, using the simple one-line call: X = BPA(X). (ii) It is fully\ndifferentiable (as Sinkhorn and the other basic operations\nare). (iii) The transform does not need to know (or even\nassume) anything about the number of features, their dimen-\nsion, or distribution statistics among classes (e.g. whether\nbalanced or not).\nIt follows the simple steps of: (i) Computing Euclidean self\npairwise distances (using cosine similarities between unit\nnormalized input features); (ii) Avoiding self-matching by\nplacing infinity values on the distances matrix diagonal; (iii)\nApplying a standard Sinkhorn procedure, given the distance\nmatrix and the only 2 (hyper-) parameters with their fixed\nvalues: entropy regularization parameter λ and the number\nof row/col iterative normalization steps. Note that Sinkhorn\ndefaultly maps between source and target vectors of ones;\n(iv) Restoring the perfect self-matching probabilities of one,\nalong the diagonal.\nB. Ablation Studies\nB.1. Scalability (accuracy, runtime vs. input size)\nBeing a transductive module, the accuracy and efficiency\nof the BPA transform depend on the number of inputs that\nare processed as a batch. Recall that BPA is a drop-in\naddition that usually follows feature extraction and precedes\nfurther computation - e.g. k-means for clustering, or (often\ntransductive) layers in FSC and ReID.\nThe ReID experiment is a good stress-test for BPA, since\nwe achieve excellent results for batch sizes of up to ∼15K\nimage descriptors. In terms of runtime, although BPA’s\ncomplexity is quadratic in sample size, its own (self) runtime\nis empirically negligible compared to that of the processing\nthat follows, in all applications tested.\nTypical FSC tasks sizes ((shots+queries)·ways) are small:\n100 = (5 + 15) · 5 at the largest. To concretely address this\nmatter, we test a resnet-12 PTMap-BPAp on large-scale FSC,\nfollowing (Dhillon et al., 2020), on the Tiered-Imagenet\ndataset and report accuracy for 1/5/10-shot (15-query) tasks\nfor an increasing range of ways. The results, shown in\nFig. 5, show that: (i) Total runtime, where BPA is only a\nsmall contributor (compare black vs. yellow dashed line),\nincreases gracefully (notice log10 x-axis) even for extremely\nlarge FSC tasks of 4000 = (10 + 15) · 160 images; (ii) Our\naccuracy scales as expected - following the observation in\nAlgorithm 1 BPA transform on a set of n features.\ninput: n × d matrix V\noutput: n × n matrix W\ndef BPA(V):\n# compute self pairwise-distances\nD = 1 - pwise cosine sim(V/V.norm())\n# infinity self-distances on diagonal\nD inf = D.fill diagonal(10e9)\n# compute optimal transport plan\nW = Sinkhorn(D inf,lambda=.1,iters=5)\n# stretch affinities to [0,1]\nW = W/W.max()\n# self-affinity on diagonal to 1\nreturn W.fill diagonal(1)\n11\n",
    "The Balanced-Pairwise-Affinities Feature Transform\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\nWays (Log10)\n0\n20\n40\n60\n80\n100\nAccuracy (%)\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\nRunning time per batch (sec)\nRunning time PT-MAP-BPA\nRunning time PT-MAP\n1 Shot\n5 Shot\n10 Shot\nFigure 5. BPA scaling in terms of accuracy and efficiency.\n(Dhillon et al., 2020) that it changes logarithmically with\nways (straight line in log-scale).\nB.2. Sinkhorn Iterations\nIn Table 6 we ablate the number of normalization iterations\nin the Sinkhorn-Knopp (SK) (Cuturi, 2013) algorithm at\ntest-time. We measured accuracy on the validation set of\nMiniImagenet (Vinyals et al., 2016), using ProtoNet-BPAp\n(which is the non-fine-tuned drop-in version of BPA within\nProtoNet (Snell et al., 2017)). As was reported in prior\nworks following (Cuturi, 2013), we empirically observe that\na very small number of iterations provide rapid convergence,\nwith diminishing return for higher numbers of iterations.\nWe observed similar behavior for other hosting methods,\nand therefore chose to use a fixed number of 5 iterations\nthroughout the experiments.\nTable 6. Sinkhorn iterations ablation study: See text for details.\nmethod\niters 5-way 1-shot 5-way 5-shot\nProtoNet-BPAp\n1\n70.71\n83.79\nProtoNet-BPAp\n2\n71.10\n84.01\nProtoNet-BPAp\n4\n71.18\n84.08\nProtoNet-BPAp\n8\n71.20\n84.10\nProtoNet-BPAp\n16\n71.20\n84.10\nB.3. Sinkhorn Entropy Regularization λ\nWe measured the impact of using different values of the\noptimal-transport entropy regularization parameter λ (the\nmain parameter of the Sinkhorn algorithm) on a variety of\nconfigurations (ways and shots) in Few-Shot-Classification\n(FSC) on MiniImagenet (Vinyals et al., 2016) in Fig. 6 as\nwell as on the Person-Re-Identification (RE-ID) experiment\non Market-1501 (Zheng et al., 2015) in Fig. 7. In both cases,\nthe ablation was executed on the validation set.\nFor FSC, in Fig. 6, the top plot shows that the effect of the\nchoice of λ is similar across tasks with a varying number of\nways. The bottom plot shows the behavior as a function of\nλ across multiple shot-values, where the optimal value of λ\nFigure 6. Ablation of entropy regularization parameter λ using\nthe Few-Shot-Classification (FSC) task: Considering different\n‘ways’ (top), and different ‘shots’ (bottom). See text for details.\ncan be seen to have a certain dependence on the number of\nshots. Recall that we chose to use a fixed value of λ = 0.1,\nwhich gives an overall good accuracy trade-off. Note that a\nfurther improvement could be achieved by picking the best\nvalues for the particular cases. Notice also the log-scale of\nthe x-axes to see that performance is rather stable around\nthe chosen value.\nFor Re-ID, in Fig. 7, we experiment with a range of λ\nvalues on the validation set of the Market-1501 dataset. The\nresults (shown both for mAP and rank-1 measures) reveal a\nstrong resemblance to those of the FSC experiment in Fig. 6,\nhowever, the optimal choices for λ are slightly higher, which\nis consistent with the dependence on the shots number, since\nthe re-ID tasks are typically large ones. We found that a\nvalue of λ = 0.25 gives good results across both datasets.\nB.4. BPA vs. Naive Baselines\nIn Fig. 8, we ablate different simple alternatives to BPA,\nwith the PTMap (Hu et al., 2020) few-shot-classifier as the\n’hosting’ method, using MiniImagenet (Vinyals et al., 2016).\nEach result is the average of 100 few-shot episodes, using\na WRN-28-10 backbone feature encoder. In blue is the\n12\n",
    "The Balanced-Pairwise-Affinities Feature Transform\nFigure 7. Ablation of entropy regularization parameter λ us-\ning the Person-Re-Identification (Re-ID) task. Accuracy vs. λ,\nusing the validation set of Market-1501 (Zheng et al., 2015) and\nconsidering both mAP and Rank-1 measures. See text for details.\nbaseline of applying no transform at all, using the original\nfeatures. In orange - using BPA. In gray and yellow, respec-\ntively, are other naive ways of transforming the features,\nwhere the affinity matrix is only row-normalized (’softmax’)\nor not normalized at all (’cosine’) before taking its rows as\nthe output features. It is empirically evident that only BPA\noutperforms the baseline consistently, which is due to the\nproperties that we had proved regarding the transform.\nC. BPA Insertion into Hosting Algorithms\nC.1. PTMap (Hu et al., 2020) (Few-Shot Classification)\nWe present the pseudo-code for utilizing BPA within the\nPTMap pipeline, as outlined in Alg. C.1. The only alteration\nfrom the original implementation pertains to row 5, wherein\nthe support and query sets are concatenated and transformed\nusing BPA. This approach can be extended to a wide range\nof distance-based methodologies, thus providing a simple\nFigure 8. Comparison of BPA to different baselines over differ-\nent configurations in few-shot learning tasks over MiniImagenet\n(Vinyals et al., 2016). Created by measuring accuracy (y-axis)\nover a varying number of shots (x-axis), with fixed 5-ways and\n15-queries. See text for details.\nand versatile solution to a variety of applications.\nAlgorithm 2 PTMap training and inference\ninputs: xs, xq # support, query images\nℓs, (ℓq) # support, (query) labels\nfϕ\n# pre-trained embedding network\nfs = fϕ(xs), fq = fϕ(xq) # extract features\n(fs ∪fq) =BPA(fs ∪fq) # BPA transformed features\ncj = 1\ns · P\nf∈fs,ℓs(f)=j f, ∀j # init class centers\nrepeat:\n· Lij = ∥fi −cj∥2, ∀i, fi∈fq # feature-center dists\n· M = Sinkhorn(L, λ) # S-horn soft assignments\n· cj ←cj + α(g(M, j) −cj), ∀j # update centers\nˆℓq(fi) = arg maxj(M[i, j]) # prediction per fi∈fq\nif inference:\nreturn ˆℓq # query predictions\nelse (training):\nupdate fϕ by ∇ϕC-Entropy(M, ℓq) # grad-desc.\nC.2. SPICE (Niu et al., 2022) (Unsupervised Clustering)\nIn our implementation of SPICE, as detailed in the paper,\nwe utilize BPA during phase 2 of the algorithm (clustering-\nhead training). Specifically, as depicted in Alg. C.2, we\ntransform the features using BPA, batch-wise, before con-\nducting a nearest-neighbor search. Afterwards, we retrieve\nthe pseudo-labels and resume with the original features, as\nin the original implementation.\nAlgorithm 3 SPICE training\nPhase (i): pre-train embedding network fϕ\nPhase (ii): train clustering network cθ\nrepeat per batch x:\n· f = fϕ(x) # extract features\n· f BPA = BPA(f) # BPA transformed features\n· Find 3 most confident samples per cluster (use f)\n· Compute cluster centers as their means (use f BPA)\n· Find nearest-neighbors of each center (use f BPA)\n· Assign them to the cluster (as pseudo-labels)\n· Use pseudo-labels to train (update) cθ\nPhase (iii): jointly fine-tune fϕ and cθ\nC.3. TopDBNet (Quispe & Pedrini, 2020) (Person ReID)\nFinally, Alg. C.3 illustrates the application of BPA dur-\ning inference in the context of Person ReID. Typically, the\nquery identity search within the gallery involves identifying\nthe nearest sample to each query. In our implementation,\nwe adopt the same methodology, with the additional step\n13\n",
    "The Balanced-Pairwise-Affinities Feature Transform\nof transforming the concatenated set of query and gallery\nfeatures, using the BPA transform prior to the search.\nAlgorithm 4 TopDBNet inference\ninputs: xg, xq # gallery images, query images\nfϕ\n# pre-trained embedding network\n# extract features\nfg = fϕ(xg), fq = fϕ(xq)\n# transform them with BPA\n(fg ∪fq) = BPA(fg ∪fq)\n# return gallery image with closest feature\nreturn argmin\n{j:fj∈fg}\n∥fi −fj∥for every {i : fi ∈fq}\nD. Clustering on the Sphere - a Case Study\nWe demonstrate the effectiveness of BPA using a\ncontrolled synthetically generated clustering experiment,\nwith k = 10 cluster centers that are distributed uniformly at\nrandom on a d-dimensional unit-sphere, and 20 points per\ncluster (200 in total) that are perturbed around the cluster\ncenters by Gaussian noise of increasing standard deviation,\nof up to 0.75, followed by a re-projection back to the sphere\nby dividing each vector by its L2 magnitude. See Fig. 9\nfor a visualization of the 3D case, for several noise STDs.\nFollowing the random data generation, we also apply dimen-\nsionality reduction with PCA to d = 50, if d > 50.\nWe performed the experiment over a logarithmic 2D grid\nof combinations of data dimensionalities d in the range\n[10, 1234] and Gaussian in-cluster noise STD in the range\n[0.1, 0.75]. Each point is represented by its d-dimensional\ncoordinates vector, where the baseline clustering is obtained\nby running k-means on these location features. In addition,\nwe run k-means on the set of features that has undergone\nBPA. Hence, the benefits of the transform (embedding) are\nmeasured indirectly through the accuracy2 achieved by run-\nning k-means on the embedded vs. original vectors.\nEvaluation results, in terms of Normalized Mutual Informa-\ntion (NMI) and Adjusted Rand Index (ARI), are reported\nin Fig. 10, averaged over 10 runs, as a function of either\ndimensionality (for different noise STDs) or noise STDs\n(for different dimensionalities). The results show (i) general\ngains and robustness to wide ranges of data dimensionality\n(ii) the ability of BPA to find meaningful representations\nthat enable clustering quality to degrade gracefully with the\nincrease in cluster noise level. Note that the levels of noise\nare rather high, as they are relative to a unit radius sphere.\n2Accuracy is measured by comparison with the optimal permu-\ntation of the predicted labels, found by the Hungarian Algorithm\n(Kuhn, 1955).\n14\n",
    "The Balanced-Pairwise-Affinities Feature Transform\nFigure 9. Clustering on the sphere: Data Generation. 10 Random cluster centers on the unit sphere, perturbed by increasing noise STD.\nFigure 10. Clustering on the sphere: Detailed Results. Clustering measures (top: ARI, bottom: NMI) of k-means, using BPA features\n(dashed lines) vs. original features (solid lines). For both measures - the higher the better. Shown over different configurations of feature\ndimensions d (left) and noise levels σ (right).\n15\n"
  ],
  "full_text": "The Balanced-Pairwise-Affinities Feature Transform\nDaniel Shalam 1 Simon Korman 1\nAbstract\nThe Balanced-Pairwise-Affinities (BPA) feature\ntransform is designed to upgrade the features of a\nset of input items to facilitate downstream match-\ning or grouping related tasks. The transformed\nset encodes a rich representation of high order\nrelations between the input features. A particu-\nlar min-cost-max-flow fractional matching prob-\nlem, whose entropy regularized version can be\napproximated by an optimal transport (OT) op-\ntimization, leads to a transform which is effi-\ncient, differentiable, equivariant, parameterless\nand probabilistically interpretable.\nWhile the\nSinkhorn OT solver has been adapted extensively\nin many contexts, we use it differently by min-\nimizing the cost between a set of features to it-\nself and using the transport plan’s rows as the\nnew representation. Empirically, the transform is\nhighly effective and flexible in its use and con-\nsistently improves networks it is inserted into,\nin a variety of tasks and training schemes. We\ndemonstrate state-of-the-art results in few-shot\nclassification, unsupervised image clustering and\nperson re-identification.\nCode is available at\ngithub.com/DanielShalam/BPA .\n1. Introduction\nIn this work, we reassess the functionality of features in set-\ninput problems, in which a task is defined over a set of items.\nProminent examples of this setting are few-shot classifica-\ntion (Ravi & Larochelle, 2017), clustering (Van Gansbeke\net al., 2020), feature matching (Korman & Avidan, 2015)\nand person re-identification (Ye et al., 2021), to name but a\nfew. In such tasks, features computed at test time are mainly\ncompared relative to one another, and less so to the features\nseen at training time. For such tasks, the practice of learning\na generic feature extractor during training and applying it at\ntest time is sub-optimal.\nIn set-input problems, such as few-shot classification, an\n1Department of Computer Science, University of Haifa, Israel.\nCorrespondence to: Daniel Shalam <dani360@gmail.com>.\ninstance of the task is in the form of a set of n items (e.g.\nimages) {xi}n\ni=1. A generic neural-network pipeline (Fig. 1\nLeft) typically uses a feature embedding (extractor) F, that\nis applied independently to each input item, to obtain a set\nof features V ={vi}n\ni=1={F(xi)}n\ni=1, prior to downstream\ntask-specific processing G (e.g. a clustering head or classi-\nfier). The features V can be of high quality (concise, unique,\ndescriptive), but are limited in representation since they are\nextracted based on knowledge acquired for similar examples\nat train time, with no context of the test time instance they\nare part of, which is critical in set-input tasks.\nWe rather consider the more general framework (Fig. 1\nRight), in which the per-item independently extracted fea-\nture collection V is passed to an attention-mechanism type\ncomputation, in which some transform jointly processes the\nentire set of instance features, re-embedding each feature in\nlight of the joint statistics of the entire instance.\nThe main idea of BPA is very intuitive and is demonstrated\non a toy example in Fig. 2. The embedding of each feature\nwill encode the distribution of its affinities to the rest of the\nset items. Specifically, items in the embedded space will be\nclose if and only if they share a similar such distribution, i.e.\n’agree’ on the way they ’see’ the entire set. In fact, the trans-\nform largely discards the item-specific feature information,\nresulting in a purely relative normalized representation that\nresults in a highly efficient embedding with many attractive\nproperties.\nThe proposed transformation can be computed very effi-\nciently, with negligible runtime within the hosting network,\nand can be easily used in different contexts, as can be seen\nin the pseudo-code snippets we provide in Sections A and\nC of the Appendix. The embedding itself is given by rows\nof an optimal-transport (OT) plan matrix, which is the solu-\ntion to a regularized min-cost-max-flow fractional matching\nproblem that is defined over the pairwise (self)-affinities\nmatrix of the features in the set.\nTechnically, it involves the computation of pairwise dis-\ntances and several normalization iterations of a Sinkhorn\n(Cuturi, 2013) algorithm, baring apparent similarities to\nmany related methods based on either Spectral Clustering\n(Ng et al., 2001) that normalize the same affinity matrix),\nattention-mechanisms (Vaswani et al., 2017) that learn fea-\ntures based on a self-affinities matrix perhaps even normal-\n1\narXiv:2407.01467v1  [cs.LG]  25 Jun 2024\n\n\nThe Balanced-Pairwise-Affinities Feature Transform\nFigure 1. Generic designs of networks that act on sets of items. These cover relevant architectures, e.g. for few-shot-classification and\nclustering. Left: A generic network for processing a set of input items typically follows the depicted structure: (i) Each item separately\ngoes through a common feature extractor F. (ii) The set of extracted features is the input to a downstream task processing module G. ;\nRight: A more general structure in which the extracted features undergo a joint processing by a transform T. Our BPA transform (as well\nas other attention mechanisms) is of this type and its high-level design (within the ‘green’ module) is detailed in Fig. 2.\nized (Sander et al., 2022) and other matching (Sarlin et al.,\n2020) or classification (Hu et al., 2020) algorithms where\noptimal-transport plans are computed between source items\nand target items or class centers. However, the most im-\nportant difference and our main novel observation is that\nthe self fractional matching itself (which can be viewed as\na balanced affinity matrix) can serve as a powerful embed-\nding, since the distances in this space (between assignment\nvectors) have explicit interpretations that we explore, which\nare highly beneficial to general grouping based algorithms\nthat are applied to such set-input tasks.\nContribution\nWe propose a parameter-less optimal-transport based feature\ntransform, termed BPA, which can be used as a drop-in addi-\ntion that converts a generic feature extraction scheme to one\nthat is well suited to set-input tasks (e.g. from Figure 1 Left\nto Right). It is analyzed and shown to have the following\nattractive set of qualities. (i) efficiency: having real-time\ninference; (ii) differentiability: allowing end-to-end training\nof the entire ‘embedding-transform-inference’ pipeline of\nFig. 1 Right; (iii) equivariance: ensuring that the embed-\nding works coherently under any order of the input items;\n(iv) probabilistic interpretation: each embedded feature\nwill encode its distribution of affinities to all other features,\nby conforming to a doubly-stochastic constraint; (iv) valu-\nable metrics for the item set: Distances between embedded\nvectors will include both direct and indirect (third-party)\nsimilarity information between input features.\nEmpirically, we show BPA’s flexibility and ease of applica-\ntion to a wide variety of tasks, by incorporating it in leading\nmethods of each type. We test different configurations, such\nas whether the hosting network is pre-trained or re-trained\nwith BPA inside, across different backbones, whether trans-\nductive or inductive. Few-shot-classification is our main ap-\nplication with extensive experimentation on standard bench-\nmarks, testing on unsupervised-image-clustering shows the\npotential of BPA in the unsupervised domain and the person-\nre-identification experiments show how BPA deals with non-\ncurated large-scale tasks. In all three applications, over the\ndifferent setups and datasets, BPA consistently improves its\nhosting methods, achieving new state-of-the-art results.\n2. Relation to Prior Work\n2.1. Related Techniques\nSet-to-Set (or Set-to-Feature) Functions have been devel-\noped to act jointly on a set of items (typically features) and\noutput an updated set (or a single feature), which are used\nfor downstream inference tasks. Deep-Sets (Zaheer et al.,\n2017) formalized fundamental requirements from architec-\ntures that process sets. Point-Net (Qi et al., 2017) presented\nan influential design for learning local and global features on\n3D point-clouds, while Maron et al. (2020) study the design\nof equi/in-variant layers. Unlike BPA, the joint processing\nin these methods is limited, amounting to weight-sharing\nbetween separate processes and joint aggregations.\nAttention Mechanisms. The introduction of Relational\nNetworks (Santoro et al., 2017) and Transformers (Vaswani\net al., 2017) with their initial applications in vision mod-\nels (Ramachandran et al., 2019) have lead to the huge impact\nof Vision Transformers (ViTs) (Dosovitskiy et al., 2020) in\nmany vision tasks (Khan et al., 2021). While BPA can be\nseen as a self-attention module, it is very different, first,\nsince it is parameterless, and hence can work at test-time on\na pre-trained network. In addition, is can provide an explicit\nprobabilistic global interpretation of the instance data.\nSpectral Methods have been widely used as simple trans-\nforms applied on data that needs to undergo grouping or\nsearch based operations, jointly processing the set of fea-\ntures, resulting in a compact and perhaps discriminative\nrepresentation. PCA (Pearson, 1901) provides a joint di-\nmension reduction, which maximally preserves data vari-\nance, but does not necessarily improve feature affinities\nfor downstream tasks. Spectral Clustering (SC) (Shi &\nMalik, 2000; Ng et al., 2001) is the leading non-learnable\nclustering method in use in the field. If we ignore its final\n2\n\n\nThe Balanced-Pairwise-Affinities Feature Transform\nFigure 2. The BPA transform: illustrated on a toy 7 image 3-class MNIST example.\nclustering stage, SC consists of forming a pairwise affinity\nmatrix which is normalized (Zass & Shashua, 2006) before\nextracting its leading eigenvectors, which form the final\nembedding. BPA is also based on normalizing an affinity\nmatrix, but uses this matrix’s rows as embedded features\nand avoids any further spectral decompositions, which are\ncostly and difficult to differentiate through.\nOptimal Transport (OT) problems are directly related to\nmeasuring distances between distributions or sets of features.\nCuturi (2013) popularized the Sinkhorn algorithm which is\na simple, differentiable and fast approximation of entropy-\nregularized OT, which has since been used extensively, for\nclustering (Lee et al., 2019; Asano et al., 2020), few-show-\nclassification (Huang et al., 2019; Ziko et al., 2020; Hu\net al., 2020; Zhang et al., 2021; Chen & Wang, 2021; Zhu\n& Koniusz, 2022), matching (Wang et al., 2019; Fey et al.,\n2020; Sarlin et al., 2020), representation learning (Caron\net al., 2020; Asano et al., 2020), retrieval (Xie et al., 2020),\nperson re-identification (Wang et al., 2022), style-transfer\n(Kolkin et al., 2019) and attention (Sander et al., 2022).\nOur approach also builds on some attractive properties of the\nSinkhorn solver. While our usage of Sinkhorn is extremely\nsimple (see Algorithm 1), it is fundamentally different from\nall other OT usages we are aware of, since: (i) We com-\npute the transport-plan between a set of features and itself -\nnot between feature-sets and label/class-prototypes (Huang\net al., 2019; Ziko et al., 2020; Hu et al., 2020; Zhang et al.,\n2021; Chen & Wang, 2021; Zhu & Koniusz, 2022; Lee et al.,\n2019; Asano et al., 2020; Xie et al., 2020; Wang et al., 2022;\nKolkin et al., 2019), or between two different feature-sets\n(Wang et al., 2019; Fey et al., 2020; Sarlin et al., 2020;\nSander et al., 2022); (ii) While others use the transport-plan\nto obtain distances or associations between features and\nfeatures/classes, we use its own rows as new feature vectors\nfor downstream tasks.\n2.2. Instance-Specific Applications\nFew-Shot Classification (FSC) is a branch of few-shot\nlearning in which a classifier learns to recognize previously\nunseen classes given a limited number of labeled examples.\nIn the meta-learning approach, the training data is split into\ntasks (or episodes) mimicking the test time tasks to which\nthe learner is required to generalize. MAML (Finn et al.,\n2017) “learns to fine-tune” by learning a network initializa-\ntion from which it can quickly adapt to novel classes. In\nProtoNet (Snell et al., 2017), a learner is meta-trained to\npredict query feature classes, based on distances from sup-\nport class-prototypes in the embedding space. The trainable\nversion of BPA can be viewed as a meta-learning algorithm.\nSubsequent works (Chen et al., 2018; Dhillon et al., 2020)\nadvocate using larger and more expressive backbones, em-\nploying transductive inference, which fully exploits the data\nat inference, including unlabeled images. BPA is transduc-\ntive, but does not make assumptions on (nor needs to know)\nthe number of classes (ways) or items per class (shots), as it\nexecutes a general probabilistic grouping action.\nRecently, attention mechanisms were shown to be effective\nfor FSC (Kang et al., 2021; Zhang et al., 2020; Ye et al.,\n2020) and a number of works (Ziko et al., 2020; Huang\net al., 2019; Hu et al., 2020; Zhang et al., 2021; Chen &\nWang, 2021) have adopted Sinkhorn (Cuturi, 2013) as a\nparameterless unsupervised classifier that computes match-\nings between query embeddings and class centers. Sill-Net\n(Zhang et al., 2021) that augments training samples with\nillumination features and PTMap-SF (Chen & Wang, 2021)\nthat proposes DCT-based feature embedding, are both based\non PTMap (Hu et al., 2020). The state-of-the-art PMF (Hu\net al., 2022), proposed a 3 stage pipeline of pre-training on\nexternal data, meta-training with labelled tasks, and fine-\ntuning on unseen tasks. BPA can be incorporated into these\nmethods, immediately after their feature extraction stage.\nUnsupervised Image Clustering (UIC) is the task of\ngrouping related images, without any label information,\ninto representative clusters. Naturally, the ability to measure\nthe similarities among samples is a crucial aspect of UIC.\nRecent methods have achieved tremendous progress in this\ntask, towards closing the gap with supervised counterparts.\nThe leading approaches directly learn to map images to la-\nbels, by constraining the training of an unsupervised classi-\nfication model with different types of indirect loss functions.\nProminent works in this area include DAC (Chang et al.,\n3\n\n\nThe Balanced-Pairwise-Affinities Feature Transform\n2017), which recasts the clustering problem into a binary\npairwise-classification framework and SCAN (Van Gans-\nbeke et al., 2020) which builds on a pre-trained encoder\nthat provides nearest-neighbor based constraints for training\na classifier. The recent state-of-the-art SPICE (Niu et al.,\n2022), is a pseudo-labeling based method, which divides\nthe clustering network into a feature model for measuring\nthe instance-level similarity and a clustering head for identi-\nfying the cluster-level discrepancy.\nPerson Re-Identification (Re-ID) is the task identifying a\ncertain person (identity) between multiple detected pedes-\ntrian images, from different non-overlapping cameras. It\nis challenging due to the scale of the problem and large\nvariation in pose, background and illumination.\nSee Ye et al. (2021) for an excellent comprehensive sur-\nvey on the topic. Among the most popular methods are\nOSNet (Zhou et al., 2019) that developed an efficient small-\nscale network with high performance and DropBlock (Top-\nDB-Net) (Quispe & Pedrini, 2020) which achieved state-of-\nthe-art results by dropping a region block in the feature map\nfor attentive learning. The Re-ID task is typically larger\nscale - querying thousands of identities against a target of\ntens of thousands. Also, the data is much more real-world\ncompared to the carefully curated FSC sets.\n3. The BPA Transform\n3.1. Derivation\nAssume we are given a task instance which consists of an\ninference problem over a set of n items {xi}n\ni=1, where\neach of the items belongs to a space of input items Ω⊆RD.\nThe inference task can be modeled as fθ({xi}n\ni=1), using a\nlearned function fθ, which acts on the set of input items and\nis parameterized by a set of parameters θ. Typically, such\nfunctions combine an initial feature extraction stage that is\napplied independently to each input item, with a subsequent\nstage of (separate or joint) processing of the feature vectors\n(see Fig. 1 Left or Right, respectively).\nThat is, the function fθ takes the form fθ({xi}n\ni=1) =\nGψ({Fϕ(xi)}n\ni=1), where Fϕ is the feature extractor (or\nembedding network) and Gψ is the task inference function,\nparameterized by ϕ and ψ respectively, where θ = ϕ ∪ψ.\nThe feature embedding F : RD →Rd, usually in the form\nof a neural-network (with d ≪D), could be either pre-\ntrained, or trained in the context of the task function f,\nalong with the inference function G.\nFor an input {xi}n\ni=1, let us define the set of embedded\nfeatures {vi}n\ni=1 = {F(xi)}n\ni=1. In the following, we con-\nsider these sets of input vectors and features as real-valued\nrow-stacked matrices X ∈Rn×D and V ∈Rn×d.\nWe suggest a novel re-embedding of the feature set V, using\na transform, that we denote by T, in order to obtain a new\nset of features W = T(V), where W ∈Rn×n. The new\nfeature set W has an explicit probabilistic interpretation,\nwhich is specifically suited for tasks related to classification,\nmatching or grouping of items in the input set X. In par-\nticular, W will be a symmetric, doubly-stochastic matrix\n(non-negative, with rows and columns that sum to 1), where\nthe entry wij (for i ̸= j) encodes the belief that items xi\nand xj belong to the same class or cluster.\nThe proposed transform T : Rn×d →Rn×n (see Fig. 2)\nacts on the original feature set V as follows. It begins by\ncomputing the squared Euclidean pairwise distances matrix\nD, namely, dij = ||vi −vj||2, which can be computed\nefficiently as dij = 2(1 −cos(vi, vj)) = 2(1 −vi · vT\nj ),\nwhen the rows of V are unit normalized. Or in a compact\nform, D = 2(1 −S), where 1 is the all ones n × n matrix\nand S = V · VT is the cosine affinity matrix of V.\nW will be computed as the optimal transport (OT) plan\nmatrix between the n-dimensional all-ones vector 1n and\nitself, under the self cost matrix D∞, which is the distance\nmatrix D with a very (infinitely) large scalar replacing each\nof the entries on its diagonal (which were all zero), that\nenforces the affinities of each feature to distribute among\nthe others. Explicitly, let D∞= D + αI, where α is a very\n(infinitely) large constant and I is the n × n identity matrix.\nW is defined to be the doubly-stochastic matrix that is the\nminimizer of the functional\nW = arg min\nW∈Bn\n⟨D∞, W⟩\n(1)\nwhere Bn is the set (known as the Birkhoff polytope) of\nn × n doubly-stochastic matrices and ⟨·, ·⟩stands for the\nFrobenius (standard) dot-product.\nThis objective can be minimized using simplex or interior\npoint methods with complexity Θ(n3 log n). In practice,\nwe use the highly efficient Sinkhorn-Knopp method (Cu-\nturi, 2013), which is an iterative scheme that optimizes an\nentropy-regularized version of the problem, where each iter-\nation takes Θ(n2). Namely:\nW = arg min\nW∈Bn\n⟨D∞, W⟩−1\nλh(W)\n(2)\nwhere h(W) = −P\ni,j wij log(wij) is the Shannon en-\ntropy of W and λ is the entropy regularization parameter.\nThe transport-plan matrix W that is the minimizer of Equa-\ntion (2) will become the result of our transform, after ’restor-\ning’ perfect affinities on the diagonal (replacing the diagonal\nentries from 0s to 1s) by W = W + I, where I is the n × n\nidentity matrix. Our final set of features is T(V) = W and\neach of its rows is the re-embedding of each of the corre-\nsponding features (rows) in V. The BPA transform is given\n4\n\n\nThe Balanced-Pairwise-Affinities Feature Transform\nFigure 3. The min-cost max-flow perspective: Costs are shown.\nin Algorithm 1 in the appendix, in PyTorch-style pseudo-\ncode. Note that W is symmetric as a result of the symmetry\nof D and its own double-stochasticity. We next explain its\nprobabilistic interpretation.\n3.2. Probabilistic interpretation\nThe optimization problem in Equation (1) can be written\nmore explicitly as follows:\nmin\nW ⟨D∞, W⟩\ns.t.\nW · 1n = WT · 1n = 1n\n(3)\nwhich can be seen to be the same as:\nmin\nW ⟨D, W⟩\ns.t.\nW · 1n = WT · 1n = 1n\nwii = 0\nfor\ni = 1, . . . n\n(4)\nsince the use of the infinite weights on the diagonal of D∞\nis equivalent to using the original D with a constraint of\nzeros along the diagonal of W.\nThe optimization problem in Equation (4) is in fact a frac-\ntional matching instance between the set of n original fea-\ntures and itself. It can be posed as a bipartite-graph min-cost\nmax-flow instance (The problem of finding a min cost flow\nout of all max-flow solutions), as depicted in Fig. 3. The\ngraph has n nodes on each side, representing the original\nfeatures {vi}n\ni=1 (the rows of V). Across the two sides, the\ncost of the edge (vi, vj) is the distance dij and the edges\nof the type (vi, vi) have a cost of infinity (or can simply be\nremoved). Each ‘left’ node is connected to a ’source’ node\nS by an edge of cost 0 and similarly each ’right’ node is\nconnected to a ‘target’ (sink) node T. All edges in the graph\nhave a capacity of 1 and the goal is to find an optimal frac-\ntional self matching, by finding a min-cost max-flow from\nsource to sink. Note that the max-flow can easily be seen to\nbe n, but a min-cost flow is sought among max-flows.\nIn this set-to-itself matching view, each vector vi is fraction-\nally matched to the set of all other vectors V −{vi} based on\nthe pairwise distances, but importantly taking into account\nthe fractional matches of the rest of the vectors in order to\nsatisfy the double-stochasticity constraint. The construction\nconstrains the max flow to have a total outgoing flow of\n1 from each ‘left’ node and a total incoming flow of 1 to\neach ‘right’ node. Therefore, the ith transformed feature\nFigure 4. The (symmetric) embedding matrix W and the abso-\nlute difference between its ith and jth rows.\nwi (ith row of W) is a distribution (non-negative entries,\nsumming to 1), where wii = 0 and wij is the relative belief\nthat features i and j belong to the same ‘class’.\n3.3. Properties\nWe can now point out some important properties of the\nproposed embedding, given by the rows of the matrix W.\nSome of these properties can be observed in the toy 3-class\nMNIST digit example, illustrated in Fig. 2.\nInterpretability of distances in the embedded space: An\nimportant property of our embedding is that each embed-\nded feature encodes its distribution of affinities to all other\nfeatures. In particular, the comparison of embedded vectors\nwi and wj (of items i and j in a set) includes both direct\nand indirect information about the similarity between the\nfeatures. Refer to Figure 4 for a detailed explanation of\nthis property. If we look at the different coordinates k of\nthe absolute difference vector a = |wi −wj|, BPA cap-\ntures (i) direct affinity: For k which is either i or j, it holds\nthat ak = 1 −wij = 1 −wji 1. This amount measures how\nhigh (close to 1) is the mutual belief of features i and j about\none another. (ii) indirect (3rd-party) affinity: For k /∈{i, j},\nwe have ak = |wik −wjk|, which is a comparison of the\nbeliefs of features i and j regarding the (third-party) feature\nk. The double-stochasticity of the transformed feature-set\nensures that the compared vectors are similarly scaled (as\ndistributions, plus 1 on the diagonal) and the symmetry\nfurther enforces the equal relative affinity between pairs.\nAs an example, observe the output features 4 and 5 in Fig. 2,\nthat re-embed the ’green’ features of the digit ’7’ images.\nAs desired, these embedding are close in the target 7D space.\nThe closeness is driven by both their closeness in the original\nspace (coordinates 4 and 5) as well as the agreement on\nspecific large differences from other images. This property\nis responsible for better separation between classes in the\ntarget domain, which leads to improved performance on\ntasks like classification, clustering or retrieval.\n1Note: (i) wii = wjj = 1 ; (ii) wij = wji from symmetry of\nW ; (iii) all elements of W are ≤1 hence the | · | can be dropped ;\n5\n\n\nThe Balanced-Pairwise-Affinities Feature Transform\nParameterless-ness, Differentiability and Equivariance:\nThese three properties are inherited from the Sinkhorn OT\nsolver. The transform is parameterless, giving it the flexi-\nbility to be used in other pipelines, directly over different\nkinds of embeddings, without the harsh requirement of re-\ntraining the entire pipeline. Retraining is certainly possible,\nand beneficial in many situations, but not mandatory, as our\nexperiments work quite well without it. Also, due to the\ndifferentiability of the Sinkhorn algorithm (Cuturi, 2013),\nback-propagating through BPA can be done naturally, hence\nit is possible to (re-)train the hosting network to adapt to\nBPA, if desired. The embedding works coherently with\nrespect to any change of order of the input items (features).\nThis can be shown by construction, since min-cost max-flow\nsolvers as well as the Sinkhorn OT solver are equivariant\nwith respect to permutations of their inputs.\nUsage flexibility: Recall that BPA is applied on sets of\nfeatures, typically computed by some embedding network\nand its output features are passed to downstream network\ncomponents. Since BPA is parameterless, it can be simply\ninserted to any trained hosting network and since it is differ-\nentiable, it is possible to train the hosting network with BPA\ninside it. We therefore denote by BPAp the basic drop-in\nusage of BPA, inserted into a pretrained network. This is the\neasiest and most flexible way to use BPA, nevertheless show-\ning consistent benefits in the different tested applications.\nWe denote by BPAt the usage where the hosting network\nis trained with BPA within. It allows to adapt the hosting\nnetwork’s parameters to the presence of the transform, with\nthe potential of further improving performance.\nTransductive or Inductive: Note that BPA is a transductive\nmethod in the sense that it needs to jointly process the data,\nbut in doing so, unlike many transductive methods, it does\nnot make any limiting assumptions about the input structure,\nsuch as knowing the number of classes, or items per class.\nIn any case, we consider the BPAp and BPAt variants to be\ntransductive, regardless of the nature of the hosting network.\nNevertheless, being transductive is possibly restrictive for\ncertain tasks, for which test-time inputs might be received\none-by-one. Therefore, we suggest a third usage type, BPAi,\nwhere the hosting network is trained with BPA inside (just\nlike in BPAt), but BPA is not applied at inference (simply\nnot inserted), hence the hosting network remains inductive\nif it was so in the first place.\nDimensionality: BPA has the unique property that the\ndimension of its embedded feature depends on (equals)\nthe number of features in the set. Given a batch of n d-\ndimensional features V ∈Rn×d, it outputs a batch of n\nn-dimensional features W = BPA(V ) ∈Rn×n. On one\nhand, this is a desired property, since it is natural that the\nfeature dimensionality (and capacity) depends on the com-\nplexity of the task, which typically grows with the number\nTable 1. Feature-dimension control strategies: Accuracy on\n5-way 1-shot MiniImagenet. * marks the dimension of original\n640d pre-trained resnet-12 features. # marks the size of a batch that\nincludes a single 5-way 1-shot 15-query task (80 = 5 · (1 + 15)),\nwhich is the output dimension of vanilla BPA. Best and second\nbest results, per dimension, are in Bold and italics.\ninput to ProtoNet / dim.\n5\n10\n20\n40\n80#\n640∗\nV (orignal)\n-\n-\n-\n-\n-\n64.6\nPCA(V )\n66.2\n65.7\n64.4\n64.1\n64.3\n-\nSC(V )\n66.8\n58.2\n46.2\n38.3\n25.5\n-\nBPAp(V )\n-\n-\n-\n-\n71.2\n-\nBPAt(V )\n-\n-\n-\n-\n72.1\n-\nBPAp Attn(V )\n-\n-\n-\n-\n-\n69.1\nBPAt Attn(V )\n-\n-\n-\n-\n-\n70.0\nBPAp Attn(SC(V ))\n69.1\n69.1\n68.1\n68.5\n69.2\n-\nBPAp Attn(PCA(V ))\n67.1\n67.8\n67.5\n67.6\n67.8\n-\nof features (Think of the inter-relations which are more\ncomplex to model). On the other hand, it might impose\na problem in situations at which the downstream calcula-\ntion that follows expects a specific feature dimension, for\nexample with a pre-trained non-convolutional layer.\nIn order to make BPA usable in such cases, we propose\nan attention-like variant, BPA Attn, in which the normal-\nized BPA matrix is used to balance the input features with-\nout changing their dimension, by simple multiplication, i.e.\nBPA Attn(V ) = BPA(V ) · V . This variant allows to main-\ntain the original feature dimension d, or even a smaller\ndimension if desired, by applying dimension reduction on\nthe original set of features prior to applying BPA Attn.\nIn Table 1, we examine few-shot classification accuracy on\nMiniImagenet (Vinyals et al., 2016) with downstream classi-\nfication by ProtoNet (Snell et al., 2017). Each classification\ninstance consists of 80 images, encoded to 640-dimensional\nfeatures by a pre-trained resnet-12 network. ProtoNet works\non either: (i) the original feature set V (ii) its dimension\nreduced versions, calculated by either PCA or Spectral-\nClustering (SC) (iii) vanilla BPA (iv) BPA Attn on original\nor reduced features. As can be observed, the best accuracies\nare achieved by vanilla BPA, but the attention provided by\nBPA is able to stabilize performance across the entire range\nof dimensions.\nHyper-parameters and ablations: BPA has two hyper-\nparameters that were chosen through cross-validation and\nkept fixed for each application over all datasets. The number\nof Sinkhorn iterations for computing the optimal transport\nplan was fixed to 5 and entropy regularization parameter λ\n(Eq. (3.1)) was set to 0.1 for UIC and FSC and to 0.25 for\nReID. In Appendix B we ablate these hyper-parameters as\nwell as the scalability of BPA in terms of set-input size (Fig.\n5) on few-shot-classification, and in Appendix D, we study\nits robustness to noise and feature dimensionality (Fig. 10)\nby a controlled synthetic clustering experiment.\n6\n\n\nThe Balanced-Pairwise-Affinities Feature Transform\nTable 2. Few-Shot Classification (FSC) accuracy on MiniIma-\ngenet. Results are ordered by backbone (resnet-12, wrn-28-10, ViT\nsmall/base), each listing baseline methods and BPA variants. BPA\nimprovements (colored percentages) are in comparison with each\nrespective baseline hosting method (obtained by division). Bold\nand italics highlight best and second best results per backbone.\nT/I denotes transductive/inductive methods. (&) from Ziko et al.\n(2020); ($) from original paper; (#) our implementation;\nmethod\nT/I\nnetwork\n5-way 1-shot\n5-way 5-shot\nProtoNet(#)\nI\nResNet\n62.39\n80.33\nDeepEMD($)\nI\nResNet\n65.91\n82.41\nFEAT($)\nI\nResNet\n66.78\n82.05\nRENet($)\nI\nResNet\n67.60\n82.58\nProtoNet-BPAp\nT\nResNet\n67.34 (+7.9%)\n81.84 (+1.6%)\nProtoNet-BPAi\nI\nResNet\n64.36 (+3.1%)\n81.82 (+1.8%)\nProtoNet-BPAt\nT\nResNet\n67.90 (+8.8%)\n83.09 (+3.2%)\nProtoNet(&)\nI\nWRN\n62.60\n79.97\nPTMap($)\nT\nWRN\n82.92\n88.80\nSillNet($)\nT\nWRN\n82.99\n89.14\nPTMap-SF($)\nT\nWRN\n84.81\n90.62\nPTMap-BPAp\nT\nWRN\n83.19 (+0.3%)\n89.56 (+0.9%)\nPTMap-BPAt\nT\nWRN\n84.18 (+1.5%)\n90.51 (+1.9%)\nSillNet-BPAp\nT\nWRN\n83.35 (+0.4%)\n89.65 (+0.6%)\nPTMap-SF-BPAp\nT\nWRN\n85.59 (+0.9%)\n91.34 (+0.8%)\nPMF($)\nI\nViT-s\n93.10\n98.00\nPMF-BPAp\nT\nViT-s\n94.49 (+1.4%)\n97.68 (-0.3%)\nPMF-BPAi\nI\nViT-s\n92.70 (-0.4%)\n98.00 (+0.0%)\nPMF-BPAp\nT\nViT-s\n95.30 (+2.3%)\n97.90 (-0.1%)\nPMF($)\nI\nViT-b\n95.30\n98.40\nPMF-BPAp\nT\nViT-b\n95.90 (+0.6%)\n98.30 (-0.1%)\nPMF-BPAi\nI\nViT-b\n95.20 (-0.1%)\n98.70 (+0.3%)\nPMF-BPAt\nT\nViT-b\n96.3 (+1.0%)\n98.5 (+0.1%)\n4. Results\nIn this section, we experiment with BPA on three applica-\ntions: Few-Shot Classification (Sec. 4.1), Unsupervised\nImage Clustering (Sec. 4.2) and Person Re-Identification\n(Sec. 4.3). In each, we achieve state-of-the-art results, by\nmerely using current state-of-the-art methods as hosting\nnetworks of the BPA transform. Perhaps more importantly,\nwe demonstrate the flexibility and simplicity of applying\nBPA in these setups, with improvements in the entire range\nof testing, including different hosting methods, different\nfeature embeddings of different complexity backbones and\nwhether retraining the hosting network or just dropping-in\nBPA and performing standard inference. To show the sim-\nplicity of inserting BPA into hosting algorithms, we provide\npseudocodes for each of the experiments in Appendix C.\n4.1. Few-Shot Classification (FSC)\nOur main experiment is a comprehensive evaluation on the\nstandard few-shot classification benchmarks MiniImagenet\n(Vinyals et al., 2016) and CIFAR-FS (Bertinetto et al., 2019),\nwith detailed results in Tables 2 and 3 respectively. We\nevaluate the performance of the proposed BPA, applying\nit to a variety of FSC methods including the recent state-\nof-the-art (PTMap (Hu et al., 2020), SillNet (Zhang et al.,\n2021), PTMap-SF (Chen & Wang, 2021) and PMF (Hu\nTable 3. Few-Shot Classification (FSC) accuracy on CIFAR-FS.\nmethod\nT/I\nnetwork\n5-way 1-shot\n5-way 5-shot\nPTMap($)\nT\nWRN\n87.69\n90.68\nSillNet($)\nT\nWRN\n87.73\n91.09\nPTMap-SF($)\nT\nWRN\n89.39\n92.08\nPTMap-BPAp\nT\nWRN\n87.37 (-0.4%)\n91.12 (+0.5%)\nSillNet-BPAp\nT\nWRN\n87.30 (-0.5%)\n91.40 (+0.3%)\nPTMap-SF-BPAp\nT\nWRN\n89.94 (+0.6%)\n92.83 (+0.8%)\nPMF($)\nI\nViT-s\n81.1\n92.5\nPMF-BPAp\nT\nViT-s\n84.7 (+4.4%)\n92.8 (+0.3%)\nPMF-BPAi\nI\nViT-s\n84.80 (+4.5%)\n93.40 (+0.9%)\nPMF-BPAt\nT\nViT-s\n88.90 (+9.6%)\n93.80 (+1.4%)\nPMF($)\nI\nViT-b\n84.30\n92.20\nPMF-BPAp\nT\nViT-b\n88.2 (+4.6%)\n94 (+1.9%)\nPMF-BPAi\nI\nViT-b\n87.10 (+3.3%)\n94.70 (+2.7%)\nPMF-BPAt\nT\nViT-b\n91.00 (+7.9%)\n95.00 (+3.0%)\net al., 2022)) as well as to conventional methods like the\npopular ProtoNet (Snell et al., 2017). While in the Mini-\nImagenet evaluation we include a wide range of methods\nand backbones, in the CIFAR-FS evaluation we focus on the\nstate-of-the-art methods and configurations.\nFor each evaluated ’hosting’ method, we incorporate BPA\ninto the pipeline as follows. Given an FSC instance, we\ntransform the entire set of method-specific feature repre-\nsentations using BPA, in order to better capture relative\ninformation. The rest of the pipeline is resumed, allowing\nfor both inference and training. Note that BPA flexibly fits\ninto the FSC task, with no required knowledge or assump-\ntions regarding the setting (# of ways, shots or queries).\nThe basic ‘drop-in’ BPAp consistently, and in many cases\nalso significantly, improves the hosting method performance,\nincluding state-of-the-art, across all benchmarks and back-\nbones with accuracy improvement of around 3.5% and 1.5%\non the 1- and 5- shot tasks. This improvement without re-\ntraining the embedding backbone shows BPA’s effectiveness\nin capturing meaningful relationships between features in a\nvery general sense. When re-training the hosting network\nwith BPA inside, in an end-to-end fashion, BPAt provides\nfurther improvements, in almost every method, with aver-\nages of 5% and 3% on the 1- and 5- shot tasks.\nWhile most of the leading methods are transductive, our\ninductive version, BPAi, can be seen to steadily improve on\ninductive methods like ProtoNet and PMF, without intro-\nducing transductive inference. This further emphasizes the\ngenerality and applicability of our method.\n4.2. Unsupervised Image Clustering (UIC)\nNext, we evaluate BPA in the unsupervised domain, using\nthe unsupervised image clustering task, with the additional\nchallenge of capturing the relation between features that\nwere learned without labels. To do so, we adopt SPICE (Niu\net al., 2022), a recent method that has shown phenomenal\nsuccess in the field. In SPICE, training is divided into\n3 phases: (i) unsupervised representation learning (using\n7\n\n\nThe Balanced-Pairwise-Affinities Feature Transform\nTable 4. Unsupervised Image Clustering (UIC) results on STL-\n10 (Coates et al., 2011), CIFAR-100-20 (Krizhevsky & Hinton,\n2009) and CIFAR-100-20 (Krizhevsky & Hinton, 2009).\nbenchmark\nSTL-10\nCIFAR-10\nCIFAR-100-20\nnetwork\nACC NMI ARI ACC NMI ARI ACC NMI ARI\nk-means\n0.192 0.125 0.061 0.229 0.087 0.049 0.130 0.084 0.028\nDAC\n0.470 0.366 0.257 0.522 0.396 0.306 0.238 0.185 0.088\nDSEC\n0.482 0.403 0.286 0.478 0.438 0.340 0.255 0.212 0.110\nIDFD\n0.756 0.643 0.575 0.815 0.711 0.663 0.425 0.426 0.264\nSPICEs\n0.908 0.817 0.812 0.838 0.734 0.705 0.468 0.448 0.294\nSPICE\n0.938 0.872 0.870 0.926 0.865 0.852 0.538 0.567 0.387\nSPICEs-BPAt 0.912 0.823 0.821 0.880 0.784 0.769 0.494 0.477 0.334\nSPICE-BPAt\n0.943 0.880 0.879 0.933 0.870 0.866 0.550 0.560 0.402\nMoCo (He et al., 2019) over a resnet-34 backbone); (ii)\nclustering-head training, with result termed SPICEs; and\n(iii) a joint training phase (using FixMatch (Sohn et al.,\n2020) over a wrn backbone), result termed SPICE.\nWe insert BPA into phase (ii), clustering-head training, as\nfollows. Given a batch of representations, SPICE assigns\nclass pseudo-labels to the nearest neighbors of the most\nprobable samples (k samples with the highest probability\nper class). In the original work, SPICE uses the dot-product\nof the MoCo features to find the neighbors. Instead, we\ntransform each batch of MoCo features using BPA and\nuse the same dot-product on the resulting informative BPA\nfeatures to find a more reliable set of neighbors. We experi-\nment on 3 standard datasets, STL-10 (Coates et al., 2011),\nCIFAR-10 and CIFAR-100-20 (Krizhevsky & Hinton, 2009),\nwhile keeping all original SPICE implementation hyper-\nparameters unchanged. We report both SPICEs and SPICE\nresults, as in the original work (Niu et al., 2022).\nTable 4 summarizes the experiment, in terms of clustering\nAccuracy (ACC), Normalized Mutual Information (NMI),\nand Adjusted Rand Index (ARI). It is done for the two stages\nof SPICE, with and without BPA, along with several other\nbaselines. The results show a significant improvement of\nSPICEs-BPAt over SPICEs (just by applying BPA to the\nlearned features), with an average increase of 5% in NMI\nand 8% in ARI. The advantage brought by the insertion\nof BPA carries on to the joint-processing stage (BPAt over\nSPICEs), though with a smaller average increase of 0.1%\nin NMI and 2.2% in ARI, leading to new state-of-the-art\nresults on these datasets. These results demonstrate the\nrelevance of BPA to unsupervised feature learning setups\nand its possible potential to other applications in this area.\n4.3. Person Re-Identification (Re-ID)\nWe explore the application of BPA to large-scale instances\nand datasets by considering the person re-identification task\n(Ye et al., 2021). Given a set of query images and a large\nset of gallery images, the task is to rank the similarities of\neach query against the entire gallery. This is typically done\nby learning specialized image features that are compared\nTable 5. Image Re-Identification (Re-ID) results on CUHK03\n(Li et al., 2014) and Market-1501 (Zheng et al., 2015).\nbenchmark\nCUHK03-det\nCUHK03-lab\nMarket-1501\nnetwork\nmAP\nRank-1\nmAP\nRank-1\nmAP\nRank-1\nMHN\n65.4\n71.7\n72.4\n77.2\n85.0\n95.1\nSONA\n76.3\n79.1\n79.2\n81.8\n88.6\n95.6\nOSNet\n67.8\n72.3\n–\n–\n84.9\n94.8\nPyramid\n74.8\n78.9\n76.9\n81.8\n88.2\n95.7\nTDB\n72.9\n75.7\n75.6\n77.7\n85.7\n94.3\nTDBRK\n87.1\n87.1\n89.1\n89.0\n94.0\n95.3\nTDB-BPAp\n77.9\n80.4\n80.4\n82.6\n88.1\n94.4\nTDBRK-BPAp\n87.9\n88.0\n89.5\n89.8\n94.0\n95.0\nby Euclidean distances. BPA is used to replace such pre-\ncomputed image features, by a well balanced representation\nwith strong relative information, that is jointly computed\nover the union of query and gallery features. BPA is applied\non pre-trained TopDBNet (Quispe & Pedrini, 2020) resnet-\n50 features and tested on the large-scale ReID benchmarks\nCUHK03 (Li et al., 2014) (both ’detected’ and ‘labeled’) as\nwell as the Market-1501 (Zheng et al., 2015) set, reporting\nmAP (mean Average Precision) and Rank-1 metrics.\nIn Table 5, TDB and TDBRK are shorthands for using\nTopDBNet features, before and after re-ranking (Zhong\net al., 2017). There is a consistent benefit in applying BPA\nto these state-of-the-art features, prior to the distance com-\nputations, with a significant average increase of over 5% in\nmAP and 4% in Rank-1 prior to re-ranking and a modest\nincrease of 0.5% in both measures after ranking. These re-\nsults demonstrate that BPA can handle large-scale instances\n(with thousands of features) and successfully improve per-\nformance measures in such retrieval oriented tasks.\n5. Conclusions, Limitations and Future Work\nWe presented a novel feature-embedding approach for set-\ninput grouping-related tasks such as clustering, classifica-\ntion and retrieval. The proposed BPA feature-set transform\nis non-parametric, differentiable, efficient, easy to use and\nis shown to capture complex relations between the set-input\nitems. Applying BPA to the tasks of few-shot-classification,\nunsupervised-image-clustering and person-re-identification,\nwhether by insertion into a pre-trained network or by re-\ntraining the hosting network, has shown across-the-board\nimprovements, setting new state-of-the-art results.\nIn future work, we plan to address current limitations and\nexplore potential extensions. BPA is currently limited to pro-\nducing features that represent relative information, within\nthe set-items. It could possibly be applied to tokens (e.g.\npatches) of a single item (e.g. image), similar to trans-\nformers, perhaps dropping the equivariance property and\nutilizing spatial encoding, to improve non-relative represen-\ntations. In addition, it could be useful for guiding contrastive\nself-supervised learning, where embeddings are trained by\nrelative information of augmented views.\n8\n\n\nThe Balanced-Pairwise-Affinities Feature Transform\nReferences\nAsano, Y., Rupprecht, C., and Vedaldi, A. Self-labelling via\nsimultaneous clustering and representation learning. In\nInternational Conference on Learning Representations\n(ICLR), 2020.\nBertinetto, L., Henriques, J. F., Torr, P., and Vedaldi, A.\nMeta-learning with differentiable closed-form solvers. In\nInternational Conference on Learning Representations\n(ICLR), 2019.\nCaron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P.,\nand Joulin, A. Unsupervised learning of visual features\nby contrasting cluster assignments. Advances in neural\ninformation processing systems (NeurIPS), 2020.\nChang, J., Wang, L., Meng, G., Xiang, S., and Pan, C. Deep\nadaptive image clustering. In Proceedings of the IEEE\nInternational Conference on Computer Vision (ICCV),\n2017.\nChen, W.-Y., Liu, Y.-C., Kira, Z., Wang, Y.-C. F., and Huang,\nJ.-B. A closer look at few-shot classification. In Interna-\ntional Conference on Learning Representations (ICLR),\n2018.\nChen, X. and Wang, G. Few-shot learning by integrating\nspatial and frequency representation. arXiv:2105.05348,\n2021.\nCoates, A., Ng, A., and Lee, H. An analysis of single-\nlayer networks in unsupervised feature learning. In Pro-\nceedings of the fourteenth international conference on\nartificial intelligence and statistics, 2011.\nCuturi, M. Sinkhorn distances: Lightspeed computation\nof optimal transport. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2013.\nDhillon, G. S., Chaudhari, P., Ravichandran, A., and Soatto,\nS. A baseline for few-shot image classification. In Inter-\nnational Conference on Learning Representations (ICLR),\n2020.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,\nM., Heigold, G., Gelly, S., et al. An image is worth\n16x16 words: Transformers for image recognition at scale.\narXiv:2010.11929, 2020.\nFey, M., Lenssen, J. E., Morris, C., Masci, J., and\nKriege, N. M.\nDeep graph matching consensus.\narXiv:2001.09621, 2020.\nFinn, C., Abbeel, P., and Levine, S. Model-agnostic meta-\nlearning for fast adaptation of deep networks. In Interna-\ntional Conference on Machine Learning (ICML), 2017.\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo-\nmentum contrast for unsupervised visual representation\nlearning. arXiv:1911.05722, 2019.\nHu, S. X., Li, D., St¨uhmer, J., Kim, M., and Hospedales,\nT. M. Pushing the limits of simple pipelines for few-shot\nlearning: External data and fine-tuning make a difference.\nIn Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition (CVPR), 2022.\nHu, Y., Gripon, V., and Pateux, S. Leveraging the fea-\nture distribution in transfer-based few-shot learning. In\narXiv:2006.03806, 2020.\nHuang, G., Larochelle, H., and Lacoste-Julien, S.\nAre\nfew-shot learning benchmarks too simple? solving them\nwithout task supervision at test-time. arXiv:1902.08605,\n2019.\nKang, D., Kwon, H., Min, J., and Cho, M. Relational\nembedding for few-shot classification. In Proceedings\nof the IEEE/CVF International Conference on Computer\nVision (ICCV), 2021.\nKhan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan,\nF. S., and Shah, M. Transformers in vision: A survey.\narXiv:2101.01169, 2021.\nKolkin, N., Salavon, J., and Shakhnarovich, G. Style trans-\nfer by relaxed optimal transport and self-similarity. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2019.\nKorman, S. and Avidan, S. Coherency sensitive hashing.\nIEEE Transactions on Pattern Analysis and Machine In-\ntelligence (PAMI), 2015.\nKrizhevsky, A. and Hinton, G. Learning multiple layers of\nfeatures from tiny images. 2009.\nKuhn, H. W. The hungarian method for the assignment\nproblem. Naval Research Logistics Quarterly, 2, 1955.\nLee, J., Lee, Y., Kim, J., Kosiorek, A., Choi, S., and Teh,\nY. W. Set transformer: A framework for attention-based\npermutation-invariant neural networks. In International\nConference on Machine Learning (ICML), 2019.\nLi, W., Zhao, R., Xiao, T., and Wang, X. Deepreid: Deep\nfilter pairing neural network for person re-identification.\nIn Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2014.\nMaron, H., Litany, O., Chechik, G., and Fetaya, E. On\nlearning sets of symmetric elements. In International\nConference on Machine Learning (ICML), 2020.\n9\n\n\nThe Balanced-Pairwise-Affinities Feature Transform\nNg, A., Jordan, M., and Weiss, Y. On spectral clustering:\nAnalysis and an algorithm. Advances in neural informa-\ntion processing systems (NeurIPS), 2001.\nNiu, C., Shan, H., and Wang, G. Spice: Semantic pseudo-\nlabeling for image clustering. IEEE Transactions on\nImage Processing (TIP), 2022.\nPearson, K. On lines and planes of closest fit to systems\nof points in space. The London, Edinburgh, and Dublin\nphilosophical magazine and journal of science, 1901.\nQi, C. R., Su, H., Mo, K., and Guibas, L. J. Pointnet:\nDeep learning on point sets for 3d classification and seg-\nmentation. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2017.\nQuispe, R. and Pedrini, H. Top-db-net: Top dropblock\nfor activation enhancement in person re-identification.\nInternational Conference on Pattern Recognition (ICPR),\n2020.\nRamachandran, P., Parmar, N., Vaswani, A., Bello, I., Lev-\nskaya, A., and Shlens, J. Stand-alone self-attention in\nvision models. Advances in Neural Information Process-\ning Systems (NeurIPS), 2019.\nRavi, S. and Larochelle, H. Optimization as a model for few-\nshot learning. In International Conference on Learning\nRepresentations (ICLR), 2017.\nSander, M. E., Ablin, P., Blondel, M., and Peyr´e, G. Sink-\nformers: Transformers with doubly stochastic attention.\nIn International conference on artificial intelligence and\nstatistics (AISTATS). PMLR, 2022.\nSantoro, A., Raposo, D., Barrett, D. G., Malinowski, M.,\nPascanu, R., Battaglia, P., and Lillicrap, T. A simple\nneural network module for relational reasoning. Advances\nin Neural Information Processing Systems (NeurIPS),\n2017.\nSarlin, P.-E., DeTone, D., Malisiewicz, T., and Rabinovich,\nA. Superglue: Learning feature matching with graph\nneural networks. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), 2020.\nShi, J. and Malik, J. Normalized cuts and image segmenta-\ntion. IEEE Transactions on pattern analysis and machine\nintelligence (PAMI), 2000.\nSnell, J., Swersky, K., and Zemel, R. Prototypical networks\nfor few-shot learning. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2017.\nSohn, K., Berthelot, D., Li, C.-L., Zhang, Z., Carlini, N.,\nCubuk, E. D., Kurakin, A., Zhang, H., and Raffel, C.\nFixmatch: Simplifying semi-supervised learning with\nconsistency and confidence. arXiv:2001.07685, 2020.\nVan Gansbeke, W., Vandenhende, S., Georgoulis, S., Proes-\nmans, M., and Van Gool, L. Scan: Learning to classify\nimages without labels. In European Conference on Com-\nputer Vision (ECCV), 2020.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-\ntion is all you need. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2017.\nVinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K.,\nand Wierstra, D. Matching networks for one shot learning.\nIn Proceedings of the 30th International Conference on\nNeural Information Processing Systems (NeurIPS), 2016.\nWang, J., Zhang, Z., Chen, M., Zhang, Y., Wang, C., Sheng,\nB., Qu, Y., and Xie, Y.\nOptimal transport for label-\nefficient visible-infrared person re-identification. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV), 2022.\nWang, R., Yan, J., and Yang, X. Learning combinatorial\nembedding networks for deep graph matching. In Pro-\nceedings of the IEEE/CVF international conference on\ncomputer vision (ICCV), 2019.\nXie, Y., Dai, H., Chen, M., Dai, B., Zhao, T., Zha, H.,\nWei, W., and Pfister, T. Differentiable top-k with optimal\ntransport. Advances in Neural Information Processing\nSystems (NeurIPS), 2020.\nYe, H.-J., Hu, H., Zhan, D.-C., and Sha, F. Few-shot learning\nvia embedding adaptation with set-to-set functions. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2020.\nYe, M., Shen, J., Lin, G., Xiang, T., Shao, L., and Hoi,\nS. C. Deep learning for person re-identification: A survey\nand outlook. IEEE Transactions on Pattern Analysis and\nMachine Intelligence (PAMI), 2021.\nZaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B.,\nSalakhutdinov, R. R., and Smola, A. J.\nDeep sets.\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\nZass, R. and Shashua, A. Doubly stochastic normalization\nfor spectral clustering. Advances in neural information\nprocessing systems (NeurIPS), 2006.\nZhang, C., Cai, Y., Lin, G., and Shen, C. Deepemd: Few-\nshot image classification with differentiable earth mover’s\ndistance and structured classifiers. In IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), 2020.\n10\n\n\nThe Balanced-Pairwise-Affinities Feature Transform\nZhang, H., Cao, Z., Yan, Z., and Zhang, C. Sill-net: Feature\naugmentation with separated illumination representation.\narXiv:2102.03539, 2021.\nZheng, L., Shen, L., Tian, L., Wang, S., Wang, J., and Tian,\nQ. Scalable person re-identification: A benchmark. In\n2015 IEEE International Conference on Computer Vision\n(ICCV), 2015.\nZhong, Z., Zheng, L., Cao, D., and Li, S.\nRe-ranking\nperson re-identification with k-reciprocal encoding. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2017.\nZhou, K., Yang, Y., Cavallaro, A., and Xiang, T. Omni-\nscale feature learning for person re-identification. In\nProceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV), 2019.\nZhu, H. and Koniusz, P. Ease: Unsupervised discriminant\nsubspace learning for transductive few-shot learning. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2022.\nZiko, I. M., Dolz, J., Granger, E., and Ayed, I. B. Laplacian\nregularized few-shot learning. In International Confer-\nence on Machine Learning (ICML), 2020.\nAppendix\nThe Appendix includes the following sections:\nA. PyTorch-style BPA Implementation\nB. Ablation Studies\nC. BPA Insertion into Hosting Algorithms\nD. Clustering on the Sphere - a Case Study\nA. PyTorch-style BPA Implementation\nWe provide in Algorithm 1 a PyTorch Style implementation\nthat fully aligns with the description in the paper as well as\nwith our actual implementation that was used to execute all\nof the experiments. In Appendix C we further demonstrate\nthe ”insertions” of BPA into hosting methods, for each of\nour three main applications.\nNotice mainly that: (i) The transform can easily be dropped-\nin, using the simple one-line call: X = BPA(X). (ii) It is fully\ndifferentiable (as Sinkhorn and the other basic operations\nare). (iii) The transform does not need to know (or even\nassume) anything about the number of features, their dimen-\nsion, or distribution statistics among classes (e.g. whether\nbalanced or not).\nIt follows the simple steps of: (i) Computing Euclidean self\npairwise distances (using cosine similarities between unit\nnormalized input features); (ii) Avoiding self-matching by\nplacing infinity values on the distances matrix diagonal; (iii)\nApplying a standard Sinkhorn procedure, given the distance\nmatrix and the only 2 (hyper-) parameters with their fixed\nvalues: entropy regularization parameter λ and the number\nof row/col iterative normalization steps. Note that Sinkhorn\ndefaultly maps between source and target vectors of ones;\n(iv) Restoring the perfect self-matching probabilities of one,\nalong the diagonal.\nB. Ablation Studies\nB.1. Scalability (accuracy, runtime vs. input size)\nBeing a transductive module, the accuracy and efficiency\nof the BPA transform depend on the number of inputs that\nare processed as a batch. Recall that BPA is a drop-in\naddition that usually follows feature extraction and precedes\nfurther computation - e.g. k-means for clustering, or (often\ntransductive) layers in FSC and ReID.\nThe ReID experiment is a good stress-test for BPA, since\nwe achieve excellent results for batch sizes of up to ∼15K\nimage descriptors. In terms of runtime, although BPA’s\ncomplexity is quadratic in sample size, its own (self) runtime\nis empirically negligible compared to that of the processing\nthat follows, in all applications tested.\nTypical FSC tasks sizes ((shots+queries)·ways) are small:\n100 = (5 + 15) · 5 at the largest. To concretely address this\nmatter, we test a resnet-12 PTMap-BPAp on large-scale FSC,\nfollowing (Dhillon et al., 2020), on the Tiered-Imagenet\ndataset and report accuracy for 1/5/10-shot (15-query) tasks\nfor an increasing range of ways. The results, shown in\nFig. 5, show that: (i) Total runtime, where BPA is only a\nsmall contributor (compare black vs. yellow dashed line),\nincreases gracefully (notice log10 x-axis) even for extremely\nlarge FSC tasks of 4000 = (10 + 15) · 160 images; (ii) Our\naccuracy scales as expected - following the observation in\nAlgorithm 1 BPA transform on a set of n features.\ninput: n × d matrix V\noutput: n × n matrix W\ndef BPA(V):\n# compute self pairwise-distances\nD = 1 - pwise cosine sim(V/V.norm())\n# infinity self-distances on diagonal\nD inf = D.fill diagonal(10e9)\n# compute optimal transport plan\nW = Sinkhorn(D inf,lambda=.1,iters=5)\n# stretch affinities to [0,1]\nW = W/W.max()\n# self-affinity on diagonal to 1\nreturn W.fill diagonal(1)\n11\n\n\nThe Balanced-Pairwise-Affinities Feature Transform\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\nWays (Log10)\n0\n20\n40\n60\n80\n100\nAccuracy (%)\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\nRunning time per batch (sec)\nRunning time PT-MAP-BPA\nRunning time PT-MAP\n1 Shot\n5 Shot\n10 Shot\nFigure 5. BPA scaling in terms of accuracy and efficiency.\n(Dhillon et al., 2020) that it changes logarithmically with\nways (straight line in log-scale).\nB.2. Sinkhorn Iterations\nIn Table 6 we ablate the number of normalization iterations\nin the Sinkhorn-Knopp (SK) (Cuturi, 2013) algorithm at\ntest-time. We measured accuracy on the validation set of\nMiniImagenet (Vinyals et al., 2016), using ProtoNet-BPAp\n(which is the non-fine-tuned drop-in version of BPA within\nProtoNet (Snell et al., 2017)). As was reported in prior\nworks following (Cuturi, 2013), we empirically observe that\na very small number of iterations provide rapid convergence,\nwith diminishing return for higher numbers of iterations.\nWe observed similar behavior for other hosting methods,\nand therefore chose to use a fixed number of 5 iterations\nthroughout the experiments.\nTable 6. Sinkhorn iterations ablation study: See text for details.\nmethod\niters 5-way 1-shot 5-way 5-shot\nProtoNet-BPAp\n1\n70.71\n83.79\nProtoNet-BPAp\n2\n71.10\n84.01\nProtoNet-BPAp\n4\n71.18\n84.08\nProtoNet-BPAp\n8\n71.20\n84.10\nProtoNet-BPAp\n16\n71.20\n84.10\nB.3. Sinkhorn Entropy Regularization λ\nWe measured the impact of using different values of the\noptimal-transport entropy regularization parameter λ (the\nmain parameter of the Sinkhorn algorithm) on a variety of\nconfigurations (ways and shots) in Few-Shot-Classification\n(FSC) on MiniImagenet (Vinyals et al., 2016) in Fig. 6 as\nwell as on the Person-Re-Identification (RE-ID) experiment\non Market-1501 (Zheng et al., 2015) in Fig. 7. In both cases,\nthe ablation was executed on the validation set.\nFor FSC, in Fig. 6, the top plot shows that the effect of the\nchoice of λ is similar across tasks with a varying number of\nways. The bottom plot shows the behavior as a function of\nλ across multiple shot-values, where the optimal value of λ\nFigure 6. Ablation of entropy regularization parameter λ using\nthe Few-Shot-Classification (FSC) task: Considering different\n‘ways’ (top), and different ‘shots’ (bottom). See text for details.\ncan be seen to have a certain dependence on the number of\nshots. Recall that we chose to use a fixed value of λ = 0.1,\nwhich gives an overall good accuracy trade-off. Note that a\nfurther improvement could be achieved by picking the best\nvalues for the particular cases. Notice also the log-scale of\nthe x-axes to see that performance is rather stable around\nthe chosen value.\nFor Re-ID, in Fig. 7, we experiment with a range of λ\nvalues on the validation set of the Market-1501 dataset. The\nresults (shown both for mAP and rank-1 measures) reveal a\nstrong resemblance to those of the FSC experiment in Fig. 6,\nhowever, the optimal choices for λ are slightly higher, which\nis consistent with the dependence on the shots number, since\nthe re-ID tasks are typically large ones. We found that a\nvalue of λ = 0.25 gives good results across both datasets.\nB.4. BPA vs. Naive Baselines\nIn Fig. 8, we ablate different simple alternatives to BPA,\nwith the PTMap (Hu et al., 2020) few-shot-classifier as the\n’hosting’ method, using MiniImagenet (Vinyals et al., 2016).\nEach result is the average of 100 few-shot episodes, using\na WRN-28-10 backbone feature encoder. In blue is the\n12\n\n\nThe Balanced-Pairwise-Affinities Feature Transform\nFigure 7. Ablation of entropy regularization parameter λ us-\ning the Person-Re-Identification (Re-ID) task. Accuracy vs. λ,\nusing the validation set of Market-1501 (Zheng et al., 2015) and\nconsidering both mAP and Rank-1 measures. See text for details.\nbaseline of applying no transform at all, using the original\nfeatures. In orange - using BPA. In gray and yellow, respec-\ntively, are other naive ways of transforming the features,\nwhere the affinity matrix is only row-normalized (’softmax’)\nor not normalized at all (’cosine’) before taking its rows as\nthe output features. It is empirically evident that only BPA\noutperforms the baseline consistently, which is due to the\nproperties that we had proved regarding the transform.\nC. BPA Insertion into Hosting Algorithms\nC.1. PTMap (Hu et al., 2020) (Few-Shot Classification)\nWe present the pseudo-code for utilizing BPA within the\nPTMap pipeline, as outlined in Alg. C.1. The only alteration\nfrom the original implementation pertains to row 5, wherein\nthe support and query sets are concatenated and transformed\nusing BPA. This approach can be extended to a wide range\nof distance-based methodologies, thus providing a simple\nFigure 8. Comparison of BPA to different baselines over differ-\nent configurations in few-shot learning tasks over MiniImagenet\n(Vinyals et al., 2016). Created by measuring accuracy (y-axis)\nover a varying number of shots (x-axis), with fixed 5-ways and\n15-queries. See text for details.\nand versatile solution to a variety of applications.\nAlgorithm 2 PTMap training and inference\ninputs: xs, xq # support, query images\nℓs, (ℓq) # support, (query) labels\nfϕ\n# pre-trained embedding network\nfs = fϕ(xs), fq = fϕ(xq) # extract features\n(fs ∪fq) =BPA(fs ∪fq) # BPA transformed features\ncj = 1\ns · P\nf∈fs,ℓs(f)=j f, ∀j # init class centers\nrepeat:\n· Lij = ∥fi −cj∥2, ∀i, fi∈fq # feature-center dists\n· M = Sinkhorn(L, λ) # S-horn soft assignments\n· cj ←cj + α(g(M, j) −cj), ∀j # update centers\nˆℓq(fi) = arg maxj(M[i, j]) # prediction per fi∈fq\nif inference:\nreturn ˆℓq # query predictions\nelse (training):\nupdate fϕ by ∇ϕC-Entropy(M, ℓq) # grad-desc.\nC.2. SPICE (Niu et al., 2022) (Unsupervised Clustering)\nIn our implementation of SPICE, as detailed in the paper,\nwe utilize BPA during phase 2 of the algorithm (clustering-\nhead training). Specifically, as depicted in Alg. C.2, we\ntransform the features using BPA, batch-wise, before con-\nducting a nearest-neighbor search. Afterwards, we retrieve\nthe pseudo-labels and resume with the original features, as\nin the original implementation.\nAlgorithm 3 SPICE training\nPhase (i): pre-train embedding network fϕ\nPhase (ii): train clustering network cθ\nrepeat per batch x:\n· f = fϕ(x) # extract features\n· f BPA = BPA(f) # BPA transformed features\n· Find 3 most confident samples per cluster (use f)\n· Compute cluster centers as their means (use f BPA)\n· Find nearest-neighbors of each center (use f BPA)\n· Assign them to the cluster (as pseudo-labels)\n· Use pseudo-labels to train (update) cθ\nPhase (iii): jointly fine-tune fϕ and cθ\nC.3. TopDBNet (Quispe & Pedrini, 2020) (Person ReID)\nFinally, Alg. C.3 illustrates the application of BPA dur-\ning inference in the context of Person ReID. Typically, the\nquery identity search within the gallery involves identifying\nthe nearest sample to each query. In our implementation,\nwe adopt the same methodology, with the additional step\n13\n\n\nThe Balanced-Pairwise-Affinities Feature Transform\nof transforming the concatenated set of query and gallery\nfeatures, using the BPA transform prior to the search.\nAlgorithm 4 TopDBNet inference\ninputs: xg, xq # gallery images, query images\nfϕ\n# pre-trained embedding network\n# extract features\nfg = fϕ(xg), fq = fϕ(xq)\n# transform them with BPA\n(fg ∪fq) = BPA(fg ∪fq)\n# return gallery image with closest feature\nreturn argmin\n{j:fj∈fg}\n∥fi −fj∥for every {i : fi ∈fq}\nD. Clustering on the Sphere - a Case Study\nWe demonstrate the effectiveness of BPA using a\ncontrolled synthetically generated clustering experiment,\nwith k = 10 cluster centers that are distributed uniformly at\nrandom on a d-dimensional unit-sphere, and 20 points per\ncluster (200 in total) that are perturbed around the cluster\ncenters by Gaussian noise of increasing standard deviation,\nof up to 0.75, followed by a re-projection back to the sphere\nby dividing each vector by its L2 magnitude. See Fig. 9\nfor a visualization of the 3D case, for several noise STDs.\nFollowing the random data generation, we also apply dimen-\nsionality reduction with PCA to d = 50, if d > 50.\nWe performed the experiment over a logarithmic 2D grid\nof combinations of data dimensionalities d in the range\n[10, 1234] and Gaussian in-cluster noise STD in the range\n[0.1, 0.75]. Each point is represented by its d-dimensional\ncoordinates vector, where the baseline clustering is obtained\nby running k-means on these location features. In addition,\nwe run k-means on the set of features that has undergone\nBPA. Hence, the benefits of the transform (embedding) are\nmeasured indirectly through the accuracy2 achieved by run-\nning k-means on the embedded vs. original vectors.\nEvaluation results, in terms of Normalized Mutual Informa-\ntion (NMI) and Adjusted Rand Index (ARI), are reported\nin Fig. 10, averaged over 10 runs, as a function of either\ndimensionality (for different noise STDs) or noise STDs\n(for different dimensionalities). The results show (i) general\ngains and robustness to wide ranges of data dimensionality\n(ii) the ability of BPA to find meaningful representations\nthat enable clustering quality to degrade gracefully with the\nincrease in cluster noise level. Note that the levels of noise\nare rather high, as they are relative to a unit radius sphere.\n2Accuracy is measured by comparison with the optimal permu-\ntation of the predicted labels, found by the Hungarian Algorithm\n(Kuhn, 1955).\n14\n\n\nThe Balanced-Pairwise-Affinities Feature Transform\nFigure 9. Clustering on the sphere: Data Generation. 10 Random cluster centers on the unit sphere, perturbed by increasing noise STD.\nFigure 10. Clustering on the sphere: Detailed Results. Clustering measures (top: ARI, bottom: NMI) of k-means, using BPA features\n(dashed lines) vs. original features (solid lines). For both measures - the higher the better. Shown over different configurations of feature\ndimensions d (left) and noise levels σ (right).\n15\n"
}