{
  "filename": "1905.01436v1.pdf",
  "num_pages": 10,
  "pages": [
    "Edge-Labeling Graph Neural Network for Few-shot Learning\nJongmin Kim∗1,3, Taesup Kim2,3, Sungwoong Kim3, and Chang D.Yoo1\n1Korea Advanced Institute of Science and Technology\n2MILA, Universit´e de Montr´eal\n3Kakao Brain\nAbstract\nIn this paper, we propose a novel edge-labeling graph\nneural network (EGNN), which adapts a deep neural net-\nwork on the edge-labeling graph, for few-shot learning.\nThe previous graph neural network (GNN) approaches in\nfew-shot learning have been based on the node-labeling\nframework, which implicitly models the intra-cluster sim-\nilarity and the inter-cluster dissimilarity. In contrast, the\nproposed EGNN learns to predict the edge-labels rather\nthan the node-labels on the graph that enables the evolution\nof an explicit clustering by iteratively updating the edge-\nlabels with direct exploitation of both intra-cluster similar-\nity and the inter-cluster dissimilarity. It is also well suited\nfor performing on various numbers of classes without re-\ntraining, and can be easily extended to perform a transduc-\ntive inference. The parameters of the EGNN are learned\nby episodic training with an edge-labeling loss to obtain a\nwell-generalizable model for unseen low-data problem. On\nboth of the supervised and semi-supervised few-shot image\nclassiﬁcation tasks with two benchmark datasets, the pro-\nposed EGNN signiﬁcantly improves the performances over\nthe existing GNNs.\n1. Introduction\nA lot of interest in meta-learning [1] has been re-\ncently arisen in various areas including especially task-\ngeneralization problems such as few-shot learning [2, 3, 4,\n5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], learn-to-learn [16, 17,\n18], non-stationary reinforcement learning[19, 20, 21], and\ncontinual learning [22, 23].\nAmong these meta-learning\nproblems, few-shot leaning aims to automatically and ef-\nﬁciently solve new tasks with few labeled data based on\nknowledge obtained from previous experiences. This is in\n∗Work done during an internship at Kakao Brain. Correspondence to\nkimjm0309@gmail.com\nFigure 1:\nAlternative node and edge feature update in\nEGNN with edge-labeling for few-shot learning\ncontrast to traditional (deep) learning methods that highly\nrely on large amounts of labeled data and cumbersome man-\nual tuning to solve a single task.\nRecently, there has also been growing interest in graph\nneural networks (GNNs) to handle rich relational structures\non data with deep neural networks [24, 25, 26, 27, 28,\n29, 30, 31, 32, 33, 34]. GNNs iteratively perform a fea-\nture aggregation from neighbors by message passing, and\ntherefore can express complex interactions among data in-\nstances.\nSince few-shot learning algorithms have shown\nto require full exploitation of the relationships between a\nsupport set and a query [2, 3, 5, 10, 11], the use of GNNs\ncan naturally have the great potential to solve the few-shot\nlearning problem.\nA few approaches that have explored\nGNNs for few-shot learning have been recently proposed\n[6, 12]. Speciﬁcally, given a new task with its few-shot sup-\nport set, Garcia and Bruna [6] proposed to ﬁrst construct a\ngraph where all examples of the support set and a query are\ndensely connected. Each input node is represented by the\nembedding feature (e.g. an output of a convolutional neural\nnetwork) and the given label information (e.g. one-hot en-\ncoded label). Then, it classiﬁes the unlabeled query by iter-\natively updating node features from neighborhood aggrega-\ntion. Liu et al. [12] proposed a transductive propagation net-\nwork (TPN) on the node features obtained from a deep neu-\narXiv:1905.01436v1  [cs.LG]  4 May 2019\n",
    "ral network. At test-time, it iteratively propagates one-hot\nencoded labels over the entire support and query instances\nas a whole with a common graph parameter set. Here, it\nis noted that the above previous GNN approaches in few-\nshot learning have been mainly based on the node-labeling\nframework, which implicitly models the intra-cluster simi-\nlarity and inter-cluster dissimilarity.\nOn the contrary, the edge-labeling framework is able to\nexplicitly perform the clustering with representation learn-\ning and metric learning, and thus it is intuitively a more con-\nducive framework for inferring a query association to an ex-\nisting support clusters. Furthermore, it does not require the\npre-speciﬁed number of clusters (e.g. class-cardinality or\nways) while the node-labeling framework has to separately\ntrain the models according to each number of clusters. The\nexplicit utilization of edge-labeling which indicates whether\nthe associated two nodes belong to the same cluster (class)\nhave been previously adapted in the naive (hyper) graphs for\ncorrelation clustering [35] and the GNNs for citation net-\nworks or dynamical systems [36, 37], but never applied to\na graph for few-shot learning. Therefore, in this paper, we\npropose an edge-labeling GNN (EGNN) for few-shot lean-\ning, especially on the task of few-shot classiﬁcation.\nThe proposed EGNN consists of a number of layers\nin which each layer is composed of a node-update block\nand an edge-update block. Speciﬁcally, across layers, the\nEGNN not only updates the node features but also ex-\nplicitly adjusts the edge features, which reﬂect the edge-\nlabels of the two connected node pairs and directly exploit\nboth the intra-cluster similarity and inter-cluster dissimilar-\nity. As shown in Figure 1, after a number of alternative\nnode and edge feature updates, the edge-label prediction\ncan be obtained from the ﬁnal edge feature. The edge loss\nis then computed to update the parameters of EGNN with a\nwell-known meta-learning strategy, called episodic training\n[2, 9]. The EGNN is naturally able to perform a transduc-\ntive inference to predict all test (query) samples at once as a\nwhole, and this has shown more robust predictions in most\ncases when a few labeled training samples are provided. In\naddition, the edge-labeling framework in the EGNN enables\nto handle various numbers of classes without remodeling or\nretraining. We will show by means of experimental results\non two benchmark few-shot image classiﬁcation datasets\nthat the EGNN outperforms other few-shot learning algo-\nrithms including the existing GNNs in both supervised and\nsemi-supervised cases.\nOur main contributions can be summarized as follows:\n• The EGNN is ﬁrst proposed for few-shot learning with\niteratively updating edge-labels with exploitation of\nboth intra-cluster similarity and inter-cluster dissimi-\nlarity. It is also able to be well suited for performing\non various numbers of classes without retraining.\n• It consists of a number of layers in which each layer is\ncomposed of a node-update block and an edge-update\nblock where the corresponding parameters are esti-\nmated under the episodic training framework.\n• Both of the transductive and non-transductive learning\nor inference are investigated with the proposed EGNN.\n• On both of the supervised and semi-supervised few-\nshot image classiﬁcation tasks with two benchmark\ndatasets, the proposed EGNN signiﬁcantly improves\nthe performances over the existing GNNs. Addition-\nally, several ablation experiments show the beneﬁts\nfrom the explicit clustering as well as the separate uti-\nlization of intra-cluster similarity and inter-cluster dis-\nsimilarity.\n2. Related works\nGraph Neural Network\nGraph neural networks were\nﬁrst proposed to directly process graph structured data with\nneural networks as of form of recurrent neural networks\n[28, 29]. Li et al. [31] further extended it with gated re-\ncurrent units and modern optimization techniques. Graph\nneural networks mainly do representation learning with a\nneighborhood aggregation framework that the node features\nare computed by recursively aggregating and transforming\nfeatures of neighboring nodes.\nGeneralized convolution\nbased propagation rules also have been directly applied to\ngraphs [34, 38, 39], and Kipf and Welling [30] especially\napplied it to semi-supervised learning on graph-structured\ndata with scalability. A few approaches [6, 12] have ex-\nplored GNNs for few-shot learning and are based on the\nnode-labeling framework.\nEdge-Labeling Graph\nCorrelation clustering (CC) is a\ngraph-partitioning algorithm [40] that infers the edge la-\nbels of the graph by simultaneously maximizing intra-\ncluster similarity and inter-cluster dissimilarity. Finley and\nJoachims [41] considered a framework that uses structured\nsupport vector machine in CC for noun-phrase clustering\nand news article clustering.\nTaskar [42] derived a max-\nmargin formulation for learning the edge scores in CC for\nproducing two different segmentations of a single image.\nKim et al. [35] explored a higher-order CC over a hy-\npergraph for task-speciﬁc image segmentation. The atten-\ntion mechanism in a graph attention network has recently\nextended to incorporate real-valued edge features that are\nadaptive to both the local contents and the global layers\nfor modeling citation networks [36]. Kipf et al. [37] intro-\nduced a method to simultaneously infer relational structure\nwith interpretable edge types while learning the dynamical\nmodel of an interacting system. Johnson [43] introduced the\nGated Graph Transformer Neural Network (GGT-NN) for\n",
    "natural language tasks, where multiple edge types and sev-\neral graph transformation operations including node state\nupdate, propagation and edge update are considered.\nFew-Shot Learning\nOne main stream approach for few-\nshot image classiﬁcation is based on representation learning\nand does prediction by using nearest-neighbor according to\nsimilarity between representations. The similarity can be a\nsimple distance function such as cosine or Euclidean dis-\ntance. A Siamese network [44] works in a pairwise man-\nner using trainable weighted L1 distance. A matching net-\nwork [2] further uses an attention mechanism to derive an\ndifferentiable nearest-neighbor classiﬁer and a prototypical\nnetwork [3] extends it with deﬁning prototypes as the mean\nof embedded support examples for each class. DEML [45]\nhas introduced a concept learner to extract high-level con-\ncept by using a large-scale auxiliary labeled dataset show-\ning that a good representation is an important component to\nimprove the performance of few-shot image classiﬁcation.\nA meta-learner that learns to optimize model parameters\nextract some transferable knowledge between tasks to lever-\nage in the context of few-shot learning. Meta-LSTM [8]\nuses LSTM as a model updater and treats the model param-\neters as its hidden states. This allows to learn the initial\nvalues of parameters and update the parameters by read-\ning few-shot examples. MAML [4] learns only the initial\nvalues of parameters and simply uses SGD. It is a model\nagnostic approach, applicable to both supervised and rein-\nforcement learning tasks. Reptile [46] is similar to MAML\nbut using only ﬁrst-order gradients. Another generic meta-\nlearner, SNAIL [10], is with a novel combination of tempo-\nral convolutions and soft attention to learn an optimal learn-\ning strategy.\n3. Method\nIn this section, the deﬁnition of few-shot classiﬁcation\ntask is introduced, and the proposed algorithm is described\nin detail.\n3.1. Problem deﬁnition: Few-shot classiﬁcation\nThe few-shot classiﬁcation aims to learn a classiﬁer\nwhen only a few training samples per each class are given.\nTherefore, each few-shot classiﬁcation task T contains a\nsupport set S, a labeled set of input-label pairs, and a query\nset Q, an unlabeled set on which the learned classiﬁer is\nevaluated. If the support set S contains K labeled samples\nfor each of N unique classes, the problem is called N-way\nK-shot classiﬁcation problem.\nRecently, meta-learning has become a standard method-\nology to tackle few-shot classiﬁcation. In principle, we can\ntrain a classiﬁer to assign a class label to each query sam-\nple with only the compact support set of the task. How-\never, a small number of labeled support samples for each\ntask are not sufﬁcient to train a model fully reﬂecting the\ninter- and intra-class variations, which often leads to un-\nsatisfactory classiﬁcation performance. Meta-learning on\nexplicit training set resolves this issue by extracting trans-\nferable knowledge that allows us to perform better few-shot\nlearning on the support set, and thus classify the query set\nmore successfully.\nAs an efﬁcient way of meta-learning, we adopt episodic\ntraining [2, 9] which is commonly employed in various lit-\neratures [3, 4, 5]. Given a relatively large labeled training\ndataset, the idea of episodic training is to sample training\ntasks (episodes) that mimic the few-shot learning setting of\ntest tasks. Here, since the distribution of training tasks is as-\nsumed to be similar to that of test tasks, the performances of\nthe test tasks can be improved by learning a model to work\nwell on the training tasks.\nMore concretely, in episodic training, both training and\ntest tasks of the N-way K-shot problem are formed as\nfollows: T\n= S S Q where S = {(xi, yi)}N×K\ni=1\nand\nQ = {(xi, yi)}N×K+T\ni=N×K+1. Here, T is the number of query\nsamples, and xi and yi ∈{C1, · · · CN} = CT ⊂C are the\nith input data and its label, respectively. C is the set of all\nclasses of either training or test dataset. Although both the\ntraining and test tasks are sampled from the common task\ndistribution, the label spaces are mutually exclusive, i.e.\nCtrain ∩Ctest = ∅. The support set S in each episode serves\nas the labeled training set on which the model is trained to\nminimize the loss of its predictions over the query set Q.\nThis training procedure is iteratively carried out episode by\nepisode until convergence.\nFinally, if some of N ×K support samples are unlabeled,\nthe problem is referred to as semi-supervised few-shot clas-\nsiﬁcation. In Section 4, the effectiveness of our algorithm\non semi-supervised setting will be presented.\n3.2. Model\nThis section describes the proposed EGNN for few-shot\nclassiﬁcation, as illustrated in Figure 2. Given the feature\nrepresentations (extracted from a jointly trained convolu-\ntional neural network) of all samples of the target task, a\nfully-connected graph is initially constructed where each\nnode represents each sample, and each edge represents the\ntypes of relationship between the two connected nodes;\nLet G = (V, E; T ) be the graph constructed with samples\nfrom the task T , where V := {Vi}i=1,...,|T | and E :=\n{Eij}i,j=1,...,|T | denote the set of nodes and edges of the\ngraph, respectively. Let vi and eij be the node feature of Vi\nand the edge feature of Eij, respectively. |T | = N ×K +T\nis the total number of samples in the task T . Each ground-\ntruth edge-label yij is deﬁned by the ground-truth node la-\nbels as:\nyij =\n\u001a 1,\nif yi = yj,\n0,\notherwise.\n(1)\n",
    "Figure 2: The overall framework of the proposed EGNN model. In this illustration, a 2-way 2-shot problem is presented as\nan example. Blue and green circles represent two different classes. Nodes with solid line represent labeled support samples,\nwhile a node with dashed line represents the unlabeled query sample. The strength of edge feature is represented by the color\nin the square. Note that although each edge has a 2-dimensional feature, only the ﬁrst dimension is depicted for simplicity.\nThe detailed process is described in Section 3.2.\nEach edge feature eij = {eijd}2\nd=1 ∈[0, 1]2 is a 2-\ndimensional vector representing the (normalized) strengths\nof the intra- and inter-class relations of the two connected\nnodes. This allows to separately exploit the intra-cluster\nsimilarity and the inter-cluster dissimilairity.\nNode features are initialized by the output of the convo-\nlutional embedding network v0\ni = femb(xi; θemb), where\nθemb is the corresponding parameter set (see Figure 3.(a)).\nEdge features are initialized by edge labels as follows:\ne0\nij =\n\n\n\n[1||0],\nif yij = 1 and i, j ≤N × K,\n[0||1],\nif yij = 0 and i, j ≤N × K,\n[0.5||0.5],\notherwise,\n(2)\nwhere || is the concatenation operation.\nThe EGNN consists of L layers to process the graph,\nand the forward propagation of EGNN for inference is an\nalternative update of node feature and edge feature through\nlayers.\nIn detail, given vℓ−1\ni\nand eℓ−1\nij\nfrom the layer ℓ−1, node\nfeature update is ﬁrstly conducted by a neighborhood ag-\ngregation procedure. The feature node vℓ\ni at the layer ℓ\nis updated by ﬁrst aggregating the features of other nodes\nproportional to their edge features, and then performing the\nfeature transformation; the edge feature eℓ−1\nij\nat the layer\nℓ−1 is used as a degree of contribution of the correspond-\ning neighbor node like an attention mechanism as follows:\nvℓ\ni = f ℓ\nv([\nX\nj\n˜eℓ−1\nij1 vℓ−1\nj\n||\nX\nj\n˜eℓ−1\nij2 vℓ−1\nj\n]; θℓ\nv),\n(3)\nwhere ˜eijd =\neijd\nP\nk eikd , and f ℓ\nv is the feature (node) trans-\nformation network, as shown in Figure 3.(b), with the pa-\nrameter set θℓ\nv. It should be noted that besides the con-\nventional intra-class aggregation, we additionally consider\ninter-class aggregation. While the intra-class aggregation\nprovides the target node the information of “similar neigh-\nbors”, the inter-class aggregation provides the information\nof “dissimilar neighbors”.\nThen, edge feature update is done based on the newly\nupdated node features. The (dis)similarities between every\npair of nodes are re-obtained, and the feature of each edge is\nupdated by combining the previous edge feature value and\nthe updated (dis)similarities such that\n¯eℓ\nij1\n=\nf ℓ\ne(vℓ\ni, vℓ\nj; θℓ\ne)eℓ−1\nij1\nP\nk f ℓe(vℓ\ni, vℓ\nk; θℓe)eℓ−1\nik1 /(P\nk eℓ−1\nik1 )\n,\n(4)\n¯eℓ\nij2\n=\n(1 −f ℓ\ne(vℓ\ni, vℓ\nj; θℓ\ne))eℓ−1\nij2\nP\nk(1 −f ℓe(vℓ\ni, vℓ\nk; θℓe))eℓ−1\nik2 /(P\nk eℓ−1\nik2 )\n,(5)\neℓ\nij\n=\n¯eℓ\nij/∥¯eℓ\nij∥1,\n(6)\nwhere f ℓ\ne is the metric network that computes similarity\nscores with the parameter set θℓ\ne (see Figure 3.(c)). In spe-\n",
    "Figure 3: Detailed network architectures used in EGNN.\n(a) Embedding network femb. (b) Feature (node) transfor-\nmation network f ℓ\nv. (c) Metric network f ℓ\ne.\nciﬁc, the node feature ﬂows into edges, and each element\nof the edge feature vector is updated separately from each\nnormalized intra-cluster similarity or inter-cluster dissim-\nilarity. Namely, each edge update considers not only the\nrelation of the corresponding pair of nodes but also the re-\nlations of the other pairs of nodes. We can optionally use\ntwo separate metric networks for the computations of each\nof similarity or dissimilarity (e.g. separate fe,dsim instead\nof (1 −fe,sim)).\nAfter L number of alternative node and edge feature up-\ndates, the edge-label prediction can be obtained from the\nﬁnal edge feature, i.e. ˆyij = eL\nij1. Here, ˆyij ∈[0, 1] can be\nconsidered as a probability that the two nodes Vi and Vj are\nfrom the same class. Therefore, each node Vi can be classi-\nﬁed by simple weighted voting with support set labels and\nedge-label prediction results. The prediction probability of\nnode Vi can be formulated as P(yi = Ck|T ) = p(k)\ni\n:\np(k)\ni\n= softmax\n\u0010\nX\n{j:j̸=i∧(xj,yj)∈S}\nˆyijδ(yj = Ck)\n\u0011\n(7)\nwhere δ(yj = Ck) is the Kronecker delta function that is\nequal to one when yj = Ck and zero otherwise. Alternative\napproach for node classiﬁcation is the use of graph cluster-\ning; the entire graph G can be ﬁrst partitioned into clusters,\nusing the edge prediction and an optimization for valid par-\ntitioning via linear programming [35], and then each cluster\ncan be labeled with the support label it contains the most.\nHowever, in this paper, we simply apply Eq. (7) to ob-\ntain the classiﬁcation results. The overall algorithm for the\nAlgorithm 1: The process of EGNN for inference\n1 Input: G = (V, E; T ), where T = S S Q,\nS = {(xi, yi)}N×K\ni=1\n, Q = {xi}N×K+T\ni=N×K+1\n2 Parameters: θemb ∪{θℓ\nv, θℓ\ne}L\nℓ=1\n3 Output: {ˆyi}N×K+T\ni=N×K+1\n4 Initialize: v0\ni = femb(xi; θemb), e0\nij, ∀i, j\n5 for ℓ= 1, · · · , L do\n/* Node feature update\n*/\n6\nfor i = 1, · · · , |V | do\n7\nvℓ\ni ←NodeUpdate({vℓ−1\ni\n}, {eℓ−1\nij }; θℓ\nv)\n8\nend\n/* Edge feature update\n*/\n9\nfor (i, j) = 1, · · · , |E| do\n10\neℓ\nij ←EdgeUpdate({vℓ\ni}, {eℓ−1\nij }; θℓ\ne)\n11\nend\n12 end\n/* Query node label prediction\n*/\n13 {ˆyi}N×K+T\ni=N×K+1 ←Edge2NodePred({yi}N×K\ni=1\n, {eL\nij})\nEGNN inference at test-time is summarized in Algorithm 1.\nThe non-transductive inference means the number of query\nsamples T = 1 or it performs the query inference one-by-\none, separately, while the transductive inference classiﬁes\nall query samples at once in a single graph.\n3.3. Training\nGiven M training tasks {T train\nm\n}M\nm=1 at a certain itera-\ntion during the episodic training, the parameters of the pro-\nposed EGNN, θemb ∪{θℓ\nv, θℓ\ne}L\nℓ=1, are trained in an end-to-\nend fashion by minimizing the following loss function:\nL =\nL\nX\nℓ=1\nM\nX\nm=1\nλℓLe(Ym,e, ˆY ℓ\nm,e),\n(8)\nwhere Ym,e and ˆY ℓ\nm,e are the set of all ground-truth query\nedge-labels and the set of all (real-valued) query-edge pre-\ndictions of the mth task at the ℓth layer, respectively, and the\nedge loss Le is deﬁned as binary cross-entropy loss. Since\nthe edge prediction results can be obtained not only from\nthe last layer but also from the other layers, the total loss\ncombines all losses that are computed in all layers in order\nto improve the gradient ﬂow in the lower layers.\n4. Experiments\nWe evaluated and compared our EGNN 1 with state-of-\nthe-art approaches on two few-shot learning benchmarks,\ni.e. miniImageNet [2] and tieredImageNet [7].\n1The\ncode\nand\nmodels\nare\navailable\non\nhttps://github.com/khy0809/fewshot-egnn.\n",
    "4.1. Datasets\nminiImageNet\nIt is the most popular few-shot learn-\ning benchmark proposed by [2] derived from the original\nILSVRC-12 dataset [47]. All images are RGB colored, and\nof size 84 × 84 pixels, sampled from 100 different classes\nwith 600 samples per class. We followed the splits used\nin [8] - 64, 16, and 20 classes for training, validation and\ntesting, respectively.\ntieredImageNet\nSimilar\nto\nminiImageNet\ndataset,\ntieredImageNet [7] is also a subset of ILSVRC-12 [47].\nCompared with miniImageNet, it has much larger number\nof images (more than 700K) sampled from larger number\nof classes (608 classes rather than 100 for miniImageNet).\nImportantly, different from miniImageNet, tieredImageNet\nadopts hierarchical category structure where each of\n608 classes belongs to one of 34 higher-level categories\nsampled from the high-level nodes in the Imagenet. Each\nhigher-level category contains 10 to 20 classes, and divided\ninto 20 training (351 classes), 6 validation (97 classes) and\n8 test (160 classes) categories.\nThe average number of\nimages in each class is 1281.\n4.2. Experimental setup\nNetwork Architecture\nFor feature embedding module,\na convolutional neural network, which consists of four\nblocks, was utilized as in most few-shot learning models\n[2, 3, 4, 6] without any skip connections 2. More concretely,\neach convolutional block consists of 3 × 3 convolutions, a\nbatch normalization and a LeakyReLU activation. All net-\nwork architectures used in EGNN are described in details in\nFigure 3.\nEvaluation\nFor both datasets, we conducted a 5-way 5-\nshot experiment which is one of standard few-shot learn-\ning settings. For evaluation, each test episode was formed\nby randomly sampling 15 queries for each of 5 classes,\nand the performance is averaged over 600 randomly gen-\nerated episodes from the test set. Especially, we addition-\nally conducted a more challenging 10-way experiment on\nminiImagenet, to demonstrate the ﬂexibility of our EGNN\nmodel when the number of classes are different between\nmeta-training stage and meta-test stage, which will be pre-\nsented in Section 4.5.\nTraining\nThe proposed model was trained with Adam op-\ntimizer with an initial learning rate of 5 × 10−4 and weight\ndecay of 10−6. The task mini-batch sizes for meta-training\nwere set to be 40 and 20 for 5-way and 10-way experi-\nments, respectively. For miniImageNet, we cut the learn-\n2Resnet-based models are excluded for fair comparison.\n(a) miniImageNet\nModel\nTrans.\n5-Way 5-Shot\nMatching Networks [2]\nNo\n55.30\nReptile [46]\nNo\n62.74\nPrototypical Net [3]\nNo\n65.77\nGNN [6]\nNo\n66.41\nEGNN\nNo\n66.85\nMAML [4]\nBN\n63.11\nReptile + BN [46]\nBN\n65.99\nRelation Net [5]\nBN\n67.07\nMAML+Transduction [4]\nYes\n66.19\nTPN [12]\nYes\n69.43\nTPN (Higher K) [12]\nYes\n69.86\nEGNN+Transduction\nYes\n76.37\n(b) tieredImageNet\nModel\nTrans.\n5-Way 5-Shot\nReptile [46]\nNo\n66.47\nPrototypical Net [3]\nNo\n69.57\nEGNN\nNo\n70.98\nMAML [4]\nBN\n70.30\nReptile + BN [46]\nBN\n71.03\nRelation Net [5]\nBN\n71.31\nMAML+Transduction [4]\nYes\n70.83\nTPN [12]\nYes\n72.58\nEGNN+Transduction\nYes\n80.15\nTable\n1:\nFew-shot\nclassiﬁcation\naccuracies\non\nminiImageNet and tieredImageNet. All results are averaged\nover 600 test episodes. Top results are highlighted.\ning rate in half every 15,000 episodes while for tieredIma-\ngeNet, the learning rate is halved for every 30,000 because\nit is larger dataset and requires more iterations to converge.\nAll our code was implemented in Pytorch [48] and run with\nNVIDIA Tesla P40 GPUs.\n4.3. Few-shot classiﬁcation\nThe few-shot classiﬁcation performance of the proposed\nEGNN model is compared with several state-of-the-art\nmodels in Table 1a and 1b.\nHere, as presented in [12],\nall models are grouped into three categories with regard\nto three different transductive settings; “No” means non-\ntransductive method, where each query sample is predicted\nindependently from other queries, “Yes” means transduc-\ntive method where all queries are simultaneously processed\nand predicted together, and “BN” means that query batch\nstatistics are used instead of global batch normalization pa-\nrameters, which can be considered as a kind of transductive\ninference at test-time.\nThe proposed EGNN was tested with both transduc-\ntive and non-transductive settings. As shown in Table 1a,\nEGNN shows the best performance in 5-way 5-shot set-\n",
    "ting, on both transductive and non-transductive settings on\nminiImagenet. Notably, EGNN performed better than node-\nlabeling GNN [6], which supports the effectiveness of our\nedge-labeling framework for few-shot learning. Moreover,\nEGNN with transduction (EGNN + Transduction) outper-\nformed the second best method (TPN [12]) on both datasets,\nespecially by large margin on miniImagenet.\nTable 1b\nshows that the transductive setting on tieredImagenet gave\nthe best performance as well as large improvement com-\npared to the non-transductive setting. In TPN, only the la-\nbels of the support set are propagated to the queries based on\nthe pairwise node feature afﬁnities using a common Lapla-\ncian matrix, so the queries communicate to each other only\nvia their embedding feature similarities. In contrast, our\nproposed EGNN allows us to consider more complicated\ninteractions between query samples, by propagating to each\nother not only their node features but also edge-label infor-\nmation across the graph layers having different parameter\nsets. Furthermore, the node features of TPN are ﬁxed and\nnever changed during label propagation, which allows them\nto derive a closed-form, one-step label propagation equa-\ntion. On the contrary, in our EGNN, both node and edge\nfeatures are dynamically changed and adapted to the given\ntask gradually with several update steps.\n4.4. Semi-supervised few-shot classiﬁcation\nFor semi-supervised experiment, we followed the same\nsetting described in [6] for fair comparison. It is a 5-way\n5-shot setting, but the support samples are only partially la-\nbeled. The labeled samples are balanced among classes so\nthat all classes have the same amount of labeled and unla-\nbeled samples. The obtained results on miniImagenet are\npresented in Table 2. Here, “LabeledOnly” denotes learn-\ning with only labeled support samples, and “Semi” means\nthe semi-supervised setting explained above. Different re-\nsults are presented according to when 20% and 40%, 60%\nof support samples were labeled, and the proposed EGNN\nis compared with node-labeling GNN [6]. As shown in Ta-\nble 2, semi-supervised learning increases the performances\nin comparison to labeled-only learning on all cases. No-\ntably, the EGNN outperformed the previous GNN [6] by a\nlarge margin (61.88% vs 52.45%, when 20% labeled) on\nsemi-supervised learning, especially when the labeled por-\ntion was small. The performance is even more increased\non transductive setting (EGNN-Semi(T)). In a nutshell, our\nEGNN is able to extract more useful information from un-\nlabeled samples compared to node-labeling framework, on\nboth transductive and non-transductive settings.\n4.5. Ablation studies\nThe proposed edge-labeling GNN has a deep architec-\nture that consists of several node and edge-update layers.\nTherefore, as the model gets deeper with more layers, the\nLabeled Ratio (5-way 5-shot)\nTraining method\n20%\n40%\n60%\n100%\nGNN-LabeledOnly [6]\n50.33\n56.91\n-\n66.41\nGNN-Semi [6]\n52.45\n58.76\n-\n66.41\nEGNN-LabeledOnly\n52.86\n-\n-\n66.85\nEGNN-Semi\n61.88\n62.52\n63.53\n66.85\nEGNN-LabeledOnly(T)\n59.18\n-\n-\n76.37\nEGNN-Semi(T)\n63.62\n64.32\n66.37\n76.37\nTable 2: Semi-supervised few-shot classiﬁcation accuracies\non miniImageNet.\n# of EGNN layers\nFeature type\n1\n2\n3\nIntra & Inter\n67.99\n73.19\n76.37\nIntra Only\n67.28\n72.20\n74.04\nTable 3: 5-way 5-shot results on miniImagenet with differ-\nent numbers of EGNN layers and different feature types\ninteractions between task samples should be propagated\nmore intensively, which may leads to performance improve-\nments. To support this statement, we compared the few-shot\nlearning performances with different numbers of EGNN\nlayers, and the results are presented in Table 3. As the num-\nber of EGNN layers increases, the performance gets bet-\nter. There exists a big jump on few-shot accuracy when the\nnumber of layers changes from 1 to 2 (67.99% →73.19%),\nand a little additional gain with three layers (76.37 %).\nAnother key ingredient of the proposed EGNN is to use\nseparate exploitation of intra-cluster similarity and inter-\ncluster dissimilarity in node/edge updates.\nTo validate\nthe effectiveness of this, we conducted experiment with\nonly intra-cluster aggregation and compared the results with\nthose obtained by using both aggregations. The results are\nalso presented in Table 3. For all EGNN layers, the use of\nseparate inter-cluster aggregation clearly improves the per-\nformances.\nIt should also be noted that compared to the previous\nnode-labeling GNN, the proposed edge-labeling framework\nis more conducive in solving the few-shot problem under\narbitrary meta-test setting, especially when the number of\nfew-shot classes for meta-testing does not match to the one\nused for meta-training. To validate this statement, we con-\nducted a cross-way experiment with EGNN, and the result\nis presented in Table 4. Here, the model was trained with 5-\nway 5-shot setting and tested on 10-way 5-shot setting, and\nvice versa. Interestingly, both cross-way results are similar\nto those obtained with the matched-way settings. There-\nfore, we can observe that the EGNN can be successfully\nextended to modiﬁed few-shot setting without re-training\nof the model, while the previous node-labeling GNN [6] is\n",
    "Model\nTrain way\nTest way\nAccuracy\nPrototypical [3]\n5\n5\n65.77\nPrototypical\n5\n10\n51.93\nPrototypical\n10\n10\n49.29\nPrototypical\n10\n5\n66.93\nGNN [6]\n5\n5\n66.41\nGNN\n5\n10\nN/A\nGNN\n10\n10\n51.75\nGNN\n10\n5\nN/A\nEGNN\n5\n5\n76.37\nEGNN\n5\n10\n56.35\nEGNN\n10\n10\n57.61\nEGNN\n10\n5\n76.27\nTable 4: Cross-way few-shot learning results on miniIma-\ngenet 5-shot setting.\nnot even applicable to cross-way setting, since the size of\nthe model and parameters are dependent on the number of\nways.\nFigure 4 shows t-SNE [49] visualizations of node fea-\ntures for the previous node-labeling GNN and EGNN. The\nGNN tends to show a good clustering among support sam-\nples after the ﬁrst layer-propagation, however, query sam-\nples are heavily clustered together, and according to each la-\nbel, query samples and their support samples never get close\ntogether, especially even with more layer-propagations,\nwhich means that the last fully-connect layer of GNN ac-\ntually seems to perform most roles in query classiﬁcation.\nIn contrast, in our EGNN, as the layer-propagation goes on,\nboth the query and support samples are pulled away if their\nlabels are different, and at the same time, equally labeled\nquery and support samples get close together.\nFor further analysis, Figure 5 shows how edge features\npropagate in EGNN. Starting from the initial feature where\nall query edges are initialized with 0.5, the edge feature\ngradually evolves to resemble ground-truth edge label, as\nthey are passes through the several EGNN layers.\n5. Conclusion\nThis work addressed the problem of few-shot learning,\nespecially on the few-shot classiﬁcation task. We proposed\nthe novel EGNN which aims to iteratively update edge-\nlabels for inferring a query association to an existing sup-\nport clusters. In the process of EGNN, a number of alter-\nnative node and edge feature updates were performed using\nexplicit intra-cluster similarity and inter-cluster dissimilar-\nity through the graph layers having different parameter sets,\nand the edge-label prediction was obtained from the ﬁnal\nedge feature. The edge-labeling loss was used to update\nthe parameters of the EGNN with episodic training. Ex-\nFigure 4: t-SNE visualization of node features. From top to\nbottom: GNN [6], EGNN. From left to right: initial embed-\nding, 1st layer, 2nd layer, 3rd layer. ’x’ represents query, ’o’\nrepresents support. Different colors mean different labels.\nFigure 5: Visualization of edge feature propagation. From\nleft to right:\ninitial edge feature, 1st layer, 2nd layer,\nground-truth edge labels. Red color denotes higher value\n(eij1 = 1), while blue color denotes lower value (eij1 = 0).\nThis illustration shows 5-way 3-shot setting, and 3 queries\nfor each class, total 30 task-samples. The ﬁrst 15 samples\nare support set, and latter 15 are query set.\nperimental results showed that the proposed EGNN outper-\nformed other few-shot learning algorithms on both of the\nsupervised and semi-supervised few-shot image classiﬁca-\ntion tasks. The proposed framework is applicable to a broad\nvariety of other meta-clustering tasks. For future work, we\ncan consider another training loss which is related to the\nvalid graph clustering such as the cycle loss [35]. Another\npromising direction is graph sparsiﬁcation, e.g. construct-\ning K-nearest neighbor graphs [50], that will make our al-\ngorithm more scalable to larger number of shots.\nAcknowledgement\nThis work was supported by the National Research\nFoundation of Korea (NRF) grant funded by the Ko-\nrea government (MSIT)(No.\nNRF-2017R1A2B2006165)\nand Institute for Information & communications Technol-\nogy Promotion(IITP) grant funded by the Korea govern-\nment(MSIT) (No.2016-0-00563, Research on Adaptive Ma-\nchine Learning Technology Development for Intelligent\nAutonomous Digital Companion).\nAlso, we thank the\nKakao Brain Cloud team for supporting to efﬁciently use\nGPU clusters for large-scale experiments.\n",
    "References\n[1] Christiane Lemke, Marcin Budka, and Bogdan Gabrys. Met-\nalearning: a survey of trends and technologies.\nArtiﬁcial\nIntelligence Review, 44(1), 2015.\n[2] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wier-\nstra, et al. Matching networks for one shot learning. In NIPS,\npages 3630–3638, 2016.\n[3] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical\nnetworks for few-shot learning. In NIPS, pages 4077–4087,\n2017.\n[4] Chelsea Finn, Pieter Abbeel, and Sergey Levine.\nModel-\nagnostic meta-learning for fast adaptation of deep networks.\nIn ICML, 2017.\n[5] Flood Sung Yongxin Yang, Li Zhang, Tao Xiang, Philip HS\nTorr, and Timothy M Hospedales. Learning to compare: Re-\nlation network for few-shot learning. In CVPR, 2018.\n[6] Victor Garcia and Joan Bruna. Few-shot learning with graph\nneural networks. In ICLR, 2018.\n[7] Mengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell,\nKevin Swersky, Joshua B Tenenbaum, Hugo Larochelle, and\nRichard S Zemel. Meta-learning for semi-supervised few-\nshot classiﬁcation. In ICLR, 2018.\n[8] Sachin Ravi and Hugo Larochelle. Optimization as a model\nfor few-shot learning. In ICLR, 2017.\n[9] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan\nWierstra, and Timothy Lillicrap.\nMeta-learning with\nmemory-augmented neural networks. In ICML, pages 1842–\n1850, 2016.\n[10] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter\nAbbeel. A simple neural attentive meta-learner. In ICLR,\n2018.\n[11] Boris N. Oreshkin, Pau Rodriguez, and Alexandre Lacoste.\nTadam: Task dependent adaptive metric for improved few-\nshot learning. In NIPS, 2018.\n[12] Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, and\nYi Yang.\nTransductive propagation network for few-shot\nlearning. In ICLR, 2019.\n[13] Yu-Xiong Wang, Ross B. Girshick, Martial Hebert, and\nBharath Hariharan. Low-shot learning from imaginary data.\nIn CVPR, 2018.\n[14] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B tenen-\nbaum. Human-level concept learning through probabilistic\nprogram induction. Science, 350(6266):1332–1338, 2015.\n[15] Taesup Kim, Jaesik Yoon, Ousmane Dia, Sungwoong Kim,\nYoshua Bengio, and Sungjin Ahn. Bayesian model-agnostic\nmeta-learning. In NIPS, 2018.\n[16] Marcin Andrychowicz, Misha Denil, Sergio Gomez Col-\nmenarejo, Matthew W. Hoffman, David Pfau, Tom Schaul,\nand Nando de Freitas. Learning to learn by gradient descent\nby gradient descent. In NIPS, 2016.\n[17] Irwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc V.\nLe.\nNeural optimizer search with reinforcement learning.\nIn ICML, 2017.\n[18] Olga Wichrowska, Niru Maheswaranathan, Matthew W.\nHoffman, Sergio Gomez Colmenarejo, Misha Denil, Nando\nde Freitas, and Jascha Sohl-Dickstein. Learned optimizers\nthat scale and generalize. In ICML, 2017.\n[19] Maruan Al-Shedivat,\nTrapit Bansal,\nYuri Burda,\nIlya\nSutskever, Igor Mordatch, and Pieter Abbeel. Continuous\nadaptation via meta-learning in nonstationary and competi-\ntive environments. In ICLR, 2018.\n[20] Rein Houthooft, Richard Y. Chen, Phillip Isola, Bradly C.\nStadie, Filip Wolski, Jonathan Ho, and Pieter Abbeel.\nEvolved policy gradients. In NIPS, 2018.\n[21] Ignasi Clavera, Anusha Nagabandi, Ronald S. Fearing, Pieter\nAbbeel, Sergey Levine, and Chelsea Finn.\nLearning to\nadapt:\nMeta-learning for model-based control.\nCoRR,\nabs/1803.11347, 2018. URL http://arxiv.org/abs/\n1803.11347.\n[22] Risto Vuorio, Dong-Yeon Cho, Daejoong Kim, and Jiwon\nKim. Meta continual learning. arXiv, 2018. URL https:\n//arxiv.org/abs/1806.06928.\n[23] Ju Xu and Zhanxing Zhu. Reinforced continual learning. In\nNIPS, 2018.\n[24] Peter W. Battaglia et al. Relational inductive biases, deep\nlearning, and graph networks. arXiv, 2018. URL https:\n//arxiv.org/abs/1806.01261.\n[25] Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur\nSzlam, and Pierre Vandergheynst.\nGeometric deep learn-\ning: going beyond euclidean data. IEEE Signal Processing\nMagazine, 34(4):18–42, 2017.\n[26] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.\nHow powerful are graph neural networks?\narXiv preprint\narXiv:1810.00826, 2018.\n[27] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol\nVinyals, and George E. Dahl. Neural message passing for\nquantum chemistry.\nCoRR, abs/1704.01212, 2017.\nURL\nhttp://arxiv.org/abs/1704.01212.\n[28] M. Gori, G. Monfardini, and F. Scarselli. A new model for\nlearning in graph domains. In IJCNN, 2005.\n[29] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Ha-\ngenbuchner, and Gabriele Monfardini. The graph neural net-\nwork model. IEEE Transactions on Neural Networks, 20(1):\n61–80, 2008.\n",
    "[30] Thomas N. Kipf and Max Welling. Semi-supervised classi-\nﬁcation with graph convolutional networks. In ICLR, 2017.\n[31] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard\nZemel. Gated graph sequence neural networks. In ICLR,\n2016.\n[32] William L. Hamilton, Rex Ying, and Jure Leskovec. Induc-\ntive representation learning on large graphs. In NIPS, 2017.\n[33] Petar Velickovic, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Lio, and Yoshua Bengio. Graph at-\ntention networks. In ICLR, 2018.\n[34] Michael Defferrard,\nXavier Bresson,\nand Pierre Van-\ndergheynst. Convolutional neural networks on graphs with\nfast localized spectral ﬁltering. In NIPS, 2016.\n[35] Sungwoong Kim, Sebastian Nowozin, Pushmeet Kohli, and\nChang D Yoo. Higher-order correlation clustering for image\nsegmentation. In NIPS, pages 1530–1538, 2011.\n[36] Liyu Gong and Qiang Cheng. Adaptive edge features guided\ngraph attention networks. arXiv preprint arXiv:1809.02709,\n2018.\n[37] Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max\nWelling, and Richard Zemel. Neural relational inference for\ninteracting systems. arXiv preprint arXiv:1802.04687, 2018.\n[38] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann Le-\nCun. Spectral networks and locally connected networks on\ngraphs. CoRR, abs/1312.6203, 2013.\n[39] Mikael Henaff, Joan Bruna, and Yann LeCun.\nDeep\nconvolutional networks on graph-structured data.\nCoRR,\nabs/1506.05163, 2015.\n[40] N. Bansal, A. Blum, and S. Chawla. Correlation clustering.\nMachine Learning, 56:89–113, 2004.\n[41] T. Finley and T. Joachims. Supervised clustering with sup-\nport vector machines. In ICML, 2005.\n[42] B. Taskar. Learning structured prediction models: a large\nmargin approach. Ph.D. thesis, Stanford University, 2004.\n[43] Daniel D Johnson. Learning graphical state transitions. In\nICLR, 2016.\n[44] Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov.\nSiamese neural networks for one-shot image recognition.\n2015.\n[45] Fengwei Zhou, Bin Wu, and Zhenguo Li.\nDeep meta-\nlearning: Learning to learn in the concept space.\nCoRR,\nabs/1802.03596, 2018.\n[46] Alex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-\norder meta-learning algorithms.\nCoRR, abs/1803.02999,\n2018.\n[47] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al.\nImagenet large\nscale visual recognition challenge. International Journal of\nComputer Vision, 115(3):211–252, 2015.\n[48] Adam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-\nban Desmaison, Luca Antiga, and Adam Lerer. Automatic\ndifferentiation in pytorch. In NIPS-W, 2017.\n[49] L. van der Maaten and G. Hinton. Visualizing data using\nt-sne. JMLR, 9:2579–2605, 2008.\n[50] Xiaojuan Qi, Renjie Liao, Jiaya Jia, Sanja Fidler, and\nRaquel Urtasun. 3d graph neural networks for rgbd seman-\ntic segmentation. In Proceedings of the IEEE International\nConference on Computer Vision, pages 5199–5208, 2017.\n"
  ],
  "full_text": "Edge-Labeling Graph Neural Network for Few-shot Learning\nJongmin Kim∗1,3, Taesup Kim2,3, Sungwoong Kim3, and Chang D.Yoo1\n1Korea Advanced Institute of Science and Technology\n2MILA, Universit´e de Montr´eal\n3Kakao Brain\nAbstract\nIn this paper, we propose a novel edge-labeling graph\nneural network (EGNN), which adapts a deep neural net-\nwork on the edge-labeling graph, for few-shot learning.\nThe previous graph neural network (GNN) approaches in\nfew-shot learning have been based on the node-labeling\nframework, which implicitly models the intra-cluster sim-\nilarity and the inter-cluster dissimilarity. In contrast, the\nproposed EGNN learns to predict the edge-labels rather\nthan the node-labels on the graph that enables the evolution\nof an explicit clustering by iteratively updating the edge-\nlabels with direct exploitation of both intra-cluster similar-\nity and the inter-cluster dissimilarity. It is also well suited\nfor performing on various numbers of classes without re-\ntraining, and can be easily extended to perform a transduc-\ntive inference. The parameters of the EGNN are learned\nby episodic training with an edge-labeling loss to obtain a\nwell-generalizable model for unseen low-data problem. On\nboth of the supervised and semi-supervised few-shot image\nclassiﬁcation tasks with two benchmark datasets, the pro-\nposed EGNN signiﬁcantly improves the performances over\nthe existing GNNs.\n1. Introduction\nA lot of interest in meta-learning [1] has been re-\ncently arisen in various areas including especially task-\ngeneralization problems such as few-shot learning [2, 3, 4,\n5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], learn-to-learn [16, 17,\n18], non-stationary reinforcement learning[19, 20, 21], and\ncontinual learning [22, 23].\nAmong these meta-learning\nproblems, few-shot leaning aims to automatically and ef-\nﬁciently solve new tasks with few labeled data based on\nknowledge obtained from previous experiences. This is in\n∗Work done during an internship at Kakao Brain. Correspondence to\nkimjm0309@gmail.com\nFigure 1:\nAlternative node and edge feature update in\nEGNN with edge-labeling for few-shot learning\ncontrast to traditional (deep) learning methods that highly\nrely on large amounts of labeled data and cumbersome man-\nual tuning to solve a single task.\nRecently, there has also been growing interest in graph\nneural networks (GNNs) to handle rich relational structures\non data with deep neural networks [24, 25, 26, 27, 28,\n29, 30, 31, 32, 33, 34]. GNNs iteratively perform a fea-\nture aggregation from neighbors by message passing, and\ntherefore can express complex interactions among data in-\nstances.\nSince few-shot learning algorithms have shown\nto require full exploitation of the relationships between a\nsupport set and a query [2, 3, 5, 10, 11], the use of GNNs\ncan naturally have the great potential to solve the few-shot\nlearning problem.\nA few approaches that have explored\nGNNs for few-shot learning have been recently proposed\n[6, 12]. Speciﬁcally, given a new task with its few-shot sup-\nport set, Garcia and Bruna [6] proposed to ﬁrst construct a\ngraph where all examples of the support set and a query are\ndensely connected. Each input node is represented by the\nembedding feature (e.g. an output of a convolutional neural\nnetwork) and the given label information (e.g. one-hot en-\ncoded label). Then, it classiﬁes the unlabeled query by iter-\natively updating node features from neighborhood aggrega-\ntion. Liu et al. [12] proposed a transductive propagation net-\nwork (TPN) on the node features obtained from a deep neu-\narXiv:1905.01436v1  [cs.LG]  4 May 2019\n\n\nral network. At test-time, it iteratively propagates one-hot\nencoded labels over the entire support and query instances\nas a whole with a common graph parameter set. Here, it\nis noted that the above previous GNN approaches in few-\nshot learning have been mainly based on the node-labeling\nframework, which implicitly models the intra-cluster simi-\nlarity and inter-cluster dissimilarity.\nOn the contrary, the edge-labeling framework is able to\nexplicitly perform the clustering with representation learn-\ning and metric learning, and thus it is intuitively a more con-\nducive framework for inferring a query association to an ex-\nisting support clusters. Furthermore, it does not require the\npre-speciﬁed number of clusters (e.g. class-cardinality or\nways) while the node-labeling framework has to separately\ntrain the models according to each number of clusters. The\nexplicit utilization of edge-labeling which indicates whether\nthe associated two nodes belong to the same cluster (class)\nhave been previously adapted in the naive (hyper) graphs for\ncorrelation clustering [35] and the GNNs for citation net-\nworks or dynamical systems [36, 37], but never applied to\na graph for few-shot learning. Therefore, in this paper, we\npropose an edge-labeling GNN (EGNN) for few-shot lean-\ning, especially on the task of few-shot classiﬁcation.\nThe proposed EGNN consists of a number of layers\nin which each layer is composed of a node-update block\nand an edge-update block. Speciﬁcally, across layers, the\nEGNN not only updates the node features but also ex-\nplicitly adjusts the edge features, which reﬂect the edge-\nlabels of the two connected node pairs and directly exploit\nboth the intra-cluster similarity and inter-cluster dissimilar-\nity. As shown in Figure 1, after a number of alternative\nnode and edge feature updates, the edge-label prediction\ncan be obtained from the ﬁnal edge feature. The edge loss\nis then computed to update the parameters of EGNN with a\nwell-known meta-learning strategy, called episodic training\n[2, 9]. The EGNN is naturally able to perform a transduc-\ntive inference to predict all test (query) samples at once as a\nwhole, and this has shown more robust predictions in most\ncases when a few labeled training samples are provided. In\naddition, the edge-labeling framework in the EGNN enables\nto handle various numbers of classes without remodeling or\nretraining. We will show by means of experimental results\non two benchmark few-shot image classiﬁcation datasets\nthat the EGNN outperforms other few-shot learning algo-\nrithms including the existing GNNs in both supervised and\nsemi-supervised cases.\nOur main contributions can be summarized as follows:\n• The EGNN is ﬁrst proposed for few-shot learning with\niteratively updating edge-labels with exploitation of\nboth intra-cluster similarity and inter-cluster dissimi-\nlarity. It is also able to be well suited for performing\non various numbers of classes without retraining.\n• It consists of a number of layers in which each layer is\ncomposed of a node-update block and an edge-update\nblock where the corresponding parameters are esti-\nmated under the episodic training framework.\n• Both of the transductive and non-transductive learning\nor inference are investigated with the proposed EGNN.\n• On both of the supervised and semi-supervised few-\nshot image classiﬁcation tasks with two benchmark\ndatasets, the proposed EGNN signiﬁcantly improves\nthe performances over the existing GNNs. Addition-\nally, several ablation experiments show the beneﬁts\nfrom the explicit clustering as well as the separate uti-\nlization of intra-cluster similarity and inter-cluster dis-\nsimilarity.\n2. Related works\nGraph Neural Network\nGraph neural networks were\nﬁrst proposed to directly process graph structured data with\nneural networks as of form of recurrent neural networks\n[28, 29]. Li et al. [31] further extended it with gated re-\ncurrent units and modern optimization techniques. Graph\nneural networks mainly do representation learning with a\nneighborhood aggregation framework that the node features\nare computed by recursively aggregating and transforming\nfeatures of neighboring nodes.\nGeneralized convolution\nbased propagation rules also have been directly applied to\ngraphs [34, 38, 39], and Kipf and Welling [30] especially\napplied it to semi-supervised learning on graph-structured\ndata with scalability. A few approaches [6, 12] have ex-\nplored GNNs for few-shot learning and are based on the\nnode-labeling framework.\nEdge-Labeling Graph\nCorrelation clustering (CC) is a\ngraph-partitioning algorithm [40] that infers the edge la-\nbels of the graph by simultaneously maximizing intra-\ncluster similarity and inter-cluster dissimilarity. Finley and\nJoachims [41] considered a framework that uses structured\nsupport vector machine in CC for noun-phrase clustering\nand news article clustering.\nTaskar [42] derived a max-\nmargin formulation for learning the edge scores in CC for\nproducing two different segmentations of a single image.\nKim et al. [35] explored a higher-order CC over a hy-\npergraph for task-speciﬁc image segmentation. The atten-\ntion mechanism in a graph attention network has recently\nextended to incorporate real-valued edge features that are\nadaptive to both the local contents and the global layers\nfor modeling citation networks [36]. Kipf et al. [37] intro-\nduced a method to simultaneously infer relational structure\nwith interpretable edge types while learning the dynamical\nmodel of an interacting system. Johnson [43] introduced the\nGated Graph Transformer Neural Network (GGT-NN) for\n\n\nnatural language tasks, where multiple edge types and sev-\neral graph transformation operations including node state\nupdate, propagation and edge update are considered.\nFew-Shot Learning\nOne main stream approach for few-\nshot image classiﬁcation is based on representation learning\nand does prediction by using nearest-neighbor according to\nsimilarity between representations. The similarity can be a\nsimple distance function such as cosine or Euclidean dis-\ntance. A Siamese network [44] works in a pairwise man-\nner using trainable weighted L1 distance. A matching net-\nwork [2] further uses an attention mechanism to derive an\ndifferentiable nearest-neighbor classiﬁer and a prototypical\nnetwork [3] extends it with deﬁning prototypes as the mean\nof embedded support examples for each class. DEML [45]\nhas introduced a concept learner to extract high-level con-\ncept by using a large-scale auxiliary labeled dataset show-\ning that a good representation is an important component to\nimprove the performance of few-shot image classiﬁcation.\nA meta-learner that learns to optimize model parameters\nextract some transferable knowledge between tasks to lever-\nage in the context of few-shot learning. Meta-LSTM [8]\nuses LSTM as a model updater and treats the model param-\neters as its hidden states. This allows to learn the initial\nvalues of parameters and update the parameters by read-\ning few-shot examples. MAML [4] learns only the initial\nvalues of parameters and simply uses SGD. It is a model\nagnostic approach, applicable to both supervised and rein-\nforcement learning tasks. Reptile [46] is similar to MAML\nbut using only ﬁrst-order gradients. Another generic meta-\nlearner, SNAIL [10], is with a novel combination of tempo-\nral convolutions and soft attention to learn an optimal learn-\ning strategy.\n3. Method\nIn this section, the deﬁnition of few-shot classiﬁcation\ntask is introduced, and the proposed algorithm is described\nin detail.\n3.1. Problem deﬁnition: Few-shot classiﬁcation\nThe few-shot classiﬁcation aims to learn a classiﬁer\nwhen only a few training samples per each class are given.\nTherefore, each few-shot classiﬁcation task T contains a\nsupport set S, a labeled set of input-label pairs, and a query\nset Q, an unlabeled set on which the learned classiﬁer is\nevaluated. If the support set S contains K labeled samples\nfor each of N unique classes, the problem is called N-way\nK-shot classiﬁcation problem.\nRecently, meta-learning has become a standard method-\nology to tackle few-shot classiﬁcation. In principle, we can\ntrain a classiﬁer to assign a class label to each query sam-\nple with only the compact support set of the task. How-\never, a small number of labeled support samples for each\ntask are not sufﬁcient to train a model fully reﬂecting the\ninter- and intra-class variations, which often leads to un-\nsatisfactory classiﬁcation performance. Meta-learning on\nexplicit training set resolves this issue by extracting trans-\nferable knowledge that allows us to perform better few-shot\nlearning on the support set, and thus classify the query set\nmore successfully.\nAs an efﬁcient way of meta-learning, we adopt episodic\ntraining [2, 9] which is commonly employed in various lit-\neratures [3, 4, 5]. Given a relatively large labeled training\ndataset, the idea of episodic training is to sample training\ntasks (episodes) that mimic the few-shot learning setting of\ntest tasks. Here, since the distribution of training tasks is as-\nsumed to be similar to that of test tasks, the performances of\nthe test tasks can be improved by learning a model to work\nwell on the training tasks.\nMore concretely, in episodic training, both training and\ntest tasks of the N-way K-shot problem are formed as\nfollows: T\n= S S Q where S = {(xi, yi)}N×K\ni=1\nand\nQ = {(xi, yi)}N×K+T\ni=N×K+1. Here, T is the number of query\nsamples, and xi and yi ∈{C1, · · · CN} = CT ⊂C are the\nith input data and its label, respectively. C is the set of all\nclasses of either training or test dataset. Although both the\ntraining and test tasks are sampled from the common task\ndistribution, the label spaces are mutually exclusive, i.e.\nCtrain ∩Ctest = ∅. The support set S in each episode serves\nas the labeled training set on which the model is trained to\nminimize the loss of its predictions over the query set Q.\nThis training procedure is iteratively carried out episode by\nepisode until convergence.\nFinally, if some of N ×K support samples are unlabeled,\nthe problem is referred to as semi-supervised few-shot clas-\nsiﬁcation. In Section 4, the effectiveness of our algorithm\non semi-supervised setting will be presented.\n3.2. Model\nThis section describes the proposed EGNN for few-shot\nclassiﬁcation, as illustrated in Figure 2. Given the feature\nrepresentations (extracted from a jointly trained convolu-\ntional neural network) of all samples of the target task, a\nfully-connected graph is initially constructed where each\nnode represents each sample, and each edge represents the\ntypes of relationship between the two connected nodes;\nLet G = (V, E; T ) be the graph constructed with samples\nfrom the task T , where V := {Vi}i=1,...,|T | and E :=\n{Eij}i,j=1,...,|T | denote the set of nodes and edges of the\ngraph, respectively. Let vi and eij be the node feature of Vi\nand the edge feature of Eij, respectively. |T | = N ×K +T\nis the total number of samples in the task T . Each ground-\ntruth edge-label yij is deﬁned by the ground-truth node la-\nbels as:\nyij =\n\u001a 1,\nif yi = yj,\n0,\notherwise.\n(1)\n\n\nFigure 2: The overall framework of the proposed EGNN model. In this illustration, a 2-way 2-shot problem is presented as\nan example. Blue and green circles represent two different classes. Nodes with solid line represent labeled support samples,\nwhile a node with dashed line represents the unlabeled query sample. The strength of edge feature is represented by the color\nin the square. Note that although each edge has a 2-dimensional feature, only the ﬁrst dimension is depicted for simplicity.\nThe detailed process is described in Section 3.2.\nEach edge feature eij = {eijd}2\nd=1 ∈[0, 1]2 is a 2-\ndimensional vector representing the (normalized) strengths\nof the intra- and inter-class relations of the two connected\nnodes. This allows to separately exploit the intra-cluster\nsimilarity and the inter-cluster dissimilairity.\nNode features are initialized by the output of the convo-\nlutional embedding network v0\ni = femb(xi; θemb), where\nθemb is the corresponding parameter set (see Figure 3.(a)).\nEdge features are initialized by edge labels as follows:\ne0\nij =\n\n\n\n[1||0],\nif yij = 1 and i, j ≤N × K,\n[0||1],\nif yij = 0 and i, j ≤N × K,\n[0.5||0.5],\notherwise,\n(2)\nwhere || is the concatenation operation.\nThe EGNN consists of L layers to process the graph,\nand the forward propagation of EGNN for inference is an\nalternative update of node feature and edge feature through\nlayers.\nIn detail, given vℓ−1\ni\nand eℓ−1\nij\nfrom the layer ℓ−1, node\nfeature update is ﬁrstly conducted by a neighborhood ag-\ngregation procedure. The feature node vℓ\ni at the layer ℓ\nis updated by ﬁrst aggregating the features of other nodes\nproportional to their edge features, and then performing the\nfeature transformation; the edge feature eℓ−1\nij\nat the layer\nℓ−1 is used as a degree of contribution of the correspond-\ning neighbor node like an attention mechanism as follows:\nvℓ\ni = f ℓ\nv([\nX\nj\n˜eℓ−1\nij1 vℓ−1\nj\n||\nX\nj\n˜eℓ−1\nij2 vℓ−1\nj\n]; θℓ\nv),\n(3)\nwhere ˜eijd =\neijd\nP\nk eikd , and f ℓ\nv is the feature (node) trans-\nformation network, as shown in Figure 3.(b), with the pa-\nrameter set θℓ\nv. It should be noted that besides the con-\nventional intra-class aggregation, we additionally consider\ninter-class aggregation. While the intra-class aggregation\nprovides the target node the information of “similar neigh-\nbors”, the inter-class aggregation provides the information\nof “dissimilar neighbors”.\nThen, edge feature update is done based on the newly\nupdated node features. The (dis)similarities between every\npair of nodes are re-obtained, and the feature of each edge is\nupdated by combining the previous edge feature value and\nthe updated (dis)similarities such that\n¯eℓ\nij1\n=\nf ℓ\ne(vℓ\ni, vℓ\nj; θℓ\ne)eℓ−1\nij1\nP\nk f ℓe(vℓ\ni, vℓ\nk; θℓe)eℓ−1\nik1 /(P\nk eℓ−1\nik1 )\n,\n(4)\n¯eℓ\nij2\n=\n(1 −f ℓ\ne(vℓ\ni, vℓ\nj; θℓ\ne))eℓ−1\nij2\nP\nk(1 −f ℓe(vℓ\ni, vℓ\nk; θℓe))eℓ−1\nik2 /(P\nk eℓ−1\nik2 )\n,(5)\neℓ\nij\n=\n¯eℓ\nij/∥¯eℓ\nij∥1,\n(6)\nwhere f ℓ\ne is the metric network that computes similarity\nscores with the parameter set θℓ\ne (see Figure 3.(c)). In spe-\n\n\nFigure 3: Detailed network architectures used in EGNN.\n(a) Embedding network femb. (b) Feature (node) transfor-\nmation network f ℓ\nv. (c) Metric network f ℓ\ne.\nciﬁc, the node feature ﬂows into edges, and each element\nof the edge feature vector is updated separately from each\nnormalized intra-cluster similarity or inter-cluster dissim-\nilarity. Namely, each edge update considers not only the\nrelation of the corresponding pair of nodes but also the re-\nlations of the other pairs of nodes. We can optionally use\ntwo separate metric networks for the computations of each\nof similarity or dissimilarity (e.g. separate fe,dsim instead\nof (1 −fe,sim)).\nAfter L number of alternative node and edge feature up-\ndates, the edge-label prediction can be obtained from the\nﬁnal edge feature, i.e. ˆyij = eL\nij1. Here, ˆyij ∈[0, 1] can be\nconsidered as a probability that the two nodes Vi and Vj are\nfrom the same class. Therefore, each node Vi can be classi-\nﬁed by simple weighted voting with support set labels and\nedge-label prediction results. The prediction probability of\nnode Vi can be formulated as P(yi = Ck|T ) = p(k)\ni\n:\np(k)\ni\n= softmax\n\u0010\nX\n{j:j̸=i∧(xj,yj)∈S}\nˆyijδ(yj = Ck)\n\u0011\n(7)\nwhere δ(yj = Ck) is the Kronecker delta function that is\nequal to one when yj = Ck and zero otherwise. Alternative\napproach for node classiﬁcation is the use of graph cluster-\ning; the entire graph G can be ﬁrst partitioned into clusters,\nusing the edge prediction and an optimization for valid par-\ntitioning via linear programming [35], and then each cluster\ncan be labeled with the support label it contains the most.\nHowever, in this paper, we simply apply Eq. (7) to ob-\ntain the classiﬁcation results. The overall algorithm for the\nAlgorithm 1: The process of EGNN for inference\n1 Input: G = (V, E; T ), where T = S S Q,\nS = {(xi, yi)}N×K\ni=1\n, Q = {xi}N×K+T\ni=N×K+1\n2 Parameters: θemb ∪{θℓ\nv, θℓ\ne}L\nℓ=1\n3 Output: {ˆyi}N×K+T\ni=N×K+1\n4 Initialize: v0\ni = femb(xi; θemb), e0\nij, ∀i, j\n5 for ℓ= 1, · · · , L do\n/* Node feature update\n*/\n6\nfor i = 1, · · · , |V | do\n7\nvℓ\ni ←NodeUpdate({vℓ−1\ni\n}, {eℓ−1\nij }; θℓ\nv)\n8\nend\n/* Edge feature update\n*/\n9\nfor (i, j) = 1, · · · , |E| do\n10\neℓ\nij ←EdgeUpdate({vℓ\ni}, {eℓ−1\nij }; θℓ\ne)\n11\nend\n12 end\n/* Query node label prediction\n*/\n13 {ˆyi}N×K+T\ni=N×K+1 ←Edge2NodePred({yi}N×K\ni=1\n, {eL\nij})\nEGNN inference at test-time is summarized in Algorithm 1.\nThe non-transductive inference means the number of query\nsamples T = 1 or it performs the query inference one-by-\none, separately, while the transductive inference classiﬁes\nall query samples at once in a single graph.\n3.3. Training\nGiven M training tasks {T train\nm\n}M\nm=1 at a certain itera-\ntion during the episodic training, the parameters of the pro-\nposed EGNN, θemb ∪{θℓ\nv, θℓ\ne}L\nℓ=1, are trained in an end-to-\nend fashion by minimizing the following loss function:\nL =\nL\nX\nℓ=1\nM\nX\nm=1\nλℓLe(Ym,e, ˆY ℓ\nm,e),\n(8)\nwhere Ym,e and ˆY ℓ\nm,e are the set of all ground-truth query\nedge-labels and the set of all (real-valued) query-edge pre-\ndictions of the mth task at the ℓth layer, respectively, and the\nedge loss Le is deﬁned as binary cross-entropy loss. Since\nthe edge prediction results can be obtained not only from\nthe last layer but also from the other layers, the total loss\ncombines all losses that are computed in all layers in order\nto improve the gradient ﬂow in the lower layers.\n4. Experiments\nWe evaluated and compared our EGNN 1 with state-of-\nthe-art approaches on two few-shot learning benchmarks,\ni.e. miniImageNet [2] and tieredImageNet [7].\n1The\ncode\nand\nmodels\nare\navailable\non\nhttps://github.com/khy0809/fewshot-egnn.\n\n\n4.1. Datasets\nminiImageNet\nIt is the most popular few-shot learn-\ning benchmark proposed by [2] derived from the original\nILSVRC-12 dataset [47]. All images are RGB colored, and\nof size 84 × 84 pixels, sampled from 100 different classes\nwith 600 samples per class. We followed the splits used\nin [8] - 64, 16, and 20 classes for training, validation and\ntesting, respectively.\ntieredImageNet\nSimilar\nto\nminiImageNet\ndataset,\ntieredImageNet [7] is also a subset of ILSVRC-12 [47].\nCompared with miniImageNet, it has much larger number\nof images (more than 700K) sampled from larger number\nof classes (608 classes rather than 100 for miniImageNet).\nImportantly, different from miniImageNet, tieredImageNet\nadopts hierarchical category structure where each of\n608 classes belongs to one of 34 higher-level categories\nsampled from the high-level nodes in the Imagenet. Each\nhigher-level category contains 10 to 20 classes, and divided\ninto 20 training (351 classes), 6 validation (97 classes) and\n8 test (160 classes) categories.\nThe average number of\nimages in each class is 1281.\n4.2. Experimental setup\nNetwork Architecture\nFor feature embedding module,\na convolutional neural network, which consists of four\nblocks, was utilized as in most few-shot learning models\n[2, 3, 4, 6] without any skip connections 2. More concretely,\neach convolutional block consists of 3 × 3 convolutions, a\nbatch normalization and a LeakyReLU activation. All net-\nwork architectures used in EGNN are described in details in\nFigure 3.\nEvaluation\nFor both datasets, we conducted a 5-way 5-\nshot experiment which is one of standard few-shot learn-\ning settings. For evaluation, each test episode was formed\nby randomly sampling 15 queries for each of 5 classes,\nand the performance is averaged over 600 randomly gen-\nerated episodes from the test set. Especially, we addition-\nally conducted a more challenging 10-way experiment on\nminiImagenet, to demonstrate the ﬂexibility of our EGNN\nmodel when the number of classes are different between\nmeta-training stage and meta-test stage, which will be pre-\nsented in Section 4.5.\nTraining\nThe proposed model was trained with Adam op-\ntimizer with an initial learning rate of 5 × 10−4 and weight\ndecay of 10−6. The task mini-batch sizes for meta-training\nwere set to be 40 and 20 for 5-way and 10-way experi-\nments, respectively. For miniImageNet, we cut the learn-\n2Resnet-based models are excluded for fair comparison.\n(a) miniImageNet\nModel\nTrans.\n5-Way 5-Shot\nMatching Networks [2]\nNo\n55.30\nReptile [46]\nNo\n62.74\nPrototypical Net [3]\nNo\n65.77\nGNN [6]\nNo\n66.41\nEGNN\nNo\n66.85\nMAML [4]\nBN\n63.11\nReptile + BN [46]\nBN\n65.99\nRelation Net [5]\nBN\n67.07\nMAML+Transduction [4]\nYes\n66.19\nTPN [12]\nYes\n69.43\nTPN (Higher K) [12]\nYes\n69.86\nEGNN+Transduction\nYes\n76.37\n(b) tieredImageNet\nModel\nTrans.\n5-Way 5-Shot\nReptile [46]\nNo\n66.47\nPrototypical Net [3]\nNo\n69.57\nEGNN\nNo\n70.98\nMAML [4]\nBN\n70.30\nReptile + BN [46]\nBN\n71.03\nRelation Net [5]\nBN\n71.31\nMAML+Transduction [4]\nYes\n70.83\nTPN [12]\nYes\n72.58\nEGNN+Transduction\nYes\n80.15\nTable\n1:\nFew-shot\nclassiﬁcation\naccuracies\non\nminiImageNet and tieredImageNet. All results are averaged\nover 600 test episodes. Top results are highlighted.\ning rate in half every 15,000 episodes while for tieredIma-\ngeNet, the learning rate is halved for every 30,000 because\nit is larger dataset and requires more iterations to converge.\nAll our code was implemented in Pytorch [48] and run with\nNVIDIA Tesla P40 GPUs.\n4.3. Few-shot classiﬁcation\nThe few-shot classiﬁcation performance of the proposed\nEGNN model is compared with several state-of-the-art\nmodels in Table 1a and 1b.\nHere, as presented in [12],\nall models are grouped into three categories with regard\nto three different transductive settings; “No” means non-\ntransductive method, where each query sample is predicted\nindependently from other queries, “Yes” means transduc-\ntive method where all queries are simultaneously processed\nand predicted together, and “BN” means that query batch\nstatistics are used instead of global batch normalization pa-\nrameters, which can be considered as a kind of transductive\ninference at test-time.\nThe proposed EGNN was tested with both transduc-\ntive and non-transductive settings. As shown in Table 1a,\nEGNN shows the best performance in 5-way 5-shot set-\n\n\nting, on both transductive and non-transductive settings on\nminiImagenet. Notably, EGNN performed better than node-\nlabeling GNN [6], which supports the effectiveness of our\nedge-labeling framework for few-shot learning. Moreover,\nEGNN with transduction (EGNN + Transduction) outper-\nformed the second best method (TPN [12]) on both datasets,\nespecially by large margin on miniImagenet.\nTable 1b\nshows that the transductive setting on tieredImagenet gave\nthe best performance as well as large improvement com-\npared to the non-transductive setting. In TPN, only the la-\nbels of the support set are propagated to the queries based on\nthe pairwise node feature afﬁnities using a common Lapla-\ncian matrix, so the queries communicate to each other only\nvia their embedding feature similarities. In contrast, our\nproposed EGNN allows us to consider more complicated\ninteractions between query samples, by propagating to each\nother not only their node features but also edge-label infor-\nmation across the graph layers having different parameter\nsets. Furthermore, the node features of TPN are ﬁxed and\nnever changed during label propagation, which allows them\nto derive a closed-form, one-step label propagation equa-\ntion. On the contrary, in our EGNN, both node and edge\nfeatures are dynamically changed and adapted to the given\ntask gradually with several update steps.\n4.4. Semi-supervised few-shot classiﬁcation\nFor semi-supervised experiment, we followed the same\nsetting described in [6] for fair comparison. It is a 5-way\n5-shot setting, but the support samples are only partially la-\nbeled. The labeled samples are balanced among classes so\nthat all classes have the same amount of labeled and unla-\nbeled samples. The obtained results on miniImagenet are\npresented in Table 2. Here, “LabeledOnly” denotes learn-\ning with only labeled support samples, and “Semi” means\nthe semi-supervised setting explained above. Different re-\nsults are presented according to when 20% and 40%, 60%\nof support samples were labeled, and the proposed EGNN\nis compared with node-labeling GNN [6]. As shown in Ta-\nble 2, semi-supervised learning increases the performances\nin comparison to labeled-only learning on all cases. No-\ntably, the EGNN outperformed the previous GNN [6] by a\nlarge margin (61.88% vs 52.45%, when 20% labeled) on\nsemi-supervised learning, especially when the labeled por-\ntion was small. The performance is even more increased\non transductive setting (EGNN-Semi(T)). In a nutshell, our\nEGNN is able to extract more useful information from un-\nlabeled samples compared to node-labeling framework, on\nboth transductive and non-transductive settings.\n4.5. Ablation studies\nThe proposed edge-labeling GNN has a deep architec-\nture that consists of several node and edge-update layers.\nTherefore, as the model gets deeper with more layers, the\nLabeled Ratio (5-way 5-shot)\nTraining method\n20%\n40%\n60%\n100%\nGNN-LabeledOnly [6]\n50.33\n56.91\n-\n66.41\nGNN-Semi [6]\n52.45\n58.76\n-\n66.41\nEGNN-LabeledOnly\n52.86\n-\n-\n66.85\nEGNN-Semi\n61.88\n62.52\n63.53\n66.85\nEGNN-LabeledOnly(T)\n59.18\n-\n-\n76.37\nEGNN-Semi(T)\n63.62\n64.32\n66.37\n76.37\nTable 2: Semi-supervised few-shot classiﬁcation accuracies\non miniImageNet.\n# of EGNN layers\nFeature type\n1\n2\n3\nIntra & Inter\n67.99\n73.19\n76.37\nIntra Only\n67.28\n72.20\n74.04\nTable 3: 5-way 5-shot results on miniImagenet with differ-\nent numbers of EGNN layers and different feature types\ninteractions between task samples should be propagated\nmore intensively, which may leads to performance improve-\nments. To support this statement, we compared the few-shot\nlearning performances with different numbers of EGNN\nlayers, and the results are presented in Table 3. As the num-\nber of EGNN layers increases, the performance gets bet-\nter. There exists a big jump on few-shot accuracy when the\nnumber of layers changes from 1 to 2 (67.99% →73.19%),\nand a little additional gain with three layers (76.37 %).\nAnother key ingredient of the proposed EGNN is to use\nseparate exploitation of intra-cluster similarity and inter-\ncluster dissimilarity in node/edge updates.\nTo validate\nthe effectiveness of this, we conducted experiment with\nonly intra-cluster aggregation and compared the results with\nthose obtained by using both aggregations. The results are\nalso presented in Table 3. For all EGNN layers, the use of\nseparate inter-cluster aggregation clearly improves the per-\nformances.\nIt should also be noted that compared to the previous\nnode-labeling GNN, the proposed edge-labeling framework\nis more conducive in solving the few-shot problem under\narbitrary meta-test setting, especially when the number of\nfew-shot classes for meta-testing does not match to the one\nused for meta-training. To validate this statement, we con-\nducted a cross-way experiment with EGNN, and the result\nis presented in Table 4. Here, the model was trained with 5-\nway 5-shot setting and tested on 10-way 5-shot setting, and\nvice versa. Interestingly, both cross-way results are similar\nto those obtained with the matched-way settings. There-\nfore, we can observe that the EGNN can be successfully\nextended to modiﬁed few-shot setting without re-training\nof the model, while the previous node-labeling GNN [6] is\n\n\nModel\nTrain way\nTest way\nAccuracy\nPrototypical [3]\n5\n5\n65.77\nPrototypical\n5\n10\n51.93\nPrototypical\n10\n10\n49.29\nPrototypical\n10\n5\n66.93\nGNN [6]\n5\n5\n66.41\nGNN\n5\n10\nN/A\nGNN\n10\n10\n51.75\nGNN\n10\n5\nN/A\nEGNN\n5\n5\n76.37\nEGNN\n5\n10\n56.35\nEGNN\n10\n10\n57.61\nEGNN\n10\n5\n76.27\nTable 4: Cross-way few-shot learning results on miniIma-\ngenet 5-shot setting.\nnot even applicable to cross-way setting, since the size of\nthe model and parameters are dependent on the number of\nways.\nFigure 4 shows t-SNE [49] visualizations of node fea-\ntures for the previous node-labeling GNN and EGNN. The\nGNN tends to show a good clustering among support sam-\nples after the ﬁrst layer-propagation, however, query sam-\nples are heavily clustered together, and according to each la-\nbel, query samples and their support samples never get close\ntogether, especially even with more layer-propagations,\nwhich means that the last fully-connect layer of GNN ac-\ntually seems to perform most roles in query classiﬁcation.\nIn contrast, in our EGNN, as the layer-propagation goes on,\nboth the query and support samples are pulled away if their\nlabels are different, and at the same time, equally labeled\nquery and support samples get close together.\nFor further analysis, Figure 5 shows how edge features\npropagate in EGNN. Starting from the initial feature where\nall query edges are initialized with 0.5, the edge feature\ngradually evolves to resemble ground-truth edge label, as\nthey are passes through the several EGNN layers.\n5. Conclusion\nThis work addressed the problem of few-shot learning,\nespecially on the few-shot classiﬁcation task. We proposed\nthe novel EGNN which aims to iteratively update edge-\nlabels for inferring a query association to an existing sup-\nport clusters. In the process of EGNN, a number of alter-\nnative node and edge feature updates were performed using\nexplicit intra-cluster similarity and inter-cluster dissimilar-\nity through the graph layers having different parameter sets,\nand the edge-label prediction was obtained from the ﬁnal\nedge feature. The edge-labeling loss was used to update\nthe parameters of the EGNN with episodic training. Ex-\nFigure 4: t-SNE visualization of node features. From top to\nbottom: GNN [6], EGNN. From left to right: initial embed-\nding, 1st layer, 2nd layer, 3rd layer. ’x’ represents query, ’o’\nrepresents support. Different colors mean different labels.\nFigure 5: Visualization of edge feature propagation. From\nleft to right:\ninitial edge feature, 1st layer, 2nd layer,\nground-truth edge labels. Red color denotes higher value\n(eij1 = 1), while blue color denotes lower value (eij1 = 0).\nThis illustration shows 5-way 3-shot setting, and 3 queries\nfor each class, total 30 task-samples. The ﬁrst 15 samples\nare support set, and latter 15 are query set.\nperimental results showed that the proposed EGNN outper-\nformed other few-shot learning algorithms on both of the\nsupervised and semi-supervised few-shot image classiﬁca-\ntion tasks. The proposed framework is applicable to a broad\nvariety of other meta-clustering tasks. For future work, we\ncan consider another training loss which is related to the\nvalid graph clustering such as the cycle loss [35]. Another\npromising direction is graph sparsiﬁcation, e.g. construct-\ning K-nearest neighbor graphs [50], that will make our al-\ngorithm more scalable to larger number of shots.\nAcknowledgement\nThis work was supported by the National Research\nFoundation of Korea (NRF) grant funded by the Ko-\nrea government (MSIT)(No.\nNRF-2017R1A2B2006165)\nand Institute for Information & communications Technol-\nogy Promotion(IITP) grant funded by the Korea govern-\nment(MSIT) (No.2016-0-00563, Research on Adaptive Ma-\nchine Learning Technology Development for Intelligent\nAutonomous Digital Companion).\nAlso, we thank the\nKakao Brain Cloud team for supporting to efﬁciently use\nGPU clusters for large-scale experiments.\n\n\nReferences\n[1] Christiane Lemke, Marcin Budka, and Bogdan Gabrys. Met-\nalearning: a survey of trends and technologies.\nArtiﬁcial\nIntelligence Review, 44(1), 2015.\n[2] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wier-\nstra, et al. Matching networks for one shot learning. In NIPS,\npages 3630–3638, 2016.\n[3] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical\nnetworks for few-shot learning. In NIPS, pages 4077–4087,\n2017.\n[4] Chelsea Finn, Pieter Abbeel, and Sergey Levine.\nModel-\nagnostic meta-learning for fast adaptation of deep networks.\nIn ICML, 2017.\n[5] Flood Sung Yongxin Yang, Li Zhang, Tao Xiang, Philip HS\nTorr, and Timothy M Hospedales. Learning to compare: Re-\nlation network for few-shot learning. In CVPR, 2018.\n[6] Victor Garcia and Joan Bruna. Few-shot learning with graph\nneural networks. In ICLR, 2018.\n[7] Mengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell,\nKevin Swersky, Joshua B Tenenbaum, Hugo Larochelle, and\nRichard S Zemel. Meta-learning for semi-supervised few-\nshot classiﬁcation. In ICLR, 2018.\n[8] Sachin Ravi and Hugo Larochelle. Optimization as a model\nfor few-shot learning. In ICLR, 2017.\n[9] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan\nWierstra, and Timothy Lillicrap.\nMeta-learning with\nmemory-augmented neural networks. In ICML, pages 1842–\n1850, 2016.\n[10] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter\nAbbeel. A simple neural attentive meta-learner. In ICLR,\n2018.\n[11] Boris N. Oreshkin, Pau Rodriguez, and Alexandre Lacoste.\nTadam: Task dependent adaptive metric for improved few-\nshot learning. In NIPS, 2018.\n[12] Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, and\nYi Yang.\nTransductive propagation network for few-shot\nlearning. In ICLR, 2019.\n[13] Yu-Xiong Wang, Ross B. Girshick, Martial Hebert, and\nBharath Hariharan. Low-shot learning from imaginary data.\nIn CVPR, 2018.\n[14] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B tenen-\nbaum. Human-level concept learning through probabilistic\nprogram induction. Science, 350(6266):1332–1338, 2015.\n[15] Taesup Kim, Jaesik Yoon, Ousmane Dia, Sungwoong Kim,\nYoshua Bengio, and Sungjin Ahn. Bayesian model-agnostic\nmeta-learning. In NIPS, 2018.\n[16] Marcin Andrychowicz, Misha Denil, Sergio Gomez Col-\nmenarejo, Matthew W. Hoffman, David Pfau, Tom Schaul,\nand Nando de Freitas. Learning to learn by gradient descent\nby gradient descent. In NIPS, 2016.\n[17] Irwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc V.\nLe.\nNeural optimizer search with reinforcement learning.\nIn ICML, 2017.\n[18] Olga Wichrowska, Niru Maheswaranathan, Matthew W.\nHoffman, Sergio Gomez Colmenarejo, Misha Denil, Nando\nde Freitas, and Jascha Sohl-Dickstein. Learned optimizers\nthat scale and generalize. In ICML, 2017.\n[19] Maruan Al-Shedivat,\nTrapit Bansal,\nYuri Burda,\nIlya\nSutskever, Igor Mordatch, and Pieter Abbeel. Continuous\nadaptation via meta-learning in nonstationary and competi-\ntive environments. In ICLR, 2018.\n[20] Rein Houthooft, Richard Y. Chen, Phillip Isola, Bradly C.\nStadie, Filip Wolski, Jonathan Ho, and Pieter Abbeel.\nEvolved policy gradients. In NIPS, 2018.\n[21] Ignasi Clavera, Anusha Nagabandi, Ronald S. Fearing, Pieter\nAbbeel, Sergey Levine, and Chelsea Finn.\nLearning to\nadapt:\nMeta-learning for model-based control.\nCoRR,\nabs/1803.11347, 2018. URL http://arxiv.org/abs/\n1803.11347.\n[22] Risto Vuorio, Dong-Yeon Cho, Daejoong Kim, and Jiwon\nKim. Meta continual learning. arXiv, 2018. URL https:\n//arxiv.org/abs/1806.06928.\n[23] Ju Xu and Zhanxing Zhu. Reinforced continual learning. In\nNIPS, 2018.\n[24] Peter W. Battaglia et al. Relational inductive biases, deep\nlearning, and graph networks. arXiv, 2018. URL https:\n//arxiv.org/abs/1806.01261.\n[25] Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur\nSzlam, and Pierre Vandergheynst.\nGeometric deep learn-\ning: going beyond euclidean data. IEEE Signal Processing\nMagazine, 34(4):18–42, 2017.\n[26] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.\nHow powerful are graph neural networks?\narXiv preprint\narXiv:1810.00826, 2018.\n[27] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol\nVinyals, and George E. Dahl. Neural message passing for\nquantum chemistry.\nCoRR, abs/1704.01212, 2017.\nURL\nhttp://arxiv.org/abs/1704.01212.\n[28] M. Gori, G. Monfardini, and F. Scarselli. A new model for\nlearning in graph domains. In IJCNN, 2005.\n[29] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Ha-\ngenbuchner, and Gabriele Monfardini. The graph neural net-\nwork model. IEEE Transactions on Neural Networks, 20(1):\n61–80, 2008.\n\n\n[30] Thomas N. Kipf and Max Welling. Semi-supervised classi-\nﬁcation with graph convolutional networks. In ICLR, 2017.\n[31] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard\nZemel. Gated graph sequence neural networks. In ICLR,\n2016.\n[32] William L. Hamilton, Rex Ying, and Jure Leskovec. Induc-\ntive representation learning on large graphs. In NIPS, 2017.\n[33] Petar Velickovic, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Lio, and Yoshua Bengio. Graph at-\ntention networks. In ICLR, 2018.\n[34] Michael Defferrard,\nXavier Bresson,\nand Pierre Van-\ndergheynst. Convolutional neural networks on graphs with\nfast localized spectral ﬁltering. In NIPS, 2016.\n[35] Sungwoong Kim, Sebastian Nowozin, Pushmeet Kohli, and\nChang D Yoo. Higher-order correlation clustering for image\nsegmentation. In NIPS, pages 1530–1538, 2011.\n[36] Liyu Gong and Qiang Cheng. Adaptive edge features guided\ngraph attention networks. arXiv preprint arXiv:1809.02709,\n2018.\n[37] Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max\nWelling, and Richard Zemel. Neural relational inference for\ninteracting systems. arXiv preprint arXiv:1802.04687, 2018.\n[38] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann Le-\nCun. Spectral networks and locally connected networks on\ngraphs. CoRR, abs/1312.6203, 2013.\n[39] Mikael Henaff, Joan Bruna, and Yann LeCun.\nDeep\nconvolutional networks on graph-structured data.\nCoRR,\nabs/1506.05163, 2015.\n[40] N. Bansal, A. Blum, and S. Chawla. Correlation clustering.\nMachine Learning, 56:89–113, 2004.\n[41] T. Finley and T. Joachims. Supervised clustering with sup-\nport vector machines. In ICML, 2005.\n[42] B. Taskar. Learning structured prediction models: a large\nmargin approach. Ph.D. thesis, Stanford University, 2004.\n[43] Daniel D Johnson. Learning graphical state transitions. In\nICLR, 2016.\n[44] Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov.\nSiamese neural networks for one-shot image recognition.\n2015.\n[45] Fengwei Zhou, Bin Wu, and Zhenguo Li.\nDeep meta-\nlearning: Learning to learn in the concept space.\nCoRR,\nabs/1802.03596, 2018.\n[46] Alex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-\norder meta-learning algorithms.\nCoRR, abs/1803.02999,\n2018.\n[47] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al.\nImagenet large\nscale visual recognition challenge. International Journal of\nComputer Vision, 115(3):211–252, 2015.\n[48] Adam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-\nban Desmaison, Luca Antiga, and Adam Lerer. Automatic\ndifferentiation in pytorch. In NIPS-W, 2017.\n[49] L. van der Maaten and G. Hinton. Visualizing data using\nt-sne. JMLR, 9:2579–2605, 2008.\n[50] Xiaojuan Qi, Renjie Liao, Jiaya Jia, Sanja Fidler, and\nRaquel Urtasun. 3d graph neural networks for rgbd seman-\ntic segmentation. In Proceedings of the IEEE International\nConference on Computer Vision, pages 5199–5208, 2017.\n"
}