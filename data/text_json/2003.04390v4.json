{
  "filename": "2003.04390v4.pdf",
  "num_pages": 12,
  "pages": [
    "Meta-Baseline: Exploring Simple Meta-Learning for Few-Shot Learning\nYinbo Chen\nUC San Diego\nZhuang Liu\nUC Berkeley\nHuijuan Xu\nPenn State University\nTrevor Darrell\nUC Berkeley\nXiaolong Wang\nUC San Diego\nAbstract\nMeta-learning has been the most common framework for\nfew-shot learning in recent years. It learns the model from\ncollections of few-shot classiﬁcation tasks, which is believed\nto have a key advantage of making the training objective\nconsistent with the testing objective.\nHowever, some re-\ncent works report that by training for whole-classiﬁcation,\ni.e. classiﬁcation on the whole label-set, it can get compa-\nrable or even better embedding than many meta-learning\nalgorithms.\nThe edge between these two lines of works\nhas yet been underexplored, and the effectiveness of meta-\nlearning in few-shot learning remains unclear. In this paper,\nwe explore a simple process: meta-learning over a whole-\nclassiﬁcation pre-trained model on its evaluation metric.\nWe observe this simple method achieves competitive per-\nformance to state-of-the-art methods on standard bench-\nmarks. Our further analysis shed some light on understand-\ning the trade-offs between the meta-learning objective and\nthe whole-classiﬁcation objective in few-shot learning. Our\ncode is available at https://github.com/yinboc/\nfew-shot-meta-baseline.\n1. Introduction\nWhile humans have shown incredible ability to learn\nfrom very few examples and generalize to many different\nnew examples, the current deep learning approaches still\nrely on a large scale of training data. To mimic this hu-\nman ability of generalization, few-shot learning [4, 29] is\nproposed for training networks to understand a new con-\ncept based on a few labeled examples. While directly learn-\ning a large number of parameters with few samples is very\nchallenging and most likely leads to overﬁtting, a practical\nsetting is applying transfer learning: train the network on\ncommon classes (also called base classes) with sufﬁcient\nsamples, then transfer the model to learn novel classes with\na few examples.\nThe meta-learning framework for few-shot learning fol-\nlows the key idea of learning to learn. Speciﬁcally, it sam-\nples few-shot classiﬁcation tasks from training samples be-\nlonging to the base classes and optimizes the model to per-\nform well on these tasks. A task typically takes the form\nof N-way and K-shot, which contains N classes with K\nsupport samples and Q query samples in each class. The\ngoal is to classify these N × Q query samples into the\nN classes based on the N × K support samples. Under\nthis framework, the model is directly optimized on few-\nshot classiﬁcation tasks. The consistency between the ob-\njectives of training and testing is considered as the key ad-\nvantage of meta-learning. Motivated by this idea, many re-\ncent works [26, 6, 25, 30, 5, 22, 11, 33] focus on improving\nthe meta-learning structure, and few-shot learning itself has\nbecome a common testbed for evaluating meta-learning al-\ngorithms.\nHowever, some recent works ﬁnd that training for whole-\nclassiﬁcation, i.e. classiﬁcation on the whole training label-\nset (base classes), provides the embedding that is compa-\nrable or even better than many recent meta-learning algo-\nrithms.\nThe effectiveness of whole-classiﬁcation models\nhas been reported in both prior works [6, 1] and some con-\ncurrent works [31, 27]. Meta-learning makes the form of\ntraining objective consistent with testing, but why it turns\nout to learn even worse embedding than simple whole-\nclassiﬁcation?\nWhile there are several possible reasons,\ne.g. optimization difﬁculty or overﬁtting, the answer has\nnot been clearly studied yet. It remains even unclear that\nwhether meta-learning is still effective compared to whole-\nclassiﬁcation in few-shot learning.\nIn this work, we aim at exploring the edge between\nwhole-classiﬁcation and meta-learning by decoupling the\ndiscrepancies. We start with Classiﬁer-Baseline: a whole-\nclassiﬁcation method that is similarly proposed in concur-\nrent works [31, 27]. In Classiﬁer-Baseline, we ﬁrst train\na classiﬁer on base classes, then remove the last fully-\nconnected (FC) layer which is class-dependent. During test\ntime, it computes mean embedding of support samples for\neach novel class as their centroids, and classiﬁes query sam-\nples to the nearest centroid with cosine distance. We ob-\nserve this baseline method outperforms many recent meta-\nlearning algorithms.\nIn order to understand whether meta-learning is still ef-\n1\narXiv:2003.04390v4  [cs.CV]  19 Aug 2021\n",
    "fective compared to whole-classiﬁcation, a natural experi-\nment is to see what happens if we perform further meta-\nlearning over a converged Classiﬁer-Baseline on its evalu-\nation metric (i.e. cosine nearest-centroid). As a resulting\nmethod, it is similar to MatchingNet [29] or ProtoNet [24]\nwith an additional classiﬁcation pre-training stage.\nWe\nobserve that meta-learning can still improve Classiﬁer-\nBaseline, and it achieves competitive performance to state-\nof-the-art methods on standard benchmarks. We call this\nsimple method Meta-Baseline.\nWe highlight that as a\nmethod, all the individual components of Meta-Baseline\nhave been proposed in prior works, but to the best of our\nknowledge, it has been overlooked that none of the prior\nworks studies them as a whole. We further decouple the\ndiscrepancies by evaluating on two types of generaliza-\ntion: base class generalization denotes performance on\nfew-shot classiﬁcation tasks from unseen data in the base\nclasses, which follows the common deﬁnition of general-\nization (i.e. evaluated in the training distribution); and novel\nclass generalization denotes performance on few-shot clas-\nsiﬁcation tasks from data in novel classes, which is the\ngoal of the few-shot learning problem. We observe that:\n(i) During meta-learning, improving base class generaliza-\ntion can lead to worse novel class generalization; (ii) When\ntraining Meta-Baseline from scratch (i.e. without whole-\nclassiﬁcation training), it achieves higher base-class gener-\nalization but much lower novel class generalization.\nOur observations suggest that there could be a trade-\noff between the objectives of meta-learning and whole-\nclassiﬁcation. It is likely that meta-learning learns the em-\nbedding that works better for N-way K-shot tasks, while\nwhole-classiﬁcation learns the embedding with stronger\nclass transferability.\nWe ﬁnd that the main advantage\nof training for whole-classiﬁcation before meta-learning is\nlikely to be improving class transferability. Our further ex-\nperiments provide a potential explanation of what makes\nMeta-Baseline a strong baseline: by inheriting one of the\nmost effective evaluation metrics of the whole-classiﬁcation\nmodel, it maximizes the reusing of the embedding with\nstrong class transferability. From another perspective, our\nresults also rethink the comparison between meta-learning\nand whole-classiﬁcation from the perspective of datasets.\nWhen base classes are collected to cover the distribution of\nnovel classes, novel-class generalization should converge to\nbase-class generalization and the strength of meta-learning\nmay overwhelm the strength of whole-classiﬁcation.\nIn summary, our contributions are as following:\n• We present a simple Meta-Baseline that has been over-\nlooked in prior work. It achieves competitive perfor-\nmance to state-of-the-art methods on standard bench-\nmarks and is easy to follow.\n• We observe a trade-off between the objectives of meta-\nlearning and whole-classiﬁcation, which potentially\nexplains the success of Meta-Baseline and rethinks the\neffectiveness of both objectives in few-shot learning.\n2. Related Work\nMost recent approaches for few-shot learning follow the\nmeta-learning framework. The various meta-learning ar-\nchitectures for few-shot learning can be roughly catego-\nrized into three groups. Memory-based methods [19, 15,\n23, 13, 14] are based on the idea to train a meta-learner\nwith memory to learn novel concepts (e.g.\nan LSTM-\nbased meta-learner). Optimization-based methods [7, 22]\nfollows the idea of differentializing an optimization pro-\ncess over support-set within the meta-learning framework:\nMAML [5] ﬁnds an initialization of the neural network that\ncan be adapted to any novel task using a few optimiza-\ntion steps. MetaOptNet [11] learns the feature represen-\ntation that can generalize well for a linear support vector\nmachine (SVM) classiﬁer. Besides explicitly considering\nthe dynamic learning process, metric-based methods [29]\nmeta-learn a deep representation with a metric in feature\nspace. For example, Prototypical Networks [24] compute\nthe average feature for each class in support-set and clas-\nsify query samples by the nearest-centroid method. They\nuse Euclidean distance since it is a Bregman divergence.\nRelation Networks [26] further generalizes this framework\nby proposing a relation module as a learnable metric jointly\ntrained with deep representations. TADAM [16] proposes to\nuse a task conditioned metric resulting in a task-dependent\nmetric space.\nWhile signiﬁcant progress is made in the meta-learning\nframework, some recent works challenge the effectiveness\nof meta-learning with simple whole-classiﬁcation, i.e.\na\nclassiﬁcation model on the whole training label-set. Co-\nsine classiﬁer [6] and Baseline++ [1] perform whole-\nclassiﬁcation training by replacing the top linear layer with\na cosine classiﬁer, and they adapt the classiﬁer to a few-\nshot classiﬁcation task of novel classes by performing near-\nest centroid or ﬁne-tuning a new layer respectively. They\nshow these whole-classiﬁcation models can achieve com-\npetitive performance compared to several popular meta-\nlearning models. Another recent work [2] studies on a trans-\nductive setting. Along with these baseline methods, more\nadvanced meta-learning methods [25, 11, 33] are proposed\nand they set up new state-of-the-art results. The effective-\nness of whole-classiﬁcation is then revisited in two of the\nconcurrent works [31, 27] with improved design choices.\nBy far, the effectiveness of meta-learning compared to\nwhole-classiﬁcation in few-shot learning is still unclear,\nsince the edge between whole-classiﬁcation models and\nmeta-learning models remains underexplored. The goal of\nthis work is to explore the insights behind the phenomenons.\nOur experiments show a potential trade-off between the\n2\n",
    "Method\nWhole-classiﬁcation training\nMeta-learning\nOthers\nMatching Networks [29]\nno / yes (large models)\nattention + cosine\nFCE\nPrototypical Networks [24]\nno\ncentroid + Euclidean\n-\nBaseline++ [1]\nyes (cosine classiﬁer)\n-\nﬁne-tuning\nMeta-Baseline (ours)\nyes\ncentroid + cosine (∗τ)\n-\nTable 1: Overview of method comparison. We summarize the differences between Meta-Baseline and prior methods.\nmeta-learning and whole-classiﬁcation objectives, which\nprovides a more clear understanding of the comparison be-\ntween both objectives for few-shot learning.\nAs a method, similar ideas to Classiﬁer-Baseline are con-\ncurrently reported in recent works [31, 27]. Unlike some\nprior works [6, 1], Classiﬁer-Baseline does not replace the\nlast layer with cosine classiﬁer during training, it trains\nthe whole-classiﬁcation model with a linear layer on the\ntop and applies cosine nearest-centroid metric during the\ntest time for few-shot classiﬁcation on novel classes. The\nMeta-Baseline is meta-learning over a converged Classiﬁer-\nBaseline on its evaluation metric (cosine nearest-centroid).\nIt is similar (with inconspicuous and important differences\nas shown in Table 1) to those simple and classical metric-\nbased meta-learning methods [29, 24]. The main purpose of\nMeta-Baseline in this paper is to understand the comparison\nbetween whole-classiﬁcation and meta-learning objectives,\nbut we ﬁnd it is also a simple meta-learning baseline that\nhas been overlooked. While every individual component in\nMeta-Baseline is not novel, to the best of our knowledge,\nnone of the prior works studies them as a whole.\n3. Method\n3.1. Problem deﬁnition\nIn standard few-shot classiﬁcation, given a labeled\ndataset of base classes Cbase with a large number of im-\nages, the goal is to learn concepts in novel classes Cnovel\nwith a few samples. In an N-way K-shot few-shot clas-\nsiﬁcation task, the support-set contains N classes with K\nsamples per class, the query-set contains samples from the\nsame N classes with Q samples per class, and the goal is to\nclassify the N × Q query images into N classes.\n3.2. Classiﬁer-Baseline\nClassiﬁer-Baseline is a whole-classiﬁcation model, i.e.\na classiﬁcation model trained for the whole label-set. It\nrefers to training a classiﬁer with classiﬁcation loss on all\nbase classes and performing few-shot tasks with the cosine\nnearest-centroid method. Speciﬁcally, we train a classiﬁer\non all base classes with standard cross-entropy loss, then re-\nmove its last FC layer and get the encoder fθ, which maps\nthe input to embedding. Given a few-shot task with the\nsupport-set S, let Sc denote the few-shot samples in class\nc, it computes the average embedding wc as the centroid of\nclass c:\nwc =\n1\n|Sc|\nX\nx∈Sc\nfθ(x),\n(1)\nthen for a query sample x in a few-shot task, it predicts the\nprobability that sample x belongs to class c according to the\ncosine similarity between the embedding of sample x and\nthe centroid of class c:\np(y = c | x) =\nexp\n\u0000⟨fθ(x), wc⟩\n\u0001\nP\nc′ exp\n\u0000⟨fθ(x), wc′⟩\n\u0001,\n(2)\nwhere ⟨·, ·⟩denotes the cosine similarity of two vectors.\nSimilar methods to Classiﬁer-Baseline have also been\nproposed in concurrent works [31, 27]. Compared to Base-\nline++ [1], the Classiﬁer-Baseline does not use the cosine\nclassiﬁer for training or perform ﬁne-tuning during testing,\nwhile it performs better on standard benchmarks. In this\nwork, we choose Classiﬁer-Baseline as the representative of\nwhole-classiﬁcation models for few-shot learning. For sim-\nplicity and clarity, we do not introduce additional complex\ntechniques for this whole-classiﬁcation training.\n3.3. Meta-Baseline\nFigure 1 visualizes the Meta-Baseline. The ﬁrst stage\nis the classiﬁcation training stage, it trains a Classiﬁer-\nBaseline, i.e. training a classiﬁer on all bases classes and\nremove its last FC layer to get fθ. The second stage is the\nmeta-learning stage, which optimizes the model on the eval-\nuation metric of Classiﬁer-Baseline. Speciﬁcally, given the\nclassiﬁcation-trained feature encoder fθ, it samples N-way\nK-shot tasks (with N × Q query samples) from training\nsamples in base classes. To compute the loss for each task,\nin support-set it computes the centroids of N classes de-\nﬁned in Equation 1, which are then used to compute the\npredicted probability distribution for each sample in query-\nset deﬁned in Equation 2. The loss is a cross-entropy loss\ncomputed from p and the labels of the samples in the query-\nset. During training, each training batch can contain several\ntasks and the average loss is computed.\nSince cosine similarity has the value range of [−1, 1],\nwhen it is used to compute the logits, it can be helpful to\n3\n",
    "mean\ncos\nscore\nMeta-Baseline\ntraining\nClassifier-Baseline / Meta-Baseline\nevaluation\nsupport-set\nquery-set\nClassifier-Baseline\ntraining\nclassification\non base classes\n𝑓𝜃\nloss\n𝑓𝜃\n𝑓𝜃\nMeta-Learning Stage\nClassification Training Stage\nrepresentation\ntransfer\nF\nC\nlabel\n𝜏\nFigure 1: Classiﬁer-Baseline and Meta-Baseline. Classiﬁer-Baseline is to train a classiﬁcation model on all base classes\nand remove its last FC layer to get the encoder fθ. Given a few-shot task, it computes the average feature for samples of each\nclass in support-set, then it classiﬁes a sample in query-set by nearest-centroid with cosine similarity as distance. In Meta-\nBaseline, it further optimizes a converged Classiﬁer-Baseline on its evaluation metric, and an additional learnable scalar τ is\nintroduced to scale cosine similarity.\nscale the value before applying Softmax function during\ntraining (a common practice in recent work [6, 17, 16]). We\nmultiply the cosine similarity by a learnable scalar τ, and\nthe probability prediction in training becomes:\np(y = c | x) =\nexp\n\u0000τ · ⟨fθ(x), wc⟩\n\u0001\nP\nc′ exp\n\u0000τ · ⟨fθ(x), wc′⟩\n\u0001.\n(3)\nIn this work, the main purpose of Meta-Baseline is to\ninvestigate whether the meta-learning objective is still ef-\nfective over a whole-classiﬁcation model.\nAs a method,\nwhile every component in Meta-Baseline has been proposed\nin prior works, we ﬁnd none of the prior works studies them\nas a whole. Therefore, Meta-Baseline should also be an im-\nportant baseline that has been overlooked.\n4. Results on Standard Benchmarks\n4.1. Datasets\nThe miniImageNet dataset [29] is a common benchmark\nfor few-shot learning. It contains 100 classes sampled from\nILSVRC-2012 [21], which are then randomly split to 64,\n16, 20 classes as training, validation, and testing set respec-\ntively. Each class contains 600 images of size 84 × 84.\nThe tieredImageNet dataset [20] is another common\nbenchmark proposed more recently with much larger scale.\nIt is a subset of ILSVRC-2012, containing 608 classes from\n34 super-categories, which are then split into 20, 6, 8 super-\ncategories, resulting in 351, 97, 160 classes as training, val-\nidation, testing set respectively. The image size is 84 × 84.\nThis setting is more challenging since base classes and\nnovel classes come from different super-categories.\nIn addition to the datasets above, we evaluate our model\non ImageNet-800, which is derived from ILSVRC-2012 1K\nclasses by randomly splitting 800 classes as base classes\nand 200 classes as novel classes. The base classes contain\nthe images from the original training set, the novel classes\ncontain the images from the original validation set. This\nlarger dataset aims at making the training setting standard\nas the ImageNet 1K classiﬁcation task [8].\n4.2. Implementation details\nWe use ResNet-12 that follows the most of recent\nworks [16, 25, 11, 33] on miniImageNet and tieredIma-\ngeNet, and we use ResNet-18, ResNet-50 [8] on ImageNet-\n800. For the classiﬁcation training stage, we use the SGD\noptimizer with momentum 0.9, the learning rate starts from\n0.1 and the decay factor is 0.1. On miniImageNet, we train\n100 epochs with batch size 128 on 4 GPUs, the learning\nrate decays at epoch 90. On tieredImageNet, we train 120\nepochs with batch size 512 on 4 GPUs, the learning rate de-\n4\n",
    "Model\nBackbone\n1-shot\n5-shot\nMatching Networks [29]\nConvNet-4\n43.56 ± 0.84\n55.31 ± 0.73\nPrototypical Networks [24]\nConvNet-4\n48.70 ± 1.84\n63.11 ± 0.92\nPrototypical Networks (re-implement)\nResNet-12\n53.81 ± 0.23\n75.68 ± 0.17\nActivation to Parameter [18]\nWRN-28-10\n59.60 ± 0.41\n73.74 ± 0.19\nLEO [22]\nWRN-28-10\n61.76 ± 0.08\n77.59 ± 0.12\nBaseline++ [1]\nResNet-18\n51.87 ± 0.77\n75.68 ± 0.63\nSNAIL [13]\nResNet-12\n55.71 ± 0.99\n68.88 ± 0.92\nAdaResNet [15]\nResNet-12\n56.88 ± 0.62\n71.94 ± 0.57\nTADAM [16]\nResNet-12\n58.50 ± 0.30\n76.70 ± 0.30\nMTL [25]\nResNet-12\n61.20 ± 1.80\n75.50 ± 0.80\nMetaOptNet [11]\nResNet-12\n62.64 ± 0.61\n78.63 ± 0.46\nSLA-AG [10]\nResNet-12\n62.93 ± 0.63\n79.63 ± 0.47\nProtoNets + TRAML [12]\nResNet-12\n60.31 ± 0.48\n77.94 ± 0.57\nConstellationNet [33]\nResNet-12\n64.89 ± 0.23\n79.95 ± 0.17\nClassiﬁer-Baseline (ours)\nResNet-12\n58.91 ± 0.23\n77.76 ± 0.17\nMeta-Baseline (ours)\nResNet-12\n63.17 ± 0.23\n79.26 ± 0.17\nTable 2: Comparison to prior works on miniImageNet. Average 5-way accuracy (%) with 95% conﬁdence interval.\nModel\nBackbone\n1-shot\n5-shot\nMAML [5]\nConvNet-4\n51.67 ± 1.81\n70.30 ± 1.75\nPrototypical Networks* [24]\nConvNet-4\n53.31 ± 0.89\n72.69 ± 0.74\nRelation Networks* [26]\nConvNet-4\n54.48 ± 0.93\n71.32 ± 0.78\nLEO [22]\nWRN-28-10\n66.33 ± 0.05\n81.44 ± 0.09\nMetaOptNet [11]\nResNet-12\n65.99 ± 0.72\n81.56 ± 0.53\nClassiﬁer-Baseline (ours)\nResNet-12\n68.07 ± 0.26\n83.74 ± 0.18\nMeta-Baseline (ours)\nResNet-12\n68.62 ± 0.27\n83.74 ± 0.18\nTable 3: Comparison to prior works on tieredImageNet. Average 5-way accuracy (%) with 95% conﬁdence interval.\ncays at epoch 40 and 80. On ImageNet-800, we train 90\nepochs with batch size 256 on 8 GPUs, the learning rate de-\ncays at epoch 30 and 60. The weight decay is 0.0005 for\nResNet-12 and 0.0001 for ResNet-18 or ResNet-50. Stan-\ndard data augmentation is applied, including random re-\nsized crop and horizontal ﬂip. For meta-learning stage, we\nuse the SGD optimizer with momentum 0.9. The learning\nrate is ﬁxed as 0.001. The batch size is 4, i.e. each training\nbatch contains 4 few-shot tasks to compute the average loss.\nThe cosine scaling parameter τ is initialized as 10.\nWe also apply consistent sampling for evaluating the per-\nformance. For the novel class split in a dataset, the sampling\nof testing few-shot tasks follows a deterministic order. Con-\nsistent sampling allows us to get a better model comparison\nwith the same number of sampled tasks. In the following\nsections, when the conﬁdence interval is omitted in the ta-\nble, it indicates that a ﬁxed set of 800 testing tasks are sam-\npled for estimating the performance.\n4.3. Results\nFollowing the standard-setting, we conduct experiments\non miniImageNet and tieredImageNet, the results are shown\nin Table 2 and 3 respectively. To get a fair comparison to\nprior works, we perform model selection according to the\nvalidation set. On both datasets, we observe that the Meta-\nBaseline achieves competitive performance to state-of-the-\nart methods. We highlight that many methods for compar-\nison introduce more parameters and architecture designs\n(e.g. self-attention in [33]), while Meta-Baseline has the\nminimum parameters and the simplest design. We also no-\ntice that the simple Classiﬁer-Baseline can achieve compet-\nitive performance when compared to meta-learning meth-\nods, especially in 5-shot tasks. We observe that the meta-\nlearning stage consistently improves Classiﬁer-Baseline on\n5\n",
    "Model\nBackbone\n1-shot\n5-shot\nClassiﬁer-Baseline (ours)\nResNet-18\n83.51 ± 0.22\n94.82 ± 0.10\nMeta-Baseline (ours)\nResNet-18\n86.39 ± 0.22\n94.82 ± 0.10\nClassiﬁer-Baseline (ours)\nResNet-50\n86.07 ± 0.21\n96.14 ± 0.08\nMeta-Baseline (ours)\nResNet-50\n89.70 ± 0.19\n96.14 ± 0.08\nTable 4: Results on ImageNet-800. Average 5-way accuracy (%) is reported with 95% conﬁdence interval.\n1\n15\n30\nepochs\n82.0\n84.0\n86.0\n88.0\n90.0\n5-way acc (%)\nminiImageNet, 1-shot\n1\n15\n30\nepochs\n92.4\n93.0\n93.6\n94.2\n94.8\nminiImageNet, 5-shot\n1\n15\n30\nepochs\n86.4\n87.0\n87.6\n88.2\n88.8\ntieredImageNet, 1-shot\n1\n15\n30\nepochs\n93.6\n94.0\n94.4\n94.8\ntieredImageNet, 5-shot\n61.5\n62.0\n62.5\n63.0\n63.5\n78.9\n79.2\n79.5\n79.8\n80.1\n66.5\n67.2\n67.9\n68.6\n81.0\n81.5\n82.0\n82.5\n83.0\nMeta-Baseline, Meta-Learning Stage (ResNet-12)\nbase class generalization\nnovel class generalization\nFigure 2: Objective discrepancy of meta-learning on miniImageNet and tieredImageNet. Each epoch contains 200\ntraining batches. Average 5-way accuracy (%) is reported.\n1\n30\n60\n90\nepochs\n94.8\n95.2\n95.6\n96.0\n5-way acc (%)\nResNet-50, 1-shot\n1\n30\n60\n90\nepochs\n98.1\n98.2\n98.3\n98.4\n98.5\nResNet-50, 5-shot\n87.5\n88.2\n88.9\n89.6\n94.8\n95.1\n95.4\n95.7\n96.0\nbase class generalization\nnovel class generalization\nFigure 3: Objective discrepancy of meta-learning on\nImageNet-800. Each epoch contains 500 training batches.\nAverage 5-way accuracy (%) is reported.\nminiImageNet. Compared to miniImageNet, we ﬁnd that\nthe gap between Meta-Baseline and Classiﬁer-Baseline is\nsmaller on tieredImageNet, and the meta-learning stage\ndoes not improve 5-shot in this case.\nWe further evaluate our methods on the larger dataset\nImageNet-800.\nIn this larger-scale experiment, we ﬁnd\nfreezing the Batch Normalization layer [9] (set to eval\nmode) is beneﬁcial.\nThe results are shown in Table 4.\nFrom the results, we observe that in this large dataset Meta-\nBaseline improves Classiﬁer-Baseline in 1-shot, while it is\nnot improving the performance in 5-shot.\n5. Observations and Hypothesis\n5.1. Objective discrepancy in meta-learning\nDespite\nthe\nimprovements\nof\nmeta-learning\nover\nClassiﬁer-Baseline, we observe the test performance drops\nduring the meta-learning stage. While a common assump-\ntion for this phenomenon is overﬁtting, we observe that this\nissue seems not to be mitigated on larger datasets. To further\nlocate the issue, we propose to evaluate base class general-\nization and novel class generalization. Base class general-\nization is measured by sampling tasks from unseen images\nin base classes, while novel class generalization refers to the\nperformance of few-shot tasks sampled from novel classes.\nThe base class generalization is the generalization in the in-\nput distribution for which the model is trained, it decouples\nthe commonly deﬁned generalization and class-level trans-\nfer performance, which helps for locating the reason for the\nperformance drop.\nFigure 2 and 3 demonstrate the meta-learning stage of\nMeta-Baseline on different datasets. We ﬁnd that during the\nmeta-learning stage, when the base class generalization is\nincreasing, the novel class generalization can be decreasing\ninstead. This fact indicates that over a converged whole-\nclassiﬁcation model, the meta-learning objective itself, i.e.\nmaking the embedding generalize better in few-shot tasks\nfrom base classes, can have a negative effect on the perfor-\nmance of few-shot tasks from novel classes. It also gives a\n6\n",
    "Task\nModel\nmini-tiered\nmini-shufﬂed\nfull-tiered\nfull-shufﬂed\n1-shot\nClassiﬁer-Baseline\n56.91\n61.64\n68.76\n77.67\nMeta-Baseline\n58.44\n65.88\n69.52\n80.48\n∆\n+1.53\n+4.24\n+0.76\n+2.81\n5-shot\nClassiﬁer-Baseline\n74.30\n79.26\n84.07\n90.58\nMeta-Baseline\n74.63\n80.58\n84.07\n90.67\n∆\n+0.33\n+1.32\n+0.00\n+0.09\nTable 5: Effect of dataset properties. Average 5-way accuracy (%), with ResNet-12.\nTraining\nBase gen.\nNovel gen.\n1-shot\nw/ ClsTr\n86.42\n63.33\nw/o ClsTr\n86.74\n58.54\n5-shot\nw/ ClsTr\n93.54\n80.02\nw/o ClsTr\n94.47\n74.95\nTable 6: Comparison on Meta-Baseline training from\nscratch. Average 5-way accuracy (%), with ResNet-12 on\nminiImageNet. ClsTr: classiﬁcation training stage.\nMethod\n1-shot\n5-shot\nClassiﬁer-Baseline\n60.58\n79.24\nClassiﬁer-Baseline (Euc.)\n56.29\n78.93\nMeta-Baseline\n63.33\n80.02\nMeta-Baseline (Euc.)\n60.19\n79.50\nTable 7: Importance of inheriting a good metric. Aver-\nage 5-way accuracy (%), with ResNet-12 on miniImageNet.\npossible explanation for why such phenomenon is not mit-\nigated on larger datasets, as this is not sample-level over-\nﬁtting, but class-level overﬁtting, which is caused by the\nobjective discrepancy that the underlying training class dis-\ntribution is different from testing class distribution.\nThis observation suggests that we may reconsider the\nmotivation of the meta-learning framework for few-shot\nlearning. In some settings, optimizing towards the train-\ning objective with a consistent form as the testing objective\n(except the inevitable class difference) may have an even\nnegative effect. It is also likely that the whole-classiﬁcation\nlearns the embedding with stronger class transferability, and\nmeta-learning makes the model perform better at N-way\nK-shot tasks but tends to lose the class transferability.\n5.2. Effect of whole-classiﬁcation training before\nmeta-learning\nAccording to our hypothesis, the whole-classiﬁcation\npre-trained model has provided extra class transferabil-\nity for the meta-learning model, therefore, it is natural to\ncompare Meta-Baseline with and without the classiﬁcation\ntraining stage. The results are shown in Table 6. We observe\nthat Meta-Baseline trained without classiﬁcation training\nstage can actually achieve higher base class generalization,\nbut its novel class generalization is much lower when com-\npared to Meta-Baseline with whole-classiﬁcation training.\nThese results support our hypothesis, that the whole-\nclassiﬁcation training provides the embedding with stronger\nclass transferability, which signiﬁcantly helps novel class\ngeneralization.\nInterestingly, TADAM [16] ﬁnds that\nco-training the meta-learning objective with a whole-\nclassiﬁcation task is beneﬁcial, which may be potentially\nrelated to our hypothesis. While our results show it is likely\nthat the key effect of the whole-classiﬁcation objective is\nimproving the class transferability, it also indicates a po-\ntential trade-off that the whole-classiﬁcation objective can\nhave a negative effect on base class generalization.\n5.3. What makes Meta-Baseline a strong baseline?\nAs a method with a similar objective as ProtoNet [24],\nMeta-Baseline achieves nearly 10% higher accuracy on 1-\nshot in Table 2. The observations and hypothesis in previ-\nous sections potentially explain its strength, as it starts with\nthe embedding of a whole-classiﬁcation model which has\nstronger class transferability.\nWe perform further experiments, that in Meta-Baseline\n(with classiﬁcation training stage) we replace the cosine\ndistance with the squared Euclidean distance proposed in\nProtoNet [24]. To get a fair comparison, we also include\nthe learnable scalar τ with a proper initialization value 0.1.\nThe results are shown in Table 7.\nWhile ProtoNet [24]\nﬁnds that squared Euclidean distance (as a Bregman di-\nvergence) works better than cosine distance when perform-\ning meta-learning from scratch, here we start meta-learning\nfrom Classiﬁer-Baseline and we observe that cosine sim-\nilarity works much better. A potential reason is that, as\nshown in Table 7, cosine nearest-centroid works much bet-\nter than nearest-centroid with squared Euclidean distance in\nClassiﬁer-Baseline (note that this is just the evaluation met-\nric and has no changes in training). Inheriting a good metric\n7\n",
    "for Classiﬁer-Baseline might be the key that makes Meta-\nBaseline strong. According to our hypothesis, the embed-\nding from the whole-classiﬁcation model has strong class\ntransferability, inheriting a good metric potentially mini-\nmizes the future modiﬁcations on the embedding from the\nwhole-classiﬁcation model, thus it can keep the class trans-\nferability better and achieve higher performance.\n5.4. Effect of dataset properties\nWe construct four variants from the tieredImageNet\ndataset. Speciﬁcally, full-tiered refers to the original tiered-\nImageNet, full-shufﬂed is constructed by randomly shuf-\nﬂing the classes in tieredImageNet and re-splitting the\nclasses into training, validation, and test set.\nThe mini-\ntiered and mini-shufﬂed datasets are constructed from full-\ntiered and full-shufﬂed respectively, their training set is con-\nstructed by randomly selecting 64 classes with 600 images\nfrom each class in the full training set, while the validation\nset and the test set remain unchanged. Since tieredImageNet\nseparates training classes and testing classes into different\nsuper categories, shufﬂing these classes will mix the classes\nin different super categories together and make the distribu-\ntion of base classes and novel classes closer.\nOur previous experiments show that base class general-\nization is always improving, if novel classes are covered\nby the distribution of base classes, the novel class gener-\nalization should also keep increasing. From Table 5, we\ncan see that from mini-tiered to mini-shufﬂed, and from\nfull-tiered to full-shufﬂed, the improvement achieved by the\nmeta-learning stage gets signiﬁcantly larger, which consis-\ntently supports our hypothesis. Therefore, our results in-\ndicate it is likely that meta-learning is mostly effective over\nwhole-classiﬁcation training when novel classes are similar\nto base classes.\nWe also observe that other factors may affect the im-\nprovement of meta-learning. From mini-tiered to full-tiered\nand from mini-shufﬂed to full-shufﬂed, when the dataset\ngets larger the improvements become less. A potential hy-\npothesis could be that the class transferability advantage of\nwhole-classiﬁcation training becomes more obvious when\ntrained on large datasets. From the results of our experi-\nments in Table 2, 3, 4, 5, we observe that the improvement\nof the meta-learning stage in 5-shot is less than 1-shot. We\nhypothesize this is because when there are more shots, tak-\ning average embedding becomes a more reasonable choice\nto estimate the class center in Classiﬁer-Baseline, therefore\nthe advantage of meta-learning becomes less.\n5.5. The trade-off between meta-learning and\nwhole-classiﬁcation\nAll of the experiments in previous sections support a key\nhypothesis, that there exists a trade-off: the meta-learning\nobjective learns better embedding for N-way K-shot tasks\n(in the same distribution), while the whole-classiﬁcation ob-\njective learns embedding with stronger class transferabil-\nity. Optimizing towards one objective may hurt the strength\nof another objective. With this hypothesis, Meta-Baseline\nbalances this trade-off by choosing to calibrate the whole-\nclassiﬁcation embedding with meta-learning and inherit the\nmetric with high initial performance.\nThe discrepancy between base class generalization and\nnovel class generalization also considers the effectiveness\nof meta-learning and whole-classiﬁcation from the perspec-\ntive of datasets. Speciﬁcally, when novel classes are similar\nenough to base classes or the base classes are sufﬁcient to\ncover the distribution of novel classes, novel class gener-\nalization should converge to base class generalization. In\npractice, this can be potentially achieved by collecting base\nclasses that are similar to the target novel classes. In this\ncase, it may be possible that the novel meta-learning algo-\nrithms outperform the whole-classiﬁcation baselines again.\n6. Additional Results on Meta-Dataset\nMeta-Dataset [28] is a new benchmark proposed for few-\nshot learning, it consists of diverse datasets for training and\nevaluation. They also propose to generate few-shot tasks\nwith a variable number of ways and shots, for having a\nsetting closer to the real world. We follow the setting in\nMeta-Dataset [28] and use ResNet-18 as the backbone, with\nthe original image size of 126×126, which is resized to be\n128×128 before feeding into the network. For the classiﬁ-\ncation training stage, we apply the training setting similar to\nour setting in ImageNet-800. For the meta-learning stage,\nthe model is trained for 5000 iterations with one task in each\niteration.\nThe left side of Table 8 demonstrates the models trained\nwith samples in ILSVRC-2012 only. We observe that the\nMeta-Baseline does not signiﬁcantly improve Classiﬁer-\nBaseline under this setting in our experiments, possibly due\nto the average number of shots are high.\nThe right side of Table 8 shows the results when the\nmodels are trained on all datasets, except Trafﬁc Signs and\nMSCOCO which have no training samples. The Classiﬁer-\nBaseline is trained as a multi-dataset classiﬁer, i.e. an en-\ncoder together with multiple FC layers over the encoded\nfeature to output the logits for different datasets. The clas-\nsiﬁcation training stage has the same number of iterations\nas training on ILSVRC only, to mimic the ILSVRC train-\ning, a batch has 0.5 probability to be from ILSVRC and 0.5\nprobability to be uniformly sampled from one of the other\ndatasets. For Classiﬁer-Baseline, comparing to the results\non the left side of Table 8, we observe that while the per-\nformance on ILSVRC is worse, the performances on other\ndatasets are mostly improved due to having their samples in\ntraining. It can be also noticed that the cases where Meta-\nBaseline improves Classiﬁer-Baseline are mostly on the\n8\n",
    "Trained on ILSVRC\nTrained on all datasets\nDataset\nfo-Proto-MAML\nClassiﬁer/Meta\nfo-Proto-MAML\nClassiﬁer\nMeta\n[28]\n(ours)\n[28]\n(ours)\n(ours)\nILSVRC\n49.5\n59.2\n46.5\n55.0\n48.0\nOmniglot\n63.4\n69.1\n82.7\n76.9\n89.4\nAircraft\n56.0\n54.1\n75.2\n69.8\n81.7\nBirds\n68.7\n77.3\n69.9\n78.3\n77.3\nTextures\n66.5\n76.0\n68.3\n71.4\n64.5\nQuick Draw\n51.5\n57.3\n66.8\n62.7\n74.5\nFungi\n40.0\n45.4\n42.0\n55.4\n60.2\nVGG Flower\n87.2\n89.6\n88.7\n90.6\n83.8\nTrafﬁc Signs\n48.8\n66.2\n52.4\n69.3\n59.5\nMSCOCO\n43.7\n55.7\n41.7\n53.1\n43.6\nTable 8: Additional results on Meta-Dataset. Average accuracy (%), with variable number of ways and shots. The fo-Proto-\nMAML method is from Meta-Dataset [28], Classiﬁer and Meta refers to Classiﬁer-Baseline and Meta-Baseline respectively,\n1000 tasks are sampled for evaluating Classiﬁer or Meta. Note that Trafﬁc Signs and MSCOCO have no training set.\ndatasets which are “less relevant” to ILSVRC (the dataset\n“relevance” could be shown in Dvornik et al. [3]). A po-\ntential reason is that the multi-dataset classiﬁcation train-\ning stage samples ILSVRC with 0.5 probability, similar\nto ILSVRC training, the meta-learning stage is hard to\nimprove on ILSVRC, therefore those datasets relevant to\nILSVRC will have similar properties so that it is hard to\nimprove on them.\n7. Conclusion and Discussion\nIn this work, we presented a simple Meta-Baseline that\nhas been overlooked for few-shot learning. Without any ad-\nditional parameters or complex design choices, it is compet-\nitive to state-of-the-art methods on standard benchmarks.\nOur experiments indicate that there might be an objec-\ntive discrepancy in the meta-learning framework for few-\nshot learning, i.e. a meta-learning model generalizing better\non unseen tasks from base classes might have worse perfor-\nmance on tasks from novel classes. This provides a possible\nexplanation that why some complex meta-learning methods\ncould not get signiﬁcantly better performance than simple\nwhole-classiﬁcation. While most recent works focus on im-\nproving the meta-learning structures, many of them did not\nexplicitly address the issue of class transferability. Our ob-\nservations suggest that the objective discrepancy might be a\npotential key challenge to tackle.\nWhile many novel meta-learning algorithms are pro-\nposed and some recent works report that simple whole-\nclassiﬁcation training is good enough for few-shot learn-\ning, we show that meta-learning is still effective over\nwhole-classiﬁcation models. We observe a potential trade-\noff between the objectives of meta-learning and whole-\nclassiﬁcation.\nFrom the perspective of datasets, we\ndemonstrate how the preference between meta-learning and\nwhole-classiﬁcation changes according to class similarity\nand other factors, indicating that these factors may need\nmore attention for model comparisons in future work.\nAcknowledgements. This work was supported, in part, by grants from\nDARPA LwLL, NSF 1730158 CI-New: Cognitive Hardware and Software\nEcosystem Community Infrastructure (CHASE-CI), NSF ACI-1541349\nCC*DNI Paciﬁc Research Platform, and gifts from Qualcomm, TuSim-\nple and Picsart. Prof. Darrell was supported, in part, by DoD including\nDARPA’s XAI, LwLL, and/or SemaFor programs, as well as BAIR’s in-\ndustrial alliance programs. We thank Hang Gao for the helpful discussions.\nReferences\n[1] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank\nWang, and Jia-Bin Huang. A closer look at few-shot classi-\nﬁcation. In International Conference on Learning Represen-\ntations, 2019. 1, 2, 3, 5, 11, 12\n[2] Guneet S Dhillon, Pratik Chaudhari, Avinash Ravichandran,\nand Stefano Soatto. A baseline for few-shot image classiﬁ-\ncation. arXiv preprint arXiv:1909.02729, 2019. 2\n[3] Nikita Dvornik, Cordelia Schmid, and Julien Mairal. Select-\ning relevant features from a universal representation for few-\nshot classiﬁcation. arXiv preprint arXiv:2003.09338, 2020.\n9\n[4] Li Fei-Fei, Rob Fergus, and Pietro Perona. One-shot learning\nof object categories. IEEE transactions on pattern analysis\nand machine intelligence, 28(4):594–611, 2006. 1\n[5] Chelsea Finn, Pieter Abbeel, and Sergey Levine.\nModel-\nagnostic meta-learning for fast adaptation of deep networks.\nIn Proceedings of the 34th International Conference on Ma-\nchine Learning-Volume 70, pages 1126–1135. JMLR. org,\n2017. 1, 2, 5\n9\n",
    "[6] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot\nvisual learning without forgetting.\nIn Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 4367–4375, 2018. 1, 2, 3, 4, 11\n[7] Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and\nThomas Grifﬁths. Recasting gradient-based meta-learning as\nhierarchical bayes. arXiv preprint arXiv:1801.08930, 2018.\n2\n[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 4\n[9] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. arXiv preprint arXiv:1502.03167, 2015. 6, 11\n[10] Hankook Lee, Sung Ju Hwang, and Jinwoo Shin.\nSelf-\nsupervised label augmentation via input transformations.\nIn International Conference on Machine Learning, pages\n5714–5724. PMLR, 2020. 5\n[11] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and\nStefano Soatto. Meta-learning with differentiable convex op-\ntimization. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 10657–10665,\n2019. 1, 2, 4, 5, 11\n[12] Aoxue Li, Weiran Huang, Xu Lan, Jiashi Feng, Zhenguo Li,\nand Liwei Wang. Boosting few-shot learning with adaptive\nmargin loss. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 12576–\n12584, 2020. 5\n[13] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter\nAbbeel. A simple neural attentive meta-learner. In Inter-\nnational Conference on Learning Representations, 2018. 2,\n5\n[14] Tsendsuren Munkhdalai and Hong Yu. Meta networks. In\nProceedings of the 34th International Conference on Ma-\nchine Learning-Volume 70, pages 2554–2563. JMLR. org,\n2017. 2\n[15] Tsendsuren Munkhdalai, Xingdi Yuan, Soroush Mehri, and\nAdam Trischler. Rapid adaptation with conditionally shifted\nneurons. arXiv preprint arXiv:1712.09926, 2017. 2, 5\n[16] Boris Oreshkin, Pau Rodr´ıguez L´opez, and Alexandre La-\ncoste. Tadam: Task dependent adaptive metric for improved\nfew-shot learning. In Advances in Neural Information Pro-\ncessing Systems, pages 721–731, 2018. 2, 4, 5, 7, 11\n[17] Hang Qi, Matthew Brown, and David G Lowe. Low-shot\nlearning with imprinted weights. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\npages 5822–5830, 2018. 4\n[18] Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan L Yuille. Few-\nshot image recognition by predicting parameters from activa-\ntions. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 7229–7238, 2018. 5\n[19] Sachin Ravi and Hugo Larochelle. Optimization as a model\nfor few-shot learning.\nIn In International Conference on\nLearning Representations (ICLR), 2017. 2\n[20] Mengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell,\nKevin Swersky, Joshua B Tenenbaum, Hugo Larochelle, and\nRichard S Zemel. Meta-learning for semi-supervised few-\nshot classiﬁcation. arXiv preprint arXiv:1803.00676, 2018.\n4\n[21] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al.\nImagenet large\nscale visual recognition challenge. International journal of\ncomputer vision, 115(3):211–252, 2015. 4\n[22] Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol\nVinyals, Razvan Pascanu, Simon Osindero, and Raia Had-\nsell.\nMeta-learning with latent embedding optimization.\nIn International Conference on Learning Representations,\n2019. 1, 2, 5\n[23] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan\nWierstra, and Timothy Lillicrap.\nMeta-learning with\nmemory-augmented neural networks. In International con-\nference on machine learning, pages 1842–1850, 2016. 2\n[24] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical\nnetworks for few-shot learning. In Advances in Neural Infor-\nmation Processing Systems, pages 4077–4087, 2017. 2, 3, 5,\n7\n[25] Qianru Sun, Yaoyao Liu, Tat-Seng Chua, and Bernt Schiele.\nMeta-transfer learning for few-shot learning.\nIn Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 403–412, 2019. 1, 2, 4, 5\n[26] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS\nTorr, and Timothy M Hospedales. Learning to compare: Re-\nlation network for few-shot learning. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 1199–1208, 2018. 1, 2, 5\n[27] Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenen-\nbaum, and Phillip Isola. Rethinking few-shot image classi-\nﬁcation: a good embedding is all you need? arXiv preprint\narXiv:2003.11539, 2020. 1, 2, 3, 11\n[28] Eleni Triantaﬁllou, Tyler Zhu, Vincent Dumoulin, Pascal\nLamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles\nGelada, Kevin Swersky, Pierre-Antoine Manzagol, et al.\nMeta-dataset: A dataset of datasets for learning to learn from\nfew examples. arXiv preprint arXiv:1903.03096, 2019. 8, 9\n[29] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan\nWierstra, et al. Matching networks for one shot learning. In\nAdvances in neural information processing systems, pages\n3630–3638, 2016. 1, 2, 3, 4, 5\n[30] Xin Wang, Fisher Yu, Ruth Wang, Trevor Darrell, and\nJoseph E Gonzalez. Tafe-net: Task-aware feature embed-\ndings for low shot learning. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n1831–1840, 2019. 1\n[31] Yan Wang, Wei-Lun Chao, Kilian Q Weinberger, and Lau-\nrens van der Maaten.\nSimpleshot:\nRevisiting nearest-\nneighbor classiﬁcation for few-shot learning. arXiv preprint\narXiv:1911.04623, 2019. 1, 2, 3\n[32] Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical\nevaluation of rectiﬁed activations in convolutional network.\narXiv preprint arXiv:1505.00853, 2015. 11\n[33] Weijian Xu, yifan xu, Huaijin Wang, and Zhuowen Tu. Con-\nstellation nets for few-shot learning. In International Con-\nference on Learning Representations, 2021. 1, 2, 4, 5, 11\n10\n",
    "1\n40\n80\n120\n160\nepochs\n40\n60\n80\n5-way acc (%)\nminiImageNet, 1-shot\n1\n40\n80\n120\n160\nepochs\n50\n60\n70\n80\n90\n5-way acc (%)\nminiImageNet, 5-shot\nMeta-Baseline, Training from Scratch\nbase class generalization\nnovel class generalization\nFigure 4: Training Meta-Baseline without classiﬁcation-\ntraining stage on miniImageNet.\nDataset\nClassiﬁer\n1-shot\n5-shot\nminiImageNet\nLinear\n60.58\n79.24\nCosine\n61.93\n78.73\ntieredImageNet\nLinear\n68.76\n84.07\nCosine\n67.58\n83.31\nTable 9: Comparison to classiﬁer trained with cosine met-\nric, Average 5-way accuracy (%), with ResNet-12.\nA. Details of ResNet-12\nThe ResNet-12 backbone consists of 4 residual blocks\nthat each residual block has 3 convolutional layers. Each\nconvolutional layer has a 3 × 3 kernel, followed by Batch\nNormalization [9] and Leaky ReLU [32] with 0.1 slope.\nThe channels of convolutional layers in each residual block\nare 64, 128, 256, 512 respectively, a 2×2 max-pooling layer\nis applied after each residual block. Finally, a 5 × 5 global\naverage pooling is applied to get a 512-dimensional feature\nvector.\nThis architecture is consistent with recent works [16, 33].\nSome other recent works also introduce additional parame-\nters and design choices in the backbone (e.g. DropBlock\nand wider channels of 64, 160, 320, 640 in [11, 27]), while\nthese modiﬁcations may make the performance higher, we\ndo not include them here for simplicity.\nB. Training plot of Meta-Baseline without clas-\nsiﬁcation training stage\nWe show the process of training Meta-Baseline from\nscratch (i.e.\nwithout the classiﬁcation-training stage) on\nminiImageNet in Figure 4. We observe that when the learn-\ning rate decays, the novel class generalization quickly starts\nto be decreasing. While it is able to achieve higher base\nclass generalization than Meta-Baseline with classiﬁcation\ntraining, its highest novel class generalization is still much\nworse, suggesting whole-classiﬁcation training may pro-\nvide representations with extra class transferability.\nC. Comparison to cosine classiﬁcation training\nWe compare the effect of classiﬁcation training with\nreplacing the last linear-classiﬁer with cosine nearest-\nneighbor metric which is proposed in prior work [6, 1], the\nresults are shown in Table 9, where Cosine denotes clas-\nsiﬁcation training with cosine metric and Linear denotes\nthe standard classiﬁcation training. On miniImageNet, we\nobserve that Cosine outperforms Linear in 1-shot, but has\nworse performance in 5-shot. On tieredImageNet, we ob-\nserve Linear outperforms Cosine in both 1-shot and 5-shot.\nWe choose to use the linear layer as it is more common and\nwe ﬁnd it works better in more cases.\nD. Objective discrepancy on ImageNet-800\nBesides miniImageNet and tieredImageNet, in our large-\nscale dataset ImageNet-800, we also observe the novel class\ngeneralization decreasing when base class generalization\nis increasing, the training process is demonstrated in Fig-\nure 5.\nFrom the ﬁgure, we see that for both backbones\nof ResNet-18 and ResNet-50, the base class generalization\nperformance is increasing during the training epochs, while\nthe novel class generalization performance quickly starts to\nbe decreasing. These observations are consistent with our\nobservations on miniImageNet and tieredImageNet, which\nfurther support our hypothesis.\nE. Comparison of the Classiﬁer-Baseline and\nBaseline++ [1]\nWe connect the Classiﬁer-Baseline to Baseline++ [1]\nwith a step-by-step ablation study on miniImageNet, the\nresults are shown in Table 10. We see that ﬁne-tuning is\noutperformed by the simple nearest-centroid method with\ncosine metric, and using a standard ImageNet-like opti-\nmizer signiﬁcantly improves the performance of the whole-\nclassiﬁcation method for few-shot learning.\n11\n",
    "1\n30\n60\n90\nepochs\n90.0\n90.9\n91.8\n92.7\n5-way acc (%)\nResNet-18, 1-shot\n1\n30\n60\n90\nepochs\n96.3\n96.6\n96.9\n97.2\n97.5\nResNet-18, 5-shot\n1\n30\n60\n90\nepochs\n94.8\n95.2\n95.6\n96.0\nResNet-50, 1-shot\n1\n30\n60\n90\nepochs\n98.1\n98.2\n98.3\n98.4\n98.5\nResNet-50, 5-shot\n85.2\n85.6\n86.0\n86.4\n93.6\n93.8\n94.0\n94.2\n87.5\n88.2\n88.9\n89.6\n94.8\n95.1\n95.4\n95.7\n96.0\nMeta-Baseline, Meta-Learning Stage (ImageNet-800)\nbase class generalization\nnovel class generalization\nFigure 5: Objective discrepancy of meta-learning on ImageNet-800. Each epoch contains 500 training batches. Average\n5-way accuracy (%) is reported.\ndifference\n5-way 1-shot accuracy (%)\n(default, reported [1])\n51.75 ± 0.80\n(default, reproduced by their code)\n50.84 ± 0.80\nﬁnetune →cosine nearest-centroid\n52.15 ± 0.83\nepoch-300 →epoch-50\n53.37 ± 0.71\nremove color-jittering\n56.06 ± 0.71\n224×224 input size →resizing 84×84 to 224×224\n50.49 ± 0.71\nResNet-18 →ResNet-12\n53.59 ± 0.72\nAdam (lr=0.001, batch=16) →SGD (lr=0.1, batch=128)\n59.19 ± 0.71\nTable 10: Comparison of the Classiﬁer-Baseline and Baseline++ [1].\n12\n"
  ],
  "full_text": "Meta-Baseline: Exploring Simple Meta-Learning for Few-Shot Learning\nYinbo Chen\nUC San Diego\nZhuang Liu\nUC Berkeley\nHuijuan Xu\nPenn State University\nTrevor Darrell\nUC Berkeley\nXiaolong Wang\nUC San Diego\nAbstract\nMeta-learning has been the most common framework for\nfew-shot learning in recent years. It learns the model from\ncollections of few-shot classiﬁcation tasks, which is believed\nto have a key advantage of making the training objective\nconsistent with the testing objective.\nHowever, some re-\ncent works report that by training for whole-classiﬁcation,\ni.e. classiﬁcation on the whole label-set, it can get compa-\nrable or even better embedding than many meta-learning\nalgorithms.\nThe edge between these two lines of works\nhas yet been underexplored, and the effectiveness of meta-\nlearning in few-shot learning remains unclear. In this paper,\nwe explore a simple process: meta-learning over a whole-\nclassiﬁcation pre-trained model on its evaluation metric.\nWe observe this simple method achieves competitive per-\nformance to state-of-the-art methods on standard bench-\nmarks. Our further analysis shed some light on understand-\ning the trade-offs between the meta-learning objective and\nthe whole-classiﬁcation objective in few-shot learning. Our\ncode is available at https://github.com/yinboc/\nfew-shot-meta-baseline.\n1. Introduction\nWhile humans have shown incredible ability to learn\nfrom very few examples and generalize to many different\nnew examples, the current deep learning approaches still\nrely on a large scale of training data. To mimic this hu-\nman ability of generalization, few-shot learning [4, 29] is\nproposed for training networks to understand a new con-\ncept based on a few labeled examples. While directly learn-\ning a large number of parameters with few samples is very\nchallenging and most likely leads to overﬁtting, a practical\nsetting is applying transfer learning: train the network on\ncommon classes (also called base classes) with sufﬁcient\nsamples, then transfer the model to learn novel classes with\na few examples.\nThe meta-learning framework for few-shot learning fol-\nlows the key idea of learning to learn. Speciﬁcally, it sam-\nples few-shot classiﬁcation tasks from training samples be-\nlonging to the base classes and optimizes the model to per-\nform well on these tasks. A task typically takes the form\nof N-way and K-shot, which contains N classes with K\nsupport samples and Q query samples in each class. The\ngoal is to classify these N × Q query samples into the\nN classes based on the N × K support samples. Under\nthis framework, the model is directly optimized on few-\nshot classiﬁcation tasks. The consistency between the ob-\njectives of training and testing is considered as the key ad-\nvantage of meta-learning. Motivated by this idea, many re-\ncent works [26, 6, 25, 30, 5, 22, 11, 33] focus on improving\nthe meta-learning structure, and few-shot learning itself has\nbecome a common testbed for evaluating meta-learning al-\ngorithms.\nHowever, some recent works ﬁnd that training for whole-\nclassiﬁcation, i.e. classiﬁcation on the whole training label-\nset (base classes), provides the embedding that is compa-\nrable or even better than many recent meta-learning algo-\nrithms.\nThe effectiveness of whole-classiﬁcation models\nhas been reported in both prior works [6, 1] and some con-\ncurrent works [31, 27]. Meta-learning makes the form of\ntraining objective consistent with testing, but why it turns\nout to learn even worse embedding than simple whole-\nclassiﬁcation?\nWhile there are several possible reasons,\ne.g. optimization difﬁculty or overﬁtting, the answer has\nnot been clearly studied yet. It remains even unclear that\nwhether meta-learning is still effective compared to whole-\nclassiﬁcation in few-shot learning.\nIn this work, we aim at exploring the edge between\nwhole-classiﬁcation and meta-learning by decoupling the\ndiscrepancies. We start with Classiﬁer-Baseline: a whole-\nclassiﬁcation method that is similarly proposed in concur-\nrent works [31, 27]. In Classiﬁer-Baseline, we ﬁrst train\na classiﬁer on base classes, then remove the last fully-\nconnected (FC) layer which is class-dependent. During test\ntime, it computes mean embedding of support samples for\neach novel class as their centroids, and classiﬁes query sam-\nples to the nearest centroid with cosine distance. We ob-\nserve this baseline method outperforms many recent meta-\nlearning algorithms.\nIn order to understand whether meta-learning is still ef-\n1\narXiv:2003.04390v4  [cs.CV]  19 Aug 2021\n\n\nfective compared to whole-classiﬁcation, a natural experi-\nment is to see what happens if we perform further meta-\nlearning over a converged Classiﬁer-Baseline on its evalu-\nation metric (i.e. cosine nearest-centroid). As a resulting\nmethod, it is similar to MatchingNet [29] or ProtoNet [24]\nwith an additional classiﬁcation pre-training stage.\nWe\nobserve that meta-learning can still improve Classiﬁer-\nBaseline, and it achieves competitive performance to state-\nof-the-art methods on standard benchmarks. We call this\nsimple method Meta-Baseline.\nWe highlight that as a\nmethod, all the individual components of Meta-Baseline\nhave been proposed in prior works, but to the best of our\nknowledge, it has been overlooked that none of the prior\nworks studies them as a whole. We further decouple the\ndiscrepancies by evaluating on two types of generaliza-\ntion: base class generalization denotes performance on\nfew-shot classiﬁcation tasks from unseen data in the base\nclasses, which follows the common deﬁnition of general-\nization (i.e. evaluated in the training distribution); and novel\nclass generalization denotes performance on few-shot clas-\nsiﬁcation tasks from data in novel classes, which is the\ngoal of the few-shot learning problem. We observe that:\n(i) During meta-learning, improving base class generaliza-\ntion can lead to worse novel class generalization; (ii) When\ntraining Meta-Baseline from scratch (i.e. without whole-\nclassiﬁcation training), it achieves higher base-class gener-\nalization but much lower novel class generalization.\nOur observations suggest that there could be a trade-\noff between the objectives of meta-learning and whole-\nclassiﬁcation. It is likely that meta-learning learns the em-\nbedding that works better for N-way K-shot tasks, while\nwhole-classiﬁcation learns the embedding with stronger\nclass transferability.\nWe ﬁnd that the main advantage\nof training for whole-classiﬁcation before meta-learning is\nlikely to be improving class transferability. Our further ex-\nperiments provide a potential explanation of what makes\nMeta-Baseline a strong baseline: by inheriting one of the\nmost effective evaluation metrics of the whole-classiﬁcation\nmodel, it maximizes the reusing of the embedding with\nstrong class transferability. From another perspective, our\nresults also rethink the comparison between meta-learning\nand whole-classiﬁcation from the perspective of datasets.\nWhen base classes are collected to cover the distribution of\nnovel classes, novel-class generalization should converge to\nbase-class generalization and the strength of meta-learning\nmay overwhelm the strength of whole-classiﬁcation.\nIn summary, our contributions are as following:\n• We present a simple Meta-Baseline that has been over-\nlooked in prior work. It achieves competitive perfor-\nmance to state-of-the-art methods on standard bench-\nmarks and is easy to follow.\n• We observe a trade-off between the objectives of meta-\nlearning and whole-classiﬁcation, which potentially\nexplains the success of Meta-Baseline and rethinks the\neffectiveness of both objectives in few-shot learning.\n2. Related Work\nMost recent approaches for few-shot learning follow the\nmeta-learning framework. The various meta-learning ar-\nchitectures for few-shot learning can be roughly catego-\nrized into three groups. Memory-based methods [19, 15,\n23, 13, 14] are based on the idea to train a meta-learner\nwith memory to learn novel concepts (e.g.\nan LSTM-\nbased meta-learner). Optimization-based methods [7, 22]\nfollows the idea of differentializing an optimization pro-\ncess over support-set within the meta-learning framework:\nMAML [5] ﬁnds an initialization of the neural network that\ncan be adapted to any novel task using a few optimiza-\ntion steps. MetaOptNet [11] learns the feature represen-\ntation that can generalize well for a linear support vector\nmachine (SVM) classiﬁer. Besides explicitly considering\nthe dynamic learning process, metric-based methods [29]\nmeta-learn a deep representation with a metric in feature\nspace. For example, Prototypical Networks [24] compute\nthe average feature for each class in support-set and clas-\nsify query samples by the nearest-centroid method. They\nuse Euclidean distance since it is a Bregman divergence.\nRelation Networks [26] further generalizes this framework\nby proposing a relation module as a learnable metric jointly\ntrained with deep representations. TADAM [16] proposes to\nuse a task conditioned metric resulting in a task-dependent\nmetric space.\nWhile signiﬁcant progress is made in the meta-learning\nframework, some recent works challenge the effectiveness\nof meta-learning with simple whole-classiﬁcation, i.e.\na\nclassiﬁcation model on the whole training label-set. Co-\nsine classiﬁer [6] and Baseline++ [1] perform whole-\nclassiﬁcation training by replacing the top linear layer with\na cosine classiﬁer, and they adapt the classiﬁer to a few-\nshot classiﬁcation task of novel classes by performing near-\nest centroid or ﬁne-tuning a new layer respectively. They\nshow these whole-classiﬁcation models can achieve com-\npetitive performance compared to several popular meta-\nlearning models. Another recent work [2] studies on a trans-\nductive setting. Along with these baseline methods, more\nadvanced meta-learning methods [25, 11, 33] are proposed\nand they set up new state-of-the-art results. The effective-\nness of whole-classiﬁcation is then revisited in two of the\nconcurrent works [31, 27] with improved design choices.\nBy far, the effectiveness of meta-learning compared to\nwhole-classiﬁcation in few-shot learning is still unclear,\nsince the edge between whole-classiﬁcation models and\nmeta-learning models remains underexplored. The goal of\nthis work is to explore the insights behind the phenomenons.\nOur experiments show a potential trade-off between the\n2\n\n\nMethod\nWhole-classiﬁcation training\nMeta-learning\nOthers\nMatching Networks [29]\nno / yes (large models)\nattention + cosine\nFCE\nPrototypical Networks [24]\nno\ncentroid + Euclidean\n-\nBaseline++ [1]\nyes (cosine classiﬁer)\n-\nﬁne-tuning\nMeta-Baseline (ours)\nyes\ncentroid + cosine (∗τ)\n-\nTable 1: Overview of method comparison. We summarize the differences between Meta-Baseline and prior methods.\nmeta-learning and whole-classiﬁcation objectives, which\nprovides a more clear understanding of the comparison be-\ntween both objectives for few-shot learning.\nAs a method, similar ideas to Classiﬁer-Baseline are con-\ncurrently reported in recent works [31, 27]. Unlike some\nprior works [6, 1], Classiﬁer-Baseline does not replace the\nlast layer with cosine classiﬁer during training, it trains\nthe whole-classiﬁcation model with a linear layer on the\ntop and applies cosine nearest-centroid metric during the\ntest time for few-shot classiﬁcation on novel classes. The\nMeta-Baseline is meta-learning over a converged Classiﬁer-\nBaseline on its evaluation metric (cosine nearest-centroid).\nIt is similar (with inconspicuous and important differences\nas shown in Table 1) to those simple and classical metric-\nbased meta-learning methods [29, 24]. The main purpose of\nMeta-Baseline in this paper is to understand the comparison\nbetween whole-classiﬁcation and meta-learning objectives,\nbut we ﬁnd it is also a simple meta-learning baseline that\nhas been overlooked. While every individual component in\nMeta-Baseline is not novel, to the best of our knowledge,\nnone of the prior works studies them as a whole.\n3. Method\n3.1. Problem deﬁnition\nIn standard few-shot classiﬁcation, given a labeled\ndataset of base classes Cbase with a large number of im-\nages, the goal is to learn concepts in novel classes Cnovel\nwith a few samples. In an N-way K-shot few-shot clas-\nsiﬁcation task, the support-set contains N classes with K\nsamples per class, the query-set contains samples from the\nsame N classes with Q samples per class, and the goal is to\nclassify the N × Q query images into N classes.\n3.2. Classiﬁer-Baseline\nClassiﬁer-Baseline is a whole-classiﬁcation model, i.e.\na classiﬁcation model trained for the whole label-set. It\nrefers to training a classiﬁer with classiﬁcation loss on all\nbase classes and performing few-shot tasks with the cosine\nnearest-centroid method. Speciﬁcally, we train a classiﬁer\non all base classes with standard cross-entropy loss, then re-\nmove its last FC layer and get the encoder fθ, which maps\nthe input to embedding. Given a few-shot task with the\nsupport-set S, let Sc denote the few-shot samples in class\nc, it computes the average embedding wc as the centroid of\nclass c:\nwc =\n1\n|Sc|\nX\nx∈Sc\nfθ(x),\n(1)\nthen for a query sample x in a few-shot task, it predicts the\nprobability that sample x belongs to class c according to the\ncosine similarity between the embedding of sample x and\nthe centroid of class c:\np(y = c | x) =\nexp\n\u0000⟨fθ(x), wc⟩\n\u0001\nP\nc′ exp\n\u0000⟨fθ(x), wc′⟩\n\u0001,\n(2)\nwhere ⟨·, ·⟩denotes the cosine similarity of two vectors.\nSimilar methods to Classiﬁer-Baseline have also been\nproposed in concurrent works [31, 27]. Compared to Base-\nline++ [1], the Classiﬁer-Baseline does not use the cosine\nclassiﬁer for training or perform ﬁne-tuning during testing,\nwhile it performs better on standard benchmarks. In this\nwork, we choose Classiﬁer-Baseline as the representative of\nwhole-classiﬁcation models for few-shot learning. For sim-\nplicity and clarity, we do not introduce additional complex\ntechniques for this whole-classiﬁcation training.\n3.3. Meta-Baseline\nFigure 1 visualizes the Meta-Baseline. The ﬁrst stage\nis the classiﬁcation training stage, it trains a Classiﬁer-\nBaseline, i.e. training a classiﬁer on all bases classes and\nremove its last FC layer to get fθ. The second stage is the\nmeta-learning stage, which optimizes the model on the eval-\nuation metric of Classiﬁer-Baseline. Speciﬁcally, given the\nclassiﬁcation-trained feature encoder fθ, it samples N-way\nK-shot tasks (with N × Q query samples) from training\nsamples in base classes. To compute the loss for each task,\nin support-set it computes the centroids of N classes de-\nﬁned in Equation 1, which are then used to compute the\npredicted probability distribution for each sample in query-\nset deﬁned in Equation 2. The loss is a cross-entropy loss\ncomputed from p and the labels of the samples in the query-\nset. During training, each training batch can contain several\ntasks and the average loss is computed.\nSince cosine similarity has the value range of [−1, 1],\nwhen it is used to compute the logits, it can be helpful to\n3\n\n\nmean\ncos\nscore\nMeta-Baseline\ntraining\nClassifier-Baseline / Meta-Baseline\nevaluation\nsupport-set\nquery-set\nClassifier-Baseline\ntraining\nclassification\non base classes\n𝑓𝜃\nloss\n𝑓𝜃\n𝑓𝜃\nMeta-Learning Stage\nClassification Training Stage\nrepresentation\ntransfer\nF\nC\nlabel\n𝜏\nFigure 1: Classiﬁer-Baseline and Meta-Baseline. Classiﬁer-Baseline is to train a classiﬁcation model on all base classes\nand remove its last FC layer to get the encoder fθ. Given a few-shot task, it computes the average feature for samples of each\nclass in support-set, then it classiﬁes a sample in query-set by nearest-centroid with cosine similarity as distance. In Meta-\nBaseline, it further optimizes a converged Classiﬁer-Baseline on its evaluation metric, and an additional learnable scalar τ is\nintroduced to scale cosine similarity.\nscale the value before applying Softmax function during\ntraining (a common practice in recent work [6, 17, 16]). We\nmultiply the cosine similarity by a learnable scalar τ, and\nthe probability prediction in training becomes:\np(y = c | x) =\nexp\n\u0000τ · ⟨fθ(x), wc⟩\n\u0001\nP\nc′ exp\n\u0000τ · ⟨fθ(x), wc′⟩\n\u0001.\n(3)\nIn this work, the main purpose of Meta-Baseline is to\ninvestigate whether the meta-learning objective is still ef-\nfective over a whole-classiﬁcation model.\nAs a method,\nwhile every component in Meta-Baseline has been proposed\nin prior works, we ﬁnd none of the prior works studies them\nas a whole. Therefore, Meta-Baseline should also be an im-\nportant baseline that has been overlooked.\n4. Results on Standard Benchmarks\n4.1. Datasets\nThe miniImageNet dataset [29] is a common benchmark\nfor few-shot learning. It contains 100 classes sampled from\nILSVRC-2012 [21], which are then randomly split to 64,\n16, 20 classes as training, validation, and testing set respec-\ntively. Each class contains 600 images of size 84 × 84.\nThe tieredImageNet dataset [20] is another common\nbenchmark proposed more recently with much larger scale.\nIt is a subset of ILSVRC-2012, containing 608 classes from\n34 super-categories, which are then split into 20, 6, 8 super-\ncategories, resulting in 351, 97, 160 classes as training, val-\nidation, testing set respectively. The image size is 84 × 84.\nThis setting is more challenging since base classes and\nnovel classes come from different super-categories.\nIn addition to the datasets above, we evaluate our model\non ImageNet-800, which is derived from ILSVRC-2012 1K\nclasses by randomly splitting 800 classes as base classes\nand 200 classes as novel classes. The base classes contain\nthe images from the original training set, the novel classes\ncontain the images from the original validation set. This\nlarger dataset aims at making the training setting standard\nas the ImageNet 1K classiﬁcation task [8].\n4.2. Implementation details\nWe use ResNet-12 that follows the most of recent\nworks [16, 25, 11, 33] on miniImageNet and tieredIma-\ngeNet, and we use ResNet-18, ResNet-50 [8] on ImageNet-\n800. For the classiﬁcation training stage, we use the SGD\noptimizer with momentum 0.9, the learning rate starts from\n0.1 and the decay factor is 0.1. On miniImageNet, we train\n100 epochs with batch size 128 on 4 GPUs, the learning\nrate decays at epoch 90. On tieredImageNet, we train 120\nepochs with batch size 512 on 4 GPUs, the learning rate de-\n4\n\n\nModel\nBackbone\n1-shot\n5-shot\nMatching Networks [29]\nConvNet-4\n43.56 ± 0.84\n55.31 ± 0.73\nPrototypical Networks [24]\nConvNet-4\n48.70 ± 1.84\n63.11 ± 0.92\nPrototypical Networks (re-implement)\nResNet-12\n53.81 ± 0.23\n75.68 ± 0.17\nActivation to Parameter [18]\nWRN-28-10\n59.60 ± 0.41\n73.74 ± 0.19\nLEO [22]\nWRN-28-10\n61.76 ± 0.08\n77.59 ± 0.12\nBaseline++ [1]\nResNet-18\n51.87 ± 0.77\n75.68 ± 0.63\nSNAIL [13]\nResNet-12\n55.71 ± 0.99\n68.88 ± 0.92\nAdaResNet [15]\nResNet-12\n56.88 ± 0.62\n71.94 ± 0.57\nTADAM [16]\nResNet-12\n58.50 ± 0.30\n76.70 ± 0.30\nMTL [25]\nResNet-12\n61.20 ± 1.80\n75.50 ± 0.80\nMetaOptNet [11]\nResNet-12\n62.64 ± 0.61\n78.63 ± 0.46\nSLA-AG [10]\nResNet-12\n62.93 ± 0.63\n79.63 ± 0.47\nProtoNets + TRAML [12]\nResNet-12\n60.31 ± 0.48\n77.94 ± 0.57\nConstellationNet [33]\nResNet-12\n64.89 ± 0.23\n79.95 ± 0.17\nClassiﬁer-Baseline (ours)\nResNet-12\n58.91 ± 0.23\n77.76 ± 0.17\nMeta-Baseline (ours)\nResNet-12\n63.17 ± 0.23\n79.26 ± 0.17\nTable 2: Comparison to prior works on miniImageNet. Average 5-way accuracy (%) with 95% conﬁdence interval.\nModel\nBackbone\n1-shot\n5-shot\nMAML [5]\nConvNet-4\n51.67 ± 1.81\n70.30 ± 1.75\nPrototypical Networks* [24]\nConvNet-4\n53.31 ± 0.89\n72.69 ± 0.74\nRelation Networks* [26]\nConvNet-4\n54.48 ± 0.93\n71.32 ± 0.78\nLEO [22]\nWRN-28-10\n66.33 ± 0.05\n81.44 ± 0.09\nMetaOptNet [11]\nResNet-12\n65.99 ± 0.72\n81.56 ± 0.53\nClassiﬁer-Baseline (ours)\nResNet-12\n68.07 ± 0.26\n83.74 ± 0.18\nMeta-Baseline (ours)\nResNet-12\n68.62 ± 0.27\n83.74 ± 0.18\nTable 3: Comparison to prior works on tieredImageNet. Average 5-way accuracy (%) with 95% conﬁdence interval.\ncays at epoch 40 and 80. On ImageNet-800, we train 90\nepochs with batch size 256 on 8 GPUs, the learning rate de-\ncays at epoch 30 and 60. The weight decay is 0.0005 for\nResNet-12 and 0.0001 for ResNet-18 or ResNet-50. Stan-\ndard data augmentation is applied, including random re-\nsized crop and horizontal ﬂip. For meta-learning stage, we\nuse the SGD optimizer with momentum 0.9. The learning\nrate is ﬁxed as 0.001. The batch size is 4, i.e. each training\nbatch contains 4 few-shot tasks to compute the average loss.\nThe cosine scaling parameter τ is initialized as 10.\nWe also apply consistent sampling for evaluating the per-\nformance. For the novel class split in a dataset, the sampling\nof testing few-shot tasks follows a deterministic order. Con-\nsistent sampling allows us to get a better model comparison\nwith the same number of sampled tasks. In the following\nsections, when the conﬁdence interval is omitted in the ta-\nble, it indicates that a ﬁxed set of 800 testing tasks are sam-\npled for estimating the performance.\n4.3. Results\nFollowing the standard-setting, we conduct experiments\non miniImageNet and tieredImageNet, the results are shown\nin Table 2 and 3 respectively. To get a fair comparison to\nprior works, we perform model selection according to the\nvalidation set. On both datasets, we observe that the Meta-\nBaseline achieves competitive performance to state-of-the-\nart methods. We highlight that many methods for compar-\nison introduce more parameters and architecture designs\n(e.g. self-attention in [33]), while Meta-Baseline has the\nminimum parameters and the simplest design. We also no-\ntice that the simple Classiﬁer-Baseline can achieve compet-\nitive performance when compared to meta-learning meth-\nods, especially in 5-shot tasks. We observe that the meta-\nlearning stage consistently improves Classiﬁer-Baseline on\n5\n\n\nModel\nBackbone\n1-shot\n5-shot\nClassiﬁer-Baseline (ours)\nResNet-18\n83.51 ± 0.22\n94.82 ± 0.10\nMeta-Baseline (ours)\nResNet-18\n86.39 ± 0.22\n94.82 ± 0.10\nClassiﬁer-Baseline (ours)\nResNet-50\n86.07 ± 0.21\n96.14 ± 0.08\nMeta-Baseline (ours)\nResNet-50\n89.70 ± 0.19\n96.14 ± 0.08\nTable 4: Results on ImageNet-800. Average 5-way accuracy (%) is reported with 95% conﬁdence interval.\n1\n15\n30\nepochs\n82.0\n84.0\n86.0\n88.0\n90.0\n5-way acc (%)\nminiImageNet, 1-shot\n1\n15\n30\nepochs\n92.4\n93.0\n93.6\n94.2\n94.8\nminiImageNet, 5-shot\n1\n15\n30\nepochs\n86.4\n87.0\n87.6\n88.2\n88.8\ntieredImageNet, 1-shot\n1\n15\n30\nepochs\n93.6\n94.0\n94.4\n94.8\ntieredImageNet, 5-shot\n61.5\n62.0\n62.5\n63.0\n63.5\n78.9\n79.2\n79.5\n79.8\n80.1\n66.5\n67.2\n67.9\n68.6\n81.0\n81.5\n82.0\n82.5\n83.0\nMeta-Baseline, Meta-Learning Stage (ResNet-12)\nbase class generalization\nnovel class generalization\nFigure 2: Objective discrepancy of meta-learning on miniImageNet and tieredImageNet. Each epoch contains 200\ntraining batches. Average 5-way accuracy (%) is reported.\n1\n30\n60\n90\nepochs\n94.8\n95.2\n95.6\n96.0\n5-way acc (%)\nResNet-50, 1-shot\n1\n30\n60\n90\nepochs\n98.1\n98.2\n98.3\n98.4\n98.5\nResNet-50, 5-shot\n87.5\n88.2\n88.9\n89.6\n94.8\n95.1\n95.4\n95.7\n96.0\nbase class generalization\nnovel class generalization\nFigure 3: Objective discrepancy of meta-learning on\nImageNet-800. Each epoch contains 500 training batches.\nAverage 5-way accuracy (%) is reported.\nminiImageNet. Compared to miniImageNet, we ﬁnd that\nthe gap between Meta-Baseline and Classiﬁer-Baseline is\nsmaller on tieredImageNet, and the meta-learning stage\ndoes not improve 5-shot in this case.\nWe further evaluate our methods on the larger dataset\nImageNet-800.\nIn this larger-scale experiment, we ﬁnd\nfreezing the Batch Normalization layer [9] (set to eval\nmode) is beneﬁcial.\nThe results are shown in Table 4.\nFrom the results, we observe that in this large dataset Meta-\nBaseline improves Classiﬁer-Baseline in 1-shot, while it is\nnot improving the performance in 5-shot.\n5. Observations and Hypothesis\n5.1. Objective discrepancy in meta-learning\nDespite\nthe\nimprovements\nof\nmeta-learning\nover\nClassiﬁer-Baseline, we observe the test performance drops\nduring the meta-learning stage. While a common assump-\ntion for this phenomenon is overﬁtting, we observe that this\nissue seems not to be mitigated on larger datasets. To further\nlocate the issue, we propose to evaluate base class general-\nization and novel class generalization. Base class general-\nization is measured by sampling tasks from unseen images\nin base classes, while novel class generalization refers to the\nperformance of few-shot tasks sampled from novel classes.\nThe base class generalization is the generalization in the in-\nput distribution for which the model is trained, it decouples\nthe commonly deﬁned generalization and class-level trans-\nfer performance, which helps for locating the reason for the\nperformance drop.\nFigure 2 and 3 demonstrate the meta-learning stage of\nMeta-Baseline on different datasets. We ﬁnd that during the\nmeta-learning stage, when the base class generalization is\nincreasing, the novel class generalization can be decreasing\ninstead. This fact indicates that over a converged whole-\nclassiﬁcation model, the meta-learning objective itself, i.e.\nmaking the embedding generalize better in few-shot tasks\nfrom base classes, can have a negative effect on the perfor-\nmance of few-shot tasks from novel classes. It also gives a\n6\n\n\nTask\nModel\nmini-tiered\nmini-shufﬂed\nfull-tiered\nfull-shufﬂed\n1-shot\nClassiﬁer-Baseline\n56.91\n61.64\n68.76\n77.67\nMeta-Baseline\n58.44\n65.88\n69.52\n80.48\n∆\n+1.53\n+4.24\n+0.76\n+2.81\n5-shot\nClassiﬁer-Baseline\n74.30\n79.26\n84.07\n90.58\nMeta-Baseline\n74.63\n80.58\n84.07\n90.67\n∆\n+0.33\n+1.32\n+0.00\n+0.09\nTable 5: Effect of dataset properties. Average 5-way accuracy (%), with ResNet-12.\nTraining\nBase gen.\nNovel gen.\n1-shot\nw/ ClsTr\n86.42\n63.33\nw/o ClsTr\n86.74\n58.54\n5-shot\nw/ ClsTr\n93.54\n80.02\nw/o ClsTr\n94.47\n74.95\nTable 6: Comparison on Meta-Baseline training from\nscratch. Average 5-way accuracy (%), with ResNet-12 on\nminiImageNet. ClsTr: classiﬁcation training stage.\nMethod\n1-shot\n5-shot\nClassiﬁer-Baseline\n60.58\n79.24\nClassiﬁer-Baseline (Euc.)\n56.29\n78.93\nMeta-Baseline\n63.33\n80.02\nMeta-Baseline (Euc.)\n60.19\n79.50\nTable 7: Importance of inheriting a good metric. Aver-\nage 5-way accuracy (%), with ResNet-12 on miniImageNet.\npossible explanation for why such phenomenon is not mit-\nigated on larger datasets, as this is not sample-level over-\nﬁtting, but class-level overﬁtting, which is caused by the\nobjective discrepancy that the underlying training class dis-\ntribution is different from testing class distribution.\nThis observation suggests that we may reconsider the\nmotivation of the meta-learning framework for few-shot\nlearning. In some settings, optimizing towards the train-\ning objective with a consistent form as the testing objective\n(except the inevitable class difference) may have an even\nnegative effect. It is also likely that the whole-classiﬁcation\nlearns the embedding with stronger class transferability, and\nmeta-learning makes the model perform better at N-way\nK-shot tasks but tends to lose the class transferability.\n5.2. Effect of whole-classiﬁcation training before\nmeta-learning\nAccording to our hypothesis, the whole-classiﬁcation\npre-trained model has provided extra class transferabil-\nity for the meta-learning model, therefore, it is natural to\ncompare Meta-Baseline with and without the classiﬁcation\ntraining stage. The results are shown in Table 6. We observe\nthat Meta-Baseline trained without classiﬁcation training\nstage can actually achieve higher base class generalization,\nbut its novel class generalization is much lower when com-\npared to Meta-Baseline with whole-classiﬁcation training.\nThese results support our hypothesis, that the whole-\nclassiﬁcation training provides the embedding with stronger\nclass transferability, which signiﬁcantly helps novel class\ngeneralization.\nInterestingly, TADAM [16] ﬁnds that\nco-training the meta-learning objective with a whole-\nclassiﬁcation task is beneﬁcial, which may be potentially\nrelated to our hypothesis. While our results show it is likely\nthat the key effect of the whole-classiﬁcation objective is\nimproving the class transferability, it also indicates a po-\ntential trade-off that the whole-classiﬁcation objective can\nhave a negative effect on base class generalization.\n5.3. What makes Meta-Baseline a strong baseline?\nAs a method with a similar objective as ProtoNet [24],\nMeta-Baseline achieves nearly 10% higher accuracy on 1-\nshot in Table 2. The observations and hypothesis in previ-\nous sections potentially explain its strength, as it starts with\nthe embedding of a whole-classiﬁcation model which has\nstronger class transferability.\nWe perform further experiments, that in Meta-Baseline\n(with classiﬁcation training stage) we replace the cosine\ndistance with the squared Euclidean distance proposed in\nProtoNet [24]. To get a fair comparison, we also include\nthe learnable scalar τ with a proper initialization value 0.1.\nThe results are shown in Table 7.\nWhile ProtoNet [24]\nﬁnds that squared Euclidean distance (as a Bregman di-\nvergence) works better than cosine distance when perform-\ning meta-learning from scratch, here we start meta-learning\nfrom Classiﬁer-Baseline and we observe that cosine sim-\nilarity works much better. A potential reason is that, as\nshown in Table 7, cosine nearest-centroid works much bet-\nter than nearest-centroid with squared Euclidean distance in\nClassiﬁer-Baseline (note that this is just the evaluation met-\nric and has no changes in training). Inheriting a good metric\n7\n\n\nfor Classiﬁer-Baseline might be the key that makes Meta-\nBaseline strong. According to our hypothesis, the embed-\nding from the whole-classiﬁcation model has strong class\ntransferability, inheriting a good metric potentially mini-\nmizes the future modiﬁcations on the embedding from the\nwhole-classiﬁcation model, thus it can keep the class trans-\nferability better and achieve higher performance.\n5.4. Effect of dataset properties\nWe construct four variants from the tieredImageNet\ndataset. Speciﬁcally, full-tiered refers to the original tiered-\nImageNet, full-shufﬂed is constructed by randomly shuf-\nﬂing the classes in tieredImageNet and re-splitting the\nclasses into training, validation, and test set.\nThe mini-\ntiered and mini-shufﬂed datasets are constructed from full-\ntiered and full-shufﬂed respectively, their training set is con-\nstructed by randomly selecting 64 classes with 600 images\nfrom each class in the full training set, while the validation\nset and the test set remain unchanged. Since tieredImageNet\nseparates training classes and testing classes into different\nsuper categories, shufﬂing these classes will mix the classes\nin different super categories together and make the distribu-\ntion of base classes and novel classes closer.\nOur previous experiments show that base class general-\nization is always improving, if novel classes are covered\nby the distribution of base classes, the novel class gener-\nalization should also keep increasing. From Table 5, we\ncan see that from mini-tiered to mini-shufﬂed, and from\nfull-tiered to full-shufﬂed, the improvement achieved by the\nmeta-learning stage gets signiﬁcantly larger, which consis-\ntently supports our hypothesis. Therefore, our results in-\ndicate it is likely that meta-learning is mostly effective over\nwhole-classiﬁcation training when novel classes are similar\nto base classes.\nWe also observe that other factors may affect the im-\nprovement of meta-learning. From mini-tiered to full-tiered\nand from mini-shufﬂed to full-shufﬂed, when the dataset\ngets larger the improvements become less. A potential hy-\npothesis could be that the class transferability advantage of\nwhole-classiﬁcation training becomes more obvious when\ntrained on large datasets. From the results of our experi-\nments in Table 2, 3, 4, 5, we observe that the improvement\nof the meta-learning stage in 5-shot is less than 1-shot. We\nhypothesize this is because when there are more shots, tak-\ning average embedding becomes a more reasonable choice\nto estimate the class center in Classiﬁer-Baseline, therefore\nthe advantage of meta-learning becomes less.\n5.5. The trade-off between meta-learning and\nwhole-classiﬁcation\nAll of the experiments in previous sections support a key\nhypothesis, that there exists a trade-off: the meta-learning\nobjective learns better embedding for N-way K-shot tasks\n(in the same distribution), while the whole-classiﬁcation ob-\njective learns embedding with stronger class transferabil-\nity. Optimizing towards one objective may hurt the strength\nof another objective. With this hypothesis, Meta-Baseline\nbalances this trade-off by choosing to calibrate the whole-\nclassiﬁcation embedding with meta-learning and inherit the\nmetric with high initial performance.\nThe discrepancy between base class generalization and\nnovel class generalization also considers the effectiveness\nof meta-learning and whole-classiﬁcation from the perspec-\ntive of datasets. Speciﬁcally, when novel classes are similar\nenough to base classes or the base classes are sufﬁcient to\ncover the distribution of novel classes, novel class gener-\nalization should converge to base class generalization. In\npractice, this can be potentially achieved by collecting base\nclasses that are similar to the target novel classes. In this\ncase, it may be possible that the novel meta-learning algo-\nrithms outperform the whole-classiﬁcation baselines again.\n6. Additional Results on Meta-Dataset\nMeta-Dataset [28] is a new benchmark proposed for few-\nshot learning, it consists of diverse datasets for training and\nevaluation. They also propose to generate few-shot tasks\nwith a variable number of ways and shots, for having a\nsetting closer to the real world. We follow the setting in\nMeta-Dataset [28] and use ResNet-18 as the backbone, with\nthe original image size of 126×126, which is resized to be\n128×128 before feeding into the network. For the classiﬁ-\ncation training stage, we apply the training setting similar to\nour setting in ImageNet-800. For the meta-learning stage,\nthe model is trained for 5000 iterations with one task in each\niteration.\nThe left side of Table 8 demonstrates the models trained\nwith samples in ILSVRC-2012 only. We observe that the\nMeta-Baseline does not signiﬁcantly improve Classiﬁer-\nBaseline under this setting in our experiments, possibly due\nto the average number of shots are high.\nThe right side of Table 8 shows the results when the\nmodels are trained on all datasets, except Trafﬁc Signs and\nMSCOCO which have no training samples. The Classiﬁer-\nBaseline is trained as a multi-dataset classiﬁer, i.e. an en-\ncoder together with multiple FC layers over the encoded\nfeature to output the logits for different datasets. The clas-\nsiﬁcation training stage has the same number of iterations\nas training on ILSVRC only, to mimic the ILSVRC train-\ning, a batch has 0.5 probability to be from ILSVRC and 0.5\nprobability to be uniformly sampled from one of the other\ndatasets. For Classiﬁer-Baseline, comparing to the results\non the left side of Table 8, we observe that while the per-\nformance on ILSVRC is worse, the performances on other\ndatasets are mostly improved due to having their samples in\ntraining. It can be also noticed that the cases where Meta-\nBaseline improves Classiﬁer-Baseline are mostly on the\n8\n\n\nTrained on ILSVRC\nTrained on all datasets\nDataset\nfo-Proto-MAML\nClassiﬁer/Meta\nfo-Proto-MAML\nClassiﬁer\nMeta\n[28]\n(ours)\n[28]\n(ours)\n(ours)\nILSVRC\n49.5\n59.2\n46.5\n55.0\n48.0\nOmniglot\n63.4\n69.1\n82.7\n76.9\n89.4\nAircraft\n56.0\n54.1\n75.2\n69.8\n81.7\nBirds\n68.7\n77.3\n69.9\n78.3\n77.3\nTextures\n66.5\n76.0\n68.3\n71.4\n64.5\nQuick Draw\n51.5\n57.3\n66.8\n62.7\n74.5\nFungi\n40.0\n45.4\n42.0\n55.4\n60.2\nVGG Flower\n87.2\n89.6\n88.7\n90.6\n83.8\nTrafﬁc Signs\n48.8\n66.2\n52.4\n69.3\n59.5\nMSCOCO\n43.7\n55.7\n41.7\n53.1\n43.6\nTable 8: Additional results on Meta-Dataset. Average accuracy (%), with variable number of ways and shots. The fo-Proto-\nMAML method is from Meta-Dataset [28], Classiﬁer and Meta refers to Classiﬁer-Baseline and Meta-Baseline respectively,\n1000 tasks are sampled for evaluating Classiﬁer or Meta. Note that Trafﬁc Signs and MSCOCO have no training set.\ndatasets which are “less relevant” to ILSVRC (the dataset\n“relevance” could be shown in Dvornik et al. [3]). A po-\ntential reason is that the multi-dataset classiﬁcation train-\ning stage samples ILSVRC with 0.5 probability, similar\nto ILSVRC training, the meta-learning stage is hard to\nimprove on ILSVRC, therefore those datasets relevant to\nILSVRC will have similar properties so that it is hard to\nimprove on them.\n7. Conclusion and Discussion\nIn this work, we presented a simple Meta-Baseline that\nhas been overlooked for few-shot learning. Without any ad-\nditional parameters or complex design choices, it is compet-\nitive to state-of-the-art methods on standard benchmarks.\nOur experiments indicate that there might be an objec-\ntive discrepancy in the meta-learning framework for few-\nshot learning, i.e. a meta-learning model generalizing better\non unseen tasks from base classes might have worse perfor-\nmance on tasks from novel classes. This provides a possible\nexplanation that why some complex meta-learning methods\ncould not get signiﬁcantly better performance than simple\nwhole-classiﬁcation. While most recent works focus on im-\nproving the meta-learning structures, many of them did not\nexplicitly address the issue of class transferability. Our ob-\nservations suggest that the objective discrepancy might be a\npotential key challenge to tackle.\nWhile many novel meta-learning algorithms are pro-\nposed and some recent works report that simple whole-\nclassiﬁcation training is good enough for few-shot learn-\ning, we show that meta-learning is still effective over\nwhole-classiﬁcation models. We observe a potential trade-\noff between the objectives of meta-learning and whole-\nclassiﬁcation.\nFrom the perspective of datasets, we\ndemonstrate how the preference between meta-learning and\nwhole-classiﬁcation changes according to class similarity\nand other factors, indicating that these factors may need\nmore attention for model comparisons in future work.\nAcknowledgements. This work was supported, in part, by grants from\nDARPA LwLL, NSF 1730158 CI-New: Cognitive Hardware and Software\nEcosystem Community Infrastructure (CHASE-CI), NSF ACI-1541349\nCC*DNI Paciﬁc Research Platform, and gifts from Qualcomm, TuSim-\nple and Picsart. Prof. Darrell was supported, in part, by DoD including\nDARPA’s XAI, LwLL, and/or SemaFor programs, as well as BAIR’s in-\ndustrial alliance programs. We thank Hang Gao for the helpful discussions.\nReferences\n[1] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank\nWang, and Jia-Bin Huang. A closer look at few-shot classi-\nﬁcation. In International Conference on Learning Represen-\ntations, 2019. 1, 2, 3, 5, 11, 12\n[2] Guneet S Dhillon, Pratik Chaudhari, Avinash Ravichandran,\nand Stefano Soatto. A baseline for few-shot image classiﬁ-\ncation. arXiv preprint arXiv:1909.02729, 2019. 2\n[3] Nikita Dvornik, Cordelia Schmid, and Julien Mairal. Select-\ning relevant features from a universal representation for few-\nshot classiﬁcation. arXiv preprint arXiv:2003.09338, 2020.\n9\n[4] Li Fei-Fei, Rob Fergus, and Pietro Perona. One-shot learning\nof object categories. IEEE transactions on pattern analysis\nand machine intelligence, 28(4):594–611, 2006. 1\n[5] Chelsea Finn, Pieter Abbeel, and Sergey Levine.\nModel-\nagnostic meta-learning for fast adaptation of deep networks.\nIn Proceedings of the 34th International Conference on Ma-\nchine Learning-Volume 70, pages 1126–1135. JMLR. org,\n2017. 1, 2, 5\n9\n\n\n[6] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot\nvisual learning without forgetting.\nIn Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 4367–4375, 2018. 1, 2, 3, 4, 11\n[7] Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and\nThomas Grifﬁths. Recasting gradient-based meta-learning as\nhierarchical bayes. arXiv preprint arXiv:1801.08930, 2018.\n2\n[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 4\n[9] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. arXiv preprint arXiv:1502.03167, 2015. 6, 11\n[10] Hankook Lee, Sung Ju Hwang, and Jinwoo Shin.\nSelf-\nsupervised label augmentation via input transformations.\nIn International Conference on Machine Learning, pages\n5714–5724. PMLR, 2020. 5\n[11] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and\nStefano Soatto. Meta-learning with differentiable convex op-\ntimization. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 10657–10665,\n2019. 1, 2, 4, 5, 11\n[12] Aoxue Li, Weiran Huang, Xu Lan, Jiashi Feng, Zhenguo Li,\nand Liwei Wang. Boosting few-shot learning with adaptive\nmargin loss. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 12576–\n12584, 2020. 5\n[13] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter\nAbbeel. A simple neural attentive meta-learner. In Inter-\nnational Conference on Learning Representations, 2018. 2,\n5\n[14] Tsendsuren Munkhdalai and Hong Yu. Meta networks. In\nProceedings of the 34th International Conference on Ma-\nchine Learning-Volume 70, pages 2554–2563. JMLR. org,\n2017. 2\n[15] Tsendsuren Munkhdalai, Xingdi Yuan, Soroush Mehri, and\nAdam Trischler. Rapid adaptation with conditionally shifted\nneurons. arXiv preprint arXiv:1712.09926, 2017. 2, 5\n[16] Boris Oreshkin, Pau Rodr´ıguez L´opez, and Alexandre La-\ncoste. Tadam: Task dependent adaptive metric for improved\nfew-shot learning. In Advances in Neural Information Pro-\ncessing Systems, pages 721–731, 2018. 2, 4, 5, 7, 11\n[17] Hang Qi, Matthew Brown, and David G Lowe. Low-shot\nlearning with imprinted weights. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\npages 5822–5830, 2018. 4\n[18] Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan L Yuille. Few-\nshot image recognition by predicting parameters from activa-\ntions. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 7229–7238, 2018. 5\n[19] Sachin Ravi and Hugo Larochelle. Optimization as a model\nfor few-shot learning.\nIn In International Conference on\nLearning Representations (ICLR), 2017. 2\n[20] Mengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell,\nKevin Swersky, Joshua B Tenenbaum, Hugo Larochelle, and\nRichard S Zemel. Meta-learning for semi-supervised few-\nshot classiﬁcation. arXiv preprint arXiv:1803.00676, 2018.\n4\n[21] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al.\nImagenet large\nscale visual recognition challenge. International journal of\ncomputer vision, 115(3):211–252, 2015. 4\n[22] Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol\nVinyals, Razvan Pascanu, Simon Osindero, and Raia Had-\nsell.\nMeta-learning with latent embedding optimization.\nIn International Conference on Learning Representations,\n2019. 1, 2, 5\n[23] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan\nWierstra, and Timothy Lillicrap.\nMeta-learning with\nmemory-augmented neural networks. In International con-\nference on machine learning, pages 1842–1850, 2016. 2\n[24] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical\nnetworks for few-shot learning. In Advances in Neural Infor-\nmation Processing Systems, pages 4077–4087, 2017. 2, 3, 5,\n7\n[25] Qianru Sun, Yaoyao Liu, Tat-Seng Chua, and Bernt Schiele.\nMeta-transfer learning for few-shot learning.\nIn Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 403–412, 2019. 1, 2, 4, 5\n[26] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS\nTorr, and Timothy M Hospedales. Learning to compare: Re-\nlation network for few-shot learning. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 1199–1208, 2018. 1, 2, 5\n[27] Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenen-\nbaum, and Phillip Isola. Rethinking few-shot image classi-\nﬁcation: a good embedding is all you need? arXiv preprint\narXiv:2003.11539, 2020. 1, 2, 3, 11\n[28] Eleni Triantaﬁllou, Tyler Zhu, Vincent Dumoulin, Pascal\nLamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles\nGelada, Kevin Swersky, Pierre-Antoine Manzagol, et al.\nMeta-dataset: A dataset of datasets for learning to learn from\nfew examples. arXiv preprint arXiv:1903.03096, 2019. 8, 9\n[29] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan\nWierstra, et al. Matching networks for one shot learning. In\nAdvances in neural information processing systems, pages\n3630–3638, 2016. 1, 2, 3, 4, 5\n[30] Xin Wang, Fisher Yu, Ruth Wang, Trevor Darrell, and\nJoseph E Gonzalez. Tafe-net: Task-aware feature embed-\ndings for low shot learning. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n1831–1840, 2019. 1\n[31] Yan Wang, Wei-Lun Chao, Kilian Q Weinberger, and Lau-\nrens van der Maaten.\nSimpleshot:\nRevisiting nearest-\nneighbor classiﬁcation for few-shot learning. arXiv preprint\narXiv:1911.04623, 2019. 1, 2, 3\n[32] Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical\nevaluation of rectiﬁed activations in convolutional network.\narXiv preprint arXiv:1505.00853, 2015. 11\n[33] Weijian Xu, yifan xu, Huaijin Wang, and Zhuowen Tu. Con-\nstellation nets for few-shot learning. In International Con-\nference on Learning Representations, 2021. 1, 2, 4, 5, 11\n10\n\n\n1\n40\n80\n120\n160\nepochs\n40\n60\n80\n5-way acc (%)\nminiImageNet, 1-shot\n1\n40\n80\n120\n160\nepochs\n50\n60\n70\n80\n90\n5-way acc (%)\nminiImageNet, 5-shot\nMeta-Baseline, Training from Scratch\nbase class generalization\nnovel class generalization\nFigure 4: Training Meta-Baseline without classiﬁcation-\ntraining stage on miniImageNet.\nDataset\nClassiﬁer\n1-shot\n5-shot\nminiImageNet\nLinear\n60.58\n79.24\nCosine\n61.93\n78.73\ntieredImageNet\nLinear\n68.76\n84.07\nCosine\n67.58\n83.31\nTable 9: Comparison to classiﬁer trained with cosine met-\nric, Average 5-way accuracy (%), with ResNet-12.\nA. Details of ResNet-12\nThe ResNet-12 backbone consists of 4 residual blocks\nthat each residual block has 3 convolutional layers. Each\nconvolutional layer has a 3 × 3 kernel, followed by Batch\nNormalization [9] and Leaky ReLU [32] with 0.1 slope.\nThe channels of convolutional layers in each residual block\nare 64, 128, 256, 512 respectively, a 2×2 max-pooling layer\nis applied after each residual block. Finally, a 5 × 5 global\naverage pooling is applied to get a 512-dimensional feature\nvector.\nThis architecture is consistent with recent works [16, 33].\nSome other recent works also introduce additional parame-\nters and design choices in the backbone (e.g. DropBlock\nand wider channels of 64, 160, 320, 640 in [11, 27]), while\nthese modiﬁcations may make the performance higher, we\ndo not include them here for simplicity.\nB. Training plot of Meta-Baseline without clas-\nsiﬁcation training stage\nWe show the process of training Meta-Baseline from\nscratch (i.e.\nwithout the classiﬁcation-training stage) on\nminiImageNet in Figure 4. We observe that when the learn-\ning rate decays, the novel class generalization quickly starts\nto be decreasing. While it is able to achieve higher base\nclass generalization than Meta-Baseline with classiﬁcation\ntraining, its highest novel class generalization is still much\nworse, suggesting whole-classiﬁcation training may pro-\nvide representations with extra class transferability.\nC. Comparison to cosine classiﬁcation training\nWe compare the effect of classiﬁcation training with\nreplacing the last linear-classiﬁer with cosine nearest-\nneighbor metric which is proposed in prior work [6, 1], the\nresults are shown in Table 9, where Cosine denotes clas-\nsiﬁcation training with cosine metric and Linear denotes\nthe standard classiﬁcation training. On miniImageNet, we\nobserve that Cosine outperforms Linear in 1-shot, but has\nworse performance in 5-shot. On tieredImageNet, we ob-\nserve Linear outperforms Cosine in both 1-shot and 5-shot.\nWe choose to use the linear layer as it is more common and\nwe ﬁnd it works better in more cases.\nD. Objective discrepancy on ImageNet-800\nBesides miniImageNet and tieredImageNet, in our large-\nscale dataset ImageNet-800, we also observe the novel class\ngeneralization decreasing when base class generalization\nis increasing, the training process is demonstrated in Fig-\nure 5.\nFrom the ﬁgure, we see that for both backbones\nof ResNet-18 and ResNet-50, the base class generalization\nperformance is increasing during the training epochs, while\nthe novel class generalization performance quickly starts to\nbe decreasing. These observations are consistent with our\nobservations on miniImageNet and tieredImageNet, which\nfurther support our hypothesis.\nE. Comparison of the Classiﬁer-Baseline and\nBaseline++ [1]\nWe connect the Classiﬁer-Baseline to Baseline++ [1]\nwith a step-by-step ablation study on miniImageNet, the\nresults are shown in Table 10. We see that ﬁne-tuning is\noutperformed by the simple nearest-centroid method with\ncosine metric, and using a standard ImageNet-like opti-\nmizer signiﬁcantly improves the performance of the whole-\nclassiﬁcation method for few-shot learning.\n11\n\n\n1\n30\n60\n90\nepochs\n90.0\n90.9\n91.8\n92.7\n5-way acc (%)\nResNet-18, 1-shot\n1\n30\n60\n90\nepochs\n96.3\n96.6\n96.9\n97.2\n97.5\nResNet-18, 5-shot\n1\n30\n60\n90\nepochs\n94.8\n95.2\n95.6\n96.0\nResNet-50, 1-shot\n1\n30\n60\n90\nepochs\n98.1\n98.2\n98.3\n98.4\n98.5\nResNet-50, 5-shot\n85.2\n85.6\n86.0\n86.4\n93.6\n93.8\n94.0\n94.2\n87.5\n88.2\n88.9\n89.6\n94.8\n95.1\n95.4\n95.7\n96.0\nMeta-Baseline, Meta-Learning Stage (ImageNet-800)\nbase class generalization\nnovel class generalization\nFigure 5: Objective discrepancy of meta-learning on ImageNet-800. Each epoch contains 500 training batches. Average\n5-way accuracy (%) is reported.\ndifference\n5-way 1-shot accuracy (%)\n(default, reported [1])\n51.75 ± 0.80\n(default, reproduced by their code)\n50.84 ± 0.80\nﬁnetune →cosine nearest-centroid\n52.15 ± 0.83\nepoch-300 →epoch-50\n53.37 ± 0.71\nremove color-jittering\n56.06 ± 0.71\n224×224 input size →resizing 84×84 to 224×224\n50.49 ± 0.71\nResNet-18 →ResNet-12\n53.59 ± 0.72\nAdam (lr=0.001, batch=16) →SGD (lr=0.1, batch=128)\n59.19 ± 0.71\nTable 10: Comparison of the Classiﬁer-Baseline and Baseline++ [1].\n12\n"
}