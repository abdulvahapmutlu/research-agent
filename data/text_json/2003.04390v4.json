{
  "filename": "2003.04390v4.pdf",
  "num_pages": 12,
  "pages": [
    "Meta-Baseline: Exploring Simple Meta-Learning for Few-Shot Learning\nYinbo Chen\nUC San Diego\nZhuang Liu\nUC Berkeley\nHuijuan Xu\nPenn State University\nTrevor Darrell\nUC Berkeley\nXiaolong Wang\nUC San Diego\nAbstract\nMeta-learning has been the most common framework for\nfew-shot learning in recent years. It learns the model from\ncollections of few-shot classiÔ¨Åcation tasks, which is believed\nto have a key advantage of making the training objective\nconsistent with the testing objective.\nHowever, some re-\ncent works report that by training for whole-classiÔ¨Åcation,\ni.e. classiÔ¨Åcation on the whole label-set, it can get compa-\nrable or even better embedding than many meta-learning\nalgorithms.\nThe edge between these two lines of works\nhas yet been underexplored, and the effectiveness of meta-\nlearning in few-shot learning remains unclear. In this paper,\nwe explore a simple process: meta-learning over a whole-\nclassiÔ¨Åcation pre-trained model on its evaluation metric.\nWe observe this simple method achieves competitive per-\nformance to state-of-the-art methods on standard bench-\nmarks. Our further analysis shed some light on understand-\ning the trade-offs between the meta-learning objective and\nthe whole-classiÔ¨Åcation objective in few-shot learning. Our\ncode is available at https://github.com/yinboc/\nfew-shot-meta-baseline.\n1. Introduction\nWhile humans have shown incredible ability to learn\nfrom very few examples and generalize to many different\nnew examples, the current deep learning approaches still\nrely on a large scale of training data. To mimic this hu-\nman ability of generalization, few-shot learning [4, 29] is\nproposed for training networks to understand a new con-\ncept based on a few labeled examples. While directly learn-\ning a large number of parameters with few samples is very\nchallenging and most likely leads to overÔ¨Åtting, a practical\nsetting is applying transfer learning: train the network on\ncommon classes (also called base classes) with sufÔ¨Åcient\nsamples, then transfer the model to learn novel classes with\na few examples.\nThe meta-learning framework for few-shot learning fol-\nlows the key idea of learning to learn. SpeciÔ¨Åcally, it sam-\nples few-shot classiÔ¨Åcation tasks from training samples be-\nlonging to the base classes and optimizes the model to per-\nform well on these tasks. A task typically takes the form\nof N-way and K-shot, which contains N classes with K\nsupport samples and Q query samples in each class. The\ngoal is to classify these N √ó Q query samples into the\nN classes based on the N √ó K support samples. Under\nthis framework, the model is directly optimized on few-\nshot classiÔ¨Åcation tasks. The consistency between the ob-\njectives of training and testing is considered as the key ad-\nvantage of meta-learning. Motivated by this idea, many re-\ncent works [26, 6, 25, 30, 5, 22, 11, 33] focus on improving\nthe meta-learning structure, and few-shot learning itself has\nbecome a common testbed for evaluating meta-learning al-\ngorithms.\nHowever, some recent works Ô¨Ånd that training for whole-\nclassiÔ¨Åcation, i.e. classiÔ¨Åcation on the whole training label-\nset (base classes), provides the embedding that is compa-\nrable or even better than many recent meta-learning algo-\nrithms.\nThe effectiveness of whole-classiÔ¨Åcation models\nhas been reported in both prior works [6, 1] and some con-\ncurrent works [31, 27]. Meta-learning makes the form of\ntraining objective consistent with testing, but why it turns\nout to learn even worse embedding than simple whole-\nclassiÔ¨Åcation?\nWhile there are several possible reasons,\ne.g. optimization difÔ¨Åculty or overÔ¨Åtting, the answer has\nnot been clearly studied yet. It remains even unclear that\nwhether meta-learning is still effective compared to whole-\nclassiÔ¨Åcation in few-shot learning.\nIn this work, we aim at exploring the edge between\nwhole-classiÔ¨Åcation and meta-learning by decoupling the\ndiscrepancies. We start with ClassiÔ¨Åer-Baseline: a whole-\nclassiÔ¨Åcation method that is similarly proposed in concur-\nrent works [31, 27]. In ClassiÔ¨Åer-Baseline, we Ô¨Årst train\na classiÔ¨Åer on base classes, then remove the last fully-\nconnected (FC) layer which is class-dependent. During test\ntime, it computes mean embedding of support samples for\neach novel class as their centroids, and classiÔ¨Åes query sam-\nples to the nearest centroid with cosine distance. We ob-\nserve this baseline method outperforms many recent meta-\nlearning algorithms.\nIn order to understand whether meta-learning is still ef-\n1\narXiv:2003.04390v4  [cs.CV]  19 Aug 2021\n",
    "fective compared to whole-classiÔ¨Åcation, a natural experi-\nment is to see what happens if we perform further meta-\nlearning over a converged ClassiÔ¨Åer-Baseline on its evalu-\nation metric (i.e. cosine nearest-centroid). As a resulting\nmethod, it is similar to MatchingNet [29] or ProtoNet [24]\nwith an additional classiÔ¨Åcation pre-training stage.\nWe\nobserve that meta-learning can still improve ClassiÔ¨Åer-\nBaseline, and it achieves competitive performance to state-\nof-the-art methods on standard benchmarks. We call this\nsimple method Meta-Baseline.\nWe highlight that as a\nmethod, all the individual components of Meta-Baseline\nhave been proposed in prior works, but to the best of our\nknowledge, it has been overlooked that none of the prior\nworks studies them as a whole. We further decouple the\ndiscrepancies by evaluating on two types of generaliza-\ntion: base class generalization denotes performance on\nfew-shot classiÔ¨Åcation tasks from unseen data in the base\nclasses, which follows the common deÔ¨Ånition of general-\nization (i.e. evaluated in the training distribution); and novel\nclass generalization denotes performance on few-shot clas-\nsiÔ¨Åcation tasks from data in novel classes, which is the\ngoal of the few-shot learning problem. We observe that:\n(i) During meta-learning, improving base class generaliza-\ntion can lead to worse novel class generalization; (ii) When\ntraining Meta-Baseline from scratch (i.e. without whole-\nclassiÔ¨Åcation training), it achieves higher base-class gener-\nalization but much lower novel class generalization.\nOur observations suggest that there could be a trade-\noff between the objectives of meta-learning and whole-\nclassiÔ¨Åcation. It is likely that meta-learning learns the em-\nbedding that works better for N-way K-shot tasks, while\nwhole-classiÔ¨Åcation learns the embedding with stronger\nclass transferability.\nWe Ô¨Ånd that the main advantage\nof training for whole-classiÔ¨Åcation before meta-learning is\nlikely to be improving class transferability. Our further ex-\nperiments provide a potential explanation of what makes\nMeta-Baseline a strong baseline: by inheriting one of the\nmost effective evaluation metrics of the whole-classiÔ¨Åcation\nmodel, it maximizes the reusing of the embedding with\nstrong class transferability. From another perspective, our\nresults also rethink the comparison between meta-learning\nand whole-classiÔ¨Åcation from the perspective of datasets.\nWhen base classes are collected to cover the distribution of\nnovel classes, novel-class generalization should converge to\nbase-class generalization and the strength of meta-learning\nmay overwhelm the strength of whole-classiÔ¨Åcation.\nIn summary, our contributions are as following:\n‚Ä¢ We present a simple Meta-Baseline that has been over-\nlooked in prior work. It achieves competitive perfor-\nmance to state-of-the-art methods on standard bench-\nmarks and is easy to follow.\n‚Ä¢ We observe a trade-off between the objectives of meta-\nlearning and whole-classiÔ¨Åcation, which potentially\nexplains the success of Meta-Baseline and rethinks the\neffectiveness of both objectives in few-shot learning.\n2. Related Work\nMost recent approaches for few-shot learning follow the\nmeta-learning framework. The various meta-learning ar-\nchitectures for few-shot learning can be roughly catego-\nrized into three groups. Memory-based methods [19, 15,\n23, 13, 14] are based on the idea to train a meta-learner\nwith memory to learn novel concepts (e.g.\nan LSTM-\nbased meta-learner). Optimization-based methods [7, 22]\nfollows the idea of differentializing an optimization pro-\ncess over support-set within the meta-learning framework:\nMAML [5] Ô¨Ånds an initialization of the neural network that\ncan be adapted to any novel task using a few optimiza-\ntion steps. MetaOptNet [11] learns the feature represen-\ntation that can generalize well for a linear support vector\nmachine (SVM) classiÔ¨Åer. Besides explicitly considering\nthe dynamic learning process, metric-based methods [29]\nmeta-learn a deep representation with a metric in feature\nspace. For example, Prototypical Networks [24] compute\nthe average feature for each class in support-set and clas-\nsify query samples by the nearest-centroid method. They\nuse Euclidean distance since it is a Bregman divergence.\nRelation Networks [26] further generalizes this framework\nby proposing a relation module as a learnable metric jointly\ntrained with deep representations. TADAM [16] proposes to\nuse a task conditioned metric resulting in a task-dependent\nmetric space.\nWhile signiÔ¨Åcant progress is made in the meta-learning\nframework, some recent works challenge the effectiveness\nof meta-learning with simple whole-classiÔ¨Åcation, i.e.\na\nclassiÔ¨Åcation model on the whole training label-set. Co-\nsine classiÔ¨Åer [6] and Baseline++ [1] perform whole-\nclassiÔ¨Åcation training by replacing the top linear layer with\na cosine classiÔ¨Åer, and they adapt the classiÔ¨Åer to a few-\nshot classiÔ¨Åcation task of novel classes by performing near-\nest centroid or Ô¨Åne-tuning a new layer respectively. They\nshow these whole-classiÔ¨Åcation models can achieve com-\npetitive performance compared to several popular meta-\nlearning models. Another recent work [2] studies on a trans-\nductive setting. Along with these baseline methods, more\nadvanced meta-learning methods [25, 11, 33] are proposed\nand they set up new state-of-the-art results. The effective-\nness of whole-classiÔ¨Åcation is then revisited in two of the\nconcurrent works [31, 27] with improved design choices.\nBy far, the effectiveness of meta-learning compared to\nwhole-classiÔ¨Åcation in few-shot learning is still unclear,\nsince the edge between whole-classiÔ¨Åcation models and\nmeta-learning models remains underexplored. The goal of\nthis work is to explore the insights behind the phenomenons.\nOur experiments show a potential trade-off between the\n2\n",
    "Method\nWhole-classiÔ¨Åcation training\nMeta-learning\nOthers\nMatching Networks [29]\nno / yes (large models)\nattention + cosine\nFCE\nPrototypical Networks [24]\nno\ncentroid + Euclidean\n-\nBaseline++ [1]\nyes (cosine classiÔ¨Åer)\n-\nÔ¨Åne-tuning\nMeta-Baseline (ours)\nyes\ncentroid + cosine (‚àóœÑ)\n-\nTable 1: Overview of method comparison. We summarize the differences between Meta-Baseline and prior methods.\nmeta-learning and whole-classiÔ¨Åcation objectives, which\nprovides a more clear understanding of the comparison be-\ntween both objectives for few-shot learning.\nAs a method, similar ideas to ClassiÔ¨Åer-Baseline are con-\ncurrently reported in recent works [31, 27]. Unlike some\nprior works [6, 1], ClassiÔ¨Åer-Baseline does not replace the\nlast layer with cosine classiÔ¨Åer during training, it trains\nthe whole-classiÔ¨Åcation model with a linear layer on the\ntop and applies cosine nearest-centroid metric during the\ntest time for few-shot classiÔ¨Åcation on novel classes. The\nMeta-Baseline is meta-learning over a converged ClassiÔ¨Åer-\nBaseline on its evaluation metric (cosine nearest-centroid).\nIt is similar (with inconspicuous and important differences\nas shown in Table 1) to those simple and classical metric-\nbased meta-learning methods [29, 24]. The main purpose of\nMeta-Baseline in this paper is to understand the comparison\nbetween whole-classiÔ¨Åcation and meta-learning objectives,\nbut we Ô¨Ånd it is also a simple meta-learning baseline that\nhas been overlooked. While every individual component in\nMeta-Baseline is not novel, to the best of our knowledge,\nnone of the prior works studies them as a whole.\n3. Method\n3.1. Problem deÔ¨Ånition\nIn standard few-shot classiÔ¨Åcation, given a labeled\ndataset of base classes Cbase with a large number of im-\nages, the goal is to learn concepts in novel classes Cnovel\nwith a few samples. In an N-way K-shot few-shot clas-\nsiÔ¨Åcation task, the support-set contains N classes with K\nsamples per class, the query-set contains samples from the\nsame N classes with Q samples per class, and the goal is to\nclassify the N √ó Q query images into N classes.\n3.2. ClassiÔ¨Åer-Baseline\nClassiÔ¨Åer-Baseline is a whole-classiÔ¨Åcation model, i.e.\na classiÔ¨Åcation model trained for the whole label-set. It\nrefers to training a classiÔ¨Åer with classiÔ¨Åcation loss on all\nbase classes and performing few-shot tasks with the cosine\nnearest-centroid method. SpeciÔ¨Åcally, we train a classiÔ¨Åer\non all base classes with standard cross-entropy loss, then re-\nmove its last FC layer and get the encoder fŒ∏, which maps\nthe input to embedding. Given a few-shot task with the\nsupport-set S, let Sc denote the few-shot samples in class\nc, it computes the average embedding wc as the centroid of\nclass c:\nwc =\n1\n|Sc|\nX\nx‚ààSc\nfŒ∏(x),\n(1)\nthen for a query sample x in a few-shot task, it predicts the\nprobability that sample x belongs to class c according to the\ncosine similarity between the embedding of sample x and\nthe centroid of class c:\np(y = c | x) =\nexp\n\u0000‚ü®fŒ∏(x), wc‚ü©\n\u0001\nP\nc‚Ä≤ exp\n\u0000‚ü®fŒ∏(x), wc‚Ä≤‚ü©\n\u0001,\n(2)\nwhere ‚ü®¬∑, ¬∑‚ü©denotes the cosine similarity of two vectors.\nSimilar methods to ClassiÔ¨Åer-Baseline have also been\nproposed in concurrent works [31, 27]. Compared to Base-\nline++ [1], the ClassiÔ¨Åer-Baseline does not use the cosine\nclassiÔ¨Åer for training or perform Ô¨Åne-tuning during testing,\nwhile it performs better on standard benchmarks. In this\nwork, we choose ClassiÔ¨Åer-Baseline as the representative of\nwhole-classiÔ¨Åcation models for few-shot learning. For sim-\nplicity and clarity, we do not introduce additional complex\ntechniques for this whole-classiÔ¨Åcation training.\n3.3. Meta-Baseline\nFigure 1 visualizes the Meta-Baseline. The Ô¨Årst stage\nis the classiÔ¨Åcation training stage, it trains a ClassiÔ¨Åer-\nBaseline, i.e. training a classiÔ¨Åer on all bases classes and\nremove its last FC layer to get fŒ∏. The second stage is the\nmeta-learning stage, which optimizes the model on the eval-\nuation metric of ClassiÔ¨Åer-Baseline. SpeciÔ¨Åcally, given the\nclassiÔ¨Åcation-trained feature encoder fŒ∏, it samples N-way\nK-shot tasks (with N √ó Q query samples) from training\nsamples in base classes. To compute the loss for each task,\nin support-set it computes the centroids of N classes de-\nÔ¨Åned in Equation 1, which are then used to compute the\npredicted probability distribution for each sample in query-\nset deÔ¨Åned in Equation 2. The loss is a cross-entropy loss\ncomputed from p and the labels of the samples in the query-\nset. During training, each training batch can contain several\ntasks and the average loss is computed.\nSince cosine similarity has the value range of [‚àí1, 1],\nwhen it is used to compute the logits, it can be helpful to\n3\n",
    "mean\ncos\nscore\nMeta-Baseline\ntraining\nClassifier-Baseline / Meta-Baseline\nevaluation\nsupport-set\nquery-set\nClassifier-Baseline\ntraining\nclassification\non base classes\nùëìùúÉ\nloss\nùëìùúÉ\nùëìùúÉ\nMeta-Learning Stage\nClassification Training Stage\nrepresentation\ntransfer\nF\nC\nlabel\nùúè\nFigure 1: ClassiÔ¨Åer-Baseline and Meta-Baseline. ClassiÔ¨Åer-Baseline is to train a classiÔ¨Åcation model on all base classes\nand remove its last FC layer to get the encoder fŒ∏. Given a few-shot task, it computes the average feature for samples of each\nclass in support-set, then it classiÔ¨Åes a sample in query-set by nearest-centroid with cosine similarity as distance. In Meta-\nBaseline, it further optimizes a converged ClassiÔ¨Åer-Baseline on its evaluation metric, and an additional learnable scalar œÑ is\nintroduced to scale cosine similarity.\nscale the value before applying Softmax function during\ntraining (a common practice in recent work [6, 17, 16]). We\nmultiply the cosine similarity by a learnable scalar œÑ, and\nthe probability prediction in training becomes:\np(y = c | x) =\nexp\n\u0000œÑ ¬∑ ‚ü®fŒ∏(x), wc‚ü©\n\u0001\nP\nc‚Ä≤ exp\n\u0000œÑ ¬∑ ‚ü®fŒ∏(x), wc‚Ä≤‚ü©\n\u0001.\n(3)\nIn this work, the main purpose of Meta-Baseline is to\ninvestigate whether the meta-learning objective is still ef-\nfective over a whole-classiÔ¨Åcation model.\nAs a method,\nwhile every component in Meta-Baseline has been proposed\nin prior works, we Ô¨Ånd none of the prior works studies them\nas a whole. Therefore, Meta-Baseline should also be an im-\nportant baseline that has been overlooked.\n4. Results on Standard Benchmarks\n4.1. Datasets\nThe miniImageNet dataset [29] is a common benchmark\nfor few-shot learning. It contains 100 classes sampled from\nILSVRC-2012 [21], which are then randomly split to 64,\n16, 20 classes as training, validation, and testing set respec-\ntively. Each class contains 600 images of size 84 √ó 84.\nThe tieredImageNet dataset [20] is another common\nbenchmark proposed more recently with much larger scale.\nIt is a subset of ILSVRC-2012, containing 608 classes from\n34 super-categories, which are then split into 20, 6, 8 super-\ncategories, resulting in 351, 97, 160 classes as training, val-\nidation, testing set respectively. The image size is 84 √ó 84.\nThis setting is more challenging since base classes and\nnovel classes come from different super-categories.\nIn addition to the datasets above, we evaluate our model\non ImageNet-800, which is derived from ILSVRC-2012 1K\nclasses by randomly splitting 800 classes as base classes\nand 200 classes as novel classes. The base classes contain\nthe images from the original training set, the novel classes\ncontain the images from the original validation set. This\nlarger dataset aims at making the training setting standard\nas the ImageNet 1K classiÔ¨Åcation task [8].\n4.2. Implementation details\nWe use ResNet-12 that follows the most of recent\nworks [16, 25, 11, 33] on miniImageNet and tieredIma-\ngeNet, and we use ResNet-18, ResNet-50 [8] on ImageNet-\n800. For the classiÔ¨Åcation training stage, we use the SGD\noptimizer with momentum 0.9, the learning rate starts from\n0.1 and the decay factor is 0.1. On miniImageNet, we train\n100 epochs with batch size 128 on 4 GPUs, the learning\nrate decays at epoch 90. On tieredImageNet, we train 120\nepochs with batch size 512 on 4 GPUs, the learning rate de-\n4\n",
    "Model\nBackbone\n1-shot\n5-shot\nMatching Networks [29]\nConvNet-4\n43.56 ¬± 0.84\n55.31 ¬± 0.73\nPrototypical Networks [24]\nConvNet-4\n48.70 ¬± 1.84\n63.11 ¬± 0.92\nPrototypical Networks (re-implement)\nResNet-12\n53.81 ¬± 0.23\n75.68 ¬± 0.17\nActivation to Parameter [18]\nWRN-28-10\n59.60 ¬± 0.41\n73.74 ¬± 0.19\nLEO [22]\nWRN-28-10\n61.76 ¬± 0.08\n77.59 ¬± 0.12\nBaseline++ [1]\nResNet-18\n51.87 ¬± 0.77\n75.68 ¬± 0.63\nSNAIL [13]\nResNet-12\n55.71 ¬± 0.99\n68.88 ¬± 0.92\nAdaResNet [15]\nResNet-12\n56.88 ¬± 0.62\n71.94 ¬± 0.57\nTADAM [16]\nResNet-12\n58.50 ¬± 0.30\n76.70 ¬± 0.30\nMTL [25]\nResNet-12\n61.20 ¬± 1.80\n75.50 ¬± 0.80\nMetaOptNet [11]\nResNet-12\n62.64 ¬± 0.61\n78.63 ¬± 0.46\nSLA-AG [10]\nResNet-12\n62.93 ¬± 0.63\n79.63 ¬± 0.47\nProtoNets + TRAML [12]\nResNet-12\n60.31 ¬± 0.48\n77.94 ¬± 0.57\nConstellationNet [33]\nResNet-12\n64.89 ¬± 0.23\n79.95 ¬± 0.17\nClassiÔ¨Åer-Baseline (ours)\nResNet-12\n58.91 ¬± 0.23\n77.76 ¬± 0.17\nMeta-Baseline (ours)\nResNet-12\n63.17 ¬± 0.23\n79.26 ¬± 0.17\nTable 2: Comparison to prior works on miniImageNet. Average 5-way accuracy (%) with 95% conÔ¨Ådence interval.\nModel\nBackbone\n1-shot\n5-shot\nMAML [5]\nConvNet-4\n51.67 ¬± 1.81\n70.30 ¬± 1.75\nPrototypical Networks* [24]\nConvNet-4\n53.31 ¬± 0.89\n72.69 ¬± 0.74\nRelation Networks* [26]\nConvNet-4\n54.48 ¬± 0.93\n71.32 ¬± 0.78\nLEO [22]\nWRN-28-10\n66.33 ¬± 0.05\n81.44 ¬± 0.09\nMetaOptNet [11]\nResNet-12\n65.99 ¬± 0.72\n81.56 ¬± 0.53\nClassiÔ¨Åer-Baseline (ours)\nResNet-12\n68.07 ¬± 0.26\n83.74 ¬± 0.18\nMeta-Baseline (ours)\nResNet-12\n68.62 ¬± 0.27\n83.74 ¬± 0.18\nTable 3: Comparison to prior works on tieredImageNet. Average 5-way accuracy (%) with 95% conÔ¨Ådence interval.\ncays at epoch 40 and 80. On ImageNet-800, we train 90\nepochs with batch size 256 on 8 GPUs, the learning rate de-\ncays at epoch 30 and 60. The weight decay is 0.0005 for\nResNet-12 and 0.0001 for ResNet-18 or ResNet-50. Stan-\ndard data augmentation is applied, including random re-\nsized crop and horizontal Ô¨Çip. For meta-learning stage, we\nuse the SGD optimizer with momentum 0.9. The learning\nrate is Ô¨Åxed as 0.001. The batch size is 4, i.e. each training\nbatch contains 4 few-shot tasks to compute the average loss.\nThe cosine scaling parameter œÑ is initialized as 10.\nWe also apply consistent sampling for evaluating the per-\nformance. For the novel class split in a dataset, the sampling\nof testing few-shot tasks follows a deterministic order. Con-\nsistent sampling allows us to get a better model comparison\nwith the same number of sampled tasks. In the following\nsections, when the conÔ¨Ådence interval is omitted in the ta-\nble, it indicates that a Ô¨Åxed set of 800 testing tasks are sam-\npled for estimating the performance.\n4.3. Results\nFollowing the standard-setting, we conduct experiments\non miniImageNet and tieredImageNet, the results are shown\nin Table 2 and 3 respectively. To get a fair comparison to\nprior works, we perform model selection according to the\nvalidation set. On both datasets, we observe that the Meta-\nBaseline achieves competitive performance to state-of-the-\nart methods. We highlight that many methods for compar-\nison introduce more parameters and architecture designs\n(e.g. self-attention in [33]), while Meta-Baseline has the\nminimum parameters and the simplest design. We also no-\ntice that the simple ClassiÔ¨Åer-Baseline can achieve compet-\nitive performance when compared to meta-learning meth-\nods, especially in 5-shot tasks. We observe that the meta-\nlearning stage consistently improves ClassiÔ¨Åer-Baseline on\n5\n",
    "Model\nBackbone\n1-shot\n5-shot\nClassiÔ¨Åer-Baseline (ours)\nResNet-18\n83.51 ¬± 0.22\n94.82 ¬± 0.10\nMeta-Baseline (ours)\nResNet-18\n86.39 ¬± 0.22\n94.82 ¬± 0.10\nClassiÔ¨Åer-Baseline (ours)\nResNet-50\n86.07 ¬± 0.21\n96.14 ¬± 0.08\nMeta-Baseline (ours)\nResNet-50\n89.70 ¬± 0.19\n96.14 ¬± 0.08\nTable 4: Results on ImageNet-800. Average 5-way accuracy (%) is reported with 95% conÔ¨Ådence interval.\n1\n15\n30\nepochs\n82.0\n84.0\n86.0\n88.0\n90.0\n5-way acc (%)\nminiImageNet, 1-shot\n1\n15\n30\nepochs\n92.4\n93.0\n93.6\n94.2\n94.8\nminiImageNet, 5-shot\n1\n15\n30\nepochs\n86.4\n87.0\n87.6\n88.2\n88.8\ntieredImageNet, 1-shot\n1\n15\n30\nepochs\n93.6\n94.0\n94.4\n94.8\ntieredImageNet, 5-shot\n61.5\n62.0\n62.5\n63.0\n63.5\n78.9\n79.2\n79.5\n79.8\n80.1\n66.5\n67.2\n67.9\n68.6\n81.0\n81.5\n82.0\n82.5\n83.0\nMeta-Baseline, Meta-Learning Stage (ResNet-12)\nbase class generalization\nnovel class generalization\nFigure 2: Objective discrepancy of meta-learning on miniImageNet and tieredImageNet. Each epoch contains 200\ntraining batches. Average 5-way accuracy (%) is reported.\n1\n30\n60\n90\nepochs\n94.8\n95.2\n95.6\n96.0\n5-way acc (%)\nResNet-50, 1-shot\n1\n30\n60\n90\nepochs\n98.1\n98.2\n98.3\n98.4\n98.5\nResNet-50, 5-shot\n87.5\n88.2\n88.9\n89.6\n94.8\n95.1\n95.4\n95.7\n96.0\nbase class generalization\nnovel class generalization\nFigure 3: Objective discrepancy of meta-learning on\nImageNet-800. Each epoch contains 500 training batches.\nAverage 5-way accuracy (%) is reported.\nminiImageNet. Compared to miniImageNet, we Ô¨Ånd that\nthe gap between Meta-Baseline and ClassiÔ¨Åer-Baseline is\nsmaller on tieredImageNet, and the meta-learning stage\ndoes not improve 5-shot in this case.\nWe further evaluate our methods on the larger dataset\nImageNet-800.\nIn this larger-scale experiment, we Ô¨Ånd\nfreezing the Batch Normalization layer [9] (set to eval\nmode) is beneÔ¨Åcial.\nThe results are shown in Table 4.\nFrom the results, we observe that in this large dataset Meta-\nBaseline improves ClassiÔ¨Åer-Baseline in 1-shot, while it is\nnot improving the performance in 5-shot.\n5. Observations and Hypothesis\n5.1. Objective discrepancy in meta-learning\nDespite\nthe\nimprovements\nof\nmeta-learning\nover\nClassiÔ¨Åer-Baseline, we observe the test performance drops\nduring the meta-learning stage. While a common assump-\ntion for this phenomenon is overÔ¨Åtting, we observe that this\nissue seems not to be mitigated on larger datasets. To further\nlocate the issue, we propose to evaluate base class general-\nization and novel class generalization. Base class general-\nization is measured by sampling tasks from unseen images\nin base classes, while novel class generalization refers to the\nperformance of few-shot tasks sampled from novel classes.\nThe base class generalization is the generalization in the in-\nput distribution for which the model is trained, it decouples\nthe commonly deÔ¨Åned generalization and class-level trans-\nfer performance, which helps for locating the reason for the\nperformance drop.\nFigure 2 and 3 demonstrate the meta-learning stage of\nMeta-Baseline on different datasets. We Ô¨Ånd that during the\nmeta-learning stage, when the base class generalization is\nincreasing, the novel class generalization can be decreasing\ninstead. This fact indicates that over a converged whole-\nclassiÔ¨Åcation model, the meta-learning objective itself, i.e.\nmaking the embedding generalize better in few-shot tasks\nfrom base classes, can have a negative effect on the perfor-\nmance of few-shot tasks from novel classes. It also gives a\n6\n",
    "Task\nModel\nmini-tiered\nmini-shufÔ¨Çed\nfull-tiered\nfull-shufÔ¨Çed\n1-shot\nClassiÔ¨Åer-Baseline\n56.91\n61.64\n68.76\n77.67\nMeta-Baseline\n58.44\n65.88\n69.52\n80.48\n‚àÜ\n+1.53\n+4.24\n+0.76\n+2.81\n5-shot\nClassiÔ¨Åer-Baseline\n74.30\n79.26\n84.07\n90.58\nMeta-Baseline\n74.63\n80.58\n84.07\n90.67\n‚àÜ\n+0.33\n+1.32\n+0.00\n+0.09\nTable 5: Effect of dataset properties. Average 5-way accuracy (%), with ResNet-12.\nTraining\nBase gen.\nNovel gen.\n1-shot\nw/ ClsTr\n86.42\n63.33\nw/o ClsTr\n86.74\n58.54\n5-shot\nw/ ClsTr\n93.54\n80.02\nw/o ClsTr\n94.47\n74.95\nTable 6: Comparison on Meta-Baseline training from\nscratch. Average 5-way accuracy (%), with ResNet-12 on\nminiImageNet. ClsTr: classiÔ¨Åcation training stage.\nMethod\n1-shot\n5-shot\nClassiÔ¨Åer-Baseline\n60.58\n79.24\nClassiÔ¨Åer-Baseline (Euc.)\n56.29\n78.93\nMeta-Baseline\n63.33\n80.02\nMeta-Baseline (Euc.)\n60.19\n79.50\nTable 7: Importance of inheriting a good metric. Aver-\nage 5-way accuracy (%), with ResNet-12 on miniImageNet.\npossible explanation for why such phenomenon is not mit-\nigated on larger datasets, as this is not sample-level over-\nÔ¨Åtting, but class-level overÔ¨Åtting, which is caused by the\nobjective discrepancy that the underlying training class dis-\ntribution is different from testing class distribution.\nThis observation suggests that we may reconsider the\nmotivation of the meta-learning framework for few-shot\nlearning. In some settings, optimizing towards the train-\ning objective with a consistent form as the testing objective\n(except the inevitable class difference) may have an even\nnegative effect. It is also likely that the whole-classiÔ¨Åcation\nlearns the embedding with stronger class transferability, and\nmeta-learning makes the model perform better at N-way\nK-shot tasks but tends to lose the class transferability.\n5.2. Effect of whole-classiÔ¨Åcation training before\nmeta-learning\nAccording to our hypothesis, the whole-classiÔ¨Åcation\npre-trained model has provided extra class transferabil-\nity for the meta-learning model, therefore, it is natural to\ncompare Meta-Baseline with and without the classiÔ¨Åcation\ntraining stage. The results are shown in Table 6. We observe\nthat Meta-Baseline trained without classiÔ¨Åcation training\nstage can actually achieve higher base class generalization,\nbut its novel class generalization is much lower when com-\npared to Meta-Baseline with whole-classiÔ¨Åcation training.\nThese results support our hypothesis, that the whole-\nclassiÔ¨Åcation training provides the embedding with stronger\nclass transferability, which signiÔ¨Åcantly helps novel class\ngeneralization.\nInterestingly, TADAM [16] Ô¨Ånds that\nco-training the meta-learning objective with a whole-\nclassiÔ¨Åcation task is beneÔ¨Åcial, which may be potentially\nrelated to our hypothesis. While our results show it is likely\nthat the key effect of the whole-classiÔ¨Åcation objective is\nimproving the class transferability, it also indicates a po-\ntential trade-off that the whole-classiÔ¨Åcation objective can\nhave a negative effect on base class generalization.\n5.3. What makes Meta-Baseline a strong baseline?\nAs a method with a similar objective as ProtoNet [24],\nMeta-Baseline achieves nearly 10% higher accuracy on 1-\nshot in Table 2. The observations and hypothesis in previ-\nous sections potentially explain its strength, as it starts with\nthe embedding of a whole-classiÔ¨Åcation model which has\nstronger class transferability.\nWe perform further experiments, that in Meta-Baseline\n(with classiÔ¨Åcation training stage) we replace the cosine\ndistance with the squared Euclidean distance proposed in\nProtoNet [24]. To get a fair comparison, we also include\nthe learnable scalar œÑ with a proper initialization value 0.1.\nThe results are shown in Table 7.\nWhile ProtoNet [24]\nÔ¨Ånds that squared Euclidean distance (as a Bregman di-\nvergence) works better than cosine distance when perform-\ning meta-learning from scratch, here we start meta-learning\nfrom ClassiÔ¨Åer-Baseline and we observe that cosine sim-\nilarity works much better. A potential reason is that, as\nshown in Table 7, cosine nearest-centroid works much bet-\nter than nearest-centroid with squared Euclidean distance in\nClassiÔ¨Åer-Baseline (note that this is just the evaluation met-\nric and has no changes in training). Inheriting a good metric\n7\n",
    "for ClassiÔ¨Åer-Baseline might be the key that makes Meta-\nBaseline strong. According to our hypothesis, the embed-\nding from the whole-classiÔ¨Åcation model has strong class\ntransferability, inheriting a good metric potentially mini-\nmizes the future modiÔ¨Åcations on the embedding from the\nwhole-classiÔ¨Åcation model, thus it can keep the class trans-\nferability better and achieve higher performance.\n5.4. Effect of dataset properties\nWe construct four variants from the tieredImageNet\ndataset. SpeciÔ¨Åcally, full-tiered refers to the original tiered-\nImageNet, full-shufÔ¨Çed is constructed by randomly shuf-\nÔ¨Çing the classes in tieredImageNet and re-splitting the\nclasses into training, validation, and test set.\nThe mini-\ntiered and mini-shufÔ¨Çed datasets are constructed from full-\ntiered and full-shufÔ¨Çed respectively, their training set is con-\nstructed by randomly selecting 64 classes with 600 images\nfrom each class in the full training set, while the validation\nset and the test set remain unchanged. Since tieredImageNet\nseparates training classes and testing classes into different\nsuper categories, shufÔ¨Çing these classes will mix the classes\nin different super categories together and make the distribu-\ntion of base classes and novel classes closer.\nOur previous experiments show that base class general-\nization is always improving, if novel classes are covered\nby the distribution of base classes, the novel class gener-\nalization should also keep increasing. From Table 5, we\ncan see that from mini-tiered to mini-shufÔ¨Çed, and from\nfull-tiered to full-shufÔ¨Çed, the improvement achieved by the\nmeta-learning stage gets signiÔ¨Åcantly larger, which consis-\ntently supports our hypothesis. Therefore, our results in-\ndicate it is likely that meta-learning is mostly effective over\nwhole-classiÔ¨Åcation training when novel classes are similar\nto base classes.\nWe also observe that other factors may affect the im-\nprovement of meta-learning. From mini-tiered to full-tiered\nand from mini-shufÔ¨Çed to full-shufÔ¨Çed, when the dataset\ngets larger the improvements become less. A potential hy-\npothesis could be that the class transferability advantage of\nwhole-classiÔ¨Åcation training becomes more obvious when\ntrained on large datasets. From the results of our experi-\nments in Table 2, 3, 4, 5, we observe that the improvement\nof the meta-learning stage in 5-shot is less than 1-shot. We\nhypothesize this is because when there are more shots, tak-\ning average embedding becomes a more reasonable choice\nto estimate the class center in ClassiÔ¨Åer-Baseline, therefore\nthe advantage of meta-learning becomes less.\n5.5. The trade-off between meta-learning and\nwhole-classiÔ¨Åcation\nAll of the experiments in previous sections support a key\nhypothesis, that there exists a trade-off: the meta-learning\nobjective learns better embedding for N-way K-shot tasks\n(in the same distribution), while the whole-classiÔ¨Åcation ob-\njective learns embedding with stronger class transferabil-\nity. Optimizing towards one objective may hurt the strength\nof another objective. With this hypothesis, Meta-Baseline\nbalances this trade-off by choosing to calibrate the whole-\nclassiÔ¨Åcation embedding with meta-learning and inherit the\nmetric with high initial performance.\nThe discrepancy between base class generalization and\nnovel class generalization also considers the effectiveness\nof meta-learning and whole-classiÔ¨Åcation from the perspec-\ntive of datasets. SpeciÔ¨Åcally, when novel classes are similar\nenough to base classes or the base classes are sufÔ¨Åcient to\ncover the distribution of novel classes, novel class gener-\nalization should converge to base class generalization. In\npractice, this can be potentially achieved by collecting base\nclasses that are similar to the target novel classes. In this\ncase, it may be possible that the novel meta-learning algo-\nrithms outperform the whole-classiÔ¨Åcation baselines again.\n6. Additional Results on Meta-Dataset\nMeta-Dataset [28] is a new benchmark proposed for few-\nshot learning, it consists of diverse datasets for training and\nevaluation. They also propose to generate few-shot tasks\nwith a variable number of ways and shots, for having a\nsetting closer to the real world. We follow the setting in\nMeta-Dataset [28] and use ResNet-18 as the backbone, with\nthe original image size of 126√ó126, which is resized to be\n128√ó128 before feeding into the network. For the classiÔ¨Å-\ncation training stage, we apply the training setting similar to\nour setting in ImageNet-800. For the meta-learning stage,\nthe model is trained for 5000 iterations with one task in each\niteration.\nThe left side of Table 8 demonstrates the models trained\nwith samples in ILSVRC-2012 only. We observe that the\nMeta-Baseline does not signiÔ¨Åcantly improve ClassiÔ¨Åer-\nBaseline under this setting in our experiments, possibly due\nto the average number of shots are high.\nThe right side of Table 8 shows the results when the\nmodels are trained on all datasets, except TrafÔ¨Åc Signs and\nMSCOCO which have no training samples. The ClassiÔ¨Åer-\nBaseline is trained as a multi-dataset classiÔ¨Åer, i.e. an en-\ncoder together with multiple FC layers over the encoded\nfeature to output the logits for different datasets. The clas-\nsiÔ¨Åcation training stage has the same number of iterations\nas training on ILSVRC only, to mimic the ILSVRC train-\ning, a batch has 0.5 probability to be from ILSVRC and 0.5\nprobability to be uniformly sampled from one of the other\ndatasets. For ClassiÔ¨Åer-Baseline, comparing to the results\non the left side of Table 8, we observe that while the per-\nformance on ILSVRC is worse, the performances on other\ndatasets are mostly improved due to having their samples in\ntraining. It can be also noticed that the cases where Meta-\nBaseline improves ClassiÔ¨Åer-Baseline are mostly on the\n8\n",
    "Trained on ILSVRC\nTrained on all datasets\nDataset\nfo-Proto-MAML\nClassiÔ¨Åer/Meta\nfo-Proto-MAML\nClassiÔ¨Åer\nMeta\n[28]\n(ours)\n[28]\n(ours)\n(ours)\nILSVRC\n49.5\n59.2\n46.5\n55.0\n48.0\nOmniglot\n63.4\n69.1\n82.7\n76.9\n89.4\nAircraft\n56.0\n54.1\n75.2\n69.8\n81.7\nBirds\n68.7\n77.3\n69.9\n78.3\n77.3\nTextures\n66.5\n76.0\n68.3\n71.4\n64.5\nQuick Draw\n51.5\n57.3\n66.8\n62.7\n74.5\nFungi\n40.0\n45.4\n42.0\n55.4\n60.2\nVGG Flower\n87.2\n89.6\n88.7\n90.6\n83.8\nTrafÔ¨Åc Signs\n48.8\n66.2\n52.4\n69.3\n59.5\nMSCOCO\n43.7\n55.7\n41.7\n53.1\n43.6\nTable 8: Additional results on Meta-Dataset. Average accuracy (%), with variable number of ways and shots. The fo-Proto-\nMAML method is from Meta-Dataset [28], ClassiÔ¨Åer and Meta refers to ClassiÔ¨Åer-Baseline and Meta-Baseline respectively,\n1000 tasks are sampled for evaluating ClassiÔ¨Åer or Meta. Note that TrafÔ¨Åc Signs and MSCOCO have no training set.\ndatasets which are ‚Äúless relevant‚Äù to ILSVRC (the dataset\n‚Äúrelevance‚Äù could be shown in Dvornik et al. [3]). A po-\ntential reason is that the multi-dataset classiÔ¨Åcation train-\ning stage samples ILSVRC with 0.5 probability, similar\nto ILSVRC training, the meta-learning stage is hard to\nimprove on ILSVRC, therefore those datasets relevant to\nILSVRC will have similar properties so that it is hard to\nimprove on them.\n7. Conclusion and Discussion\nIn this work, we presented a simple Meta-Baseline that\nhas been overlooked for few-shot learning. Without any ad-\nditional parameters or complex design choices, it is compet-\nitive to state-of-the-art methods on standard benchmarks.\nOur experiments indicate that there might be an objec-\ntive discrepancy in the meta-learning framework for few-\nshot learning, i.e. a meta-learning model generalizing better\non unseen tasks from base classes might have worse perfor-\nmance on tasks from novel classes. This provides a possible\nexplanation that why some complex meta-learning methods\ncould not get signiÔ¨Åcantly better performance than simple\nwhole-classiÔ¨Åcation. While most recent works focus on im-\nproving the meta-learning structures, many of them did not\nexplicitly address the issue of class transferability. Our ob-\nservations suggest that the objective discrepancy might be a\npotential key challenge to tackle.\nWhile many novel meta-learning algorithms are pro-\nposed and some recent works report that simple whole-\nclassiÔ¨Åcation training is good enough for few-shot learn-\ning, we show that meta-learning is still effective over\nwhole-classiÔ¨Åcation models. We observe a potential trade-\noff between the objectives of meta-learning and whole-\nclassiÔ¨Åcation.\nFrom the perspective of datasets, we\ndemonstrate how the preference between meta-learning and\nwhole-classiÔ¨Åcation changes according to class similarity\nand other factors, indicating that these factors may need\nmore attention for model comparisons in future work.\nAcknowledgements. This work was supported, in part, by grants from\nDARPA LwLL, NSF 1730158 CI-New: Cognitive Hardware and Software\nEcosystem Community Infrastructure (CHASE-CI), NSF ACI-1541349\nCC*DNI PaciÔ¨Åc Research Platform, and gifts from Qualcomm, TuSim-\nple and Picsart. Prof. Darrell was supported, in part, by DoD including\nDARPA‚Äôs XAI, LwLL, and/or SemaFor programs, as well as BAIR‚Äôs in-\ndustrial alliance programs. We thank Hang Gao for the helpful discussions.\nReferences\n[1] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank\nWang, and Jia-Bin Huang. A closer look at few-shot classi-\nÔ¨Åcation. In International Conference on Learning Represen-\ntations, 2019. 1, 2, 3, 5, 11, 12\n[2] Guneet S Dhillon, Pratik Chaudhari, Avinash Ravichandran,\nand Stefano Soatto. A baseline for few-shot image classiÔ¨Å-\ncation. arXiv preprint arXiv:1909.02729, 2019. 2\n[3] Nikita Dvornik, Cordelia Schmid, and Julien Mairal. Select-\ning relevant features from a universal representation for few-\nshot classiÔ¨Åcation. arXiv preprint arXiv:2003.09338, 2020.\n9\n[4] Li Fei-Fei, Rob Fergus, and Pietro Perona. One-shot learning\nof object categories. IEEE transactions on pattern analysis\nand machine intelligence, 28(4):594‚Äì611, 2006. 1\n[5] Chelsea Finn, Pieter Abbeel, and Sergey Levine.\nModel-\nagnostic meta-learning for fast adaptation of deep networks.\nIn Proceedings of the 34th International Conference on Ma-\nchine Learning-Volume 70, pages 1126‚Äì1135. JMLR. org,\n2017. 1, 2, 5\n9\n",
    "[6] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot\nvisual learning without forgetting.\nIn Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 4367‚Äì4375, 2018. 1, 2, 3, 4, 11\n[7] Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and\nThomas GrifÔ¨Åths. Recasting gradient-based meta-learning as\nhierarchical bayes. arXiv preprint arXiv:1801.08930, 2018.\n2\n[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770‚Äì778, 2016. 4\n[9] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. arXiv preprint arXiv:1502.03167, 2015. 6, 11\n[10] Hankook Lee, Sung Ju Hwang, and Jinwoo Shin.\nSelf-\nsupervised label augmentation via input transformations.\nIn International Conference on Machine Learning, pages\n5714‚Äì5724. PMLR, 2020. 5\n[11] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and\nStefano Soatto. Meta-learning with differentiable convex op-\ntimization. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 10657‚Äì10665,\n2019. 1, 2, 4, 5, 11\n[12] Aoxue Li, Weiran Huang, Xu Lan, Jiashi Feng, Zhenguo Li,\nand Liwei Wang. Boosting few-shot learning with adaptive\nmargin loss. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 12576‚Äì\n12584, 2020. 5\n[13] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter\nAbbeel. A simple neural attentive meta-learner. In Inter-\nnational Conference on Learning Representations, 2018. 2,\n5\n[14] Tsendsuren Munkhdalai and Hong Yu. Meta networks. In\nProceedings of the 34th International Conference on Ma-\nchine Learning-Volume 70, pages 2554‚Äì2563. JMLR. org,\n2017. 2\n[15] Tsendsuren Munkhdalai, Xingdi Yuan, Soroush Mehri, and\nAdam Trischler. Rapid adaptation with conditionally shifted\nneurons. arXiv preprint arXiv:1712.09926, 2017. 2, 5\n[16] Boris Oreshkin, Pau Rodr¬¥ƒ±guez L¬¥opez, and Alexandre La-\ncoste. Tadam: Task dependent adaptive metric for improved\nfew-shot learning. In Advances in Neural Information Pro-\ncessing Systems, pages 721‚Äì731, 2018. 2, 4, 5, 7, 11\n[17] Hang Qi, Matthew Brown, and David G Lowe. Low-shot\nlearning with imprinted weights. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\npages 5822‚Äì5830, 2018. 4\n[18] Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan L Yuille. Few-\nshot image recognition by predicting parameters from activa-\ntions. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 7229‚Äì7238, 2018. 5\n[19] Sachin Ravi and Hugo Larochelle. Optimization as a model\nfor few-shot learning.\nIn In International Conference on\nLearning Representations (ICLR), 2017. 2\n[20] Mengye Ren, Eleni TriantaÔ¨Ållou, Sachin Ravi, Jake Snell,\nKevin Swersky, Joshua B Tenenbaum, Hugo Larochelle, and\nRichard S Zemel. Meta-learning for semi-supervised few-\nshot classiÔ¨Åcation. arXiv preprint arXiv:1803.00676, 2018.\n4\n[21] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al.\nImagenet large\nscale visual recognition challenge. International journal of\ncomputer vision, 115(3):211‚Äì252, 2015. 4\n[22] Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol\nVinyals, Razvan Pascanu, Simon Osindero, and Raia Had-\nsell.\nMeta-learning with latent embedding optimization.\nIn International Conference on Learning Representations,\n2019. 1, 2, 5\n[23] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan\nWierstra, and Timothy Lillicrap.\nMeta-learning with\nmemory-augmented neural networks. In International con-\nference on machine learning, pages 1842‚Äì1850, 2016. 2\n[24] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical\nnetworks for few-shot learning. In Advances in Neural Infor-\nmation Processing Systems, pages 4077‚Äì4087, 2017. 2, 3, 5,\n7\n[25] Qianru Sun, Yaoyao Liu, Tat-Seng Chua, and Bernt Schiele.\nMeta-transfer learning for few-shot learning.\nIn Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 403‚Äì412, 2019. 1, 2, 4, 5\n[26] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS\nTorr, and Timothy M Hospedales. Learning to compare: Re-\nlation network for few-shot learning. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 1199‚Äì1208, 2018. 1, 2, 5\n[27] Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenen-\nbaum, and Phillip Isola. Rethinking few-shot image classi-\nÔ¨Åcation: a good embedding is all you need? arXiv preprint\narXiv:2003.11539, 2020. 1, 2, 3, 11\n[28] Eleni TriantaÔ¨Ållou, Tyler Zhu, Vincent Dumoulin, Pascal\nLamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles\nGelada, Kevin Swersky, Pierre-Antoine Manzagol, et al.\nMeta-dataset: A dataset of datasets for learning to learn from\nfew examples. arXiv preprint arXiv:1903.03096, 2019. 8, 9\n[29] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan\nWierstra, et al. Matching networks for one shot learning. In\nAdvances in neural information processing systems, pages\n3630‚Äì3638, 2016. 1, 2, 3, 4, 5\n[30] Xin Wang, Fisher Yu, Ruth Wang, Trevor Darrell, and\nJoseph E Gonzalez. Tafe-net: Task-aware feature embed-\ndings for low shot learning. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n1831‚Äì1840, 2019. 1\n[31] Yan Wang, Wei-Lun Chao, Kilian Q Weinberger, and Lau-\nrens van der Maaten.\nSimpleshot:\nRevisiting nearest-\nneighbor classiÔ¨Åcation for few-shot learning. arXiv preprint\narXiv:1911.04623, 2019. 1, 2, 3\n[32] Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical\nevaluation of rectiÔ¨Åed activations in convolutional network.\narXiv preprint arXiv:1505.00853, 2015. 11\n[33] Weijian Xu, yifan xu, Huaijin Wang, and Zhuowen Tu. Con-\nstellation nets for few-shot learning. In International Con-\nference on Learning Representations, 2021. 1, 2, 4, 5, 11\n10\n",
    "1\n40\n80\n120\n160\nepochs\n40\n60\n80\n5-way acc (%)\nminiImageNet, 1-shot\n1\n40\n80\n120\n160\nepochs\n50\n60\n70\n80\n90\n5-way acc (%)\nminiImageNet, 5-shot\nMeta-Baseline, Training from Scratch\nbase class generalization\nnovel class generalization\nFigure 4: Training Meta-Baseline without classiÔ¨Åcation-\ntraining stage on miniImageNet.\nDataset\nClassiÔ¨Åer\n1-shot\n5-shot\nminiImageNet\nLinear\n60.58\n79.24\nCosine\n61.93\n78.73\ntieredImageNet\nLinear\n68.76\n84.07\nCosine\n67.58\n83.31\nTable 9: Comparison to classiÔ¨Åer trained with cosine met-\nric, Average 5-way accuracy (%), with ResNet-12.\nA. Details of ResNet-12\nThe ResNet-12 backbone consists of 4 residual blocks\nthat each residual block has 3 convolutional layers. Each\nconvolutional layer has a 3 √ó 3 kernel, followed by Batch\nNormalization [9] and Leaky ReLU [32] with 0.1 slope.\nThe channels of convolutional layers in each residual block\nare 64, 128, 256, 512 respectively, a 2√ó2 max-pooling layer\nis applied after each residual block. Finally, a 5 √ó 5 global\naverage pooling is applied to get a 512-dimensional feature\nvector.\nThis architecture is consistent with recent works [16, 33].\nSome other recent works also introduce additional parame-\nters and design choices in the backbone (e.g. DropBlock\nand wider channels of 64, 160, 320, 640 in [11, 27]), while\nthese modiÔ¨Åcations may make the performance higher, we\ndo not include them here for simplicity.\nB. Training plot of Meta-Baseline without clas-\nsiÔ¨Åcation training stage\nWe show the process of training Meta-Baseline from\nscratch (i.e.\nwithout the classiÔ¨Åcation-training stage) on\nminiImageNet in Figure 4. We observe that when the learn-\ning rate decays, the novel class generalization quickly starts\nto be decreasing. While it is able to achieve higher base\nclass generalization than Meta-Baseline with classiÔ¨Åcation\ntraining, its highest novel class generalization is still much\nworse, suggesting whole-classiÔ¨Åcation training may pro-\nvide representations with extra class transferability.\nC. Comparison to cosine classiÔ¨Åcation training\nWe compare the effect of classiÔ¨Åcation training with\nreplacing the last linear-classiÔ¨Åer with cosine nearest-\nneighbor metric which is proposed in prior work [6, 1], the\nresults are shown in Table 9, where Cosine denotes clas-\nsiÔ¨Åcation training with cosine metric and Linear denotes\nthe standard classiÔ¨Åcation training. On miniImageNet, we\nobserve that Cosine outperforms Linear in 1-shot, but has\nworse performance in 5-shot. On tieredImageNet, we ob-\nserve Linear outperforms Cosine in both 1-shot and 5-shot.\nWe choose to use the linear layer as it is more common and\nwe Ô¨Ånd it works better in more cases.\nD. Objective discrepancy on ImageNet-800\nBesides miniImageNet and tieredImageNet, in our large-\nscale dataset ImageNet-800, we also observe the novel class\ngeneralization decreasing when base class generalization\nis increasing, the training process is demonstrated in Fig-\nure 5.\nFrom the Ô¨Ågure, we see that for both backbones\nof ResNet-18 and ResNet-50, the base class generalization\nperformance is increasing during the training epochs, while\nthe novel class generalization performance quickly starts to\nbe decreasing. These observations are consistent with our\nobservations on miniImageNet and tieredImageNet, which\nfurther support our hypothesis.\nE. Comparison of the ClassiÔ¨Åer-Baseline and\nBaseline++ [1]\nWe connect the ClassiÔ¨Åer-Baseline to Baseline++ [1]\nwith a step-by-step ablation study on miniImageNet, the\nresults are shown in Table 10. We see that Ô¨Åne-tuning is\noutperformed by the simple nearest-centroid method with\ncosine metric, and using a standard ImageNet-like opti-\nmizer signiÔ¨Åcantly improves the performance of the whole-\nclassiÔ¨Åcation method for few-shot learning.\n11\n",
    "1\n30\n60\n90\nepochs\n90.0\n90.9\n91.8\n92.7\n5-way acc (%)\nResNet-18, 1-shot\n1\n30\n60\n90\nepochs\n96.3\n96.6\n96.9\n97.2\n97.5\nResNet-18, 5-shot\n1\n30\n60\n90\nepochs\n94.8\n95.2\n95.6\n96.0\nResNet-50, 1-shot\n1\n30\n60\n90\nepochs\n98.1\n98.2\n98.3\n98.4\n98.5\nResNet-50, 5-shot\n85.2\n85.6\n86.0\n86.4\n93.6\n93.8\n94.0\n94.2\n87.5\n88.2\n88.9\n89.6\n94.8\n95.1\n95.4\n95.7\n96.0\nMeta-Baseline, Meta-Learning Stage (ImageNet-800)\nbase class generalization\nnovel class generalization\nFigure 5: Objective discrepancy of meta-learning on ImageNet-800. Each epoch contains 500 training batches. Average\n5-way accuracy (%) is reported.\ndifference\n5-way 1-shot accuracy (%)\n(default, reported [1])\n51.75 ¬± 0.80\n(default, reproduced by their code)\n50.84 ¬± 0.80\nÔ¨Ånetune ‚Üícosine nearest-centroid\n52.15 ¬± 0.83\nepoch-300 ‚Üíepoch-50\n53.37 ¬± 0.71\nremove color-jittering\n56.06 ¬± 0.71\n224√ó224 input size ‚Üíresizing 84√ó84 to 224√ó224\n50.49 ¬± 0.71\nResNet-18 ‚ÜíResNet-12\n53.59 ¬± 0.72\nAdam (lr=0.001, batch=16) ‚ÜíSGD (lr=0.1, batch=128)\n59.19 ¬± 0.71\nTable 10: Comparison of the ClassiÔ¨Åer-Baseline and Baseline++ [1].\n12\n"
  ],
  "full_text": "Meta-Baseline: Exploring Simple Meta-Learning for Few-Shot Learning\nYinbo Chen\nUC San Diego\nZhuang Liu\nUC Berkeley\nHuijuan Xu\nPenn State University\nTrevor Darrell\nUC Berkeley\nXiaolong Wang\nUC San Diego\nAbstract\nMeta-learning has been the most common framework for\nfew-shot learning in recent years. It learns the model from\ncollections of few-shot classiÔ¨Åcation tasks, which is believed\nto have a key advantage of making the training objective\nconsistent with the testing objective.\nHowever, some re-\ncent works report that by training for whole-classiÔ¨Åcation,\ni.e. classiÔ¨Åcation on the whole label-set, it can get compa-\nrable or even better embedding than many meta-learning\nalgorithms.\nThe edge between these two lines of works\nhas yet been underexplored, and the effectiveness of meta-\nlearning in few-shot learning remains unclear. In this paper,\nwe explore a simple process: meta-learning over a whole-\nclassiÔ¨Åcation pre-trained model on its evaluation metric.\nWe observe this simple method achieves competitive per-\nformance to state-of-the-art methods on standard bench-\nmarks. Our further analysis shed some light on understand-\ning the trade-offs between the meta-learning objective and\nthe whole-classiÔ¨Åcation objective in few-shot learning. Our\ncode is available at https://github.com/yinboc/\nfew-shot-meta-baseline.\n1. Introduction\nWhile humans have shown incredible ability to learn\nfrom very few examples and generalize to many different\nnew examples, the current deep learning approaches still\nrely on a large scale of training data. To mimic this hu-\nman ability of generalization, few-shot learning [4, 29] is\nproposed for training networks to understand a new con-\ncept based on a few labeled examples. While directly learn-\ning a large number of parameters with few samples is very\nchallenging and most likely leads to overÔ¨Åtting, a practical\nsetting is applying transfer learning: train the network on\ncommon classes (also called base classes) with sufÔ¨Åcient\nsamples, then transfer the model to learn novel classes with\na few examples.\nThe meta-learning framework for few-shot learning fol-\nlows the key idea of learning to learn. SpeciÔ¨Åcally, it sam-\nples few-shot classiÔ¨Åcation tasks from training samples be-\nlonging to the base classes and optimizes the model to per-\nform well on these tasks. A task typically takes the form\nof N-way and K-shot, which contains N classes with K\nsupport samples and Q query samples in each class. The\ngoal is to classify these N √ó Q query samples into the\nN classes based on the N √ó K support samples. Under\nthis framework, the model is directly optimized on few-\nshot classiÔ¨Åcation tasks. The consistency between the ob-\njectives of training and testing is considered as the key ad-\nvantage of meta-learning. Motivated by this idea, many re-\ncent works [26, 6, 25, 30, 5, 22, 11, 33] focus on improving\nthe meta-learning structure, and few-shot learning itself has\nbecome a common testbed for evaluating meta-learning al-\ngorithms.\nHowever, some recent works Ô¨Ånd that training for whole-\nclassiÔ¨Åcation, i.e. classiÔ¨Åcation on the whole training label-\nset (base classes), provides the embedding that is compa-\nrable or even better than many recent meta-learning algo-\nrithms.\nThe effectiveness of whole-classiÔ¨Åcation models\nhas been reported in both prior works [6, 1] and some con-\ncurrent works [31, 27]. Meta-learning makes the form of\ntraining objective consistent with testing, but why it turns\nout to learn even worse embedding than simple whole-\nclassiÔ¨Åcation?\nWhile there are several possible reasons,\ne.g. optimization difÔ¨Åculty or overÔ¨Åtting, the answer has\nnot been clearly studied yet. It remains even unclear that\nwhether meta-learning is still effective compared to whole-\nclassiÔ¨Åcation in few-shot learning.\nIn this work, we aim at exploring the edge between\nwhole-classiÔ¨Åcation and meta-learning by decoupling the\ndiscrepancies. We start with ClassiÔ¨Åer-Baseline: a whole-\nclassiÔ¨Åcation method that is similarly proposed in concur-\nrent works [31, 27]. In ClassiÔ¨Åer-Baseline, we Ô¨Årst train\na classiÔ¨Åer on base classes, then remove the last fully-\nconnected (FC) layer which is class-dependent. During test\ntime, it computes mean embedding of support samples for\neach novel class as their centroids, and classiÔ¨Åes query sam-\nples to the nearest centroid with cosine distance. We ob-\nserve this baseline method outperforms many recent meta-\nlearning algorithms.\nIn order to understand whether meta-learning is still ef-\n1\narXiv:2003.04390v4  [cs.CV]  19 Aug 2021\n\n\nfective compared to whole-classiÔ¨Åcation, a natural experi-\nment is to see what happens if we perform further meta-\nlearning over a converged ClassiÔ¨Åer-Baseline on its evalu-\nation metric (i.e. cosine nearest-centroid). As a resulting\nmethod, it is similar to MatchingNet [29] or ProtoNet [24]\nwith an additional classiÔ¨Åcation pre-training stage.\nWe\nobserve that meta-learning can still improve ClassiÔ¨Åer-\nBaseline, and it achieves competitive performance to state-\nof-the-art methods on standard benchmarks. We call this\nsimple method Meta-Baseline.\nWe highlight that as a\nmethod, all the individual components of Meta-Baseline\nhave been proposed in prior works, but to the best of our\nknowledge, it has been overlooked that none of the prior\nworks studies them as a whole. We further decouple the\ndiscrepancies by evaluating on two types of generaliza-\ntion: base class generalization denotes performance on\nfew-shot classiÔ¨Åcation tasks from unseen data in the base\nclasses, which follows the common deÔ¨Ånition of general-\nization (i.e. evaluated in the training distribution); and novel\nclass generalization denotes performance on few-shot clas-\nsiÔ¨Åcation tasks from data in novel classes, which is the\ngoal of the few-shot learning problem. We observe that:\n(i) During meta-learning, improving base class generaliza-\ntion can lead to worse novel class generalization; (ii) When\ntraining Meta-Baseline from scratch (i.e. without whole-\nclassiÔ¨Åcation training), it achieves higher base-class gener-\nalization but much lower novel class generalization.\nOur observations suggest that there could be a trade-\noff between the objectives of meta-learning and whole-\nclassiÔ¨Åcation. It is likely that meta-learning learns the em-\nbedding that works better for N-way K-shot tasks, while\nwhole-classiÔ¨Åcation learns the embedding with stronger\nclass transferability.\nWe Ô¨Ånd that the main advantage\nof training for whole-classiÔ¨Åcation before meta-learning is\nlikely to be improving class transferability. Our further ex-\nperiments provide a potential explanation of what makes\nMeta-Baseline a strong baseline: by inheriting one of the\nmost effective evaluation metrics of the whole-classiÔ¨Åcation\nmodel, it maximizes the reusing of the embedding with\nstrong class transferability. From another perspective, our\nresults also rethink the comparison between meta-learning\nand whole-classiÔ¨Åcation from the perspective of datasets.\nWhen base classes are collected to cover the distribution of\nnovel classes, novel-class generalization should converge to\nbase-class generalization and the strength of meta-learning\nmay overwhelm the strength of whole-classiÔ¨Åcation.\nIn summary, our contributions are as following:\n‚Ä¢ We present a simple Meta-Baseline that has been over-\nlooked in prior work. It achieves competitive perfor-\nmance to state-of-the-art methods on standard bench-\nmarks and is easy to follow.\n‚Ä¢ We observe a trade-off between the objectives of meta-\nlearning and whole-classiÔ¨Åcation, which potentially\nexplains the success of Meta-Baseline and rethinks the\neffectiveness of both objectives in few-shot learning.\n2. Related Work\nMost recent approaches for few-shot learning follow the\nmeta-learning framework. The various meta-learning ar-\nchitectures for few-shot learning can be roughly catego-\nrized into three groups. Memory-based methods [19, 15,\n23, 13, 14] are based on the idea to train a meta-learner\nwith memory to learn novel concepts (e.g.\nan LSTM-\nbased meta-learner). Optimization-based methods [7, 22]\nfollows the idea of differentializing an optimization pro-\ncess over support-set within the meta-learning framework:\nMAML [5] Ô¨Ånds an initialization of the neural network that\ncan be adapted to any novel task using a few optimiza-\ntion steps. MetaOptNet [11] learns the feature represen-\ntation that can generalize well for a linear support vector\nmachine (SVM) classiÔ¨Åer. Besides explicitly considering\nthe dynamic learning process, metric-based methods [29]\nmeta-learn a deep representation with a metric in feature\nspace. For example, Prototypical Networks [24] compute\nthe average feature for each class in support-set and clas-\nsify query samples by the nearest-centroid method. They\nuse Euclidean distance since it is a Bregman divergence.\nRelation Networks [26] further generalizes this framework\nby proposing a relation module as a learnable metric jointly\ntrained with deep representations. TADAM [16] proposes to\nuse a task conditioned metric resulting in a task-dependent\nmetric space.\nWhile signiÔ¨Åcant progress is made in the meta-learning\nframework, some recent works challenge the effectiveness\nof meta-learning with simple whole-classiÔ¨Åcation, i.e.\na\nclassiÔ¨Åcation model on the whole training label-set. Co-\nsine classiÔ¨Åer [6] and Baseline++ [1] perform whole-\nclassiÔ¨Åcation training by replacing the top linear layer with\na cosine classiÔ¨Åer, and they adapt the classiÔ¨Åer to a few-\nshot classiÔ¨Åcation task of novel classes by performing near-\nest centroid or Ô¨Åne-tuning a new layer respectively. They\nshow these whole-classiÔ¨Åcation models can achieve com-\npetitive performance compared to several popular meta-\nlearning models. Another recent work [2] studies on a trans-\nductive setting. Along with these baseline methods, more\nadvanced meta-learning methods [25, 11, 33] are proposed\nand they set up new state-of-the-art results. The effective-\nness of whole-classiÔ¨Åcation is then revisited in two of the\nconcurrent works [31, 27] with improved design choices.\nBy far, the effectiveness of meta-learning compared to\nwhole-classiÔ¨Åcation in few-shot learning is still unclear,\nsince the edge between whole-classiÔ¨Åcation models and\nmeta-learning models remains underexplored. The goal of\nthis work is to explore the insights behind the phenomenons.\nOur experiments show a potential trade-off between the\n2\n\n\nMethod\nWhole-classiÔ¨Åcation training\nMeta-learning\nOthers\nMatching Networks [29]\nno / yes (large models)\nattention + cosine\nFCE\nPrototypical Networks [24]\nno\ncentroid + Euclidean\n-\nBaseline++ [1]\nyes (cosine classiÔ¨Åer)\n-\nÔ¨Åne-tuning\nMeta-Baseline (ours)\nyes\ncentroid + cosine (‚àóœÑ)\n-\nTable 1: Overview of method comparison. We summarize the differences between Meta-Baseline and prior methods.\nmeta-learning and whole-classiÔ¨Åcation objectives, which\nprovides a more clear understanding of the comparison be-\ntween both objectives for few-shot learning.\nAs a method, similar ideas to ClassiÔ¨Åer-Baseline are con-\ncurrently reported in recent works [31, 27]. Unlike some\nprior works [6, 1], ClassiÔ¨Åer-Baseline does not replace the\nlast layer with cosine classiÔ¨Åer during training, it trains\nthe whole-classiÔ¨Åcation model with a linear layer on the\ntop and applies cosine nearest-centroid metric during the\ntest time for few-shot classiÔ¨Åcation on novel classes. The\nMeta-Baseline is meta-learning over a converged ClassiÔ¨Åer-\nBaseline on its evaluation metric (cosine nearest-centroid).\nIt is similar (with inconspicuous and important differences\nas shown in Table 1) to those simple and classical metric-\nbased meta-learning methods [29, 24]. The main purpose of\nMeta-Baseline in this paper is to understand the comparison\nbetween whole-classiÔ¨Åcation and meta-learning objectives,\nbut we Ô¨Ånd it is also a simple meta-learning baseline that\nhas been overlooked. While every individual component in\nMeta-Baseline is not novel, to the best of our knowledge,\nnone of the prior works studies them as a whole.\n3. Method\n3.1. Problem deÔ¨Ånition\nIn standard few-shot classiÔ¨Åcation, given a labeled\ndataset of base classes Cbase with a large number of im-\nages, the goal is to learn concepts in novel classes Cnovel\nwith a few samples. In an N-way K-shot few-shot clas-\nsiÔ¨Åcation task, the support-set contains N classes with K\nsamples per class, the query-set contains samples from the\nsame N classes with Q samples per class, and the goal is to\nclassify the N √ó Q query images into N classes.\n3.2. ClassiÔ¨Åer-Baseline\nClassiÔ¨Åer-Baseline is a whole-classiÔ¨Åcation model, i.e.\na classiÔ¨Åcation model trained for the whole label-set. It\nrefers to training a classiÔ¨Åer with classiÔ¨Åcation loss on all\nbase classes and performing few-shot tasks with the cosine\nnearest-centroid method. SpeciÔ¨Åcally, we train a classiÔ¨Åer\non all base classes with standard cross-entropy loss, then re-\nmove its last FC layer and get the encoder fŒ∏, which maps\nthe input to embedding. Given a few-shot task with the\nsupport-set S, let Sc denote the few-shot samples in class\nc, it computes the average embedding wc as the centroid of\nclass c:\nwc =\n1\n|Sc|\nX\nx‚ààSc\nfŒ∏(x),\n(1)\nthen for a query sample x in a few-shot task, it predicts the\nprobability that sample x belongs to class c according to the\ncosine similarity between the embedding of sample x and\nthe centroid of class c:\np(y = c | x) =\nexp\n\u0000‚ü®fŒ∏(x), wc‚ü©\n\u0001\nP\nc‚Ä≤ exp\n\u0000‚ü®fŒ∏(x), wc‚Ä≤‚ü©\n\u0001,\n(2)\nwhere ‚ü®¬∑, ¬∑‚ü©denotes the cosine similarity of two vectors.\nSimilar methods to ClassiÔ¨Åer-Baseline have also been\nproposed in concurrent works [31, 27]. Compared to Base-\nline++ [1], the ClassiÔ¨Åer-Baseline does not use the cosine\nclassiÔ¨Åer for training or perform Ô¨Åne-tuning during testing,\nwhile it performs better on standard benchmarks. In this\nwork, we choose ClassiÔ¨Åer-Baseline as the representative of\nwhole-classiÔ¨Åcation models for few-shot learning. For sim-\nplicity and clarity, we do not introduce additional complex\ntechniques for this whole-classiÔ¨Åcation training.\n3.3. Meta-Baseline\nFigure 1 visualizes the Meta-Baseline. The Ô¨Årst stage\nis the classiÔ¨Åcation training stage, it trains a ClassiÔ¨Åer-\nBaseline, i.e. training a classiÔ¨Åer on all bases classes and\nremove its last FC layer to get fŒ∏. The second stage is the\nmeta-learning stage, which optimizes the model on the eval-\nuation metric of ClassiÔ¨Åer-Baseline. SpeciÔ¨Åcally, given the\nclassiÔ¨Åcation-trained feature encoder fŒ∏, it samples N-way\nK-shot tasks (with N √ó Q query samples) from training\nsamples in base classes. To compute the loss for each task,\nin support-set it computes the centroids of N classes de-\nÔ¨Åned in Equation 1, which are then used to compute the\npredicted probability distribution for each sample in query-\nset deÔ¨Åned in Equation 2. The loss is a cross-entropy loss\ncomputed from p and the labels of the samples in the query-\nset. During training, each training batch can contain several\ntasks and the average loss is computed.\nSince cosine similarity has the value range of [‚àí1, 1],\nwhen it is used to compute the logits, it can be helpful to\n3\n\n\nmean\ncos\nscore\nMeta-Baseline\ntraining\nClassifier-Baseline / Meta-Baseline\nevaluation\nsupport-set\nquery-set\nClassifier-Baseline\ntraining\nclassification\non base classes\nùëìùúÉ\nloss\nùëìùúÉ\nùëìùúÉ\nMeta-Learning Stage\nClassification Training Stage\nrepresentation\ntransfer\nF\nC\nlabel\nùúè\nFigure 1: ClassiÔ¨Åer-Baseline and Meta-Baseline. ClassiÔ¨Åer-Baseline is to train a classiÔ¨Åcation model on all base classes\nand remove its last FC layer to get the encoder fŒ∏. Given a few-shot task, it computes the average feature for samples of each\nclass in support-set, then it classiÔ¨Åes a sample in query-set by nearest-centroid with cosine similarity as distance. In Meta-\nBaseline, it further optimizes a converged ClassiÔ¨Åer-Baseline on its evaluation metric, and an additional learnable scalar œÑ is\nintroduced to scale cosine similarity.\nscale the value before applying Softmax function during\ntraining (a common practice in recent work [6, 17, 16]). We\nmultiply the cosine similarity by a learnable scalar œÑ, and\nthe probability prediction in training becomes:\np(y = c | x) =\nexp\n\u0000œÑ ¬∑ ‚ü®fŒ∏(x), wc‚ü©\n\u0001\nP\nc‚Ä≤ exp\n\u0000œÑ ¬∑ ‚ü®fŒ∏(x), wc‚Ä≤‚ü©\n\u0001.\n(3)\nIn this work, the main purpose of Meta-Baseline is to\ninvestigate whether the meta-learning objective is still ef-\nfective over a whole-classiÔ¨Åcation model.\nAs a method,\nwhile every component in Meta-Baseline has been proposed\nin prior works, we Ô¨Ånd none of the prior works studies them\nas a whole. Therefore, Meta-Baseline should also be an im-\nportant baseline that has been overlooked.\n4. Results on Standard Benchmarks\n4.1. Datasets\nThe miniImageNet dataset [29] is a common benchmark\nfor few-shot learning. It contains 100 classes sampled from\nILSVRC-2012 [21], which are then randomly split to 64,\n16, 20 classes as training, validation, and testing set respec-\ntively. Each class contains 600 images of size 84 √ó 84.\nThe tieredImageNet dataset [20] is another common\nbenchmark proposed more recently with much larger scale.\nIt is a subset of ILSVRC-2012, containing 608 classes from\n34 super-categories, which are then split into 20, 6, 8 super-\ncategories, resulting in 351, 97, 160 classes as training, val-\nidation, testing set respectively. The image size is 84 √ó 84.\nThis setting is more challenging since base classes and\nnovel classes come from different super-categories.\nIn addition to the datasets above, we evaluate our model\non ImageNet-800, which is derived from ILSVRC-2012 1K\nclasses by randomly splitting 800 classes as base classes\nand 200 classes as novel classes. The base classes contain\nthe images from the original training set, the novel classes\ncontain the images from the original validation set. This\nlarger dataset aims at making the training setting standard\nas the ImageNet 1K classiÔ¨Åcation task [8].\n4.2. Implementation details\nWe use ResNet-12 that follows the most of recent\nworks [16, 25, 11, 33] on miniImageNet and tieredIma-\ngeNet, and we use ResNet-18, ResNet-50 [8] on ImageNet-\n800. For the classiÔ¨Åcation training stage, we use the SGD\noptimizer with momentum 0.9, the learning rate starts from\n0.1 and the decay factor is 0.1. On miniImageNet, we train\n100 epochs with batch size 128 on 4 GPUs, the learning\nrate decays at epoch 90. On tieredImageNet, we train 120\nepochs with batch size 512 on 4 GPUs, the learning rate de-\n4\n\n\nModel\nBackbone\n1-shot\n5-shot\nMatching Networks [29]\nConvNet-4\n43.56 ¬± 0.84\n55.31 ¬± 0.73\nPrototypical Networks [24]\nConvNet-4\n48.70 ¬± 1.84\n63.11 ¬± 0.92\nPrototypical Networks (re-implement)\nResNet-12\n53.81 ¬± 0.23\n75.68 ¬± 0.17\nActivation to Parameter [18]\nWRN-28-10\n59.60 ¬± 0.41\n73.74 ¬± 0.19\nLEO [22]\nWRN-28-10\n61.76 ¬± 0.08\n77.59 ¬± 0.12\nBaseline++ [1]\nResNet-18\n51.87 ¬± 0.77\n75.68 ¬± 0.63\nSNAIL [13]\nResNet-12\n55.71 ¬± 0.99\n68.88 ¬± 0.92\nAdaResNet [15]\nResNet-12\n56.88 ¬± 0.62\n71.94 ¬± 0.57\nTADAM [16]\nResNet-12\n58.50 ¬± 0.30\n76.70 ¬± 0.30\nMTL [25]\nResNet-12\n61.20 ¬± 1.80\n75.50 ¬± 0.80\nMetaOptNet [11]\nResNet-12\n62.64 ¬± 0.61\n78.63 ¬± 0.46\nSLA-AG [10]\nResNet-12\n62.93 ¬± 0.63\n79.63 ¬± 0.47\nProtoNets + TRAML [12]\nResNet-12\n60.31 ¬± 0.48\n77.94 ¬± 0.57\nConstellationNet [33]\nResNet-12\n64.89 ¬± 0.23\n79.95 ¬± 0.17\nClassiÔ¨Åer-Baseline (ours)\nResNet-12\n58.91 ¬± 0.23\n77.76 ¬± 0.17\nMeta-Baseline (ours)\nResNet-12\n63.17 ¬± 0.23\n79.26 ¬± 0.17\nTable 2: Comparison to prior works on miniImageNet. Average 5-way accuracy (%) with 95% conÔ¨Ådence interval.\nModel\nBackbone\n1-shot\n5-shot\nMAML [5]\nConvNet-4\n51.67 ¬± 1.81\n70.30 ¬± 1.75\nPrototypical Networks* [24]\nConvNet-4\n53.31 ¬± 0.89\n72.69 ¬± 0.74\nRelation Networks* [26]\nConvNet-4\n54.48 ¬± 0.93\n71.32 ¬± 0.78\nLEO [22]\nWRN-28-10\n66.33 ¬± 0.05\n81.44 ¬± 0.09\nMetaOptNet [11]\nResNet-12\n65.99 ¬± 0.72\n81.56 ¬± 0.53\nClassiÔ¨Åer-Baseline (ours)\nResNet-12\n68.07 ¬± 0.26\n83.74 ¬± 0.18\nMeta-Baseline (ours)\nResNet-12\n68.62 ¬± 0.27\n83.74 ¬± 0.18\nTable 3: Comparison to prior works on tieredImageNet. Average 5-way accuracy (%) with 95% conÔ¨Ådence interval.\ncays at epoch 40 and 80. On ImageNet-800, we train 90\nepochs with batch size 256 on 8 GPUs, the learning rate de-\ncays at epoch 30 and 60. The weight decay is 0.0005 for\nResNet-12 and 0.0001 for ResNet-18 or ResNet-50. Stan-\ndard data augmentation is applied, including random re-\nsized crop and horizontal Ô¨Çip. For meta-learning stage, we\nuse the SGD optimizer with momentum 0.9. The learning\nrate is Ô¨Åxed as 0.001. The batch size is 4, i.e. each training\nbatch contains 4 few-shot tasks to compute the average loss.\nThe cosine scaling parameter œÑ is initialized as 10.\nWe also apply consistent sampling for evaluating the per-\nformance. For the novel class split in a dataset, the sampling\nof testing few-shot tasks follows a deterministic order. Con-\nsistent sampling allows us to get a better model comparison\nwith the same number of sampled tasks. In the following\nsections, when the conÔ¨Ådence interval is omitted in the ta-\nble, it indicates that a Ô¨Åxed set of 800 testing tasks are sam-\npled for estimating the performance.\n4.3. Results\nFollowing the standard-setting, we conduct experiments\non miniImageNet and tieredImageNet, the results are shown\nin Table 2 and 3 respectively. To get a fair comparison to\nprior works, we perform model selection according to the\nvalidation set. On both datasets, we observe that the Meta-\nBaseline achieves competitive performance to state-of-the-\nart methods. We highlight that many methods for compar-\nison introduce more parameters and architecture designs\n(e.g. self-attention in [33]), while Meta-Baseline has the\nminimum parameters and the simplest design. We also no-\ntice that the simple ClassiÔ¨Åer-Baseline can achieve compet-\nitive performance when compared to meta-learning meth-\nods, especially in 5-shot tasks. We observe that the meta-\nlearning stage consistently improves ClassiÔ¨Åer-Baseline on\n5\n\n\nModel\nBackbone\n1-shot\n5-shot\nClassiÔ¨Åer-Baseline (ours)\nResNet-18\n83.51 ¬± 0.22\n94.82 ¬± 0.10\nMeta-Baseline (ours)\nResNet-18\n86.39 ¬± 0.22\n94.82 ¬± 0.10\nClassiÔ¨Åer-Baseline (ours)\nResNet-50\n86.07 ¬± 0.21\n96.14 ¬± 0.08\nMeta-Baseline (ours)\nResNet-50\n89.70 ¬± 0.19\n96.14 ¬± 0.08\nTable 4: Results on ImageNet-800. Average 5-way accuracy (%) is reported with 95% conÔ¨Ådence interval.\n1\n15\n30\nepochs\n82.0\n84.0\n86.0\n88.0\n90.0\n5-way acc (%)\nminiImageNet, 1-shot\n1\n15\n30\nepochs\n92.4\n93.0\n93.6\n94.2\n94.8\nminiImageNet, 5-shot\n1\n15\n30\nepochs\n86.4\n87.0\n87.6\n88.2\n88.8\ntieredImageNet, 1-shot\n1\n15\n30\nepochs\n93.6\n94.0\n94.4\n94.8\ntieredImageNet, 5-shot\n61.5\n62.0\n62.5\n63.0\n63.5\n78.9\n79.2\n79.5\n79.8\n80.1\n66.5\n67.2\n67.9\n68.6\n81.0\n81.5\n82.0\n82.5\n83.0\nMeta-Baseline, Meta-Learning Stage (ResNet-12)\nbase class generalization\nnovel class generalization\nFigure 2: Objective discrepancy of meta-learning on miniImageNet and tieredImageNet. Each epoch contains 200\ntraining batches. Average 5-way accuracy (%) is reported.\n1\n30\n60\n90\nepochs\n94.8\n95.2\n95.6\n96.0\n5-way acc (%)\nResNet-50, 1-shot\n1\n30\n60\n90\nepochs\n98.1\n98.2\n98.3\n98.4\n98.5\nResNet-50, 5-shot\n87.5\n88.2\n88.9\n89.6\n94.8\n95.1\n95.4\n95.7\n96.0\nbase class generalization\nnovel class generalization\nFigure 3: Objective discrepancy of meta-learning on\nImageNet-800. Each epoch contains 500 training batches.\nAverage 5-way accuracy (%) is reported.\nminiImageNet. Compared to miniImageNet, we Ô¨Ånd that\nthe gap between Meta-Baseline and ClassiÔ¨Åer-Baseline is\nsmaller on tieredImageNet, and the meta-learning stage\ndoes not improve 5-shot in this case.\nWe further evaluate our methods on the larger dataset\nImageNet-800.\nIn this larger-scale experiment, we Ô¨Ånd\nfreezing the Batch Normalization layer [9] (set to eval\nmode) is beneÔ¨Åcial.\nThe results are shown in Table 4.\nFrom the results, we observe that in this large dataset Meta-\nBaseline improves ClassiÔ¨Åer-Baseline in 1-shot, while it is\nnot improving the performance in 5-shot.\n5. Observations and Hypothesis\n5.1. Objective discrepancy in meta-learning\nDespite\nthe\nimprovements\nof\nmeta-learning\nover\nClassiÔ¨Åer-Baseline, we observe the test performance drops\nduring the meta-learning stage. While a common assump-\ntion for this phenomenon is overÔ¨Åtting, we observe that this\nissue seems not to be mitigated on larger datasets. To further\nlocate the issue, we propose to evaluate base class general-\nization and novel class generalization. Base class general-\nization is measured by sampling tasks from unseen images\nin base classes, while novel class generalization refers to the\nperformance of few-shot tasks sampled from novel classes.\nThe base class generalization is the generalization in the in-\nput distribution for which the model is trained, it decouples\nthe commonly deÔ¨Åned generalization and class-level trans-\nfer performance, which helps for locating the reason for the\nperformance drop.\nFigure 2 and 3 demonstrate the meta-learning stage of\nMeta-Baseline on different datasets. We Ô¨Ånd that during the\nmeta-learning stage, when the base class generalization is\nincreasing, the novel class generalization can be decreasing\ninstead. This fact indicates that over a converged whole-\nclassiÔ¨Åcation model, the meta-learning objective itself, i.e.\nmaking the embedding generalize better in few-shot tasks\nfrom base classes, can have a negative effect on the perfor-\nmance of few-shot tasks from novel classes. It also gives a\n6\n\n\nTask\nModel\nmini-tiered\nmini-shufÔ¨Çed\nfull-tiered\nfull-shufÔ¨Çed\n1-shot\nClassiÔ¨Åer-Baseline\n56.91\n61.64\n68.76\n77.67\nMeta-Baseline\n58.44\n65.88\n69.52\n80.48\n‚àÜ\n+1.53\n+4.24\n+0.76\n+2.81\n5-shot\nClassiÔ¨Åer-Baseline\n74.30\n79.26\n84.07\n90.58\nMeta-Baseline\n74.63\n80.58\n84.07\n90.67\n‚àÜ\n+0.33\n+1.32\n+0.00\n+0.09\nTable 5: Effect of dataset properties. Average 5-way accuracy (%), with ResNet-12.\nTraining\nBase gen.\nNovel gen.\n1-shot\nw/ ClsTr\n86.42\n63.33\nw/o ClsTr\n86.74\n58.54\n5-shot\nw/ ClsTr\n93.54\n80.02\nw/o ClsTr\n94.47\n74.95\nTable 6: Comparison on Meta-Baseline training from\nscratch. Average 5-way accuracy (%), with ResNet-12 on\nminiImageNet. ClsTr: classiÔ¨Åcation training stage.\nMethod\n1-shot\n5-shot\nClassiÔ¨Åer-Baseline\n60.58\n79.24\nClassiÔ¨Åer-Baseline (Euc.)\n56.29\n78.93\nMeta-Baseline\n63.33\n80.02\nMeta-Baseline (Euc.)\n60.19\n79.50\nTable 7: Importance of inheriting a good metric. Aver-\nage 5-way accuracy (%), with ResNet-12 on miniImageNet.\npossible explanation for why such phenomenon is not mit-\nigated on larger datasets, as this is not sample-level over-\nÔ¨Åtting, but class-level overÔ¨Åtting, which is caused by the\nobjective discrepancy that the underlying training class dis-\ntribution is different from testing class distribution.\nThis observation suggests that we may reconsider the\nmotivation of the meta-learning framework for few-shot\nlearning. In some settings, optimizing towards the train-\ning objective with a consistent form as the testing objective\n(except the inevitable class difference) may have an even\nnegative effect. It is also likely that the whole-classiÔ¨Åcation\nlearns the embedding with stronger class transferability, and\nmeta-learning makes the model perform better at N-way\nK-shot tasks but tends to lose the class transferability.\n5.2. Effect of whole-classiÔ¨Åcation training before\nmeta-learning\nAccording to our hypothesis, the whole-classiÔ¨Åcation\npre-trained model has provided extra class transferabil-\nity for the meta-learning model, therefore, it is natural to\ncompare Meta-Baseline with and without the classiÔ¨Åcation\ntraining stage. The results are shown in Table 6. We observe\nthat Meta-Baseline trained without classiÔ¨Åcation training\nstage can actually achieve higher base class generalization,\nbut its novel class generalization is much lower when com-\npared to Meta-Baseline with whole-classiÔ¨Åcation training.\nThese results support our hypothesis, that the whole-\nclassiÔ¨Åcation training provides the embedding with stronger\nclass transferability, which signiÔ¨Åcantly helps novel class\ngeneralization.\nInterestingly, TADAM [16] Ô¨Ånds that\nco-training the meta-learning objective with a whole-\nclassiÔ¨Åcation task is beneÔ¨Åcial, which may be potentially\nrelated to our hypothesis. While our results show it is likely\nthat the key effect of the whole-classiÔ¨Åcation objective is\nimproving the class transferability, it also indicates a po-\ntential trade-off that the whole-classiÔ¨Åcation objective can\nhave a negative effect on base class generalization.\n5.3. What makes Meta-Baseline a strong baseline?\nAs a method with a similar objective as ProtoNet [24],\nMeta-Baseline achieves nearly 10% higher accuracy on 1-\nshot in Table 2. The observations and hypothesis in previ-\nous sections potentially explain its strength, as it starts with\nthe embedding of a whole-classiÔ¨Åcation model which has\nstronger class transferability.\nWe perform further experiments, that in Meta-Baseline\n(with classiÔ¨Åcation training stage) we replace the cosine\ndistance with the squared Euclidean distance proposed in\nProtoNet [24]. To get a fair comparison, we also include\nthe learnable scalar œÑ with a proper initialization value 0.1.\nThe results are shown in Table 7.\nWhile ProtoNet [24]\nÔ¨Ånds that squared Euclidean distance (as a Bregman di-\nvergence) works better than cosine distance when perform-\ning meta-learning from scratch, here we start meta-learning\nfrom ClassiÔ¨Åer-Baseline and we observe that cosine sim-\nilarity works much better. A potential reason is that, as\nshown in Table 7, cosine nearest-centroid works much bet-\nter than nearest-centroid with squared Euclidean distance in\nClassiÔ¨Åer-Baseline (note that this is just the evaluation met-\nric and has no changes in training). Inheriting a good metric\n7\n\n\nfor ClassiÔ¨Åer-Baseline might be the key that makes Meta-\nBaseline strong. According to our hypothesis, the embed-\nding from the whole-classiÔ¨Åcation model has strong class\ntransferability, inheriting a good metric potentially mini-\nmizes the future modiÔ¨Åcations on the embedding from the\nwhole-classiÔ¨Åcation model, thus it can keep the class trans-\nferability better and achieve higher performance.\n5.4. Effect of dataset properties\nWe construct four variants from the tieredImageNet\ndataset. SpeciÔ¨Åcally, full-tiered refers to the original tiered-\nImageNet, full-shufÔ¨Çed is constructed by randomly shuf-\nÔ¨Çing the classes in tieredImageNet and re-splitting the\nclasses into training, validation, and test set.\nThe mini-\ntiered and mini-shufÔ¨Çed datasets are constructed from full-\ntiered and full-shufÔ¨Çed respectively, their training set is con-\nstructed by randomly selecting 64 classes with 600 images\nfrom each class in the full training set, while the validation\nset and the test set remain unchanged. Since tieredImageNet\nseparates training classes and testing classes into different\nsuper categories, shufÔ¨Çing these classes will mix the classes\nin different super categories together and make the distribu-\ntion of base classes and novel classes closer.\nOur previous experiments show that base class general-\nization is always improving, if novel classes are covered\nby the distribution of base classes, the novel class gener-\nalization should also keep increasing. From Table 5, we\ncan see that from mini-tiered to mini-shufÔ¨Çed, and from\nfull-tiered to full-shufÔ¨Çed, the improvement achieved by the\nmeta-learning stage gets signiÔ¨Åcantly larger, which consis-\ntently supports our hypothesis. Therefore, our results in-\ndicate it is likely that meta-learning is mostly effective over\nwhole-classiÔ¨Åcation training when novel classes are similar\nto base classes.\nWe also observe that other factors may affect the im-\nprovement of meta-learning. From mini-tiered to full-tiered\nand from mini-shufÔ¨Çed to full-shufÔ¨Çed, when the dataset\ngets larger the improvements become less. A potential hy-\npothesis could be that the class transferability advantage of\nwhole-classiÔ¨Åcation training becomes more obvious when\ntrained on large datasets. From the results of our experi-\nments in Table 2, 3, 4, 5, we observe that the improvement\nof the meta-learning stage in 5-shot is less than 1-shot. We\nhypothesize this is because when there are more shots, tak-\ning average embedding becomes a more reasonable choice\nto estimate the class center in ClassiÔ¨Åer-Baseline, therefore\nthe advantage of meta-learning becomes less.\n5.5. The trade-off between meta-learning and\nwhole-classiÔ¨Åcation\nAll of the experiments in previous sections support a key\nhypothesis, that there exists a trade-off: the meta-learning\nobjective learns better embedding for N-way K-shot tasks\n(in the same distribution), while the whole-classiÔ¨Åcation ob-\njective learns embedding with stronger class transferabil-\nity. Optimizing towards one objective may hurt the strength\nof another objective. With this hypothesis, Meta-Baseline\nbalances this trade-off by choosing to calibrate the whole-\nclassiÔ¨Åcation embedding with meta-learning and inherit the\nmetric with high initial performance.\nThe discrepancy between base class generalization and\nnovel class generalization also considers the effectiveness\nof meta-learning and whole-classiÔ¨Åcation from the perspec-\ntive of datasets. SpeciÔ¨Åcally, when novel classes are similar\nenough to base classes or the base classes are sufÔ¨Åcient to\ncover the distribution of novel classes, novel class gener-\nalization should converge to base class generalization. In\npractice, this can be potentially achieved by collecting base\nclasses that are similar to the target novel classes. In this\ncase, it may be possible that the novel meta-learning algo-\nrithms outperform the whole-classiÔ¨Åcation baselines again.\n6. Additional Results on Meta-Dataset\nMeta-Dataset [28] is a new benchmark proposed for few-\nshot learning, it consists of diverse datasets for training and\nevaluation. They also propose to generate few-shot tasks\nwith a variable number of ways and shots, for having a\nsetting closer to the real world. We follow the setting in\nMeta-Dataset [28] and use ResNet-18 as the backbone, with\nthe original image size of 126√ó126, which is resized to be\n128√ó128 before feeding into the network. For the classiÔ¨Å-\ncation training stage, we apply the training setting similar to\nour setting in ImageNet-800. For the meta-learning stage,\nthe model is trained for 5000 iterations with one task in each\niteration.\nThe left side of Table 8 demonstrates the models trained\nwith samples in ILSVRC-2012 only. We observe that the\nMeta-Baseline does not signiÔ¨Åcantly improve ClassiÔ¨Åer-\nBaseline under this setting in our experiments, possibly due\nto the average number of shots are high.\nThe right side of Table 8 shows the results when the\nmodels are trained on all datasets, except TrafÔ¨Åc Signs and\nMSCOCO which have no training samples. The ClassiÔ¨Åer-\nBaseline is trained as a multi-dataset classiÔ¨Åer, i.e. an en-\ncoder together with multiple FC layers over the encoded\nfeature to output the logits for different datasets. The clas-\nsiÔ¨Åcation training stage has the same number of iterations\nas training on ILSVRC only, to mimic the ILSVRC train-\ning, a batch has 0.5 probability to be from ILSVRC and 0.5\nprobability to be uniformly sampled from one of the other\ndatasets. For ClassiÔ¨Åer-Baseline, comparing to the results\non the left side of Table 8, we observe that while the per-\nformance on ILSVRC is worse, the performances on other\ndatasets are mostly improved due to having their samples in\ntraining. It can be also noticed that the cases where Meta-\nBaseline improves ClassiÔ¨Åer-Baseline are mostly on the\n8\n\n\nTrained on ILSVRC\nTrained on all datasets\nDataset\nfo-Proto-MAML\nClassiÔ¨Åer/Meta\nfo-Proto-MAML\nClassiÔ¨Åer\nMeta\n[28]\n(ours)\n[28]\n(ours)\n(ours)\nILSVRC\n49.5\n59.2\n46.5\n55.0\n48.0\nOmniglot\n63.4\n69.1\n82.7\n76.9\n89.4\nAircraft\n56.0\n54.1\n75.2\n69.8\n81.7\nBirds\n68.7\n77.3\n69.9\n78.3\n77.3\nTextures\n66.5\n76.0\n68.3\n71.4\n64.5\nQuick Draw\n51.5\n57.3\n66.8\n62.7\n74.5\nFungi\n40.0\n45.4\n42.0\n55.4\n60.2\nVGG Flower\n87.2\n89.6\n88.7\n90.6\n83.8\nTrafÔ¨Åc Signs\n48.8\n66.2\n52.4\n69.3\n59.5\nMSCOCO\n43.7\n55.7\n41.7\n53.1\n43.6\nTable 8: Additional results on Meta-Dataset. Average accuracy (%), with variable number of ways and shots. The fo-Proto-\nMAML method is from Meta-Dataset [28], ClassiÔ¨Åer and Meta refers to ClassiÔ¨Åer-Baseline and Meta-Baseline respectively,\n1000 tasks are sampled for evaluating ClassiÔ¨Åer or Meta. Note that TrafÔ¨Åc Signs and MSCOCO have no training set.\ndatasets which are ‚Äúless relevant‚Äù to ILSVRC (the dataset\n‚Äúrelevance‚Äù could be shown in Dvornik et al. [3]). A po-\ntential reason is that the multi-dataset classiÔ¨Åcation train-\ning stage samples ILSVRC with 0.5 probability, similar\nto ILSVRC training, the meta-learning stage is hard to\nimprove on ILSVRC, therefore those datasets relevant to\nILSVRC will have similar properties so that it is hard to\nimprove on them.\n7. Conclusion and Discussion\nIn this work, we presented a simple Meta-Baseline that\nhas been overlooked for few-shot learning. Without any ad-\nditional parameters or complex design choices, it is compet-\nitive to state-of-the-art methods on standard benchmarks.\nOur experiments indicate that there might be an objec-\ntive discrepancy in the meta-learning framework for few-\nshot learning, i.e. a meta-learning model generalizing better\non unseen tasks from base classes might have worse perfor-\nmance on tasks from novel classes. This provides a possible\nexplanation that why some complex meta-learning methods\ncould not get signiÔ¨Åcantly better performance than simple\nwhole-classiÔ¨Åcation. While most recent works focus on im-\nproving the meta-learning structures, many of them did not\nexplicitly address the issue of class transferability. Our ob-\nservations suggest that the objective discrepancy might be a\npotential key challenge to tackle.\nWhile many novel meta-learning algorithms are pro-\nposed and some recent works report that simple whole-\nclassiÔ¨Åcation training is good enough for few-shot learn-\ning, we show that meta-learning is still effective over\nwhole-classiÔ¨Åcation models. We observe a potential trade-\noff between the objectives of meta-learning and whole-\nclassiÔ¨Åcation.\nFrom the perspective of datasets, we\ndemonstrate how the preference between meta-learning and\nwhole-classiÔ¨Åcation changes according to class similarity\nand other factors, indicating that these factors may need\nmore attention for model comparisons in future work.\nAcknowledgements. This work was supported, in part, by grants from\nDARPA LwLL, NSF 1730158 CI-New: Cognitive Hardware and Software\nEcosystem Community Infrastructure (CHASE-CI), NSF ACI-1541349\nCC*DNI PaciÔ¨Åc Research Platform, and gifts from Qualcomm, TuSim-\nple and Picsart. Prof. Darrell was supported, in part, by DoD including\nDARPA‚Äôs XAI, LwLL, and/or SemaFor programs, as well as BAIR‚Äôs in-\ndustrial alliance programs. We thank Hang Gao for the helpful discussions.\nReferences\n[1] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank\nWang, and Jia-Bin Huang. A closer look at few-shot classi-\nÔ¨Åcation. In International Conference on Learning Represen-\ntations, 2019. 1, 2, 3, 5, 11, 12\n[2] Guneet S Dhillon, Pratik Chaudhari, Avinash Ravichandran,\nand Stefano Soatto. A baseline for few-shot image classiÔ¨Å-\ncation. arXiv preprint arXiv:1909.02729, 2019. 2\n[3] Nikita Dvornik, Cordelia Schmid, and Julien Mairal. Select-\ning relevant features from a universal representation for few-\nshot classiÔ¨Åcation. arXiv preprint arXiv:2003.09338, 2020.\n9\n[4] Li Fei-Fei, Rob Fergus, and Pietro Perona. One-shot learning\nof object categories. IEEE transactions on pattern analysis\nand machine intelligence, 28(4):594‚Äì611, 2006. 1\n[5] Chelsea Finn, Pieter Abbeel, and Sergey Levine.\nModel-\nagnostic meta-learning for fast adaptation of deep networks.\nIn Proceedings of the 34th International Conference on Ma-\nchine Learning-Volume 70, pages 1126‚Äì1135. JMLR. org,\n2017. 1, 2, 5\n9\n\n\n[6] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot\nvisual learning without forgetting.\nIn Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 4367‚Äì4375, 2018. 1, 2, 3, 4, 11\n[7] Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and\nThomas GrifÔ¨Åths. Recasting gradient-based meta-learning as\nhierarchical bayes. arXiv preprint arXiv:1801.08930, 2018.\n2\n[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770‚Äì778, 2016. 4\n[9] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. arXiv preprint arXiv:1502.03167, 2015. 6, 11\n[10] Hankook Lee, Sung Ju Hwang, and Jinwoo Shin.\nSelf-\nsupervised label augmentation via input transformations.\nIn International Conference on Machine Learning, pages\n5714‚Äì5724. PMLR, 2020. 5\n[11] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and\nStefano Soatto. Meta-learning with differentiable convex op-\ntimization. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 10657‚Äì10665,\n2019. 1, 2, 4, 5, 11\n[12] Aoxue Li, Weiran Huang, Xu Lan, Jiashi Feng, Zhenguo Li,\nand Liwei Wang. Boosting few-shot learning with adaptive\nmargin loss. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 12576‚Äì\n12584, 2020. 5\n[13] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter\nAbbeel. A simple neural attentive meta-learner. In Inter-\nnational Conference on Learning Representations, 2018. 2,\n5\n[14] Tsendsuren Munkhdalai and Hong Yu. Meta networks. In\nProceedings of the 34th International Conference on Ma-\nchine Learning-Volume 70, pages 2554‚Äì2563. JMLR. org,\n2017. 2\n[15] Tsendsuren Munkhdalai, Xingdi Yuan, Soroush Mehri, and\nAdam Trischler. Rapid adaptation with conditionally shifted\nneurons. arXiv preprint arXiv:1712.09926, 2017. 2, 5\n[16] Boris Oreshkin, Pau Rodr¬¥ƒ±guez L¬¥opez, and Alexandre La-\ncoste. Tadam: Task dependent adaptive metric for improved\nfew-shot learning. In Advances in Neural Information Pro-\ncessing Systems, pages 721‚Äì731, 2018. 2, 4, 5, 7, 11\n[17] Hang Qi, Matthew Brown, and David G Lowe. Low-shot\nlearning with imprinted weights. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\npages 5822‚Äì5830, 2018. 4\n[18] Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan L Yuille. Few-\nshot image recognition by predicting parameters from activa-\ntions. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 7229‚Äì7238, 2018. 5\n[19] Sachin Ravi and Hugo Larochelle. Optimization as a model\nfor few-shot learning.\nIn In International Conference on\nLearning Representations (ICLR), 2017. 2\n[20] Mengye Ren, Eleni TriantaÔ¨Ållou, Sachin Ravi, Jake Snell,\nKevin Swersky, Joshua B Tenenbaum, Hugo Larochelle, and\nRichard S Zemel. Meta-learning for semi-supervised few-\nshot classiÔ¨Åcation. arXiv preprint arXiv:1803.00676, 2018.\n4\n[21] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al.\nImagenet large\nscale visual recognition challenge. International journal of\ncomputer vision, 115(3):211‚Äì252, 2015. 4\n[22] Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol\nVinyals, Razvan Pascanu, Simon Osindero, and Raia Had-\nsell.\nMeta-learning with latent embedding optimization.\nIn International Conference on Learning Representations,\n2019. 1, 2, 5\n[23] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan\nWierstra, and Timothy Lillicrap.\nMeta-learning with\nmemory-augmented neural networks. In International con-\nference on machine learning, pages 1842‚Äì1850, 2016. 2\n[24] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical\nnetworks for few-shot learning. In Advances in Neural Infor-\nmation Processing Systems, pages 4077‚Äì4087, 2017. 2, 3, 5,\n7\n[25] Qianru Sun, Yaoyao Liu, Tat-Seng Chua, and Bernt Schiele.\nMeta-transfer learning for few-shot learning.\nIn Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 403‚Äì412, 2019. 1, 2, 4, 5\n[26] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS\nTorr, and Timothy M Hospedales. Learning to compare: Re-\nlation network for few-shot learning. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 1199‚Äì1208, 2018. 1, 2, 5\n[27] Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenen-\nbaum, and Phillip Isola. Rethinking few-shot image classi-\nÔ¨Åcation: a good embedding is all you need? arXiv preprint\narXiv:2003.11539, 2020. 1, 2, 3, 11\n[28] Eleni TriantaÔ¨Ållou, Tyler Zhu, Vincent Dumoulin, Pascal\nLamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles\nGelada, Kevin Swersky, Pierre-Antoine Manzagol, et al.\nMeta-dataset: A dataset of datasets for learning to learn from\nfew examples. arXiv preprint arXiv:1903.03096, 2019. 8, 9\n[29] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan\nWierstra, et al. Matching networks for one shot learning. In\nAdvances in neural information processing systems, pages\n3630‚Äì3638, 2016. 1, 2, 3, 4, 5\n[30] Xin Wang, Fisher Yu, Ruth Wang, Trevor Darrell, and\nJoseph E Gonzalez. Tafe-net: Task-aware feature embed-\ndings for low shot learning. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n1831‚Äì1840, 2019. 1\n[31] Yan Wang, Wei-Lun Chao, Kilian Q Weinberger, and Lau-\nrens van der Maaten.\nSimpleshot:\nRevisiting nearest-\nneighbor classiÔ¨Åcation for few-shot learning. arXiv preprint\narXiv:1911.04623, 2019. 1, 2, 3\n[32] Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical\nevaluation of rectiÔ¨Åed activations in convolutional network.\narXiv preprint arXiv:1505.00853, 2015. 11\n[33] Weijian Xu, yifan xu, Huaijin Wang, and Zhuowen Tu. Con-\nstellation nets for few-shot learning. In International Con-\nference on Learning Representations, 2021. 1, 2, 4, 5, 11\n10\n\n\n1\n40\n80\n120\n160\nepochs\n40\n60\n80\n5-way acc (%)\nminiImageNet, 1-shot\n1\n40\n80\n120\n160\nepochs\n50\n60\n70\n80\n90\n5-way acc (%)\nminiImageNet, 5-shot\nMeta-Baseline, Training from Scratch\nbase class generalization\nnovel class generalization\nFigure 4: Training Meta-Baseline without classiÔ¨Åcation-\ntraining stage on miniImageNet.\nDataset\nClassiÔ¨Åer\n1-shot\n5-shot\nminiImageNet\nLinear\n60.58\n79.24\nCosine\n61.93\n78.73\ntieredImageNet\nLinear\n68.76\n84.07\nCosine\n67.58\n83.31\nTable 9: Comparison to classiÔ¨Åer trained with cosine met-\nric, Average 5-way accuracy (%), with ResNet-12.\nA. Details of ResNet-12\nThe ResNet-12 backbone consists of 4 residual blocks\nthat each residual block has 3 convolutional layers. Each\nconvolutional layer has a 3 √ó 3 kernel, followed by Batch\nNormalization [9] and Leaky ReLU [32] with 0.1 slope.\nThe channels of convolutional layers in each residual block\nare 64, 128, 256, 512 respectively, a 2√ó2 max-pooling layer\nis applied after each residual block. Finally, a 5 √ó 5 global\naverage pooling is applied to get a 512-dimensional feature\nvector.\nThis architecture is consistent with recent works [16, 33].\nSome other recent works also introduce additional parame-\nters and design choices in the backbone (e.g. DropBlock\nand wider channels of 64, 160, 320, 640 in [11, 27]), while\nthese modiÔ¨Åcations may make the performance higher, we\ndo not include them here for simplicity.\nB. Training plot of Meta-Baseline without clas-\nsiÔ¨Åcation training stage\nWe show the process of training Meta-Baseline from\nscratch (i.e.\nwithout the classiÔ¨Åcation-training stage) on\nminiImageNet in Figure 4. We observe that when the learn-\ning rate decays, the novel class generalization quickly starts\nto be decreasing. While it is able to achieve higher base\nclass generalization than Meta-Baseline with classiÔ¨Åcation\ntraining, its highest novel class generalization is still much\nworse, suggesting whole-classiÔ¨Åcation training may pro-\nvide representations with extra class transferability.\nC. Comparison to cosine classiÔ¨Åcation training\nWe compare the effect of classiÔ¨Åcation training with\nreplacing the last linear-classiÔ¨Åer with cosine nearest-\nneighbor metric which is proposed in prior work [6, 1], the\nresults are shown in Table 9, where Cosine denotes clas-\nsiÔ¨Åcation training with cosine metric and Linear denotes\nthe standard classiÔ¨Åcation training. On miniImageNet, we\nobserve that Cosine outperforms Linear in 1-shot, but has\nworse performance in 5-shot. On tieredImageNet, we ob-\nserve Linear outperforms Cosine in both 1-shot and 5-shot.\nWe choose to use the linear layer as it is more common and\nwe Ô¨Ånd it works better in more cases.\nD. Objective discrepancy on ImageNet-800\nBesides miniImageNet and tieredImageNet, in our large-\nscale dataset ImageNet-800, we also observe the novel class\ngeneralization decreasing when base class generalization\nis increasing, the training process is demonstrated in Fig-\nure 5.\nFrom the Ô¨Ågure, we see that for both backbones\nof ResNet-18 and ResNet-50, the base class generalization\nperformance is increasing during the training epochs, while\nthe novel class generalization performance quickly starts to\nbe decreasing. These observations are consistent with our\nobservations on miniImageNet and tieredImageNet, which\nfurther support our hypothesis.\nE. Comparison of the ClassiÔ¨Åer-Baseline and\nBaseline++ [1]\nWe connect the ClassiÔ¨Åer-Baseline to Baseline++ [1]\nwith a step-by-step ablation study on miniImageNet, the\nresults are shown in Table 10. We see that Ô¨Åne-tuning is\noutperformed by the simple nearest-centroid method with\ncosine metric, and using a standard ImageNet-like opti-\nmizer signiÔ¨Åcantly improves the performance of the whole-\nclassiÔ¨Åcation method for few-shot learning.\n11\n\n\n1\n30\n60\n90\nepochs\n90.0\n90.9\n91.8\n92.7\n5-way acc (%)\nResNet-18, 1-shot\n1\n30\n60\n90\nepochs\n96.3\n96.6\n96.9\n97.2\n97.5\nResNet-18, 5-shot\n1\n30\n60\n90\nepochs\n94.8\n95.2\n95.6\n96.0\nResNet-50, 1-shot\n1\n30\n60\n90\nepochs\n98.1\n98.2\n98.3\n98.4\n98.5\nResNet-50, 5-shot\n85.2\n85.6\n86.0\n86.4\n93.6\n93.8\n94.0\n94.2\n87.5\n88.2\n88.9\n89.6\n94.8\n95.1\n95.4\n95.7\n96.0\nMeta-Baseline, Meta-Learning Stage (ImageNet-800)\nbase class generalization\nnovel class generalization\nFigure 5: Objective discrepancy of meta-learning on ImageNet-800. Each epoch contains 500 training batches. Average\n5-way accuracy (%) is reported.\ndifference\n5-way 1-shot accuracy (%)\n(default, reported [1])\n51.75 ¬± 0.80\n(default, reproduced by their code)\n50.84 ¬± 0.80\nÔ¨Ånetune ‚Üícosine nearest-centroid\n52.15 ¬± 0.83\nepoch-300 ‚Üíepoch-50\n53.37 ¬± 0.71\nremove color-jittering\n56.06 ¬± 0.71\n224√ó224 input size ‚Üíresizing 84√ó84 to 224√ó224\n50.49 ¬± 0.71\nResNet-18 ‚ÜíResNet-12\n53.59 ¬± 0.72\nAdam (lr=0.001, batch=16) ‚ÜíSGD (lr=0.1, batch=128)\n59.19 ¬± 0.71\nTable 10: Comparison of the ClassiÔ¨Åer-Baseline and Baseline++ [1].\n12\n"
}