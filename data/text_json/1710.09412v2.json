{
  "filename": "1710.09412v2.pdf",
  "num_pages": 13,
  "pages": [
    "Published as a conference paper at ICLR 2018\nmixup: BEYOND EMPIRICAL RISK MINIMIZATION\nHongyi Zhang\nMIT\nMoustapha Cisse, Yann N. Dauphin, David Lopez-Paz∗\nFAIR\nABSTRACT\nLarge deep neural networks are powerful, but exhibit undesirable behaviors such\nas memorization and sensitivity to adversarial examples. In this work, we propose\nmixup, a simple learning principle to alleviate these issues. In essence, mixup trains\na neural network on convex combinations of pairs of examples and their labels.\nBy doing so, mixup regularizes the neural network to favor simple linear behavior\nin-between training examples. Our experiments on the ImageNet-2012, CIFAR-10,\nCIFAR-100, Google commands and UCI datasets show that mixup improves the\ngeneralization of state-of-the-art neural network architectures. We also ﬁnd that\nmixup reduces the memorization of corrupt labels, increases the robustness to\nadversarial examples, and stabilizes the training of generative adversarial networks.\n1\nINTRODUCTION\nLarge deep neural networks have enabled breakthroughs in ﬁelds such as computer vision (Krizhevsky\net al., 2012), speech recognition (Hinton et al., 2012), and reinforcement learning (Silver et al., 2016).\nIn most successful applications, these neural networks share two commonalities. First, they are\ntrained as to minimize their average error over the training data, a learning rule also known as the\nEmpirical Risk Minimization (ERM) principle (Vapnik, 1998). Second, the size of these state-of-the-\nart neural networks scales linearly with the number of training examples. For instance, the network of\nSpringenberg et al. (2015) used 106 parameters to model the 5 · 104 images in the CIFAR-10 dataset,\nthe network of (Simonyan & Zisserman, 2015) used 108 parameters to model the 106 images in the\nImageNet-2012 dataset, and the network of Chelba et al. (2013) used 2 · 1010 parameters to model\nthe 109 words in the One Billion Word dataset.\nStrikingly, a classical result in learning theory (Vapnik & Chervonenkis, 1971) tells us that the\nconvergence of ERM is guaranteed as long as the size of the learning machine (e.g., the neural\nnetwork) does not increase with the number of training data. Here, the size of a learning machine is\nmeasured in terms of its number of parameters or, relatedly, its VC-complexity (Harvey et al., 2017).\nThis contradiction challenges the suitability of ERM to train our current neural network models, as\nhighlighted in recent research. On the one hand, ERM allows large neural networks to memorize\n(instead of generalize from) the training data even in the presence of strong regularization, or in\nclassiﬁcation problems where the labels are assigned at random (Zhang et al., 2017). On the other\nhand, neural networks trained with ERM change their predictions drastically when evaluated on\nexamples just outside the training distribution (Szegedy et al., 2014), also known as adversarial\nexamples. This evidence suggests that ERM is unable to explain or provide generalization on testing\ndistributions that differ only slightly from the training data. However, what is the alternative to ERM?\nThe method of choice to train on similar but different examples to the training data is known as data\naugmentation (Simard et al., 1998), formalized by the Vicinal Risk Minimization (VRM) principle\n(Chapelle et al., 2000). In VRM, human knowledge is required to describe a vicinity or neighborhood\naround each example in the training data. Then, additional virtual examples can be drawn from the\nvicinity distribution of the training examples to enlarge the support of the training distribution. For\ninstance, when performing image classiﬁcation, it is common to deﬁne the vicinity of one image\nas the set of its horizontal reﬂections, slight rotations, and mild scalings. While data augmentation\nconsistently leads to improved generalization (Simard et al., 1998), the procedure is dataset-dependent,\nand thus requires the use of expert knowledge. Furthermore, data augmentation assumes that the\n∗Alphabetical order.\n1\narXiv:1710.09412v2  [cs.LG]  27 Apr 2018\n",
    "Published as a conference paper at ICLR 2018\nexamples in the vicinity share the same class, and does not model the vicinity relation across examples\nof different classes.\nContribution\nMotivated by these issues, we introduce a simple and data-agnostic data augmenta-\ntion routine, termed mixup (Section 2). In a nutshell, mixup constructs virtual training examples\n˜x = λxi + (1 −λ)xj,\nwhere xi, xj are raw input vectors\n˜y = λyi + (1 −λ)yj,\nwhere yi, yj are one-hot label encodings\n(xi, yi) and (xj, yj) are two examples drawn at random from our training data, and λ ∈[0, 1].\nTherefore, mixup extends the training distribution by incorporating the prior knowledge that linear\ninterpolations of feature vectors should lead to linear interpolations of the associated targets. mixup\ncan be implemented in a few lines of code, and introduces minimal computation overhead.\nDespite its simplicity, mixup allows a new state-of-the-art performance in the CIFAR-10, CIFAR-\n100, and ImageNet-2012 image classiﬁcation datasets (Sections 3.1 and 3.2). Furthermore, mixup\nincreases the robustness of neural networks when learning from corrupt labels (Section 3.4), or facing\nadversarial examples (Section 3.5). Finally, mixup improves generalization on speech (Sections 3.3)\nand tabular (Section 3.6) data, and can be used to stabilize the training of GANs (Section 3.7). The\nsource-code necessary to replicate our CIFAR-10 experiments is available at:\nhttps://github.com/facebookresearch/mixup-cifar10.\nTo understand the effects of various design choices in mixup, we conduct a thorough set of ablation\nstudy experiments (Section 3.8). The results suggest that mixup performs signiﬁcantly better than\nrelated methods in previous work, and each of the design choices contributes to the ﬁnal performance.\nWe conclude by exploring the connections to prior work (Section 4), as well as offering some points\nfor discussion (Section 5).\n2\nFROM EMPIRICAL RISK MINIMIZATION TO mixup\nIn supervised learning, we are interested in ﬁnding a function f ∈F that describes the relationship\nbetween a random feature vector X and a random target vector Y , which follow the joint distribution\nP(X, Y ). To this end, we ﬁrst deﬁne a loss function ℓthat penalizes the differences between\npredictions f(x) and actual targets y, for examples (x, y) ∼P. Then, we minimize the average of\nthe loss function ℓover the data distribution P, also known as the expected risk:\nR(f) =\nZ\nℓ(f(x), y)dP(x, y).\nUnfortunately, the distribution P is unknown in most practical situations. Instead, we usually have\naccess to a set of training data D = {(xi, yi)}n\ni=1, where (xi, yi) ∼P for all i = 1, . . . , n. Using\nthe training data D, we may approximate P by the empirical distribution\nPδ(x, y) = 1\nn\nn\nX\ni=1\nδ(x = xi, y = yi),\nwhere δ(x = xi, y = yi) is a Dirac mass centered at (xi, yi). Using the empirical distribution Pδ, we\ncan now approximate the expected risk by the empirical risk:\nRδ(f) =\nZ\nℓ(f(x), y)dPδ(x, y) = 1\nn\nn\nX\ni=1\nℓ(f(xi), yi).\n(1)\nLearning the function f by minimizing (1) is known as the Empirical Risk Minimization (ERM)\nprinciple (Vapnik, 1998). While efﬁcient to compute, the empirical risk (1) monitors the behaviour\nof f only at a ﬁnite set of n examples. When considering functions with a number parameters\ncomparable to n (such as large neural networks), one trivial way to minimize (1) is to memorize the\ntraining data (Zhang et al., 2017). Memorization, in turn, leads to the undesirable behaviour of f\noutside the training data (Szegedy et al., 2014).\n2\n",
    "Published as a conference paper at ICLR 2018\n# y1, y2 should be one-hot vectors\nfor (x1, y1), (x2, y2) in zip(loader1, loader2):\nlam = numpy.random.beta(alpha, alpha)\nx = Variable(lam * x1 + (1. - lam) * x2)\ny = Variable(lam * y1 + (1. - lam) * y2)\noptimizer.zero_grad()\nloss(net(x), y).backward()\noptimizer.step()\n(a) One epoch of mixup training in PyTorch.\nERM\nmixup\n(b) Effect of mixup (α = 1) on a\ntoy problem.\nGreen: Class 0.\nOr-\nange: Class 1. Blue shading indicates\np(y = 1|x).\nFigure 1: Illustration of mixup, which converges to ERM as α →0.\nHowever, the na¨ıve estimate Pδ is one out of many possible choices to approximate the true distribu-\ntion P. For instance, in the Vicinal Risk Minimization (VRM) principle (Chapelle et al., 2000), the\ndistribution P is approximated by\nPν(˜x, ˜y) = 1\nn\nn\nX\ni=1\nν(˜x, ˜y|xi, yi),\nwhere ν is a vicinity distribution that measures the probability of ﬁnding the virtual feature-target\npair (˜x, ˜y) in the vicinity of the training feature-target pair (xi, yi). In particular, Chapelle et al.\n(2000) considered Gaussian vicinities ν(˜x, ˜y|xi, yi) = N(˜x −xi, σ2)δ(˜y = yi), which is equivalent\nto augmenting the training data with additive Gaussian noise. To learn using VRM, we sample the\nvicinal distribution to construct a dataset Dν := {(˜xi, ˜yi)}m\ni=1, and minimize the empirical vicinal\nrisk:\nRν(f) = 1\nm\nm\nX\ni=1\nℓ(f(˜xi), ˜yi).\nThe contribution of this paper is to propose a generic vicinal distribution, called mixup:\nµ(˜x, ˜y|xi, yi) = 1\nn\nn\nX\nj\nE\nλ [δ(˜x = λ · xi + (1 −λ) · xj, ˜y = λ · yi + (1 −λ) · yj)] ,\nwhere λ ∼Beta(α, α), for α ∈(0, ∞). In a nutshell, sampling from the mixup vicinal distribution\nproduces virtual feature-target vectors\n˜x = λxi + (1 −λ)xj,\n˜y = λyi + (1 −λ)yj,\nwhere (xi, yi) and (xj, yj) are two feature-target vectors drawn at random from the training data, and\nλ ∈[0, 1]. The mixup hyper-parameter α controls the strength of interpolation between feature-target\npairs, recovering the ERM principle as α →0.\nThe implementation of mixup training is straightforward, and introduces a minimal computation\noverhead. Figure 1a shows the few lines of code necessary to implement mixup training in PyTorch.\nFinally, we mention alternative design choices. First, in preliminary experiments we ﬁnd that convex\ncombinations of three or more examples with weights sampled from a Dirichlet distribution does not\nprovide further gain, but increases the computation cost of mixup. Second, our current implementation\nuses a single data loader to obtain one minibatch, and then mixup is applied to the same minibatch\nafter random shufﬂing. We found this strategy works equally well, while reducing I/O requirements.\nThird, interpolating only between inputs with equal label did not lead to the performance gains of\nmixup discussed in the sequel. More empirical comparison can be found in Section 3.8.\nWhat is mixup doing?\nThe mixup vicinal distribution can be understood as a form of data aug-\nmentation that encourages the model f to behave linearly in-between training examples. We argue\nthat this linear behaviour reduces the amount of undesirable oscillations when predicting outside the\ntraining examples. Also, linearity is a good inductive bias from the perspective of Occam’s razor,\n3\n",
    "Published as a conference paper at ICLR 2018\n0.00\n0.25\n0.50\n0.75\n1.00\nλ\n0\n10\n20\n30\n40\n50\n% miss\nERM\nmixup\n(a) Prediction errors in-between training data. Evalu-\nated at x = λxi+(1−λ)xj, a prediction is counted as\na “miss” if it does not belong to {yi, yj}. The model\ntrained with mixup has fewer misses.\n0.00\n0.25\n0.50\n0.75\n1.00\nλ\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n∥∇ℓ∥\nERM\nmixup\n(b) Norm of the gradients of the model w.r.t. input\nin-between training data, evaluated at x = λxi +\n(1 −λ)xj. The model trained with mixup has smaller\ngradient norms.\nFigure 2: mixup leads to more robust model behaviors in-between the training data.\nModel\nMethod\nEpochs\nTop-1 Error\nTop-5 Error\nResNet-50\nERM (Goyal et al., 2017)\n90\n23.5\n-\nmixup α = 0.2\n90\n23.3\n6.6\nResNet-101\nERM (Goyal et al., 2017)\n90\n22.1\n-\nmixup α = 0.2\n90\n21.5\n5.6\nResNeXt-101 32*4d\nERM (Xie et al., 2016)\n100\n21.2\n-\nERM\n90\n21.2\n5.6\nmixup α = 0.4\n90\n20.7\n5.3\nResNeXt-101 64*4d\nERM (Xie et al., 2016)\n100\n20.4\n5.3\nmixup α = 0.4\n90\n19.8\n4.9\nResNet-50\nERM\n200\n23.6\n7.0\nmixup α = 0.2\n200\n22.1\n6.1\nResNet-101\nERM\n200\n22.0\n6.1\nmixup α = 0.2\n200\n20.8\n5.4\nResNeXt-101 32*4d\nERM\n200\n21.3\n5.9\nmixup α = 0.4\n200\n20.1\n5.0\nTable 1: Validation errors for ERM and mixup on the development set of ImageNet-2012.\nsince it is one of the simplest possible behaviors. Figure 1b shows that mixup leads to decision\nboundaries that transition linearly from class to class, providing a smoother estimate of uncertainty.\nFigure 2 illustrate the average behaviors of two neural network models trained on the CIFAR-10\ndataset using ERM and mixup. Both models have the same architecture, are trained with the same\nprocedure, and are evaluated at the same points in-between randomly sampled training data. The\nmodel trained with mixup is more stable in terms of model predictions and gradient norms in-between\ntraining samples.\n3\nEXPERIMENTS\n3.1\nIMAGENET CLASSIFICATION\nWe evaluate mixup on the ImageNet-2012 classiﬁcation dataset (Russakovsky et al., 2015). This\ndataset contains 1.3 million training images and 50,000 validation images, from a total of 1,000 classes.\nFor training, we follow standard data augmentation practices: scale and aspect ratio distortions,\nrandom crops, and horizontal ﬂips (Goyal et al., 2017). During evaluation, only the 224 × 224 central\ncrop of each image is tested. We use mixup and ERM to train several state-of-the-art ImageNet-2012\nclassiﬁcation models, and report both top-1 and top-5 error rates in Table 1.\n4\n",
    "Published as a conference paper at ICLR 2018\nDataset\nModel\nERM\nmixup\nCIFAR-10\nPreAct ResNet-18\n5.6\n4.2\nWideResNet-28-10\n3.8\n2.7\nDenseNet-BC-190\n3.7\n2.7\nCIFAR-100\nPreAct ResNet-18\n25.6\n21.1\nWideResNet-28-10\n19.4\n17.5\nDenseNet-BC-190\n19.0\n16.8\n(a) Test errors for the CIFAR experiments.\n0\n50\n100\n150\n200\nepoch\n0\n5\n10\n15\n20\nerror\nCIFAR-10 Test Error\nDenseNet-190 baseline\nDenseNet-190 mixup\n(b) Test error evolution for the best\nERM and mixup models.\nFigure 3: Test errors for ERM and mixup on the CIFAR experiments.\nFor all the experiments in this section, we use data-parallel distributed training in Caffe21 with\na minibatch size of 1,024. We use the learning rate schedule described in (Goyal et al., 2017).\nSpeciﬁcally, the learning rate is increased linearly from 0.1 to 0.4 during the ﬁrst 5 epochs, and it is\nthen divided by 10 after 30, 60 and 80 epochs when training for 90 epochs; or after 60, 120 and 180\nepochs when training for 200 epochs.\nFor mixup, we ﬁnd that α ∈[0.1, 0.4] leads to improved performance over ERM, whereas for large α,\nmixup leads to underﬁtting. We also ﬁnd that models with higher capacities and/or longer training\nruns are the ones to beneﬁt the most from mixup. For example, when trained for 90 epochs, the mixup\nvariants of ResNet-101 and ResNeXt-101 obtain a greater improvement (0.5% to 0.6%) over their\nERM analogues than the gain of smaller models such as ResNet-50 (0.2%). When trained for 200\nepochs, the top-1 error of the mixup variant of ResNet-50 is further reduced by 1.2% compared to the\n90 epoch run, whereas its ERM analogue stays the same.\n3.2\nCIFAR-10 AND CIFAR-100\nWe conduct additional image classiﬁcation experiments on the CIFAR-10 and CIFAR-100 datasets\nto further evaluate the generalization performance of mixup. In particular, we compare ERM and\nmixup training for: PreAct ResNet-18 (He et al., 2016) as implemented in (Liu, 2017), WideResNet-\n28-10 (Zagoruyko & Komodakis, 2016a) as implemented in (Zagoruyko & Komodakis, 2016b), and\nDenseNet (Huang et al., 2017) as implemented in (Veit, 2017). For DenseNet, we change the growth\nrate to 40 to follow the DenseNet-BC-190 speciﬁcation from (Huang et al., 2017). For mixup, we\nﬁx α = 1, which results in interpolations λ uniformly distributed between zero and one. All models\nare trained on a single Nvidia Tesla P100 GPU using PyTorch2 for 200 epochs on the training set\nwith 128 examples per minibatch, and evaluated on the test set. Learning rates start at 0.1 and are\ndivided by 10 after 100 and 150 epochs for all models except WideResNet. For WideResNet, we\nfollow (Zagoruyko & Komodakis, 2016a) and divide the learning rate by 10 after 60, 120 and 180\nepochs. Weight decay is set to 10−4. We do not use dropout in these experiments.\nWe summarize our results in Figure 3a. In both CIFAR-10 and CIFAR-100 classiﬁcation problems,\nthe models trained using mixup signiﬁcantly outperform their analogues trained with ERM. As seen\nin Figure 3b, mixup and ERM converge at a similar speed to their best test errors. Note that the\nDenseNet models in (Huang et al., 2017) were trained for 300 epochs with further learning rate\ndecays scheduled at the 150 and 225 epochs, which may explain the discrepancy the performance of\nDenseNet reported in Figure 3a and the original result of Huang et al. (2017).\n3.3\nSPEECH DATA\nNext, we perform speech recognition experiments using the Google commands dataset (Warden,\n2017). The dataset contains 65,000 utterances, where each utterance is about one-second long and\nbelongs to one out of 30 classes. The classes correspond to voice commands such as yes, no, down,\nleft, as pronounced by a few thousand different speakers. To preprocess the utterances, we ﬁrst\n1https://caffe2.ai\n2http://pytorch.org\n5\n",
    "Published as a conference paper at ICLR 2018\nModel\nMethod\nValidation set\nTest set\nLeNet\nERM\n9.8\n10.3\nmixup (α = 0.1)\n10.1\n10.8\nmixup (α = 0.2)\n10.2\n11.3\nVGG-11\nERM\n5.0\n4.6\nmixup (α = 0.1)\n4.0\n3.8\nmixup (α = 0.2)\n3.9\n3.4\nFigure 4: Classiﬁcation errors of ERM and mixup on the Google commands dataset.\nextract normalized spectrograms from the original waveforms at a sampling rate of 16 kHz. Next, we\nzero-pad the spectrograms to equalize their sizes at 160 × 101. For speech data, it is reasonable to\napply mixup both at the waveform and spectrogram levels. Here, we apply mixup at the spectrogram\nlevel just before feeding the data to the network.\nFor this experiment, we compare a LeNet (Lecun et al., 2001) and a VGG-11 (Simonyan & Zisserman,\n2015) architecture, each of them composed by two convolutional and two fully-connected layers.\nWe train each model for 30 epochs with minibatches of 100 examples, using Adam as the optimizer\n(Kingma & Ba, 2015). Training starts with a learning rate equal to 3 × 10−3 and is divided by 10\nevery 10 epochs. For mixup, we use a warm-up period of ﬁve epochs where we train the network on\noriginal training examples, since we ﬁnd it speeds up initial convergence. Table 4 shows that mixup\noutperforms ERM on this task, specially when using VGG-11, the model with larger capacity.\n3.4\nMEMORIZATION OF CORRUPTED LABELS\nFollowing Zhang et al. (2017), we evaluate the robustness of ERM and mixup models against randomly\ncorrupted labels. We hypothesize that increasing the strength of mixup interpolation α should generate\nvirtual examples further from the training examples, making memorization more difﬁcult to achieve.\nIn particular, it should be easier to learn interpolations between real examples compared to memorizing\ninterpolations involving random labels. We adapt an open-source implementation (Zhang, 2017)\nto generate three CIFAR-10 training sets, where 20%, 50%, or 80% of the labels are replaced by\nrandom noise, respectively. All the test labels are kept intact for evaluation. Dropout (Srivastava\net al., 2014) is considered the state-of-the-art method for learning with corrupted labels (Arpit et al.,\n2017). Thus, we compare in these experiments mixup, dropout, mixup + dropout, and ERM. For\nmixup, we choose α ∈{1, 2, 8, 32}; for dropout, we add one dropout layer in each PreAct block after\nthe ReLU activation layer between two convolution layers, as suggested in (Zagoruyko & Komodakis,\n2016a). We choose the dropout probability p ∈{0.5, 0.7, 0.8, 0.9}. For the combination of mixup\nand dropout, we choose α ∈{1, 2, 4, 8} and p ∈{0.3, 0.5, 0.7}. These experiments use the PreAct\nResNet-18 (He et al., 2016) model implemented in (Liu, 2017). All the other settings are the same as\nin Section 3.2.\nWe summarize our results in Table 2, where we note the best test error achieved during the training\nsession, as well as the ﬁnal test error after 200 epochs. To quantify the amount of memorization, we\nalso evaluate the training errors at the last epoch on real labels and corrupted labels. As the training\nprogresses with a smaller learning rate (e.g. less than 0.01), the ERM model starts to overﬁt the\ncorrupted labels. When using a large probability (e.g. 0.7 or 0.8), dropout can effectively reduce\noverﬁtting. mixup with a large α (e.g. 8 or 32) outperforms dropout on both the best and last epoch\ntest errors, and achieves lower training error on real labels while remaining resistant to noisy labels.\nInterestingly, mixup + dropout performs the best of all, showing that the two methods are compatible.\n3.5\nROBUSTNESS TO ADVERSARIAL EXAMPLES\nOne undesirable consequence of models trained using ERM is their fragility to adversarial exam-\nples (Szegedy et al., 2014). Adversarial examples are obtained by adding tiny (visually imperceptible)\nperturbations to legitimate examples in order to deteriorate the performance of the model. The adver-\nsarial noise is generated by ascending the gradient of the loss surface with respect to the legitimate\nexample. Improving the robustness to adversarial examples is a topic of active research.\n6\n",
    "Published as a conference paper at ICLR 2018\nLabel corruption\nMethod\nTest error\nTraining error\nBest\nLast\nReal\nCorrupted\n20%\nERM\n12.7\n16.6\n0.05\n0.28\nERM + dropout (p = 0.7)\n8.8\n10.4\n5.26\n83.55\nmixup (α = 8)\n5.9\n6.4\n2.27\n86.32\nmixup + dropout (α = 4, p = 0.1)\n6.2\n6.2\n1.92\n85.02\n50%\nERM\n18.8\n44.6\n0.26\n0.64\nERM + dropout (p = 0.8)\n14.1\n15.5\n12.71\n86.98\nmixup (α = 32)\n11.3\n12.7\n5.84\n85.71\nmixup + dropout (α = 8, p = 0.3)\n10.9\n10.9\n7.56\n87.90\n80%\nERM\n36.5\n73.9\n0.62\n0.83\nERM + dropout (p = 0.8)\n30.9\n35.1\n29.84\n86.37\nmixup (α = 32)\n25.3\n30.9\n18.92\n85.44\nmixup + dropout (α = 8, p = 0.3)\n24.0\n24.8\n19.70\n87.67\nTable 2: Results on the corrupted label experiments for the best models.\nMetric\nMethod\nFGSM\nI-FGSM\nTop-1\nERM\n90.7\n99.9\nmixup\n75.2\n99.6\nTop-5\nERM\n63.1\n93.4\nmixup\n49.1\n95.8\n(a) White box attacks.\nMetric\nMethod\nFGSM\nI-FGSM\nTop-1\nERM\n57.0\n57.3\nmixup\n46.0\n40.9\nTop-5\nERM\n24.8\n18.1\nmixup\n17.4\n11.8\n(b) Black box attacks.\nTable 3: Classiﬁcation errors of ERM and mixup models when tested on adversarial examples.\nAmong the several methods aiming to solve this problem, some have proposed to penalize the norm of\nthe Jacobian of the model to control its Lipschitz constant (Drucker & Le Cun, 1992; Cisse et al., 2017;\nBartlett et al., 2017; Hein & Andriushchenko, 2017). Other approaches perform data augmentation\nby producing and training on adversarial examples (Goodfellow et al., 2015). Unfortunately, all\nof these methods add signiﬁcant computational overhead to ERM. Here, we show that mixup can\nsigniﬁcantly improve the robustness of neural networks without hindering the speed of ERM by\npenalizing the norm of the gradient of the loss w.r.t a given input along the most plausible directions\n(e.g. the directions to other training points). Indeed, Figure 2 shows that mixup results in models\nhaving a smaller loss and gradient norm between examples compared to vanilla ERM.\nTo assess the robustness of mixup models to adversarial examples, we use three ResNet-101 models:\ntwo of them trained using ERM on ImageNet-2012, and the third trained using mixup. In the ﬁrst\nset of experiments, we study the robustness of one ERM model and the mixup model against white\nbox attacks. That is, for each of the two models, we use the model itself to generate adversarial\nexamples, either using the Fast Gradient Sign Method (FGSM) or the Iterative FGSM (I-FGSM)\nmethods (Goodfellow et al., 2015), allowing a maximum perturbation of ϵ = 4 for every pixel. For\nI-FGSM, we use 10 iterations with equal step size. In the second set of experiments, we evaluate\nrobustness against black box attacks. That is, we use the ﬁrst ERM model to produce adversarial\nexamples using FGSM and I-FGSM. Then, we test the robustness of the second ERM model and the\nmixup model to these examples. The results of both settings are summarized in Table 3.\nFor the FGSM white box attack, the mixup model is 2.7 times more robust than the ERM model in\nterms of Top-1 error. For the FGSM black box attack, the mixup model is 1.25 times more robust\nthan the ERM model in terms of Top-1 error. Also, while both mixup and ERM are not robust to\nwhite box I-FGSM attacks, mixup is about 40% more robust than ERM in the black box I-FGSM\nsetting. Overall, mixup produces neural networks that are signiﬁcantly more robust than ERM against\nadversarial examples in white box and black settings without additional overhead compared to ERM.\n7\n",
    "Published as a conference paper at ICLR 2018\nDataset\nERM\nmixup\nAbalone\n74.0\n73.6\nArcene\n57.6\n48.0\nArrhythmia\n56.6\n46.3\nDataset\nERM\nmixup\nHtru2\n2.0\n2.0\nIris\n21.3\n17.3\nPhishing\n16.3\n15.2\nTable 4: ERM and mixup classiﬁcation errors on the UCI datasets.\nERM GAN\nmixup GAN (α = 0.2)\nFigure 5: Effect of mixup on stabilizing GAN training at iterations 10, 100, 1000, 10000, and 20000.\n3.6\nTABULAR DATA\nTo further explore the performance of mixup on non-image data, we performed a series of experiments\non six arbitrary classiﬁcation problems drawn from the UCI dataset (Lichman, 2013). The neural\nnetworks in this section are fully-connected, and have two hidden layers of 128 ReLU units. The\nparameters of these neural networks are learned using Adam (Kingma & Ba, 2015) with default\nhyper-parameters, over 10 epochs of mini-batches of size 16. Table 4 shows that mixup improves the\naverage test error on four out of the six considered datasets, and never underperforms ERM.\n3.7\nSTABILIZATION OF GENERATIVE ADVERSARIAL NETWORKS (GANS)\nGenerative Adversarial Networks, also known as GANs (Goodfellow et al., 2014), are a powerful\nfamily of implicit generative models. In GANs, a generator and a discriminator compete against\neach other to model a distribution P. On the one hand, the generator g competes to transform noise\nvectors z ∼Q into fake samples g(z) that resemble real samples x ∼P. On the other hand, the\ndiscriminator competes to distinguish between real samples x and fake samples g(z). Mathematically,\ntraining a GAN is equivalent to solving the optimization problem\nmax\ng\nmin\nd\nE\nx,z ℓ(d(x), 1) + ℓ(d(g(z)), 0),\nwhere ℓis the binary cross entropy loss. Unfortunately, solving the previous min-max equation is a\nnotoriously difﬁcult optimization problem (Goodfellow, 2016), since the discriminator often provides\nthe generator with vanishing gradients. We argue that mixup should stabilize GAN training because it\nacts as a regularizer on the gradients of the discriminator, akin to the binary classiﬁer in Figure 1b.\nThen, the smoothness of the discriminator guarantees a stable source of gradient information to the\ngenerator. The mixup formulation of GANs is:\nmax\ng\nmin\nd\nE\nx,z,λ ℓ(d(λx + (1 −λ)g(z)), λ).\nFigure 5 illustrates the stabilizing effect of mixup the training of GAN (orange samples) when\nmodeling two toy datasets (blue samples). The neural networks in these experiments are fully-\nconnected and have three hidden layers of 512 ReLU units. The generator network accepts two-\ndimensional Gaussian noise vectors. The networks are trained for 20,000 mini-batches of size\n128 using the Adam optimizer with default parameters, where the discriminator is trained for ﬁve\niterations before every generator iteration. The training of mixup GANs seems promisingly robust to\nhyper-parameter and architectural choices.\n8\n",
    "Published as a conference paper at ICLR 2018\nMethod\nSpeciﬁcation\nModiﬁed\nWeight decay\nInput\nTarget\n10−4\n5 × 10−4\nERM\n\u0017\n\u0017\n5.53\n5.18\nmixup\nAC + RP\n\u0013\n\u0013\n4.24\n4.68\nAC + KNN\n\u0013\n\u0013\n4.98\n5.26\nmix labels and latent\nLayer 1\n\u0013\n\u0013\n4.44\n4.51\nrepresentations\nLayer 2\n\u0013\n\u0013\n4.56\n4.61\n(AC + RP)\nLayer 3\n\u0013\n\u0013\n5.39\n5.55\nLayer 4\n\u0013\n\u0013\n5.95\n5.43\nLayer 5\n\u0013\n\u0013\n5.39\n5.15\nmix inputs only\nSC + KNN (Chawla et al., 2002)\n\u0013\n\u0017\n5.45\n5.52\nAC + KNN\n\u0013\n\u0017\n5.43\n5.48\nSC + RP\n\u0013\n\u0017\n5.23\n5.55\nAC + RP\n\u0013\n\u0017\n5.17\n5.72\nlabel smoothing\nϵ = 0.05\n\u0017\n\u0013\n5.25\n5.02\n(Szegedy et al., 2016)\nϵ = 0.1\n\u0017\n\u0013\n5.33\n5.17\nϵ = 0.2\n\u0017\n\u0013\n5.34\n5.06\nmix inputs +\nϵ = 0.05\n\u0013\n\u0013\n5.02\n5.40\nlabel smoothing\nϵ = 0.1\n\u0013\n\u0013\n5.08\n5.09\n(AC + RP)\nϵ = 0.2\n\u0013\n\u0013\n4.98\n5.06\nϵ = 0.4\n\u0013\n\u0013\n5.25\n5.39\nadd Gaussian noise\nσ = 0.05\n\u0013\n\u0017\n5.53\n5.04\nto inputs\nσ = 0.1\n\u0013\n\u0017\n6.41\n5.86\nσ = 0.2\n\u0013\n\u0017\n7.16\n7.24\nTable 5: Results of the ablation studies on the CIFAR-10 dataset. Reported are the median test errors\nof the last 10 epochs. A tick (\u0013) means the component is different from standard ERM training,\nwhereas a cross (\u0017) means it follows the standard training practice. AC: mix between all classes. SC:\nmix within the same class. RP: mix between random pairs. KNN: mix between k-nearest neighbors\n(k=200). Please refer to the text for details about the experiments and interpretations.\n3.8\nABLATION STUDIES\nmixup is a data augmentation method that consists of only two parts: random convex combination of\nraw inputs, and correspondingly, convex combination of one-hot label encodings. However, there are\nseveral design choices to make. For example, on how to augment the inputs, we could have chosen\nto interpolate the latent representations (i.e. feature maps) of a neural network, and we could have\nchosen to interpolate only between the nearest neighbors, or only between inputs of the same class.\nWhen the inputs to interpolate come from two different classes, we could have chosen to assign a\nsingle label to the synthetic input, for example using the label of the input that weights more in the\nconvex combination. To compare mixup with these alternative possibilities, we run a set of ablation\nstudy experiments using the PreAct ResNet-18 architecture on the CIFAR-10 dataset.\nSpeciﬁcally, for each of the data augmentation methods, we test two weight decay settings (10−4\nwhich works well for mixup, and 5 × 10−4 which works well for ERM). All the other settings and\nhyperparameters are the same as reported in Section 3.2.\nTo compare interpolating raw inputs with interpolating latent representations, we test on random\nconvex combination of the learned representations before each residual block (denoted Layer 1-4)\nor before the uppermost “average pooling + fully connected” layer (denoted Layer 5). To compare\nmixing random pairs of inputs (RP) with mixing nearest neighbors (KNN), we ﬁrst compute the 200\nnearest neighbors for each training sample, either from the same class (SC) or from all the classes\n(AC). Then during training, for each sample in a minibatch, we replace the sample with a synthetic\nsample by convex combination with a random draw from its nearest neighbors. To compare mixing\nall the classes (AC) with mixing within the same class (SC), we convex combine a minibatch with a\n9\n",
    "Published as a conference paper at ICLR 2018\nrandom permutation of its sample index, where the permutation is done in a per-batch basis (AC) or a\nper-class basis (SC). To compare mixing inputs and labels with mixing inputs only, we either use a\nconvex combination of the two one-hot encodings as the target, or select the one-hot encoding of the\ncloser training sample as the target. For label smoothing, we follow Szegedy et al. (2016) and use\nϵ\n10 as the target for incorrect classes, and 1 −9ϵ\n10 as the target for the correct class.Adding Gaussian\nnoise to inputs is used as another baseline. We report the median test errors of the last 10 epochs.\nResults are shown in Table 5.\nFrom the ablation study experiments, we have the following observations. First, mixup is the best\ndata augmentation method we test, and is signiﬁcantly better than the second best method (mix input\n+ label smoothing). Second, the effect of regularization can be seen by comparing the test error with a\nsmall weight decay (10−4) with a large one (5 × 10−4). For example, for ERM a large weight decay\nworks better, whereas for mixup a small weight decay is preferred, conﬁrming its regularization effects.\nWe also see an increasing advantage of large weight decay when interpolating in higher layers of latent\nrepresentations, indicating decreasing strength of regularization. Among all the input interpolation\nmethods, mixing random pairs from all classes (AC + RP) has the strongest regularization effect.\nLabel smoothing and adding Gaussian noise have a relatively small regularization effect. Finally,\nwe note that the SMOTE algorithm (Chawla et al., 2002) does not lead to a noticeable gain in\nperformance.\n4\nRELATED WORK\nData augmentation lies at the heart of all successful applications of deep learning, ranging from image\nclassiﬁcation (Krizhevsky et al., 2012) to speech recognition (Graves et al., 2013; Amodei et al.,\n2016). In all cases, substantial domain knowledge is leveraged to design suitable data transformations\nleading to improved generalization. In image classiﬁcation, for example, one routinely uses rotation,\ntranslation, cropping, resizing, ﬂipping (Lecun et al., 2001; Simonyan & Zisserman, 2015), and\nrandom erasing (Zhong et al., 2017) to enforce visually plausible invariances in the model through\nthe training data. Similarly, in speech recognition, noise injection is a prevalent practice to improve\nthe robustness and accuracy of the trained models (Amodei et al., 2016).\nMore related to mixup, Chawla et al. (2002) propose to augment the rare class in an imbalanced\ndataset by interpolating the nearest neighbors; DeVries & Taylor (2017) show that interpolation and\nextrapolation the nearest neighbors of the same class in feature space can improve generalization.\nHowever, their proposals only operate among the nearest neighbors within a certain class at the\ninput / feature level, and hence does not account for changes in the corresponding labels. Recent\napproaches have also proposed to regularize the output distribution of a neural network by label\nsmoothing (Szegedy et al., 2016), or penalizing high-conﬁdence softmax distributions (Pereyra et al.,\n2017). These methods bear similarities with mixup in the sense that supervision depends on multiple\nsmooth labels, rather than on single hard labels as in traditional ERM. However, the label smoothing\nin these works is applied or regularized independently from the associated feature values.\nmixup enjoys several desirable aspects of previous data augmentation and regularization schemes\nwithout suffering from their drawbacks. Like the method of DeVries & Taylor (2017), it does not\nrequire signiﬁcant domain knowledge. Like label smoothing, the supervision of every example is not\noverly dominated by the ground-truth label. Unlike both of these approaches, the mixup transformation\nestablishes a linear relationship between data augmentation and the supervision signal. We believe\nthat this leads to a strong regularizer that improves generalization as demonstrated by our experiments.\nThe linearity constraint, through its effect on the derivatives of the function approximated, also relates\nmixup to other methods such as Sobolev training of neural networks (Czarnecki et al., 2017) or\nWGAN-GP (Gulrajani et al., 2017).\n5\nDISCUSSION\nWe have proposed mixup, a data-agnostic and straightforward data augmentation principle. We\nhave shown that mixup is a form of vicinal risk minimization, which trains on virtual examples\nconstructed as the linear interpolation of two random examples from the training set and their labels.\nIncorporating mixup into existing training pipelines reduces to a few lines of code, and introduces\nlittle or no computational overhead. Throughout an extensive evaluation, we have shown that mixup\n10\n",
    "Published as a conference paper at ICLR 2018\nimproves the generalization error of state-of-the-art models on ImageNet, CIFAR, speech, and\ntabular datasets. Furthermore, mixup helps to combat memorization of corrupt labels, sensitivity to\nadversarial examples, and instability in adversarial training.\nIn our experiments, the following trend is consistent: with increasingly large α, the training error on\nreal data increases, while the generalization gap decreases. This sustains our hypothesis that mixup\nimplicitly controls model complexity. However, we do not yet have a good theory for understanding\nthe ‘sweet spot’ of this bias-variance trade-off. For example, in CIFAR-10 classiﬁcation we can\nget very low training error on real data even when α →∞(i.e., training only on averages of pairs\nof real examples), whereas in ImageNet classiﬁcation, the training error on real data increases\nsigniﬁcantly with α →∞. Based on our ImageNet and Google commands experiments with different\nmodel architectures, we conjecture that increasing the model capacity would make training error less\nsensitive to large α, hence giving mixup a more signiﬁcant advantage.\nmixup also opens up several possibilities for further exploration. First, is it possible to make\nsimilar ideas work on other types of supervised learning problems, such as regression and structured\nprediction? While generalizing mixup to regression problems is straightforward, its application\nto structured prediction problems such as image segmentation remains less obvious. Second, can\nsimilar methods prove helpful beyond supervised learning? The interpolation principle seems like a\nreasonable inductive bias which might also help in unsupervised, semi-supervised, and reinforcement\nlearning. Can we extend mixup to feature-label extrapolation to guarantee a robust model behavior\nfar away from the training data? Although our discussion of these directions is still speculative, we\nare excited about the possibilities mixup opens up, and hope that our observations will prove useful\nfor future development.\nACKNOWLEDGEMENTS\nWe would like to thank Priya Goyal, Yossi Adi and the PyTorch team. We also thank the Anonymous\nReview 2 for proposing the mixup + dropout experiments.\nREFERENCES\nD. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, Q. Cheng,\nG. Chen, et al. Deep speech 2: End-to-end speech recognition in English and Mandarin. In ICML, 2016.\nD. Arpit, S. Jastrzebski, N. Ballas, D. Krueger, E. Bengio, M. S. Kanwal, T. Maharaj, A. Fischer, A. Courville,\nY. Bengio, et al. A closer look at memorization in deep networks. ICML, 2017.\nP. Bartlett, D. J. Foster, and M. Telgarsky. Spectrally-normalized margin bounds for neural networks. NIPS,\n2017.\nO. Chapelle, J. Weston, L. Bottou, and V. Vapnik. Vicinal risk minimization. NIPS, 2000.\nN. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. SMOTE: synthetic minority over-sampling\ntechnique. Journal of artiﬁcial intelligence research, 16:321–357, 2002.\nC. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn, and T. Robinson. One billion word benchmark\nfor measuring progress in statistical language modeling. arXiv, 2013.\nM. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, and N. Usunier. Parseval networks: Improving robustness to\nadversarial examples. ICML, 2017.\nW. M. Czarnecki, S. Osindero, M. Jaderberg, G. ´Swirszcz, and R. Pascanu. Sobolev training for neural networks.\nNIPS, 2017.\nT. DeVries and G. W. Taylor. Dataset augmentation in feature space. ICLR Workshops, 2017.\nH. Drucker and Y. Le Cun. Improving generalization performance using double backpropagation. IEEE\nTransactions on Neural Networks, 3(6):991–997, 1992.\nI. Goodfellow. Tutorial: Generative adversarial networks. NIPS, 2016.\nI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio.\nGenerative adversarial nets. NIPS, 2014.\n11\n",
    "Published as a conference paper at ICLR 2018\nI. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. ICLR, 2015.\nP. Goyal, P. Doll´ar, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He. Accurate,\nlarge minibatch SGD: Training ImageNet in 1 hour. arXiv, 2017.\nA. Graves, A.-r. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In ICASSP.\nIEEE, 2013.\nI. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. Improved training of Wasserstein GANs.\nNIPS, 2017.\nN. Harvey, C. Liaw, and A. Mehrabian. Nearly-tight VC-dimension bounds for piecewise linear neural networks.\nJMLR, 2017.\nK. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. ECCV, 2016.\nM. Hein and M. Andriushchenko. Formal guarantees on the robustness of a classiﬁer against adversarial\nmanipulation. NIPS, 2017.\nG. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N.\nSainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four\nresearch groups. IEEE Signal Processing Magazine, 2012.\nG. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. CVPR,\n2017.\nD. Kingma and J. Ba. Adam: A method for stochastic optimization. ICLR, 2015.\nA. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classiﬁcation with deep convolutional neural networks.\nNIPS, 2012.\nY. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.\nProceedings of IEEE, 2001.\nM. Lichman. UCI machine learning repository, 2013.\nK. Liu, 2017. URL https://github.com/kuangliu/pytorch-cifar.\nG. Pereyra, G. Tucker, J. Chorowski, Ł. Kaiser, and G. Hinton. Regularizing neural networks by penalizing\nconﬁdent output distributions. ICLR Workshops, 2017.\nO. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\nA. C. Berg, and L. Fei-Fei. ImageNet large scale visual recognition challenge. IJCV, 2015.\nD. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou,\nV. Panneershelvam, M. Lanctot, et al. Mastering the game of Go with deep neural networks and tree search.\nNature, 2016.\nP. Simard, Y. LeCun, J. Denker, and B. Victorri. Transformation invariance in pattern recognitiontangent distance\nand tangent propagation. Neural networks: tricks of the trade, 1998.\nK. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. ICLR,\n2015.\nJ. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller. Striving for simplicity: The all convolutional\nnet. ICLR Workshops, 2015.\nN. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple way to\nprevent neural networks from overﬁtting. Journal of Machine Learning Research, 15(1):1929–1958, 2014.\nC. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfellow, and R. Fergus. Intriguing properties\nof neural networks. ICLR, 2014.\nC. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the Inception architecture for computer\nvision. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.\nV. N. Vapnik. Statistical learning theory. J. Wiley, 1998.\nV. Vapnik and A. Y. Chervonenkis. On the uniform convergence of relative frequencies of events to their\nprobabilities. Theory of Probability and its Applications, 1971.\n12\n",
    "Published as a conference paper at ICLR 2018\nA. Veit, 2017. URL https://github.com/andreasveit.\nP.\nWarden,\n2017.\nURL\nhttps://research.googleblog.com/2017/08/\nlaunching-speech-commands-dataset.html.\nS. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He. Aggregated residual transformations for deep neural networks.\nCVPR, 2016.\nS. Zagoruyko and N. Komodakis. Wide residual networks. BMVC, 2016a.\nS.\nZagoruyko\nand\nN.\nKomodakis,\n2016b.\nURL\nhttps://github.com/szagoruyko/\nwide-residual-networks.\nC. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking\ngeneralization. ICLR, 2017.\nC. Zhang, 2017. URL https://github.com/pluskid/fitting-random-labels.\nZ. Zhong, L. Zheng, G. Kang, S. Li, and Y. Yang. Random erasing data augmentation. arXiv, 2017.\n13\n"
  ],
  "full_text": "Published as a conference paper at ICLR 2018\nmixup: BEYOND EMPIRICAL RISK MINIMIZATION\nHongyi Zhang\nMIT\nMoustapha Cisse, Yann N. Dauphin, David Lopez-Paz∗\nFAIR\nABSTRACT\nLarge deep neural networks are powerful, but exhibit undesirable behaviors such\nas memorization and sensitivity to adversarial examples. In this work, we propose\nmixup, a simple learning principle to alleviate these issues. In essence, mixup trains\na neural network on convex combinations of pairs of examples and their labels.\nBy doing so, mixup regularizes the neural network to favor simple linear behavior\nin-between training examples. Our experiments on the ImageNet-2012, CIFAR-10,\nCIFAR-100, Google commands and UCI datasets show that mixup improves the\ngeneralization of state-of-the-art neural network architectures. We also ﬁnd that\nmixup reduces the memorization of corrupt labels, increases the robustness to\nadversarial examples, and stabilizes the training of generative adversarial networks.\n1\nINTRODUCTION\nLarge deep neural networks have enabled breakthroughs in ﬁelds such as computer vision (Krizhevsky\net al., 2012), speech recognition (Hinton et al., 2012), and reinforcement learning (Silver et al., 2016).\nIn most successful applications, these neural networks share two commonalities. First, they are\ntrained as to minimize their average error over the training data, a learning rule also known as the\nEmpirical Risk Minimization (ERM) principle (Vapnik, 1998). Second, the size of these state-of-the-\nart neural networks scales linearly with the number of training examples. For instance, the network of\nSpringenberg et al. (2015) used 106 parameters to model the 5 · 104 images in the CIFAR-10 dataset,\nthe network of (Simonyan & Zisserman, 2015) used 108 parameters to model the 106 images in the\nImageNet-2012 dataset, and the network of Chelba et al. (2013) used 2 · 1010 parameters to model\nthe 109 words in the One Billion Word dataset.\nStrikingly, a classical result in learning theory (Vapnik & Chervonenkis, 1971) tells us that the\nconvergence of ERM is guaranteed as long as the size of the learning machine (e.g., the neural\nnetwork) does not increase with the number of training data. Here, the size of a learning machine is\nmeasured in terms of its number of parameters or, relatedly, its VC-complexity (Harvey et al., 2017).\nThis contradiction challenges the suitability of ERM to train our current neural network models, as\nhighlighted in recent research. On the one hand, ERM allows large neural networks to memorize\n(instead of generalize from) the training data even in the presence of strong regularization, or in\nclassiﬁcation problems where the labels are assigned at random (Zhang et al., 2017). On the other\nhand, neural networks trained with ERM change their predictions drastically when evaluated on\nexamples just outside the training distribution (Szegedy et al., 2014), also known as adversarial\nexamples. This evidence suggests that ERM is unable to explain or provide generalization on testing\ndistributions that differ only slightly from the training data. However, what is the alternative to ERM?\nThe method of choice to train on similar but different examples to the training data is known as data\naugmentation (Simard et al., 1998), formalized by the Vicinal Risk Minimization (VRM) principle\n(Chapelle et al., 2000). In VRM, human knowledge is required to describe a vicinity or neighborhood\naround each example in the training data. Then, additional virtual examples can be drawn from the\nvicinity distribution of the training examples to enlarge the support of the training distribution. For\ninstance, when performing image classiﬁcation, it is common to deﬁne the vicinity of one image\nas the set of its horizontal reﬂections, slight rotations, and mild scalings. While data augmentation\nconsistently leads to improved generalization (Simard et al., 1998), the procedure is dataset-dependent,\nand thus requires the use of expert knowledge. Furthermore, data augmentation assumes that the\n∗Alphabetical order.\n1\narXiv:1710.09412v2  [cs.LG]  27 Apr 2018\n\n\nPublished as a conference paper at ICLR 2018\nexamples in the vicinity share the same class, and does not model the vicinity relation across examples\nof different classes.\nContribution\nMotivated by these issues, we introduce a simple and data-agnostic data augmenta-\ntion routine, termed mixup (Section 2). In a nutshell, mixup constructs virtual training examples\n˜x = λxi + (1 −λ)xj,\nwhere xi, xj are raw input vectors\n˜y = λyi + (1 −λ)yj,\nwhere yi, yj are one-hot label encodings\n(xi, yi) and (xj, yj) are two examples drawn at random from our training data, and λ ∈[0, 1].\nTherefore, mixup extends the training distribution by incorporating the prior knowledge that linear\ninterpolations of feature vectors should lead to linear interpolations of the associated targets. mixup\ncan be implemented in a few lines of code, and introduces minimal computation overhead.\nDespite its simplicity, mixup allows a new state-of-the-art performance in the CIFAR-10, CIFAR-\n100, and ImageNet-2012 image classiﬁcation datasets (Sections 3.1 and 3.2). Furthermore, mixup\nincreases the robustness of neural networks when learning from corrupt labels (Section 3.4), or facing\nadversarial examples (Section 3.5). Finally, mixup improves generalization on speech (Sections 3.3)\nand tabular (Section 3.6) data, and can be used to stabilize the training of GANs (Section 3.7). The\nsource-code necessary to replicate our CIFAR-10 experiments is available at:\nhttps://github.com/facebookresearch/mixup-cifar10.\nTo understand the effects of various design choices in mixup, we conduct a thorough set of ablation\nstudy experiments (Section 3.8). The results suggest that mixup performs signiﬁcantly better than\nrelated methods in previous work, and each of the design choices contributes to the ﬁnal performance.\nWe conclude by exploring the connections to prior work (Section 4), as well as offering some points\nfor discussion (Section 5).\n2\nFROM EMPIRICAL RISK MINIMIZATION TO mixup\nIn supervised learning, we are interested in ﬁnding a function f ∈F that describes the relationship\nbetween a random feature vector X and a random target vector Y , which follow the joint distribution\nP(X, Y ). To this end, we ﬁrst deﬁne a loss function ℓthat penalizes the differences between\npredictions f(x) and actual targets y, for examples (x, y) ∼P. Then, we minimize the average of\nthe loss function ℓover the data distribution P, also known as the expected risk:\nR(f) =\nZ\nℓ(f(x), y)dP(x, y).\nUnfortunately, the distribution P is unknown in most practical situations. Instead, we usually have\naccess to a set of training data D = {(xi, yi)}n\ni=1, where (xi, yi) ∼P for all i = 1, . . . , n. Using\nthe training data D, we may approximate P by the empirical distribution\nPδ(x, y) = 1\nn\nn\nX\ni=1\nδ(x = xi, y = yi),\nwhere δ(x = xi, y = yi) is a Dirac mass centered at (xi, yi). Using the empirical distribution Pδ, we\ncan now approximate the expected risk by the empirical risk:\nRδ(f) =\nZ\nℓ(f(x), y)dPδ(x, y) = 1\nn\nn\nX\ni=1\nℓ(f(xi), yi).\n(1)\nLearning the function f by minimizing (1) is known as the Empirical Risk Minimization (ERM)\nprinciple (Vapnik, 1998). While efﬁcient to compute, the empirical risk (1) monitors the behaviour\nof f only at a ﬁnite set of n examples. When considering functions with a number parameters\ncomparable to n (such as large neural networks), one trivial way to minimize (1) is to memorize the\ntraining data (Zhang et al., 2017). Memorization, in turn, leads to the undesirable behaviour of f\noutside the training data (Szegedy et al., 2014).\n2\n\n\nPublished as a conference paper at ICLR 2018\n# y1, y2 should be one-hot vectors\nfor (x1, y1), (x2, y2) in zip(loader1, loader2):\nlam = numpy.random.beta(alpha, alpha)\nx = Variable(lam * x1 + (1. - lam) * x2)\ny = Variable(lam * y1 + (1. - lam) * y2)\noptimizer.zero_grad()\nloss(net(x), y).backward()\noptimizer.step()\n(a) One epoch of mixup training in PyTorch.\nERM\nmixup\n(b) Effect of mixup (α = 1) on a\ntoy problem.\nGreen: Class 0.\nOr-\nange: Class 1. Blue shading indicates\np(y = 1|x).\nFigure 1: Illustration of mixup, which converges to ERM as α →0.\nHowever, the na¨ıve estimate Pδ is one out of many possible choices to approximate the true distribu-\ntion P. For instance, in the Vicinal Risk Minimization (VRM) principle (Chapelle et al., 2000), the\ndistribution P is approximated by\nPν(˜x, ˜y) = 1\nn\nn\nX\ni=1\nν(˜x, ˜y|xi, yi),\nwhere ν is a vicinity distribution that measures the probability of ﬁnding the virtual feature-target\npair (˜x, ˜y) in the vicinity of the training feature-target pair (xi, yi). In particular, Chapelle et al.\n(2000) considered Gaussian vicinities ν(˜x, ˜y|xi, yi) = N(˜x −xi, σ2)δ(˜y = yi), which is equivalent\nto augmenting the training data with additive Gaussian noise. To learn using VRM, we sample the\nvicinal distribution to construct a dataset Dν := {(˜xi, ˜yi)}m\ni=1, and minimize the empirical vicinal\nrisk:\nRν(f) = 1\nm\nm\nX\ni=1\nℓ(f(˜xi), ˜yi).\nThe contribution of this paper is to propose a generic vicinal distribution, called mixup:\nµ(˜x, ˜y|xi, yi) = 1\nn\nn\nX\nj\nE\nλ [δ(˜x = λ · xi + (1 −λ) · xj, ˜y = λ · yi + (1 −λ) · yj)] ,\nwhere λ ∼Beta(α, α), for α ∈(0, ∞). In a nutshell, sampling from the mixup vicinal distribution\nproduces virtual feature-target vectors\n˜x = λxi + (1 −λ)xj,\n˜y = λyi + (1 −λ)yj,\nwhere (xi, yi) and (xj, yj) are two feature-target vectors drawn at random from the training data, and\nλ ∈[0, 1]. The mixup hyper-parameter α controls the strength of interpolation between feature-target\npairs, recovering the ERM principle as α →0.\nThe implementation of mixup training is straightforward, and introduces a minimal computation\noverhead. Figure 1a shows the few lines of code necessary to implement mixup training in PyTorch.\nFinally, we mention alternative design choices. First, in preliminary experiments we ﬁnd that convex\ncombinations of three or more examples with weights sampled from a Dirichlet distribution does not\nprovide further gain, but increases the computation cost of mixup. Second, our current implementation\nuses a single data loader to obtain one minibatch, and then mixup is applied to the same minibatch\nafter random shufﬂing. We found this strategy works equally well, while reducing I/O requirements.\nThird, interpolating only between inputs with equal label did not lead to the performance gains of\nmixup discussed in the sequel. More empirical comparison can be found in Section 3.8.\nWhat is mixup doing?\nThe mixup vicinal distribution can be understood as a form of data aug-\nmentation that encourages the model f to behave linearly in-between training examples. We argue\nthat this linear behaviour reduces the amount of undesirable oscillations when predicting outside the\ntraining examples. Also, linearity is a good inductive bias from the perspective of Occam’s razor,\n3\n\n\nPublished as a conference paper at ICLR 2018\n0.00\n0.25\n0.50\n0.75\n1.00\nλ\n0\n10\n20\n30\n40\n50\n% miss\nERM\nmixup\n(a) Prediction errors in-between training data. Evalu-\nated at x = λxi+(1−λ)xj, a prediction is counted as\na “miss” if it does not belong to {yi, yj}. The model\ntrained with mixup has fewer misses.\n0.00\n0.25\n0.50\n0.75\n1.00\nλ\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n∥∇ℓ∥\nERM\nmixup\n(b) Norm of the gradients of the model w.r.t. input\nin-between training data, evaluated at x = λxi +\n(1 −λ)xj. The model trained with mixup has smaller\ngradient norms.\nFigure 2: mixup leads to more robust model behaviors in-between the training data.\nModel\nMethod\nEpochs\nTop-1 Error\nTop-5 Error\nResNet-50\nERM (Goyal et al., 2017)\n90\n23.5\n-\nmixup α = 0.2\n90\n23.3\n6.6\nResNet-101\nERM (Goyal et al., 2017)\n90\n22.1\n-\nmixup α = 0.2\n90\n21.5\n5.6\nResNeXt-101 32*4d\nERM (Xie et al., 2016)\n100\n21.2\n-\nERM\n90\n21.2\n5.6\nmixup α = 0.4\n90\n20.7\n5.3\nResNeXt-101 64*4d\nERM (Xie et al., 2016)\n100\n20.4\n5.3\nmixup α = 0.4\n90\n19.8\n4.9\nResNet-50\nERM\n200\n23.6\n7.0\nmixup α = 0.2\n200\n22.1\n6.1\nResNet-101\nERM\n200\n22.0\n6.1\nmixup α = 0.2\n200\n20.8\n5.4\nResNeXt-101 32*4d\nERM\n200\n21.3\n5.9\nmixup α = 0.4\n200\n20.1\n5.0\nTable 1: Validation errors for ERM and mixup on the development set of ImageNet-2012.\nsince it is one of the simplest possible behaviors. Figure 1b shows that mixup leads to decision\nboundaries that transition linearly from class to class, providing a smoother estimate of uncertainty.\nFigure 2 illustrate the average behaviors of two neural network models trained on the CIFAR-10\ndataset using ERM and mixup. Both models have the same architecture, are trained with the same\nprocedure, and are evaluated at the same points in-between randomly sampled training data. The\nmodel trained with mixup is more stable in terms of model predictions and gradient norms in-between\ntraining samples.\n3\nEXPERIMENTS\n3.1\nIMAGENET CLASSIFICATION\nWe evaluate mixup on the ImageNet-2012 classiﬁcation dataset (Russakovsky et al., 2015). This\ndataset contains 1.3 million training images and 50,000 validation images, from a total of 1,000 classes.\nFor training, we follow standard data augmentation practices: scale and aspect ratio distortions,\nrandom crops, and horizontal ﬂips (Goyal et al., 2017). During evaluation, only the 224 × 224 central\ncrop of each image is tested. We use mixup and ERM to train several state-of-the-art ImageNet-2012\nclassiﬁcation models, and report both top-1 and top-5 error rates in Table 1.\n4\n\n\nPublished as a conference paper at ICLR 2018\nDataset\nModel\nERM\nmixup\nCIFAR-10\nPreAct ResNet-18\n5.6\n4.2\nWideResNet-28-10\n3.8\n2.7\nDenseNet-BC-190\n3.7\n2.7\nCIFAR-100\nPreAct ResNet-18\n25.6\n21.1\nWideResNet-28-10\n19.4\n17.5\nDenseNet-BC-190\n19.0\n16.8\n(a) Test errors for the CIFAR experiments.\n0\n50\n100\n150\n200\nepoch\n0\n5\n10\n15\n20\nerror\nCIFAR-10 Test Error\nDenseNet-190 baseline\nDenseNet-190 mixup\n(b) Test error evolution for the best\nERM and mixup models.\nFigure 3: Test errors for ERM and mixup on the CIFAR experiments.\nFor all the experiments in this section, we use data-parallel distributed training in Caffe21 with\na minibatch size of 1,024. We use the learning rate schedule described in (Goyal et al., 2017).\nSpeciﬁcally, the learning rate is increased linearly from 0.1 to 0.4 during the ﬁrst 5 epochs, and it is\nthen divided by 10 after 30, 60 and 80 epochs when training for 90 epochs; or after 60, 120 and 180\nepochs when training for 200 epochs.\nFor mixup, we ﬁnd that α ∈[0.1, 0.4] leads to improved performance over ERM, whereas for large α,\nmixup leads to underﬁtting. We also ﬁnd that models with higher capacities and/or longer training\nruns are the ones to beneﬁt the most from mixup. For example, when trained for 90 epochs, the mixup\nvariants of ResNet-101 and ResNeXt-101 obtain a greater improvement (0.5% to 0.6%) over their\nERM analogues than the gain of smaller models such as ResNet-50 (0.2%). When trained for 200\nepochs, the top-1 error of the mixup variant of ResNet-50 is further reduced by 1.2% compared to the\n90 epoch run, whereas its ERM analogue stays the same.\n3.2\nCIFAR-10 AND CIFAR-100\nWe conduct additional image classiﬁcation experiments on the CIFAR-10 and CIFAR-100 datasets\nto further evaluate the generalization performance of mixup. In particular, we compare ERM and\nmixup training for: PreAct ResNet-18 (He et al., 2016) as implemented in (Liu, 2017), WideResNet-\n28-10 (Zagoruyko & Komodakis, 2016a) as implemented in (Zagoruyko & Komodakis, 2016b), and\nDenseNet (Huang et al., 2017) as implemented in (Veit, 2017). For DenseNet, we change the growth\nrate to 40 to follow the DenseNet-BC-190 speciﬁcation from (Huang et al., 2017). For mixup, we\nﬁx α = 1, which results in interpolations λ uniformly distributed between zero and one. All models\nare trained on a single Nvidia Tesla P100 GPU using PyTorch2 for 200 epochs on the training set\nwith 128 examples per minibatch, and evaluated on the test set. Learning rates start at 0.1 and are\ndivided by 10 after 100 and 150 epochs for all models except WideResNet. For WideResNet, we\nfollow (Zagoruyko & Komodakis, 2016a) and divide the learning rate by 10 after 60, 120 and 180\nepochs. Weight decay is set to 10−4. We do not use dropout in these experiments.\nWe summarize our results in Figure 3a. In both CIFAR-10 and CIFAR-100 classiﬁcation problems,\nthe models trained using mixup signiﬁcantly outperform their analogues trained with ERM. As seen\nin Figure 3b, mixup and ERM converge at a similar speed to their best test errors. Note that the\nDenseNet models in (Huang et al., 2017) were trained for 300 epochs with further learning rate\ndecays scheduled at the 150 and 225 epochs, which may explain the discrepancy the performance of\nDenseNet reported in Figure 3a and the original result of Huang et al. (2017).\n3.3\nSPEECH DATA\nNext, we perform speech recognition experiments using the Google commands dataset (Warden,\n2017). The dataset contains 65,000 utterances, where each utterance is about one-second long and\nbelongs to one out of 30 classes. The classes correspond to voice commands such as yes, no, down,\nleft, as pronounced by a few thousand different speakers. To preprocess the utterances, we ﬁrst\n1https://caffe2.ai\n2http://pytorch.org\n5\n\n\nPublished as a conference paper at ICLR 2018\nModel\nMethod\nValidation set\nTest set\nLeNet\nERM\n9.8\n10.3\nmixup (α = 0.1)\n10.1\n10.8\nmixup (α = 0.2)\n10.2\n11.3\nVGG-11\nERM\n5.0\n4.6\nmixup (α = 0.1)\n4.0\n3.8\nmixup (α = 0.2)\n3.9\n3.4\nFigure 4: Classiﬁcation errors of ERM and mixup on the Google commands dataset.\nextract normalized spectrograms from the original waveforms at a sampling rate of 16 kHz. Next, we\nzero-pad the spectrograms to equalize their sizes at 160 × 101. For speech data, it is reasonable to\napply mixup both at the waveform and spectrogram levels. Here, we apply mixup at the spectrogram\nlevel just before feeding the data to the network.\nFor this experiment, we compare a LeNet (Lecun et al., 2001) and a VGG-11 (Simonyan & Zisserman,\n2015) architecture, each of them composed by two convolutional and two fully-connected layers.\nWe train each model for 30 epochs with minibatches of 100 examples, using Adam as the optimizer\n(Kingma & Ba, 2015). Training starts with a learning rate equal to 3 × 10−3 and is divided by 10\nevery 10 epochs. For mixup, we use a warm-up period of ﬁve epochs where we train the network on\noriginal training examples, since we ﬁnd it speeds up initial convergence. Table 4 shows that mixup\noutperforms ERM on this task, specially when using VGG-11, the model with larger capacity.\n3.4\nMEMORIZATION OF CORRUPTED LABELS\nFollowing Zhang et al. (2017), we evaluate the robustness of ERM and mixup models against randomly\ncorrupted labels. We hypothesize that increasing the strength of mixup interpolation α should generate\nvirtual examples further from the training examples, making memorization more difﬁcult to achieve.\nIn particular, it should be easier to learn interpolations between real examples compared to memorizing\ninterpolations involving random labels. We adapt an open-source implementation (Zhang, 2017)\nto generate three CIFAR-10 training sets, where 20%, 50%, or 80% of the labels are replaced by\nrandom noise, respectively. All the test labels are kept intact for evaluation. Dropout (Srivastava\net al., 2014) is considered the state-of-the-art method for learning with corrupted labels (Arpit et al.,\n2017). Thus, we compare in these experiments mixup, dropout, mixup + dropout, and ERM. For\nmixup, we choose α ∈{1, 2, 8, 32}; for dropout, we add one dropout layer in each PreAct block after\nthe ReLU activation layer between two convolution layers, as suggested in (Zagoruyko & Komodakis,\n2016a). We choose the dropout probability p ∈{0.5, 0.7, 0.8, 0.9}. For the combination of mixup\nand dropout, we choose α ∈{1, 2, 4, 8} and p ∈{0.3, 0.5, 0.7}. These experiments use the PreAct\nResNet-18 (He et al., 2016) model implemented in (Liu, 2017). All the other settings are the same as\nin Section 3.2.\nWe summarize our results in Table 2, where we note the best test error achieved during the training\nsession, as well as the ﬁnal test error after 200 epochs. To quantify the amount of memorization, we\nalso evaluate the training errors at the last epoch on real labels and corrupted labels. As the training\nprogresses with a smaller learning rate (e.g. less than 0.01), the ERM model starts to overﬁt the\ncorrupted labels. When using a large probability (e.g. 0.7 or 0.8), dropout can effectively reduce\noverﬁtting. mixup with a large α (e.g. 8 or 32) outperforms dropout on both the best and last epoch\ntest errors, and achieves lower training error on real labels while remaining resistant to noisy labels.\nInterestingly, mixup + dropout performs the best of all, showing that the two methods are compatible.\n3.5\nROBUSTNESS TO ADVERSARIAL EXAMPLES\nOne undesirable consequence of models trained using ERM is their fragility to adversarial exam-\nples (Szegedy et al., 2014). Adversarial examples are obtained by adding tiny (visually imperceptible)\nperturbations to legitimate examples in order to deteriorate the performance of the model. The adver-\nsarial noise is generated by ascending the gradient of the loss surface with respect to the legitimate\nexample. Improving the robustness to adversarial examples is a topic of active research.\n6\n\n\nPublished as a conference paper at ICLR 2018\nLabel corruption\nMethod\nTest error\nTraining error\nBest\nLast\nReal\nCorrupted\n20%\nERM\n12.7\n16.6\n0.05\n0.28\nERM + dropout (p = 0.7)\n8.8\n10.4\n5.26\n83.55\nmixup (α = 8)\n5.9\n6.4\n2.27\n86.32\nmixup + dropout (α = 4, p = 0.1)\n6.2\n6.2\n1.92\n85.02\n50%\nERM\n18.8\n44.6\n0.26\n0.64\nERM + dropout (p = 0.8)\n14.1\n15.5\n12.71\n86.98\nmixup (α = 32)\n11.3\n12.7\n5.84\n85.71\nmixup + dropout (α = 8, p = 0.3)\n10.9\n10.9\n7.56\n87.90\n80%\nERM\n36.5\n73.9\n0.62\n0.83\nERM + dropout (p = 0.8)\n30.9\n35.1\n29.84\n86.37\nmixup (α = 32)\n25.3\n30.9\n18.92\n85.44\nmixup + dropout (α = 8, p = 0.3)\n24.0\n24.8\n19.70\n87.67\nTable 2: Results on the corrupted label experiments for the best models.\nMetric\nMethod\nFGSM\nI-FGSM\nTop-1\nERM\n90.7\n99.9\nmixup\n75.2\n99.6\nTop-5\nERM\n63.1\n93.4\nmixup\n49.1\n95.8\n(a) White box attacks.\nMetric\nMethod\nFGSM\nI-FGSM\nTop-1\nERM\n57.0\n57.3\nmixup\n46.0\n40.9\nTop-5\nERM\n24.8\n18.1\nmixup\n17.4\n11.8\n(b) Black box attacks.\nTable 3: Classiﬁcation errors of ERM and mixup models when tested on adversarial examples.\nAmong the several methods aiming to solve this problem, some have proposed to penalize the norm of\nthe Jacobian of the model to control its Lipschitz constant (Drucker & Le Cun, 1992; Cisse et al., 2017;\nBartlett et al., 2017; Hein & Andriushchenko, 2017). Other approaches perform data augmentation\nby producing and training on adversarial examples (Goodfellow et al., 2015). Unfortunately, all\nof these methods add signiﬁcant computational overhead to ERM. Here, we show that mixup can\nsigniﬁcantly improve the robustness of neural networks without hindering the speed of ERM by\npenalizing the norm of the gradient of the loss w.r.t a given input along the most plausible directions\n(e.g. the directions to other training points). Indeed, Figure 2 shows that mixup results in models\nhaving a smaller loss and gradient norm between examples compared to vanilla ERM.\nTo assess the robustness of mixup models to adversarial examples, we use three ResNet-101 models:\ntwo of them trained using ERM on ImageNet-2012, and the third trained using mixup. In the ﬁrst\nset of experiments, we study the robustness of one ERM model and the mixup model against white\nbox attacks. That is, for each of the two models, we use the model itself to generate adversarial\nexamples, either using the Fast Gradient Sign Method (FGSM) or the Iterative FGSM (I-FGSM)\nmethods (Goodfellow et al., 2015), allowing a maximum perturbation of ϵ = 4 for every pixel. For\nI-FGSM, we use 10 iterations with equal step size. In the second set of experiments, we evaluate\nrobustness against black box attacks. That is, we use the ﬁrst ERM model to produce adversarial\nexamples using FGSM and I-FGSM. Then, we test the robustness of the second ERM model and the\nmixup model to these examples. The results of both settings are summarized in Table 3.\nFor the FGSM white box attack, the mixup model is 2.7 times more robust than the ERM model in\nterms of Top-1 error. For the FGSM black box attack, the mixup model is 1.25 times more robust\nthan the ERM model in terms of Top-1 error. Also, while both mixup and ERM are not robust to\nwhite box I-FGSM attacks, mixup is about 40% more robust than ERM in the black box I-FGSM\nsetting. Overall, mixup produces neural networks that are signiﬁcantly more robust than ERM against\nadversarial examples in white box and black settings without additional overhead compared to ERM.\n7\n\n\nPublished as a conference paper at ICLR 2018\nDataset\nERM\nmixup\nAbalone\n74.0\n73.6\nArcene\n57.6\n48.0\nArrhythmia\n56.6\n46.3\nDataset\nERM\nmixup\nHtru2\n2.0\n2.0\nIris\n21.3\n17.3\nPhishing\n16.3\n15.2\nTable 4: ERM and mixup classiﬁcation errors on the UCI datasets.\nERM GAN\nmixup GAN (α = 0.2)\nFigure 5: Effect of mixup on stabilizing GAN training at iterations 10, 100, 1000, 10000, and 20000.\n3.6\nTABULAR DATA\nTo further explore the performance of mixup on non-image data, we performed a series of experiments\non six arbitrary classiﬁcation problems drawn from the UCI dataset (Lichman, 2013). The neural\nnetworks in this section are fully-connected, and have two hidden layers of 128 ReLU units. The\nparameters of these neural networks are learned using Adam (Kingma & Ba, 2015) with default\nhyper-parameters, over 10 epochs of mini-batches of size 16. Table 4 shows that mixup improves the\naverage test error on four out of the six considered datasets, and never underperforms ERM.\n3.7\nSTABILIZATION OF GENERATIVE ADVERSARIAL NETWORKS (GANS)\nGenerative Adversarial Networks, also known as GANs (Goodfellow et al., 2014), are a powerful\nfamily of implicit generative models. In GANs, a generator and a discriminator compete against\neach other to model a distribution P. On the one hand, the generator g competes to transform noise\nvectors z ∼Q into fake samples g(z) that resemble real samples x ∼P. On the other hand, the\ndiscriminator competes to distinguish between real samples x and fake samples g(z). Mathematically,\ntraining a GAN is equivalent to solving the optimization problem\nmax\ng\nmin\nd\nE\nx,z ℓ(d(x), 1) + ℓ(d(g(z)), 0),\nwhere ℓis the binary cross entropy loss. Unfortunately, solving the previous min-max equation is a\nnotoriously difﬁcult optimization problem (Goodfellow, 2016), since the discriminator often provides\nthe generator with vanishing gradients. We argue that mixup should stabilize GAN training because it\nacts as a regularizer on the gradients of the discriminator, akin to the binary classiﬁer in Figure 1b.\nThen, the smoothness of the discriminator guarantees a stable source of gradient information to the\ngenerator. The mixup formulation of GANs is:\nmax\ng\nmin\nd\nE\nx,z,λ ℓ(d(λx + (1 −λ)g(z)), λ).\nFigure 5 illustrates the stabilizing effect of mixup the training of GAN (orange samples) when\nmodeling two toy datasets (blue samples). The neural networks in these experiments are fully-\nconnected and have three hidden layers of 512 ReLU units. The generator network accepts two-\ndimensional Gaussian noise vectors. The networks are trained for 20,000 mini-batches of size\n128 using the Adam optimizer with default parameters, where the discriminator is trained for ﬁve\niterations before every generator iteration. The training of mixup GANs seems promisingly robust to\nhyper-parameter and architectural choices.\n8\n\n\nPublished as a conference paper at ICLR 2018\nMethod\nSpeciﬁcation\nModiﬁed\nWeight decay\nInput\nTarget\n10−4\n5 × 10−4\nERM\n\u0017\n\u0017\n5.53\n5.18\nmixup\nAC + RP\n\u0013\n\u0013\n4.24\n4.68\nAC + KNN\n\u0013\n\u0013\n4.98\n5.26\nmix labels and latent\nLayer 1\n\u0013\n\u0013\n4.44\n4.51\nrepresentations\nLayer 2\n\u0013\n\u0013\n4.56\n4.61\n(AC + RP)\nLayer 3\n\u0013\n\u0013\n5.39\n5.55\nLayer 4\n\u0013\n\u0013\n5.95\n5.43\nLayer 5\n\u0013\n\u0013\n5.39\n5.15\nmix inputs only\nSC + KNN (Chawla et al., 2002)\n\u0013\n\u0017\n5.45\n5.52\nAC + KNN\n\u0013\n\u0017\n5.43\n5.48\nSC + RP\n\u0013\n\u0017\n5.23\n5.55\nAC + RP\n\u0013\n\u0017\n5.17\n5.72\nlabel smoothing\nϵ = 0.05\n\u0017\n\u0013\n5.25\n5.02\n(Szegedy et al., 2016)\nϵ = 0.1\n\u0017\n\u0013\n5.33\n5.17\nϵ = 0.2\n\u0017\n\u0013\n5.34\n5.06\nmix inputs +\nϵ = 0.05\n\u0013\n\u0013\n5.02\n5.40\nlabel smoothing\nϵ = 0.1\n\u0013\n\u0013\n5.08\n5.09\n(AC + RP)\nϵ = 0.2\n\u0013\n\u0013\n4.98\n5.06\nϵ = 0.4\n\u0013\n\u0013\n5.25\n5.39\nadd Gaussian noise\nσ = 0.05\n\u0013\n\u0017\n5.53\n5.04\nto inputs\nσ = 0.1\n\u0013\n\u0017\n6.41\n5.86\nσ = 0.2\n\u0013\n\u0017\n7.16\n7.24\nTable 5: Results of the ablation studies on the CIFAR-10 dataset. Reported are the median test errors\nof the last 10 epochs. A tick (\u0013) means the component is different from standard ERM training,\nwhereas a cross (\u0017) means it follows the standard training practice. AC: mix between all classes. SC:\nmix within the same class. RP: mix between random pairs. KNN: mix between k-nearest neighbors\n(k=200). Please refer to the text for details about the experiments and interpretations.\n3.8\nABLATION STUDIES\nmixup is a data augmentation method that consists of only two parts: random convex combination of\nraw inputs, and correspondingly, convex combination of one-hot label encodings. However, there are\nseveral design choices to make. For example, on how to augment the inputs, we could have chosen\nto interpolate the latent representations (i.e. feature maps) of a neural network, and we could have\nchosen to interpolate only between the nearest neighbors, or only between inputs of the same class.\nWhen the inputs to interpolate come from two different classes, we could have chosen to assign a\nsingle label to the synthetic input, for example using the label of the input that weights more in the\nconvex combination. To compare mixup with these alternative possibilities, we run a set of ablation\nstudy experiments using the PreAct ResNet-18 architecture on the CIFAR-10 dataset.\nSpeciﬁcally, for each of the data augmentation methods, we test two weight decay settings (10−4\nwhich works well for mixup, and 5 × 10−4 which works well for ERM). All the other settings and\nhyperparameters are the same as reported in Section 3.2.\nTo compare interpolating raw inputs with interpolating latent representations, we test on random\nconvex combination of the learned representations before each residual block (denoted Layer 1-4)\nor before the uppermost “average pooling + fully connected” layer (denoted Layer 5). To compare\nmixing random pairs of inputs (RP) with mixing nearest neighbors (KNN), we ﬁrst compute the 200\nnearest neighbors for each training sample, either from the same class (SC) or from all the classes\n(AC). Then during training, for each sample in a minibatch, we replace the sample with a synthetic\nsample by convex combination with a random draw from its nearest neighbors. To compare mixing\nall the classes (AC) with mixing within the same class (SC), we convex combine a minibatch with a\n9\n\n\nPublished as a conference paper at ICLR 2018\nrandom permutation of its sample index, where the permutation is done in a per-batch basis (AC) or a\nper-class basis (SC). To compare mixing inputs and labels with mixing inputs only, we either use a\nconvex combination of the two one-hot encodings as the target, or select the one-hot encoding of the\ncloser training sample as the target. For label smoothing, we follow Szegedy et al. (2016) and use\nϵ\n10 as the target for incorrect classes, and 1 −9ϵ\n10 as the target for the correct class.Adding Gaussian\nnoise to inputs is used as another baseline. We report the median test errors of the last 10 epochs.\nResults are shown in Table 5.\nFrom the ablation study experiments, we have the following observations. First, mixup is the best\ndata augmentation method we test, and is signiﬁcantly better than the second best method (mix input\n+ label smoothing). Second, the effect of regularization can be seen by comparing the test error with a\nsmall weight decay (10−4) with a large one (5 × 10−4). For example, for ERM a large weight decay\nworks better, whereas for mixup a small weight decay is preferred, conﬁrming its regularization effects.\nWe also see an increasing advantage of large weight decay when interpolating in higher layers of latent\nrepresentations, indicating decreasing strength of regularization. Among all the input interpolation\nmethods, mixing random pairs from all classes (AC + RP) has the strongest regularization effect.\nLabel smoothing and adding Gaussian noise have a relatively small regularization effect. Finally,\nwe note that the SMOTE algorithm (Chawla et al., 2002) does not lead to a noticeable gain in\nperformance.\n4\nRELATED WORK\nData augmentation lies at the heart of all successful applications of deep learning, ranging from image\nclassiﬁcation (Krizhevsky et al., 2012) to speech recognition (Graves et al., 2013; Amodei et al.,\n2016). In all cases, substantial domain knowledge is leveraged to design suitable data transformations\nleading to improved generalization. In image classiﬁcation, for example, one routinely uses rotation,\ntranslation, cropping, resizing, ﬂipping (Lecun et al., 2001; Simonyan & Zisserman, 2015), and\nrandom erasing (Zhong et al., 2017) to enforce visually plausible invariances in the model through\nthe training data. Similarly, in speech recognition, noise injection is a prevalent practice to improve\nthe robustness and accuracy of the trained models (Amodei et al., 2016).\nMore related to mixup, Chawla et al. (2002) propose to augment the rare class in an imbalanced\ndataset by interpolating the nearest neighbors; DeVries & Taylor (2017) show that interpolation and\nextrapolation the nearest neighbors of the same class in feature space can improve generalization.\nHowever, their proposals only operate among the nearest neighbors within a certain class at the\ninput / feature level, and hence does not account for changes in the corresponding labels. Recent\napproaches have also proposed to regularize the output distribution of a neural network by label\nsmoothing (Szegedy et al., 2016), or penalizing high-conﬁdence softmax distributions (Pereyra et al.,\n2017). These methods bear similarities with mixup in the sense that supervision depends on multiple\nsmooth labels, rather than on single hard labels as in traditional ERM. However, the label smoothing\nin these works is applied or regularized independently from the associated feature values.\nmixup enjoys several desirable aspects of previous data augmentation and regularization schemes\nwithout suffering from their drawbacks. Like the method of DeVries & Taylor (2017), it does not\nrequire signiﬁcant domain knowledge. Like label smoothing, the supervision of every example is not\noverly dominated by the ground-truth label. Unlike both of these approaches, the mixup transformation\nestablishes a linear relationship between data augmentation and the supervision signal. We believe\nthat this leads to a strong regularizer that improves generalization as demonstrated by our experiments.\nThe linearity constraint, through its effect on the derivatives of the function approximated, also relates\nmixup to other methods such as Sobolev training of neural networks (Czarnecki et al., 2017) or\nWGAN-GP (Gulrajani et al., 2017).\n5\nDISCUSSION\nWe have proposed mixup, a data-agnostic and straightforward data augmentation principle. We\nhave shown that mixup is a form of vicinal risk minimization, which trains on virtual examples\nconstructed as the linear interpolation of two random examples from the training set and their labels.\nIncorporating mixup into existing training pipelines reduces to a few lines of code, and introduces\nlittle or no computational overhead. Throughout an extensive evaluation, we have shown that mixup\n10\n\n\nPublished as a conference paper at ICLR 2018\nimproves the generalization error of state-of-the-art models on ImageNet, CIFAR, speech, and\ntabular datasets. Furthermore, mixup helps to combat memorization of corrupt labels, sensitivity to\nadversarial examples, and instability in adversarial training.\nIn our experiments, the following trend is consistent: with increasingly large α, the training error on\nreal data increases, while the generalization gap decreases. This sustains our hypothesis that mixup\nimplicitly controls model complexity. However, we do not yet have a good theory for understanding\nthe ‘sweet spot’ of this bias-variance trade-off. For example, in CIFAR-10 classiﬁcation we can\nget very low training error on real data even when α →∞(i.e., training only on averages of pairs\nof real examples), whereas in ImageNet classiﬁcation, the training error on real data increases\nsigniﬁcantly with α →∞. Based on our ImageNet and Google commands experiments with different\nmodel architectures, we conjecture that increasing the model capacity would make training error less\nsensitive to large α, hence giving mixup a more signiﬁcant advantage.\nmixup also opens up several possibilities for further exploration. First, is it possible to make\nsimilar ideas work on other types of supervised learning problems, such as regression and structured\nprediction? While generalizing mixup to regression problems is straightforward, its application\nto structured prediction problems such as image segmentation remains less obvious. Second, can\nsimilar methods prove helpful beyond supervised learning? The interpolation principle seems like a\nreasonable inductive bias which might also help in unsupervised, semi-supervised, and reinforcement\nlearning. Can we extend mixup to feature-label extrapolation to guarantee a robust model behavior\nfar away from the training data? Although our discussion of these directions is still speculative, we\nare excited about the possibilities mixup opens up, and hope that our observations will prove useful\nfor future development.\nACKNOWLEDGEMENTS\nWe would like to thank Priya Goyal, Yossi Adi and the PyTorch team. We also thank the Anonymous\nReview 2 for proposing the mixup + dropout experiments.\nREFERENCES\nD. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, Q. Cheng,\nG. Chen, et al. Deep speech 2: End-to-end speech recognition in English and Mandarin. In ICML, 2016.\nD. Arpit, S. Jastrzebski, N. Ballas, D. Krueger, E. Bengio, M. S. Kanwal, T. Maharaj, A. Fischer, A. Courville,\nY. Bengio, et al. A closer look at memorization in deep networks. ICML, 2017.\nP. Bartlett, D. J. Foster, and M. Telgarsky. Spectrally-normalized margin bounds for neural networks. NIPS,\n2017.\nO. Chapelle, J. Weston, L. Bottou, and V. Vapnik. Vicinal risk minimization. NIPS, 2000.\nN. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. SMOTE: synthetic minority over-sampling\ntechnique. Journal of artiﬁcial intelligence research, 16:321–357, 2002.\nC. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn, and T. Robinson. One billion word benchmark\nfor measuring progress in statistical language modeling. arXiv, 2013.\nM. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, and N. Usunier. Parseval networks: Improving robustness to\nadversarial examples. ICML, 2017.\nW. M. Czarnecki, S. Osindero, M. Jaderberg, G. ´Swirszcz, and R. Pascanu. Sobolev training for neural networks.\nNIPS, 2017.\nT. DeVries and G. W. Taylor. Dataset augmentation in feature space. ICLR Workshops, 2017.\nH. Drucker and Y. Le Cun. Improving generalization performance using double backpropagation. IEEE\nTransactions on Neural Networks, 3(6):991–997, 1992.\nI. Goodfellow. Tutorial: Generative adversarial networks. NIPS, 2016.\nI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio.\nGenerative adversarial nets. NIPS, 2014.\n11\n\n\nPublished as a conference paper at ICLR 2018\nI. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. ICLR, 2015.\nP. Goyal, P. Doll´ar, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He. Accurate,\nlarge minibatch SGD: Training ImageNet in 1 hour. arXiv, 2017.\nA. Graves, A.-r. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In ICASSP.\nIEEE, 2013.\nI. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. Improved training of Wasserstein GANs.\nNIPS, 2017.\nN. Harvey, C. Liaw, and A. Mehrabian. Nearly-tight VC-dimension bounds for piecewise linear neural networks.\nJMLR, 2017.\nK. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. ECCV, 2016.\nM. Hein and M. Andriushchenko. Formal guarantees on the robustness of a classiﬁer against adversarial\nmanipulation. NIPS, 2017.\nG. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N.\nSainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four\nresearch groups. IEEE Signal Processing Magazine, 2012.\nG. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. CVPR,\n2017.\nD. Kingma and J. Ba. Adam: A method for stochastic optimization. ICLR, 2015.\nA. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classiﬁcation with deep convolutional neural networks.\nNIPS, 2012.\nY. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.\nProceedings of IEEE, 2001.\nM. Lichman. UCI machine learning repository, 2013.\nK. Liu, 2017. URL https://github.com/kuangliu/pytorch-cifar.\nG. Pereyra, G. Tucker, J. Chorowski, Ł. Kaiser, and G. Hinton. Regularizing neural networks by penalizing\nconﬁdent output distributions. ICLR Workshops, 2017.\nO. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\nA. C. Berg, and L. Fei-Fei. ImageNet large scale visual recognition challenge. IJCV, 2015.\nD. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou,\nV. Panneershelvam, M. Lanctot, et al. Mastering the game of Go with deep neural networks and tree search.\nNature, 2016.\nP. Simard, Y. LeCun, J. Denker, and B. Victorri. Transformation invariance in pattern recognitiontangent distance\nand tangent propagation. Neural networks: tricks of the trade, 1998.\nK. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. ICLR,\n2015.\nJ. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller. Striving for simplicity: The all convolutional\nnet. ICLR Workshops, 2015.\nN. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple way to\nprevent neural networks from overﬁtting. Journal of Machine Learning Research, 15(1):1929–1958, 2014.\nC. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfellow, and R. Fergus. Intriguing properties\nof neural networks. ICLR, 2014.\nC. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the Inception architecture for computer\nvision. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.\nV. N. Vapnik. Statistical learning theory. J. Wiley, 1998.\nV. Vapnik and A. Y. Chervonenkis. On the uniform convergence of relative frequencies of events to their\nprobabilities. Theory of Probability and its Applications, 1971.\n12\n\n\nPublished as a conference paper at ICLR 2018\nA. Veit, 2017. URL https://github.com/andreasveit.\nP.\nWarden,\n2017.\nURL\nhttps://research.googleblog.com/2017/08/\nlaunching-speech-commands-dataset.html.\nS. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He. Aggregated residual transformations for deep neural networks.\nCVPR, 2016.\nS. Zagoruyko and N. Komodakis. Wide residual networks. BMVC, 2016a.\nS.\nZagoruyko\nand\nN.\nKomodakis,\n2016b.\nURL\nhttps://github.com/szagoruyko/\nwide-residual-networks.\nC. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking\ngeneralization. ICLR, 2017.\nC. Zhang, 2017. URL https://github.com/pluskid/fitting-random-labels.\nZ. Zhong, L. Zheng, G. Kang, S. Li, and Y. Yang. Random erasing data augmentation. arXiv, 2017.\n13\n"
}