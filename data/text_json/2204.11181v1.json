{
  "filename": "2204.11181v1.pdf",
  "num_pages": 19,
  "pages": [
    "Realistic Evaluation of Transductive\nFew-Shot Learning\nOlivier Veilleux ∗\nÉTS Montreal\nMalik Boudiaf *\nÉTS Montreal\nPablo Piantanida\nL2S, CentraleSupélec CNRS\nUniversité Paris-Saclay\nIsmail Ben Ayed\nÉTS Montreal\nAbstract\nTransductive inference is widely used in few-shot learning, as it leverages the statis-\ntics of the unlabeled query set of a few-shot task, typically yielding substantially bet-\nter performances than its inductive counterpart. The current few-shot benchmarks\nuse perfectly class-balanced tasks at inference. We argue that such an artiﬁcial reg-\nularity is unrealistic, as it assumes that the marginal label probability of the testing\nsamples is known and ﬁxed to the uniform distribution. In fact, in realistic scenar-\nios, the unlabeled query sets come with arbitrary and unknown label marginals. We\nintroduce and study the effect of arbitrary class distributions within the query sets\nof few-shot tasks at inference, removing the class-balance artefact. Speciﬁcally,\nwe model the marginal probabilities of the classes as Dirichlet-distributed random\nvariables, which yields a principled and realistic sampling within the simplex. This\nleverages the current few-shot benchmarks, building testing tasks with arbitrary\nclass distributions. We evaluate experimentally state-of-the-art transductive meth-\nods over 3 widely used data sets, and observe, surprisingly, substantial performance\ndrops, even below inductive methods in some cases. Furthermore, we propose\na generalization of the mutual-information loss, based on α-divergences, which\ncan handle effectively class-distribution variations. Empirically, we show that our\ntransductive α-divergence optimization outperforms state-of-the-art methods across\nseveral data sets, models and few-shot settings. Our code is publicly available at\nhttps://github.com/oveilleux/Realistic_Transductive_Few_Shot.\n1\nIntroduction\nDeep learning models are widely dominating the ﬁeld. However, their outstanding performances are\noften built upon training on large-scale labeled data sets, and the models are seriously challenged\nwhen dealing with novel classes that were not seen during training. Few-shot learning [1, 2, 3] tackles\nthis challenge, and has recently triggered substantial interest within the community. In standard\nfew-shot settings, a model is initially trained on large-scale data containing labeled examples from a\nset of base classes. Then, supervision for a new set of classes, which are different from those seen in\nthe base training, is restricted to just one or a few labeled samples per class. Model generalization\nis evaluated over few-shot tasks. Each task includes a query set containing unlabeled samples for\nevaluation, and is supervised by a support set containing a few labeled samples per new class.\nThe recent few-shot classiﬁcation literature is abundant and widely dominated by convoluted meta-\nlearning and episodic-training strategies. To simulate generalization challenges at test times, such\nstrategies build sequences of artiﬁcially balanced few-shot tasks (or episodes) during base training,\neach containing both query and support samples. Widely adopted methods within this paradigm\ninclude: Prototypical networks [4], which optimizes the log-posteriors of the query points within\neach base-training episode; Matching networks [3], which expresses the predictions of query points\n∗Equal contributions, corresponding authors: {olivier.veilleux.2, malik.boudiaf.1}@ens.etsmtl.ca\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2204.11181v1  [cs.LG]  24 Apr 2022\n",
    "as linear functions of the support labels, while deploying episodic training and memory architectures;\nMAML (Model-Agnostic Meta-Learning) [5], which encourages a model to be “easy” to ﬁne-tune;\nand the meta-learner in [6], which prescribes optimization as a model for few-shot learning. These\npopular methods have recently triggered a large body of few-shot learning literature, for instance,\n[7, 8, 9, 10, 11, 12, 13], to list a few.\nRecently, a large body of works investigated transductive inference for few-shot tasks, e.g., [11, 14,\n12, 15, 16, 17, 18, 19, 20, 21, 22, 23], among many others, showing substantial improvements in\nperformances over inductive inference2. Also, as discussed in [24], most meta-learning approches\nrely critically on transductive batch normalization (TBN) to achieve competitive performances, for\ninstance, the methods in [5, 25, 26], among others. Adopted initially in the widely used MAML\n[5], TBN performs normalization using the statistics of the query set of a given few-shot task, and\nyields signiﬁcant increases in performances [24]. Therefore, due to the popularity of MAML, several\nmeta-learning techniques have used TBN. The transductive setting is appealing for few-shot learning,\nand the outstanding performances observed recently resonate well with a well-known fact in classical\ntransductive inference [27, 28, 29]: On small labeled data sets, transductive inference outperforms its\ninductive counterpart. In few-shot learning, transductive inference has access to exactly the same\ntraining and testing data as its inductive counterpart3. The difference is that it classiﬁes all the\nunlabeled query samples of each single few-shot task jointly, rather than one sample at a time.\nThe current few-shot benchmarks use perfectly class-balanced tasks at inference: For each task used\nat testing, all the classes have exactly the same number of samples, i.e., the marginal probability of\nthe classes is assumed to be known and ﬁxed to the uniform distribution across all tasks. This may\nnot reﬂect realistic scenarios, in which testing tasks might come with arbitrary class proportions.\nFor instance, the unlabeled query set of a task could be highly imbalanced. In fact, using perfectly\nbalanced query sets for benchmarking the models assumes exact knowledge of the marginal distri-\nbutions of the true labels of the testing points, but such labels are unknown. This is, undeniably, an\nunrealistic assumption and an important limitation of the current few-shot classiﬁcation benchmarks\nand datasets. Furthermore, this suggests that the recent progress in performances might be, in part,\ndue to class-balancing priors (or biases) that are encoded in state-of-the-art transductive models. Such\npriors might be implicit, e.g., through carefully designed episodic-training schemes and specialized\narchitectures, or explicit, e.g., in the design of transductive loss functions and constraints. For\ninstance, the best performing methods in [23, 31] use explicit label-marginal terms or constraints,\nwhich strongly enforce perfect class balance within the query set of each task. In practice, those\nclass-balance priors and assumptions may limit the applicability of the existing few-shot benchmarks\nand methods. In fact, our experiments show that, over few-shot tasks with random class balance, the\nperformances of state-of-the-art methods may decrease by margins. This motivates re-considering the\nexisting benchmarks and re-thinking the relevance of class-balance biases in state-of-the-art methods.\nContributions\nWe introduce and study the effect of arbitrary class distributions within the query\nsets of few-shot tasks at inference. Speciﬁcally, we relax the assumption of perfectly balanced query\nsets and model the marginal probabilities of the classes as Dirichlet-distributed random variables. We\ndevise a principled procedure for sampling simplex vectors from the Dirichlet distribution, which is\nwidely used in Bayesian statistics for modeling categorical events. This leverages the current few-shot\nbenchmarks by generating testing tasks with arbitrary class distributions, thereby reﬂecting realistic\nscenarios. We evaluate experimentally state-of-the-art transductive few-shot methods over 3 widely\nused datasets, and observe that the performances decrease by important margins, albeit at various\ndegrees, when dealing with arbitrary class distributions. In some cases, the performances drop even\nbelow the inductive baselines, which are not affected by class-distribution variations (as they do not\nuse the query-set statistics). Furthermore, we propose a generalization of the transductive mutual-\ninformation loss, based on α-divergences, which can handle effectively class-distribution variations.\nEmpirically, we show that our transductive α-divergence optimization outperforms state-of-the-art\nfew-shot methods across different data sets, models and few-shot settings.\n2The best-performing state-of-the-art few-shot methods in the transductive-inference setting have achieved\nperformances that are up to 10% higher than their inductive counterparts; see [23], for instance.\n3Each single few-shot task is treated independently of the other tasks in the transductive-inference setting.\nHence, the setting does not use additional unlabeled data, unlike semi-supervised few-shot learning [30].\n2\n",
    "2\nStandard few-shot settings\nBase training\nAssume that we have access to a fully labelled base dataset Dbase = {xi, yi}Nbase\ni=1\n,\nwhere xi ∈Xbase are data points in an input space Xbase, yi ∈{0, 1}|Ybase| the one-hot labels, and\nYbase the set of base classes. Base training learns a feature extractor fφ : X →Z, with φ its learnable\nparameters and Z a (lower-dimensional) feature space. The vast majority of the literature adopts\nepisodic training at this stage, which consists in formatting Dbase as a series of tasks (=episodes)\nin order to mimic the testing stage, and train a meta-learner to produce, through a differentiable\nprocess, predictions for the query set. However, it has been repeatedly demonstrated over the last\ncouple years that a standard supervised training followed by standard transfer learning strategies\nactually outperforms most meta-learning based approaches [32, 33, 34, 20, 23]. Therefore, we adopt\na standard cross-entropy training in this work.\nTesting\nThe model is evaluated on a set of few-shot tasks, each formed with samples from\nDtest = {xi, yi}Ntest\ni=1 , where yi ∈{0, 1}|Ytest| such that Ybase ∩Ytest = ∅. Each task is composed\nof a labelled support set S = {xi, yi}i∈IS and an unlabelled query set Q = {xi}i∈IQ, both containing\ninstances only from K distinct classes randomly sampled from Ytest, with K < |Ytest|. Leveraging\na feature extractor fφ pre-trained on the base data, the objective is to learn, for each few-shot task, a\nclassiﬁer fW : Z →∆K, with W the learnable parameters and ∆K = {y ∈[0, 1]K / P\nk yk = 1}\nthe (K −1)-simplex. To simplify the equations for the rest of the paper, we use the following\nnotations for the posterior predictions of each i ∈IS ∪IQ and for the class marginals within Q:\npik = fW (fφ(xi))k = P(Y = k|X = xi; W , φ) and bpk =\n1\n|IQ|\nX\ni∈IQ\npik = P(YQ = k; W , φ),\nwhere X and Y are the random variables associated with the raw features and labels, respectively;\nXQ and YQ means restriction of the random variable to set Q. The end goal is to predict the classes of\nthe unlabeled samples in Q for each few-shot task, independently of the other tasks. A large body of\nworks followed a transductive-prediction setting, e.g., [11, 14, 12, 15, 16, 17, 18, 19, 20, 21, 22, 23],\namong many others. Transductive inference performs a joint prediction for all the unlabeled query\nsamples of each single few-shot task, thereby leveraging the query-set statistics. On the current\nbenchmarks, tranductive inference often outperforms substantially its inductive counterpart (i.e.,\nclassifying one sample at a time for a given task). Note that our method is agnostic to the speciﬁc\nchoice of classiﬁerfW , whose parameters are learned at inference. In the experimental evaluations\nof our method, similarly to [23], we used pik ∝exp(−τ\n2 ∥wk −zi∥2), with W := (w1, . . . , wK),\nzi =\nfφ(xi)\n∥fφ(xi)∥2 , τ is a temperature parameter and base-training parameters φ are ﬁxed4.\nPerfectly balanced vs imbalanced tasks\nIn standard K-way few-shot settings, the support and\nquery sets of each task T are formed using the following procedure: (i) Randomly sample K\nclasses YT ⊂Ytest; (ii) For each class k ∈YT , randomly sample nS\nk support examples, such that\nnS\nk = |S|/K; and (iii) For each class k ∈YT , randomly sample nQ\nk query examples, such that\nnQ\nk = |Q|/K. Such a setting is undeniably artiﬁcial as we assume S and Q have the same perfectly\nbalanced class distribution. Several recent works [35, 36, 37, 38] studied class imbalance exclusively\non the support set S. This makes sense as, in realistic scenarios, some classes might have more\nlabelled samples than others. However, even these works rely on the assumption that query set Q is\nperfectly balanced. We argue that such an assumption is not realistic, as one typically has even less\ncontrol over the class distribution of Q than it has over that of S. For the labeled support S, the class\ndistribution is at least fully known and standard strategies from imbalanced supervised learning could\nbe applied [38]. This does not hold for Q, for which we need to make class predictions at testing time\nand whose class distribution is unknown. In fact, generating perfectly balanced tasks at test times for\nbenchmarking the models assumes that one has access to the unknown class distributions of the query\npoints, which requires access to their unknown labels. More importantly, artiﬁcial balancing of Q is\nimplicitly or explicitly encoded in several transductive methods, which use the query set statistics to\nmake class predictions, as will be discussed in section 4.\n4φ is either ﬁxed, e.g., [23], or ﬁne-tuned during inference, e.g., [15]. There is, however, evidence in the\nliterature that freezing φ yields better performances [23, 32, 34, 33], while reducing the inference time.\n3\n",
    "a =(0.5, 0.5, 0.5)\na =(2, 2, 2)\na =(5, 5, 5)\na =(50, 50, 50)\nLow density\nHigh density\nFigure 1: Dirichlet density function for K = 3, with different choices of parameter vector a.\n3\nDirichlet-distributed class marginals for few-shot query sets\nStandard few-shot settings assume that pk, the proportion of the query samples belonging to a class\nk within a few-shot task, is deterministic (ﬁxed) and known priori: pk = nQ\nk /|Q| = 1/K, for all\nk and all few-shot tasks. We propose to relax this unrealistic assumption, and to use the Dirichlet\ndistribution to model the proportions (or marginal probabilities) of the classes in few-shot query sets\nas random variables. Dirichlet distributions are widely used in Bayesian statistics to model K-way\ncategorical events5. The domain of the Dirichlet distribution is the set of K-dimensional discrete\ndistributions, i.e., the set of vectors in (K −1)-simplex ∆K = {p ∈[0, 1]K | P\nk pk = 1}. Let Pk\ndenotes a random variable associated with class probability pk, and P the random simplex vector\ngiven by P = (P1, . . . , PK). We assume that P follows a Dirichlet distribution with parameter vector\na = (a1, . . . , aK) ∈RK: P ∼Dir(a). The Dirichlet distribution has the following density function:\nfDir(p; a) =\n1\nB(a)\nQK\nk=1 pak−1\nk\nfor p = (p1, . . . , pK) ∈∆K, with B denoting the multivariate Beta\nfunction, which could be expressed with the Gamma function6: B(a) =\nQK\nk=1 Γ(ak)\nΓ(\nPK\nk=1 αk).\nFigure 1 illustrates the Dirichlet density for K = 3, with a 2-simplex support represented with an\nequilateral triangle, whose vertices are probability vectors (1, 0, 0), (0, 1, 0) and (0, 0, 1). We show\nthe density for a = a1K, with 1K the K-dimensional vector whose all components are equal to\n1 and concentration parameter a equal to 0.5, 2, 5 and 50. Note that the limiting case a →+∞\ncorresponds to the standard settings with perfectly balanced tasks, where only uniform distribution,\ni.e., the point in the middle of the simplex, could occur as marginal distribution of the classes.\nThe following result, well-known in the literature of random variate generation [39], suggests that one\ncould generate samples from the multivariate Dirichlet distribution via simple and standard univariate\nGamma generators.\nTheorem 3.1. ([39, p. 594]) Let N1, . . . , NK be K independent Gamma-distributed random vari-\nables with parameters ak: Nk ∼Gamma(ak), i.e., the probability density of Nk is univariate Gamma7,\nwith shape parameter ak. Let Pk =\nNk\nPK\nk=1 Nk , k = 1, . . . , K. Then, P = (P1, . . . , PK) is Dirichlet\ndistributed: P ∼Dir(a), with a = (a1, . . . , aK).\nA proof based on the Jacobian of random-variable transformations Pk =\nNk\nPK\nk=1 Nk , k = 1, . . . , K,\ncould be found in [39], p. 594. This result prescribes the following simple procedure for sampling\nrandom simplex vectors (p1, . . . , pK) from the multivariate Dirichlet distribution with parameters\na = (a1, . . . , aK): First, we draw K independent random samples (n1, . . . , nK) from Gamma\ndistributions, with each nk drawn from univariate density fGamma(n; ak); To do so, one could use\nstandard random generators for the univariate Gamma density; see Chapter 9 in [39]. Then, we set\npk =\nnk\nPK\nk=1 nk . This enables to generate randomly nQ\nk , the number of samples of class k within query\nset Q, as follows: nQ\nk is the closest integer to pk|Q| such that P\nk nQ\nk = |Q|.\n5Note that the Dirichlet distribution is the conjugate prior of the categorical and multinomial distributions.\n6The Gamma function is given by: Γ(a) =\nR ∞\n0\nta−1 exp(−t)dt for a > 0. Note that Γ(a) = (a −1)!\nwhen a is a strictly positive integer.\n7Univariate Gamma density is given by: fGamma(n; ak) = nak−1 exp(−n)\nΓ(ak)\n, n ∈R.\n4\n",
    "4\nOn the class-balance bias of the best-performing few-shot methods\nAs brieﬂy evoked in section 2, the strict balancing of the classes in both S and Q represents a\nstrong inductive bias, which few-shot methods can either meta-learn during training or leverage at\ninference. In this section, we explicitly show how such a class-balance prior is encoded in the two\nbest-performing transductive methods in the literature [23, 31], one based on mutual-information\nmaximization [23] and the other on optimal transport [31].\nClass-balance bias of optimal transport\nRecently, the transductive method in [31], referred to as\nPT-MAP, achieved the best performances reported in the literature on several popular benchmarks, to\nthe best of our knowledge. However, the method explicitly embeds a class-balance prior. Formally,\nthe objective is to ﬁnd, for each few-shot task, an optimal mapping matrix M ∈R|Q|×K\n+\n, which\ncould be viewed as a joint probability distribution over XQ × YQ. At inference, a hard constraint\nM ∈{M :\nM1K = r, 1|Q|M = c} for some r and c is enforced through the use of the\nSinkhorn-Knopp algorithm. In other words, the columns and rows of M are constrained to sum\nto pre-deﬁned vectors r ∈R|Q| and c ∈RK. Setting c =\n1\nK 1K as done in [31] ensures that M\ndeﬁnes a valid joint distribution, but also crucially encodes the strong prior that all the classes within\nthe query sets are equally likely. Such a hard constraint is detrimental to the performance in more\nrealistic scenarios where the class distributions of the query sets could be arbitrary, and not necessarily\nuniform. Unsurprisingly, PT-MAP undergoes a substantial performance drop in the realistic scenario\nwith Dirichlet-distributed class proportions, with a consistent decrease in accuracy between 18 and\n20 % on all benchmarks, in the 5-ways case.\nClass-balance bias of transductive mutual-information maximization\nLet us now have a closer\nlook at the mutual-information maximization in [23]. Following the notations introduced in section 2,\nthe transductive loss minimized in [23] for a given few-shot task reads:\nLTIM = CE −I(XQ; YQ) = CE −1\n|IQ|\nX\ni∈Q\nK\nX\nk=1\npik log(pik)\n|\n{z\n}\nH(YQ|XQ)\n+λ\nK\nX\nk=1\nbpk log bpk\n|\n{z\n}\n−H(YQ)\n,\n(1)\nwhere I(XQ; YQ) = −H(YQ|XQ) + λH(YQ) is a weighted mutual information between the\nquery samples and their unknown labels (the mutual information corresponds to λ = 1), and\nCE := −\n1\n|IS|\nP\ni∈S\nPK\nk=1 yik log(pik) is a supervised cross-entropy term deﬁned over the support\nsamples. Let us now focus our attention on the label-marginal entropy term, H(YQ). As mentioned in\n[23], this term is of signiﬁcant importance as it prevents trivial, single-class solutions stemming from\nminimizing only conditional entropy H(YQ|XQ). However, we argue that this term also encourages\nclass-balanced solutions. In fact, we can write it as an explicit KL divergence, which penalizes\ndeviation of the label marginals within a query set from the uniform distribution:\nH(YQ) = −\nK\nX\nk=1\nbpk log (bpk) = log(K) −DKL(bp∥uK).\n(2)\nTherefore, minimizing marginal entropy H(YQ) is equivalent to minimizing the KL divergence\nbetween the predicted marginal distribution bp = (bp1, . . . , bpK) and uniform distribution uK = 1\nK 1K.\nThis KL penalty could harm the performances whenever the class distribution of the few-shot task\nis no longer uniform. In line with this analysis, and unsurprisingly, we observe in section 6 that\nthe original model in [23] also undergoes a dramatic performance drop, up to 20%. While naively\nremoving this marginal-entropy term leads to even worse performances, we observe that simply\ndown-weighting it, i.e., decreasing λ in Eq. (1), can drastically alleviate the problem, in contrast to\nthe case of optimal transport where the class-balance constraint is enforced in a hard manner.\n5\nGeneralizing mutual information with α-divergences\nIn this section, we propose a non-trivial, but simple generalization of the mutual-information loss in\n(1), based on α-divergences, which can tolerate more effectively class-distribution variations. We\nidentiﬁed in section 4 a class-balance bias encoded in the marginal Shannon entropy term. Ideally,\n5\n",
    "0.0\n0.2\n0.4\n0.6\n0.8\n1.0\np\n0.0\n0.2\n0.4\n0.6\n(p)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\np\n0.2\n0.1\n0.0\n0.1\n0.2\n(p)\nl\n=1\n=2\n=3\n=5\n=7\n=9\n=11\nFigure 2: (Left) α-entropy as a function of p = σ(l). (Right) Gradient of α-entropy w.r.t to the logit\nl ∈R as a function of p = σ(l). Best viewed in color.\nwe would like to extend this Shannon-entropy term in a way that allows for more ﬂexibility: Our\npurpose is to control how far the predicted label-marginal distribution, bp, could depart from the\nuniform distribution without being heavily penalized.\n5.1\nBackground\nWe argue that such a ﬂexibility could be controlled through the use of α-divergences [40, 41, 42,\n43, 44], which generalize the well-known and widely used KL divergence. α-divergences form a\nwhole family of divergences, which encompasses Tsallis and Renyi α-divergences, among others.\nIn this work, we focus on Tsallis’s [40, 43] formulation of α-divergence. Let us ﬁrst introduce\nthe generalized logarithm [44]: logα(x) =\n1\n1−α\n\u0000x1−α −1\n\u0001\n. Using the latter, Tsallis α-divergence\nnaturally extends KL. For two discrete distributions p = (pk)K\nk=1 and q = (qk)K\nk=1, we have:\nDα(p∥q) = −\nK\nX\nk=1\npk logα\n\u0012qk\npk\n\u0013\n=\n1\n1 −α\n \n1 −\nK\nX\nk=1\npα\nkq1−α\nk\n!\n.\n(3)\nNote that the Shannon entropy in Eq. (2) elegantly generalizes to Tsallis α-entropy:\nHα(p) = logα(K) −K1−α Dα(p∥uK) =\n1\nα −1\n \n1 −\nX\nk\npα\nk\n!\n.\n(4)\nThe derivation of Eq. (4) is provided in appendix. Also, limα→1 logα(x) = log(x), which implies:\nlim\nα→1 Dα(p∥q) = DKL(p∥q)\nand\nlim\nα→1 Hα(p) = H(p) = −\nK\nX\nk=1\nbpk log (bpk) .\nNote that α-divergence Dα(p∥q) inherits the nice properties of the KL divergence, including but not\nlimited to convexity with respect to both p and q and strict positivity Dα(p∥q) ≥0 with equality if\np = q. Furthermore, beyond its link to the forward KL divergence DKL(p∥q), α-divergence smoothly\nconnects several well-known divergences, including the reverse KL divergence DKL(q∥p) (α →0),\nthe Hellinger (α = 0.5) and the Pearson Chi-square (α = 2) distances [44].\n5.2\nAnalysis of the gradients\nAs observed from Eq. (4), α-entropy is, just like Shannon Entropy, intrinsically biased toward\nthe uniform distribution. Therefore, we still have not properly answered the question: why would\nα-entropy be better suited to imbalanced situations? We argue the that learning dynamics subtly\nbut crucially differ. To illustrate this point, let us consider a simple toy logistic-regression example.\nLet l ∈R denotes a logit, and p = σ(l) the corresponding probability, where σ stands for the usual\nsigmoid function. The resulting probability distribution simply reads p = {p, 1 −p}. In Figure 2, we\nplot both the α-entropy Hα (left) and its gradients ∂Hα/∂l (right) as functions of p. The advantage\nof α-divergence now becomes clearer: as α increases, Hα(p) accepts more and more deviation from\nthe uniform distribution (p = 0.5 on Figure 2), while still providing a barrier preventing trivial\nsolutions (i.e., p = 0 or p = 1, which corresponds to all the samples predicted as 0 or 1). Intuitively,\nsuch a behavior makes α-entropy with α > 1 better suited to class imbalance than Shannon entropy.\n6\n",
    "5.3\nProposed formulation\nIn light of the previous discussions, we advocate a new α-mutual information loss, a simple but very\neffective extension of the Shannon mutual information in Eq. (1):\nIα(XQ; YQ) = Hα(YQ) −Hα(YQ|XQ) =\n1\nα −1\n\n1\n|IQ|\nX\ni∈IQ\nK\nX\nk=1\npα\nik −\nK\nX\nk=1\nbp α\nk\n\n\n(5)\nwith Hα the α-entropy as deﬁned in Eq. (4). Note that our generalization in Eq. (5) has no link to the\nα-mutual information derived in [45]. Finally, our loss for transductive few-shot inference reads:\nLα-TIM = CE −Iα(XQ; YQ).\n(6)\n6\nExperiments\nIn this section, we thoroughly evaluate the most recent few-shot transductive methods using our\nimbalanced setting. Except for SIB [16] and LR-ICI [17] all the methods have been reproduced in\nour common framework. All the experiments have been executed on a single GTX 1080 Ti GPU.\nDatasets\nWe use three standard benchmarks for few-shot classiﬁcation: mini-Imagenet [46], tiered-\nImagenet [30] and Caltech-UCSD Birds 200 (CUB) [47]. The mini-Imagenet benchmark is a subset\nof the ILSVRC-12 dataset [46], composed of 60,000 color images of size 84 x 84 pixels [3]. It\nincludes 100 classes, each having 600 images. In all experiments, we used the standard split of 64\nbase-training, 16 validation and 20 test classes [6, 33]. The tiered-Imagenet benchmark is a larger\nsubset of ILSVRC-12, with 608 classes and 779,165 color images of size 84 × 84 pixels. We used a\nstandard split of 351 base-training, 97 validation and 160 test classes. The Caltech-UCSD Birds 200\n(CUB) benchmark also contains images of size 84 × 84 pixels, with 200 classes. For CUB, we used a\nstandard split of 100 base-training, 50 validation and 50 test classes, as in [32]. It is important to note\nthat for all the splits and data-sets, the base-training, validation and test classes are all different.\nTask sampling\nWe evaluate all the methods in the 1-shot 5-way, 5-shot 5-way, 10-shot 5-way and\n20-shot 5-way scenarios, with the classes of the query sets randomly distributed following Dirichlet’s\ndistribution, as described in section 3. Note that the total amount of query samples |Q| remains ﬁxed\nto 75. All the methods are evaluated by the average accuracy over 10,000 tasks, following [33]. We\nused different Dirichlet’s concentration parameter a for validation and testing. The validation-task\ngeneration is based on a random sampling within the simplex (i.e Dirichlet with a = 1K). Testing-\ntask generation uses a = 2·1K to reﬂect the fact that extremely imbalanced tasks (i.e., only one class\nis present in the task) are unlikely to happen in practical scenarios; see Figure 1 for visualization.\nHyper-parameters\nUnless identiﬁed as directly linked to a class-balance bias, all the hyper-\nparameters are kept similar to the ones prescribed in the original papers of the reproduced methods.\nFor instance, the marginal entropy in TIM [23] was identiﬁed in section 4 as a penalty that encourages\nclass balance. Therefore, the weight controlling the relative importance of this term is tuned. For\nall methods, hyper-parameter tuning is performed on the validation set of each dataset, using the\nvalidation sampling described in the previous paragraph.\nBase-training procedure\nAll non-episodic methods use the same feature extractors, which are\ntrained using the same procedure as in [23, 20], via a standard cross-entropy minimization on the base\nclasses with label smoothing. The feature extractors are trained for 90 epochs, using a learning rate\ninitialized to 0.1 and divided by 10 at epochs 45 and 66. We use a batch size of 256 for ResNet-18\nand of 128 for WRN28-10. During training, color jitter, random croping and random horizontal\nﬂipping augmentations are applied. For episodic/meta-learning methods, given that each requires a\nspeciﬁc training, we use the pre-trained models provided with the GitHub repository of each method.\n6.1\nMain results\nThe main results are reported in Table 1. As baselines, we also report the performances of state-of-\nthe-art inductive methods that do not use the statistics of the query set at adaptation and are, therefore,\n7\n",
    "Table 1: Comparisons of state-of-the-art methods in our realistic setting on mini-ImageNet, tiered-\nImageNet and CUB. Query sets are sampled following a Dirichlet distribution with a = 2 · 1K.\nAccuracy is averaged over 10,000 tasks. A red arrow (↓) indicates a performance drop between the\nartiﬁcially-balanced setting and our testing procedure, and a blue arrow (↑) an improvement. Arrows\nare not displayed for the inductive methods as, for these, there is no signiﬁcant change in performance\nbetween both settings (expected). ‘–’ signiﬁes the result was computationally intractable to obtain.\nmini-ImageNet\nMethod\nNetwork\n1-shot\n5-shot\n10-shot\n20-shot\nInduct.\nProtonet (NEURIPS’17 [4])\nRN-18\n53.4\n74.2\n79.2\n82.4\nBaseline (ICLR’19 [32])\n56.0\n78.9\n83.2\n85.9\nBaseline++ (ICLR’19 [32])\n60.4\n79.7\n83.8\n86.3\nSimpleshot (ARXIV [33])\n63.0\n80.1\n84.0\n86.1\nTransduct.\nMAML (ICML’17 [5])\nRN-18\n47.6 (↓3.8)\n64.5 (↓5.0)\n66.2 (↓5.7)\n67.2 (↓3.6)\nVersa (ICLR’19 [25])\n47.8 (↓2.2)\n61.9 (↓3.7)\n65.6 (↓3.6)\n67.3 (↓4.0)\nEntropy-min (ICLR’20 [15])\n58.5 (↓5.1)\n74.8 (↓7.3)\n77.2 (↓8.0)\n79.3 (↓7.9)\nLR+ICI (CVPR’2020 [17])\n58.7 (↓8.1)\n73.5 (↓5.7)\n78.4 (↓2.7)\n82.1 (↓1.7)\nPT-MAP (ARXIV [31])\n60.1 (↓16.8)\n67.1 (↓18.2)\n68.8 (↓18.0)\n70.4 (↓17.4)\nLaplacianShot (ICML’20 [20])\n65.4 (↓4.7)\n81.6 (↓0.5)\n84.1 (↓0.2)\n86.0 (↑0.5)\nBD-CSPN (ECCV’20 [21])\n67.0 (↓2.4)\n80.2(↓1.8)\n82.9 (↓1.4)\n84.6 (↓1.1)\nTIM (NEURIPS’20 [23])\n67.3 (↓4.5)\n79.8 (↓4.1)\n82.3 (↓3.8)\n84.2 (↓3.7)\nα-TIM (ours)\n67.4\n82.5\n85.9\n87.9\nInduct.\nBaseline (ICLR’19 [32])\nWRN\n62.2\n81.9\n85.5\n87.9\nBaseline++ (ICLR’19 [32])\n64.5\n82.1\n85.7\n87.9\nSimpleshot (ARXIV [33])\n66.2\n82.4\n85.6\n87.4\nTransduct.\nEntropy-min (ICLR’20 [15])\nWRN\n60.4 (↓5.7)\n76.2 (↓8.0)\n–\n–\nPT-MAP (ARXIV [31])\n60.6 (↓18.3)\n66.8 (↓19.8)\n68.5 (↓19.3)\n69.9 (↓19.0)\nSIB (ICLR’20 [16])\n64.7 (↓5.3)\n72.5 (↓6.7)\n73.6 (↓8.4)\n74.2 (↓8.7)\nLaplacianShot (ICML’20 [20])\n68.1 (↓4.8)\n83.2 (↓0.6)\n85.9 (↑0.4)\n87.2 (↑0.6)\nTIM (NEURIPS’20 [23])\n69.8 (↓4.8)\n81.6 (↓4.3)\n84.2 (↓3.9)\n85.9 (↓3.7)\nBD-CSPN (ECCV’20 [21])\n70.4 (↓2.1)\n82.3(↓1.4)\n84.5 (↓1.4)\n85.7 (↓1.1)\nα-TIM (ours)\n69.8\n84.8\n87.9\n89.7\ntiered-ImageNet\n1-shot\n5-shot\n10-shot\n20-shot\nInduct.\nBaseline (ICLR’19 [32])\nRN-18\n63.5\n83.8\n87.3\n89.0\nBaseline++ (ICLR’19 [32])\n68.0\n84.2\n87.4\n89.2\nSimpleshot (ARXIV [33])\n69.6\n84.7\n87.5\n89.1\nTransduct.\nEntropy-min (ICLR’20 [15])\nRN-18\n61.2 (↓5.8)\n75.5 (↓7.6)\n78.0 (↓7.9)\n79.8 (↓7.9)\nPT-MAP (ARXIV [31])\n64.1 (↓18.8)\n70.0 (↓18.8)\n71.9 (↓17.8)\n73.4 (↓17.1)\nLaplacianShot (ICML’20 [20])\n72.3 (↓4.8)\n85.7 (↓0.5)\n87.9 (↓0.1)\n89.0 (↑0.3)\nBD-CSPN (ECCV’20 [21])\n74.1 (↓2.2)\n84.8 (↓1.4)\n86.7 (↓1.1)\n87.9 (↓0.8)\nTIM (NEURIPS’20 [23])\n74.1 (↓4.5)\n84.1 (↓3.6)\n86.0 (↓3.3)\n87.4 (↓3.1)\nLR+ICI (CVPR’20 [17])\n74.6 (↓6.2)\n85.1 (↓2.8)\n88.0 (↓2.1)\n90.2 (↓1.2)\nα-TIM (ours)\n74.4\n86.6\n89.3\n90.9\nInduct.\nBaseline (ICLR’19 [32])\nWRN\n64.6\n84.9\n88.2\n89.9\nBaseline++ (ICLR’19 [32])\n68.7\n85.4\n88.4\n90.1\nSimpleshot (ARXIV [33])\n70.7\n85.9\n88.7\n90.1\nTransduct.\nEntropy-min (ICLR’20 [15])\nWRN\n62.9 (↓6.0)\n77.3 (↓7.5)\n–\n–\nPT-MAP (ARXIV [31])\n65.1 (↓19.5)\n71.0 (↓19.0)\n72.5 (↓18.3)\n74.0 (↓17.7)\nLaplacianShot (ICML’20 [20])\n73.5 (↓5.3)\n86.8 (↓0.5)\n88.6 (↓0.4)\n89.6 (↓0.2)\nBD-CSPN (ECCV’20 [21])\n75.4 (↓2.3)\n85.9 (↓1.5)\n87.8 (↓1.0)\n89.1 (↓0.6)\nTIM (NEURIPS’20 [23])\n75.8 (↓4.5)\n85.4 (↓3.5)\n87.3 (↓3.2)\n88.7 (↓2.9)\nα-TIM (ours)\n76.0\n87.8\n90.4\n91.9\n8\n",
    "Table 2: Comparaisons of state-of-the-art methods in our realistic setting on CUB. Query sets are\nsampled following a Dirichlet distribution with a = 2 · 1K. Accuracy is averaged over 10,000 tasks.\nA red arrow (↓) indicates a performance drop between the artiﬁcially-balanced setting and our testing\nprocedure, and a blue arrow (↑) an improvement. Arrows are not displayed for the inductive methods\nas, for these, there is no signiﬁcant change in performance between both settings (expected). ‘–’\nsigniﬁes the result was computationally intractable to obtain.\nCUB\n1-shot\n5-shot\n10-shot\n20-shot\nInduct.\nBaseline (ICLR’19 [32])\nRN-18\n64.6\n86.9\n90.6\n92.7\nBaseline++ (ICLR’19 [32])\n69.4\n87.5\n91.0\n93.2\nSimpleshot (ARXIV [33])\n70.6\n87.5\n90.6\n92.2\nTransduct.\nPT-MAP (ARXIV [31])\nRN-18\n65.1 (↓20.4)\n71.3 (↓20.0)\n73.0 (↓19.2)\n72.2 (↓18.9)\nEntropy-min (ICLR’20 [15])\n67.5 (↓5.3)\n82.9 (↓6.0)\n85.5 (↓5.6)\n86.8 (↓5.7)\nLaplacianShot (ICML’20 [20])\n73.7 (↓5.2)\n87.7 (↓1.1)\n89.8 (↓0.7)\n90.6 (↓0.5)\nBD-CSPN (ECCV’20 [21])\n74.5 (↓3.4)\n87.1 (↓1.8)\n89.3 (↓1.3)\n90.3 (↓1.1)\nTIM (NEURIPS’20 [23])\n74.8 (↓5.5)\n86.9 (↓3.6)\n89.5 (↓2.9)\n91.7 (↓2.8)\nα-TIM (ours)\n75.7\n89.8\n92.3\n94.6\nunaffected by class imbalance. In the 1-shot scenario, all the transductive methods, without exception,\nundergo a signiﬁcant drop in performances as compared to the balanced setting. Even though the\nbest-performing transductive methods still outperforms the inductive ones, we observe that more\nthan half of the transductive methods evaluated perform overall worse than inductive baselines in our\nrealistic setting. Such a surprising ﬁnding highlights that the standard benchmarks, initially developed\nfor the inductive setting, are not well suited to evaluate transductive methods. In particular, when\nevaluated with our protocol, the current state-of-the-art holder PT-MAP averages more than 18%\nperformance drop across datasets and backbones, Entropy-Min around 7%, and TIM around 4%. Our\nproposed α-TIM method outperforms transductive methods across almost all task formats, datasets\nand backbones, and is the only method that consistently inductive baselines in fair setting. While\nstronger inductive baselines have been proposed in the literature [48], we show in the supplementary\nmaterial that α-TIM keeps a consistent relative improvement when evaluated under the same setting.\n6.2\nAblation studies\nIn-depth comparison of TIM and α-TIM\nWhile not included in the main Table 1, keeping the\nsame hyper-parameters for TIM as prescribed in the original paper [23] would result in a drastic\ndrop of about 18% across the benchmarks. As brieﬂy mentioned in section 4 and implemented for\ntuning [23] in Table 1, adjusting marginal-entropy weight λ in Eq. (1) strongly helps in imbalanced\nscenarios, reducing the drop from 18% to only 4%. However, we argue that such a strategy is\nsub-optimal in comparison to using α-divergences, where the only hyper-parameter controlling the\nﬂexibility of the marginal-distribution term becomes α. First, as seen from Table 1, our α-TIM\nachieves consistently better performances with the same budget of hyper-parameter optimization as\nthe standard TIM. In fact, in higher-shots scenarios (5 or higher), the performances of α-TIM are\nsubstantially better that the standard mutual information (i.e. TIM). Even more crucially, we show\nin Figure 3 that α-TIM does not fail drastically when α is chosen sub-optimally, as opposed to the\ncase of weighting parameter λ for the TIM formulation. We argue that such a robustness makes of\nα-divergences a particularly interesting choice for more practical applications, where such a tuning\nmight be intractable. Our results points to the high potential of α-divergences as loss functions for\nleveraging unlabelled data, beyond the few-shot scenario, e.g., in semi-supervised or unsupervised\ndomain adaptation problems.\nVarying imbalance severity\nWhile our main experiments used a ﬁxed value a = 2·1K, we wonder\nwhether our conclusions generalize to different levels of imbalance. Controlling for Dirichlet’s\nparameter a naturally allows us to vary the imbalance severity. In Figure 4, we display the results\nobtained by varying a, while keeping the same hyper-parameters obtained through our validation\nprotocol. Generally, most methods follow the expected trend: as a decreases and tasks become more\nseverely imbalanced, performances decrease, with sharpest losses for TIM [23] and PT-MAP [31]. In\nfact, past a certain imbalance severity, the inductive baseline in [33] becomes more competitive than\n9\n",
    "0\n2\n4\nλ\n45\n50\n55\n60\n65\n70\nAccuracy\nmini-ImageNet\n1-shot\n2\n4\n6\nα\n45\n50\n55\n60\n65\n70\n1-shot\n0\n2\n4\nλ\n60\n65\n70\n75\n80\n85\n5-shot\n2\n4\n6\nα\n60\n65\n70\n75\n80\n85\n5-shot\nTIM (val)\nTIM (test)\nα-TIM (val)\nα-TIM (test)\n0\n2\n4\nλ\n50\n55\n60\n65\n70\n75\n80\nAccuracy\nCUB\n2\n4\n6\nα\n50\n55\n60\n65\n70\n75\n80\n0\n2\n4\nλ\n70\n75\n80\n85\n90\n2\n4\n6\nα\n70\n75\n80\n85\n90\nFigure 3: Validation and Test accuracy versus λ for TIM [23] and α for our proposed α-TIM, using\nour protocol. Results are obtained with a RN-18. Best viewed in color.\nmost transductive methods. Interestingly, both LaplacianShot [20] and our proposed α-TIM are able\nto cope with extreme imbalance, while still conserving good performances on balanced tasks.\n2\n4\n6\n8\n10\na\n50\n55\n60\n65\n70\n75\n80\n85\n5-shot Accuracy\nmini-Imagenet\n2\n4\n6\n8\n10\na\n50\n55\n60\n65\n70\n75\n80\n85\n90\ntiered-Imagenet\n2\n4\n6\n8\n10\na\n50\n55\n60\n65\n70\n75\n80\n85\n90\n95\nCUB\nα-TIM\nTIM\nLaplacianShot\nBDCSPN\nPT-MAP\nLR+ICI\nSimpleShot\nFigure 4: 5-shot test accuracy of transductive methods versus imbalance level (lower a corresponds\nto more severe imbalance, as depicted in Figure 1).\nOn the use of transductive BN\nIn the case of imbalanced query sets, we note that transductive\nbatch normalization (e.g as done in the popular MAML [49]) hurts the performances. This aligns with\nrecent observations from the concurrent work in [50], where a shift in the marginal label distribution\nis clearly identiﬁed as a failure case of statistic alignment via batch normalization.\nConclusion\nWe make the unfortunate observation that recent transductive few-shot methods claiming large gains\nover inductive ones may perform worse when evaluated with our realistic setting. The artiﬁcial\nbalance of the query sets in the vanilla setting makes it easy for transductive methods to implicitly\nencode this strong prior at meta-training stage, or even explicitly at inference. When rendering such a\nproperty obsolete at test-time, the current top-performing method becomes noncompetitive, and all\nthe transductive methods undergo performance drops. Future works could study the mixed effect of\nimbalance on both support and query sets. We hope that our observations encourage the community\nto rethink the current transductive literature, and build upon our work to provide fairer grounds of\ncomparison between inductive and transductive methods.\n10\n",
    "Acknowledgments\nThis project was supported by the Natural Sciences and Engineering Research Council of Canada\n(Discovery Grant RGPIN 2019-05954). This project has received funding from the European\nUnion’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant\nagreement №792464.\nReferences\n[1] L. Fei-Fei, R. Fergus, and P. Perona, “One-shot learning of object categories,” IEEE Transactions on\nPattern Analysis and Machine Intelligence, vol. 28, no. 4, pp. 594–611, 2006.\n[2] E. Miller, N. Matsakis, and P. Viola, “Learning from one example through shared densities on transforms,”\nin IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2000.\n[3] O. Vinyals, C. Blundell, T. P. Lillicrap, K. Kavukcuoglu, and D. Wierstra, “Matching networks for one\nshot learning,” in Neural Information Processing Systems (NeurIPS), 2016.\n[4] J. Snell, K. Swersky, and R. Zemel, “Prototypical networks for few-shot learning,” in Neural Information\nProcessing Systems (NeurIPS), 2017.\n[5] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for fast adaptation of deep networks,” in\nInternational Conference on Machine Learning (ICML), 2017.\n[6] S. Ravi and H. Larochelle, “Optimization as a model for few-shot learning,” in International Conference\non Learning Representations (ICLR), 2017.\n[7] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and T. M. Hospedales, “Learning to compare: Relation\nnetwork for few-shot learning,” in IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 2018.\n[8] B. Oreshkin, P. R. López, and A. Lacoste, “Tadam: Task dependent adaptive metric for improved few-shot\nlearning,” in Neural Information Processing Systems (NeurIPS), 2018.\n[9] N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel, “A simple neural attentive meta-learner,” in Interna-\ntional Conference on Learning Representations (ICLR), 2018.\n[10] A. A. Rusu, D. Rao, J. Sygnowski, O. Vinyals, R. Pascanu, S. Osindero, and R. Hadsell, “Meta-learning\nwith latent embedding optimization,” in International Conference on Learning Representations (ICLR),\n2019.\n[11] L. Yanbin, J. Lee, M. Park, S. Kim, E. Yang, S. Hwang, and Y. Yang, “Learning to propagate labels:\nTransductive propagation network for few-shot learning,” in International Conference on Learning Repre-\nsentations (ICLR), 2019.\n[12] R. Hou, H. Chang, M. Bingpeng, S. Shan, and X. Chen, “Cross attention network for few-shot classiﬁcation,”\nin Neural Information Processing Systems (NeurIPS), 2019.\n[13] H.-J. Ye, H. Hu, D.-C. Zhan, and F. Sha, “Few-shot learning via embedding adaptation with set-to-set\nfunctions,” in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\n[14] L. Qiao, Y. Shi, J. Li, Y. Wang, T. Huang, and Y. Tian, “Transductive episodic-wise adaptive metric for\nfew-shot learning,” in IEEE/CVF International Conference on Computer Vision (ICCV), 2019.\n[15] G. S. Dhillon, P. Chaudhari, A. Ravichandran, and S. Soatto, “A baseline for few-shot image classiﬁcation,”\nin International Conference on Learning Representations (ICLR), 2020.\n[16] S. X. Hu, P. G. Moreno, Y. Xiao, X. Shen, G. Obozinski, N. D. Lawrence, and A. Damianou, “Empirical\nbayes transductive meta-learning with synthetic gradients,” in International Conference on Learning\nRepresentations (ICLR), 2020.\n[17] Y. Wang, C. Xu, C. Liu, L. Zhang, and Y. Fu, “Instance credibility inference for few-shot learning,” in\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\n[18] Y. Guo and N.-M. Cheung, “Attentive weights generation for few shot learning via information maximiza-\ntion,” in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\n[19] L. Yang, L. Li, Z. Zhang, X. Zhou, E. Zhou, and Y. Liu, “Dpgn: Distribution propagation graph network\nfor few-shot learning,” in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\n2020.\n[20] I. M. Ziko, J. Dolz, E. Granger, and I. Ben Ayed, “Laplacian regularized few-shot learning,” in International\nConference on Machine Learning (ICML), 2020.\n[21] J. Liu, L. Song, and Y. Qin, “Prototype rectiﬁcation for few-shot learning,” in European Conference on\nComputer Vision (ECCV), 2020.\n11\n",
    "[22] Y. Liu, B. Schiele, and Q. Sun, “An ensemble of epoch-wise empirical bayes for few-shot learning,” in\nEuropean Conference on Computer Vision (ECCV), 2020.\n[23] M. Boudiaf, I. M. Ziko, J. Rony, J. Dolz, P. Piantanida, and I. Ben Ayed, “Transductive information\nmaximization for few-shot learning,” in Neural Information Processing Systems (NeurIPS), 2020.\n[24] J. Bronskill, J. Gordon, J. Requeima, S. Nowozin, and R. E. Turner, “Tasknorm: Rethinking batch\nnormalization for meta-learning,” in International Conference on Machine Learning (ICML), 2020.\n[25] J. Gordon, J. Bronskill, M. Bauer, S. Nowozin, and R. Turner, “Meta-learning probabilistic inference for\nprediction,” in International Conference on Learning Representations (ICLR), 2019.\n[26] X. Zhen, H. Sun, Y. Du, J. Xu, Y. Yin, L. Shao, and C. Snoek, “Learning to learn kernels with variational\nrandom features,” in International Conference on Machine Learning (ICML), 2020.\n[27] V. N. Vapnik, “An overview of statistical learning theory,” IEEE Transactions on Neural Networks (TNN),\nvol. 10, no. 5, pp. 988–999, 1999.\n[28] T. Joachims, “Transductive inference for text classiﬁcation using support vector machines,” in International\nConference on Machine Learning (ICML), 1999.\n[29] Z. Dengyong, O. Bousquet, T. N. Lal, J. Weston, and B. Schölkopf, “Learning with local and global\nconsistency,” in Neural Information Processing Systems (NeurIPS), 2004.\n[30] M. Ren, E. Triantaﬁllou, S. Ravi, J. Snell, K. Swersky, J. B. Tenenbaum, H. Larochelle, and R. S. Zemel,\n“Meta-learning for semi-supervised few-shot classiﬁcation,” in International Conference on Learning\nRepresentations (ICLR), 2018.\n[31] Y. Hu, V. Gripon, and S. Pateux, “Leveraging the feature distribution in transfer-based few-shot learning,”\narXiv preprint:2006.03806, 2020.\n[32] W.-Y. Chen, Y.-C. Liu, Z. Kira, Y.-C. F. Wang, and J.-B. Huang, “A closer look at few-shot classiﬁcation,”\nin International Conference on Learning Representations (ICLR), 2019.\n[33] Y. Wang, W.-L. Chao, K. Q. Weinberger, and L. van der Maaten, “Simpleshot: Revisiting nearest-neighbor\nclassiﬁcation for few-shot learning,” arXiv preprint:1911.04623, 2019.\n[34] Y. Tian, Y. Wang, D. Krishnan, J. B. Tenenbaum, and P. Isola, “Rethinking few-shot image classiﬁcation: a\ngood embedding is all you need?” in European Conference on Computer Vision (ECCV), 2020.\n[35] E. Triantaﬁllou, T. Zhu, V. Dumoulin, P. Lamblin, U. Evci, K. Xu, R. Goroshin, C. Gelada, K. Swersky,\nP.-A. Manzagol et al., “Meta-dataset: A dataset of datasets for learning to learn from few examples,” arXiv\npreprint:1903.03096, 2019.\n[36] H. B. Lee, H. Lee, D. Na, S. Kim, M. Park, E. Yang, and S. J. Hwang, “Learning to balance: Bayesian\nmeta-learning for imbalanced and out-of-distribution tasks,” arXiv preprint:1905.12917, 2019.\n[37] X. Chen, H. Dai, Y. Li, X. Gao, and L. Song, “Learning to stop while learning to predict,” in International\nConference on Machine Learning (ICML), 2020.\n[38] M. Ochal, M. Patacchiola, A. Storkey, J. Vazquez, and S. Wang, “Few-shot learning with class imbalance,”\narXiv preprint:2101.02523, 2021.\n[39] L. Devroye, Non-Uniform Random Variate Generation.\nSpringer, 1986.\n[40] H. Chernoff et al., “A measure of asymptotic efﬁciency for tests of a hypothesis based on the sum of\nobservations,” The Annals of Mathematical Statistics, vol. 23, no. 4, pp. 493–507, 1952.\n[41] S.-I. Amari, “α-divergence is unique, belonging to both f-divergence and bregman divergence classes,”\nIEEE Transactions on Information Theory, vol. 55, no. 11, pp. 4925–4931, 2000.\n[42] J. Havrda and F. Charvát, “Quantiﬁcation method of classiﬁcation processes – concept of structural\na-entropy,” Kybernetika, vol. 3, no. 1, pp. 30–35, 1967.\n[43] C. Tsallis, “Possible generalization of boltzmann-gibbs statistics,” Journal of statistical physics, vol. 52,\nno. 1, pp. 479–487, 1988.\n[44] A. Cichocki and S.-I. Amari, “Families of alpha-beta-and gamma-divergences: Flexible and robust\nmeasures of similarities,” Entropy, vol. 12, no. 6, pp. 1532–1568, 2010.\n[45] S. Arimoto, “Information measures and capacity of order α for discrete memoryless channels,” Topics in\ninformation theory, 1977.\n[46] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bern-\nstein, A. C. Berg, and L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,” International\nJournal of Computer Vision (IJCV), vol. 115, no. 3, pp. 211–252, 2015.\n[47] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie, “The caltech-ucsd birds-200-2011 dataset,”\n2011.\n12\n",
    "[48] C. Zhang, Y. Cai, G. Lin, and C. Shen, “Deepemd: Few-shot image classiﬁcation with differentiable earth\nmover’s distance and structured classiﬁers,” in Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition (CVPR), 2020.\n[49] C. Finn, K. Xu, and S. Levine, “Probabilistic model-agnostic meta-learning,” in Advances in Neural\nInformation Processing Systems (NeurIPS), 2018.\n[50] C. Burns and J. Steinhardt, “Limitations of post-hoc feature alignment for robustness,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n13\n",
    "A\nOn the performance of α-TIM on the standard balanced setting\nIn the main tables of the paper, we did not include the performances of α-TIM in the standard\nbalanced setting. Here, we emphasize that α-TIM is a generalization of TIM [23] as when α →1\n(i.e., the α-entropies tend to the Shannon entropies), α-TIM tends to TIM. Therefore, in the standard\nsetting, where optimal hyper-parameter α is obtained over validation tasks that are balanced (as in\nthe standard validation tasks of the original TIM and the other existing methods), the performance\nof α-TIM is the same as TIM. When α is tuned on balanced validation tasks, we obtain an optimal\nvalue of α very close to 1, and our α-mutual information approaches the standard mutual information.\nWhen the validation tasks are uniformly random, as in our new setting and in the validation plots we\nprovided in the main ﬁgure, one can see that the performance of α-TIM remains competitive when\nwe tend to balanced testing tasks (i.e., when a is increasing), but is signiﬁcantly better than TIM\nwhen we tend to uniformly-random testing tasks (a = 1). These results illustrate the ﬂexibility of\nα-divergences, and are in line with the technical analysis provided in the main paper.\nB\nComparison with DeepEMD\nThe recent method [48] achieves impressive results in the inductive setting. As conveyed in the\nmain paper, inductive methods tend to be unaffected by class imbalance on the query set, which\nlegitimately questions whether strong inductive methods should be preferred over transductive ones,\nincluding our proposed α-TIM. In the case of DeepEMD, we expand below on the levers used to\nobtain such results, and argue those are orthogonal to the loss function, and therefore to our proposed\nα-TIM method. More speciﬁcally:\n1. DeepEMD uses richer feature representations: While all the methods we reproduce use the\nstandard global average pooling to obtain a single feature vector per image, DeepEMD-FCN\nleverages dense feature maps (i.e without the pooling layer). This results in a richer, much\nhigher-dimensional embeddings. For instance, the standard RN-18 yields a 512-D vector\nper image, while the FCN RN-12 used by DeepEMD yields a 5x5x640-D feature map (i.e\n31x larger). As for DeepEMD-Grid and DeepEMD-Sampling, they build feature maps by\nconcatenating feature extracted from N different patches taken from the original image\n(which requires as many forward passes through the backbone). Also, note that prototypes\noptimized for during inference have the same dimension as the feature maps. Therefore,\ntaking richer and larger feature representations also means increasing the number of trainable\nparameters at inference by the same ratio.\n2. DeepEMD uses a more sophisticated notion of distance (namely the Earth Moving Distance),\nintroducing an EMD layer, different from the standard classiﬁcation layer. While all methods\nwe reproduced are based on simple and easy-to-compute distances between each feature and\nthe prototypes (e.g Euclidian, dot-product, cosine distance), the ﬂow-based distance used by\nDeepEMD captures more complex patterns than the usual Euclidian distance, but is also\nmuch more demanding computationally (as it requires solving an LP program).\nNow, we want to emphasize that the model differences mentioned above can be straightforwardly\napplied to our α-TIM (and likely the other methods) in order to boost the results at the cost of\na signiﬁcant increase of compute requirement. To demonstrate this point, we implemented our\nα-TIM in with the three ResNet-12 based architectures proposed in DeepEMD (cf Table 3) using\nour imbalanced tasks, and consistently observed +3 to +4 without changing any optimization hyper-\nparameter from their setting, and using the pre-trained models the authors have provided. This ﬁgure\nmatches the improvement observed w.r.t to SimpleShot with the standard models (RN-18 and WRN).\n14\n",
    "Table 3: Comparison with DeepEMD [48]. Input: W=Whole images are used as input ; P = Multiples\npatches of the whole image are used as input. Embeddings: G=Global averaged features are used (i.e\n1 feature vector per image) ; L = Local features are used (i.e 1 feature map per image ).\nMethod\nDistance\nRN-18 (W/G)\nWRN (W/G)\nFCN RN-12 (W/L)\nGrid RN-12 (P/L)\nSampling RN-12 (P/L)\nSimpleShot [33]\nEuclidian\n63.0\n66.2\n—\n—\n—\nα-TIM\nEuclidian\n67.4\n69.8\n—\n—\n—\nDeepEMD [48]\nEMD\n—\n—\n65.9\n67.8\n68.8\nα-TIM\nEMD\n—\n—\n68.9\n72.0\n72.6\n15\n",
    "C\nRelation between α-entropy and α-divergence\nWe provide the derivation of Eq. (4) in the main paper, which links α-entropy Hα(p) to the α-\ndivergence:\nlogα(K) −K1−αDα(p||uK) =\n1\n1 −α\n\u0000K1−α −1\n\u0001\n−K1−α\nα −1\n K\nX\nk=1\npα\nk\n\u0012 1\nK\n\u00131−α\n−1\n!\n=\n1\n1 −αK1−α −\n1\n1 −α −\n1\nα −1\nK\nX\nk=1\npα\nk + K1−α\nα −1\n=\n1\nα −1\n \n1 −\nK\nX\nk=1\npα\nk\n!\n(7)\nD\nComparison with other types of imbalance\nThe study in [38] examined the effect of class imbalance on the support set after deﬁning several\nprocesses to generate class-imbalanced support sets. In particular, the authors proposed linear and\nstep imbalance. In a 5-way setting, a typical linearly imbalanced few-shot support would look like {1,\n3, 5, 7, 9} (keeping the total number of support samples equivalent to standard 5-ways 5-shot tasks),\nwhile a step imbalance task could be {1, 9, 9, 9}. To provide intuition as to how these two types of\nimbalance related to our proposed Dirichlet-based sampling scheme, we super-impose Dirichlet’s\ndensity on all valid linear and step imbalanced distributions for 3-ways tasks in Figure 5. Combined,\nlinear and step imbalanced valid distributions allow to cover a fair part of the simplex, but Dirichlet\nsampling allows to sample more diverse and arbitrary class ratios.\nFigure 5: Comparison of Dirichlet sampling with linear and step imbalance.\n16\n",
    "E\nInﬂuence of each term in TIM and α-TIM\nWe report a comprehensive ablation study, evaluating the beneﬁts of using the α-entropy instead of the\nShannon entropy (both conditional and marginal terms), as well as the effect of the marginal-entropy\nterms in the loss functions of TIM and α-TIM. The results are reported in Table 4. α-TIM yields\nbetter performances in all settings.\nOn the α-conditional entropy: The results of α-TIM obtained by optimizing the conditional entropy\nalone (without the marginal term) are 4.5 to 7.2% higher in 1-shot, 0.8 to 3.5% higher in 5-shot\nand 0.1 to 1.3% higher in 10-shot scenarios, in comparison to its Shannon-entropy counterpart in\nTIM. Note that, for the conditional-probability term, the α-entropy formulation has a stronger effect\nin lower-shot scenarios (1-shot and 5-shot). We hypothesize that this is due to the shapes of the\nα-entropy functions and their gradient dynamics (see Fig. 2 in the main paper), which, during training,\nassigns more weight to conﬁdent predictions near the vertices of the simplex (p = 1 or p = 0) and less\nweight to uncertain predictions at the middle of the simplex (p = 0.5). This discourages propagation\nof errors during training (i.e., learning from uncertain predictions), which are more likely to happen\nin lower-shot regimes.\nFlexibility of the α-marginal entropy: An important observation is that the marginal-entropy term\ndoes even hurt the performances of TIM in the higher shot scenarios (10-shot), even though the results\ncorrespond to the best λ over the validation set. We hypothesize that this is due to the strong class-\nbalance bias in the Shannon marginal entropy. Again, due to the shapes of the α-entropy functions\nand their gradient dynamics, α-TIM tolerates better class imbalance. In the 10-shot scenarios, the\nperformances of TIM decrease by 1.8 to 3.2% when including the marginal entropy, whereas the\nperformance of α-TIM remains approximately the same (with or without the marginal-entropy term).\nThese performances demonstrate the ﬂexibility of α-TIM.\nTable 4: An ablation study evaluating the beneﬁts of using the α-entropy instead of the Shannon\nentropy (both conditional and marginal terms), as well as the effect of the marginal-entropy terms in\nthe loss functions of TIM and α-TIM.\nLoss\nDataset\nNetwork\nMethod\n1-shot\n5-shot\n10-shot\nCE + H(YQ|XQ)\nmini-Imagenet\nRN-18\nTIM\n42.2\n79.5\n85.5\nα-TIM\n48.4\n82.4\n86.0\nWRN\nTIM\n52.8\n82.7\n87.5\nα-TIM\n57.3\n84.6\n88.0\ntiered-Imagenet\nRN-18\nTIM\n52.4\n83.7\n88.4\nα-TIM\n59.0\n86.3\n89.2\nWRN\nTIM\n49.6\n84.1\n89.1\nα-TIM\n56.8\n87.6\n90.4\nCUB\nRN-18\nTIM\n56.4\n89.0\n92.2\nα-TIM\n63.2\n89.8\n92.3\nCE + H(YQ|XQ) −H(YQ)\nmini-Imagenet\nRN-18\nTIM\n67.3\n79.8\n82.3\nα-TIM\n67.4\n82.5\n85.9\nWRN\nTIM\n69.8\n82.3\n84.5\nα-TIM\n69.8\n84.8\n87.9\ntiered-Imagenet\nRN-18\nTIM\n74.1\n84.1\n86.0\nα-TIM\n74.4\n86.6\n89.3\nWRN\nTIM\n75.8\n85.4\n87.3\nα-TIM\n76.0\n87.8\n90.4\nCUB\nRN-18\nTIM\n74.8\n86.9\n89.5\nα-TIM\n75.7\n89.8\n92.3\n17\n",
    "F\nHyper-parameters validation\n0\n2\n4\nλ\n50\n55\n60\n65\n70\n75\nAccuracy\n1-shot\n2\n4\n6\nα\n50\n55\n60\n65\n70\n75\n1-shot\n0\n2\n4\nλ\n60\n65\n70\n75\n80\n85\n90\n5-shot\n2\n4\n6\nα\n60\n65\n70\n75\n80\n85\n90\n5-shot\ntiered-Imagenet\nTIM (val)\nTIM (test)\nα-TIM (val)\nα-TIM (test)\nFigure 6: Validation and Test accuracy versus λ for TIM [23] and versus α for α-TIM, using our\ntask-generation protocol. Results are obtained with a RN-18. Best viewed in color.\n0\n2\n4\nλ\n45\n50\n55\n60\n65\n70\n75\nAccuracy\n1-shot\n2\n4\n6\nα\n45\n50\n55\n60\n65\n70\n75\n1-shot\n0\n2\n4\nλ\n60\n65\n70\n75\n80\n85\n90\n5-shot\n2\n4\n6\nα\n60\n65\n70\n75\n80\n85\n90\n5-shot\nmini-Imagenet\nTIM (val)\nTIM (test)\nα-TIM (val)\nα-TIM (test)\n0\n2\n4\nλ\n45\n50\n55\n60\n65\n70\n75\n80\nAccuracy\n2\n4\n6\nα\n45\n50\n55\n60\n65\n70\n75\n80\n0\n2\n4\nλ\n60\n65\n70\n75\n80\n85\n90\n2\n4\n6\nα\n60\n65\n70\n75\n80\n85\n90\ntiered-Imagenet\nFigure 7: Validation and Test accuracy versus λ for TIM [23] and versus α for α-TIM, using our\ntask-generation protocol. Results are obtained with a WRN. Best viewed in color.\nG\nCode – Implementation of our framework\nAs mentioned in our main experimental section, all the methods have been reproduced in our\ncommon framework, except for SIB8 [16] and LR-ICI9 [17], for which we used the ofﬁcial public\nimplementations of the works.\n8SIB public implementation: https://github.com/hushell/sib_meta_learn\n9LR-ICI public implementation: https://github.com/Yikai-Wang/ICI-FSL\n18\n",
    "0\n2\n4\n6\nλ\n60\n65\n70\n75\n80\n85\n90\nAccuracy\n10-shot\n2\n4\n6\nα\n60\n65\n70\n75\n80\n85\n90\n10-shot\n0\n2\n4\n6\nλ\n65\n70\n75\n80\n85\n90\n20-shot\n2\n4\n6\nα\n65\n70\n75\n80\n85\n90\n20-shot\nmini-Imagenet\nTIM (val)\nTIM (test)\nα-TIM (val)\nα-TIM (test)\n0\n2\n4\n6\nλ\n60\n65\n70\n75\n80\n85\n90\nAccuracy\n2\n4\n6\nα\n60\n65\n70\n75\n80\n85\n90\n0\n2\n4\n6\nλ\n65\n70\n75\n80\n85\n90\n2\n4\n6\nα\n65\n70\n75\n80\n85\n90\ntiered-Imagenet\n0\n2\n4\n6\nλ\n65\n70\n75\n80\n85\n90\n95\nAccuracy\n2\n4\n6\nα\n65\n70\n75\n80\n85\n90\n95\n0\n2\n4\n6\nλ\n70\n75\n80\n85\n90\n95\n2\n4\n6\nα\n70\n75\n80\n85\n90\n95\nCUB\nFigure 8: Validation and Test accuracy versus λ for TIM [23] and versus α for α-TIM on 10-shot and\n20-shot tasks, using our task-generation protocol. Results are obtained with a RN-18. Best viewed in\ncolor.\n0\n2\n4\n6\nλ\n65\n70\n75\n80\n85\n90\nAccuracy\n10-shot\n2\n4\n6\nα\n65\n70\n75\n80\n85\n90\n10-shot\n0\n2\n4\n6\nλ\n65\n70\n75\n80\n85\n90\n20-shot\n2\n4\n6\nα\n65\n70\n75\n80\n85\n90\n20-shot\nmini-Imagenet\nTIM (val)\nTIM (test)\nα-TIM (val)\nα-TIM (test)\n0\n2\n4\n6\nλ\n60\n65\n70\n75\n80\n85\n90\nAccuracy\n2\n4\n6\nα\n60\n65\n70\n75\n80\n85\n90\n0\n2\n4\n6\nλ\n60\n65\n70\n75\n80\n85\n90\n2\n4\n6\nα\n60\n65\n70\n75\n80\n85\n90\ntiered-Imagenet\nFigure 9: Validation and Test accuracy versus λ for TIM [23] and versus α for α-TIM on 10-shot and\n20-shot tasks, using our task-generation protocol. Results are obtained with a WRN. Best viewed in\ncolor.\n19\n"
  ],
  "full_text": "Realistic Evaluation of Transductive\nFew-Shot Learning\nOlivier Veilleux ∗\nÉTS Montreal\nMalik Boudiaf *\nÉTS Montreal\nPablo Piantanida\nL2S, CentraleSupélec CNRS\nUniversité Paris-Saclay\nIsmail Ben Ayed\nÉTS Montreal\nAbstract\nTransductive inference is widely used in few-shot learning, as it leverages the statis-\ntics of the unlabeled query set of a few-shot task, typically yielding substantially bet-\nter performances than its inductive counterpart. The current few-shot benchmarks\nuse perfectly class-balanced tasks at inference. We argue that such an artiﬁcial reg-\nularity is unrealistic, as it assumes that the marginal label probability of the testing\nsamples is known and ﬁxed to the uniform distribution. In fact, in realistic scenar-\nios, the unlabeled query sets come with arbitrary and unknown label marginals. We\nintroduce and study the effect of arbitrary class distributions within the query sets\nof few-shot tasks at inference, removing the class-balance artefact. Speciﬁcally,\nwe model the marginal probabilities of the classes as Dirichlet-distributed random\nvariables, which yields a principled and realistic sampling within the simplex. This\nleverages the current few-shot benchmarks, building testing tasks with arbitrary\nclass distributions. We evaluate experimentally state-of-the-art transductive meth-\nods over 3 widely used data sets, and observe, surprisingly, substantial performance\ndrops, even below inductive methods in some cases. Furthermore, we propose\na generalization of the mutual-information loss, based on α-divergences, which\ncan handle effectively class-distribution variations. Empirically, we show that our\ntransductive α-divergence optimization outperforms state-of-the-art methods across\nseveral data sets, models and few-shot settings. Our code is publicly available at\nhttps://github.com/oveilleux/Realistic_Transductive_Few_Shot.\n1\nIntroduction\nDeep learning models are widely dominating the ﬁeld. However, their outstanding performances are\noften built upon training on large-scale labeled data sets, and the models are seriously challenged\nwhen dealing with novel classes that were not seen during training. Few-shot learning [1, 2, 3] tackles\nthis challenge, and has recently triggered substantial interest within the community. In standard\nfew-shot settings, a model is initially trained on large-scale data containing labeled examples from a\nset of base classes. Then, supervision for a new set of classes, which are different from those seen in\nthe base training, is restricted to just one or a few labeled samples per class. Model generalization\nis evaluated over few-shot tasks. Each task includes a query set containing unlabeled samples for\nevaluation, and is supervised by a support set containing a few labeled samples per new class.\nThe recent few-shot classiﬁcation literature is abundant and widely dominated by convoluted meta-\nlearning and episodic-training strategies. To simulate generalization challenges at test times, such\nstrategies build sequences of artiﬁcially balanced few-shot tasks (or episodes) during base training,\neach containing both query and support samples. Widely adopted methods within this paradigm\ninclude: Prototypical networks [4], which optimizes the log-posteriors of the query points within\neach base-training episode; Matching networks [3], which expresses the predictions of query points\n∗Equal contributions, corresponding authors: {olivier.veilleux.2, malik.boudiaf.1}@ens.etsmtl.ca\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2204.11181v1  [cs.LG]  24 Apr 2022\n\n\nas linear functions of the support labels, while deploying episodic training and memory architectures;\nMAML (Model-Agnostic Meta-Learning) [5], which encourages a model to be “easy” to ﬁne-tune;\nand the meta-learner in [6], which prescribes optimization as a model for few-shot learning. These\npopular methods have recently triggered a large body of few-shot learning literature, for instance,\n[7, 8, 9, 10, 11, 12, 13], to list a few.\nRecently, a large body of works investigated transductive inference for few-shot tasks, e.g., [11, 14,\n12, 15, 16, 17, 18, 19, 20, 21, 22, 23], among many others, showing substantial improvements in\nperformances over inductive inference2. Also, as discussed in [24], most meta-learning approches\nrely critically on transductive batch normalization (TBN) to achieve competitive performances, for\ninstance, the methods in [5, 25, 26], among others. Adopted initially in the widely used MAML\n[5], TBN performs normalization using the statistics of the query set of a given few-shot task, and\nyields signiﬁcant increases in performances [24]. Therefore, due to the popularity of MAML, several\nmeta-learning techniques have used TBN. The transductive setting is appealing for few-shot learning,\nand the outstanding performances observed recently resonate well with a well-known fact in classical\ntransductive inference [27, 28, 29]: On small labeled data sets, transductive inference outperforms its\ninductive counterpart. In few-shot learning, transductive inference has access to exactly the same\ntraining and testing data as its inductive counterpart3. The difference is that it classiﬁes all the\nunlabeled query samples of each single few-shot task jointly, rather than one sample at a time.\nThe current few-shot benchmarks use perfectly class-balanced tasks at inference: For each task used\nat testing, all the classes have exactly the same number of samples, i.e., the marginal probability of\nthe classes is assumed to be known and ﬁxed to the uniform distribution across all tasks. This may\nnot reﬂect realistic scenarios, in which testing tasks might come with arbitrary class proportions.\nFor instance, the unlabeled query set of a task could be highly imbalanced. In fact, using perfectly\nbalanced query sets for benchmarking the models assumes exact knowledge of the marginal distri-\nbutions of the true labels of the testing points, but such labels are unknown. This is, undeniably, an\nunrealistic assumption and an important limitation of the current few-shot classiﬁcation benchmarks\nand datasets. Furthermore, this suggests that the recent progress in performances might be, in part,\ndue to class-balancing priors (or biases) that are encoded in state-of-the-art transductive models. Such\npriors might be implicit, e.g., through carefully designed episodic-training schemes and specialized\narchitectures, or explicit, e.g., in the design of transductive loss functions and constraints. For\ninstance, the best performing methods in [23, 31] use explicit label-marginal terms or constraints,\nwhich strongly enforce perfect class balance within the query set of each task. In practice, those\nclass-balance priors and assumptions may limit the applicability of the existing few-shot benchmarks\nand methods. In fact, our experiments show that, over few-shot tasks with random class balance, the\nperformances of state-of-the-art methods may decrease by margins. This motivates re-considering the\nexisting benchmarks and re-thinking the relevance of class-balance biases in state-of-the-art methods.\nContributions\nWe introduce and study the effect of arbitrary class distributions within the query\nsets of few-shot tasks at inference. Speciﬁcally, we relax the assumption of perfectly balanced query\nsets and model the marginal probabilities of the classes as Dirichlet-distributed random variables. We\ndevise a principled procedure for sampling simplex vectors from the Dirichlet distribution, which is\nwidely used in Bayesian statistics for modeling categorical events. This leverages the current few-shot\nbenchmarks by generating testing tasks with arbitrary class distributions, thereby reﬂecting realistic\nscenarios. We evaluate experimentally state-of-the-art transductive few-shot methods over 3 widely\nused datasets, and observe that the performances decrease by important margins, albeit at various\ndegrees, when dealing with arbitrary class distributions. In some cases, the performances drop even\nbelow the inductive baselines, which are not affected by class-distribution variations (as they do not\nuse the query-set statistics). Furthermore, we propose a generalization of the transductive mutual-\ninformation loss, based on α-divergences, which can handle effectively class-distribution variations.\nEmpirically, we show that our transductive α-divergence optimization outperforms state-of-the-art\nfew-shot methods across different data sets, models and few-shot settings.\n2The best-performing state-of-the-art few-shot methods in the transductive-inference setting have achieved\nperformances that are up to 10% higher than their inductive counterparts; see [23], for instance.\n3Each single few-shot task is treated independently of the other tasks in the transductive-inference setting.\nHence, the setting does not use additional unlabeled data, unlike semi-supervised few-shot learning [30].\n2\n\n\n2\nStandard few-shot settings\nBase training\nAssume that we have access to a fully labelled base dataset Dbase = {xi, yi}Nbase\ni=1\n,\nwhere xi ∈Xbase are data points in an input space Xbase, yi ∈{0, 1}|Ybase| the one-hot labels, and\nYbase the set of base classes. Base training learns a feature extractor fφ : X →Z, with φ its learnable\nparameters and Z a (lower-dimensional) feature space. The vast majority of the literature adopts\nepisodic training at this stage, which consists in formatting Dbase as a series of tasks (=episodes)\nin order to mimic the testing stage, and train a meta-learner to produce, through a differentiable\nprocess, predictions for the query set. However, it has been repeatedly demonstrated over the last\ncouple years that a standard supervised training followed by standard transfer learning strategies\nactually outperforms most meta-learning based approaches [32, 33, 34, 20, 23]. Therefore, we adopt\na standard cross-entropy training in this work.\nTesting\nThe model is evaluated on a set of few-shot tasks, each formed with samples from\nDtest = {xi, yi}Ntest\ni=1 , where yi ∈{0, 1}|Ytest| such that Ybase ∩Ytest = ∅. Each task is composed\nof a labelled support set S = {xi, yi}i∈IS and an unlabelled query set Q = {xi}i∈IQ, both containing\ninstances only from K distinct classes randomly sampled from Ytest, with K < |Ytest|. Leveraging\na feature extractor fφ pre-trained on the base data, the objective is to learn, for each few-shot task, a\nclassiﬁer fW : Z →∆K, with W the learnable parameters and ∆K = {y ∈[0, 1]K / P\nk yk = 1}\nthe (K −1)-simplex. To simplify the equations for the rest of the paper, we use the following\nnotations for the posterior predictions of each i ∈IS ∪IQ and for the class marginals within Q:\npik = fW (fφ(xi))k = P(Y = k|X = xi; W , φ) and bpk =\n1\n|IQ|\nX\ni∈IQ\npik = P(YQ = k; W , φ),\nwhere X and Y are the random variables associated with the raw features and labels, respectively;\nXQ and YQ means restriction of the random variable to set Q. The end goal is to predict the classes of\nthe unlabeled samples in Q for each few-shot task, independently of the other tasks. A large body of\nworks followed a transductive-prediction setting, e.g., [11, 14, 12, 15, 16, 17, 18, 19, 20, 21, 22, 23],\namong many others. Transductive inference performs a joint prediction for all the unlabeled query\nsamples of each single few-shot task, thereby leveraging the query-set statistics. On the current\nbenchmarks, tranductive inference often outperforms substantially its inductive counterpart (i.e.,\nclassifying one sample at a time for a given task). Note that our method is agnostic to the speciﬁc\nchoice of classiﬁerfW , whose parameters are learned at inference. In the experimental evaluations\nof our method, similarly to [23], we used pik ∝exp(−τ\n2 ∥wk −zi∥2), with W := (w1, . . . , wK),\nzi =\nfφ(xi)\n∥fφ(xi)∥2 , τ is a temperature parameter and base-training parameters φ are ﬁxed4.\nPerfectly balanced vs imbalanced tasks\nIn standard K-way few-shot settings, the support and\nquery sets of each task T are formed using the following procedure: (i) Randomly sample K\nclasses YT ⊂Ytest; (ii) For each class k ∈YT , randomly sample nS\nk support examples, such that\nnS\nk = |S|/K; and (iii) For each class k ∈YT , randomly sample nQ\nk query examples, such that\nnQ\nk = |Q|/K. Such a setting is undeniably artiﬁcial as we assume S and Q have the same perfectly\nbalanced class distribution. Several recent works [35, 36, 37, 38] studied class imbalance exclusively\non the support set S. This makes sense as, in realistic scenarios, some classes might have more\nlabelled samples than others. However, even these works rely on the assumption that query set Q is\nperfectly balanced. We argue that such an assumption is not realistic, as one typically has even less\ncontrol over the class distribution of Q than it has over that of S. For the labeled support S, the class\ndistribution is at least fully known and standard strategies from imbalanced supervised learning could\nbe applied [38]. This does not hold for Q, for which we need to make class predictions at testing time\nand whose class distribution is unknown. In fact, generating perfectly balanced tasks at test times for\nbenchmarking the models assumes that one has access to the unknown class distributions of the query\npoints, which requires access to their unknown labels. More importantly, artiﬁcial balancing of Q is\nimplicitly or explicitly encoded in several transductive methods, which use the query set statistics to\nmake class predictions, as will be discussed in section 4.\n4φ is either ﬁxed, e.g., [23], or ﬁne-tuned during inference, e.g., [15]. There is, however, evidence in the\nliterature that freezing φ yields better performances [23, 32, 34, 33], while reducing the inference time.\n3\n\n\na =(0.5, 0.5, 0.5)\na =(2, 2, 2)\na =(5, 5, 5)\na =(50, 50, 50)\nLow density\nHigh density\nFigure 1: Dirichlet density function for K = 3, with different choices of parameter vector a.\n3\nDirichlet-distributed class marginals for few-shot query sets\nStandard few-shot settings assume that pk, the proportion of the query samples belonging to a class\nk within a few-shot task, is deterministic (ﬁxed) and known priori: pk = nQ\nk /|Q| = 1/K, for all\nk and all few-shot tasks. We propose to relax this unrealistic assumption, and to use the Dirichlet\ndistribution to model the proportions (or marginal probabilities) of the classes in few-shot query sets\nas random variables. Dirichlet distributions are widely used in Bayesian statistics to model K-way\ncategorical events5. The domain of the Dirichlet distribution is the set of K-dimensional discrete\ndistributions, i.e., the set of vectors in (K −1)-simplex ∆K = {p ∈[0, 1]K | P\nk pk = 1}. Let Pk\ndenotes a random variable associated with class probability pk, and P the random simplex vector\ngiven by P = (P1, . . . , PK). We assume that P follows a Dirichlet distribution with parameter vector\na = (a1, . . . , aK) ∈RK: P ∼Dir(a). The Dirichlet distribution has the following density function:\nfDir(p; a) =\n1\nB(a)\nQK\nk=1 pak−1\nk\nfor p = (p1, . . . , pK) ∈∆K, with B denoting the multivariate Beta\nfunction, which could be expressed with the Gamma function6: B(a) =\nQK\nk=1 Γ(ak)\nΓ(\nPK\nk=1 αk).\nFigure 1 illustrates the Dirichlet density for K = 3, with a 2-simplex support represented with an\nequilateral triangle, whose vertices are probability vectors (1, 0, 0), (0, 1, 0) and (0, 0, 1). We show\nthe density for a = a1K, with 1K the K-dimensional vector whose all components are equal to\n1 and concentration parameter a equal to 0.5, 2, 5 and 50. Note that the limiting case a →+∞\ncorresponds to the standard settings with perfectly balanced tasks, where only uniform distribution,\ni.e., the point in the middle of the simplex, could occur as marginal distribution of the classes.\nThe following result, well-known in the literature of random variate generation [39], suggests that one\ncould generate samples from the multivariate Dirichlet distribution via simple and standard univariate\nGamma generators.\nTheorem 3.1. ([39, p. 594]) Let N1, . . . , NK be K independent Gamma-distributed random vari-\nables with parameters ak: Nk ∼Gamma(ak), i.e., the probability density of Nk is univariate Gamma7,\nwith shape parameter ak. Let Pk =\nNk\nPK\nk=1 Nk , k = 1, . . . , K. Then, P = (P1, . . . , PK) is Dirichlet\ndistributed: P ∼Dir(a), with a = (a1, . . . , aK).\nA proof based on the Jacobian of random-variable transformations Pk =\nNk\nPK\nk=1 Nk , k = 1, . . . , K,\ncould be found in [39], p. 594. This result prescribes the following simple procedure for sampling\nrandom simplex vectors (p1, . . . , pK) from the multivariate Dirichlet distribution with parameters\na = (a1, . . . , aK): First, we draw K independent random samples (n1, . . . , nK) from Gamma\ndistributions, with each nk drawn from univariate density fGamma(n; ak); To do so, one could use\nstandard random generators for the univariate Gamma density; see Chapter 9 in [39]. Then, we set\npk =\nnk\nPK\nk=1 nk . This enables to generate randomly nQ\nk , the number of samples of class k within query\nset Q, as follows: nQ\nk is the closest integer to pk|Q| such that P\nk nQ\nk = |Q|.\n5Note that the Dirichlet distribution is the conjugate prior of the categorical and multinomial distributions.\n6The Gamma function is given by: Γ(a) =\nR ∞\n0\nta−1 exp(−t)dt for a > 0. Note that Γ(a) = (a −1)!\nwhen a is a strictly positive integer.\n7Univariate Gamma density is given by: fGamma(n; ak) = nak−1 exp(−n)\nΓ(ak)\n, n ∈R.\n4\n\n\n4\nOn the class-balance bias of the best-performing few-shot methods\nAs brieﬂy evoked in section 2, the strict balancing of the classes in both S and Q represents a\nstrong inductive bias, which few-shot methods can either meta-learn during training or leverage at\ninference. In this section, we explicitly show how such a class-balance prior is encoded in the two\nbest-performing transductive methods in the literature [23, 31], one based on mutual-information\nmaximization [23] and the other on optimal transport [31].\nClass-balance bias of optimal transport\nRecently, the transductive method in [31], referred to as\nPT-MAP, achieved the best performances reported in the literature on several popular benchmarks, to\nthe best of our knowledge. However, the method explicitly embeds a class-balance prior. Formally,\nthe objective is to ﬁnd, for each few-shot task, an optimal mapping matrix M ∈R|Q|×K\n+\n, which\ncould be viewed as a joint probability distribution over XQ × YQ. At inference, a hard constraint\nM ∈{M :\nM1K = r, 1|Q|M = c} for some r and c is enforced through the use of the\nSinkhorn-Knopp algorithm. In other words, the columns and rows of M are constrained to sum\nto pre-deﬁned vectors r ∈R|Q| and c ∈RK. Setting c =\n1\nK 1K as done in [31] ensures that M\ndeﬁnes a valid joint distribution, but also crucially encodes the strong prior that all the classes within\nthe query sets are equally likely. Such a hard constraint is detrimental to the performance in more\nrealistic scenarios where the class distributions of the query sets could be arbitrary, and not necessarily\nuniform. Unsurprisingly, PT-MAP undergoes a substantial performance drop in the realistic scenario\nwith Dirichlet-distributed class proportions, with a consistent decrease in accuracy between 18 and\n20 % on all benchmarks, in the 5-ways case.\nClass-balance bias of transductive mutual-information maximization\nLet us now have a closer\nlook at the mutual-information maximization in [23]. Following the notations introduced in section 2,\nthe transductive loss minimized in [23] for a given few-shot task reads:\nLTIM = CE −I(XQ; YQ) = CE −1\n|IQ|\nX\ni∈Q\nK\nX\nk=1\npik log(pik)\n|\n{z\n}\nH(YQ|XQ)\n+λ\nK\nX\nk=1\nbpk log bpk\n|\n{z\n}\n−H(YQ)\n,\n(1)\nwhere I(XQ; YQ) = −H(YQ|XQ) + λH(YQ) is a weighted mutual information between the\nquery samples and their unknown labels (the mutual information corresponds to λ = 1), and\nCE := −\n1\n|IS|\nP\ni∈S\nPK\nk=1 yik log(pik) is a supervised cross-entropy term deﬁned over the support\nsamples. Let us now focus our attention on the label-marginal entropy term, H(YQ). As mentioned in\n[23], this term is of signiﬁcant importance as it prevents trivial, single-class solutions stemming from\nminimizing only conditional entropy H(YQ|XQ). However, we argue that this term also encourages\nclass-balanced solutions. In fact, we can write it as an explicit KL divergence, which penalizes\ndeviation of the label marginals within a query set from the uniform distribution:\nH(YQ) = −\nK\nX\nk=1\nbpk log (bpk) = log(K) −DKL(bp∥uK).\n(2)\nTherefore, minimizing marginal entropy H(YQ) is equivalent to minimizing the KL divergence\nbetween the predicted marginal distribution bp = (bp1, . . . , bpK) and uniform distribution uK = 1\nK 1K.\nThis KL penalty could harm the performances whenever the class distribution of the few-shot task\nis no longer uniform. In line with this analysis, and unsurprisingly, we observe in section 6 that\nthe original model in [23] also undergoes a dramatic performance drop, up to 20%. While naively\nremoving this marginal-entropy term leads to even worse performances, we observe that simply\ndown-weighting it, i.e., decreasing λ in Eq. (1), can drastically alleviate the problem, in contrast to\nthe case of optimal transport where the class-balance constraint is enforced in a hard manner.\n5\nGeneralizing mutual information with α-divergences\nIn this section, we propose a non-trivial, but simple generalization of the mutual-information loss in\n(1), based on α-divergences, which can tolerate more effectively class-distribution variations. We\nidentiﬁed in section 4 a class-balance bias encoded in the marginal Shannon entropy term. Ideally,\n5\n\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\np\n0.0\n0.2\n0.4\n0.6\n(p)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\np\n0.2\n0.1\n0.0\n0.1\n0.2\n(p)\nl\n=1\n=2\n=3\n=5\n=7\n=9\n=11\nFigure 2: (Left) α-entropy as a function of p = σ(l). (Right) Gradient of α-entropy w.r.t to the logit\nl ∈R as a function of p = σ(l). Best viewed in color.\nwe would like to extend this Shannon-entropy term in a way that allows for more ﬂexibility: Our\npurpose is to control how far the predicted label-marginal distribution, bp, could depart from the\nuniform distribution without being heavily penalized.\n5.1\nBackground\nWe argue that such a ﬂexibility could be controlled through the use of α-divergences [40, 41, 42,\n43, 44], which generalize the well-known and widely used KL divergence. α-divergences form a\nwhole family of divergences, which encompasses Tsallis and Renyi α-divergences, among others.\nIn this work, we focus on Tsallis’s [40, 43] formulation of α-divergence. Let us ﬁrst introduce\nthe generalized logarithm [44]: logα(x) =\n1\n1−α\n\u0000x1−α −1\n\u0001\n. Using the latter, Tsallis α-divergence\nnaturally extends KL. For two discrete distributions p = (pk)K\nk=1 and q = (qk)K\nk=1, we have:\nDα(p∥q) = −\nK\nX\nk=1\npk logα\n\u0012qk\npk\n\u0013\n=\n1\n1 −α\n \n1 −\nK\nX\nk=1\npα\nkq1−α\nk\n!\n.\n(3)\nNote that the Shannon entropy in Eq. (2) elegantly generalizes to Tsallis α-entropy:\nHα(p) = logα(K) −K1−α Dα(p∥uK) =\n1\nα −1\n \n1 −\nX\nk\npα\nk\n!\n.\n(4)\nThe derivation of Eq. (4) is provided in appendix. Also, limα→1 logα(x) = log(x), which implies:\nlim\nα→1 Dα(p∥q) = DKL(p∥q)\nand\nlim\nα→1 Hα(p) = H(p) = −\nK\nX\nk=1\nbpk log (bpk) .\nNote that α-divergence Dα(p∥q) inherits the nice properties of the KL divergence, including but not\nlimited to convexity with respect to both p and q and strict positivity Dα(p∥q) ≥0 with equality if\np = q. Furthermore, beyond its link to the forward KL divergence DKL(p∥q), α-divergence smoothly\nconnects several well-known divergences, including the reverse KL divergence DKL(q∥p) (α →0),\nthe Hellinger (α = 0.5) and the Pearson Chi-square (α = 2) distances [44].\n5.2\nAnalysis of the gradients\nAs observed from Eq. (4), α-entropy is, just like Shannon Entropy, intrinsically biased toward\nthe uniform distribution. Therefore, we still have not properly answered the question: why would\nα-entropy be better suited to imbalanced situations? We argue the that learning dynamics subtly\nbut crucially differ. To illustrate this point, let us consider a simple toy logistic-regression example.\nLet l ∈R denotes a logit, and p = σ(l) the corresponding probability, where σ stands for the usual\nsigmoid function. The resulting probability distribution simply reads p = {p, 1 −p}. In Figure 2, we\nplot both the α-entropy Hα (left) and its gradients ∂Hα/∂l (right) as functions of p. The advantage\nof α-divergence now becomes clearer: as α increases, Hα(p) accepts more and more deviation from\nthe uniform distribution (p = 0.5 on Figure 2), while still providing a barrier preventing trivial\nsolutions (i.e., p = 0 or p = 1, which corresponds to all the samples predicted as 0 or 1). Intuitively,\nsuch a behavior makes α-entropy with α > 1 better suited to class imbalance than Shannon entropy.\n6\n\n\n5.3\nProposed formulation\nIn light of the previous discussions, we advocate a new α-mutual information loss, a simple but very\neffective extension of the Shannon mutual information in Eq. (1):\nIα(XQ; YQ) = Hα(YQ) −Hα(YQ|XQ) =\n1\nα −1\n\n1\n|IQ|\nX\ni∈IQ\nK\nX\nk=1\npα\nik −\nK\nX\nk=1\nbp α\nk\n\n\n(5)\nwith Hα the α-entropy as deﬁned in Eq. (4). Note that our generalization in Eq. (5) has no link to the\nα-mutual information derived in [45]. Finally, our loss for transductive few-shot inference reads:\nLα-TIM = CE −Iα(XQ; YQ).\n(6)\n6\nExperiments\nIn this section, we thoroughly evaluate the most recent few-shot transductive methods using our\nimbalanced setting. Except for SIB [16] and LR-ICI [17] all the methods have been reproduced in\nour common framework. All the experiments have been executed on a single GTX 1080 Ti GPU.\nDatasets\nWe use three standard benchmarks for few-shot classiﬁcation: mini-Imagenet [46], tiered-\nImagenet [30] and Caltech-UCSD Birds 200 (CUB) [47]. The mini-Imagenet benchmark is a subset\nof the ILSVRC-12 dataset [46], composed of 60,000 color images of size 84 x 84 pixels [3]. It\nincludes 100 classes, each having 600 images. In all experiments, we used the standard split of 64\nbase-training, 16 validation and 20 test classes [6, 33]. The tiered-Imagenet benchmark is a larger\nsubset of ILSVRC-12, with 608 classes and 779,165 color images of size 84 × 84 pixels. We used a\nstandard split of 351 base-training, 97 validation and 160 test classes. The Caltech-UCSD Birds 200\n(CUB) benchmark also contains images of size 84 × 84 pixels, with 200 classes. For CUB, we used a\nstandard split of 100 base-training, 50 validation and 50 test classes, as in [32]. It is important to note\nthat for all the splits and data-sets, the base-training, validation and test classes are all different.\nTask sampling\nWe evaluate all the methods in the 1-shot 5-way, 5-shot 5-way, 10-shot 5-way and\n20-shot 5-way scenarios, with the classes of the query sets randomly distributed following Dirichlet’s\ndistribution, as described in section 3. Note that the total amount of query samples |Q| remains ﬁxed\nto 75. All the methods are evaluated by the average accuracy over 10,000 tasks, following [33]. We\nused different Dirichlet’s concentration parameter a for validation and testing. The validation-task\ngeneration is based on a random sampling within the simplex (i.e Dirichlet with a = 1K). Testing-\ntask generation uses a = 2·1K to reﬂect the fact that extremely imbalanced tasks (i.e., only one class\nis present in the task) are unlikely to happen in practical scenarios; see Figure 1 for visualization.\nHyper-parameters\nUnless identiﬁed as directly linked to a class-balance bias, all the hyper-\nparameters are kept similar to the ones prescribed in the original papers of the reproduced methods.\nFor instance, the marginal entropy in TIM [23] was identiﬁed in section 4 as a penalty that encourages\nclass balance. Therefore, the weight controlling the relative importance of this term is tuned. For\nall methods, hyper-parameter tuning is performed on the validation set of each dataset, using the\nvalidation sampling described in the previous paragraph.\nBase-training procedure\nAll non-episodic methods use the same feature extractors, which are\ntrained using the same procedure as in [23, 20], via a standard cross-entropy minimization on the base\nclasses with label smoothing. The feature extractors are trained for 90 epochs, using a learning rate\ninitialized to 0.1 and divided by 10 at epochs 45 and 66. We use a batch size of 256 for ResNet-18\nand of 128 for WRN28-10. During training, color jitter, random croping and random horizontal\nﬂipping augmentations are applied. For episodic/meta-learning methods, given that each requires a\nspeciﬁc training, we use the pre-trained models provided with the GitHub repository of each method.\n6.1\nMain results\nThe main results are reported in Table 1. As baselines, we also report the performances of state-of-\nthe-art inductive methods that do not use the statistics of the query set at adaptation and are, therefore,\n7\n\n\nTable 1: Comparisons of state-of-the-art methods in our realistic setting on mini-ImageNet, tiered-\nImageNet and CUB. Query sets are sampled following a Dirichlet distribution with a = 2 · 1K.\nAccuracy is averaged over 10,000 tasks. A red arrow (↓) indicates a performance drop between the\nartiﬁcially-balanced setting and our testing procedure, and a blue arrow (↑) an improvement. Arrows\nare not displayed for the inductive methods as, for these, there is no signiﬁcant change in performance\nbetween both settings (expected). ‘–’ signiﬁes the result was computationally intractable to obtain.\nmini-ImageNet\nMethod\nNetwork\n1-shot\n5-shot\n10-shot\n20-shot\nInduct.\nProtonet (NEURIPS’17 [4])\nRN-18\n53.4\n74.2\n79.2\n82.4\nBaseline (ICLR’19 [32])\n56.0\n78.9\n83.2\n85.9\nBaseline++ (ICLR’19 [32])\n60.4\n79.7\n83.8\n86.3\nSimpleshot (ARXIV [33])\n63.0\n80.1\n84.0\n86.1\nTransduct.\nMAML (ICML’17 [5])\nRN-18\n47.6 (↓3.8)\n64.5 (↓5.0)\n66.2 (↓5.7)\n67.2 (↓3.6)\nVersa (ICLR’19 [25])\n47.8 (↓2.2)\n61.9 (↓3.7)\n65.6 (↓3.6)\n67.3 (↓4.0)\nEntropy-min (ICLR’20 [15])\n58.5 (↓5.1)\n74.8 (↓7.3)\n77.2 (↓8.0)\n79.3 (↓7.9)\nLR+ICI (CVPR’2020 [17])\n58.7 (↓8.1)\n73.5 (↓5.7)\n78.4 (↓2.7)\n82.1 (↓1.7)\nPT-MAP (ARXIV [31])\n60.1 (↓16.8)\n67.1 (↓18.2)\n68.8 (↓18.0)\n70.4 (↓17.4)\nLaplacianShot (ICML’20 [20])\n65.4 (↓4.7)\n81.6 (↓0.5)\n84.1 (↓0.2)\n86.0 (↑0.5)\nBD-CSPN (ECCV’20 [21])\n67.0 (↓2.4)\n80.2(↓1.8)\n82.9 (↓1.4)\n84.6 (↓1.1)\nTIM (NEURIPS’20 [23])\n67.3 (↓4.5)\n79.8 (↓4.1)\n82.3 (↓3.8)\n84.2 (↓3.7)\nα-TIM (ours)\n67.4\n82.5\n85.9\n87.9\nInduct.\nBaseline (ICLR’19 [32])\nWRN\n62.2\n81.9\n85.5\n87.9\nBaseline++ (ICLR’19 [32])\n64.5\n82.1\n85.7\n87.9\nSimpleshot (ARXIV [33])\n66.2\n82.4\n85.6\n87.4\nTransduct.\nEntropy-min (ICLR’20 [15])\nWRN\n60.4 (↓5.7)\n76.2 (↓8.0)\n–\n–\nPT-MAP (ARXIV [31])\n60.6 (↓18.3)\n66.8 (↓19.8)\n68.5 (↓19.3)\n69.9 (↓19.0)\nSIB (ICLR’20 [16])\n64.7 (↓5.3)\n72.5 (↓6.7)\n73.6 (↓8.4)\n74.2 (↓8.7)\nLaplacianShot (ICML’20 [20])\n68.1 (↓4.8)\n83.2 (↓0.6)\n85.9 (↑0.4)\n87.2 (↑0.6)\nTIM (NEURIPS’20 [23])\n69.8 (↓4.8)\n81.6 (↓4.3)\n84.2 (↓3.9)\n85.9 (↓3.7)\nBD-CSPN (ECCV’20 [21])\n70.4 (↓2.1)\n82.3(↓1.4)\n84.5 (↓1.4)\n85.7 (↓1.1)\nα-TIM (ours)\n69.8\n84.8\n87.9\n89.7\ntiered-ImageNet\n1-shot\n5-shot\n10-shot\n20-shot\nInduct.\nBaseline (ICLR’19 [32])\nRN-18\n63.5\n83.8\n87.3\n89.0\nBaseline++ (ICLR’19 [32])\n68.0\n84.2\n87.4\n89.2\nSimpleshot (ARXIV [33])\n69.6\n84.7\n87.5\n89.1\nTransduct.\nEntropy-min (ICLR’20 [15])\nRN-18\n61.2 (↓5.8)\n75.5 (↓7.6)\n78.0 (↓7.9)\n79.8 (↓7.9)\nPT-MAP (ARXIV [31])\n64.1 (↓18.8)\n70.0 (↓18.8)\n71.9 (↓17.8)\n73.4 (↓17.1)\nLaplacianShot (ICML’20 [20])\n72.3 (↓4.8)\n85.7 (↓0.5)\n87.9 (↓0.1)\n89.0 (↑0.3)\nBD-CSPN (ECCV’20 [21])\n74.1 (↓2.2)\n84.8 (↓1.4)\n86.7 (↓1.1)\n87.9 (↓0.8)\nTIM (NEURIPS’20 [23])\n74.1 (↓4.5)\n84.1 (↓3.6)\n86.0 (↓3.3)\n87.4 (↓3.1)\nLR+ICI (CVPR’20 [17])\n74.6 (↓6.2)\n85.1 (↓2.8)\n88.0 (↓2.1)\n90.2 (↓1.2)\nα-TIM (ours)\n74.4\n86.6\n89.3\n90.9\nInduct.\nBaseline (ICLR’19 [32])\nWRN\n64.6\n84.9\n88.2\n89.9\nBaseline++ (ICLR’19 [32])\n68.7\n85.4\n88.4\n90.1\nSimpleshot (ARXIV [33])\n70.7\n85.9\n88.7\n90.1\nTransduct.\nEntropy-min (ICLR’20 [15])\nWRN\n62.9 (↓6.0)\n77.3 (↓7.5)\n–\n–\nPT-MAP (ARXIV [31])\n65.1 (↓19.5)\n71.0 (↓19.0)\n72.5 (↓18.3)\n74.0 (↓17.7)\nLaplacianShot (ICML’20 [20])\n73.5 (↓5.3)\n86.8 (↓0.5)\n88.6 (↓0.4)\n89.6 (↓0.2)\nBD-CSPN (ECCV’20 [21])\n75.4 (↓2.3)\n85.9 (↓1.5)\n87.8 (↓1.0)\n89.1 (↓0.6)\nTIM (NEURIPS’20 [23])\n75.8 (↓4.5)\n85.4 (↓3.5)\n87.3 (↓3.2)\n88.7 (↓2.9)\nα-TIM (ours)\n76.0\n87.8\n90.4\n91.9\n8\n\n\nTable 2: Comparaisons of state-of-the-art methods in our realistic setting on CUB. Query sets are\nsampled following a Dirichlet distribution with a = 2 · 1K. Accuracy is averaged over 10,000 tasks.\nA red arrow (↓) indicates a performance drop between the artiﬁcially-balanced setting and our testing\nprocedure, and a blue arrow (↑) an improvement. Arrows are not displayed for the inductive methods\nas, for these, there is no signiﬁcant change in performance between both settings (expected). ‘–’\nsigniﬁes the result was computationally intractable to obtain.\nCUB\n1-shot\n5-shot\n10-shot\n20-shot\nInduct.\nBaseline (ICLR’19 [32])\nRN-18\n64.6\n86.9\n90.6\n92.7\nBaseline++ (ICLR’19 [32])\n69.4\n87.5\n91.0\n93.2\nSimpleshot (ARXIV [33])\n70.6\n87.5\n90.6\n92.2\nTransduct.\nPT-MAP (ARXIV [31])\nRN-18\n65.1 (↓20.4)\n71.3 (↓20.0)\n73.0 (↓19.2)\n72.2 (↓18.9)\nEntropy-min (ICLR’20 [15])\n67.5 (↓5.3)\n82.9 (↓6.0)\n85.5 (↓5.6)\n86.8 (↓5.7)\nLaplacianShot (ICML’20 [20])\n73.7 (↓5.2)\n87.7 (↓1.1)\n89.8 (↓0.7)\n90.6 (↓0.5)\nBD-CSPN (ECCV’20 [21])\n74.5 (↓3.4)\n87.1 (↓1.8)\n89.3 (↓1.3)\n90.3 (↓1.1)\nTIM (NEURIPS’20 [23])\n74.8 (↓5.5)\n86.9 (↓3.6)\n89.5 (↓2.9)\n91.7 (↓2.8)\nα-TIM (ours)\n75.7\n89.8\n92.3\n94.6\nunaffected by class imbalance. In the 1-shot scenario, all the transductive methods, without exception,\nundergo a signiﬁcant drop in performances as compared to the balanced setting. Even though the\nbest-performing transductive methods still outperforms the inductive ones, we observe that more\nthan half of the transductive methods evaluated perform overall worse than inductive baselines in our\nrealistic setting. Such a surprising ﬁnding highlights that the standard benchmarks, initially developed\nfor the inductive setting, are not well suited to evaluate transductive methods. In particular, when\nevaluated with our protocol, the current state-of-the-art holder PT-MAP averages more than 18%\nperformance drop across datasets and backbones, Entropy-Min around 7%, and TIM around 4%. Our\nproposed α-TIM method outperforms transductive methods across almost all task formats, datasets\nand backbones, and is the only method that consistently inductive baselines in fair setting. While\nstronger inductive baselines have been proposed in the literature [48], we show in the supplementary\nmaterial that α-TIM keeps a consistent relative improvement when evaluated under the same setting.\n6.2\nAblation studies\nIn-depth comparison of TIM and α-TIM\nWhile not included in the main Table 1, keeping the\nsame hyper-parameters for TIM as prescribed in the original paper [23] would result in a drastic\ndrop of about 18% across the benchmarks. As brieﬂy mentioned in section 4 and implemented for\ntuning [23] in Table 1, adjusting marginal-entropy weight λ in Eq. (1) strongly helps in imbalanced\nscenarios, reducing the drop from 18% to only 4%. However, we argue that such a strategy is\nsub-optimal in comparison to using α-divergences, where the only hyper-parameter controlling the\nﬂexibility of the marginal-distribution term becomes α. First, as seen from Table 1, our α-TIM\nachieves consistently better performances with the same budget of hyper-parameter optimization as\nthe standard TIM. In fact, in higher-shots scenarios (5 or higher), the performances of α-TIM are\nsubstantially better that the standard mutual information (i.e. TIM). Even more crucially, we show\nin Figure 3 that α-TIM does not fail drastically when α is chosen sub-optimally, as opposed to the\ncase of weighting parameter λ for the TIM formulation. We argue that such a robustness makes of\nα-divergences a particularly interesting choice for more practical applications, where such a tuning\nmight be intractable. Our results points to the high potential of α-divergences as loss functions for\nleveraging unlabelled data, beyond the few-shot scenario, e.g., in semi-supervised or unsupervised\ndomain adaptation problems.\nVarying imbalance severity\nWhile our main experiments used a ﬁxed value a = 2·1K, we wonder\nwhether our conclusions generalize to different levels of imbalance. Controlling for Dirichlet’s\nparameter a naturally allows us to vary the imbalance severity. In Figure 4, we display the results\nobtained by varying a, while keeping the same hyper-parameters obtained through our validation\nprotocol. Generally, most methods follow the expected trend: as a decreases and tasks become more\nseverely imbalanced, performances decrease, with sharpest losses for TIM [23] and PT-MAP [31]. In\nfact, past a certain imbalance severity, the inductive baseline in [33] becomes more competitive than\n9\n\n\n0\n2\n4\nλ\n45\n50\n55\n60\n65\n70\nAccuracy\nmini-ImageNet\n1-shot\n2\n4\n6\nα\n45\n50\n55\n60\n65\n70\n1-shot\n0\n2\n4\nλ\n60\n65\n70\n75\n80\n85\n5-shot\n2\n4\n6\nα\n60\n65\n70\n75\n80\n85\n5-shot\nTIM (val)\nTIM (test)\nα-TIM (val)\nα-TIM (test)\n0\n2\n4\nλ\n50\n55\n60\n65\n70\n75\n80\nAccuracy\nCUB\n2\n4\n6\nα\n50\n55\n60\n65\n70\n75\n80\n0\n2\n4\nλ\n70\n75\n80\n85\n90\n2\n4\n6\nα\n70\n75\n80\n85\n90\nFigure 3: Validation and Test accuracy versus λ for TIM [23] and α for our proposed α-TIM, using\nour protocol. Results are obtained with a RN-18. Best viewed in color.\nmost transductive methods. Interestingly, both LaplacianShot [20] and our proposed α-TIM are able\nto cope with extreme imbalance, while still conserving good performances on balanced tasks.\n2\n4\n6\n8\n10\na\n50\n55\n60\n65\n70\n75\n80\n85\n5-shot Accuracy\nmini-Imagenet\n2\n4\n6\n8\n10\na\n50\n55\n60\n65\n70\n75\n80\n85\n90\ntiered-Imagenet\n2\n4\n6\n8\n10\na\n50\n55\n60\n65\n70\n75\n80\n85\n90\n95\nCUB\nα-TIM\nTIM\nLaplacianShot\nBDCSPN\nPT-MAP\nLR+ICI\nSimpleShot\nFigure 4: 5-shot test accuracy of transductive methods versus imbalance level (lower a corresponds\nto more severe imbalance, as depicted in Figure 1).\nOn the use of transductive BN\nIn the case of imbalanced query sets, we note that transductive\nbatch normalization (e.g as done in the popular MAML [49]) hurts the performances. This aligns with\nrecent observations from the concurrent work in [50], where a shift in the marginal label distribution\nis clearly identiﬁed as a failure case of statistic alignment via batch normalization.\nConclusion\nWe make the unfortunate observation that recent transductive few-shot methods claiming large gains\nover inductive ones may perform worse when evaluated with our realistic setting. The artiﬁcial\nbalance of the query sets in the vanilla setting makes it easy for transductive methods to implicitly\nencode this strong prior at meta-training stage, or even explicitly at inference. When rendering such a\nproperty obsolete at test-time, the current top-performing method becomes noncompetitive, and all\nthe transductive methods undergo performance drops. Future works could study the mixed effect of\nimbalance on both support and query sets. We hope that our observations encourage the community\nto rethink the current transductive literature, and build upon our work to provide fairer grounds of\ncomparison between inductive and transductive methods.\n10\n\n\nAcknowledgments\nThis project was supported by the Natural Sciences and Engineering Research Council of Canada\n(Discovery Grant RGPIN 2019-05954). This project has received funding from the European\nUnion’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant\nagreement №792464.\nReferences\n[1] L. Fei-Fei, R. Fergus, and P. Perona, “One-shot learning of object categories,” IEEE Transactions on\nPattern Analysis and Machine Intelligence, vol. 28, no. 4, pp. 594–611, 2006.\n[2] E. Miller, N. Matsakis, and P. Viola, “Learning from one example through shared densities on transforms,”\nin IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2000.\n[3] O. Vinyals, C. Blundell, T. P. Lillicrap, K. Kavukcuoglu, and D. Wierstra, “Matching networks for one\nshot learning,” in Neural Information Processing Systems (NeurIPS), 2016.\n[4] J. Snell, K. Swersky, and R. Zemel, “Prototypical networks for few-shot learning,” in Neural Information\nProcessing Systems (NeurIPS), 2017.\n[5] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for fast adaptation of deep networks,” in\nInternational Conference on Machine Learning (ICML), 2017.\n[6] S. Ravi and H. Larochelle, “Optimization as a model for few-shot learning,” in International Conference\non Learning Representations (ICLR), 2017.\n[7] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and T. M. Hospedales, “Learning to compare: Relation\nnetwork for few-shot learning,” in IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 2018.\n[8] B. Oreshkin, P. R. López, and A. Lacoste, “Tadam: Task dependent adaptive metric for improved few-shot\nlearning,” in Neural Information Processing Systems (NeurIPS), 2018.\n[9] N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel, “A simple neural attentive meta-learner,” in Interna-\ntional Conference on Learning Representations (ICLR), 2018.\n[10] A. A. Rusu, D. Rao, J. Sygnowski, O. Vinyals, R. Pascanu, S. Osindero, and R. Hadsell, “Meta-learning\nwith latent embedding optimization,” in International Conference on Learning Representations (ICLR),\n2019.\n[11] L. Yanbin, J. Lee, M. Park, S. Kim, E. Yang, S. Hwang, and Y. Yang, “Learning to propagate labels:\nTransductive propagation network for few-shot learning,” in International Conference on Learning Repre-\nsentations (ICLR), 2019.\n[12] R. Hou, H. Chang, M. Bingpeng, S. Shan, and X. Chen, “Cross attention network for few-shot classiﬁcation,”\nin Neural Information Processing Systems (NeurIPS), 2019.\n[13] H.-J. Ye, H. Hu, D.-C. Zhan, and F. Sha, “Few-shot learning via embedding adaptation with set-to-set\nfunctions,” in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\n[14] L. Qiao, Y. Shi, J. Li, Y. Wang, T. Huang, and Y. Tian, “Transductive episodic-wise adaptive metric for\nfew-shot learning,” in IEEE/CVF International Conference on Computer Vision (ICCV), 2019.\n[15] G. S. Dhillon, P. Chaudhari, A. Ravichandran, and S. Soatto, “A baseline for few-shot image classiﬁcation,”\nin International Conference on Learning Representations (ICLR), 2020.\n[16] S. X. Hu, P. G. Moreno, Y. Xiao, X. Shen, G. Obozinski, N. D. Lawrence, and A. Damianou, “Empirical\nbayes transductive meta-learning with synthetic gradients,” in International Conference on Learning\nRepresentations (ICLR), 2020.\n[17] Y. Wang, C. Xu, C. Liu, L. Zhang, and Y. Fu, “Instance credibility inference for few-shot learning,” in\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\n[18] Y. Guo and N.-M. Cheung, “Attentive weights generation for few shot learning via information maximiza-\ntion,” in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\n[19] L. Yang, L. Li, Z. Zhang, X. Zhou, E. Zhou, and Y. Liu, “Dpgn: Distribution propagation graph network\nfor few-shot learning,” in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\n2020.\n[20] I. M. Ziko, J. Dolz, E. Granger, and I. Ben Ayed, “Laplacian regularized few-shot learning,” in International\nConference on Machine Learning (ICML), 2020.\n[21] J. Liu, L. Song, and Y. Qin, “Prototype rectiﬁcation for few-shot learning,” in European Conference on\nComputer Vision (ECCV), 2020.\n11\n\n\n[22] Y. Liu, B. Schiele, and Q. Sun, “An ensemble of epoch-wise empirical bayes for few-shot learning,” in\nEuropean Conference on Computer Vision (ECCV), 2020.\n[23] M. Boudiaf, I. M. Ziko, J. Rony, J. Dolz, P. Piantanida, and I. Ben Ayed, “Transductive information\nmaximization for few-shot learning,” in Neural Information Processing Systems (NeurIPS), 2020.\n[24] J. Bronskill, J. Gordon, J. Requeima, S. Nowozin, and R. E. Turner, “Tasknorm: Rethinking batch\nnormalization for meta-learning,” in International Conference on Machine Learning (ICML), 2020.\n[25] J. Gordon, J. Bronskill, M. Bauer, S. Nowozin, and R. Turner, “Meta-learning probabilistic inference for\nprediction,” in International Conference on Learning Representations (ICLR), 2019.\n[26] X. Zhen, H. Sun, Y. Du, J. Xu, Y. Yin, L. Shao, and C. Snoek, “Learning to learn kernels with variational\nrandom features,” in International Conference on Machine Learning (ICML), 2020.\n[27] V. N. Vapnik, “An overview of statistical learning theory,” IEEE Transactions on Neural Networks (TNN),\nvol. 10, no. 5, pp. 988–999, 1999.\n[28] T. Joachims, “Transductive inference for text classiﬁcation using support vector machines,” in International\nConference on Machine Learning (ICML), 1999.\n[29] Z. Dengyong, O. Bousquet, T. N. Lal, J. Weston, and B. Schölkopf, “Learning with local and global\nconsistency,” in Neural Information Processing Systems (NeurIPS), 2004.\n[30] M. Ren, E. Triantaﬁllou, S. Ravi, J. Snell, K. Swersky, J. B. Tenenbaum, H. Larochelle, and R. S. Zemel,\n“Meta-learning for semi-supervised few-shot classiﬁcation,” in International Conference on Learning\nRepresentations (ICLR), 2018.\n[31] Y. Hu, V. Gripon, and S. Pateux, “Leveraging the feature distribution in transfer-based few-shot learning,”\narXiv preprint:2006.03806, 2020.\n[32] W.-Y. Chen, Y.-C. Liu, Z. Kira, Y.-C. F. Wang, and J.-B. Huang, “A closer look at few-shot classiﬁcation,”\nin International Conference on Learning Representations (ICLR), 2019.\n[33] Y. Wang, W.-L. Chao, K. Q. Weinberger, and L. van der Maaten, “Simpleshot: Revisiting nearest-neighbor\nclassiﬁcation for few-shot learning,” arXiv preprint:1911.04623, 2019.\n[34] Y. Tian, Y. Wang, D. Krishnan, J. B. Tenenbaum, and P. Isola, “Rethinking few-shot image classiﬁcation: a\ngood embedding is all you need?” in European Conference on Computer Vision (ECCV), 2020.\n[35] E. Triantaﬁllou, T. Zhu, V. Dumoulin, P. Lamblin, U. Evci, K. Xu, R. Goroshin, C. Gelada, K. Swersky,\nP.-A. Manzagol et al., “Meta-dataset: A dataset of datasets for learning to learn from few examples,” arXiv\npreprint:1903.03096, 2019.\n[36] H. B. Lee, H. Lee, D. Na, S. Kim, M. Park, E. Yang, and S. J. Hwang, “Learning to balance: Bayesian\nmeta-learning for imbalanced and out-of-distribution tasks,” arXiv preprint:1905.12917, 2019.\n[37] X. Chen, H. Dai, Y. Li, X. Gao, and L. Song, “Learning to stop while learning to predict,” in International\nConference on Machine Learning (ICML), 2020.\n[38] M. Ochal, M. Patacchiola, A. Storkey, J. Vazquez, and S. Wang, “Few-shot learning with class imbalance,”\narXiv preprint:2101.02523, 2021.\n[39] L. Devroye, Non-Uniform Random Variate Generation.\nSpringer, 1986.\n[40] H. Chernoff et al., “A measure of asymptotic efﬁciency for tests of a hypothesis based on the sum of\nobservations,” The Annals of Mathematical Statistics, vol. 23, no. 4, pp. 493–507, 1952.\n[41] S.-I. Amari, “α-divergence is unique, belonging to both f-divergence and bregman divergence classes,”\nIEEE Transactions on Information Theory, vol. 55, no. 11, pp. 4925–4931, 2000.\n[42] J. Havrda and F. Charvát, “Quantiﬁcation method of classiﬁcation processes – concept of structural\na-entropy,” Kybernetika, vol. 3, no. 1, pp. 30–35, 1967.\n[43] C. Tsallis, “Possible generalization of boltzmann-gibbs statistics,” Journal of statistical physics, vol. 52,\nno. 1, pp. 479–487, 1988.\n[44] A. Cichocki and S.-I. Amari, “Families of alpha-beta-and gamma-divergences: Flexible and robust\nmeasures of similarities,” Entropy, vol. 12, no. 6, pp. 1532–1568, 2010.\n[45] S. Arimoto, “Information measures and capacity of order α for discrete memoryless channels,” Topics in\ninformation theory, 1977.\n[46] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bern-\nstein, A. C. Berg, and L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,” International\nJournal of Computer Vision (IJCV), vol. 115, no. 3, pp. 211–252, 2015.\n[47] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie, “The caltech-ucsd birds-200-2011 dataset,”\n2011.\n12\n\n\n[48] C. Zhang, Y. Cai, G. Lin, and C. Shen, “Deepemd: Few-shot image classiﬁcation with differentiable earth\nmover’s distance and structured classiﬁers,” in Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition (CVPR), 2020.\n[49] C. Finn, K. Xu, and S. Levine, “Probabilistic model-agnostic meta-learning,” in Advances in Neural\nInformation Processing Systems (NeurIPS), 2018.\n[50] C. Burns and J. Steinhardt, “Limitations of post-hoc feature alignment for robustness,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n13\n\n\nA\nOn the performance of α-TIM on the standard balanced setting\nIn the main tables of the paper, we did not include the performances of α-TIM in the standard\nbalanced setting. Here, we emphasize that α-TIM is a generalization of TIM [23] as when α →1\n(i.e., the α-entropies tend to the Shannon entropies), α-TIM tends to TIM. Therefore, in the standard\nsetting, where optimal hyper-parameter α is obtained over validation tasks that are balanced (as in\nthe standard validation tasks of the original TIM and the other existing methods), the performance\nof α-TIM is the same as TIM. When α is tuned on balanced validation tasks, we obtain an optimal\nvalue of α very close to 1, and our α-mutual information approaches the standard mutual information.\nWhen the validation tasks are uniformly random, as in our new setting and in the validation plots we\nprovided in the main ﬁgure, one can see that the performance of α-TIM remains competitive when\nwe tend to balanced testing tasks (i.e., when a is increasing), but is signiﬁcantly better than TIM\nwhen we tend to uniformly-random testing tasks (a = 1). These results illustrate the ﬂexibility of\nα-divergences, and are in line with the technical analysis provided in the main paper.\nB\nComparison with DeepEMD\nThe recent method [48] achieves impressive results in the inductive setting. As conveyed in the\nmain paper, inductive methods tend to be unaffected by class imbalance on the query set, which\nlegitimately questions whether strong inductive methods should be preferred over transductive ones,\nincluding our proposed α-TIM. In the case of DeepEMD, we expand below on the levers used to\nobtain such results, and argue those are orthogonal to the loss function, and therefore to our proposed\nα-TIM method. More speciﬁcally:\n1. DeepEMD uses richer feature representations: While all the methods we reproduce use the\nstandard global average pooling to obtain a single feature vector per image, DeepEMD-FCN\nleverages dense feature maps (i.e without the pooling layer). This results in a richer, much\nhigher-dimensional embeddings. For instance, the standard RN-18 yields a 512-D vector\nper image, while the FCN RN-12 used by DeepEMD yields a 5x5x640-D feature map (i.e\n31x larger). As for DeepEMD-Grid and DeepEMD-Sampling, they build feature maps by\nconcatenating feature extracted from N different patches taken from the original image\n(which requires as many forward passes through the backbone). Also, note that prototypes\noptimized for during inference have the same dimension as the feature maps. Therefore,\ntaking richer and larger feature representations also means increasing the number of trainable\nparameters at inference by the same ratio.\n2. DeepEMD uses a more sophisticated notion of distance (namely the Earth Moving Distance),\nintroducing an EMD layer, different from the standard classiﬁcation layer. While all methods\nwe reproduced are based on simple and easy-to-compute distances between each feature and\nthe prototypes (e.g Euclidian, dot-product, cosine distance), the ﬂow-based distance used by\nDeepEMD captures more complex patterns than the usual Euclidian distance, but is also\nmuch more demanding computationally (as it requires solving an LP program).\nNow, we want to emphasize that the model differences mentioned above can be straightforwardly\napplied to our α-TIM (and likely the other methods) in order to boost the results at the cost of\na signiﬁcant increase of compute requirement. To demonstrate this point, we implemented our\nα-TIM in with the three ResNet-12 based architectures proposed in DeepEMD (cf Table 3) using\nour imbalanced tasks, and consistently observed +3 to +4 without changing any optimization hyper-\nparameter from their setting, and using the pre-trained models the authors have provided. This ﬁgure\nmatches the improvement observed w.r.t to SimpleShot with the standard models (RN-18 and WRN).\n14\n\n\nTable 3: Comparison with DeepEMD [48]. Input: W=Whole images are used as input ; P = Multiples\npatches of the whole image are used as input. Embeddings: G=Global averaged features are used (i.e\n1 feature vector per image) ; L = Local features are used (i.e 1 feature map per image ).\nMethod\nDistance\nRN-18 (W/G)\nWRN (W/G)\nFCN RN-12 (W/L)\nGrid RN-12 (P/L)\nSampling RN-12 (P/L)\nSimpleShot [33]\nEuclidian\n63.0\n66.2\n—\n—\n—\nα-TIM\nEuclidian\n67.4\n69.8\n—\n—\n—\nDeepEMD [48]\nEMD\n—\n—\n65.9\n67.8\n68.8\nα-TIM\nEMD\n—\n—\n68.9\n72.0\n72.6\n15\n\n\nC\nRelation between α-entropy and α-divergence\nWe provide the derivation of Eq. (4) in the main paper, which links α-entropy Hα(p) to the α-\ndivergence:\nlogα(K) −K1−αDα(p||uK) =\n1\n1 −α\n\u0000K1−α −1\n\u0001\n−K1−α\nα −1\n K\nX\nk=1\npα\nk\n\u0012 1\nK\n\u00131−α\n−1\n!\n=\n1\n1 −αK1−α −\n1\n1 −α −\n1\nα −1\nK\nX\nk=1\npα\nk + K1−α\nα −1\n=\n1\nα −1\n \n1 −\nK\nX\nk=1\npα\nk\n!\n(7)\nD\nComparison with other types of imbalance\nThe study in [38] examined the effect of class imbalance on the support set after deﬁning several\nprocesses to generate class-imbalanced support sets. In particular, the authors proposed linear and\nstep imbalance. In a 5-way setting, a typical linearly imbalanced few-shot support would look like {1,\n3, 5, 7, 9} (keeping the total number of support samples equivalent to standard 5-ways 5-shot tasks),\nwhile a step imbalance task could be {1, 9, 9, 9}. To provide intuition as to how these two types of\nimbalance related to our proposed Dirichlet-based sampling scheme, we super-impose Dirichlet’s\ndensity on all valid linear and step imbalanced distributions for 3-ways tasks in Figure 5. Combined,\nlinear and step imbalanced valid distributions allow to cover a fair part of the simplex, but Dirichlet\nsampling allows to sample more diverse and arbitrary class ratios.\nFigure 5: Comparison of Dirichlet sampling with linear and step imbalance.\n16\n\n\nE\nInﬂuence of each term in TIM and α-TIM\nWe report a comprehensive ablation study, evaluating the beneﬁts of using the α-entropy instead of the\nShannon entropy (both conditional and marginal terms), as well as the effect of the marginal-entropy\nterms in the loss functions of TIM and α-TIM. The results are reported in Table 4. α-TIM yields\nbetter performances in all settings.\nOn the α-conditional entropy: The results of α-TIM obtained by optimizing the conditional entropy\nalone (without the marginal term) are 4.5 to 7.2% higher in 1-shot, 0.8 to 3.5% higher in 5-shot\nand 0.1 to 1.3% higher in 10-shot scenarios, in comparison to its Shannon-entropy counterpart in\nTIM. Note that, for the conditional-probability term, the α-entropy formulation has a stronger effect\nin lower-shot scenarios (1-shot and 5-shot). We hypothesize that this is due to the shapes of the\nα-entropy functions and their gradient dynamics (see Fig. 2 in the main paper), which, during training,\nassigns more weight to conﬁdent predictions near the vertices of the simplex (p = 1 or p = 0) and less\nweight to uncertain predictions at the middle of the simplex (p = 0.5). This discourages propagation\nof errors during training (i.e., learning from uncertain predictions), which are more likely to happen\nin lower-shot regimes.\nFlexibility of the α-marginal entropy: An important observation is that the marginal-entropy term\ndoes even hurt the performances of TIM in the higher shot scenarios (10-shot), even though the results\ncorrespond to the best λ over the validation set. We hypothesize that this is due to the strong class-\nbalance bias in the Shannon marginal entropy. Again, due to the shapes of the α-entropy functions\nand their gradient dynamics, α-TIM tolerates better class imbalance. In the 10-shot scenarios, the\nperformances of TIM decrease by 1.8 to 3.2% when including the marginal entropy, whereas the\nperformance of α-TIM remains approximately the same (with or without the marginal-entropy term).\nThese performances demonstrate the ﬂexibility of α-TIM.\nTable 4: An ablation study evaluating the beneﬁts of using the α-entropy instead of the Shannon\nentropy (both conditional and marginal terms), as well as the effect of the marginal-entropy terms in\nthe loss functions of TIM and α-TIM.\nLoss\nDataset\nNetwork\nMethod\n1-shot\n5-shot\n10-shot\nCE + H(YQ|XQ)\nmini-Imagenet\nRN-18\nTIM\n42.2\n79.5\n85.5\nα-TIM\n48.4\n82.4\n86.0\nWRN\nTIM\n52.8\n82.7\n87.5\nα-TIM\n57.3\n84.6\n88.0\ntiered-Imagenet\nRN-18\nTIM\n52.4\n83.7\n88.4\nα-TIM\n59.0\n86.3\n89.2\nWRN\nTIM\n49.6\n84.1\n89.1\nα-TIM\n56.8\n87.6\n90.4\nCUB\nRN-18\nTIM\n56.4\n89.0\n92.2\nα-TIM\n63.2\n89.8\n92.3\nCE + H(YQ|XQ) −H(YQ)\nmini-Imagenet\nRN-18\nTIM\n67.3\n79.8\n82.3\nα-TIM\n67.4\n82.5\n85.9\nWRN\nTIM\n69.8\n82.3\n84.5\nα-TIM\n69.8\n84.8\n87.9\ntiered-Imagenet\nRN-18\nTIM\n74.1\n84.1\n86.0\nα-TIM\n74.4\n86.6\n89.3\nWRN\nTIM\n75.8\n85.4\n87.3\nα-TIM\n76.0\n87.8\n90.4\nCUB\nRN-18\nTIM\n74.8\n86.9\n89.5\nα-TIM\n75.7\n89.8\n92.3\n17\n\n\nF\nHyper-parameters validation\n0\n2\n4\nλ\n50\n55\n60\n65\n70\n75\nAccuracy\n1-shot\n2\n4\n6\nα\n50\n55\n60\n65\n70\n75\n1-shot\n0\n2\n4\nλ\n60\n65\n70\n75\n80\n85\n90\n5-shot\n2\n4\n6\nα\n60\n65\n70\n75\n80\n85\n90\n5-shot\ntiered-Imagenet\nTIM (val)\nTIM (test)\nα-TIM (val)\nα-TIM (test)\nFigure 6: Validation and Test accuracy versus λ for TIM [23] and versus α for α-TIM, using our\ntask-generation protocol. Results are obtained with a RN-18. Best viewed in color.\n0\n2\n4\nλ\n45\n50\n55\n60\n65\n70\n75\nAccuracy\n1-shot\n2\n4\n6\nα\n45\n50\n55\n60\n65\n70\n75\n1-shot\n0\n2\n4\nλ\n60\n65\n70\n75\n80\n85\n90\n5-shot\n2\n4\n6\nα\n60\n65\n70\n75\n80\n85\n90\n5-shot\nmini-Imagenet\nTIM (val)\nTIM (test)\nα-TIM (val)\nα-TIM (test)\n0\n2\n4\nλ\n45\n50\n55\n60\n65\n70\n75\n80\nAccuracy\n2\n4\n6\nα\n45\n50\n55\n60\n65\n70\n75\n80\n0\n2\n4\nλ\n60\n65\n70\n75\n80\n85\n90\n2\n4\n6\nα\n60\n65\n70\n75\n80\n85\n90\ntiered-Imagenet\nFigure 7: Validation and Test accuracy versus λ for TIM [23] and versus α for α-TIM, using our\ntask-generation protocol. Results are obtained with a WRN. Best viewed in color.\nG\nCode – Implementation of our framework\nAs mentioned in our main experimental section, all the methods have been reproduced in our\ncommon framework, except for SIB8 [16] and LR-ICI9 [17], for which we used the ofﬁcial public\nimplementations of the works.\n8SIB public implementation: https://github.com/hushell/sib_meta_learn\n9LR-ICI public implementation: https://github.com/Yikai-Wang/ICI-FSL\n18\n\n\n0\n2\n4\n6\nλ\n60\n65\n70\n75\n80\n85\n90\nAccuracy\n10-shot\n2\n4\n6\nα\n60\n65\n70\n75\n80\n85\n90\n10-shot\n0\n2\n4\n6\nλ\n65\n70\n75\n80\n85\n90\n20-shot\n2\n4\n6\nα\n65\n70\n75\n80\n85\n90\n20-shot\nmini-Imagenet\nTIM (val)\nTIM (test)\nα-TIM (val)\nα-TIM (test)\n0\n2\n4\n6\nλ\n60\n65\n70\n75\n80\n85\n90\nAccuracy\n2\n4\n6\nα\n60\n65\n70\n75\n80\n85\n90\n0\n2\n4\n6\nλ\n65\n70\n75\n80\n85\n90\n2\n4\n6\nα\n65\n70\n75\n80\n85\n90\ntiered-Imagenet\n0\n2\n4\n6\nλ\n65\n70\n75\n80\n85\n90\n95\nAccuracy\n2\n4\n6\nα\n65\n70\n75\n80\n85\n90\n95\n0\n2\n4\n6\nλ\n70\n75\n80\n85\n90\n95\n2\n4\n6\nα\n70\n75\n80\n85\n90\n95\nCUB\nFigure 8: Validation and Test accuracy versus λ for TIM [23] and versus α for α-TIM on 10-shot and\n20-shot tasks, using our task-generation protocol. Results are obtained with a RN-18. Best viewed in\ncolor.\n0\n2\n4\n6\nλ\n65\n70\n75\n80\n85\n90\nAccuracy\n10-shot\n2\n4\n6\nα\n65\n70\n75\n80\n85\n90\n10-shot\n0\n2\n4\n6\nλ\n65\n70\n75\n80\n85\n90\n20-shot\n2\n4\n6\nα\n65\n70\n75\n80\n85\n90\n20-shot\nmini-Imagenet\nTIM (val)\nTIM (test)\nα-TIM (val)\nα-TIM (test)\n0\n2\n4\n6\nλ\n60\n65\n70\n75\n80\n85\n90\nAccuracy\n2\n4\n6\nα\n60\n65\n70\n75\n80\n85\n90\n0\n2\n4\n6\nλ\n60\n65\n70\n75\n80\n85\n90\n2\n4\n6\nα\n60\n65\n70\n75\n80\n85\n90\ntiered-Imagenet\nFigure 9: Validation and Test accuracy versus λ for TIM [23] and versus α for α-TIM on 10-shot and\n20-shot tasks, using our task-generation protocol. Results are obtained with a WRN. Best viewed in\ncolor.\n19\n"
}