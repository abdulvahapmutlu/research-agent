{
  "filename": "1903.03096v4.pdf",
  "num_pages": 24,
  "pages": [
    "Published as a conference paper at ICLR 2020\nMETA-DATASET:\nA DATASET\nOF DATASETS\nFOR\nLEARNING TO LEARN FROM FEW EXAMPLES\nEleni Triantaﬁllou∗†, Tyler Zhu†, Vincent Dumoulin†, Pascal Lamblin†, Utku Evci†,\nKelvin Xu‡†, Ross Goroshin†, Carles Gelada†, Kevin Swersky†,\nPierre-Antoine Manzagol† & Hugo Larochelle†\n∗University of Toronto and Vector Institute, †Google AI, ‡University of California, Berkeley\nCorrespondence to: eleni@cs.toronto.edu\nABSTRACT\nFew-shot classiﬁcation refers to learning a classiﬁer for new classes given only a\nfew examples. While a plethora of models have emerged to tackle it, we ﬁnd the\nprocedure and datasets that are used to assess their progress lacking. To address\nthis limitation, we propose META-DATASET: a new benchmark for training and\nevaluating models that is large-scale, consists of diverse datasets, and presents\nmore realistic tasks. We experiment with popular baselines and meta-learners\non META-DATASET, along with a competitive method that we propose. We\nanalyze performance as a function of various characteristics of test tasks and\nexamine the models’ ability to leverage diverse training sources for improving their\ngeneralization. We also propose a new set of baselines for quantifying the beneﬁt of\nmeta-learning in META-DATASET. Our extensive experimentation has uncovered\nimportant research challenges and we hope to inspire work in these directions.\n1\nINTRODUCTION\nFew-shot learning refers to learning new concepts from few examples, an ability that humans naturally\npossess, but machines still lack. Improving on this aspect would lead to more efﬁcient algorithms\nthat can ﬂexibly expand their knowledge without requiring large labeled datasets. We focus on\nfew-shot classiﬁcation: classifying unseen examples into one of N new ‘test’ classes, given only a\nfew reference examples of each. Recent progress in this direction has been made by considering a\nmeta-problem: though we are not interested in learning about any training class in particular, we can\nexploit the training classes for the purpose of learning to learn new classes from few examples, thus\nacquiring a learning procedure that can be directly applied to new few-shot learning problems too.\nThis intuition has inspired numerous models of increasing complexity (see Related Work for some\nexamples). However, we believe that the commonly-used setup for measuring success in this direction\nis lacking. Speciﬁcally, two datasets have emerged as de facto benchmarks for few-shot learning:\nOmniglot (Lake et al., 2015), and mini-ImageNet (Vinyals et al., 2016), and we believe that both\nof them are approaching their limit in terms of allowing one to discriminate between the merits of\ndifferent approaches. Omniglot is a dataset of 1623 handwritten characters from 50 different alphabets\nand contains 20 examples per class (character). Most recent methods obtain very high accuracy\non Omniglot, rendering the comparisons between them mostly uninformative. mini-ImageNet is\nformed out of 100 ImageNet (Russakovsky et al., 2015) classes (64/16/20 for train/validation/test)\nand contains 600 examples per class. Albeit harder than Omniglot, it has the same property that\nmost recent methods trained on it present similar accuracy when controlling for model capacity. We\nadvocate that a more challenging and realistic benchmark is required for further progress in this area.\nMore speciﬁcally, current benchmarks: 1) Consider homogeneous learning tasks. In contrast, real-life\nlearning experiences are heterogeneous: they vary in terms of the number of classes and examples\nper class, and are unbalanced. 2) Measure only within-dataset generalization. However, we are\neventually after models that can generalize to entirely new distributions (e.g., datasets). 3) Ignore the\nrelationships between classes when forming episodes. Speciﬁcally, the coarse-grained classiﬁcation\nof dogs and chairs may present different difﬁculties than the ﬁne-grained classiﬁcation of dog breeds,\nand current benchmarks do not establish a distinction between the two.\n1\narXiv:1903.03096v4  [cs.LG]  8 Apr 2020\n",
    "Published as a conference paper at ICLR 2020\nMETA-DATASET aims to improve upon previous benchmarks in the above directions: it is signiﬁcantly\nlarger-scale and is comprised of multiple datasets of diverse data distributions; its task creation is\ninformed by class structure for ImageNet and Omniglot; it introduces realistic class imbalance; and it\nvaries the number of classes in each task and the size of the training set, thus testing the robustness of\nmodels across the spectrum from very-low-shot learning onwards.\nThe main contributions of this work are: 1) A more realistic, large-scale and diverse environment for\ntraining and testing few-shot learners. 2) Experimental evaluation of popular models, and a new set of\nbaselines combining inference algorithms of meta-learners with non-episodic training. 3) Analyses of\nwhether different models beneﬁt from more data, heterogeneous training sources, pre-trained weights,\nand meta-training. 4) A novel meta-learner that performs strongly on META-DATASET.\n2\nFEW-SHOT CLASSIFICATION: TASK FORMULATION AND APPROACHES\nTask Formulation\nThe end-goal of few-shot classiﬁcation is to produce a model which, given a\nnew learning episode with N classes and a few labeled examples (kc per class, c ∈1, . . . , N), is\nable to generalize to unseen examples for that episode. In other words, the model learns from a\ntraining (support) set S = {(x1, y1), (x2, y2), . . . , (xK, yK)} (with K = P\nc kc) and is evaluated on\na held-out test (query) set Q = {(x∗\n1, y∗\n1), (x∗\n2, y∗\n2), . . . , (x∗\nT , y∗\nT )}. Each example (x, y) is formed of\nan input vector x ∈RD and a class label y ∈{1, . . . , N}. Episodes with balanced training sets (i.e.,\nkc = k, ∀c) are usually described as ‘N-way, k-shot’ episodes. Evaluation episodes are constructed\nby sampling their N classes from a larger set Ctest of classes and sampling the desired number of\nexamples per class.\nA disjoint set Ctrain of classes is available to train the model; note that this notion of training is\ndistinct from the training that occurs within a few-shot learning episode. Few-shot learning does not\nprescribe a speciﬁc procedure for exploiting Ctrain, but a common approach matches the conditions\nin which the model is trained and evaluated (Vinyals et al., 2016). In other words, training often (but\nnot always) proceeds in an episodic fashion. Some authors use training and testing to refer to what\nhappens within any given episode, and meta-training and meta-testing to refer to using Ctrain to turn\nthe model into a learner capable of fast adaptation and Ctest for evaluating its success to learn using\nfew shots, respectively. This nomenclature highlights the meta-learning perspective alluded to earlier,\nbut to avoid confusion we will adopt another common nomenclature and refer to the training and test\nsets of an episode as the support and query sets and to the process of learning from Ctrain simply as\ntraining. We use the term ‘meta-learner’ to describe a model that is trained episodically, i.e., learns to\nlearn across multiple tasks that are sampled from the training set Ctrain.\nNon-episodic Approaches to Few-shot Classiﬁcation\nA natural non-episodic approach simply\ntrains a classiﬁer over all of the training classes Ctrain at once, which can be parameterized by a\nneural network with a linear layer on top with one output unit per class. After training, this neural\nnetwork is used as an embedding function g that maps images into a meaningful representation space.\nThe hope of using this model for few-shot learning is that this representation space is useful even for\nexamples of classes that were not included in training. It would then remain to deﬁne an algorithm\nfor performing few-shot classiﬁcation on top of these representations of the images of a task. We\nconsider two choices for this algorithm, yielding the ‘k-NN’ and ‘Finetune’ variants of this baseline.\nGiven a test episode, the ‘k-NN’ baseline classiﬁes each query example as the class that its ‘closest’\nsupport example belongs to. Closeness is measured by either Euclidean or cosine distance in the\nlearned embedding space; a choice that we treat as a hyperparameter. On the other hand, the ‘Finetune’\nbaseline uses the support set of the given test episode to train a new ‘output layer’ on top of the\nembeddings g, and optionally ﬁnetune those embedding too (another hyperparameter), for the purpose\nof classifying between the N new classes of the associated task.\nA variant of the ‘Finetune’ baseline has recently become popular: Baseline++ (Chen et al., 2019),\noriginally inspired by Gidaris & Komodakis (2018); Qi et al. (2018). It uses a ‘cosine classiﬁer’ as\nthe ﬁnal layer (ℓ2-normalizing embeddings and weights before taking the dot product), both during\nthe non-episodic training phase, and for evaluation on test episodes. We incorporate this idea in our\ncodebase by adding a hyperparameter that optionally enables using a cosine classiﬁer for the ‘k-NN’\n(training only) and ‘Finetune’ (both phases) baselines.\n2\n",
    "Published as a conference paper at ICLR 2020\nMeta-Learners for Few-shot Classiﬁcation\nIn the episodic setting, models are trained end-to-end\nfor the purpose of learning to build classiﬁers from a few examples. We choose to experiment\nwith Matching Networks (Vinyals et al., 2016), Relation Networks (Sung et al., 2018), Prototypical\nNetworks (Snell et al., 2017) and Model Agnostic Meta-Learning (MAML, Finn et al., 2017) since\nthey cover a diverse set of approaches to few-shot learning. We also introduce a novel meta-learner\nwhich is inspired by the last two models.\nIn each training episode, episodic models compute for each query example x∗∈Q, the distribution\nfor its label p(y∗|x∗, S) conditioned on the support set S and allow to train this differentiably-\nparameterized conditional distribution end-to-end via gradient descent. The different models are\ndistinguished by the manner in which this conditioning on the support set is realized. In all cases, the\nperformance on the query set drives the update of the meta-learner’s weights, which include (and\nsometimes consist only of) the embedding weights. We brieﬂy describe each method below.\nPrototypical Networks\nPrototypical Networks construct a prototype for each class and then classify\neach query example as the class whose prototype is ‘nearest’ to it under Euclidean distance. More\nconcretely, the probability that a query example x∗belongs to class k is deﬁned as:\np(y∗= k|x∗, S) =\nexp(−||g(x∗) −ck||2\n2)\nP\nk′∈{1,...,N} exp(−||g(x∗) −ck′||2\n2)\nwhere ck is the ‘prototype’ for class k: the average of the embeddings of class k’s support examples.\nMatching Networks\nMatching Networks (in their simplest form) label each query example as a\n(cosine) distance-weighted linear combination of the support labels:\np(y∗= k|x∗, S) =\n|S|\nX\ni=1\nα(x∗, xi)1yi=k,\nwhere 1A is the indicator function and α(x∗, xi) is the cosine similarity between g(x∗) and g(xi),\nsoftmax-normalized over all support examples xi, where 1 ≤i ≤|S|.\nRelation Networks\nRelation Networks are comprised of an embedding function g as usual, and\na ‘relation module’ parameterized by some additional neural network layers. They ﬁrst embed\neach support and query using g and create a prototype pc for each class c by averaging its support\nembeddings. Each prototype pc is concatenated with each embedded query and fed through the\nrelation module which outputs a number in [0, 1] representing the predicted probability that that query\nbelongs to class c. The query loss is then deﬁned as the mean square error of that prediction compared\nto the (binary) ground truth. Both g and the relation module are trained to minimize this loss.\nMAML\nMAML uses a linear layer parametrized by W and b on top of the embedding function\ng(·; θ) and classiﬁes a query example as\np(y∗|x∗, S) = softmax(b′ + W′g(x∗; θ′)),\nwhere the output layer parameters W′ and b′ and the embedding function parameters θ′ are obtained\nby performing a small number of within-episode training steps on the support set S, starting from\ninitial parameter values (b, W, θ). The model is trained by backpropagating the query set loss\nthrough the within-episode gradient descent procedure and into (b, W, θ). This normally requires\ncomputing second-order gradients, which can be expensive to obtain (both in terms of time and\nmemory). For this reason, an approximation is often used whereby gradients of the within-episode\ndescent steps are ignored. This variant is referred to as ﬁrst-order MAML (fo-MAML) and was used\nin our experiments. We did attempt to use the full-order version, but found it to be impractically\nexpensive (e.g., it caused frequent out-of-memory problems).\nMoreover, since in our setting the number of ways varies between episodes, b, W are set to zero and\nare not trained (i.e., b′, W′ are the result of within-episode gradient descent initialized at 0), leaving\nonly θ to be trained. In other words, MAML focuses on learning the within-episode initialization θ of\nthe embedding network so that it can be rapidly adapted for a new task.\n3\n",
    "Published as a conference paper at ICLR 2020\nIntroducing Proto-MAML\nWe introduce a novel meta-learner that combines the complementary\nstrengths of Prototypical Networks and MAML: the former’s simple inductive bias that is evidently\neffective for very-few-shot learning, and the latter’s ﬂexible adaptation mechanism.\nAs explained by Snell et al. (2017), Prototypical Networks can be re-interpreted as a linear classiﬁer\napplied to a learned representation g(x). The use of a squared Euclidean distance means that output\nlogits are expressed as\n−||g(x∗) −ck||2 = −g(x∗)T g(x∗) + 2cT\nk g(x∗) −cT\nk ck = 2cT\nk g(x∗) −||ck||2 + constant\nwhere constant is a class-independent scalar which can be ignored, as it leaves output probabilities\nunchanged. The k-th unit of the equivalent linear layer therefore has weights Wk,· = 2ck and biases\nbk = −||ck||2, which are both differentiable with respect to θ as they are a function of g(·; θ).\nWe refer to (fo-)Proto-MAML as the (fo-)MAML model where the task-speciﬁc linear layer of each\nepisode is initialized from the Prototypical Network-equivalent weights and bias deﬁned above and\nsubsequently optimized as usual on the given support set. When computing the update for θ, we\nallow gradients to ﬂow through the Prototypical Network-equivalent linear layer initialization. We\nshow that this simple modiﬁcation signiﬁcantly helps the optimization of this model and outperforms\nvanilla fo-MAML by a large margin on META-DATASET.\n3\nMETA-DATASET: A NEW FEW-SHOT CLASSIFICATION BENCHMARK\nMETA-DATASET aims to offer an environment for measuring progress in realistic few-shot classiﬁca-\ntion tasks. Our approach is twofold: 1) changing the data and 2) changing the formulation of the task\n(i.e., how episodes are generated). The following sections describe these modiﬁcations in detail. The\ncode is open source and publicly available1.\n3.1\nMETA-DATASET’S DATA\nMETA-DATASET’s data is much larger in size than any previous benchmark, and is comprised of\nmultiple existing datasets. This invites research into how diverse sources of data can be exploited\nby a meta-learner, and allows us to evaluate a more challenging generalization problem, to new\ndatasets altogether. Speciﬁcally, META-DATASET leverages data from the following 10 datasets:\nILSVRC-2012 (ImageNet, Russakovsky et al., 2015), Omniglot (Lake et al., 2015), Aircraft (Maji\net al., 2013), CUB-200-2011 (Birds, Wah et al., 2011), Describable Textures (Cimpoi et al., 2014),\nQuick Draw (Jongejan et al., 2016), Fungi (Schroeder & Cui, 2018), VGG Flower (Nilsback &\nZisserman, 2008), Trafﬁc Signs (Houben et al., 2013) and MSCOCO (Lin et al., 2014). These\ndatasets were chosen because they are free and easy to obtain, span a variety of visual concepts\n(natural and human-made) and vary in how ﬁne-grained the class deﬁnition is. More information\nabout each of these datasets is provided in the Appendix.\nTo ensure that episodes correspond to realistic classiﬁcation problems, each episode generated in\nMETA-DATASET uses classes from a single dataset. Moreover, two of these datasets, Trafﬁc Signs\nand MSCOCO, are fully reserved for evaluation, meaning that no classes from them participate in\nthe training set. The remaining ones contribute some classes to each of the training, validation and\ntest splits of classes, roughly with 70% / 15% / 15% proportions. Two of these datasets, ImageNet\nand Omniglot, possess a class hierarchy that we exploit in META-DATASET. For each dataset, the\ncomposition of splits is available online2.\nImageNet\nImageNet is comprised of 82,115 ‘synsets’, i.e., concepts of the WordNet ontology, and\nit provides ‘is-a’ relationships for its synsets, thus deﬁning a DAG over them. META-DATASET uses\nthe 1K synsets that were chosen for the ILSVRC 2012 classiﬁcation challenge and deﬁnes a new class\nsplit for it and a novel procedure for sampling classes from it for episode creation, both informed by\nits class hierarchy.\nSpeciﬁcally, we construct a sub-graph of the overall DAG whose leaves are the 1K classes of ILSVRC-\n2012. We then ‘cut’ this sub-graph into three pieces, for the training, validation, and test splits,\n1github.com/google-research/meta-dataset\n2github.com/google-research/meta-dataset/tree/master/meta_dataset/dataset_conversion/splits\n4\n",
    "Published as a conference paper at ICLR 2020\nsuch that there is no overlap between the leaves of any of these pieces. For this, we selected the\nsynsets ‘carnivore’ and ‘device’ as the roots of the validation and test sub-graphs, respectively. The\nleaves that are reachable from ‘carnivore’ and ‘device’ form the sets of the validation and test classes,\nrespectively. All of the remaining leaves constitute the training classes. This method of splitting\nensures that the training classes are semantically different from the test classes. We end up with 712\ntraining, 158 validation and 130 test classes, roughly adhering to the standard 70 / 15 / 15 (%) splits.\nOmniglot\nThis dataset is one of the established benchmarks for few-shot classiﬁcation as men-\ntioned earlier. However, contrary to the common setup that ﬂattens and ignores its two-level hierarchy\nof alphabets and characters, we allow it to inﬂuence the episode class selection in META-DATASET,\nyielding ﬁner-grained tasks. We also use the original splits proposed in Lake et al. (2015): (all char-\nacters of) the ‘background’ and ‘evaluation’ alphabets are used for training and testing, respectively.\nHowever, we reserve the 5 smallest alphabets from the ‘background’ set for validation.\n3.2\nEPISODE SAMPLING\nIn this section we outline META-DATASET’s algorithm for sampling episodes, featuring hierarchically-\naware procedures for sampling classes of ImageNet and Omniglot, and an algorithm that yields\nrealistically imbalanced episodes of variable shots and ways. The steps for sampling an episode for\na given split are: Step 0) uniformly sample a dataset D, Step 1) sample a set of classes C from the\nclasses of D assigned to the requested split, and Step 2) sample support and query examples from C.\nStep 1: Sampling the episode’s class set\nThis procedure differs depending on which dataset is\nchosen. For datasets without a known class organization, we sample the ‘way’ uniformly from the\nrange [5, MAX-CLASSES], where MAX-CLASSES is either 50 or as many as there are available.\nThen we sample ‘way’ many classes uniformly at random from the requested class split of the given\ndataset. ImageNet and Omniglot use class-structure-aware procedures outlined below.\nImageNet class sampling\nWe adopt a hierarchy-aware sampling procedure: First, we sample an\ninternal (non-leaf) node uniformly from the DAG of the given split. The chosen set of classes is then\nthe set of leaves spanned by that node (or a random subset of it, if more than 50). We prevent nodes\nthat are too close to the root to be selected as the internal node, as explained in more detail in the\nAppendix. This procedure enables the creation of tasks of varying degrees of ﬁne-grainedness: the\nlarger the height of the internal node, the more coarse-grained the resulting episode.\nOmniglot class sampling\nWe sample classes from Omniglot by ﬁrst sampling an alphabet uni-\nformly at random from the chosen split of alphabets (train, validation or test). Then, the ‘way’ of the\nepisode is sampled uniformly at random using the same restrictions as for the rest of the datasets, but\ntaking care not to sample a larger number than the number of characters that belong to the chosen\nalphabet. Finally, the prescribed number of characters of that alphabet are randomly sampled. This\nensures that each episode presents a within-alphabet ﬁne-grained classiﬁcation.\nStep 2: Sampling the episode’s examples\nHaving already selected a set of classes, the choice\nof the examples from them that will populate an episode can be broken down into three steps. We\nprovide a high-level description here and elaborate in the Appendix with the accompanying formulas.\nStep 2a: Compute the query set size The query set is class-balanced, reﬂecting the fact that we\ncare equally to perform well on all classes of an episode. The number of query images per class is\nset to a number such that all chosen classes have enough images to contribute that number and still\nremain with roughly half on their images to possibly add to the support set (in a later step). This\nnumber is capped to 10 images per class.\nStep 2b: Compute the support set size We allow each chosen class to contribute to the support set\nat most 100 of its remaining examples (i.e., excluding the ones added to the query set). We multiply\nthis remaining number by a scalar sampled uniformly from the interval (0, 1] to enable the potential\ngeneration of ‘few-shot’ episodes even when multiple images are available, as we are also interested\nin studying that end of the spectrum. We do enforce, however, that each chosen class has a budget for\nat least one image in the support set, and we cap the total support set size to 500 examples.\n5\n",
    "Published as a conference paper at ICLR 2020\nStep 2c: Compute the shot of each class We now discuss how to distribute the total support set\nsize chosen above across the participating classes. The un-normalized proportion of the support set\nthat will be occupied by a given chosen class is a noisy version of the total number of images of\nthat class in the dataset. This design choice is made in the hopes of obtaining realistic class ratios,\nunder the hypothesis that the dataset class statistics are a reasonable approximation of the real-world\nstatistics of appearances of the corresponding classes. We ensure that each class has at least one\nimage in the support set and distribute the rest according to the above rule.\nAfter these steps, we complete the episode creation process by choosing the prescribed number of\nexamples of each chosen class uniformly at random to populate the support and query sets.\n4\nRELATED WORK\nIn this work we evaluate four meta-learners on META-DATASET that we believe capture a good\ndiversity of well-established models. Evaluating other few-shot classiﬁers on META-DATASET is\nbeyond the scope of this paper, but we discuss some additional related models below.\nSimilarly to MAML, some train a meta-learner for quick adaptation to new tasks (Ravi & Larochelle,\n2017; Munkhdalai & Yu, 2017; Rusu et al., 2019; Yoon et al., 2018). Others relate to Prototypical\nNetworks by learning a representation on which differentiable training can be performed on some form\nof classiﬁer (Bertinetto et al., 2019; Gidaris & Komodakis, 2018; Oreshkin et al., 2018). Others relate\nto Matching Networks in that they perform comparisons between pairs of support and query examples,\nusing either a graph neural network (Satorras & Estrach, 2018) or an attention mechanism (Mishra\net al., 2018). Finally, some make use of memory-augmented recurrent networks (Santoro et al.,\n2016), some learn to perform data augmentation (Hariharan & Girshick, 2017; Wang et al., 2018) in\na low-shot learning setting, and some learn to predict the parameters of a large-shot classiﬁer from\nthe parameters learned in a few-shot setting (Wang & Hebert, 2016; Wang et al., 2017). Of relevance\nto Proto-MAML is MAML++ (Antoniou et al., 2019), which consists of a collection of adjustments\nto MAML, such as multiple meta-trained inner loop learning rates and derivative-order annealing.\nProto-MAML instead modiﬁes the output weight initialization scheme and could be combined with\nthose adjustments.\nFinally, META-DATASET relates to other recent image classiﬁcation benchmarks. The CVPR\n2017 Visual Domain Decathlon Challenge trains a model on 10 different datasets, many of which\nare included in our benchmark, and measures its ability to generalize to held-out examples for\nthose same datasets but does not measure generalization to new classes (or datasets). Hariharan\n& Girshick (2017) propose a benchmark where a model is given abundant data from certain base\nImageNet classes and is tested on few-shot learning novel ImageNet classes in a way that doesn’t\ncompromise its knowledge of the base classes. Wang et al. (2018) build upon that benchmark\nand propose a new evaluation protocol for it. Chen et al. (2019) investigate ﬁne-grained few-shot\nclassiﬁcation using the CUB dataset (Wah et al., 2011, also featured in our benchmark) and cross-\ndomain transfer between mini-ImageNet and CUB. Larger-scale few-shot classiﬁcation benchmarks\nwere also proposed using CIFAR-100 (Krizhevsky et al., 2009; Bertinetto et al., 2019; Oreshkin et al.,\n2018), tiered-ImageNet (Ren et al., 2018), and ImageNet-21k (Dhillon et al., 2019). Compared to\nthese, META-DATASET contains the largest set of diverse datasets in the context of few-shot learning\nand is additionally accompanied by an algorithm for creating learning scenarios from that data that\nwe advocate are more realistic than the previous ones.\n5\nEXPERIMENTS\nTraining procedure\nMETA-DATASET does not prescribe a procedure for learning from the training\ndata. In these experiments, keeping with the spirit of matching training and testing conditions, we\ntrained the meta-learners via training episodes sampled using the same algorithm as we used for\nMETA-DATASET’s evaluation episodes, described above. The choice of the dataset from which to\nsample the next episode was random uniform. The non-episodic baselines are trained to solve the\nlarge classiﬁcation problem that results from ‘concatenating’ the training classes of all datasets.\nValidation\nAnother design choice was to perform validation on (the validation split of) ImageNet\nonly, ignoring the validation sets of the other datasets. The rationale behind this choice is that the\n6\n",
    "Published as a conference paper at ICLR 2020\nTable 1: Few-shot classiﬁcation results on META-DATASET using models trained on ILSVRC-2012\nonly (top) and trained on all datasets (bottom).\nTest Source\nk-NN\nFinetune\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\nILSVRC\n41.03\n45.78\n45.00\n50.50\n45.51\n34.69\n49.53\nOmniglot\n37.07\n60.85\n52.27\n59.98\n55.55\n45.35\n63.37\nAircraft\n46.81\n68.69\n48.97\n53.10\n56.24\n40.73\n55.95\nBirds\n50.13\n57.31\n62.21\n68.79\n63.61\n49.51\n68.66\nTextures\n66.36\n69.05\n64.15\n66.56\n68.04\n52.97\n66.49\nQuick Draw\n32.06\n42.60\n42.87\n48.96\n43.96\n43.30\n51.52\nFungi\n36.16\n38.20\n33.97\n39.71\n32.10\n30.55\n39.96\nVGG Flower\n83.10\n85.51\n80.13\n85.27\n81.74\n68.76\n87.15\nTrafﬁc Signs\n44.59\n66.79\n47.80\n47.12\n50.93\n33.67\n48.83\nMSCOCO\n30.38\n34.86\n34.99\n41.00\n35.30\n29.15\n43.74\nAvg. rank\n5.7\n2.9\n4.65\n2.65\n3.7\n6.55\n1.85\nTest Source\nk-NN\nFinetune\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\nILSVRC\n38.55\n43.08\n36.08\n44.50\n37.83\n30.89\n46.52\nOmniglot\n74.60\n71.11\n78.25\n79.56\n83.92\n86.57\n82.69\nAircraft\n64.98\n72.03\n69.17\n71.14\n76.41\n69.71\n75.23\nBirds\n66.35\n59.82\n56.40\n67.01\n62.43\n54.14\n69.88\nTextures\n63.58\n69.14\n61.80\n65.18\n64.16\n56.56\n68.25\nQuick Draw\n44.88\n47.05\n60.81\n64.88\n59.73\n61.75\n66.84\nFungi\n37.12\n38.16\n33.70\n40.26\n33.54\n32.56\n41.99\nVGG Flower\n83.47\n85.28\n81.90\n86.85\n79.94\n76.08\n88.72\nTrafﬁc Signs\n40.11\n66.74\n55.57\n46.48\n42.91\n37.48\n52.42\nMSCOCO\n29.55\n35.17\n28.79\n39.87\n29.37\n27.41\n41.74\nAvg. rank\n5.05\n3.6\n4.95\n2.85\n4.25\n5.8\n1.5\nperformance on ImageNet has been known to be a good proxy for the performance on different\ndatasets. We used this validation performance to select our hyperparameters, including backbone\narchitectures, image resolutions and model-speciﬁc ones. We describe these further in the Appendix.\nPre-training\nWe gave each meta-learner the opportunity to initialize its embedding function from\nthe embedding weights to which the k-NN Baseline model trained on ImageNet converged to. We\ntreated the choice of starting from scratch or starting from this initialization as a hyperparameter.\nFor a fair comparison with the baselines, we allowed the non-episodic models to start from this\ninitialization too. This is especially important for the baselines in the case of training on all datasets\nsince it offers the opportunity to start from ImageNet-pretrained weights.\nMain results\nTable 1 displays the accuracy of each model on the test set of each dataset, after they\nwere trained on ImageNet-only or all datasets. Trafﬁc Signs and MSCOCO are not used for training\nin either case, as they are reserved for evaluation. We propose to use the average (over the datasets)\nrank of each method as our metric for comparison, where smaller is better. A method receives rank 1\nif it has the highest accuracy, rank 2 if it has the second highest, and so on. If two models share the\nbest accuracy, they both get rank 1.5, and so on. We ﬁnd that fo-Proto-MAML is the top-performer\naccording to this metric, Prototypical Networks also perform strongly, and the Finetune Baseline\nnotably presents a worthy opponent3. We include more detailed versions of these tables displaying\nconﬁdence intervals and per-dataset ranks in the Appendix.\nEffect of training on all datasets instead of ImageNet only\nIt’s interesting to examine whether\ntraining on (the training splits of) all datasets leads to improved generalizaton compared to training on\n(the training split of) ImageNet only. Speciﬁcally, while we might expect that training on more data\nhelps improve generalization, it is an empirical question whether that still holds for heterogeneous\ndata. We can examine this by comparing the performance of each model between the top and bottom\nsets of results of Table 1, corresponding to the two training sources (ImageNet only and all datasets,\nrespectively). For convenience, Figure 1 visualizes this difference in a barplot. Notably, for Omniglot,\nQuick Draw and Aircraft we observe a substantial increase across the board from training on all\n3We improved MAML’s performance signiﬁcantly in the time between the acceptance decision and publi-\ncation, thanks to a suggestion to use a more aggressive inner-loop learning rate. In the Appendix, we present\nthe older MAML results side by side with the new ones, and discuss the importance of this hyperparameter for\nMAML.\n7\n",
    "Published as a conference paper at ICLR 2020\n20\n10\n0\n10\n20\n30\n40\n50\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\nModel\nk-NN baseline\nFinetune baseline\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\n20\n10\n0\n10\n20\n30\n40\n50 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\nFigure 1: The performance difference on test datasets, when training on all datasets instead of\nILSVRC only. A positive value indicates an improvement from all-dataset training.\nsources. This is reasonable for datasets whose images are signiﬁcantly different from ImageNet’s:\nwe indeed expect to gain a large beneﬁt from training on some images from (the training classes of)\nthese datasets. Interestingly though, on the remainder of the test sources, we don’t observe a gain\nfrom all-dataset training. This result invites research into methods for exploiting heterogeneous data\nfor generalization to unseen classes of diverse sources. Our experiments show that learning ‘naively’\nacross the training datasets (e.g., by picking the next dataset to use uniformly at random) does not\nautomatically lead to that desired beneﬁt in most cases.\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nk-NN baseline\nFinetune baseline\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\n(a) Ways analysis\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\nClass Precision\nk-NN baseline\nFinetune baseline\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\n(b) Shots analysis\nFigure 2: The effect of different ways and shots on test performance (w/ 95% conﬁdence intervals)\nwhen training on ImageNet.\nWays and shots analysis\nWe further study the accuracy as a function of ‘ways’ (Figure 2a) and the\nclass precision as a function of ‘shots’ (Figure 2b). As expected, we found that the difﬁculty increases\nas the way increases, and performance degrades. More examples per class, on the other hand, indeed\nmake it easier to correctly classify that class. Interestingly, though, not all models beneﬁt at the\nsame rate from more data: Prototypical Networks and fo-Proto-MAML outshine other models in\nvery-low-shot settings but saturate faster, whereas the Finetune baseline, Matching Networks, and\nfo-MAML improve at a higher rate when the shot increases. We draw the same conclusions when\nperforming this analysis on all datasets, and include those plots in the Appendix. As discussed in the\nAppendix, we recommend including this analysis when reporting results on Meta-Dataset, aside from\nthe main table. The rationale is that we’re not only interested in performing well on average, but also\nin performing well under different speciﬁcations of test tasks.\nEffect of pre-training\nIn Figures 3a and 3b, we quantify how beneﬁcial it is to initialize the\nembedding network of meta-learners using the weights of the k-NN baseline pre-trained on ImageNet,\nas opposed to starting their episodic training from scratch. We ﬁnd this procedure to often be\nbeneﬁcial, both for ImageNet-only training and for training on all datasets. It seems that this\nImageNet-inﬂuenced initialization drives the meta-learner towards a solution which yields increased\nperformance on natural image test datasets, especially ILSVRC, Birds, Fungi, Flowers and MSCOCO.\n8\n",
    "Published as a conference paper at ICLR 2020\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\nModel\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\nInitialization\nPre-trained\nFrom scratch\n(a) Effect of pre-training (ImageNet)\n0\n20\n40\n60\n80\n100\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\n0\n20\n40\n60\n80\n100 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\n(b) Effect of pre-training (All datasets)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\nModel\nMatchingNet\nProtoNet\nfo-Proto-MAML\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\nMeta-training\nInference-only\n(c) Effect of meta-learning (ImageNet)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\n(d) Effect of meta-learning (All datasets)\nFigure 3: The effects of pre-training and meta-training (w/ 95% conﬁdence intervals). (ImageNet) or\n(All datasets) is the training source.\nPerhaps unsusprisingly, though, it underperforms on signiﬁcantly different datasets such as Omniglot\nand Quick Draw. These ﬁndings show that, aside from the choice of the training data source(s) (e.g.,\nImageNet only or all datasets, as discussed above), the choice of the initialization scheme can also\ninﬂuence to an important degree the ﬁnal solution and consequently the aptness of applying the\nresulting meta-learner to different data sources at test time. Finally, an interesting observation is\nthat MAML seems to beneﬁt the most from the pre-trained initialization, which may speak to the\ndifﬁculty of optimization associated with that model.\nEffect of meta-training\nWe propose to disentangle the inference algorithm of each meta-learner\nfrom the fact that it is meta-learned, to assess the beneﬁt of meta-learning on META-DATASET.\nTo this end, we propose a new set of baselines: ‘Prototypical Networks Inference’, ‘Matching\nNetworks Inference’, and ‘fo-Proto-MAML Inference’, that are trained non-episodically but evaluated\nepisodically (for validation and testing) using the inference algorithm of the respective meta-learner.\nThis is possible for these meta-learners as they don’t have any additional parameters aside from the\nembedding function that explicitly need to be learned episodically (as opposed to the relation module\nof Relation Networks, for example). We compare each Inference-only method to its corresponding\nmeta-learner in Figures 3c and 3d. We ﬁnd that these baselines are strong: when training on ImageNet\nonly, we can usually observe a small beneﬁt from meta-learning the embedding weights but this\nbeneﬁt often disappears when training on all datasets, in which case meta-learning sometimes actually\nhurts. We ﬁnd this result very interesting and we believe it emphasizes the need for research on how\nto meta-learn across multiple diverse sources, an important challenge that META-DATASET puts\nforth.\nFine-grainedness analysis\nWe use ILVRC-2012 to investigate the hypothesis that ﬁner-grained\ntasks are harder than coarse-grained ones. Our ﬁndings suggest that while the test sub-graph is not\nrich enough to exhibit any trend, the performance on the train sub-graph does seem to agree with this\nhypothesis. We include the experimental setup and results for this analysis in the Appendix.\n6\nCONCLUSION\nWe have introduced a new large-scale, diverse, and realistic environment for few-shot classiﬁcation.\nWe believe that our exploration of various models on META-DATASET has uncovered interesting\n9\n",
    "Published as a conference paper at ICLR 2020\ndirections for future work pertaining to meta-learning across heterogeneous data: it remains unclear\nwhat is the best strategy for creating training episodes, the most appropriate validation creation and\nthe most appropriate initialization. Current models don’t always improve when trained on multiple\nsources and meta-learning is not always beneﬁcial across datasets. Current models are also not robust\nto the amount of data in test episodes, each excelling in a different part of the spectrum. We believe\nthat addressing these shortcomings consitutes an important research goal moving forward.\nAUTHOR CONTRIBUTIONS\nEleni, Hugo, and Kevin came up with the benchmark idea and requirements. Eleni developed the core\nof the project, and worked on the experiment design and management with Tyler and Kevin, as well as\nexperiment analysis. Carles, Ross, Kelvin, Pascal, Vincent, and Tyler helped extend the benchmark by\nadding datasets. Eleni, Vincent, and Utku contributed the Prototypical Networks, Matching Networks,\nand Relation Networks implementations, respectively. Tyler implemented baselines, MAML (with\nKevin) and Proto-MAML models, and updated the backbones to support them. Writing was mostly\nled by Eleni, with contributions by Hugo, Vincent, and Kevin and help from Tyler and Pascal for\nvisualizations. Pascal and Pierre-Antoine worked on code organization, efﬁciency, and open-sourcing,\nPascal and Vincent optimized the efﬁciency of the data input pipeline. Pierre-Antoine supervised the\ncode development process and reviewed most of the changes, Hugo and Kevin supervised the overall\ndirection of the research.\nACKNOWLEDGMENTS\nWe would like to thank Chelsea Finn for fruitful discussions and advice on tuning fo-MAML and\nensuring the correctness of implementation, as well as Zack Nado and Dan Moldovan for the initial\ndataset code that was adapted, and Cristina Vasconcelos for spotting an issue in the ranking of\nmodels. Finally, we’d like to thank John Bronskill for suggesting that we experiment with a larger\ninner-loop learning rate for MAML which indeed signiﬁcantly improved our fo-MAML results on\nMETA-DATASET.\n10\n",
    "Published as a conference paper at ICLR 2020\nREFERENCES\nAntreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your MAML. In Proceedings\nof the International Conference on Learning Representations, 2019.\nLuca Bertinetto, Joao F. Henriques, Philip Torr, and Andrea Vedaldi. Meta-learning with differentiable\nclosed-form solvers. In Proceedings of the International Conference on Learning Representations,\n2019.\nWei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer\nlook at few-shot classiﬁcation. In Proceedings of the International Conference on Learning\nRepresentations, 2019.\nM. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi. Describing textures in the wild. In\nIEEE Conference on Computer Vision and Pattern Recognition, 2014.\nGuneet S. Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline for\nfew-shot image classiﬁcation. arXiv, abs/1909.02729, 2019.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of\ndeep networks. In Proceedings of the International Conference of Machine Learning, 2017.\nSpyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In IEEE\nConference on Computer Vision and Pattern Recognition, 2018.\nBharath Hariharan and Ross Girshick. Low-shot visual recognition by shrinking and hallucinating\nfeatures. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3018–3027,\n2017.\nSebastian Houben, Johannes Stallkamp, Jan Salmen, Marc Schlipsing, and Christian Igel. Detection of\ntrafﬁc signs in real-world images: The German Trafﬁc Sign Detection Benchmark. In International\nJoint Conference on Neural Networks, 2013.\nJonas Jongejan, Henry Rowley, Takashi Kawashima, Jongmin Kim, and Nick Fox-Gieg. The Quick,\nDraw! – A.I. experiment. quickdraw.withgoogle.com, 2016.\nAlex Krizhevsky et al. Learning multiple layers of features from tiny images. Technical report,\nUniversity of Toronto, 2009.\nBrenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning\nthrough probabilistic program induction. Science, 350(6266):1332–1338, 2015.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollár, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In European\nConference on Computer Vision, pp. 740–755, 2014.\nSubhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained\nvisual classiﬁcation of aircraft. arXiv, abs/1306.5151, 2013.\nNikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-\nlearner. In Proceedings of the International Conference on Learning Representations, 2018.\nTsendsuren Munkhdalai and Hong Yu. Meta networks. In Proceedings of the International Conference\non Machine Learning, pp. 2554–2563, 2017.\nM-E. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over a large number of classes. In\nProceedings of the Indian Conference on Computer Vision, Graphics and Image Processing, 2008.\nBoris N. Oreshkin, Pau Rodriguez, and Alexandre Lacoste. TADAM: Task dependent adaptive\nmetric for improved few-shot learning. In Advances in Neural Information Processing Systems, pp.\n719–729, 2018.\nHang Qi, Matthew Brown, and David G Lowe. Low-shot learning with imprinted weights. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5822–5830,\n2018.\n11\n",
    "Published as a conference paper at ICLR 2020\nSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In Proceedings of\nthe International Conference on Learning Representations, 2017.\nMengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum,\nHugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classiﬁcation.\nIn Proceedings of the International Conference on Learning Representations, 2018.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, and Li Fei-Fei. Imagenet\nlarge scale visual recognition challenge. International Journal of Computer Vision, 115(3):211–252,\n2015.\nAndrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero,\nand Raia Hadsell. Meta-learning with latent embedding optimization. In Proceedings of the\nInternational Conference on Learning Representations, 2019.\nTim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate\ntraining of deep neural networks. In Advances in Neural Information Processing Systems, pp.\n901–909, 2016.\nAdam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-\nlearning with memory-augmented neural networks. In Proceedings of the International Conference\non Machine Learning, pp. 1842–1850, 2016.\nVictor Garcia Satorras and Joan Bruna Estrach. Few-shot learning with graph neural networks. In\nProceedings of the International Conference on Learning Representations, 2018.\nBrigit Schroeder and Yin Cui.\nFGVCx fungi classiﬁcation challenge 2018.\ngithub.com/\nvisipedia/fgvcx_fungi_comp, 2018.\nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In\nAdvances in Neural Information Processing Systems, pp. 4077–4087, 2017.\nFlood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.\nLearning to compare: Relation network for few-shot learning. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pp. 1199–1208, 2018.\nOriol Vinyals, Charles Blundell, Tim Lillicrap, and Daan Wierstra. Matching networks for one shot\nlearning. In Advances in Neural Information Processing Systems, pp. 3630–3638, 2016.\nC. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011\nDataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.\nYu-Xiong Wang and Martial Hebert. Learning to learn: Model regression networks for easy small\nsample learning. In European Conference on Computer Vision, pp. 616–634. Springer, 2016.\nYu-Xiong Wang, Deva Ramanan, and Martial Hebert. Learning to model the tail. In Advances in\nNeural Information Processing Systems, pp. 7029–7039, 2017.\nYu-Xiong Wang, Ross Girshick, Martial Hebert, and Bharath Hariharan. Low-shot learning from\nimaginary data. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni-\ntion, pp. 7278–7286, 2018.\nJaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn.\nBayesian model-agnostic meta-learning. In Advances in Neural Information Processing Systems,\n2018.\n12\n",
    "Published as a conference paper at ICLR 2020\nAPPENDIX\n.1\nRECOMMENDATION FOR REPORTING RESULTS ON META-DATASET\nWe recommend that future work on META-DATASET reports two sets of results:\n1. The main tables storing the average (over 600 test episodes) accuracy of each method on\neach dataset, after it has been trained on ImageNet only and on All datasets, where the\nevaluation metric is the average rank. This corresponds to Table 1 in our case (or the more\ncomplete version in Table 2 in the Appendix).\n2. The plots that measure robustness in variations of shots and ways. In our case these are\nFigures 2b and 2a in the main text for ImageNet-only training, and Figures 5b and 5a in the\nAppendix for the case of training on all datasets.\nWe propose to use both of these aspects to evaluate performance on META-DATASET: it is not only\ndesirable to perform well on average, but also to perform well under different speciﬁcations of test\ntasks, as it is not realistic in general to assume that we will know in advance what setup (number\nof ways and shots) will be encountered at test time. Our ﬁnal source code will include scripts for\ngenerating these plots and for automatically computing ranks given a table to help standardize the\nprocedure for reporting results.\n.2\nDETAILS OF META-DATASET’S SAMPLING ALGORITHM\nWe now provide a complete description of certain steps that were explained on a higher level in the\nmain paper.\nSTEP 1: SAMPLING THE EPISODE’S CLASS SET\nImageNet class sampling\nThe procedure we use for sampling classes for an ImageNet episode\nis the following. First, we sample a node uniformly at random from the set of ‘eligible’ nodes of\nthe DAG structure corresponding to the speciﬁed split (train, validation or test). An internal node is\n‘eligible’ for this selection if it spans at least 5 leaves, but no more than 392 leaves. The number 392\nwas chosen because it is the smallest number so that, collectively, all eligible internal nodes span all\nleaves in the DAG. Once an eligible node is selected, some of the leaves that it spans will constitute\nthe classes of the episode. Speciﬁcally, if the number of those leaves is no greater than 50, we use all\nof them. Otherwise, we randomly choose 50 of them.\nThis procedure enables the creation of tasks of varying degrees of ﬁne-grainedness. For instance, if\nthe sampled internal node has a small height, the leaf classes that it spans will represent semantically-\nrelated concepts, thus posing a ﬁne-grained classiﬁcation task. As the height of the sampled node\nincreases, we ‘zoom out’ to consider a broader scope from which we sample classes and the resulting\nepisodes are more coarse-grained.\nSTEP 2: SAMPLING THE EPISODE’S EXAMPLES\na) Computing the query set size\nThe query set is class-balanced, reﬂecting the fact that we care\nequally to perform well on all classes of an episode. The number of query images per class is\ncomputed as:\nq = min\n\u001a\n10,\n\u0012\nmin\nc∈C ⌊0.5 ∗|Im(c)|⌋\n\u0013\u001b\nwhere C is the set of selected classes and Im(c) denotes the set of images belonging to class c. The\nmin over classes ensures that each class has at least q images to add to the query set, thus allowing it\nto be class-balanced. The 0.5 multiplier ensures that enough images of each class will be available to\nadd to the support set, and the minimum with 10 prevents the query set from being too large.\nb) Computing the support set size\nWe compute the total support set size as:\n|S| = min\n(\n500,\nX\nc∈C\n⌈β min{100, |Im(c)| −q}⌉\n)\n13\n",
    "Published as a conference paper at ICLR 2020\nwhere β is a scalar sampled uniformly from interval (0, 1]. Intuitively, each class on average\ncontributes either all its remaining examples (after placing q of them in the query set) if there are less\nthan 100 or 100 otherwise, to avoid having too large support sets. The multiplication with β enables\nthe potential generation of smaller support sets even when multiple images are available, since we are\nalso interested in examining the very-low-shot end of the spectrum. The ‘ceiling’ operation ensures\nthat each selected class will have at least one image in the support set. Finally, we cap the total\nsupport set size to 500.\nc) Computing the shot of each class\nWe are now ready to compute the ‘shot’ of each class.\nSpeciﬁcally, the proportion of the support set that will be devoted to class c is computed as:\nRc =\nexp(αc)|Im(c)|\nX\nc′∈C\nexp(α′\nc)|Im(c′)|\nwhere αc is sampled uniformly from the interval [log(0.5), log(2)). Intuitively, the un-normalized\nproportion of the support set that will be occupied by class c is a noisy version of the total number of\nimages of that class in the dataset Im(c). This design choice is made in the hopes of obtaining realistic\nclass ratios, under the hypothesis that the dataset class statistics are a reasonable approximation of the\nreal-world statistics of appearances of the corresponding classes. The shot of a class c is then set to:\nkc = min {⌊Rc ∗(|S| −|C|)⌋+ 1, |Im(c)| −q}\nwhich ensures that at least one example is selected for each class, with additional examples selected\nproportionally to Rc, if enough are available.\n.3\nDATASETS\nMETA-DATASET is formed of data originating from 10 different image datasets. A complete list of\nthe datasets we use is the following.\n(a) ImageNet\n(b) Omniglot\n(c) Aircraft\n(d) Birds\n(e) DTD\n(f) Quick Draw\n(g) Fungi\n(h) VGG Flower\n(i) Trafﬁc Signs\n(j) MSCOCO\nFigure 4: Training examples taken from the various datasets forming META-DATASET.\nILSVRC-2012 (ImageNet, Russakovsky et al., 2015)\nA dataset of natural images from 1000\ncategories (Figure 4a). We removed some images that were duplicates of images in another dataset in\nMETA-DATASET (43 images that were also part of Birds) or other standard datasets of interest (92\nfrom Caltech-101 and 286 from Caltech-256). The complete list of duplicates is part of the source\ncode release.\nOmniglot (Lake et al., 2015)\nA dataset of images of 1623 handwritten characters from 50 different\nalphabets, with 20 examples per class (Figure 4b). While recently Vinyals et al. (2016) proposed a\n14\n",
    "Published as a conference paper at ICLR 2020\nnew split for this dataset, we instead make use of the original intended split Lake et al. (2015) which\nis more challenging since the split is on the level of alphabets (30 training alphabets and 20 evaluation\nalphabets), not characters from those alphabets, therefore posing a more challenging generalization\nproblem. Out of the 30 training alphabets, we hold out the 5 smallest ones (i.e., with the least number\nof character classes) to form our validation set, and use the remaining 25 for training.\nAircraft (Maji et al., 2013)\nA dataset of images of aircrafts spanning 102 model variants, with\n100 images per class (Figure 4c). The images are cropped according to the providing bounding boxes,\nin order not to include other aircrafts, or the copyright text at the bottom of images.\nCUB-200-2011 (Birds, Wah et al., 2011)\nA dataset for ﬁne-grained classiﬁcation of 200 different\nbird species (Figure 4d). We did not use the provided bounding boxes to crop the images, instead the\nfull images are used, which provides a harder challenge.\nDescribable Textures (DTD, Cimpoi et al., 2014)\nA texture database, consisting of 5640 images,\norganized according to a list of 47 terms (categories) inspired from human perception (Figure 4e).\nQuick Draw (Jongejan et al., 2016)\nA dataset of 50 million black-and-white drawings across 345\ncategories, contributed by players of the game Quick, Draw! (Figure 4f).\nFungi (Schroeder & Cui, 2018)\nA large dataset of approximately 100K images of nearly 1,500\nwild mushrooms species (Figure 4g).\nVGG Flower (Nilsback & Zisserman, 2008)\nA dataset of natural images of 102 ﬂower categories.\nThe ﬂowers chosen to be ones commonly occurring in the United Kingdom. Each class consists of\nbetween 40 and 258 images (Figure 4h).\nTrafﬁc Signs (Houben et al., 2013)\nA dataset of 50,000 images of German road signs in 43 classes\n(Figure 4i).\nMSCOCO Lin et al. (2014)\nA dataset of images collected from Flickr with 1.5 million object\ninstances belonging to 80 classes labelled and localized using bounding boxes. We choose the\ntrain2017 split and create images crops from original images using each object instance’s groundtruth\nbounding box (Figure 4j).\n.4\nHYPERPARAMETERS\nWe used three architectures: a commonly-used four-layer convolutional network, an 18-layer residual\nnetwork and a wide residual network. While some of the baseline models performed best with the\nlatter, we noticed that the meta-learners preferred the resnet-18 backbone and rarely the four-layer-\nconvnet. For Relation Networks only, we also allow the option to use another architecture, aside from\nthe aforementioned three, inspired by the four-layer-convnet used in the Relation Networks paper\n(Sung et al., 2018). The main difference is that they used the usual max-pooling operation only in the\nﬁrst two layers, omitting it in the last two, yielding activations of larger spatial dimensions. In our\ncase, we found that these increased spatial dimensions did not ﬁt in memory, so as a compromise we\nused max-pooling on the ﬁrst 3 out of the 4 layer of the convnet.\nFor fo-MAML and fo-Proto-MAML, we tuned the inner-loop learning rate, the number of inner loop\nsteps, and the number of additional such steps to be performed in evaluation (i.e., validation or test)\nepisodes.\nFor the baselines, we tuned whether the cosine classiﬁer of Baseline++ will be used, as opposed\nto a standard forward pass through a linear classiﬁcation layer. Also, since Chen et al. (2019)\nadded weight normalization (Salimans & Kingma, 2016) to their implementation of the cosine\nclassiﬁer layer, we also implemented this and created a hyperparameter choice for whether or not it\nis enabled. This hyperparameter is independent from the one that decides if the cosine classiﬁer is\nused. Both are applicable to the k-NN Basline (for its all-way training classiﬁcation task) and to the\nFinetune Baseline (both for its all-way training classiﬁcation and for its within-episode classiﬁcation\n15\n",
    "Published as a conference paper at ICLR 2020\nat validation and test times). For the Finetune Baseline, we tuned a binary hyperparameter deciding if\ngradient descent or ADAM is used for the within-task optimization. We also tuned the decision of\nwhether all embedding layers are ﬁnetuned or, alternatively, the embedding is held ﬁxed and only the\nﬁnal classiﬁer on top of it is optimized. Finally, we tuned the number of ﬁnetuning steps that will be\ncarried out.\nWe also tried two different image resolutions: the commonly-used 84x84 and 126x126. Finally, we\ntuned the learning rate schedule and weight decay and we used ADAM to train all of our models. All\nother details, dataset splits and the complete set of best hyperparameters discovered for each model\nare included in the source code.\n.5\nCOMPLETE MAIN RESULTS AND RANK COMPUTATION\nRank computation\nWe rank models by decreasing order of accuracy and handle ties by assigning\ntied models the average of their ranks. A tie between two models occurs when a 95% conﬁdence\ninterval statistical test on the difference between their mean accuracies is inconclusive in rejecting the\nnull hypothesis that this difference is 0. Our recommendation is that this test is ran to determine if\nties occur. As mentioned earlier, our source code will include this computation.\nComplete main tables\nFor completeness, Table 2 presents a more detailed version of Table 1 that\nalso displays conﬁdence intervals and per-dataset ranks computed using the above procedure.\n.6\nANALYSIS OF PERFORMANCE ACROSS SHOTS AND WAYS\nFor completeness, in Figure 5 we show the results of the analysis of the robustness to different ways\nand shots for the variants of the models that were trained on all datasets. We observe the same trends\nas discussed in our Experiments section for the variants of the models that were trained on ImageNet.\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nk-NN baseline\nFinetune baseline\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\n(a) Ways Analysis\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\nClass Precision\nk-NN baseline\nFinetune baseline\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\n(b) Shots Analysis\nFigure 5: Analysis of performance as a function of the episode’s way, shots for models whose\ntraining source is (the training data of) all datasets. The bands display 95% conﬁdence intervals.\n.7\nEFFECT OF TRAINING ON ALL DATASETS OVER TRAINING ON ILSVRC-2012 ONLY\nFor more clearly observing whether training on all datasets leads to improved generalization over\ntraining on ImageNet only, Figure 6 shows side-to-side the performance of each model trained on\nILSVRC only vs. all datasets. The difference between the performance of the all-dataset trained\nmodels versus the ImageNet-only trained ones is also visualized in Figure 1 in the main paper.\nAs discussed in the main paper, we notice that we do not always observe a clear generalization\nadvantage in training from a wider collection of image datasets. While some of the datasets that were\nadded to the meta-training phase did see an improvement across all models, in particular for Omniglot\nand Quick Draw, this was not true across the board. In fact, in certain cases the performance is\nslightly worse. We believe that more successfully leveraging diverse sources of data is an interesting\nopen research problem.\n16\n",
    "Published as a conference paper at ICLR 2020\nTable 2: Few-shot classiﬁcation results on META-DATASET.\n(a) Models trained on ILSVRC-2012 only.\nTest Source\nMethod: Accuracy (%) ± conﬁdence (%)\nk-NN\nFinetune\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nProto-MAML\nILSVRC\n41.03±1.01 (6)\n45.78±1.10 (4)\n45.00±1.10 (4)\n50.50±1.08 (1.5)\n45.51±1.11 (4)\n34.69±1.01 (7)\n49.53±1.05 (1.5)\nOmniglot\n37.07±1.15 (7)\n60.85±1.58 (2.5)\n52.27±1.28 (5)\n59.98±1.35 (2.5)\n55.55±1.54 (4)\n45.35±1.36 (6)\n63.37±1.33 (1)\nAircraft\n46.81±0.89 (6)\n68.69±1.26 (1)\n48.97±0.93 (5)\n53.10±1.00 (4)\n56.24±1.11 (2.5)\n40.73±0.83 (7)\n55.95±0.99 (2.5)\nBirds\n50.13±1.00 (6.5)\n57.31±1.26 (5)\n62.21±0.95 (3.5)\n68.79±1.01 (1.5)\n63.61±1.06 (3.5)\n49.51±1.05 (6.5)\n68.66±0.96 (1.5)\nTextures\n66.36±0.75 (4)\n69.05±0.90 (1.5)\n64.15±0.85 (6)\n66.56±0.83 (4)\n68.04±0.81 (1.5)\n52.97±0.69 (7)\n66.49±0.83 (4)\nQuick Draw\n32.06±1.08 (7)\n42.60±1.17 (4.5)\n42.87±1.09 (4.5)\n48.96±1.08 (2)\n43.96±1.29 (4.5)\n43.30±1.08 (4.5)\n51.52±1.00 (1)\nFungi\n36.16±1.02 (4)\n38.20±1.02 (3)\n33.97±1.00 (5)\n39.71±1.11 (1.5)\n32.10±1.10 (6)\n30.55±1.04 (7)\n39.96±1.14 (1.5)\nVGG Flower\n83.10±0.68 (4)\n85.51±0.68 (2.5)\n80.13±0.71 (6)\n85.27±0.77 (2.5)\n81.74±0.83 (5)\n68.76±0.83 (7)\n87.15±0.69 (1)\nTrafﬁc Signs\n44.59±1.19 (6)\n66.79±1.31 (1)\n47.80±1.14 (3.5)\n47.12±1.10 (5)\n50.93±1.51 (2)\n33.67±1.05 (7)\n48.83±1.09 (3.5)\nMSCOCO\n30.38±0.99 (6.5)\n34.86±0.97 (4)\n34.99±1.00 (4)\n41.00±1.10 (2)\n35.30±1.23 (4)\n29.15±1.01 (6.5)\n43.74±1.12 (1)\nAvg. rank\n5.7\n2.9\n4.65\n2.65\n3.7\n6.55\n1.85\n(b) Models trained on all datasets.\nTest Source\nMethod: Accuracy (%) ± conﬁdence (%) (rank)\nk-NN\nFinetune\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nProto-MAML\nILSVRC\n38.55±0.94 (4.5)\n43.08±1.08 (2.5)\n36.08±1.00 (6)\n44.50±1.05 (2.5)\n37.83±1.01 (4.5)\n30.89±0.93 (7)\n46.52±1.05 (1)\nOmniglot\n74.60±1.08 (6)\n71.11±1.37 (7)\n78.25±1.01 (4.5)\n79.56±1.12 (4.5)\n83.92±0.95 (2.5)\n86.57±0.79 (1)\n82.69±0.97 (2.5)\nAircraft\n64.98±0.82 (7)\n72.03±1.07 (3.5)\n69.17±0.96 (5.5)\n71.14±0.86 (3.5)\n76.41±0.69 (1)\n69.71±0.83 (5.5)\n75.23±0.76 (2)\nBirds\n66.35±0.92 (2.5)\n59.82±1.15 (5)\n56.40±1.00 (6)\n67.01±1.02 (2.5)\n62.43±1.08 (4)\n54.14±0.99 (7)\n69.88±1.02 (1)\nTextures\n63.58±0.79 (5)\n69.14±0.85 (1.5)\n61.80±0.74 (6)\n65.18±0.84 (3.5)\n64.14±0.83 (3.5)\n56.56±0.73 (7)\n68.25±0.81 (1.5)\nQuick Draw\n44.88±1.05 (7)\n47.05±1.16 (6)\n60.81±1.03 (3.5)\n64.88±0.89 (2)\n59.73±1.10 (5)\n61.75±0.97 (3.5)\n66.84±0.94 (1)\nFungi\n37.12±1.06 (3.5)\n38.16±1.04 (3.5)\n33.70±1.04 (6)\n40.26±1.13 (2)\n33.54±1.11 (6)\n32.56±1.08 (6)\n41.99±1.17 (1)\nVGG Flower\n83.47±0.61 (4)\n85.28±0.69 (3)\n81.90±0.72 (5)\n86.85±0.71 (2)\n79.94±0.84 (6)\n76.08±0.76 (7)\n88.72±0.67 (1)\nTrafﬁc Signs\n40.11±1.10 (6)\n66.74±1.23 (1)\n55.57±1.08 (2)\n46.48±1.00 (4)\n42.91±1.31 (5)\n37.48±0.93 (7)\n52.42±1.08 (3)\nMSCOCO\n29.55±0.96 (5)\n35.17±1.08 (3)\n28.79±0.96 (5)\n39.87±1.06 (2)\n29.37±1.08 (5)\n27.41±0.89 (7)\n41.74±1.13 (1)\nAvg. rank\n5.05\n3.6\n4.95\n2.85\n4.25\n5.8\n1.5\n17\n",
    "Published as a conference paper at ICLR 2020\nFigure 6: Accuracy on the test datasets, when training on ILSVRC only or All datasets (same results\nas shown in the main tables). The bars display 95% conﬁdence intervals.\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\nModel\nk-NN baseline\nFinetune baseline\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\nTrain Source\nILSVRC-2012\nAll datasets\n.8\nEFFECT OF PRE-TRAINING VERSUS TRAINING FROM SCRATCH\nFor each meta-learner, we selected the best model (based on validation on ImageNet’s validation\nsplit) out of the ones that used the pre-trained initialization, and the best out of the ones that trained\nfrom scratch. We then ran the evaluation of each on (the test split of) all datasets in order to quantify\nhow beneﬁcial this pre-trained initialization is. We performed this experiment twice: for the models\nthat are trained on ImageNet only and for the models that are trained on (the training splits of) all\ndatasets.\nThe results of this investigation were reported in the main paper in Figure 3a and Figure 3b, for\nImageNet-only training and all dataset training, respectively. We show the same results in Figure 7,\nprinted larger to facilitate viewing of error bars. For easier comparison, we also plot the difference in\nperformance of the models that were pre-trained over the ones that weren’t, in Figures 8a and 8b.\nThese ﬁgures make it easier to spot that while using the pre-trained solution usually helps for datasets\nthat are visually not too different from ImageNet, it may hurt for datasets that are signiﬁcantly different\nfrom it, such as Omniglot, Quickdraw (and surprisingly Aircraft). Note that these three datasets\nare the same three that we found beneﬁt from training on All datasets instead of ImageNet-only. It\nappears that using the pre-trained solution biases the ﬁnal solution to specialize on ImageNet-like\ndatasets.\n.9\nEFFECT OF META-LEARNING VERSUS INFERENCE-ONLY\nFigure 9 shows the same plots as in Figures 3c and 3d but printed larger to facilitate viewing of\nerror bars. Furthermore, as we have done for visualizing the observed gain of pre-training, we also\npresent in Figures 10a and 10b the gain observed from meta-learning as opposed to training the\ncorresponding inference-only baseline, as explained in the Experiments section of the main paper.\nThis visulization makes it clear that while meta-training usually helps on ImageNet (or doesn’t hurt\ntoo much), it sometimes hurts when it is performed on all datasets, emphasizing the need for further\nresearch into best practices of meta-learning across heterogeneous sources.\n.10\nFINEGRAINEDNESS ANALYSIS\nWe investigate the hypothesis that ﬁner-grained tasks are more challenging than coarse-grained ones\nby creating binary ImageNet episodes with the two classes chosen uniformly at random from the\nDAG’s set of leaves. We then deﬁne the degree of coarse-grainedness of a task as the height of\nthe lowest common ancestor of the two chosen leaves, where the height is deﬁned as the length of\nthe longest path from the lowest common ancestor to one of the selected leaves. Larger heights\nthen correspond to coarser-grained tasks. We present these results in Figure 11. We do not detect a\nsigniﬁcant trend when performing this analysis on the test DAG. The results on the training DAG,\n18\n",
    "Published as a conference paper at ICLR 2020\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\nModel\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\nInitialization\nPre-trained\nFrom scratch\n(a) ImageNet.\n0\n20\n40\n60\n80\n100\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\nModel\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\n0\n20\n40\n60\n80\n100 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\nInitialization\nPre-trained\nFrom scratch\n(b) All datasets.\nFigure 7: Comparing pre-training to starting from scratch. Same plots as Figure 3a and Figure 3b,\nonly larger.\n15\n10\n5\n0\n5\n10\n15\n20\n25\n30\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\nModel\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\n15\n10\n5\n0\n5\n10\n15\n20\n25\n30 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\n(a) The gain from pre-training (ImageNet).\n20\n10\n0\n10\n20\n30\n40\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\nModel\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\n20\n10\n0\n10\n20\n30\n40 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\n(b) The gain from pre-training (All datasets).\nFigure 8: The performance difference of initializing the embedding weights from a pre-trained\nsolution, before episodically training on ImageNet or all datasets, over using a random initialization\nof those weights. The pre-trained weights that we consider are the ones that the k-NN baseline\nconverged to when it was trained on ImageNet. Positive values indicate that this pre-training was\nbeneﬁcial.\n19\n",
    "Published as a conference paper at ICLR 2020\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\nModel\nMatchingNet\nProtoNet\nfo-Proto-MAML\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\nMeta-training\nInference-only\n(a) ImageNet.\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\nModel\nMatchingNet\nProtoNet\nfo-Proto-MAML\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\nMeta-training\nInference-only\n(b) All datasets.\nFigure 9: Comparing the meta-trained variant of meta-learners against their inference-only counter-\npart. Same plots as Figure 3c and Figure 3d, only larger.\n5\n0\n5\n10\n15\n20\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\nModel\nMatchingNet\nProtoNet\nfo-Proto-MAML\n5\n0\n5\n10\n15\n20 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\n(a) The gain from meta-training on ImageNet.\n15\n10\n5\n0\n5\n10\n15\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\nModel\nMatchingNet\nProtoNet\nfo-Proto-MAML\n15\n10\n5\n0\n5\n10\n15 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\n(b) The gain from meta-training on All datasets.\nFigure 10: The performance difference of meta-learning over the corresponding inference-only\nbaseline of each meta-learner. Positive values indicate that meta-learning was beneﬁcial.\n1\n2\n3\n4\n5\n6\nHeight of Lowest Common Ancestor\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAccuracy\nModel\nk-NN baseline\nFinetune baseline\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\n(a)\nFine-grainedness Analysis (on ImageNet’s test\ngraph)\n0\n2\n4\n6\n8\n10\n12\nHeight of Lowest Common Ancestor\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\nAccuracy\nModel\nk-NN baseline\nFinetune baseline\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\n(b) Fine-grainedness Analysis (on ImageNet’s train\ngraph graph)\nFigure 11: Analysis of performance as a function of the degree of ﬁne-grainedness. Larger heights\ncorrespond to coarser-grained tasks. The bands display 95% conﬁdence intervals.\n20\n",
    "Published as a conference paper at ICLR 2020\nTable 3: Improvement of fo-MAML when using a larger inner learning rate α.\n(a) Models trained on ILSVRC-2012 only.\nTest Source\nMethod: Accuracy (%) ± conﬁdence (%)\nfo-MAML α = 0.01 (old)\nfo-MAML α ≈0.1\nILSVRC\n36.09±1.01\n45.51±1.11\nOmniglot\n38.67±1.39\n55.55±1.54\nAircraft\n34.50±0.90\n56.24±1.11\nBirds\n49.10±1.18\n63.61±1.06\nTextures\n56.50±0.80\n68.04±0.81\nQuick Draw\n27.24±1.24\n43.96±1.29\nFungi\n23.50±1.00\n32.10±1.10\nVGG Flower\n66.42±0.96\n81.74±0.83\nTrafﬁc Signs\n33.23±1.34\n50.93±1.51\nMSCOCO\n27.52±1.11\n35.30±1.23\n(b) Models trained on all datasets.\nTest Source\nMethod: Accuracy (%) ± conﬁdence (%)\nfo-MAML α = 0.01 (old)\nfo-MAML α ≈0.1\nILSVRC\n32.36±1.02\n37.83±1.01\nOmniglot\n71.91±1.20\n83.92±0.95\nAircraft\n52.76±0.90\n76.41±0.69\nBirds\n47.24±1.14\n62.43±1.08\nTextures\n56.66±0.74\n64.16±0.83\nQuick Draw\n50.50±1.19\n59.73±1.10\nFungi\n21.02±0.99\n33.54±1.11\nVGG Flower\n70.93±0.99\n79.94±0.84\nTrafﬁc Signs\n34.18±1.26\n42.91±1.31\nMSCOCO\n24.05±1.10\n29.37±1.08\nthough, do seem to indicate that our hypothesis holds to some extent. We conjecture that this may be\ndue to the richer structure of the training DAG, but we encourage further investigation.\n.11\nTHE IMPORTANCE OF MAML’S INNER-LOOP LEARNING RATE HYPERPARAMETER.\nThe camera-ready version includes updated results for MAML and Proto-MAML following an\nexternal suggestion to experiment with larger values for the inner-loop learning rate α of MAML.\nWe found that re-doing our hyperparameter search with a revised range that includes larger α values\nsigniﬁcantly improved fo-MAML’s performance on META-DATASET. For consistency, we applied\nthe same change to fo-Proto-MAML and re-ran those experiments too.\nWe found that the value of this α that performs best for fo-MAML both for training on ImageNet\nonly and training on all datasets is approximately 0.1, which is an order of magnitude larger than our\nprevious best value. Interestingly, fo-Proto-MAML does not choose such a large α value, with best\nα being 0.0054 when training on ImageNet only and 0.02 when training on all datasets. Plausibly\nthis difference can be attributed to the better initialization of Proto-MAML which requires a less\naggressive optimization for the adaptation to each new task. This hypothesis is also supported by the\nfact that fo-Proto-MAML chooses to take fewer adaptation steps than fo-MAML does. The complete\nset of best discovered hyperparameters is available in our public code.\nTo emphasize the importance of properly tuning this hyperparameter, Table 3 displays the previous\nbest and the new best fo-MAML results side-by-side, showcasing the large performance gap when\nusing the appropriate value for α.\n21\n",
    "Published as a conference paper at ICLR 2020\n.12\nTHE CHOICE OF A META-VALIDATION PROCEDURE FOR META-DATASET\nThe design choice we made, as discussed in the main paper, is to use (the meta-validation set of)\nImageNet only for model selection in all of our experiments. In the absence of previous results on the\ntopic, this is a reasonable strategy since ImageNet has been known to consitute a useful proxy for\nperformance on other datasets. However, it is likely that this is not the optimal choice: there might be\ncertain hyperparameters for a given model that work best for held-out ImageNet episodes, but not\nfor held-out episodes of other datasets. An alternative meta-validation scheme would be to use the\naverage (across datasets) validation accuracy as an indicator for early stopping and model selection.\nWe did not choose this method due to concerns about the reliability of this average performance.\nNotably, taking a simple average would over-emphasize larger datasets, or might over-emphasize\ndatasets with natural images (as opposed to Omniglot and Quickdraw). Nevertheless, whether this\nstrategy is beneﬁcial is an interesting empirical question.\n.13\nADDITIONAL PER-DATASET ANALYSIS OF SHOTS AND WAYS\nIn our previous analysis of performance across different shots and ways (Figures 2b, 2a, and 5), the\nperformance is averaged over all evaluation datasets. In this section we further break down those\nplots by presenting the results separately for each dataset. Figures 12 and 13 show the analysis of\nperformance as a function of ways and shots (respectively) for each evaluation dataset, for models\nthat were trained on ImageNet only. For completeness, Figures 14 and 15 show the same for the\nmodels trained on (the training splits of) all datasets.\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(a) ILSVRC\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(b) Omniglot\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(c) Aircraft\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(d) Birds\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(e) Textures\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(f) Quickdraw\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(g) Fungi\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(h) VGG Flower\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(i) Trafﬁc Signs\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(j) MSCOCO\nk-NN baseline\nFinetune baseline\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\nFigure 12: The performance across different ways, with 95% conﬁdence intervals, shown separately\nfor each evaluation dataset. All models had been trained on ImageNet-only.\n22\n",
    "Published as a conference paper at ICLR 2020\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(a) ILSVRC\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(b) Omniglot\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(c) Aircraft\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(d) Birds\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(e) Textures\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(f) Quickdraw\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(g) Fungi\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(h) VGG Flower\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(i) Trafﬁc Signs\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(j) MSCOCO\nk-NN baseline\nFinetune baseline\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\nFigure 13: The performance across different shots, with 95% conﬁdence intervals, shown separately\nfor each evaluation dataset. All models had been trained on ImageNet-only.\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(a) ILSVRC\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(b) Omniglot\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(c) Aircraft\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(d) Birds\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(e) Textures\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(f) Quickdraw\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(g) Fungi\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(h) VGG Flower\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(i) Trafﬁc Signs\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(j) MSCOCO\nk-NN baseline\nFinetune baseline\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\nFigure 14: The performance across different ways, with 95% conﬁdence intervals, shown separately\nfor each evaluation dataset. All models had been trained on (the training splits of) all datasets.\n23\n",
    "Published as a conference paper at ICLR 2020\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(a) ILSVRC\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(b) Omniglot\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(c) Aircraft\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(d) Birds\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(e) Textures\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(f) Quickdraw\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(g) Fungi\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(h) VGG Flower\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(i) Trafﬁc Signs\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(j) MSCOCO\nk-NN baseline\nFinetune baseline\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\nFigure 15: The performance across different shots, with 95% conﬁdence intervals, shown separately\nfor each evaluation dataset. All models had been trained on (the training splits of) all datasets.\n24\n"
  ],
  "full_text": "Published as a conference paper at ICLR 2020\nMETA-DATASET:\nA DATASET\nOF DATASETS\nFOR\nLEARNING TO LEARN FROM FEW EXAMPLES\nEleni Triantaﬁllou∗†, Tyler Zhu†, Vincent Dumoulin†, Pascal Lamblin†, Utku Evci†,\nKelvin Xu‡†, Ross Goroshin†, Carles Gelada†, Kevin Swersky†,\nPierre-Antoine Manzagol† & Hugo Larochelle†\n∗University of Toronto and Vector Institute, †Google AI, ‡University of California, Berkeley\nCorrespondence to: eleni@cs.toronto.edu\nABSTRACT\nFew-shot classiﬁcation refers to learning a classiﬁer for new classes given only a\nfew examples. While a plethora of models have emerged to tackle it, we ﬁnd the\nprocedure and datasets that are used to assess their progress lacking. To address\nthis limitation, we propose META-DATASET: a new benchmark for training and\nevaluating models that is large-scale, consists of diverse datasets, and presents\nmore realistic tasks. We experiment with popular baselines and meta-learners\non META-DATASET, along with a competitive method that we propose. We\nanalyze performance as a function of various characteristics of test tasks and\nexamine the models’ ability to leverage diverse training sources for improving their\ngeneralization. We also propose a new set of baselines for quantifying the beneﬁt of\nmeta-learning in META-DATASET. Our extensive experimentation has uncovered\nimportant research challenges and we hope to inspire work in these directions.\n1\nINTRODUCTION\nFew-shot learning refers to learning new concepts from few examples, an ability that humans naturally\npossess, but machines still lack. Improving on this aspect would lead to more efﬁcient algorithms\nthat can ﬂexibly expand their knowledge without requiring large labeled datasets. We focus on\nfew-shot classiﬁcation: classifying unseen examples into one of N new ‘test’ classes, given only a\nfew reference examples of each. Recent progress in this direction has been made by considering a\nmeta-problem: though we are not interested in learning about any training class in particular, we can\nexploit the training classes for the purpose of learning to learn new classes from few examples, thus\nacquiring a learning procedure that can be directly applied to new few-shot learning problems too.\nThis intuition has inspired numerous models of increasing complexity (see Related Work for some\nexamples). However, we believe that the commonly-used setup for measuring success in this direction\nis lacking. Speciﬁcally, two datasets have emerged as de facto benchmarks for few-shot learning:\nOmniglot (Lake et al., 2015), and mini-ImageNet (Vinyals et al., 2016), and we believe that both\nof them are approaching their limit in terms of allowing one to discriminate between the merits of\ndifferent approaches. Omniglot is a dataset of 1623 handwritten characters from 50 different alphabets\nand contains 20 examples per class (character). Most recent methods obtain very high accuracy\non Omniglot, rendering the comparisons between them mostly uninformative. mini-ImageNet is\nformed out of 100 ImageNet (Russakovsky et al., 2015) classes (64/16/20 for train/validation/test)\nand contains 600 examples per class. Albeit harder than Omniglot, it has the same property that\nmost recent methods trained on it present similar accuracy when controlling for model capacity. We\nadvocate that a more challenging and realistic benchmark is required for further progress in this area.\nMore speciﬁcally, current benchmarks: 1) Consider homogeneous learning tasks. In contrast, real-life\nlearning experiences are heterogeneous: they vary in terms of the number of classes and examples\nper class, and are unbalanced. 2) Measure only within-dataset generalization. However, we are\neventually after models that can generalize to entirely new distributions (e.g., datasets). 3) Ignore the\nrelationships between classes when forming episodes. Speciﬁcally, the coarse-grained classiﬁcation\nof dogs and chairs may present different difﬁculties than the ﬁne-grained classiﬁcation of dog breeds,\nand current benchmarks do not establish a distinction between the two.\n1\narXiv:1903.03096v4  [cs.LG]  8 Apr 2020\n\n\nPublished as a conference paper at ICLR 2020\nMETA-DATASET aims to improve upon previous benchmarks in the above directions: it is signiﬁcantly\nlarger-scale and is comprised of multiple datasets of diverse data distributions; its task creation is\ninformed by class structure for ImageNet and Omniglot; it introduces realistic class imbalance; and it\nvaries the number of classes in each task and the size of the training set, thus testing the robustness of\nmodels across the spectrum from very-low-shot learning onwards.\nThe main contributions of this work are: 1) A more realistic, large-scale and diverse environment for\ntraining and testing few-shot learners. 2) Experimental evaluation of popular models, and a new set of\nbaselines combining inference algorithms of meta-learners with non-episodic training. 3) Analyses of\nwhether different models beneﬁt from more data, heterogeneous training sources, pre-trained weights,\nand meta-training. 4) A novel meta-learner that performs strongly on META-DATASET.\n2\nFEW-SHOT CLASSIFICATION: TASK FORMULATION AND APPROACHES\nTask Formulation\nThe end-goal of few-shot classiﬁcation is to produce a model which, given a\nnew learning episode with N classes and a few labeled examples (kc per class, c ∈1, . . . , N), is\nable to generalize to unseen examples for that episode. In other words, the model learns from a\ntraining (support) set S = {(x1, y1), (x2, y2), . . . , (xK, yK)} (with K = P\nc kc) and is evaluated on\na held-out test (query) set Q = {(x∗\n1, y∗\n1), (x∗\n2, y∗\n2), . . . , (x∗\nT , y∗\nT )}. Each example (x, y) is formed of\nan input vector x ∈RD and a class label y ∈{1, . . . , N}. Episodes with balanced training sets (i.e.,\nkc = k, ∀c) are usually described as ‘N-way, k-shot’ episodes. Evaluation episodes are constructed\nby sampling their N classes from a larger set Ctest of classes and sampling the desired number of\nexamples per class.\nA disjoint set Ctrain of classes is available to train the model; note that this notion of training is\ndistinct from the training that occurs within a few-shot learning episode. Few-shot learning does not\nprescribe a speciﬁc procedure for exploiting Ctrain, but a common approach matches the conditions\nin which the model is trained and evaluated (Vinyals et al., 2016). In other words, training often (but\nnot always) proceeds in an episodic fashion. Some authors use training and testing to refer to what\nhappens within any given episode, and meta-training and meta-testing to refer to using Ctrain to turn\nthe model into a learner capable of fast adaptation and Ctest for evaluating its success to learn using\nfew shots, respectively. This nomenclature highlights the meta-learning perspective alluded to earlier,\nbut to avoid confusion we will adopt another common nomenclature and refer to the training and test\nsets of an episode as the support and query sets and to the process of learning from Ctrain simply as\ntraining. We use the term ‘meta-learner’ to describe a model that is trained episodically, i.e., learns to\nlearn across multiple tasks that are sampled from the training set Ctrain.\nNon-episodic Approaches to Few-shot Classiﬁcation\nA natural non-episodic approach simply\ntrains a classiﬁer over all of the training classes Ctrain at once, which can be parameterized by a\nneural network with a linear layer on top with one output unit per class. After training, this neural\nnetwork is used as an embedding function g that maps images into a meaningful representation space.\nThe hope of using this model for few-shot learning is that this representation space is useful even for\nexamples of classes that were not included in training. It would then remain to deﬁne an algorithm\nfor performing few-shot classiﬁcation on top of these representations of the images of a task. We\nconsider two choices for this algorithm, yielding the ‘k-NN’ and ‘Finetune’ variants of this baseline.\nGiven a test episode, the ‘k-NN’ baseline classiﬁes each query example as the class that its ‘closest’\nsupport example belongs to. Closeness is measured by either Euclidean or cosine distance in the\nlearned embedding space; a choice that we treat as a hyperparameter. On the other hand, the ‘Finetune’\nbaseline uses the support set of the given test episode to train a new ‘output layer’ on top of the\nembeddings g, and optionally ﬁnetune those embedding too (another hyperparameter), for the purpose\nof classifying between the N new classes of the associated task.\nA variant of the ‘Finetune’ baseline has recently become popular: Baseline++ (Chen et al., 2019),\noriginally inspired by Gidaris & Komodakis (2018); Qi et al. (2018). It uses a ‘cosine classiﬁer’ as\nthe ﬁnal layer (ℓ2-normalizing embeddings and weights before taking the dot product), both during\nthe non-episodic training phase, and for evaluation on test episodes. We incorporate this idea in our\ncodebase by adding a hyperparameter that optionally enables using a cosine classiﬁer for the ‘k-NN’\n(training only) and ‘Finetune’ (both phases) baselines.\n2\n\n\nPublished as a conference paper at ICLR 2020\nMeta-Learners for Few-shot Classiﬁcation\nIn the episodic setting, models are trained end-to-end\nfor the purpose of learning to build classiﬁers from a few examples. We choose to experiment\nwith Matching Networks (Vinyals et al., 2016), Relation Networks (Sung et al., 2018), Prototypical\nNetworks (Snell et al., 2017) and Model Agnostic Meta-Learning (MAML, Finn et al., 2017) since\nthey cover a diverse set of approaches to few-shot learning. We also introduce a novel meta-learner\nwhich is inspired by the last two models.\nIn each training episode, episodic models compute for each query example x∗∈Q, the distribution\nfor its label p(y∗|x∗, S) conditioned on the support set S and allow to train this differentiably-\nparameterized conditional distribution end-to-end via gradient descent. The different models are\ndistinguished by the manner in which this conditioning on the support set is realized. In all cases, the\nperformance on the query set drives the update of the meta-learner’s weights, which include (and\nsometimes consist only of) the embedding weights. We brieﬂy describe each method below.\nPrototypical Networks\nPrototypical Networks construct a prototype for each class and then classify\neach query example as the class whose prototype is ‘nearest’ to it under Euclidean distance. More\nconcretely, the probability that a query example x∗belongs to class k is deﬁned as:\np(y∗= k|x∗, S) =\nexp(−||g(x∗) −ck||2\n2)\nP\nk′∈{1,...,N} exp(−||g(x∗) −ck′||2\n2)\nwhere ck is the ‘prototype’ for class k: the average of the embeddings of class k’s support examples.\nMatching Networks\nMatching Networks (in their simplest form) label each query example as a\n(cosine) distance-weighted linear combination of the support labels:\np(y∗= k|x∗, S) =\n|S|\nX\ni=1\nα(x∗, xi)1yi=k,\nwhere 1A is the indicator function and α(x∗, xi) is the cosine similarity between g(x∗) and g(xi),\nsoftmax-normalized over all support examples xi, where 1 ≤i ≤|S|.\nRelation Networks\nRelation Networks are comprised of an embedding function g as usual, and\na ‘relation module’ parameterized by some additional neural network layers. They ﬁrst embed\neach support and query using g and create a prototype pc for each class c by averaging its support\nembeddings. Each prototype pc is concatenated with each embedded query and fed through the\nrelation module which outputs a number in [0, 1] representing the predicted probability that that query\nbelongs to class c. The query loss is then deﬁned as the mean square error of that prediction compared\nto the (binary) ground truth. Both g and the relation module are trained to minimize this loss.\nMAML\nMAML uses a linear layer parametrized by W and b on top of the embedding function\ng(·; θ) and classiﬁes a query example as\np(y∗|x∗, S) = softmax(b′ + W′g(x∗; θ′)),\nwhere the output layer parameters W′ and b′ and the embedding function parameters θ′ are obtained\nby performing a small number of within-episode training steps on the support set S, starting from\ninitial parameter values (b, W, θ). The model is trained by backpropagating the query set loss\nthrough the within-episode gradient descent procedure and into (b, W, θ). This normally requires\ncomputing second-order gradients, which can be expensive to obtain (both in terms of time and\nmemory). For this reason, an approximation is often used whereby gradients of the within-episode\ndescent steps are ignored. This variant is referred to as ﬁrst-order MAML (fo-MAML) and was used\nin our experiments. We did attempt to use the full-order version, but found it to be impractically\nexpensive (e.g., it caused frequent out-of-memory problems).\nMoreover, since in our setting the number of ways varies between episodes, b, W are set to zero and\nare not trained (i.e., b′, W′ are the result of within-episode gradient descent initialized at 0), leaving\nonly θ to be trained. In other words, MAML focuses on learning the within-episode initialization θ of\nthe embedding network so that it can be rapidly adapted for a new task.\n3\n\n\nPublished as a conference paper at ICLR 2020\nIntroducing Proto-MAML\nWe introduce a novel meta-learner that combines the complementary\nstrengths of Prototypical Networks and MAML: the former’s simple inductive bias that is evidently\neffective for very-few-shot learning, and the latter’s ﬂexible adaptation mechanism.\nAs explained by Snell et al. (2017), Prototypical Networks can be re-interpreted as a linear classiﬁer\napplied to a learned representation g(x). The use of a squared Euclidean distance means that output\nlogits are expressed as\n−||g(x∗) −ck||2 = −g(x∗)T g(x∗) + 2cT\nk g(x∗) −cT\nk ck = 2cT\nk g(x∗) −||ck||2 + constant\nwhere constant is a class-independent scalar which can be ignored, as it leaves output probabilities\nunchanged. The k-th unit of the equivalent linear layer therefore has weights Wk,· = 2ck and biases\nbk = −||ck||2, which are both differentiable with respect to θ as they are a function of g(·; θ).\nWe refer to (fo-)Proto-MAML as the (fo-)MAML model where the task-speciﬁc linear layer of each\nepisode is initialized from the Prototypical Network-equivalent weights and bias deﬁned above and\nsubsequently optimized as usual on the given support set. When computing the update for θ, we\nallow gradients to ﬂow through the Prototypical Network-equivalent linear layer initialization. We\nshow that this simple modiﬁcation signiﬁcantly helps the optimization of this model and outperforms\nvanilla fo-MAML by a large margin on META-DATASET.\n3\nMETA-DATASET: A NEW FEW-SHOT CLASSIFICATION BENCHMARK\nMETA-DATASET aims to offer an environment for measuring progress in realistic few-shot classiﬁca-\ntion tasks. Our approach is twofold: 1) changing the data and 2) changing the formulation of the task\n(i.e., how episodes are generated). The following sections describe these modiﬁcations in detail. The\ncode is open source and publicly available1.\n3.1\nMETA-DATASET’S DATA\nMETA-DATASET’s data is much larger in size than any previous benchmark, and is comprised of\nmultiple existing datasets. This invites research into how diverse sources of data can be exploited\nby a meta-learner, and allows us to evaluate a more challenging generalization problem, to new\ndatasets altogether. Speciﬁcally, META-DATASET leverages data from the following 10 datasets:\nILSVRC-2012 (ImageNet, Russakovsky et al., 2015), Omniglot (Lake et al., 2015), Aircraft (Maji\net al., 2013), CUB-200-2011 (Birds, Wah et al., 2011), Describable Textures (Cimpoi et al., 2014),\nQuick Draw (Jongejan et al., 2016), Fungi (Schroeder & Cui, 2018), VGG Flower (Nilsback &\nZisserman, 2008), Trafﬁc Signs (Houben et al., 2013) and MSCOCO (Lin et al., 2014). These\ndatasets were chosen because they are free and easy to obtain, span a variety of visual concepts\n(natural and human-made) and vary in how ﬁne-grained the class deﬁnition is. More information\nabout each of these datasets is provided in the Appendix.\nTo ensure that episodes correspond to realistic classiﬁcation problems, each episode generated in\nMETA-DATASET uses classes from a single dataset. Moreover, two of these datasets, Trafﬁc Signs\nand MSCOCO, are fully reserved for evaluation, meaning that no classes from them participate in\nthe training set. The remaining ones contribute some classes to each of the training, validation and\ntest splits of classes, roughly with 70% / 15% / 15% proportions. Two of these datasets, ImageNet\nand Omniglot, possess a class hierarchy that we exploit in META-DATASET. For each dataset, the\ncomposition of splits is available online2.\nImageNet\nImageNet is comprised of 82,115 ‘synsets’, i.e., concepts of the WordNet ontology, and\nit provides ‘is-a’ relationships for its synsets, thus deﬁning a DAG over them. META-DATASET uses\nthe 1K synsets that were chosen for the ILSVRC 2012 classiﬁcation challenge and deﬁnes a new class\nsplit for it and a novel procedure for sampling classes from it for episode creation, both informed by\nits class hierarchy.\nSpeciﬁcally, we construct a sub-graph of the overall DAG whose leaves are the 1K classes of ILSVRC-\n2012. We then ‘cut’ this sub-graph into three pieces, for the training, validation, and test splits,\n1github.com/google-research/meta-dataset\n2github.com/google-research/meta-dataset/tree/master/meta_dataset/dataset_conversion/splits\n4\n\n\nPublished as a conference paper at ICLR 2020\nsuch that there is no overlap between the leaves of any of these pieces. For this, we selected the\nsynsets ‘carnivore’ and ‘device’ as the roots of the validation and test sub-graphs, respectively. The\nleaves that are reachable from ‘carnivore’ and ‘device’ form the sets of the validation and test classes,\nrespectively. All of the remaining leaves constitute the training classes. This method of splitting\nensures that the training classes are semantically different from the test classes. We end up with 712\ntraining, 158 validation and 130 test classes, roughly adhering to the standard 70 / 15 / 15 (%) splits.\nOmniglot\nThis dataset is one of the established benchmarks for few-shot classiﬁcation as men-\ntioned earlier. However, contrary to the common setup that ﬂattens and ignores its two-level hierarchy\nof alphabets and characters, we allow it to inﬂuence the episode class selection in META-DATASET,\nyielding ﬁner-grained tasks. We also use the original splits proposed in Lake et al. (2015): (all char-\nacters of) the ‘background’ and ‘evaluation’ alphabets are used for training and testing, respectively.\nHowever, we reserve the 5 smallest alphabets from the ‘background’ set for validation.\n3.2\nEPISODE SAMPLING\nIn this section we outline META-DATASET’s algorithm for sampling episodes, featuring hierarchically-\naware procedures for sampling classes of ImageNet and Omniglot, and an algorithm that yields\nrealistically imbalanced episodes of variable shots and ways. The steps for sampling an episode for\na given split are: Step 0) uniformly sample a dataset D, Step 1) sample a set of classes C from the\nclasses of D assigned to the requested split, and Step 2) sample support and query examples from C.\nStep 1: Sampling the episode’s class set\nThis procedure differs depending on which dataset is\nchosen. For datasets without a known class organization, we sample the ‘way’ uniformly from the\nrange [5, MAX-CLASSES], where MAX-CLASSES is either 50 or as many as there are available.\nThen we sample ‘way’ many classes uniformly at random from the requested class split of the given\ndataset. ImageNet and Omniglot use class-structure-aware procedures outlined below.\nImageNet class sampling\nWe adopt a hierarchy-aware sampling procedure: First, we sample an\ninternal (non-leaf) node uniformly from the DAG of the given split. The chosen set of classes is then\nthe set of leaves spanned by that node (or a random subset of it, if more than 50). We prevent nodes\nthat are too close to the root to be selected as the internal node, as explained in more detail in the\nAppendix. This procedure enables the creation of tasks of varying degrees of ﬁne-grainedness: the\nlarger the height of the internal node, the more coarse-grained the resulting episode.\nOmniglot class sampling\nWe sample classes from Omniglot by ﬁrst sampling an alphabet uni-\nformly at random from the chosen split of alphabets (train, validation or test). Then, the ‘way’ of the\nepisode is sampled uniformly at random using the same restrictions as for the rest of the datasets, but\ntaking care not to sample a larger number than the number of characters that belong to the chosen\nalphabet. Finally, the prescribed number of characters of that alphabet are randomly sampled. This\nensures that each episode presents a within-alphabet ﬁne-grained classiﬁcation.\nStep 2: Sampling the episode’s examples\nHaving already selected a set of classes, the choice\nof the examples from them that will populate an episode can be broken down into three steps. We\nprovide a high-level description here and elaborate in the Appendix with the accompanying formulas.\nStep 2a: Compute the query set size The query set is class-balanced, reﬂecting the fact that we\ncare equally to perform well on all classes of an episode. The number of query images per class is\nset to a number such that all chosen classes have enough images to contribute that number and still\nremain with roughly half on their images to possibly add to the support set (in a later step). This\nnumber is capped to 10 images per class.\nStep 2b: Compute the support set size We allow each chosen class to contribute to the support set\nat most 100 of its remaining examples (i.e., excluding the ones added to the query set). We multiply\nthis remaining number by a scalar sampled uniformly from the interval (0, 1] to enable the potential\ngeneration of ‘few-shot’ episodes even when multiple images are available, as we are also interested\nin studying that end of the spectrum. We do enforce, however, that each chosen class has a budget for\nat least one image in the support set, and we cap the total support set size to 500 examples.\n5\n\n\nPublished as a conference paper at ICLR 2020\nStep 2c: Compute the shot of each class We now discuss how to distribute the total support set\nsize chosen above across the participating classes. The un-normalized proportion of the support set\nthat will be occupied by a given chosen class is a noisy version of the total number of images of\nthat class in the dataset. This design choice is made in the hopes of obtaining realistic class ratios,\nunder the hypothesis that the dataset class statistics are a reasonable approximation of the real-world\nstatistics of appearances of the corresponding classes. We ensure that each class has at least one\nimage in the support set and distribute the rest according to the above rule.\nAfter these steps, we complete the episode creation process by choosing the prescribed number of\nexamples of each chosen class uniformly at random to populate the support and query sets.\n4\nRELATED WORK\nIn this work we evaluate four meta-learners on META-DATASET that we believe capture a good\ndiversity of well-established models. Evaluating other few-shot classiﬁers on META-DATASET is\nbeyond the scope of this paper, but we discuss some additional related models below.\nSimilarly to MAML, some train a meta-learner for quick adaptation to new tasks (Ravi & Larochelle,\n2017; Munkhdalai & Yu, 2017; Rusu et al., 2019; Yoon et al., 2018). Others relate to Prototypical\nNetworks by learning a representation on which differentiable training can be performed on some form\nof classiﬁer (Bertinetto et al., 2019; Gidaris & Komodakis, 2018; Oreshkin et al., 2018). Others relate\nto Matching Networks in that they perform comparisons between pairs of support and query examples,\nusing either a graph neural network (Satorras & Estrach, 2018) or an attention mechanism (Mishra\net al., 2018). Finally, some make use of memory-augmented recurrent networks (Santoro et al.,\n2016), some learn to perform data augmentation (Hariharan & Girshick, 2017; Wang et al., 2018) in\na low-shot learning setting, and some learn to predict the parameters of a large-shot classiﬁer from\nthe parameters learned in a few-shot setting (Wang & Hebert, 2016; Wang et al., 2017). Of relevance\nto Proto-MAML is MAML++ (Antoniou et al., 2019), which consists of a collection of adjustments\nto MAML, such as multiple meta-trained inner loop learning rates and derivative-order annealing.\nProto-MAML instead modiﬁes the output weight initialization scheme and could be combined with\nthose adjustments.\nFinally, META-DATASET relates to other recent image classiﬁcation benchmarks. The CVPR\n2017 Visual Domain Decathlon Challenge trains a model on 10 different datasets, many of which\nare included in our benchmark, and measures its ability to generalize to held-out examples for\nthose same datasets but does not measure generalization to new classes (or datasets). Hariharan\n& Girshick (2017) propose a benchmark where a model is given abundant data from certain base\nImageNet classes and is tested on few-shot learning novel ImageNet classes in a way that doesn’t\ncompromise its knowledge of the base classes. Wang et al. (2018) build upon that benchmark\nand propose a new evaluation protocol for it. Chen et al. (2019) investigate ﬁne-grained few-shot\nclassiﬁcation using the CUB dataset (Wah et al., 2011, also featured in our benchmark) and cross-\ndomain transfer between mini-ImageNet and CUB. Larger-scale few-shot classiﬁcation benchmarks\nwere also proposed using CIFAR-100 (Krizhevsky et al., 2009; Bertinetto et al., 2019; Oreshkin et al.,\n2018), tiered-ImageNet (Ren et al., 2018), and ImageNet-21k (Dhillon et al., 2019). Compared to\nthese, META-DATASET contains the largest set of diverse datasets in the context of few-shot learning\nand is additionally accompanied by an algorithm for creating learning scenarios from that data that\nwe advocate are more realistic than the previous ones.\n5\nEXPERIMENTS\nTraining procedure\nMETA-DATASET does not prescribe a procedure for learning from the training\ndata. In these experiments, keeping with the spirit of matching training and testing conditions, we\ntrained the meta-learners via training episodes sampled using the same algorithm as we used for\nMETA-DATASET’s evaluation episodes, described above. The choice of the dataset from which to\nsample the next episode was random uniform. The non-episodic baselines are trained to solve the\nlarge classiﬁcation problem that results from ‘concatenating’ the training classes of all datasets.\nValidation\nAnother design choice was to perform validation on (the validation split of) ImageNet\nonly, ignoring the validation sets of the other datasets. The rationale behind this choice is that the\n6\n\n\nPublished as a conference paper at ICLR 2020\nTable 1: Few-shot classiﬁcation results on META-DATASET using models trained on ILSVRC-2012\nonly (top) and trained on all datasets (bottom).\nTest Source\nk-NN\nFinetune\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\nILSVRC\n41.03\n45.78\n45.00\n50.50\n45.51\n34.69\n49.53\nOmniglot\n37.07\n60.85\n52.27\n59.98\n55.55\n45.35\n63.37\nAircraft\n46.81\n68.69\n48.97\n53.10\n56.24\n40.73\n55.95\nBirds\n50.13\n57.31\n62.21\n68.79\n63.61\n49.51\n68.66\nTextures\n66.36\n69.05\n64.15\n66.56\n68.04\n52.97\n66.49\nQuick Draw\n32.06\n42.60\n42.87\n48.96\n43.96\n43.30\n51.52\nFungi\n36.16\n38.20\n33.97\n39.71\n32.10\n30.55\n39.96\nVGG Flower\n83.10\n85.51\n80.13\n85.27\n81.74\n68.76\n87.15\nTrafﬁc Signs\n44.59\n66.79\n47.80\n47.12\n50.93\n33.67\n48.83\nMSCOCO\n30.38\n34.86\n34.99\n41.00\n35.30\n29.15\n43.74\nAvg. rank\n5.7\n2.9\n4.65\n2.65\n3.7\n6.55\n1.85\nTest Source\nk-NN\nFinetune\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\nILSVRC\n38.55\n43.08\n36.08\n44.50\n37.83\n30.89\n46.52\nOmniglot\n74.60\n71.11\n78.25\n79.56\n83.92\n86.57\n82.69\nAircraft\n64.98\n72.03\n69.17\n71.14\n76.41\n69.71\n75.23\nBirds\n66.35\n59.82\n56.40\n67.01\n62.43\n54.14\n69.88\nTextures\n63.58\n69.14\n61.80\n65.18\n64.16\n56.56\n68.25\nQuick Draw\n44.88\n47.05\n60.81\n64.88\n59.73\n61.75\n66.84\nFungi\n37.12\n38.16\n33.70\n40.26\n33.54\n32.56\n41.99\nVGG Flower\n83.47\n85.28\n81.90\n86.85\n79.94\n76.08\n88.72\nTrafﬁc Signs\n40.11\n66.74\n55.57\n46.48\n42.91\n37.48\n52.42\nMSCOCO\n29.55\n35.17\n28.79\n39.87\n29.37\n27.41\n41.74\nAvg. rank\n5.05\n3.6\n4.95\n2.85\n4.25\n5.8\n1.5\nperformance on ImageNet has been known to be a good proxy for the performance on different\ndatasets. We used this validation performance to select our hyperparameters, including backbone\narchitectures, image resolutions and model-speciﬁc ones. We describe these further in the Appendix.\nPre-training\nWe gave each meta-learner the opportunity to initialize its embedding function from\nthe embedding weights to which the k-NN Baseline model trained on ImageNet converged to. We\ntreated the choice of starting from scratch or starting from this initialization as a hyperparameter.\nFor a fair comparison with the baselines, we allowed the non-episodic models to start from this\ninitialization too. This is especially important for the baselines in the case of training on all datasets\nsince it offers the opportunity to start from ImageNet-pretrained weights.\nMain results\nTable 1 displays the accuracy of each model on the test set of each dataset, after they\nwere trained on ImageNet-only or all datasets. Trafﬁc Signs and MSCOCO are not used for training\nin either case, as they are reserved for evaluation. We propose to use the average (over the datasets)\nrank of each method as our metric for comparison, where smaller is better. A method receives rank 1\nif it has the highest accuracy, rank 2 if it has the second highest, and so on. If two models share the\nbest accuracy, they both get rank 1.5, and so on. We ﬁnd that fo-Proto-MAML is the top-performer\naccording to this metric, Prototypical Networks also perform strongly, and the Finetune Baseline\nnotably presents a worthy opponent3. We include more detailed versions of these tables displaying\nconﬁdence intervals and per-dataset ranks in the Appendix.\nEffect of training on all datasets instead of ImageNet only\nIt’s interesting to examine whether\ntraining on (the training splits of) all datasets leads to improved generalizaton compared to training on\n(the training split of) ImageNet only. Speciﬁcally, while we might expect that training on more data\nhelps improve generalization, it is an empirical question whether that still holds for heterogeneous\ndata. We can examine this by comparing the performance of each model between the top and bottom\nsets of results of Table 1, corresponding to the two training sources (ImageNet only and all datasets,\nrespectively). For convenience, Figure 1 visualizes this difference in a barplot. Notably, for Omniglot,\nQuick Draw and Aircraft we observe a substantial increase across the board from training on all\n3We improved MAML’s performance signiﬁcantly in the time between the acceptance decision and publi-\ncation, thanks to a suggestion to use a more aggressive inner-loop learning rate. In the Appendix, we present\nthe older MAML results side by side with the new ones, and discuss the importance of this hyperparameter for\nMAML.\n7\n\n\nPublished as a conference paper at ICLR 2020\n20\n10\n0\n10\n20\n30\n40\n50\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\nModel\nk-NN baseline\nFinetune baseline\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\n20\n10\n0\n10\n20\n30\n40\n50 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\nFigure 1: The performance difference on test datasets, when training on all datasets instead of\nILSVRC only. A positive value indicates an improvement from all-dataset training.\nsources. This is reasonable for datasets whose images are signiﬁcantly different from ImageNet’s:\nwe indeed expect to gain a large beneﬁt from training on some images from (the training classes of)\nthese datasets. Interestingly though, on the remainder of the test sources, we don’t observe a gain\nfrom all-dataset training. This result invites research into methods for exploiting heterogeneous data\nfor generalization to unseen classes of diverse sources. Our experiments show that learning ‘naively’\nacross the training datasets (e.g., by picking the next dataset to use uniformly at random) does not\nautomatically lead to that desired beneﬁt in most cases.\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nk-NN baseline\nFinetune baseline\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\n(a) Ways analysis\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\nClass Precision\nk-NN baseline\nFinetune baseline\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\n(b) Shots analysis\nFigure 2: The effect of different ways and shots on test performance (w/ 95% conﬁdence intervals)\nwhen training on ImageNet.\nWays and shots analysis\nWe further study the accuracy as a function of ‘ways’ (Figure 2a) and the\nclass precision as a function of ‘shots’ (Figure 2b). As expected, we found that the difﬁculty increases\nas the way increases, and performance degrades. More examples per class, on the other hand, indeed\nmake it easier to correctly classify that class. Interestingly, though, not all models beneﬁt at the\nsame rate from more data: Prototypical Networks and fo-Proto-MAML outshine other models in\nvery-low-shot settings but saturate faster, whereas the Finetune baseline, Matching Networks, and\nfo-MAML improve at a higher rate when the shot increases. We draw the same conclusions when\nperforming this analysis on all datasets, and include those plots in the Appendix. As discussed in the\nAppendix, we recommend including this analysis when reporting results on Meta-Dataset, aside from\nthe main table. The rationale is that we’re not only interested in performing well on average, but also\nin performing well under different speciﬁcations of test tasks.\nEffect of pre-training\nIn Figures 3a and 3b, we quantify how beneﬁcial it is to initialize the\nembedding network of meta-learners using the weights of the k-NN baseline pre-trained on ImageNet,\nas opposed to starting their episodic training from scratch. We ﬁnd this procedure to often be\nbeneﬁcial, both for ImageNet-only training and for training on all datasets. It seems that this\nImageNet-inﬂuenced initialization drives the meta-learner towards a solution which yields increased\nperformance on natural image test datasets, especially ILSVRC, Birds, Fungi, Flowers and MSCOCO.\n8\n\n\nPublished as a conference paper at ICLR 2020\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\nModel\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\nInitialization\nPre-trained\nFrom scratch\n(a) Effect of pre-training (ImageNet)\n0\n20\n40\n60\n80\n100\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\n0\n20\n40\n60\n80\n100 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\n(b) Effect of pre-training (All datasets)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\nModel\nMatchingNet\nProtoNet\nfo-Proto-MAML\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\nMeta-training\nInference-only\n(c) Effect of meta-learning (ImageNet)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\n(d) Effect of meta-learning (All datasets)\nFigure 3: The effects of pre-training and meta-training (w/ 95% conﬁdence intervals). (ImageNet) or\n(All datasets) is the training source.\nPerhaps unsusprisingly, though, it underperforms on signiﬁcantly different datasets such as Omniglot\nand Quick Draw. These ﬁndings show that, aside from the choice of the training data source(s) (e.g.,\nImageNet only or all datasets, as discussed above), the choice of the initialization scheme can also\ninﬂuence to an important degree the ﬁnal solution and consequently the aptness of applying the\nresulting meta-learner to different data sources at test time. Finally, an interesting observation is\nthat MAML seems to beneﬁt the most from the pre-trained initialization, which may speak to the\ndifﬁculty of optimization associated with that model.\nEffect of meta-training\nWe propose to disentangle the inference algorithm of each meta-learner\nfrom the fact that it is meta-learned, to assess the beneﬁt of meta-learning on META-DATASET.\nTo this end, we propose a new set of baselines: ‘Prototypical Networks Inference’, ‘Matching\nNetworks Inference’, and ‘fo-Proto-MAML Inference’, that are trained non-episodically but evaluated\nepisodically (for validation and testing) using the inference algorithm of the respective meta-learner.\nThis is possible for these meta-learners as they don’t have any additional parameters aside from the\nembedding function that explicitly need to be learned episodically (as opposed to the relation module\nof Relation Networks, for example). We compare each Inference-only method to its corresponding\nmeta-learner in Figures 3c and 3d. We ﬁnd that these baselines are strong: when training on ImageNet\nonly, we can usually observe a small beneﬁt from meta-learning the embedding weights but this\nbeneﬁt often disappears when training on all datasets, in which case meta-learning sometimes actually\nhurts. We ﬁnd this result very interesting and we believe it emphasizes the need for research on how\nto meta-learn across multiple diverse sources, an important challenge that META-DATASET puts\nforth.\nFine-grainedness analysis\nWe use ILVRC-2012 to investigate the hypothesis that ﬁner-grained\ntasks are harder than coarse-grained ones. Our ﬁndings suggest that while the test sub-graph is not\nrich enough to exhibit any trend, the performance on the train sub-graph does seem to agree with this\nhypothesis. We include the experimental setup and results for this analysis in the Appendix.\n6\nCONCLUSION\nWe have introduced a new large-scale, diverse, and realistic environment for few-shot classiﬁcation.\nWe believe that our exploration of various models on META-DATASET has uncovered interesting\n9\n\n\nPublished as a conference paper at ICLR 2020\ndirections for future work pertaining to meta-learning across heterogeneous data: it remains unclear\nwhat is the best strategy for creating training episodes, the most appropriate validation creation and\nthe most appropriate initialization. Current models don’t always improve when trained on multiple\nsources and meta-learning is not always beneﬁcial across datasets. Current models are also not robust\nto the amount of data in test episodes, each excelling in a different part of the spectrum. We believe\nthat addressing these shortcomings consitutes an important research goal moving forward.\nAUTHOR CONTRIBUTIONS\nEleni, Hugo, and Kevin came up with the benchmark idea and requirements. Eleni developed the core\nof the project, and worked on the experiment design and management with Tyler and Kevin, as well as\nexperiment analysis. Carles, Ross, Kelvin, Pascal, Vincent, and Tyler helped extend the benchmark by\nadding datasets. Eleni, Vincent, and Utku contributed the Prototypical Networks, Matching Networks,\nand Relation Networks implementations, respectively. Tyler implemented baselines, MAML (with\nKevin) and Proto-MAML models, and updated the backbones to support them. Writing was mostly\nled by Eleni, with contributions by Hugo, Vincent, and Kevin and help from Tyler and Pascal for\nvisualizations. Pascal and Pierre-Antoine worked on code organization, efﬁciency, and open-sourcing,\nPascal and Vincent optimized the efﬁciency of the data input pipeline. Pierre-Antoine supervised the\ncode development process and reviewed most of the changes, Hugo and Kevin supervised the overall\ndirection of the research.\nACKNOWLEDGMENTS\nWe would like to thank Chelsea Finn for fruitful discussions and advice on tuning fo-MAML and\nensuring the correctness of implementation, as well as Zack Nado and Dan Moldovan for the initial\ndataset code that was adapted, and Cristina Vasconcelos for spotting an issue in the ranking of\nmodels. Finally, we’d like to thank John Bronskill for suggesting that we experiment with a larger\ninner-loop learning rate for MAML which indeed signiﬁcantly improved our fo-MAML results on\nMETA-DATASET.\n10\n\n\nPublished as a conference paper at ICLR 2020\nREFERENCES\nAntreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your MAML. In Proceedings\nof the International Conference on Learning Representations, 2019.\nLuca Bertinetto, Joao F. Henriques, Philip Torr, and Andrea Vedaldi. Meta-learning with differentiable\nclosed-form solvers. In Proceedings of the International Conference on Learning Representations,\n2019.\nWei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer\nlook at few-shot classiﬁcation. In Proceedings of the International Conference on Learning\nRepresentations, 2019.\nM. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi. Describing textures in the wild. In\nIEEE Conference on Computer Vision and Pattern Recognition, 2014.\nGuneet S. Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline for\nfew-shot image classiﬁcation. arXiv, abs/1909.02729, 2019.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of\ndeep networks. In Proceedings of the International Conference of Machine Learning, 2017.\nSpyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In IEEE\nConference on Computer Vision and Pattern Recognition, 2018.\nBharath Hariharan and Ross Girshick. Low-shot visual recognition by shrinking and hallucinating\nfeatures. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3018–3027,\n2017.\nSebastian Houben, Johannes Stallkamp, Jan Salmen, Marc Schlipsing, and Christian Igel. Detection of\ntrafﬁc signs in real-world images: The German Trafﬁc Sign Detection Benchmark. In International\nJoint Conference on Neural Networks, 2013.\nJonas Jongejan, Henry Rowley, Takashi Kawashima, Jongmin Kim, and Nick Fox-Gieg. The Quick,\nDraw! – A.I. experiment. quickdraw.withgoogle.com, 2016.\nAlex Krizhevsky et al. Learning multiple layers of features from tiny images. Technical report,\nUniversity of Toronto, 2009.\nBrenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning\nthrough probabilistic program induction. Science, 350(6266):1332–1338, 2015.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollár, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In European\nConference on Computer Vision, pp. 740–755, 2014.\nSubhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained\nvisual classiﬁcation of aircraft. arXiv, abs/1306.5151, 2013.\nNikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-\nlearner. In Proceedings of the International Conference on Learning Representations, 2018.\nTsendsuren Munkhdalai and Hong Yu. Meta networks. In Proceedings of the International Conference\non Machine Learning, pp. 2554–2563, 2017.\nM-E. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over a large number of classes. In\nProceedings of the Indian Conference on Computer Vision, Graphics and Image Processing, 2008.\nBoris N. Oreshkin, Pau Rodriguez, and Alexandre Lacoste. TADAM: Task dependent adaptive\nmetric for improved few-shot learning. In Advances in Neural Information Processing Systems, pp.\n719–729, 2018.\nHang Qi, Matthew Brown, and David G Lowe. Low-shot learning with imprinted weights. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5822–5830,\n2018.\n11\n\n\nPublished as a conference paper at ICLR 2020\nSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In Proceedings of\nthe International Conference on Learning Representations, 2017.\nMengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum,\nHugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classiﬁcation.\nIn Proceedings of the International Conference on Learning Representations, 2018.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, and Li Fei-Fei. Imagenet\nlarge scale visual recognition challenge. International Journal of Computer Vision, 115(3):211–252,\n2015.\nAndrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero,\nand Raia Hadsell. Meta-learning with latent embedding optimization. In Proceedings of the\nInternational Conference on Learning Representations, 2019.\nTim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate\ntraining of deep neural networks. In Advances in Neural Information Processing Systems, pp.\n901–909, 2016.\nAdam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-\nlearning with memory-augmented neural networks. In Proceedings of the International Conference\non Machine Learning, pp. 1842–1850, 2016.\nVictor Garcia Satorras and Joan Bruna Estrach. Few-shot learning with graph neural networks. In\nProceedings of the International Conference on Learning Representations, 2018.\nBrigit Schroeder and Yin Cui.\nFGVCx fungi classiﬁcation challenge 2018.\ngithub.com/\nvisipedia/fgvcx_fungi_comp, 2018.\nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In\nAdvances in Neural Information Processing Systems, pp. 4077–4087, 2017.\nFlood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.\nLearning to compare: Relation network for few-shot learning. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pp. 1199–1208, 2018.\nOriol Vinyals, Charles Blundell, Tim Lillicrap, and Daan Wierstra. Matching networks for one shot\nlearning. In Advances in Neural Information Processing Systems, pp. 3630–3638, 2016.\nC. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011\nDataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.\nYu-Xiong Wang and Martial Hebert. Learning to learn: Model regression networks for easy small\nsample learning. In European Conference on Computer Vision, pp. 616–634. Springer, 2016.\nYu-Xiong Wang, Deva Ramanan, and Martial Hebert. Learning to model the tail. In Advances in\nNeural Information Processing Systems, pp. 7029–7039, 2017.\nYu-Xiong Wang, Ross Girshick, Martial Hebert, and Bharath Hariharan. Low-shot learning from\nimaginary data. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni-\ntion, pp. 7278–7286, 2018.\nJaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn.\nBayesian model-agnostic meta-learning. In Advances in Neural Information Processing Systems,\n2018.\n12\n\n\nPublished as a conference paper at ICLR 2020\nAPPENDIX\n.1\nRECOMMENDATION FOR REPORTING RESULTS ON META-DATASET\nWe recommend that future work on META-DATASET reports two sets of results:\n1. The main tables storing the average (over 600 test episodes) accuracy of each method on\neach dataset, after it has been trained on ImageNet only and on All datasets, where the\nevaluation metric is the average rank. This corresponds to Table 1 in our case (or the more\ncomplete version in Table 2 in the Appendix).\n2. The plots that measure robustness in variations of shots and ways. In our case these are\nFigures 2b and 2a in the main text for ImageNet-only training, and Figures 5b and 5a in the\nAppendix for the case of training on all datasets.\nWe propose to use both of these aspects to evaluate performance on META-DATASET: it is not only\ndesirable to perform well on average, but also to perform well under different speciﬁcations of test\ntasks, as it is not realistic in general to assume that we will know in advance what setup (number\nof ways and shots) will be encountered at test time. Our ﬁnal source code will include scripts for\ngenerating these plots and for automatically computing ranks given a table to help standardize the\nprocedure for reporting results.\n.2\nDETAILS OF META-DATASET’S SAMPLING ALGORITHM\nWe now provide a complete description of certain steps that were explained on a higher level in the\nmain paper.\nSTEP 1: SAMPLING THE EPISODE’S CLASS SET\nImageNet class sampling\nThe procedure we use for sampling classes for an ImageNet episode\nis the following. First, we sample a node uniformly at random from the set of ‘eligible’ nodes of\nthe DAG structure corresponding to the speciﬁed split (train, validation or test). An internal node is\n‘eligible’ for this selection if it spans at least 5 leaves, but no more than 392 leaves. The number 392\nwas chosen because it is the smallest number so that, collectively, all eligible internal nodes span all\nleaves in the DAG. Once an eligible node is selected, some of the leaves that it spans will constitute\nthe classes of the episode. Speciﬁcally, if the number of those leaves is no greater than 50, we use all\nof them. Otherwise, we randomly choose 50 of them.\nThis procedure enables the creation of tasks of varying degrees of ﬁne-grainedness. For instance, if\nthe sampled internal node has a small height, the leaf classes that it spans will represent semantically-\nrelated concepts, thus posing a ﬁne-grained classiﬁcation task. As the height of the sampled node\nincreases, we ‘zoom out’ to consider a broader scope from which we sample classes and the resulting\nepisodes are more coarse-grained.\nSTEP 2: SAMPLING THE EPISODE’S EXAMPLES\na) Computing the query set size\nThe query set is class-balanced, reﬂecting the fact that we care\nequally to perform well on all classes of an episode. The number of query images per class is\ncomputed as:\nq = min\n\u001a\n10,\n\u0012\nmin\nc∈C ⌊0.5 ∗|Im(c)|⌋\n\u0013\u001b\nwhere C is the set of selected classes and Im(c) denotes the set of images belonging to class c. The\nmin over classes ensures that each class has at least q images to add to the query set, thus allowing it\nto be class-balanced. The 0.5 multiplier ensures that enough images of each class will be available to\nadd to the support set, and the minimum with 10 prevents the query set from being too large.\nb) Computing the support set size\nWe compute the total support set size as:\n|S| = min\n(\n500,\nX\nc∈C\n⌈β min{100, |Im(c)| −q}⌉\n)\n13\n\n\nPublished as a conference paper at ICLR 2020\nwhere β is a scalar sampled uniformly from interval (0, 1]. Intuitively, each class on average\ncontributes either all its remaining examples (after placing q of them in the query set) if there are less\nthan 100 or 100 otherwise, to avoid having too large support sets. The multiplication with β enables\nthe potential generation of smaller support sets even when multiple images are available, since we are\nalso interested in examining the very-low-shot end of the spectrum. The ‘ceiling’ operation ensures\nthat each selected class will have at least one image in the support set. Finally, we cap the total\nsupport set size to 500.\nc) Computing the shot of each class\nWe are now ready to compute the ‘shot’ of each class.\nSpeciﬁcally, the proportion of the support set that will be devoted to class c is computed as:\nRc =\nexp(αc)|Im(c)|\nX\nc′∈C\nexp(α′\nc)|Im(c′)|\nwhere αc is sampled uniformly from the interval [log(0.5), log(2)). Intuitively, the un-normalized\nproportion of the support set that will be occupied by class c is a noisy version of the total number of\nimages of that class in the dataset Im(c). This design choice is made in the hopes of obtaining realistic\nclass ratios, under the hypothesis that the dataset class statistics are a reasonable approximation of the\nreal-world statistics of appearances of the corresponding classes. The shot of a class c is then set to:\nkc = min {⌊Rc ∗(|S| −|C|)⌋+ 1, |Im(c)| −q}\nwhich ensures that at least one example is selected for each class, with additional examples selected\nproportionally to Rc, if enough are available.\n.3\nDATASETS\nMETA-DATASET is formed of data originating from 10 different image datasets. A complete list of\nthe datasets we use is the following.\n(a) ImageNet\n(b) Omniglot\n(c) Aircraft\n(d) Birds\n(e) DTD\n(f) Quick Draw\n(g) Fungi\n(h) VGG Flower\n(i) Trafﬁc Signs\n(j) MSCOCO\nFigure 4: Training examples taken from the various datasets forming META-DATASET.\nILSVRC-2012 (ImageNet, Russakovsky et al., 2015)\nA dataset of natural images from 1000\ncategories (Figure 4a). We removed some images that were duplicates of images in another dataset in\nMETA-DATASET (43 images that were also part of Birds) or other standard datasets of interest (92\nfrom Caltech-101 and 286 from Caltech-256). The complete list of duplicates is part of the source\ncode release.\nOmniglot (Lake et al., 2015)\nA dataset of images of 1623 handwritten characters from 50 different\nalphabets, with 20 examples per class (Figure 4b). While recently Vinyals et al. (2016) proposed a\n14\n\n\nPublished as a conference paper at ICLR 2020\nnew split for this dataset, we instead make use of the original intended split Lake et al. (2015) which\nis more challenging since the split is on the level of alphabets (30 training alphabets and 20 evaluation\nalphabets), not characters from those alphabets, therefore posing a more challenging generalization\nproblem. Out of the 30 training alphabets, we hold out the 5 smallest ones (i.e., with the least number\nof character classes) to form our validation set, and use the remaining 25 for training.\nAircraft (Maji et al., 2013)\nA dataset of images of aircrafts spanning 102 model variants, with\n100 images per class (Figure 4c). The images are cropped according to the providing bounding boxes,\nin order not to include other aircrafts, or the copyright text at the bottom of images.\nCUB-200-2011 (Birds, Wah et al., 2011)\nA dataset for ﬁne-grained classiﬁcation of 200 different\nbird species (Figure 4d). We did not use the provided bounding boxes to crop the images, instead the\nfull images are used, which provides a harder challenge.\nDescribable Textures (DTD, Cimpoi et al., 2014)\nA texture database, consisting of 5640 images,\norganized according to a list of 47 terms (categories) inspired from human perception (Figure 4e).\nQuick Draw (Jongejan et al., 2016)\nA dataset of 50 million black-and-white drawings across 345\ncategories, contributed by players of the game Quick, Draw! (Figure 4f).\nFungi (Schroeder & Cui, 2018)\nA large dataset of approximately 100K images of nearly 1,500\nwild mushrooms species (Figure 4g).\nVGG Flower (Nilsback & Zisserman, 2008)\nA dataset of natural images of 102 ﬂower categories.\nThe ﬂowers chosen to be ones commonly occurring in the United Kingdom. Each class consists of\nbetween 40 and 258 images (Figure 4h).\nTrafﬁc Signs (Houben et al., 2013)\nA dataset of 50,000 images of German road signs in 43 classes\n(Figure 4i).\nMSCOCO Lin et al. (2014)\nA dataset of images collected from Flickr with 1.5 million object\ninstances belonging to 80 classes labelled and localized using bounding boxes. We choose the\ntrain2017 split and create images crops from original images using each object instance’s groundtruth\nbounding box (Figure 4j).\n.4\nHYPERPARAMETERS\nWe used three architectures: a commonly-used four-layer convolutional network, an 18-layer residual\nnetwork and a wide residual network. While some of the baseline models performed best with the\nlatter, we noticed that the meta-learners preferred the resnet-18 backbone and rarely the four-layer-\nconvnet. For Relation Networks only, we also allow the option to use another architecture, aside from\nthe aforementioned three, inspired by the four-layer-convnet used in the Relation Networks paper\n(Sung et al., 2018). The main difference is that they used the usual max-pooling operation only in the\nﬁrst two layers, omitting it in the last two, yielding activations of larger spatial dimensions. In our\ncase, we found that these increased spatial dimensions did not ﬁt in memory, so as a compromise we\nused max-pooling on the ﬁrst 3 out of the 4 layer of the convnet.\nFor fo-MAML and fo-Proto-MAML, we tuned the inner-loop learning rate, the number of inner loop\nsteps, and the number of additional such steps to be performed in evaluation (i.e., validation or test)\nepisodes.\nFor the baselines, we tuned whether the cosine classiﬁer of Baseline++ will be used, as opposed\nto a standard forward pass through a linear classiﬁcation layer. Also, since Chen et al. (2019)\nadded weight normalization (Salimans & Kingma, 2016) to their implementation of the cosine\nclassiﬁer layer, we also implemented this and created a hyperparameter choice for whether or not it\nis enabled. This hyperparameter is independent from the one that decides if the cosine classiﬁer is\nused. Both are applicable to the k-NN Basline (for its all-way training classiﬁcation task) and to the\nFinetune Baseline (both for its all-way training classiﬁcation and for its within-episode classiﬁcation\n15\n\n\nPublished as a conference paper at ICLR 2020\nat validation and test times). For the Finetune Baseline, we tuned a binary hyperparameter deciding if\ngradient descent or ADAM is used for the within-task optimization. We also tuned the decision of\nwhether all embedding layers are ﬁnetuned or, alternatively, the embedding is held ﬁxed and only the\nﬁnal classiﬁer on top of it is optimized. Finally, we tuned the number of ﬁnetuning steps that will be\ncarried out.\nWe also tried two different image resolutions: the commonly-used 84x84 and 126x126. Finally, we\ntuned the learning rate schedule and weight decay and we used ADAM to train all of our models. All\nother details, dataset splits and the complete set of best hyperparameters discovered for each model\nare included in the source code.\n.5\nCOMPLETE MAIN RESULTS AND RANK COMPUTATION\nRank computation\nWe rank models by decreasing order of accuracy and handle ties by assigning\ntied models the average of their ranks. A tie between two models occurs when a 95% conﬁdence\ninterval statistical test on the difference between their mean accuracies is inconclusive in rejecting the\nnull hypothesis that this difference is 0. Our recommendation is that this test is ran to determine if\nties occur. As mentioned earlier, our source code will include this computation.\nComplete main tables\nFor completeness, Table 2 presents a more detailed version of Table 1 that\nalso displays conﬁdence intervals and per-dataset ranks computed using the above procedure.\n.6\nANALYSIS OF PERFORMANCE ACROSS SHOTS AND WAYS\nFor completeness, in Figure 5 we show the results of the analysis of the robustness to different ways\nand shots for the variants of the models that were trained on all datasets. We observe the same trends\nas discussed in our Experiments section for the variants of the models that were trained on ImageNet.\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nk-NN baseline\nFinetune baseline\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\n(a) Ways Analysis\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\nClass Precision\nk-NN baseline\nFinetune baseline\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\n(b) Shots Analysis\nFigure 5: Analysis of performance as a function of the episode’s way, shots for models whose\ntraining source is (the training data of) all datasets. The bands display 95% conﬁdence intervals.\n.7\nEFFECT OF TRAINING ON ALL DATASETS OVER TRAINING ON ILSVRC-2012 ONLY\nFor more clearly observing whether training on all datasets leads to improved generalization over\ntraining on ImageNet only, Figure 6 shows side-to-side the performance of each model trained on\nILSVRC only vs. all datasets. The difference between the performance of the all-dataset trained\nmodels versus the ImageNet-only trained ones is also visualized in Figure 1 in the main paper.\nAs discussed in the main paper, we notice that we do not always observe a clear generalization\nadvantage in training from a wider collection of image datasets. While some of the datasets that were\nadded to the meta-training phase did see an improvement across all models, in particular for Omniglot\nand Quick Draw, this was not true across the board. In fact, in certain cases the performance is\nslightly worse. We believe that more successfully leveraging diverse sources of data is an interesting\nopen research problem.\n16\n\n\nPublished as a conference paper at ICLR 2020\nTable 2: Few-shot classiﬁcation results on META-DATASET.\n(a) Models trained on ILSVRC-2012 only.\nTest Source\nMethod: Accuracy (%) ± conﬁdence (%)\nk-NN\nFinetune\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nProto-MAML\nILSVRC\n41.03±1.01 (6)\n45.78±1.10 (4)\n45.00±1.10 (4)\n50.50±1.08 (1.5)\n45.51±1.11 (4)\n34.69±1.01 (7)\n49.53±1.05 (1.5)\nOmniglot\n37.07±1.15 (7)\n60.85±1.58 (2.5)\n52.27±1.28 (5)\n59.98±1.35 (2.5)\n55.55±1.54 (4)\n45.35±1.36 (6)\n63.37±1.33 (1)\nAircraft\n46.81±0.89 (6)\n68.69±1.26 (1)\n48.97±0.93 (5)\n53.10±1.00 (4)\n56.24±1.11 (2.5)\n40.73±0.83 (7)\n55.95±0.99 (2.5)\nBirds\n50.13±1.00 (6.5)\n57.31±1.26 (5)\n62.21±0.95 (3.5)\n68.79±1.01 (1.5)\n63.61±1.06 (3.5)\n49.51±1.05 (6.5)\n68.66±0.96 (1.5)\nTextures\n66.36±0.75 (4)\n69.05±0.90 (1.5)\n64.15±0.85 (6)\n66.56±0.83 (4)\n68.04±0.81 (1.5)\n52.97±0.69 (7)\n66.49±0.83 (4)\nQuick Draw\n32.06±1.08 (7)\n42.60±1.17 (4.5)\n42.87±1.09 (4.5)\n48.96±1.08 (2)\n43.96±1.29 (4.5)\n43.30±1.08 (4.5)\n51.52±1.00 (1)\nFungi\n36.16±1.02 (4)\n38.20±1.02 (3)\n33.97±1.00 (5)\n39.71±1.11 (1.5)\n32.10±1.10 (6)\n30.55±1.04 (7)\n39.96±1.14 (1.5)\nVGG Flower\n83.10±0.68 (4)\n85.51±0.68 (2.5)\n80.13±0.71 (6)\n85.27±0.77 (2.5)\n81.74±0.83 (5)\n68.76±0.83 (7)\n87.15±0.69 (1)\nTrafﬁc Signs\n44.59±1.19 (6)\n66.79±1.31 (1)\n47.80±1.14 (3.5)\n47.12±1.10 (5)\n50.93±1.51 (2)\n33.67±1.05 (7)\n48.83±1.09 (3.5)\nMSCOCO\n30.38±0.99 (6.5)\n34.86±0.97 (4)\n34.99±1.00 (4)\n41.00±1.10 (2)\n35.30±1.23 (4)\n29.15±1.01 (6.5)\n43.74±1.12 (1)\nAvg. rank\n5.7\n2.9\n4.65\n2.65\n3.7\n6.55\n1.85\n(b) Models trained on all datasets.\nTest Source\nMethod: Accuracy (%) ± conﬁdence (%) (rank)\nk-NN\nFinetune\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nProto-MAML\nILSVRC\n38.55±0.94 (4.5)\n43.08±1.08 (2.5)\n36.08±1.00 (6)\n44.50±1.05 (2.5)\n37.83±1.01 (4.5)\n30.89±0.93 (7)\n46.52±1.05 (1)\nOmniglot\n74.60±1.08 (6)\n71.11±1.37 (7)\n78.25±1.01 (4.5)\n79.56±1.12 (4.5)\n83.92±0.95 (2.5)\n86.57±0.79 (1)\n82.69±0.97 (2.5)\nAircraft\n64.98±0.82 (7)\n72.03±1.07 (3.5)\n69.17±0.96 (5.5)\n71.14±0.86 (3.5)\n76.41±0.69 (1)\n69.71±0.83 (5.5)\n75.23±0.76 (2)\nBirds\n66.35±0.92 (2.5)\n59.82±1.15 (5)\n56.40±1.00 (6)\n67.01±1.02 (2.5)\n62.43±1.08 (4)\n54.14±0.99 (7)\n69.88±1.02 (1)\nTextures\n63.58±0.79 (5)\n69.14±0.85 (1.5)\n61.80±0.74 (6)\n65.18±0.84 (3.5)\n64.14±0.83 (3.5)\n56.56±0.73 (7)\n68.25±0.81 (1.5)\nQuick Draw\n44.88±1.05 (7)\n47.05±1.16 (6)\n60.81±1.03 (3.5)\n64.88±0.89 (2)\n59.73±1.10 (5)\n61.75±0.97 (3.5)\n66.84±0.94 (1)\nFungi\n37.12±1.06 (3.5)\n38.16±1.04 (3.5)\n33.70±1.04 (6)\n40.26±1.13 (2)\n33.54±1.11 (6)\n32.56±1.08 (6)\n41.99±1.17 (1)\nVGG Flower\n83.47±0.61 (4)\n85.28±0.69 (3)\n81.90±0.72 (5)\n86.85±0.71 (2)\n79.94±0.84 (6)\n76.08±0.76 (7)\n88.72±0.67 (1)\nTrafﬁc Signs\n40.11±1.10 (6)\n66.74±1.23 (1)\n55.57±1.08 (2)\n46.48±1.00 (4)\n42.91±1.31 (5)\n37.48±0.93 (7)\n52.42±1.08 (3)\nMSCOCO\n29.55±0.96 (5)\n35.17±1.08 (3)\n28.79±0.96 (5)\n39.87±1.06 (2)\n29.37±1.08 (5)\n27.41±0.89 (7)\n41.74±1.13 (1)\nAvg. rank\n5.05\n3.6\n4.95\n2.85\n4.25\n5.8\n1.5\n17\n\n\nPublished as a conference paper at ICLR 2020\nFigure 6: Accuracy on the test datasets, when training on ILSVRC only or All datasets (same results\nas shown in the main tables). The bars display 95% conﬁdence intervals.\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\nModel\nk-NN baseline\nFinetune baseline\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\nTrain Source\nILSVRC-2012\nAll datasets\n.8\nEFFECT OF PRE-TRAINING VERSUS TRAINING FROM SCRATCH\nFor each meta-learner, we selected the best model (based on validation on ImageNet’s validation\nsplit) out of the ones that used the pre-trained initialization, and the best out of the ones that trained\nfrom scratch. We then ran the evaluation of each on (the test split of) all datasets in order to quantify\nhow beneﬁcial this pre-trained initialization is. We performed this experiment twice: for the models\nthat are trained on ImageNet only and for the models that are trained on (the training splits of) all\ndatasets.\nThe results of this investigation were reported in the main paper in Figure 3a and Figure 3b, for\nImageNet-only training and all dataset training, respectively. We show the same results in Figure 7,\nprinted larger to facilitate viewing of error bars. For easier comparison, we also plot the difference in\nperformance of the models that were pre-trained over the ones that weren’t, in Figures 8a and 8b.\nThese ﬁgures make it easier to spot that while using the pre-trained solution usually helps for datasets\nthat are visually not too different from ImageNet, it may hurt for datasets that are signiﬁcantly different\nfrom it, such as Omniglot, Quickdraw (and surprisingly Aircraft). Note that these three datasets\nare the same three that we found beneﬁt from training on All datasets instead of ImageNet-only. It\nappears that using the pre-trained solution biases the ﬁnal solution to specialize on ImageNet-like\ndatasets.\n.9\nEFFECT OF META-LEARNING VERSUS INFERENCE-ONLY\nFigure 9 shows the same plots as in Figures 3c and 3d but printed larger to facilitate viewing of\nerror bars. Furthermore, as we have done for visualizing the observed gain of pre-training, we also\npresent in Figures 10a and 10b the gain observed from meta-learning as opposed to training the\ncorresponding inference-only baseline, as explained in the Experiments section of the main paper.\nThis visulization makes it clear that while meta-training usually helps on ImageNet (or doesn’t hurt\ntoo much), it sometimes hurts when it is performed on all datasets, emphasizing the need for further\nresearch into best practices of meta-learning across heterogeneous sources.\n.10\nFINEGRAINEDNESS ANALYSIS\nWe investigate the hypothesis that ﬁner-grained tasks are more challenging than coarse-grained ones\nby creating binary ImageNet episodes with the two classes chosen uniformly at random from the\nDAG’s set of leaves. We then deﬁne the degree of coarse-grainedness of a task as the height of\nthe lowest common ancestor of the two chosen leaves, where the height is deﬁned as the length of\nthe longest path from the lowest common ancestor to one of the selected leaves. Larger heights\nthen correspond to coarser-grained tasks. We present these results in Figure 11. We do not detect a\nsigniﬁcant trend when performing this analysis on the test DAG. The results on the training DAG,\n18\n\n\nPublished as a conference paper at ICLR 2020\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\nModel\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\nInitialization\nPre-trained\nFrom scratch\n(a) ImageNet.\n0\n20\n40\n60\n80\n100\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\nModel\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\n0\n20\n40\n60\n80\n100 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\nInitialization\nPre-trained\nFrom scratch\n(b) All datasets.\nFigure 7: Comparing pre-training to starting from scratch. Same plots as Figure 3a and Figure 3b,\nonly larger.\n15\n10\n5\n0\n5\n10\n15\n20\n25\n30\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\nModel\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\n15\n10\n5\n0\n5\n10\n15\n20\n25\n30 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\n(a) The gain from pre-training (ImageNet).\n20\n10\n0\n10\n20\n30\n40\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\nModel\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\n20\n10\n0\n10\n20\n30\n40 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\n(b) The gain from pre-training (All datasets).\nFigure 8: The performance difference of initializing the embedding weights from a pre-trained\nsolution, before episodically training on ImageNet or all datasets, over using a random initialization\nof those weights. The pre-trained weights that we consider are the ones that the k-NN baseline\nconverged to when it was trained on ImageNet. Positive values indicate that this pre-training was\nbeneﬁcial.\n19\n\n\nPublished as a conference paper at ICLR 2020\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\nModel\nMatchingNet\nProtoNet\nfo-Proto-MAML\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\nMeta-training\nInference-only\n(a) ImageNet.\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\nModel\nMatchingNet\nProtoNet\nfo-Proto-MAML\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\nMeta-training\nInference-only\n(b) All datasets.\nFigure 9: Comparing the meta-trained variant of meta-learners against their inference-only counter-\npart. Same plots as Figure 3c and Figure 3d, only larger.\n5\n0\n5\n10\n15\n20\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\nModel\nMatchingNet\nProtoNet\nfo-Proto-MAML\n5\n0\n5\n10\n15\n20 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\n(a) The gain from meta-training on ImageNet.\n15\n10\n5\n0\n5\n10\n15\nILSVRC\nOmniglot\nAircraft\nCU Birds\nTextures\nModel\nMatchingNet\nProtoNet\nfo-Proto-MAML\n15\n10\n5\n0\n5\n10\n15 Quick Draw\nFungi\nVGG Flower\nTraffic Sign\nMSCOCO\n(b) The gain from meta-training on All datasets.\nFigure 10: The performance difference of meta-learning over the corresponding inference-only\nbaseline of each meta-learner. Positive values indicate that meta-learning was beneﬁcial.\n1\n2\n3\n4\n5\n6\nHeight of Lowest Common Ancestor\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAccuracy\nModel\nk-NN baseline\nFinetune baseline\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\n(a)\nFine-grainedness Analysis (on ImageNet’s test\ngraph)\n0\n2\n4\n6\n8\n10\n12\nHeight of Lowest Common Ancestor\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\nAccuracy\nModel\nk-NN baseline\nFinetune baseline\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\n(b) Fine-grainedness Analysis (on ImageNet’s train\ngraph graph)\nFigure 11: Analysis of performance as a function of the degree of ﬁne-grainedness. Larger heights\ncorrespond to coarser-grained tasks. The bands display 95% conﬁdence intervals.\n20\n\n\nPublished as a conference paper at ICLR 2020\nTable 3: Improvement of fo-MAML when using a larger inner learning rate α.\n(a) Models trained on ILSVRC-2012 only.\nTest Source\nMethod: Accuracy (%) ± conﬁdence (%)\nfo-MAML α = 0.01 (old)\nfo-MAML α ≈0.1\nILSVRC\n36.09±1.01\n45.51±1.11\nOmniglot\n38.67±1.39\n55.55±1.54\nAircraft\n34.50±0.90\n56.24±1.11\nBirds\n49.10±1.18\n63.61±1.06\nTextures\n56.50±0.80\n68.04±0.81\nQuick Draw\n27.24±1.24\n43.96±1.29\nFungi\n23.50±1.00\n32.10±1.10\nVGG Flower\n66.42±0.96\n81.74±0.83\nTrafﬁc Signs\n33.23±1.34\n50.93±1.51\nMSCOCO\n27.52±1.11\n35.30±1.23\n(b) Models trained on all datasets.\nTest Source\nMethod: Accuracy (%) ± conﬁdence (%)\nfo-MAML α = 0.01 (old)\nfo-MAML α ≈0.1\nILSVRC\n32.36±1.02\n37.83±1.01\nOmniglot\n71.91±1.20\n83.92±0.95\nAircraft\n52.76±0.90\n76.41±0.69\nBirds\n47.24±1.14\n62.43±1.08\nTextures\n56.66±0.74\n64.16±0.83\nQuick Draw\n50.50±1.19\n59.73±1.10\nFungi\n21.02±0.99\n33.54±1.11\nVGG Flower\n70.93±0.99\n79.94±0.84\nTrafﬁc Signs\n34.18±1.26\n42.91±1.31\nMSCOCO\n24.05±1.10\n29.37±1.08\nthough, do seem to indicate that our hypothesis holds to some extent. We conjecture that this may be\ndue to the richer structure of the training DAG, but we encourage further investigation.\n.11\nTHE IMPORTANCE OF MAML’S INNER-LOOP LEARNING RATE HYPERPARAMETER.\nThe camera-ready version includes updated results for MAML and Proto-MAML following an\nexternal suggestion to experiment with larger values for the inner-loop learning rate α of MAML.\nWe found that re-doing our hyperparameter search with a revised range that includes larger α values\nsigniﬁcantly improved fo-MAML’s performance on META-DATASET. For consistency, we applied\nthe same change to fo-Proto-MAML and re-ran those experiments too.\nWe found that the value of this α that performs best for fo-MAML both for training on ImageNet\nonly and training on all datasets is approximately 0.1, which is an order of magnitude larger than our\nprevious best value. Interestingly, fo-Proto-MAML does not choose such a large α value, with best\nα being 0.0054 when training on ImageNet only and 0.02 when training on all datasets. Plausibly\nthis difference can be attributed to the better initialization of Proto-MAML which requires a less\naggressive optimization for the adaptation to each new task. This hypothesis is also supported by the\nfact that fo-Proto-MAML chooses to take fewer adaptation steps than fo-MAML does. The complete\nset of best discovered hyperparameters is available in our public code.\nTo emphasize the importance of properly tuning this hyperparameter, Table 3 displays the previous\nbest and the new best fo-MAML results side-by-side, showcasing the large performance gap when\nusing the appropriate value for α.\n21\n\n\nPublished as a conference paper at ICLR 2020\n.12\nTHE CHOICE OF A META-VALIDATION PROCEDURE FOR META-DATASET\nThe design choice we made, as discussed in the main paper, is to use (the meta-validation set of)\nImageNet only for model selection in all of our experiments. In the absence of previous results on the\ntopic, this is a reasonable strategy since ImageNet has been known to consitute a useful proxy for\nperformance on other datasets. However, it is likely that this is not the optimal choice: there might be\ncertain hyperparameters for a given model that work best for held-out ImageNet episodes, but not\nfor held-out episodes of other datasets. An alternative meta-validation scheme would be to use the\naverage (across datasets) validation accuracy as an indicator for early stopping and model selection.\nWe did not choose this method due to concerns about the reliability of this average performance.\nNotably, taking a simple average would over-emphasize larger datasets, or might over-emphasize\ndatasets with natural images (as opposed to Omniglot and Quickdraw). Nevertheless, whether this\nstrategy is beneﬁcial is an interesting empirical question.\n.13\nADDITIONAL PER-DATASET ANALYSIS OF SHOTS AND WAYS\nIn our previous analysis of performance across different shots and ways (Figures 2b, 2a, and 5), the\nperformance is averaged over all evaluation datasets. In this section we further break down those\nplots by presenting the results separately for each dataset. Figures 12 and 13 show the analysis of\nperformance as a function of ways and shots (respectively) for each evaluation dataset, for models\nthat were trained on ImageNet only. For completeness, Figures 14 and 15 show the same for the\nmodels trained on (the training splits of) all datasets.\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(a) ILSVRC\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(b) Omniglot\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(c) Aircraft\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(d) Birds\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(e) Textures\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(f) Quickdraw\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(g) Fungi\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(h) VGG Flower\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(i) Trafﬁc Signs\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(j) MSCOCO\nk-NN baseline\nFinetune baseline\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\nFigure 12: The performance across different ways, with 95% conﬁdence intervals, shown separately\nfor each evaluation dataset. All models had been trained on ImageNet-only.\n22\n\n\nPublished as a conference paper at ICLR 2020\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(a) ILSVRC\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(b) Omniglot\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(c) Aircraft\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(d) Birds\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(e) Textures\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(f) Quickdraw\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(g) Fungi\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(h) VGG Flower\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(i) Trafﬁc Signs\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(j) MSCOCO\nk-NN baseline\nFinetune baseline\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\nFigure 13: The performance across different shots, with 95% conﬁdence intervals, shown separately\nfor each evaluation dataset. All models had been trained on ImageNet-only.\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(a) ILSVRC\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(b) Omniglot\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(c) Aircraft\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(d) Birds\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(e) Textures\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(f) Quickdraw\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(g) Fungi\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(h) VGG Flower\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(i) Trafﬁc Signs\n10\n20\n30\n40\n50\nWay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(j) MSCOCO\nk-NN baseline\nFinetune baseline\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\nFigure 14: The performance across different ways, with 95% conﬁdence intervals, shown separately\nfor each evaluation dataset. All models had been trained on (the training splits of) all datasets.\n23\n\n\nPublished as a conference paper at ICLR 2020\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(a) ILSVRC\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(b) Omniglot\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(c) Aircraft\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(d) Birds\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(e) Textures\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(f) Quickdraw\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(g) Fungi\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(h) VGG Flower\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(i) Trafﬁc Signs\n0\n20\n40\n60\n80\n100\nShot\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClass Precision\n(j) MSCOCO\nk-NN baseline\nFinetune baseline\nMatchingNet\nProtoNet\nfo-MAML\nRelationNet\nfo-Proto-MAML\nFigure 15: The performance across different shots, with 95% conﬁdence intervals, shown separately\nfor each evaluation dataset. All models had been trained on (the training splits of) all datasets.\n24\n"
}