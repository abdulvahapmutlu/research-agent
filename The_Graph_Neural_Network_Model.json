{
  "filename": "The_Graph_Neural_Network_Model.pdf",
  "num_pages": 20,
  "pages": [
    "IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 1, JANUARY 2009\n61\nThe Graph Neural Network Model\nFranco Scarselli, Marco Gori, Fellow, IEEE, Ah Chung Tsoi, Markus Hagenbuchner, Member, IEEE, and\nGabriele Monfardini\nAbstract—Many underlying relationships among data in several\nareas of science and engineering, e.g., computer vision, molec-\nular chemistry, molecular biology, pattern recognition, and data\nmining, can be represented in terms of graphs. In this paper, we\npropose a new neural network model, called graph neural network\n(GNN) model, that extends existing neural network methods for\nprocessing the data represented in graph domains. This GNN\nmodel, which can directly process most of the practically useful\ntypes of graphs, e.g., acyclic, cyclic, directed, and undirected,\nimplements a function\n(\n)\nthat maps a graph\nand one of its nodes\ninto an\n-dimensional Euclidean space. A\nsupervised learning algorithm is derived to estimate the param-\neters of the proposed GNN model. The computational cost of the\nproposed algorithm is also considered. Some experimental results\nare shown to validate the proposed learning algorithm, and to\ndemonstrate its generalization capabilities.\nIndex Terms—Graphical domains, graph neural networks\n(GNNs), graph processing, recursive neural networks.\nI. INTRODUCTION\nD\nATA can be naturally represented by graph structures in\nseveral application areas, including proteomics [1], image\nanalysis [2], scene description [3], [4], software engineering [5],\n[6], and natural language processing [7]. The simplest kinds of\ngraph structures include single nodes and sequences. But in sev-\neral applications, the information is organized in more complex\ngraph structures such as trees, acyclic graphs, or cyclic graphs.\nTraditionally, data relationships exploitation has been the sub-\nject of many studies in the community of inductive logic pro-\ngramming and, recently, this research theme has been evolving\nin different directions [8], also because of the applications of\nrelevant concepts in statistics and neural networks to such areas\n(see, for example, the recent workshops [9]–[12]).\nIn machine learning, structured data is often associated with\nthe goal of (supervised or unsupervised) learning from exam-\nManuscript received May 24, 2007; revised January 08, 2008 and May 02,\n2008; accepted June 15, 2008. First published December 09, 2008; current ver-\nsion published January 05, 2009. This work was supported by the Australian\nResearch Council in the form of an International Research Exchange scheme\nwhich facilitated the visit by F. Scarselli to University of Wollongong when the\ninitial work on this paper was performed. This work was also supported by the\nARC Linkage International Grant LX045446 and the ARC Discovery Project\nGrant DP0453089.\nF. Scarselli, M. Gori, and G. Monfardini are with the Faculty of Informa-\ntion Engineering, University of Siena, Siena 53100, Italy (e-mail: franco@dii.\nunisi.it; marco@dii.unisi.it; monfardini@dii.unisi.it).\nA. C. Tsoi is with Hong Kong Baptist University, Kowloon, Hong Kong\n(e-mail: act@hkbu.edu.hk).\nM. Hagenbuchner is with the University of Wollongong, Wollongong, N.S.W.\n2522, Australia (e-mail: markus@uow.edu.au).\nColor versions of one or more of the ﬁgures in this paper are available online\nat http://ieeexplore.ieee.org.\nDigital Object Identiﬁer 10.1109/TNN.2008.2005605\nples a function\nthat maps a graph\nand one of its nodes\nto\na vector of reals1:\n. Applications to a graphical\ndomain can generally be divided into two broad classes, called\ngraph-focused and node-focused applications, respectively, in\nthis paper. In graph-focused applications, the function\nis in-\ndependent of the node\nand implements a classiﬁer or a re-\ngressor on a graph structured data set. For example, a chemical\ncompound can be modeled by a graph\n, the nodes of which\nstand for atoms (or chemical groups) and the edges of which\nrepresent chemical bonds [see Fig. 1(a)] linking together some\nof the atoms. The mapping\nmay be used to estimate the\nprobability that the chemical compound causes a certain disease\n[13]. In Fig. 1(b), an image is represented by a region adjacency\ngraph where nodes denote homogeneous regions of intensity of\nthe image and arcs represent their adjacency relationship [14]. In\nthis case,\nmay be used to classify the image into different\nclasses according to its contents, e.g., castles, cars, people, and\nso on.\nIn node-focused applications,\ndepends on the node\n, so\nthat the classiﬁcation (or the regression) depends on the proper-\nties of each node. Object detection is an example of this class of\napplications. It consists of ﬁnding whether an image contains a\ngiven object, and, if so, localizing its position [15]. This problem\ncan be solved by a function\n, which classiﬁes the nodes of the\nregion adjacency graph according to whether the corresponding\nregion belongs to the object. For example, the output of\nfor\nFig. 1(b) might be 1 for black nodes, which correspond to the\ncastle, and 0 otherwise. Another example comes from web page\nclassiﬁcation. The web can be represented by a graph where\nnodes stand for pages and edges represent the hyperlinks be-\ntween them [Fig. 1(c)]. The web connectivity can be exploited,\nalong with page contents, for several purposes, e.g., classifying\nthe pages into a set of topics.\nTraditional machine learning applications cope with graph\nstructured data by using a preprocessing phase which maps the\ngraph structured information to a simpler representation, e.g.,\nvectors of reals [16]. In other words, the preprocessing step ﬁrst\n“squashes” the graph structured data into a vector of reals and\nthen deals with the preprocessed data using a list-based data\nprocessing technique. However, important information, e.g., the\ntopological dependency of information on each node may be\nlost during the preprocessing stage and the ﬁnal result may de-\npend, in an unpredictable manner, on the details of the prepro-\ncessing algorithm. More recently, there have been various ap-\nproaches [17], [18] attempting to preserve the graph structured\nnature of the data for as long as required before the processing\n1Note that in most classiﬁcation problems, the mapping is to a vector of inte-\ngers IN\n, while in regression problems, the mapping is to a vector of reals IR\n.\nHere, for simplicity of exposition, we will denote only the regression case. The\nproposed formulation can be trivially rewritten for the situation of classiﬁcation.\n1045-9227/$25.00 © 2008 IEEE\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n",
    "62\nIEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 1, JANUARY 2009\nFig. 1. Some applications where the information is represented by graphs: (a) a chemical compound (adrenaline), (b) an image, and (c) a subset of the web.\nphase. The idea is to encode the underlying graph structured\ndata using the topological relationships among the nodes of the\ngraph, in order to incorporate graph structured information in\nthe data processing step. Recursive neural networks [17], [19],\n[20] and Markov chains [18], [21], [22] belong to this set of tech-\nniques and are commonly applied both to graph and node-fo-\ncused problems. The method presented in this paper extends\nthese two approaches in that it can deal directly with graph struc-\ntured information.\nExisting recursive neural networks are neural network models\nwhose input domain consists of directed acyclic graphs [17],\n[19], [20]. The method estimates the parameters\nof a func-\ntion\n, which maps a graph to a vector of reals. The approach\ncan also be used for node-focused applications, but in this case,\nthe graph must undergo a preprocessing phase [23]. Similarly,\nusing a preprocessing phase, it is possible to handle certain types\nof cyclic graphs [24]. Recursive neural networks have been ap-\nplied to several problems including logical term classiﬁcation\n[25], chemical compound classiﬁcation [26], logo recognition\n[2], [27], web page scoring [28], and face localization [29].\nRecursive neural networks are also related to support vector\nmachines [30]–[32], which adopt special kernels to operate on\ngraph structured data. For example, the diffusion kernel [33] is\nbased on heat diffusion equation; the kernels proposed in [34]\nand [35] exploit the vectors produced by a graph random walker\nand those designed in [36]–[38] use a method of counting the\nnumber of common substructures of two trees. In fact, recursive\nneural networks, similar to support vector machine methods,\nautomatically encode the input graph into an internal represen-\ntation. However, in recursive neural networks, the internal en-\ncoding is learned, while in support vector machine, it is designed\nby the user.\nOn the other hand, Markov chain models can emulate\nprocesses where the causal connections among events are\nrepresented by graphs. Recently, random walk theory, which\naddresses a particular class of Markov chain models, has been\napplied with some success to the realization of web page\nranking algorithms [18], [21]. Internet search engines use\nranking algorithms to measure the relative “importance” of\nweb pages. Such measurements are generally exploited, along\nwith other page features, by “horizontal” search engines, e.g.,\nGoogle [18], or by personalized search engines (“vertical”\nsearch engines; see, e.g., [22]) to sort the universal resource\nlocators (URLs) returned on user queries.2 Some attempts have\nbeen made to extend these models with learning capabilities\nsuch that a parametric model representing the behavior of\nthe system can be estimated from a set of training examples\nextracted from a collection [22], [40], [41]. Those models are\nable to generalize the results to score all the web pages in the\ncollection. More generally, several other statistical methods\nhave been proposed, which assume that the data set consists of\npatterns and relationships between patterns. Those techniques\ninclude random ﬁelds [42], Bayesian networks [43], statistical\nrelational learning [44], transductive learning [45], and semisu-\npervised approaches for graph processing [46].\nIn this paper, we present a supervised neural network model,\nwhich is suitable for both graph and node-focused applications.\nThis model uniﬁes these two existing models into a common\n2The relative importance measure of a web page is also used to serve other\ngoals, e.g., to improve the efﬁciency of crawlers [39].\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n",
    "SCARSELLI et al.: THE GRAPH NEURAL NETWORK MODEL\n63\nframework. We will call this novel neural network model a\ngraph neural network (GNN). It will be shown that the GNN\nis an extension of both recursive neural networks and random\nwalk models and that it retains their characteristics. The model\nextends recursive neural networks since it can process a more\ngeneral class of graphs including cyclic, directed, and undi-\nrected graphs, and it can deal with node-focused applications\nwithout any preprocessing steps. The approach extends random\nwalk theory by the introduction of a learning algorithm and by\nenlarging the class of processes that can be modeled.\nGNNs are based on an information diffusion mechanism. A\ngraph is processed by a set of units, each one corresponding to a\nnode of the graph, which are linked according to the graph con-\nnectivity. The units update their states and exchange informa-\ntion until they reach a stable equilibrium. The output of a GNN\nis then computed locally at each node on the base of the unit\nstate. The diffusion mechanism is constrained in order to en-\nsure that a unique stable equilibrium always exists. Such a real-\nization mechanism was already used in cellular neural networks\n[47]–[50] and Hopﬁeld neural networks [51]. In those neural\nnetwork models, the connectivity is speciﬁed according to a pre-\ndeﬁned graph, the network connections are recurrent in nature,\nand the neuron states are computed by relaxation to an equilib-\nrium point. GNNs differ from both the cellular neural networks\nand Hopﬁeld neural networks in that they can be used for the\nprocessing of more general classes of graphs, e.g., graphs con-\ntaining undirected links, and they adopt a more general diffusion\nmechanism.\nIn this paper, a learning algorithm will be introduced, which\nestimates the parameters of the GNN model on a set of given\ntraining examples. In addition, the computational cost of the pa-\nrameter estimation algorithm will be considered. It is also worth\nmentioning that elsewhere [52] it is proved that GNNs show a\nsort of universal approximation property and, under mild condi-\ntions, they can approximate most of the practically useful func-\ntions\non graphs.3\nThe structure of this paper is as follows. After a brief de-\nscription of the notation used in this paper as well as some pre-\nliminary deﬁnitions, Section II presents the concept of a GNN\nmodel, together with the proposed learning algorithm for the\nestimation of the GNN parameters. Moreover, Section III dis-\ncusses the computational cost of the learning algorithm. Some\nexperimental results are presented in Section IV. Conclusions\nare drawn in Section V.\nII. THE GRAPH NEURAL NETWORK MODEL\nWe begin by introducing some notations that will be used\nthroughout the paper. A graph\nis a pair\n, where\nis\nthe set of nodes and\nis the set of edges. The set\nstands\nfor the neighbors of\n, i.e., the nodes connected to\nby an arc,\nwhile\ndenotes the set of arcs having\nas a vertex. Nodes\nand edges may have labels represented by real vectors. The la-\nbels attached to node\nand edge\nwill be represented\nby\nand\n, respectively. Let denote the\nvector obtained by stacking together all the labels of the graph.\n3Due to the length of proofs, such results cannot be shown here and is included\nin [52].\nThe notation adopted for labels follows a more general scheme:\nif\nis a vector that contains data from a graph and\nis a subset of\nthe nodes (the edges), then\ndenotes the vector obtained by se-\nlecting from\nthe components related to the node (the edges) in\n. For example,\nstands for the vector containing the labels\nof all the neighbors of\n. Labels usually include features of ob-\njects related to nodes and features of the relationships between\nthe objects. For example, in the case of an image as in Fig. 1(b),\nnode labels might represent properties of the regions (e.g., area,\nperimeter, and average color intensity), while edge labels might\nrepresent the relative position of the regions (e.g., the distance\nbetween their barycenters and the angle between their principal\naxes). No assumption is made on the arcs; directed and undi-\nrected edges are both permitted. However, when different kinds\nof edges coexist in the same data set, it is necessary to distin-\nguish them. This can be easily achieved by attaching a proper\nlabel to each edge. In this case, different kinds of arcs turn out\nto be just arcs with different labels.\nThe considered graphs may be either positional or nonposi-\ntional. Nonpositional graphs are those described so far; posi-\ntional graphs differ since a unique integer identiﬁer is assigned\nto each neighbors of a node\nto indicate its logical position.\nFormally, for each node\nin a positional graph, there exists an\ninjective function\n, which assigns to\neach neighbor\nof\na position\n. Note that the position\nof the neighbor can be implicitly used for storing useful infor-\nmation. For instance, let us consider the example of the region\nadjacency graph [see Fig. 1(b)]:\ncan be used to represent the\nrelative spatial position of the regions, e.g.,\nmight enumerate\nthe neighbors of a node , which represents the adjacent regions,\nfollowing a clockwise ordering convention.\nThe domain considered in this paper is the set\nof pairs of\na graph and a node, i.e.,\nwhere\nis a set of the\ngraphs and\nis a subset of their nodes. We assume a supervised\nlearning framework with the learning set\nwhere\ndenotes the th node in the set\nand\nis the desired target associated to\n. Finally,\nand\n. Interestingly, all the graphs of the learning set can be\ncombined into a unique disconnected graph, and, therefore, one\nmight think of the learning set as the pair\nwhere\nis a graph and\na is set of pairs\n. It is worth mentioning that this com-\npact deﬁnition is not only useful for its simplicity, but that it\nalso captures directly the very nature of some problems where\nthe domain consists of only one graph, for instance, a large por-\ntion of the web [see Fig. 1(c)].\nA. The Model\nThe intuitive idea underlining the proposed approach is that\nnodes in a graph represent objects or concepts, and edges rep-\nresent their relationships. Each concept is naturally deﬁned by\nits features and the related concepts. Thus, we can attach a state\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n",
    "64\nIEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 1, JANUARY 2009\nFig. 2. Graph and the neighborhood of a node. The state xxx\nof the node 1\ndepends on the information contained in its neighborhood.\nto each node\nthat is based on the information con-\ntained in the neighborhood of\n(see Fig. 2). The state\ncon-\ntains a representation of the concept denoted by\nand can be\nused to produce an output\n, i.e., a decision about the concept.\nLet\nbe a parametric function, called local transition func-\ntion, that expresses the dependence of a node\non its neighbor-\nhood and let\nbe the local output function that describes how\nthe output is produced. Then,\nand\nare deﬁned as follows:\n(1)\nwhere\n,\n,\n, and\nare the label of\n, the labels\nof its edges, the states, and the labels of the nodes in the neigh-\nborhood of\n, respectively.\nRemark 1: Different notions of neighborhood can be adopted.\nFor example, one may wish to remove the labels\n, since\nthey include information that is implicitly contained in\n.\nMoreover, the neighborhood could contain nodes that are two\nor more links away from\n. In general, (1) could be simpliﬁed\nin several different ways and several minimal models4 exist. In\nthe following, the discussion will mainly be based on the form\ndeﬁned by (1), which is not minimal, but it is the one that more\nclosely represents our intuitive notion of neighborhood.\nRemark 2: Equation (1) is customized for undirected graphs.\nWhen dealing with directed graphs, the function\ncan also ac-\ncept as input a representation of the direction of the arcs. For ex-\nample,\nmay take as input a variable\nfor each arc\nsuch that\n, if\nis directed towards\nand\n, if\ncomes from\n. In the following, in order to keep the notations\ncompact, we maintain the customization of (1). However, un-\nless explicitly stated, all the results proposed in this paper hold\n4A model is said to be minimal if it has the smallest number of variables while\nretaining the same computational power.\nalso for directed graphs and for graphs with mixed directed and\nundirected links.\nRemark 3: In general, the transition and the output functions\nand their parameters may depend on the node\n. In fact, it is\nplausible that different mechanisms (implementations) are used\nto represent different kinds of objects. In this case, each kind of\nnodes\nhas its own transition function\n, output function\n, and a set of parameters\n. Thus, (1) becomes\nand\n.\nHowever, for the sake of simplicity, our analysis will consider\n(1) that describes a particular model where all the nodes share\nthe same implementation.\nLet\n, , , and\nbe the vectors constructed by stacking all\nthe states, all the outputs, all the labels, and all the node labels,\nrespectively. Then, (1) can be rewritten in a compact form as\n(2)\nwhere\n, the global transition function and\n, the global\noutput function are stacked versions of\ninstances of\nand\n, respectively.\nWe are interested in the case when\nare uniquely deﬁned\nand (2) deﬁnes a map\n, which takes a graph\nas input and returns an output\nfor each node. The Banach\nﬁxed point theorem [53] provides a sufﬁcient condition for the\nexistence and uniqueness of the solution of a system of equa-\ntions. According to Banach’s theorem [53], (2) has a unique so-\nlution provided that\nis a contraction map with respect to the\nstate, i.e., there exists\n,\n, such that\nholds for any\n, where\ndenotes\na vectorial norm. Thus, for the moment, let us assume that\nis a contraction map. Later, we will show that, in GNNs, this\nproperty is enforced by an appropriate implementation of the\ntransition function.\nNote that (1) makes it possible to process both positional and\nnonpositional graphs. For positional graphs,\nmust receive the\npositions of the neighbors as additional inputs. In practice, this\ncan be easily achieved provided that information contained in\n,\n, and\nis sorted according to neighbors’ po-\nsitions and is properly padded with special null values in po-\nsitions corresponding to nonexisting neighbors. For example,\n, where\nis the max-\nimal number of neighbors of a node;\nholds, if\nis the\nth neighbor of\n; and\n, for some prede-\nﬁned null state\n, if there is no th neighbor.\nHowever, for nonpositional graphs, it is useful to replace\nfunction\nof (1) with\n(3)\nwhere\nis a parametric function. This transition function,\nwhich has been successfully used in recursive neural networks\n[54], is not affected by the positions and the number of the chil-\ndren. In the following, (3) is referred to as the nonpositional\nform, while (1) is called the positional form. In order to imple-\nment the GNN model, the following items must be provided:\n1) a method to solve (1);\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n",
    "SCARSELLI et al.: THE GRAPH NEURAL NETWORK MODEL\n65\nFig. 3. Graph (on the top), the corresponding encoding network (in the middle), and the network obtained by unfolding the encoding network (at the bottom).\nThe nodes (the circles) of the graph are replaced, in the encoding network, by units computing f\nand g\n(the squares). When f\nand g\nare implemented by\nfeedforward neural networks, the encoding network is a recurrent neural network. In the unfolding network, each layer corresponds to a time instant and contains\na copy of all the units of the encoding network. Connections between layers depend on encoding network connectivity.\n2) a learning algorithm to adapt\nand\nusing examples\nfrom the training data set5;\n3) an implementation of\nand\n.\nThese aspects will be considered in turn in the following\nsections.\nB. Computation of the State\nBanach’s ﬁxed point theorem [53] does not only ensure the\nexistence and the uniqueness of the solution of (1) but it also\nsuggests the following classic iterative scheme for computing\nthe state:\n(4)\n5In other words, the parameters www are estimated using examples contained in\nthe training data set.\nwhere\ndenotes the th iteration of . The dynamical system\n(4) converges exponentially fast to the solution of (2) for any ini-\ntial value\n. We can, therefore, think of\nas the state that\nis updated by the transition function\n. In fact, (4) implements\nthe Jacobi iterative method for solving nonlinear equations [55].\nThus, the outputs and the states can be computed by iterating\n(5)\nNote that the computation described in (5) can be interpreted\nas the representation of a network consisting of units, which\ncompute\nand\n. Such a network will be called an encoding\nnetwork, following an analog terminology used for the recursive\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n",
    "66\nIEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 1, JANUARY 2009\nneural network model [17]. In order to build the encoding net-\nwork, each node of the graph is replaced by a unit computing the\nfunction\n(see Fig. 3). Each unit stores the current state\nof node\n, and, when activated, it calculates the state\nusing the node label and the information stored in the neigh-\nborhood. The simultaneous and repeated activation of the units\nproduce the behavior described in (5). The output of node\nis\nproduced by another unit, which implements\n.\nWhen\nand\nare implemented by feedforward neural net-\nworks, the encoding network turns out to be a recurrent neural\nnetwork where the connections between the neurons can be di-\nvided into internal and external connections. The internal con-\nnectivity is determined by the neural network architecture used\nto implement the unit. The external connectivity depends on the\nedges of the processed graph.\nC. The Learning Algorithm\nLearning in GNNs consists of estimating the parameter\nsuch that\napproximates the data in the learning data set\nwhere\nis the number of supervised nodes in\n. For graph-fo-\ncused tasks, one special node is used for the target (\nholds), whereas for node-focused tasks, in principle, the super-\nvision can be performed on every node. The learning task can\nbe posed as the minimization of a quadratic cost function\n(6)\nRemark 4: As common in neural network applications, the\ncost function may include a penalty term to control other prop-\nerties of the model. For example, the cost function may contain\na smoothing factor to penalize any abrupt changes of the outputs\nand to improve the generalization performance.\nThe learning algorithm is based on a gradient-descent\nstrategy and is composed of the following steps.\na) The states\nare iteratively updated by (5) until at time\nthey approach the ﬁxed point solution of (2):\n.\nb) The gradient\nis computed.\nc) The weights\nare updated according to the gradient com-\nputed in step b).\nConcerning step a), note that the hypothesis that\nis a\ncontraction map ensures the convergence to the ﬁxed point.\nStep c) is carried out within the traditional framework of gra-\ndient descent. As shown in the following, step b) can be carried\nout in a very efﬁcient way by exploiting the diffusion process\nthat takes place in GNNs. Interestingly, this diffusion process\nis very much related to the one which takes place in recurrent\nneural networks, for which the gradient computation is based\non backpropagation-through-time algorithm [17], [56], [57]. In\nthis case, the encoding network is unfolded from time\nback to\nan initial time\n. The unfolding produces the layered network\nshown in Fig. 3. Each layer corresponds to a time instant and\ncontains a copy of all the units\nof the encoding network. The\nunits of two consecutive layers are connected following graph\nconnectivity. The last layer corresponding to time\nincludes\nalso the units\nand computes the output of the network.\nBackpropagation through time consists of carrying out the\ntraditional backpropagation step on the unfolded network to\ncompute the gradient of the cost function at time\nwith respect\nto (w.r.t.) all the instances of\nand\n. Then,\nis\nobtained by summing the gradients of all instances. However,\nbackpropagation through time requires to store the states of\nevery instance of the units. When the graphs and\nare\nlarge, the memory required may be considerable.6 On the\nother hand, in our case, a more efﬁcient approach is possible,\nbased on the Almeida–Pineda algorithm [58], [59]. Since (5)\nhas reached a stable point\nbefore the gradient computation,\nwe can assume that\nholds for any\n. Thus,\nbackpropagation through time can be carried out by storing\nonly\n. The following two theorems show that such an intuitive\napproach has a formal justiﬁcation. The former theorem proves\nthat function\nis differentiable.\nTheorem 1 (Differentiability):\nLet\nand\nbe the\nglobal transition and the global output functions of a GNN,\nrespectively. If\nand\nare continuously differ-\nentiable w.r.t.\nand\n, then\nis continuously differentiable\nw.r.t.\n.\nProof: Let a function\nbe deﬁned as\nSuch a function is continuously differ-\nentiable w.r.t.\nand\n, since it is the difference of\ntwo continuously differentiable functions. Note that the\nJacobian\nmatrix\nof\nw.r.t.\nfulﬁlls\nwhere\nde-\nnotes the\n-dimensional identity matrix and\n,\nis\nthe dimension of the state. Since\nis a contraction map,\nthere exists\nsuch that\n,\nwhich implies\n. Thus, the de-\nterminant of\nis not null and we can apply the\nimplicit function theorem (see [60]) to\nand point\n. As\na consequence, there exists a function\n, which is deﬁned\nand continuously differentiable in a neighborhood of\n, such\nthat\nand\nSince this\nresult holds for any\n, it is demonstrated that\nis continu-\nously differentiable on the whole domain. Finally, note that\n, where\ndenotes the operator\nthat returns the components corresponding to node\n. Thus,\nis the composition of differentiable functions and hence is\nitself differentiable.\nIt is worth mentioning that this property does not hold for\ngeneral dynamical systems for which a slight change in the pa-\nrameters can force the transition from one ﬁxed point to another.\nThe fact that\nis differentiable in GNNs is due to the assump-\ntion that\nis a contraction map. The next theorem provides a\nmethod for an efﬁcient computation of the gradient.\nTheorem 2 (Backpropagation): Let\nand\nbe the tran-\nsition and the output functions of a GNN, respectively, and as-\nsume that\nand\nare continuously differen-\ntiable w.r.t.\nand\n. Let\nbe deﬁned by\n(7)\n6For internet applications, the graph may represent a signiﬁcant portion of\nthe web. This is an example of cases when the amount of the required memory\nstorage may play a very important role.\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n",
    "SCARSELLI et al.: THE GRAPH NEURAL NETWORK MODEL\n67\nThen, the sequence\nconverges to a vector\nand the convergence is exponential and in-\ndependent of the initial state\n. Moreover\n(8)\nholds, where\nis the stable state of the GNN.\nProof: Since\nis a contraction map, there exists\nsuch that\nholds. Thus,\n(7) converges to a stable ﬁxed point for each initial state. The\nstable ﬁxed point\nis the solution of (7) and satisﬁes\n(9)\nwhere\nholds. Moreover, let us consider again the\nfunction\ndeﬁned in the proof of Theorem 1. By the implicit\nfunction theorem\n(10)\nholds. On the other hand, since the error\ndepends on\nthe output of the network\n, the gra-\ndient\ncan be computed using the chain rule for\ndifferentiation\n(11)\nThe theorem follows by putting together (9)–(11)\nThe relationship between the gradient deﬁned by (8) and the\ngradient computed by the Almeida–Pineda algorithm can be\neasily recognized. The ﬁrst term on the right-hand side of (8)\nrepresents the contribution to the gradient due to the output func-\ntion\n. Backpropagation calculates the ﬁrst term while it is\npropagating the derivatives through the layer of the functions\n(see Fig. 3). The second term represents the contribution due to\nthe transition function\n. In fact, from (7)\nIf we assume\nand\n, for\n, it follows:\nTABLE I\nLEARNING ALGORITHM. THE FUNCTION FORWARD COMPUTES THE STATES,\nWHILE BACKWARD CALCULATES THE GRADIENT. THE PROCEDURE\nMAIN MINIMIZES THE ERROR BY CALLING ITERATIVELY\nFORWARD AND BACKWARD\nThus, (7) accumulates the\ninto the variable\n. This mechanism corresponds to backpropagate the gradients\nthrough the layers containing the\nunits.\nThe learning algorithm is detailed in Table I. It consists of a\nmain procedure and of two functions FORWARD and BACKWARD.\nFunction FORWARD takes as input the current set of parameters\nand iterates to ﬁnd the convergent point, i.e., the ﬁxed point.\nThe iteration is stopped when\nis less than\na given threshold\naccording to a given norm\n. Function\nBACKWARD computes the gradient: system (7) is iterated until\nis smaller than a threshold\n; then, the gradient\nis calculated by (8).\nThe main procedure updates the weights until the output\nreaches a desired accuracy or some other stopping criterion is\nachieved. In Table I, a predeﬁned learning rate\nis adopted,\nbut most of the common strategies based on the gradient-de-\nscent strategy can be used as well, for example, we can use a\nmomentum term and an adaptive learning rate scheme. In our\nGNN simulator, the weights are updated by the resilient back-\npropagation [61] strategy, which, according to the literature\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n",
    "68\nIEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 1, JANUARY 2009\non feedforward neural networks, is one of the most efﬁcient\nstrategies for this purpose. On the other hand, the design of\nlearning algorithms for GNNs that are not explicitly based on\ngradient is not obvious and it is a matter of future research.\nIn fact, the encoding network is only apparently similar to a\nstatic feedforward network, because the number of the layers\nis dynamically determined and the weights are partially shared\naccording to input graph topology. Thus, second-order learning\nalgorithms [62], pruning [63], and growing learning algorithms\n[64]–[66] designed for static networks cannot be directly\napplied to GNNs. Other implementation details along with\na computational cost analysis of the proposed algorithm are\nincluded in Section III.\nD. Transition and Output Function Implementations\nThe implementation of the local output function\ndoes not\nneed to fulﬁll any particular constraint. In GNNs,\nis a mul-\ntilayered feedforward neural network. On the other hand, the\nlocal transition function\nplays a crucial role in the proposed\nmodel, since its implementation determines the number and the\nexistence of the solutions of (1). The assumption behind GNN\nis that the design of\nis such that the global transition func-\ntion\nis a contraction map w.r.t. the state\n. In the following,\nwe describe two neural network models that fulﬁll this purpose\nusing different strategies. These models are based on the non-\npositional form described by (3). It can be easily observed that\nthere exist two corresponding models based on the positional\nform as well.\n1) Linear (nonpositional) GNN. Equation (3) can naturally be\nimplemented by\n(12)\nwhere the vector\nand the matrix\nare deﬁned by the output of two feedforward neural net-\nworks (FNNs), whose parameters correspond to the param-\neters of the GNN. More precisely, let us call transition net-\nwork an FNN that has to generate\nand forcing net-\nwork another FNN that has to generate\n. Moreover, let\nand\nbe the func-\ntions implemented by the transition and the forcing net-\nwork, respectively. Then, we deﬁne\n(13)\n(14)\nwhere\nand\nhold,\nand\ndenotes the operator that allocates the ele-\nments of a\n-dimensional vector into as\nmatrix. Thus,\nis obtained by arranging the outputs of the transition\nnetwork into the square matrix\nand by multiplication\nwith the factor\n. On the other hand,\nis just\na vector that contains the outputs of the forcing network.\nHere, it is further assumed that\nholds7; this can be straightforwardly veriﬁed if the output\nneurons of the transition network use an appropriately\n7The 1-norm of a matrix M\n=\nfm\ng is deﬁned as kMk\n=\nmax\njm\nj.\nbounded activation function, e.g., a hyperbolic tangent.\nNote that in this case\nwhere\nis the\nvector constructed by stacking all the\n, and\nis a block\nmatrix\n, with\nif\nis a neighbor of\nand\notherwise. Moreover, vectors\nand\nmatrices\ndo not depend on the state\n, but only on\nnode and edge labels. Thus,\n, and, by simple\nalgebra\nwhich implies that\nis a contraction map (w.r.t.\n)\nfor any set of parameters\n.\n2) Nonlinear (nonpositional) GNN. In this case,\nis real-\nized by a multilayered FNN. Since three-layered neural\nnetworks are universal approximators [67],\ncan approx-\nimate any desired function. However, not all the parameters\ncan be used, because it must be ensured that the corre-\nsponding transition function\nis a contraction map. This\ncan be achieved by adding a penalty term to (6), i.e.,\nwhere the penalty term\nis\nif\nand 0\notherwise, and the parameter\ndeﬁnes the desired\ncontraction constant of\n. More generally, the penalty\nterm can be any expression, differentiable w.r.t.\n, that\nis monotone increasing w.r.t. the norm of the Jacobian.\nFor example, in our experiments, we use the penalty term\n, where\nis the th column of\n. In fact, such an expression is an approximation\nof\n.\nE. A Comparison With Random Walks and\nRecursive Neural Networks\nGNNs turn out to be an extension of other models already pro-\nposed in the literature. In particular, recursive neural networks\n[17] are a special case of GNNs, where:\n1) the input graph is a directed acyclic graph;\n2) the inputs of\nare limited to\nand\n, where\nis the set of children of\n8;\n3) there is a supersource node\nfrom which all the other\nnodes can be reached. This node is typically used for output\n(graph-focused tasks).\nThe neural architectures, which have been suggested for real-\nizing\nand\n, include multilayered FNNs [17], [19], cascade\ncorrelation [68], and self-organizing maps [20], [69]. Note that\nthe above constraints on the processed graphs and on the inputs\nof\nexclude any sort of cyclic dependence of a state on itself.\nThus, in the recursive neural network model, the encoding net-\nworks are FNNs. This assumption simpliﬁes the computation of\n8A node u is child of n if there exists an arc from n to u. Obviously, ch[n] \u0012\nne[n] holds.\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n",
    "SCARSELLI et al.: THE GRAPH NEURAL NETWORK MODEL\n69\nTABLE II\nTIME COMPLEXITY OF THE MOST EXPENSIVE INSTRUCTIONS OF THE LEARNING ALGORITHM. FOR EACH INSTRUCTION AND EACH GNN MODEL,\nA BOUND ON THE ORDER OF FLOATING POINT OPERATIONS IS GIVEN. THE TABLE ALSO DISPLAYS\nTHE NUMBER OF TIMES PER EPOCH THAT EACH INSTRUCTION IS EXECUTED\nthe states. In fact, the states can be computed following a prede-\nﬁned ordering that is induced by the partial ordering of the input\ngraph.\nInterestingly, the GNN model captures also the random walks\non graphs when choosing\nas a linear function. Random walks\nand, more generally, Markov chain models are useful in several\napplication areas and have been recently used to develop ranking\nalgorithms for internet search engines [18], [21]. In random\nwalks on graphs, the state\nassociated with a node is a real\nvalue and is described by\n(15)\nwhere\nis the set of parents of\n, and\n,\nholds for each\n. The\nare normalized so that\n. In fact, (15) can represent a random walker\nwho is traveling on the graph. The value\nrepresents the\nprobability that the walker, when visiting node\n, decides to go\nto node . The state\nstands for the probability that the walker\nis on node\nin the steady state. When all\nare stacked into\na vector\n, (15) becomes\nwhere\nand\nis deﬁned as in (15) if\nand\notherwise. It is\neasily veriﬁed that\n. Markov chain theory suggests\nthat if there exists\nsuch that all the elements of the matrix\nare nonnull, then (15) is a contraction map [70]. Thus, provided\nthat the above condition on\nholds, random walks on graphs\nare an instance of GNNs, where\nis a constant stochastic\nmatrix instead of being generated by neural networks.\nIII. COMPUTATIONAL COMPLEXITY ISSUES\nIn this section, an accurate analysis of the computational cost\nwill be derived. The analysis will focus on three different GNN\nmodels: positional GNNs, where the functions\nand\nof (1)\nare implemented by FNNs; linear (nonpositional) GNNs; and\nnonlinear (nonpositional) GNNs.\nFirst, we will describe with more details the most complex\ninstructions involved in the learning procedure (see Table II).\nThen, the complexity of the learning algorithm will be deﬁned.\nFor the sake of simplicity, the cost is derived assuming that the\ntraining set contains just one graph\n. Such an assumption does\nnot cause any loss of generality, since the graphs of the training\nset can always be merged into a single graph. The complexity is\nmeasured by the order of ﬂoating point operations.9\nIn Table II, the notation\nis used to denote the number of\nhidden-layer neurons. For example,\nindicates the number of\nhidden-layer neurons in the implementation of function\n.\nIn the following,\n,\n, and\ndenote the number of epochs,\nthe mean number of forward iterations (of the repeat cycle in\nfunction FORWARD), and the mean number of backward itera-\ntions (of the repeat cycle in function BACKWARD), respectively.\nMoreover, we will assume that there exist two procedures\nand\n, which implement the forward phase and the backward\nphase of the backpropagation procedure [71], respectively. For-\nmally, given a function\nimplemented by an\nFNN, we have\nHere,\nis the input vector and the row vector\nis a\nsignal that suggests how the network output must be adjusted to\nimprove the cost function. In most applications, the cost func-\ntion is\nand\n),\nwhere\nand\nis the vector of the desired output cor-\nresponding to input\n. On the other hand,\nis the\ngradient of\nw.r.t. the network input and is easily computed\n9According to the common deﬁnition of time complexity, an algorithm re-\nquires O(l(a)) operations, if there exist \u000b > 0, \u0016a \u0015 0, such that c(a) \u0014 \u000b l(a)\nholds for each a \u0015 \u0016a, where c(a) is the maximal number of operations executed\nby the algorithm when the length of input is a.\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n",
    "70\nIEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 1, JANUARY 2009\nas a side product of backpropagation.10 Finally,\nand\nde-\nnote the computational complexity required by the application\nof\nand\non\n, respectively. For example, if\nis imple-\nmented by a multilayered FNN with\ninputs,\nhidden neurons,\nand\noutputs, then\nholds.\nA. Complexity of Instructions\n1) Instructions\n,\n, and\n: Since\nis a matrix having at most\nnonnull elements, the multiplication of\nby\n, and as\na consequence, the instruction\n, costs\nﬂoating points operations. Moreover, the state\nand the output vector\nare calculated by applying the local\ntransition function and the local output function to each node\n. Thus, in positional GNNs and in nonlinear GNNs, where\n,\n, and\nare directly implemented by FNNs,\nand\nare computed by running the forward phase of backpropagation\nonce for each node or edge (see Table II).\nOn the other hand, in linear GNNs,\nis calculated in\ntwo steps: the matrices\nof (13) and the vectors\n(14) are\nevaluated; then,\nis computed. The former phase, the cost\nof which is\n, is executed once for each\nepoch, whereas the latter phase, the cost of which is\n,\nis executed at every step of the cycle in the function FORWARD.\n2) Instruction\n: This instruction re-\nquires the computation of the Jacobian of\n. Note that\nis a block matrix where the block\nmeasures the\neffect of node\non node\n, if there is an arc\nfrom\nto\n, and is null otherwise. In the linear model, the matrices\ncorrespond to those displayed in (13) and used to calculate\nin the forward phase. Thus, such an instruction has no cost in\nthe backward phase in linear GNNs.\nIn nonlinear GNNs,\n,\nis computed by appropriately exploiting the backpropagation\nprocedure. More precisely, let\nbe a vector where all\nthe components are zero except for the th one, which equals\none, i.e.,\n,\n, and so on.\nNote that\n, when it is applied to\nwith\n, returns\n, i.e., the th column of the Jacobian\n. Thus,\ncan be computed by applying\non\nall the\n, i.e.,\n(16)\nwhere\nindicates that we are considering only the ﬁrst com-\nponent of the output of\n. A similar reasoning can also be used\nwith positional GNNs. The complexity of these procedures is\neasily derived and is displayed in the fourth row of Table II.\n3) Computation of\nand\n: In linear GNNs,\nthe cost function is\n, and, as a con-\nsequence,\n, if\nis a node belonging\nto the training set, and 0 otherwise. Thus,\nis easily cal-\nculated by\noperations.\n10Backpropagation\ncomputes\nfor\neach\nneuron\nv\nthe\ndelta\nvalue\n(@e =@a )(yyy) = \u000e(@l =@a )(yyy), where e\nis the cost function and a\nthe\nactivation level of neuron v. Thus, \u000e(@l =@yyy)(yyy) is just a vector stacking all\nthe delta values of the input neurons.\nIn positional and nonlinear GNNs, a penalty term\nis added\nto the cost function to force the transition function to be a con-\ntraction map. In this case, it is necessary to compute\n,\nbecause such a vector must be added to the gradient. Let\ndenote the element in position\nof the block\n. According\nto the deﬁnition of\n, we have\nwhere\n, if the sum is larger\nthan 0, and it is 0 otherwise. It follows:\nwhere\nis the sign function. Moreover, let\nbe a matrix\nwhose element in position\nis\nand let\nbe\nthe operator that takes a matrix and produce a column vector by\nstacking all its columns one on top of the other. Then\n(17)\nholds. The vector\ndepends on selected imple-\nmentation of\nor\n. For sake of simplicity, let us restrict our\nattention to nonlinear GNNs and assume that the transition net-\nwork is a three-layered FNN.\n,\n,\n, and\nare the activa-\ntion function11, the vector of the activation levels, the matrix of\nthe weights, and the thresholds of the th layer, respectively. The\nfollowing reasoning can also be extended to positional GNNs\nand networks with a different number of layers. The function\nis formally deﬁned in terms of\n,\n,\n, and\nBy the chain differentiation rule, it follows:\nwhere\nis the derivative of\n,\nis an operator that\ntransforms a vector into a diagonal matrix having such a vector\nas diagonal, and\nis the submatrix of\nthat contains only\nthe weights that connect the inputs corresponding to\nto\nthe hidden layer. The parameters\naffect four components of\n11\u001b\nis a vectorial function that takes as input the vector of the activation\nlevels of neurons in a layer and returns the vector of the outputs of the neurons\nof the same layer.\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n",
    "SCARSELLI et al.: THE GRAPH NEURAL NETWORK MODEL\n71\n, i.e.,\n,\n,\n, and\n. By properties of derivatives\nfor matrix products and the chain rule\n(18)\nholds.\nThus,\nis the sum of four con-\ntributions. In order to derive a method to compute those terms,\nlet\ndenote the\nidentity matrix. Let\nbe the Kro-\nnecker product and suppose that\nis a\nmatrix such\nthat\nfor any vector\n. By the Kro-\nnecker product properties,\nholds\nfor matrices\n,\n, and\nhaving compatible dimensions [72].\nThus, we have\nwhich implies\nSimilarly, using the properties\nand\n, it follows:\nwhere\nis the number of hidden neurons. Then, we have\n(19)\n(20)\n(21)\n(22)\nwhere the mentioned Kronecker product properties have been\nused.\nIt follows that\ncan be written\nas the sum of the four contributions represented by (19)–(22).\nThe second and the fourth term [(20) and (22)] can be computed\ndirectly using the corresponding formulas. The ﬁrst one can be\ncalculated by observing that\nlooks like the function com-\nputed by a three-layered FNN that is the same as\nexcept for\nthe activation function of the last layer. In fact, if we denote by\nsuch a network, then\n(23)\nholds, where\n. A sim-\nilar reasoning can be applied also to the third contribution.\nThe above described method includes two tasks: the matrix\nmultiplications of (19)–(22) and the backpropagation as deﬁned\nby (23). The former task consists of several matrix multiplica-\ntions. By inspection of (19)–(22), the number of ﬂoating point\noperations is approximately estimated as\n,12 where\ndenotes the number of hidden-layer neu-\nrons implementing the function\n. The second task has approx-\nimately the same cost as a backpropagation phase through the\noriginal function\n.\nThus, the complexity of computing\nis\n. Note, however, that even if the sum in (17)\nranges over all the arcs of the graph, only those arcs\nsuch\nthat\nhave to be considered. In practice,\nis a rare event, since it happens only when the columns of the\nJacobian are larger than\nand a penalty function was used\nto limit the occurrence of these cases. As a consequence, a\nbetter estimate of the complexity of computing\nis\n, where\nis the average number of\nnodes\nsuch that\nholds for some\n.\n4) Instructions\nand\n:\nThe terms\nand\ncan be\ncalculated by the backpropagation of\nthrough the\nnetwork that implements\n. Since such an operation must\nbe repeated for each node, the time complexity of instruc-\ntions\nand\nis\nfor all the GNN models.\n12Such a value is obtained by considering the following observations: for an\na \u0002 b matrix CCC and b \u0002 c matrix D\nD\nD, the multiplication CCCD\nD\nD requires approxi-\nmately 2abc operations; more precisely, abc multiplications and ac(b\u00021) sums.\nIf D\nD\nD is a diagonal b \u0002 b matrix, then CCCD\nD\nD requires 2ab operations. Moreover, if\nCCC is an a \u0002 b matrix, D\nD\nD is a b \u0002 a matrix, and PPP\nis the a \u0002 a matrix deﬁned\nabove and used in (19)–(22), then computing vec(CCCD\nD\nD)PPP\ncosts only 2ab op-\nerations provided that a sparse representation is used for P . Finally, aaa ; aaa ; aaa\nare already available, since they are computed during the forward phase of the\nlearning algorithm.\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n",
    "72\nIEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 1, JANUARY 2009\n5) Instruction\n: By deﬁnition of\n,\n, and\n, we have\n(24)\nwhere\nand\nindicates that we are\nconsidering only the ﬁrst part of the output of\n. Similarly\n(25)\nwhere\n. Thus, (24) and (25) provide a\ndirect method to compute\nin positional and nonlinear GNNs,\nrespectively.\nFor linear GNNs, let\ndenote the th output of\nand note\nthat\nholds. Here,\nand\nare the element in position\nof ma-\ntrix\nand the corresponding output of the transition network\n[see (13)], respectively, while\nis the th element of vector\n,\nis the corresponding output of the forcing network [see (14)],\nand\nis the th element of\n. Then\nwhere\n,\n,\n, and\nis\na vector that stores\nin the position corre-\nsponding to\n, that is,\n. Thus,\nin linear GNNs,\nis computed by calling the backpropagation\nprocedure on each arc and node.\nB. Time Complexity of the GNN Model\nAccording to our experiments, the application of a trained\nGNN on a graph (test phase) is relatively fast even for large\ngraphs. Formally, the complexity is easily derived from\nTable II and it is\nfor positional\nGNNs,\nfor nonlinear GNNs, and\nfor linear GNNs.\nIn practice, the cost of the test phase is mainly due to the\nrepeated computation of the state\n. The cost of each it-\neration is linear both w.r.t. the dimension of the input graph\n(the number of edges), the dimension of the employed FNNs\nand the state, with the only exception of linear GNNs, whose\nsingle iteration cost is quadratic w.r.t. to the state. The number\nof iterations required for the convergence of the state depends\non the problem at hand, but Banach’s theorem ensures that the\nconvergence is exponentially fast and experiments have shown\nthat 5–15 iterations are generally sufﬁcient to approximate the\nﬁxed point.\nIn positional and nonlinear GNNs, the transition function\nmust be activated\nand\ntimes, respectively. Even\nif such a difference may appear signiﬁcant, in practice, the\ncomplexity of the two models is similar, because the network\nthat implements the\nis larger than the one that implements\n. In fact,\nhas\ninput neurons, where\nis\nthe maximum number of neighbors for a node, whereas\nhas only\ninput neurons. An appreciable difference can\nbe noticed only for graphs where the number of neighbors\nof nodes is highly variable, since the inputs of\nmust be\nsufﬁcient to accommodate the maximal number of neighbors\nand many inputs may remain unused when\nis applied. On\nthe other hand, it is observed that in the linear model the FNNs\nare used only once for each iteration, so that the complexity\nof each iteration is\ninstead of\n. Note that\nholds, when\nis\nimplemented by a three-layered FNN with\nhidden neurons.\nIn practical cases, where\nis often larger than\n, the linear\nmodel is faster than the nonlinear model. As conﬁrmed by the\nexperiments, such an advantage is mitigated by the smaller\naccuracy that the model usually achieves.\nIn GNNs, the learning phase requires much more time than\nthe test phase, mainly due to the repetition of the forward and\nbackward phases for several epochs. The experiments have\nshown that the time spent in the forward and backward phases\nis not very different. Similarly to the forward phase, the cost\nof function BACKWARD is mainly due to the repetition of the\ninstruction that computes\n. Theorem 2 ensures that\nconverges exponentially fast and the experiments conﬁrmed\nthat\nis usually a small number.\nFormally, the cost of each learning epoch is given by the sum\nof all the instructions times the iterations in Table II. An inspec-\ntion of Table II shows that the cost of all instructions involved in\nthe learning phase are linear both with respect to the dimension\nof the input graph and of the FNNs. The only exceptions are due\nto the computation of\n,\nand\n, which depend quadratically on .\nThe most expensive instruction is apparently the computa-\ntion of\nin nonlinear GNNs, which costs\n. On the other hand, the experiments have shown that\nusually\nis a small number. In most epochs,\nis 0, since\nthe Jacobian does not violate the imposed constraint, and in\nthe other cases,\nis usually in the range 1–5. Thus, for a\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n",
    "SCARSELLI et al.: THE GRAPH NEURAL NETWORK MODEL\n73\nsmall state dimension\n, the computation of\nrequires\nfew applications of backpropagation on\nand has a small im-\npact on the global complexity of the learning process. On the\nother hand, in theory, if\nis very large, it might happen that\nand at the same time\n, causing the computation of the gradient to be very\nslow. However, it is worth mentioning that this case was never\nobserved in our experiments.\nIV. EXPERIMENTAL RESULTS\nIn this section, we present the experimental results, obtained\non a set of simple problems carried out to study the properties\nof the GNN model and to prove that the method can be ap-\nplied to relevant applications in relational domains. The prob-\nlems that we consider, viz., the subgraph matching, the mutage-\nnesis, and the web page ranking, have been selected since they\nare particularly suited to discover the properties of the model\nand are correlated to important real-world applications. From\na practical point of view, we will see that the results obtained\non some parts of mutagenesis data sets are among the best that\nare currently reported in the open literature (please see detailed\ncomparison in Section IV-B). Moreover, the subgraph matching\nproblem is relevant to several application domains. Even if the\nperformance of our method is not comparable in terms of best\naccuracy on the same problem with the most efﬁcient algorithms\nin the literature, the proposed approach is a very general tech-\nnique that can be applied on extension of the subgraph matching\nproblems [73]–[75]. Finally, the web page ranking is an inter-\nesting problem, since it is important in information retrieval and\nvery few techniques have been proposed for its solution [76]. It\nis worth mentioning that the GNN model has been already suc-\ncessfully applied on larger applications, which include image\nclassiﬁcation and object localization in images [77], [78], web\npage ranking [79], relational learning [80], and XML classiﬁca-\ntion [81].\nThe following facts hold for each experiment, unless other-\nwise speciﬁed. The experiments have been carried out with both\nlinear and nonlinear GNNs. According to existing results on re-\ncursive neural networks, the nonpositional transition function\nslightly outperforms the positional ones, hence, currently only\nnonpositional GNNs have been implemented and tested. Both\nthe (nonpositional) linear and the nonlinear model were tested.\nAll the functions involved in the two models, i.e.,\n,\n, and\nfor linear GNNs, and\nand\nfor nonlinear GNNs were\nimplemented by three-layered FNNs with sigmoidal activation\nfunctions. The presented results were averaged over ﬁve dif-\nferent runs. In each run, the data set was a collection of random\ngraphs constructed by the following procedure: each pair of\nnodes was connected with a certain probability ; the resulting\ngraph was checked to verify whether it was connected and if\nit was not, random edges were inserted until the condition was\nsatisﬁed.\nThe data set was split into a training set, a validation set, and\na test set and the validation set was used to avoid possible issues\nwith overﬁtting. For the problems where the original data is only\none single big graph\n, a training set, a validation set, and a test\nFig. 4. Two graphs GG\nG and GG\nG that contain a subgraph SSS. The numbers inside\nthe nodes represent the labels. The function \u001c to be learned is \u001c(GG\nG ; n\n) = 1,\nif n\nis a black node, and \u001c(GG\nG ; n\n) = \u00001, if n\nis a white node.\nset include different supervised nodes of\n. Otherwise, when\nseveral graphs were available, all the patterns of a graph\nwere\nassigned to only one set. In every trial, the training procedure\nperformed at most 5000 epochs and every 20 epochs the GNN\nwas evaluated on the validation set. The GNN that achieved the\nlowest cost on the validation set was considered the best model\nand was applied to the test set.\nThe performance of the model is measured by the accuracy\nin classiﬁcation problems (when\ncan take only the values\nor 1) and by the relative error in regression problems (when\nmay be any real number). More precisely, in a classiﬁ-\ncation problem, a pattern is considered correctly classiﬁed if\nand\nor if\nand\n. Thus, accuracy is deﬁned as the percentage of\npatterns correctly classiﬁed by the GNN on the test set. On\nthe other hand, in regression problems, the relative error on a\npattern is given by\n.\nThe algorithm was implemented in Matlab® 713 and the soft-\nware can be freely downloaded, together with the source and\nsome examples [82]. The experiments were carried out on a\nPower Mac G5 with a 2-GHz PowerPC processor.\nA. The Subgraph Matching Problem\nThe subgraph matching problem consists of ﬁnding the nodes\nof a given subgraph\nin a larger graph\n. More precisely, the\nfunction\nthat has to be learned is such that\nif\nbelongs to a subgraph of\n, which is isomorphic to\n, and\n, otherwise (see Fig. 4). Subgraph\nmatching has a number of practical applications, such as ob-\nject localization and detection of active parts in chemical com-\npounds [73]–[75]. This problem is a basic test to assess a method\nfor graph processing. The experiments will demonstrate that\nthe GNN model can cope with the given task. Of course, the\npresented results cannot be compared with those achievable by\nother speciﬁc methods for subgraph matching, which are faster\nand more accurate. On the other hand, the GNN model is a gen-\neral approach and can be used without any modiﬁcation to a\nvariety of extensions of the subgraph matching problem, where,\nfor example, several graphs must be detected at the same time,\nthe graphs are corrupted by noise on the structure and the labels,\n13Copyright © 1994–2006 by The MathWorks, Inc., Natick, MA.\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n",
    "74\nIEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 1, JANUARY 2009\nTABLE III\nACCURACIES ACHIEVED BY NONLINEAR MODEL (NL), LINEAR MODEL\n(L), AND A FEEDFORWARD NEURAL NETWORK\nON SUBGRAPH MATCHING PROBLEM\nand the target to be detected is unknown and provided only by\nexamples.\nIn our experiments, the data set\nconsisted of 600 connected\nrandom graphs (constructed using\n), equally divided into\na training set, a validation set, and a test set. A smaller subgraph\n, which was randomly generated in each trial, was inserted into\nevery graph of the data set. Thus, each graph\ncontained at\nleast a copy of\n, even if more copies might have been included\nby the random construction procedure. All the nodes had integer\nlabels in the range\nand, in order to deﬁne the correct tar-\ngets\n, a brute force algorithm located all the\ncopies of\nin\n. Finally, a small Gaussian noise, with zero\nmean and a standard deviation of 0.25, was added to all the la-\nbels. As a consequence, all the copies of\nin our data set were\ndifferent due to the introduced noise.\nIn all the experiments, the state dimension was\nand all\nthe neural networks involved in the GNNs had ﬁve hidden neu-\nrons. More network architectures have been tested with similar\nresults.\nIn order to evaluate the relative importance of the labels and\nthe connectivity in the subgraph localization, also a feedforward\nneural network was applied to this test. The FNN had one output,\n20 hidden, and one input units. The FNN predicted\nusing\nonly the label\nof node\n. Thus, the FNN did not use the\nconnectivity and exploited only the relative distribution of the\nlabels in\nw.r.t. the labels in graphs\n.\nTable III presents the accuracies achieved by the nonlinear\nGNN model (nonlinear), the linear GNN model (linear), and the\nFNN with several dimensions for\nand\n. The results allow to\nsingle out some of the factors that have inﬂuence on the com-\nplexity of the problem and on the performance of the models.\nObviously, the proportion of positive and negative patterns af-\nfects the performance of all the methods. The results improve\nwhen\nis close to\n, whereas when\nis about a half of\n, the performance is lower. In fact, in the latter case, the data\nset is perfectly balanced and it is more difﬁcult to guess the right\nresponse. Moreover, the dimension\n, by itself, has inﬂuence\non the performance, because the labels can assume only 11 dif-\nferent values and when\nis small most of the nodes of the sub-\ngraph can be identiﬁed by their labels. In fact, the performances\nare better for smaller\n, even if we restrict our attention to the\ncases when\nholds.\nThe results show that GNNs always outperform the FNNs,\nconﬁrming that the GNNs can exploit label contents and graph\ntopology at the same time. Moreover, the nonlinear GNN model\nachieved a slightly better performance than the linear one, prob-\nably because nonlinear GNNs implement a more general model\nthat can approximate a larger class of functions. Finally, it can\nbe observed that the total average error for FNNs is about 50%\nlarger than the GNN error (12.7 for nonlinear GNNs, 13.5 for\nlinear GNNs, and 22.8 for FNNs). Actually, the relative differ-\nence between the GNN and FNN errors, which measures the\nadvantage provided by the topology, tend to become smaller\nfor larger values of\n(see the last column of Table III). In\nfact, GNNs use an information diffusion mechanism to decide\nwhether a node belongs to the subgraph. When\nis larger, more\ninformation has to be diffused and, as a consequence, the func-\ntion to be learned is more complex.\nThe subgraph matching problem was used also to evaluate the\nperformance of the GNN model and to experimentally verify the\nﬁndings about the computational cost of the model described\nin Section III. For this purpose, some experiments have been\ncarried out varying the number of nodes, the number of edges\nin the data set, the number of hidden units in the neural networks\nimplementing the GNN, and the dimensionality of the state. In\nthe base case, the training set contained ten random graphs, each\none made of 20 nodes and 40 edges, the networks implementing\nthe GNN had ﬁve hidden neurons, and the state dimension was\n2. The GNN was trained for 1000 epochs and the results were\naveraged over ten trials. As expected, the central processing unit\n(CPU) time required by the gradient computation grows linearly\nw.r.t. the number of nodes, edges and hidden units, whereas\nthe growth is quadratic w.r.t. the state dimension. For example,\nFig. 5 depicts the CPU time spent by the gradient computation\nprocess when the nodes of each graph14 [Fig. 5(a)] and the states\nof the GNN [Fig. 5(b)] are increased, respectively.\nIt is worth mentioning that, in nonlinear GNNs, the\nquadratic growth w.r.t. the states, according to the discus-\nsion of Section III, depends on the time spent to calculate the\nJacobian\nand its derivative\n. Fig. 5\nshows how the total time spent by the gradient computation\nprocess is composed in this case: line\ndenotes the time\nrequired by the computation of\nand\n; line\nde-\nnotes that for the Jacobian\n; line\ndenotes\nthat for the derivative\n; the dotted line and the dashed\nline represent the rest of the time15 required by the FORWARD\nand the BACKWARD procedure, respectively; the continuous\nline stands for the rest of the time required by the gradient\ncomputation process.\n14More precisely, in this experiment, nodes and edges were increased keeping\nconstant to 1=2 their ratio.\n15That is, the time required by those procedures except for that already con-\nsidered in the previous points.\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n",
    "SCARSELLI et al.: THE GRAPH NEURAL NETWORK MODEL\n75\nFig. 5. Some plots about the cost of the gradient computation on GNNs. (a) and (b) CPU times required for 1000 learning epochs by nonlinear GNNs (continuous\nline) and linear GNN (dashed line), respectively, as a function of the number of nodes of the training set (a) and the dimension of the state (b). (c) Composition\nof the learning time for nonlinear GNNs: the computation of e\nand @e =@www (\u0000o\u0000); the Jacobian (@F =@xxx)(xxx; lll) (\u0000 \u0003 \u0002); the derivative @p =@w (\u0002x\u0002);\nthe rest of the FORWARD procedure (dotted line); the rest of the BACKWARD procedure (dashed line); the rest of the time learning procedure (continuous line).\n(d) Histogram of the number of the forward iterations, the backward iterations, and the number of nodes u such that RR\nR\n6= 0 [see (17)] encountered in each\nepoch of a learning session.\nFrom Fig. 5(c), we can observe that the computation of\nthat, in theory, is quadratic w.r.t. the states may have a\nsmall effect in practice. In fact, as already noticed in Section III,\nthe cost of such a computation depends on the number\nof\ncolumns of\nwhose norm is larger than the\nprescribed threshold, i.e., the number of nodes\nand\nsuch\nthat\n[see (17)]. Such a number is usually small due to\nthe effect of the penalty term\n. Fig. 5(d) shows a histogram\nof the number of nodes\nfor which\nin each epoch\nof a learning session: in practice, in this experiment, the non-\nnull\nare often zero and never exceed four in magnitude.\nAnother factor that affects the learning time is the number of\nforward and backward iterations needed to compute the stable\nstate and the gradient, respectively.16 Fig. 5(d) shows also the\n16The number of iterations depends also on the constant \u000f and \u000f of Table I,\nwhich were both set to 1e\u00023 in the experiments. However, due to the exponen-\ntial convergence of the iterative methods, these constants have a linear effect.\nhistograms of the number of required iterations, suggesting that\nalso those numbers are often small.\nB. The Mutagenesis Problem\nThe Mutagenesis data set [13] is a small data set, which is\navailable online and is often used as a benchmark in the re-\nlational learning and inductive logic programming literature.\nIt contains the descriptions of 230 nitroaromatic compounds\nthat are common intermediate subproducts of many industrial\nchemical reactions [83]. The goal of the benchmark consists of\nlearning to recognize the mutagenic compounds. The log mu-\ntagenicity was thresholded at zero, so the prediction is a bi-\nnary classiﬁcation problem. We will demonstrate that GNNs\nachieved the best result compared with those reported in the lit-\nerature on some parts of the data set.\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n",
    "76\nIEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 1, JANUARY 2009\nFig. 6. Atom-bond structure of a molecule represented by a graph with labeled\nnodes. Nodes represent atoms and edges denote atom bonds. Only one node is\nsupervised.\nIn [83], it is shown that 188 molecules out of 230 are\namenable to a linear regression analysis. This subset was called\n“regression friendly,” while the remaining 42 compounds were\ntermed “regression unfriendly.” Many different features have\nbeen used in the prediction. Apart from the atom-bond (AB)\nstructure, each compound is provided with four global features\n[83]. The ﬁrst two features are chemical measurements (C):\nthe lowest unoccupied molecule orbital and the water/octanol\npartition coefﬁcient, while the remaining two are precoded\nstructural (PS) attributes. Finally, the AB description can be\nused to deﬁne functional groups (FG), e.g., methyl groups and\nmany different rings that can be used as higher level features.\nIn our experiments, the best results were achieved using AB,\nC, and PS, without the functional groups. Probably the reason\nis that GNNs can recover the substructures that are relevant to\nthe classiﬁcation, exploiting the graphical structure contained\nin the AB description.\nIn our experiments, each molecule of the data set was trans-\nformed into a graph where nodes represent atoms and edges\nstand for ABs. The average number of nodes in a molecule is\naround 26. Node labels contain atom type, its energy state, and\nthe global properties AB, C, and PS. In each graph, there is\nonly one supervised node, the ﬁrst atom in the AB description\n(Fig. 6). The desired output is 1, if the molecule is mutagenic,\nand\n1, otherwise.\nIn Tables IV–VI, the results obtained by nonlinear GNNs17\nare compared with those achieved by other methods. The pre-\nsented results were evaluated using a tenfold cross-validation\nprocedure, i.e., the data set was randomly split into ten parts and\nthe experiments were repeated ten times, each time using a dif-\nferent part as the test set and the remaining patterns as training\nset. The results were averaged on ﬁve runs of the cross-valida-\ntion procedure.\nGNNs achieved the best accuracy on the regression-un-\nfriendly part (Table V) and on the whole data set (Table VI),\nwhile the results are close to the state of the art techniques\non the regression-friendly part (Table IV). It is worth noticing\nthat, whereas most of the approaches showed a higher level of\naccuracy when applied to the whole data set with respect to the\n17Some results were already presented in [80].\nTABLE IV\nACCURACIES ACHIEVED ON THE REGRESSION-FRIENDLY PART OF THE\nMUTAGENESIS DATA SET. THE TABLE DISPLAYS THE METHOD, THE\nFEATURES USED TO MAKE THE PREDICTION, AND A POSSIBLE\nREFERENCE TO THE PAPER WHERE THE RESULT IS DESCRIBED\nTABLE V\nACCURACIES ACHIEVED ON THE REGRESSION-UNFRIENDLY PART OF THE\nMUTAGENESIS DATA SET. THE TABLE DISPLAYS THE METHOD, THE\nFEATURES USED TO MAKE THE PREDICTION, AND A POSSIBLE\nREFERENCE TO THE PAPER WHERE THE RESULT IS DESCRIBED\nTABLE VI\nACCURACIES ACHIEVED ON THE WHOLE MUTAGENESIS DATA SET. THE TABLE\nDISPLAYS THE METHOD, THE FEATURES USED TO MAKE THE PREDICTION, AND\nA POSSIBLE REFERENCE TO THE PAPER WHERE THE RESULT IS DESCRIBED\nunfriendly part, the converse holds for GNNs. This suggests that\nGNNs can capture characteristics of the patterns that are useful\nto solve the problem but are not homogeneously distributed in\nthe two parts.\nC. Web Page Ranking\nIn this experiment, the goal is to learn the rank of a web\npage, inspired by Google’s PageRank [18]. According to\nPageRank, a page is considered authoritative if it is referred\nby many other pages and if the referring pages are authori-\ntative themselves. Formally, the PageRank\nof a page\nis\nwhere\nis the outdegree\nof\n, and\nis the damping factor [18]. In this experi-\nment, it is shown that a GNN can learn a modiﬁed version of\nPageRank, which adapts the “authority” measure according\nto the page content. For this purpose, a random web graph\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n",
    "SCARSELLI et al.: THE GRAPH NEURAL NETWORK MODEL\n77\nFig. 7. Desired function \u001c (the continuous lines) and the output of the GNN (the dotted lines) on the pages that belong to only one topic (a) and on the other pages\n(b). Horizontal axis stands for pages and vertical axis stands for scores. Pages have been sorted according to the desired value \u001c(GG\nG; n).\ncontaining 5000 nodes was generated, with\n. Training,\nvalidation, and test sets consisted of different nodes of this\ngraph. More precisely, only 50 nodes were supervised in the\ntraining set, other 50 nodes belonged to the validation set, and\nthe remaining nodes were in the test set.\nTo each node\n, a bidimensional boolean label\nis at-\ntached that represents whether the page belongs to two given\ntopics. If the page\nbelongs to both topics, then\n, while if it belongs to only one topic, then\n,\nor\n, and if it does not belong to either topics, then\n. The GNN was trained in order to produce the\nfollowing output:\nif\notherwise\nwhere\nstands for the Google’s PageRank.\nWeb page ranking algorithms are used by search engines to\nsort the URLs returned in response to user’s queries and more\ngenerally to evaluate the data returned by information retrieval\nsystems. The design of ranking algorithms capable of mixing to-\ngether the information provided by web connectivity and page\ncontent has been a matter of recent research [93]–[96]. In gen-\neral, this is an interesting and hard problem due to the difﬁculty\nin coping with structured information and large data sets. Here,\nwe present the results obtained by GNNs on a synthetic data set.\nMore results achieved on a snapshot of the web are available in\n[79].\nFor this example, only the linear model has been used, be-\ncause it is naturally suited to approximate the linear dynamics\nof the PageRank. Moreover, the transition and forcing networks\n(see Section I) were implemented by three-layered neural net-\nworks with ﬁve hidden neurons, and the dimension of the state\nwas\n. For the output function,\nis implemented as\nwhere\nis the function realized\nby a three-layered neural networks with ﬁve hidden neurons.\nFig. 7 shows the output of the GNN\nand the target function\non the test set. Fig. 7(a) displays the result for the pages that\nbelong to only one topic and Fig. 7(b) displays the result for\nthe other pages. Pages are displayed on horizontal axes and are\nsorted according to the desired output\n. The plots denote\nFig. 8. Error function on the training set (continuous line) and on the validation\n(dashed line) set during learning phase.\nthe value of function\n(continuous lines) and the value of the\nfunction implemented by the GNN (the dotted lines). The ﬁgure\nclearly suggests that GNN performs very well on this problem.\nFinally, Fig. 8 displays the error function during the learning\nprocess. The continuous line is the error on the training set,\nwhereas the dotted line is the error on the validation set. It\nis worth noting that the two curves are always very close and\nthat the error on the validation set is still decreasing after 2400\nepochs. This suggests that the GNN does not experiment over-\nﬁtting problems, despite the fact that the learning set consists of\nonly 50 pages from a graph containing 5000 nodes.\nV. CONCLUSION\nIn this paper, we introduced a novel neural network model\nthat can handle graph inputs: the graphs can be cyclic, directed,\nundirected, or a mixture of these. The model is based on in-\nformation diffusion and relaxation mechanisms. The approach\nextends into a common framework, the previous connectionist\ntechniques for processing structured data, and the methods\nbased on random walk models. A learning algorithm to esti-\nmate model parameters was provided and its computational\ncomplexity was studied, demonstrating that the method is\nsuitable also for large data sets.\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n",
    "78\nIEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 1, JANUARY 2009\nSome promising experimental results were provided to assess\nthe model. In particular, the results achieved on the whole Mu-\ntagenisis data set and on the unfriendly part of such a data set\nare the best compared with those reported in the open literature.\nMoreover, the experiments on the subgraph matching and on the\nweb page ranking show that the method can be applied to prob-\nlems that are related to important practical applications.\nThe possibility of dealing with domains where the data con-\nsists of patterns and relationships gives rise to several new topics\nof research. For example, while in this paper it is assumed that\nthe domain is static, it may happen that the input graphs change\nwith time. In this case, at least two interesting issues can be\nconsidered: ﬁrst, GNNs must be extended to cope with a dy-\nnamic domain; and second, no method exists, to the best of our\nknowledge, to model the evolution of the domain. The solution\nof the latter problem, for instance, may allow to model the evolu-\ntion of the web and, more generally, of social networks. Another\ntopic of future research is the study on how to deal with domains\nwhere the relationships, which are not known in advance, must\nbe inferred. In this case, the input contains ﬂat data and is auto-\nmatically transformed into a set of graphs in order to shed some\nlight on possible hidden relationships.\nREFERENCES\n[1] P. Baldi and G. Pollastri, “The principled design of large-scale recur-\nsive neural network architectures-dag-RNNs and the protein structure\nprediction problem,” J. Mach. Learn. Res., vol. 4, pp. 575–602, 2003.\n[2] E. Francesconi, P. Frasconi, M. Gori, S. Marinai, J. Sheng, G. Soda, and\nA. Sperduti, “Logo recognition by recursive neural networks,” in Lec-\nture Notes in Computer Science — Graphics Recognition, K. Tombre\nand A. K. Chhabra, Eds.\nBerlin, Germany: Springer-Verlag, 1997.\n[3] E. Krahmer, S. Erk, and A. Verleg, “Graph-based generation of refer-\nring expressions,” Comput. Linguist., vol. 29, no. 1, pp. 53–72, 2003.\n[4] A. Mason and E. Blake, “A graphical representation of the state spaces\nof hierarchical level-of-detail scene descriptions,” IEEE Trans. Vis.\nComput. Graphics, vol. 7, no. 1, pp. 70–75, Jan.-Mar. 2001.\n[5] L. Baresi and R. Heckel, “Tutorial introduction to graph transforma-\ntion: A software engineering perspective,” in Lecture Notes in Com-\nputer Science.\nBerlin, Germany: Springer-Verlag, 2002, vol. 2505,\npp. 402–429.\n[6] C. Collberg, S. Kobourov, J. Nagra, J. Pitts, and K. Wampler, “A system\nfor graph-based visualization of the evolution of software,” in Proc.\nACM Symp. Software Vis., 2003, pp. 77–86.\n[7] A. Bua, M. Gori, and F. Santini, “Recursive neural networks applied\nto discourse representation theory,” in Lecture Notes in Computer Sci-\nence.\nBerlin, Germany: Springer-Verlag, 2002, vol. 2415.\n[8] L. De Raedt, Logical and Relational Learning.\nNew York: Springer-\nVerlag, 2008, to be published.\n[9] T. Dietterich, L. Getoor, and K. Murphy, Eds., Proc. Int. Workshop\nStatist. Relat. Learn. Connect. Other Fields, 2004.\n[10] P. Avesani and M. Gori, Eds., Proc. Int. Workshop Sub-Symbol.\nParadigms Structured Domains, 2005.\n[11] S. Nijseen, Ed., Proc. 3rd Int. Workshop Mining Graphs Trees Se-\nquences, 2005.\n[12] T. Gaertner, G. Garriga, and T. Meini, Eds., Proc. 4th Int. Workshop\nMining Graphs Trees Sequences, 2006.\n[13] A. Srinivasan, S. Muggleton, R. King, and M. Sternberg, “Mutagenesis:\nIlp experiments in a non-determinate biological domain,” in Proc. 4th\nInt. Workshop Inductive Logic Programm., 1994, pp. 217–232.\n[14] T. Pavlidis, Structural Pattern Recognition, ser. Electrophysics.\nNew\nYork: Springer-Verlag, 1977.\n[15] M. Bianchini, M. Maggini, L. Sarti, and F. Scarselli, “Recursive neural\nnetworks learn to localize faces,” Phys. Rev. Lett., vol. 26, no. 12, pp.\n1885–1895, Sep. 2005.\n[16] S. Haykin, Neural Networks: A Comprehensive Foundation.\nNew\nYork: Prentice-Hall, 1994.\n[17] P. Frasconi, M. Gori, and A. Sperduti, “A general framework for adap-\ntive processing of data structures,” IEEE Trans. Neural Netw., vol. 9,\nno. 5, pp. 768–786, Sep. 1998.\n[18] S. Brin and L. Page, “The anatomy of a large-scale hypertextual web\nsearch engine,” in Proc. 7th World Wide Web Conf., Apr. 1998, pp.\n107–117.\n[19] A. Sperduti and A. Starita, “Supervised neural networks for the clas-\nsiﬁcation of structures,” IEEE Trans. Neural Netw., vol. 8, no. 2, pp.\n429–459, Mar. 1997.\n[20] M. Hagenbuchner, A. Sperduti, and A. C. Tsoi, “A self-organizing map\nfor adaptive processing of structured data,” IEEE Trans. Neural Netw.,\nvol. 14, no. 3, pp. 491–505, May 2003.\n[21] J. Kleinberg, “Authoritative sources in a hyperlinked environment,” J.\nACM, vol. 46, no. 5, pp. 604–632, 1999.\n[22] A. C. Tsoi, G. Morini, F. Scarselli, M. Hagenbuchner, and M. Maggini,\n“Adaptive ranking of web pages,” in Proc. 12th World Wide Web Conf.,\nBudapest, Hungary, May 2003, pp. 356–365.\n[23] M. Bianchini, P. Mazzoni, L. Sarti, and F. Scarselli, “Face spotting in\ncolor images using recursive neural networks,” in Proc. 1st Int. Work-\nshop Artif. Neural Netw. Pattern Recognit., Florence, Italy, Sep. 2003,\npp. 76–81.\n[24] M. Bianchini, M. Gori, and F. Scarselli, “Processing directed acyclic\ngraphs with recursive neural networks,” IEEE Trans. Neural Netw., vol.\n12, no. 6, pp. 1464–1470, Nov. 2001.\n[25] A. Küchler and C. Goller, “Inductive learning in symbolic domains\nusing structure-driven recurrent neural networks,” in Lecture Notes in\nComputer Science, G. Görz and S. Hölldobler, Eds.\nBerlin, Germany:\nSpringer-Verlag, Sep. 1996, vol. 1137.\n[26] T. Schmitt and C. Goller, “Relating chemical structure to activity:\nAn application of the neural folding architecture,” in Proc. Workshop\nFuzzy-Neuro Syst./Conf. Eng. Appl. Neural Netw., 1998, pp. 170–177.\n[27] M. Hagenbuchner and A. C. Tsoi, “A supervised training algorithm\nfor self-organizing maps for structures,” Pattern Recognit. Lett., vol.\n26, no. 12, pp. 1874–1884, 2005.\n[28] M. Gori, M. Maggini, E. Martinelli, and F. Scarselli, “Learning user\nproﬁles in NAUTILUS,” in Proc. Int. Conf. Adaptive Hypermedia\nAdaptive Web-Based Syst., Trento, Italy, Aug. 2000, pp. 323–326.\n[29] M. Bianchini, P. Mazzoni, L. Sarti, and F. Scarselli, “Face spotting in\ncolor images using recursive neural networks,” in Proc. Italian Work-\nshop Neural Netw., Vietri sul Mare, Italy, Jul. 2003.\n[30] B. Hammer and J. Jain, “Neural methods for non-standard data,” in\nProc. 12th Eur. Symp. Artif. Neural Netw., M. Verleysen, Ed., 2004,\npp. 281–292.\n[31] T. Gärtner, “Kernel-based learning in multi-relational data mining,”\nACM SIGKDD Explorations, vol. 5, no. 1, pp. 49–58, 2003.\n[32] T. Gärtner, J. Lloyd, and P. Flach, “Kernels and distances for structured\ndata,” Mach. Learn., vol. 57, no. 3, pp. 205–232, 2004.\n[33] R. Kondor and J. Lafferty, “Diffusion kernels on graphs and other dis-\ncrete structures,” in Proc. 19th Int. Conf. Mach. Learn., C. Sammut and\nA. e. Hoffmann, Eds., 2002, pp. 315–322.\n[34] H. Kashima, K. Tsuda, and A. Inokuchi, “Marginalized kernels\nbetween labeled graphs,” in Proc. 20th Int. Conf. Mach. Learn., T.\nFawcett and N. e. Mishra, Eds., 2003, pp. 321–328.\n[35] P. Mahé, N. Ueda, T. Akutsu, J.-L Perret, and J.-P. Vert, “Extensions\nof marginalized graph kernels,” in Proc. 21st Int. Conf. Mach. Learn.,\n2004, pp. 552–559.\n[36] M. Collins and N. Duffy, “Convolution kernels for natural language,”\nin Advances in Neural Information Processing Systems, T. G. Diet-\nterich, S. Becker, and Z. Ghahramani, Eds.\nCambridge, MA: MIT\nPress, 2002, vol. 14, pp. 625–632.\n[37] J. Suzuki, Y. Sasaki, and E. Maeda, “Kernels for structured natural\nlanguage data,” in Proc. Conf. Neural Inf. Process. Syst., 2003.\n[38] J. Suzuki, H. Isozaki, and E. Maeda, “Convolution kernels with fea-\nture selection for natural language processing tasks,” in Proc. Annu.\nMeeting Assoc. Comput. Linguistics, 2004, pp. 119–126.\n[39] J. Cho, H. Garcia-Molina, and L. Page, “Efﬁcient crawling through url\nordering,” in Proc. 7th World Wide Web Conf., Brisbane, Australia, Apr.\n1998, pp. 161–172.\n[40] A. C. Tsoi, M. Hagenbuchner, and F. Scarselli, “Computing cus-\ntomized page ranks,” ACM Trans. Internet Technol., vol. 6, no. 4, pp.\n381–414, Nov. 2006.\n[41] H. Chang, D. Cohn, and A. K. McCallum, “Learning to create cus-\ntomized authority lists,” in Proc. 17th Int. Conf. Mach. Learn., 2000,\npp. 127–134.\n[42] J. Lafferty, A. McCallum, and F. Pereira, “Conditional random ﬁelds:\nProbabilistic models for segmenting and labeling sequence data,” in\nProc. 18th Int. Conf. Mach. Learn., 2001, pp. 282–289.\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n",
    "SCARSELLI et al.: THE GRAPH NEURAL NETWORK MODEL\n79\n[43] F. V. Jensen, Introduction to Bayesian Networks.\nNew York:\nSpringer-Verlag, 1996.\n[44] L. Getoor and B. Taskar, Introduction to Statistical Relational\nLearning.\nCambridge, MA: MIT Press, 2007.\n[45] V. N. Vapnik, Statistical Learning Theory.\nNew York: Wiley, 1998.\n[46] O. Chapelle, B. Schölkopf, and A. Zien, Eds., Semi-Supervised\nLearning.\nCambridge, MA: MIT Press, 2006.\n[47] L. Chua and L. Yang, “Cellular neural networks: Theory,” IEEE Trans.\nCircuits Syst., vol. CAS-35, no. 10, pp. 1257–1272, Oct. 1988.\n[48] L. Chua and L. Yang, “Cellular neural networks: Applications,” IEEE\nTrans. Circuits Syst., vol. CAS-35, no. 10, pp. 1273–1290, Oct. 1988.\n[49] P. Kaluzny, “Counting stable equilibria of cellular neural networks-A\ngraph theoretic approach,” in Proc. Int. Workshop Cellular Neural\nNetw. Appl., 1992, pp. 112–116.\n[50] M. Ogorzatek, C. Merkwirth, and J. Wichard, “Pattern recognition\nusing ﬁnite-iteration cellular systems,” in Proc. 9th Int. Workshop\nCellular Neural Netw. Appl., 2005, pp. 57–60.\n[51] J. Hopﬁeld, “Neural networks and physical systems with emergent\ncollective computational abilities,” Proc. Nat. Acad. Sci., vol. 79, pp.\n2554–2558, 1982.\n[52] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfar-\ndini, “Computation capabilities of graph neural networks,” IEEE Trans.\nNeural Netw., vol. 20, no. 1, Jan. 2009, to be published.\n[53] M. A. Khamsi, An Introduction to Metric Spaces and Fixed Point\nTheory.\nNew York: Wiley, 2001.\n[54] M. Bianchini, M. Maggini, L. Sarti, and F. Scarselli, “Recursive neural\nnetworks for processing graphs with labelled edges: Theory and appli-\ncations,” Neural Netw., vol. 18, Special Issue on Neural Networks and\nKernel Methods for Structured Domains, no. 8, pp. 1040–1050, 2005.\n[55] M. J. D. Powell, “An efﬁcient method for ﬁnding the minimum of a\nfunction of several variables without calculating derivatives,” Comput.\nJ., vol. 7, pp. 155–162, 1964.\n[56] W. T. Miller, III, R. Sutton, and P. E. Werbos, Neural Network for\nControl.\nCamrbidge, MA: MIT Press, 1990.\n[57] A. C. Tsoi, “Adaptive processing of sequences and data structures,” in\nLecture Notes in Computer Science, C. L. Giles and M. Gori, Eds.\nBerlin, Germany: Springer-Verlag, 1998, vol. 1387, pp. 27–62.\n[58] L. Almeida, “A learning rule for asynchronous perceptrons with feed-\nback in a combinatorial environment,” in Proc. IEEE Int. Conf. Neural\nNetw., M. Caudill and C. Butler, Eds., San Diego, 1987, vol. 2, pp.\n609–618.\n[59] F. Pineda, “Generalization of back-propagation to recurrent neural net-\nworks,” Phys. Rev. Lett., vol. 59, pp. 2229–2232, 1987.\n[60] W. Rudin, Real and Complex Analysis, 3rd ed.\nNew York: McGraw-\nHill, 1987.\n[61] M. Riedmiller and H. Braun, “A direct adaptive method for faster back-\npropagation learning: The rprop algorithm,” in Proc. IEEE Int. Conf.\nNeural Netw., San Francisco, CA, 1993, pp. 586–591.\n[62] M. Bishop, Neural Networks for Pattern Recognition.\nOxford, U.K.:\nOxford Univ. Press, 1995.\n[63] R. Reed, “Pruning algorithms — A survey,” IEEE Trans. Neural Netw.,\nvol. 4, no. 5, pp. 740–747, Sep. 1993.\n[64] S. Fahlman and C. Lebiere, “The cascade-correlation learning archi-\ntecture,” in Advances in Neural Information Processing Systems, D.\nTouretzky, Ed.\nDenver, San Mateo: Morgan Kaufmann, 1989, vol. 2,\npp. 524–532.\n[65] T.-Y. Kwok and D.-Y. Yeung, “Constructive algorithms for structure\nlearning in feedforward neural networks for regression problems,”\nIEEE Trans. Neural Netw., vol. 8, no. 3, pp. 630–645, May 1997.\n[66] G.-B. Huang, L. Chen, and C. K. Siew, “Universal approximation using\nincremental constructive feedforward networks with random hidden\nnodes,” IEEE Trans. Neural Netw., vol. 17, no. 1, pp. 879–892, Jan.\n2006.\n[67] F. Scarselli and A. C. Tsoi, “Universal approximation using feedfor-\nward neural networks: A survey of some existing methods, and some\nnew results,” Neural Netw., vol. 11, no. 1, pp. 15–37, 1998.\n[68] A. M. Bianucci, A. Micheli, A. Sperduti, and A. Starita, “Analysis of\nthe internal representations developed by neural networks for structures\napplied to quantitative structure-activity relationship studies of benzo-\ndiazepines,” J. Chem. Inf. Comput. Sci., vol. 41, no. 1, pp. 202–218,\n2001.\n[69] M. Hagenbuchner, A. C. Tsoi, and A. Sperduti, “A supervised self-\norganising map for structured data,” in Advances in Self-Organising\nMaps, N. Allinson, H. Yin, L. Allinson, and J. Slack, Eds.\nBerlin,\nGermany: Springer-Verlag, 2001, pp. 21–28.\n[70] E. Seneta, Non-Negative Matrices and Markov Chains.\nNew York:\nSpringer-Verlag, 1981, ch. 4, pp. 112–158.\n[71] D. E. Rumelhart and J. McClelland, Parallel Distributed Processing:\nExplorations in the Microstructure of Cognition.\nCambridge, MA:\nPDP Research Group, MIT Press, 1986, vol. 1.\n[72] A. Graham, Kronecker Products and Matrix Calculus: With Applica-\ntions.\nNew York: Wiley, 1982.\n[73] H. Bunke, “Graph matching: Theoretical foundations, algorithms, and\napplications,” in Proc. Vis. Interface, Montreal, QC, Canada, 2000, pp.\n82–88.\n[74] D. Conte, P. Foggia, C. Sansone, and M. Vento, “Graph matching ap-\nplications in pattern recognition and image processing,” in Proc. Int.\nConf. Image Process., Sep. 2003, vol. 2, pp. 21–24.\n[75] D. Conte, P. Foggia, C. Sansone, and M. Vento, “Thirty years of graph\nmatching in pattern recognition,” Int. J. Pattern Recognit. Artif. Intell.,\nvol. 18, no. 3, pp. 265–268, 2004.\n[76] A. Agarwal, S. Chakrabarti, and S. Aggarwal, “Learning to rank net-\nworked entities,” in Proc. 12th ACM SIGKDD Int. Conf. Knowl. Disc.\nData Mining, New York, 2006, pp. 14–23.\n[77] V. Di Massa, G. Monfardini, L. Sarti, F. Scarselli, M. Maggini, and\nM. Gori, “A comparison between recursive neural networks and graph\nneural networks,” in Proc. Int. Joint Conf. Neural Netw., Jul. 2006, pp.\n778–785.\n[78] G. Monfardini, V. Di Massa, F. Scarselli, and M. Gori, “Graph neural\nnetworks for object localization,” in Proc. 17th Eur. Conf. Artif. Intell.,\nAug. 2006, pp. 665–670.\n[79] F. Scarselli, S. Yong, M. Gori, M. Hagenbuchner, A. C. Tsoi, and M.\nMaggini, “Graph neural networks for ranking web pages,” in Proc.\nIEEE/WIC/ACM Conf. Web Intelligence, 2005, pp. 666–672.\n[80] W. Uwents, G. Monfardini, H. Blockeel, F. Scarselli, and M. Gori,\n“Two connectionist models for graph processing: An experimental\ncomparison on relational data,” in Proc. Eur. Conf. Mach. Learn.,\n2006, pp. 213–220.\n[81] S. Yong, M. Hagenbuchner, F. Scarselli, A. C. Tsoi, and M. Gori, “Doc-\nument mining using graph neural networks,” in Proc. 5th Int. Work-\nshop Initiative Evaluat. XML Retrieval, N. Fuhr, M. Lalmas, and A.\nTrotman, Eds., 2007, pp. 458–472.\n[82] F. Scarselli and G. Monfardini, The GNN Toolbox, [Online]. Avail-\nable: http://airgroup.dii.unisi.it/projects/GraphNeuralNetwork/down-\nload.htm\n[83] A. K. Debnath, R. Lopex de Compandre, G. Debnath, A. Schusterman,\nand C. Hansch, “Structure-activity relationship of mutagenic aromatic\nand heteroaromatic nitro compounds. correlation with molecular or-\nbital energies and hydrophobicity,” J. Med. Chem., vol. 34, no. 2, pp.\n786–797, 1991.\n[84] S. Kramer and L. De Raedt, “Feature construction with version spaces\nfor biochemical applications,” in Proc. 18th Int. Conf. Mach. Learn.,\n2001, pp. 258–265.\n[85] J. Quinlan and R. Cameron-Jones, “FOIL: A midterm report,” in Proc.\nEur. Conf. Mach. Learn., 1993, pp. 3–20.\n[86] J. Quinlan, “Boosting ﬁrst-order learning,” in Lecture Notes in Com-\nputer Science.\nBerlin, Germany: Springer-Verlag, 1996, vol. 1160,\np. 143.\n[87] J. Ramon, “Clustering and instance based learning in ﬁrst order logic,”\nPh.D. dissertation, Dept. Comput. Sci., K.U. Leuven, Leuven, Bel-\ngium, 2002.\n[88] M. Kirsten, “Multirelational distance-based clustering,” Ph.D. disser-\ntation, Schl. Comput. Sci., Otto-von-Guericke Univ., Magdeburg, Ger-\nmany, 2002.\n[89] M. Krogel, S. Rawles, F. Zelezny, P. Flach, N. Lavrac, and S.\nWrobel, “Comparative evaluation of approaches to propositionaliza-\ntion,” in Proc. 13th Int. Conf. Inductive Logic Programm., 2003,\npp. 197–214.\n[90] S. Muggleton, “Machine learning for systems biology,” in Proc. 15th\nInt. Conf. Inductive Logic Programm., Bonn, Germany, Aug. 10 – 13,\n2005, pp. 416–423.\n[91] A. Woz´nica, A. Kalousis, and M. Hilario, “Matching based kernels for\nlabeled graphs,” in Proc. Int. Workshop Mining Learn. Graphs/ECML/\nPKDD, T. Gärtner, G. Garriga, and T. Meinl, Eds., 2006, pp. 97–108.\n[92] L. De Raedt and H. Blockeel, “Using logical decision trees for clus-\ntering,” in Lecture Notes in Artiﬁcial Intelligence.\nBerlin, Germany:\nSpringer-Verlag, 1997, vol. 1297, pp. 133–141.\n[93] M. Diligenti, M. Gori, and M. Maggini, “Web page scoring systems for\nhorizontal and vertical search,” in Proc. 11th World Wide Web Conf.,\n2002, pp. 508–516.\n[94] T. H. Haveliwala, “Topic sensitive pagerank,” in Proc. 11th World Wide\nWeb Conf., 2002, pp. 517–526.\n[95] G. Jeh and J. Widom, “Scaling personalized web search,” in Proc. 12th\nWorld Wide Web Conf., May 20–24, 2003, pp. 271–279.\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n",
    "80\nIEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 1, JANUARY 2009\n[96] F. Scarselli, A. C. Tsoi, and M. Hagenbuchner, “Computing person-\nalized pageranks,” in Proc. 12th World Wide Web Conf., 2004, pp.\n282–283.\nFranco Scarselli received the Laurea degree with\nhonors in computer science from the University of\nPisa, Pisa, Italy, in 1989 and the Ph.D. degree in\ncomputer science and automation engineering from\nthe University of Florence, Florence, Italy, in 1994.\nHe has been supported by foundations of private\nand public companies and by a postdoctoral of the\nUniversity of Florence. In 1999, he moved to the Uni-\nversity of Siena, Siena, Italy, where he was initially a\nResearch Associate and is currently an Associate Pro-\nfessor at the Department of Information Engineering.\nHe is the author of more than 50 journal and conference papers and has been\nhas been involved in several research projects, founded by public institutions\nand private companies, focused on software engineering, machine learning, and\ninformation retrieval. His current theoretical research activity is mainly in the\nﬁeld of machine learning with a particular focus on adaptive processing of data\nstructures, neural networks, and approximation theory. His research interests\ninclude also image understanding, information retrieval, and web applications.\nMarco Gori (S’88–M’91–SM’97–F’01) received the\nPh.D. degree from University di Bologna, Bologna,\nItaly, in 1990, while working in part as a visiting stu-\ndent at the School of Computer Science, McGill Uni-\nversity, Montréal, QC, Canada.\nIn 1992, he became an Associate Professor of\nComputer Science at Università di Firenze and, in\nNovember 1995, he joint the Università di Siena,\nSiena, Italy, where he is currently Full Professor of\nComputer Science. His main interests are in machine\nlearning with applications to pattern recognition,\nweb mining, and game playing. He is especially interested in the formulation of\nrelational machine learning schemes in the continuum setting. He is the leader\nof the WebCrow project for automatic solving of crosswords that outperformed\nhuman competitors in an ofﬁcial competition taken place within the 2006\nEuropean Conference on Artiﬁcial Intelligence. He is coauthor of the book\nWeb Dragons: Inside the Myths of Search Engines Technologies (San Mateo:\nMorgan Kauffman, 2006).\nDr. Gori serves (has served) as an Associate Editor of a number of technical\njournals related to his areas of expertise and he has been the recipient of best\npaper awards and keynote speakers in a number of international conferences. He\nwas the Chairman of the Italian Chapter of the IEEE Computational Intelligence\nSociety and the President of the Italian Association for Artiﬁcial Intelligence. He\nis a Fellow of the European Coordinating Committee for Artiﬁcial Intelligence.\nAh Chung Tsoi received the Higher Diploma degree\nin electronic engineering from the Hong Kong Tech-\nnical College, Hong Kong, in 1969 and the M.Sc. de-\ngree in electronic control engineering and the Ph.D.\ndegree in control engineering from University of Sal-\nford, Salford, U.K., in 1970 and 1972, respectively.\nHe was a Postdoctoral Fellow at the Inter-Univer-\nsity Institute of Engineering Control at University\nCollege of North Wales, Bangor, North Wales and a\nLecturer at Paisley College of Technology, Paisley,\nScotland. He was a Senior Lecturer in Electrical\nEngineering in the Department of Electrical Engineering, University of Auck-\nland, New Zealand, and a Senior Lecturer in Electrical Engineering, University\nCollege University of New South Wales, Australia, for ﬁve years. He then\nserved as Professor of Electrical Engineering at University of Queensland,\nAustralia; Dean, and simultaneously, Director of Information Technology\nServices, and then foundation Pro-Vice Chancellor (Information Technology\nand Communications) at University of Wollongong, before joining the Aus-\ntralian Research Council as an Executive Director, Mathematics, Information\nand Communications Sciences. He was Director, Monash e-Research Centre,\nMonash University, Melbourne, Australia. In April 2007, became a Vice\nPresident (Research and Institutional Advancement), Hong Kong Baptist\nUniversity, Hong Kong. In recent years, he has been working in the area of\nartiﬁcial intelligence in particular neural networks and fuzzy systems. He has\npublished in neural network literature. Recently, he has been working in the\napplication of neural networks to graph domains, with applications to the world\nwide web searching, and ranking problems, and subgraph matching problem.\nMarkus Hagenbuchner (M’02) received the B.Sc.\ndegree (with honors) and the Ph.D. degree in com-\nputer science from University of Wollongong, Wol-\nlongong, Australia, in 1996 and 2002, respectively.\nCurrently, he is a Lecturer at Faculty of In-\nformatics, University of Wollongong. His main\nresearch activities are in the area of machine learning\nwith special focus on supervised and unsupervised\nmethods for the graph structured domain. His\ncontribution to the development of a self-organizing\nmap for graphs led to winning the international\ncompetition on document mining on several occasions.\nGabriele Monfardini received the Laurea degree\nand the Ph.D. degree in information engineering\nfrom the University of Siena, Siena, Italy, in 2003\nand 2007, respectively.\nCurrently, he is Contract Professor at the School\nof Engineering, University of Siena. His main\nresearch interests include neural networks, adaptive\nprocessing of structured data, image analysis, and\npattern recognition.\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n"
  ],
  "full_text": "IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 1, JANUARY 2009\n61\nThe Graph Neural Network Model\nFranco Scarselli, Marco Gori, Fellow, IEEE, Ah Chung Tsoi, Markus Hagenbuchner, Member, IEEE, and\nGabriele Monfardini\nAbstract—Many underlying relationships among data in several\nareas of science and engineering, e.g., computer vision, molec-\nular chemistry, molecular biology, pattern recognition, and data\nmining, can be represented in terms of graphs. In this paper, we\npropose a new neural network model, called graph neural network\n(GNN) model, that extends existing neural network methods for\nprocessing the data represented in graph domains. This GNN\nmodel, which can directly process most of the practically useful\ntypes of graphs, e.g., acyclic, cyclic, directed, and undirected,\nimplements a function\n(\n)\nthat maps a graph\nand one of its nodes\ninto an\n-dimensional Euclidean space. A\nsupervised learning algorithm is derived to estimate the param-\neters of the proposed GNN model. The computational cost of the\nproposed algorithm is also considered. Some experimental results\nare shown to validate the proposed learning algorithm, and to\ndemonstrate its generalization capabilities.\nIndex Terms—Graphical domains, graph neural networks\n(GNNs), graph processing, recursive neural networks.\nI. INTRODUCTION\nD\nATA can be naturally represented by graph structures in\nseveral application areas, including proteomics [1], image\nanalysis [2], scene description [3], [4], software engineering [5],\n[6], and natural language processing [7]. The simplest kinds of\ngraph structures include single nodes and sequences. But in sev-\neral applications, the information is organized in more complex\ngraph structures such as trees, acyclic graphs, or cyclic graphs.\nTraditionally, data relationships exploitation has been the sub-\nject of many studies in the community of inductive logic pro-\ngramming and, recently, this research theme has been evolving\nin different directions [8], also because of the applications of\nrelevant concepts in statistics and neural networks to such areas\n(see, for example, the recent workshops [9]–[12]).\nIn machine learning, structured data is often associated with\nthe goal of (supervised or unsupervised) learning from exam-\nManuscript received May 24, 2007; revised January 08, 2008 and May 02,\n2008; accepted June 15, 2008. First published December 09, 2008; current ver-\nsion published January 05, 2009. This work was supported by the Australian\nResearch Council in the form of an International Research Exchange scheme\nwhich facilitated the visit by F. Scarselli to University of Wollongong when the\ninitial work on this paper was performed. This work was also supported by the\nARC Linkage International Grant LX045446 and the ARC Discovery Project\nGrant DP0453089.\nF. Scarselli, M. Gori, and G. Monfardini are with the Faculty of Informa-\ntion Engineering, University of Siena, Siena 53100, Italy (e-mail: franco@dii.\nunisi.it; marco@dii.unisi.it; monfardini@dii.unisi.it).\nA. C. Tsoi is with Hong Kong Baptist University, Kowloon, Hong Kong\n(e-mail: act@hkbu.edu.hk).\nM. Hagenbuchner is with the University of Wollongong, Wollongong, N.S.W.\n2522, Australia (e-mail: markus@uow.edu.au).\nColor versions of one or more of the ﬁgures in this paper are available online\nat http://ieeexplore.ieee.org.\nDigital Object Identiﬁer 10.1109/TNN.2008.2005605\nples a function\nthat maps a graph\nand one of its nodes\nto\na vector of reals1:\n. Applications to a graphical\ndomain can generally be divided into two broad classes, called\ngraph-focused and node-focused applications, respectively, in\nthis paper. In graph-focused applications, the function\nis in-\ndependent of the node\nand implements a classiﬁer or a re-\ngressor on a graph structured data set. For example, a chemical\ncompound can be modeled by a graph\n, the nodes of which\nstand for atoms (or chemical groups) and the edges of which\nrepresent chemical bonds [see Fig. 1(a)] linking together some\nof the atoms. The mapping\nmay be used to estimate the\nprobability that the chemical compound causes a certain disease\n[13]. In Fig. 1(b), an image is represented by a region adjacency\ngraph where nodes denote homogeneous regions of intensity of\nthe image and arcs represent their adjacency relationship [14]. In\nthis case,\nmay be used to classify the image into different\nclasses according to its contents, e.g., castles, cars, people, and\nso on.\nIn node-focused applications,\ndepends on the node\n, so\nthat the classiﬁcation (or the regression) depends on the proper-\nties of each node. Object detection is an example of this class of\napplications. It consists of ﬁnding whether an image contains a\ngiven object, and, if so, localizing its position [15]. This problem\ncan be solved by a function\n, which classiﬁes the nodes of the\nregion adjacency graph according to whether the corresponding\nregion belongs to the object. For example, the output of\nfor\nFig. 1(b) might be 1 for black nodes, which correspond to the\ncastle, and 0 otherwise. Another example comes from web page\nclassiﬁcation. The web can be represented by a graph where\nnodes stand for pages and edges represent the hyperlinks be-\ntween them [Fig. 1(c)]. The web connectivity can be exploited,\nalong with page contents, for several purposes, e.g., classifying\nthe pages into a set of topics.\nTraditional machine learning applications cope with graph\nstructured data by using a preprocessing phase which maps the\ngraph structured information to a simpler representation, e.g.,\nvectors of reals [16]. In other words, the preprocessing step ﬁrst\n“squashes” the graph structured data into a vector of reals and\nthen deals with the preprocessed data using a list-based data\nprocessing technique. However, important information, e.g., the\ntopological dependency of information on each node may be\nlost during the preprocessing stage and the ﬁnal result may de-\npend, in an unpredictable manner, on the details of the prepro-\ncessing algorithm. More recently, there have been various ap-\nproaches [17], [18] attempting to preserve the graph structured\nnature of the data for as long as required before the processing\n1Note that in most classiﬁcation problems, the mapping is to a vector of inte-\ngers IN\n, while in regression problems, the mapping is to a vector of reals IR\n.\nHere, for simplicity of exposition, we will denote only the regression case. The\nproposed formulation can be trivially rewritten for the situation of classiﬁcation.\n1045-9227/$25.00 © 2008 IEEE\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n\n\n62\nIEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 1, JANUARY 2009\nFig. 1. Some applications where the information is represented by graphs: (a) a chemical compound (adrenaline), (b) an image, and (c) a subset of the web.\nphase. The idea is to encode the underlying graph structured\ndata using the topological relationships among the nodes of the\ngraph, in order to incorporate graph structured information in\nthe data processing step. Recursive neural networks [17], [19],\n[20] and Markov chains [18], [21], [22] belong to this set of tech-\nniques and are commonly applied both to graph and node-fo-\ncused problems. The method presented in this paper extends\nthese two approaches in that it can deal directly with graph struc-\ntured information.\nExisting recursive neural networks are neural network models\nwhose input domain consists of directed acyclic graphs [17],\n[19], [20]. The method estimates the parameters\nof a func-\ntion\n, which maps a graph to a vector of reals. The approach\ncan also be used for node-focused applications, but in this case,\nthe graph must undergo a preprocessing phase [23]. Similarly,\nusing a preprocessing phase, it is possible to handle certain types\nof cyclic graphs [24]. Recursive neural networks have been ap-\nplied to several problems including logical term classiﬁcation\n[25], chemical compound classiﬁcation [26], logo recognition\n[2], [27], web page scoring [28], and face localization [29].\nRecursive neural networks are also related to support vector\nmachines [30]–[32], which adopt special kernels to operate on\ngraph structured data. For example, the diffusion kernel [33] is\nbased on heat diffusion equation; the kernels proposed in [34]\nand [35] exploit the vectors produced by a graph random walker\nand those designed in [36]–[38] use a method of counting the\nnumber of common substructures of two trees. In fact, recursive\nneural networks, similar to support vector machine methods,\nautomatically encode the input graph into an internal represen-\ntation. However, in recursive neural networks, the internal en-\ncoding is learned, while in support vector machine, it is designed\nby the user.\nOn the other hand, Markov chain models can emulate\nprocesses where the causal connections among events are\nrepresented by graphs. Recently, random walk theory, which\naddresses a particular class of Markov chain models, has been\napplied with some success to the realization of web page\nranking algorithms [18], [21]. Internet search engines use\nranking algorithms to measure the relative “importance” of\nweb pages. Such measurements are generally exploited, along\nwith other page features, by “horizontal” search engines, e.g.,\nGoogle [18], or by personalized search engines (“vertical”\nsearch engines; see, e.g., [22]) to sort the universal resource\nlocators (URLs) returned on user queries.2 Some attempts have\nbeen made to extend these models with learning capabilities\nsuch that a parametric model representing the behavior of\nthe system can be estimated from a set of training examples\nextracted from a collection [22], [40], [41]. Those models are\nable to generalize the results to score all the web pages in the\ncollection. More generally, several other statistical methods\nhave been proposed, which assume that the data set consists of\npatterns and relationships between patterns. Those techniques\ninclude random ﬁelds [42], Bayesian networks [43], statistical\nrelational learning [44], transductive learning [45], and semisu-\npervised approaches for graph processing [46].\nIn this paper, we present a supervised neural network model,\nwhich is suitable for both graph and node-focused applications.\nThis model uniﬁes these two existing models into a common\n2The relative importance measure of a web page is also used to serve other\ngoals, e.g., to improve the efﬁciency of crawlers [39].\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n\n\nSCARSELLI et al.: THE GRAPH NEURAL NETWORK MODEL\n63\nframework. We will call this novel neural network model a\ngraph neural network (GNN). It will be shown that the GNN\nis an extension of both recursive neural networks and random\nwalk models and that it retains their characteristics. The model\nextends recursive neural networks since it can process a more\ngeneral class of graphs including cyclic, directed, and undi-\nrected graphs, and it can deal with node-focused applications\nwithout any preprocessing steps. The approach extends random\nwalk theory by the introduction of a learning algorithm and by\nenlarging the class of processes that can be modeled.\nGNNs are based on an information diffusion mechanism. A\ngraph is processed by a set of units, each one corresponding to a\nnode of the graph, which are linked according to the graph con-\nnectivity. The units update their states and exchange informa-\ntion until they reach a stable equilibrium. The output of a GNN\nis then computed locally at each node on the base of the unit\nstate. The diffusion mechanism is constrained in order to en-\nsure that a unique stable equilibrium always exists. Such a real-\nization mechanism was already used in cellular neural networks\n[47]–[50] and Hopﬁeld neural networks [51]. In those neural\nnetwork models, the connectivity is speciﬁed according to a pre-\ndeﬁned graph, the network connections are recurrent in nature,\nand the neuron states are computed by relaxation to an equilib-\nrium point. GNNs differ from both the cellular neural networks\nand Hopﬁeld neural networks in that they can be used for the\nprocessing of more general classes of graphs, e.g., graphs con-\ntaining undirected links, and they adopt a more general diffusion\nmechanism.\nIn this paper, a learning algorithm will be introduced, which\nestimates the parameters of the GNN model on a set of given\ntraining examples. In addition, the computational cost of the pa-\nrameter estimation algorithm will be considered. It is also worth\nmentioning that elsewhere [52] it is proved that GNNs show a\nsort of universal approximation property and, under mild condi-\ntions, they can approximate most of the practically useful func-\ntions\non graphs.3\nThe structure of this paper is as follows. After a brief de-\nscription of the notation used in this paper as well as some pre-\nliminary deﬁnitions, Section II presents the concept of a GNN\nmodel, together with the proposed learning algorithm for the\nestimation of the GNN parameters. Moreover, Section III dis-\ncusses the computational cost of the learning algorithm. Some\nexperimental results are presented in Section IV. Conclusions\nare drawn in Section V.\nII. THE GRAPH NEURAL NETWORK MODEL\nWe begin by introducing some notations that will be used\nthroughout the paper. A graph\nis a pair\n, where\nis\nthe set of nodes and\nis the set of edges. The set\nstands\nfor the neighbors of\n, i.e., the nodes connected to\nby an arc,\nwhile\ndenotes the set of arcs having\nas a vertex. Nodes\nand edges may have labels represented by real vectors. The la-\nbels attached to node\nand edge\nwill be represented\nby\nand\n, respectively. Let denote the\nvector obtained by stacking together all the labels of the graph.\n3Due to the length of proofs, such results cannot be shown here and is included\nin [52].\nThe notation adopted for labels follows a more general scheme:\nif\nis a vector that contains data from a graph and\nis a subset of\nthe nodes (the edges), then\ndenotes the vector obtained by se-\nlecting from\nthe components related to the node (the edges) in\n. For example,\nstands for the vector containing the labels\nof all the neighbors of\n. Labels usually include features of ob-\njects related to nodes and features of the relationships between\nthe objects. For example, in the case of an image as in Fig. 1(b),\nnode labels might represent properties of the regions (e.g., area,\nperimeter, and average color intensity), while edge labels might\nrepresent the relative position of the regions (e.g., the distance\nbetween their barycenters and the angle between their principal\naxes). No assumption is made on the arcs; directed and undi-\nrected edges are both permitted. However, when different kinds\nof edges coexist in the same data set, it is necessary to distin-\nguish them. This can be easily achieved by attaching a proper\nlabel to each edge. In this case, different kinds of arcs turn out\nto be just arcs with different labels.\nThe considered graphs may be either positional or nonposi-\ntional. Nonpositional graphs are those described so far; posi-\ntional graphs differ since a unique integer identiﬁer is assigned\nto each neighbors of a node\nto indicate its logical position.\nFormally, for each node\nin a positional graph, there exists an\ninjective function\n, which assigns to\neach neighbor\nof\na position\n. Note that the position\nof the neighbor can be implicitly used for storing useful infor-\nmation. For instance, let us consider the example of the region\nadjacency graph [see Fig. 1(b)]:\ncan be used to represent the\nrelative spatial position of the regions, e.g.,\nmight enumerate\nthe neighbors of a node , which represents the adjacent regions,\nfollowing a clockwise ordering convention.\nThe domain considered in this paper is the set\nof pairs of\na graph and a node, i.e.,\nwhere\nis a set of the\ngraphs and\nis a subset of their nodes. We assume a supervised\nlearning framework with the learning set\nwhere\ndenotes the th node in the set\nand\nis the desired target associated to\n. Finally,\nand\n. Interestingly, all the graphs of the learning set can be\ncombined into a unique disconnected graph, and, therefore, one\nmight think of the learning set as the pair\nwhere\nis a graph and\na is set of pairs\n. It is worth mentioning that this com-\npact deﬁnition is not only useful for its simplicity, but that it\nalso captures directly the very nature of some problems where\nthe domain consists of only one graph, for instance, a large por-\ntion of the web [see Fig. 1(c)].\nA. The Model\nThe intuitive idea underlining the proposed approach is that\nnodes in a graph represent objects or concepts, and edges rep-\nresent their relationships. Each concept is naturally deﬁned by\nits features and the related concepts. Thus, we can attach a state\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n\n\n64\nIEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 1, JANUARY 2009\nFig. 2. Graph and the neighborhood of a node. The state xxx\nof the node 1\ndepends on the information contained in its neighborhood.\nto each node\nthat is based on the information con-\ntained in the neighborhood of\n(see Fig. 2). The state\ncon-\ntains a representation of the concept denoted by\nand can be\nused to produce an output\n, i.e., a decision about the concept.\nLet\nbe a parametric function, called local transition func-\ntion, that expresses the dependence of a node\non its neighbor-\nhood and let\nbe the local output function that describes how\nthe output is produced. Then,\nand\nare deﬁned as follows:\n(1)\nwhere\n,\n,\n, and\nare the label of\n, the labels\nof its edges, the states, and the labels of the nodes in the neigh-\nborhood of\n, respectively.\nRemark 1: Different notions of neighborhood can be adopted.\nFor example, one may wish to remove the labels\n, since\nthey include information that is implicitly contained in\n.\nMoreover, the neighborhood could contain nodes that are two\nor more links away from\n. In general, (1) could be simpliﬁed\nin several different ways and several minimal models4 exist. In\nthe following, the discussion will mainly be based on the form\ndeﬁned by (1), which is not minimal, but it is the one that more\nclosely represents our intuitive notion of neighborhood.\nRemark 2: Equation (1) is customized for undirected graphs.\nWhen dealing with directed graphs, the function\ncan also ac-\ncept as input a representation of the direction of the arcs. For ex-\nample,\nmay take as input a variable\nfor each arc\nsuch that\n, if\nis directed towards\nand\n, if\ncomes from\n. In the following, in order to keep the notations\ncompact, we maintain the customization of (1). However, un-\nless explicitly stated, all the results proposed in this paper hold\n4A model is said to be minimal if it has the smallest number of variables while\nretaining the same computational power.\nalso for directed graphs and for graphs with mixed directed and\nundirected links.\nRemark 3: In general, the transition and the output functions\nand their parameters may depend on the node\n. In fact, it is\nplausible that different mechanisms (implementations) are used\nto represent different kinds of objects. In this case, each kind of\nnodes\nhas its own transition function\n, output function\n, and a set of parameters\n. Thus, (1) becomes\nand\n.\nHowever, for the sake of simplicity, our analysis will consider\n(1) that describes a particular model where all the nodes share\nthe same implementation.\nLet\n, , , and\nbe the vectors constructed by stacking all\nthe states, all the outputs, all the labels, and all the node labels,\nrespectively. Then, (1) can be rewritten in a compact form as\n(2)\nwhere\n, the global transition function and\n, the global\noutput function are stacked versions of\ninstances of\nand\n, respectively.\nWe are interested in the case when\nare uniquely deﬁned\nand (2) deﬁnes a map\n, which takes a graph\nas input and returns an output\nfor each node. The Banach\nﬁxed point theorem [53] provides a sufﬁcient condition for the\nexistence and uniqueness of the solution of a system of equa-\ntions. According to Banach’s theorem [53], (2) has a unique so-\nlution provided that\nis a contraction map with respect to the\nstate, i.e., there exists\n,\n, such that\nholds for any\n, where\ndenotes\na vectorial norm. Thus, for the moment, let us assume that\nis a contraction map. Later, we will show that, in GNNs, this\nproperty is enforced by an appropriate implementation of the\ntransition function.\nNote that (1) makes it possible to process both positional and\nnonpositional graphs. For positional graphs,\nmust receive the\npositions of the neighbors as additional inputs. In practice, this\ncan be easily achieved provided that information contained in\n,\n, and\nis sorted according to neighbors’ po-\nsitions and is properly padded with special null values in po-\nsitions corresponding to nonexisting neighbors. For example,\n, where\nis the max-\nimal number of neighbors of a node;\nholds, if\nis the\nth neighbor of\n; and\n, for some prede-\nﬁned null state\n, if there is no th neighbor.\nHowever, for nonpositional graphs, it is useful to replace\nfunction\nof (1) with\n(3)\nwhere\nis a parametric function. This transition function,\nwhich has been successfully used in recursive neural networks\n[54], is not affected by the positions and the number of the chil-\ndren. In the following, (3) is referred to as the nonpositional\nform, while (1) is called the positional form. In order to imple-\nment the GNN model, the following items must be provided:\n1) a method to solve (1);\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n\n\nSCARSELLI et al.: THE GRAPH NEURAL NETWORK MODEL\n65\nFig. 3. Graph (on the top), the corresponding encoding network (in the middle), and the network obtained by unfolding the encoding network (at the bottom).\nThe nodes (the circles) of the graph are replaced, in the encoding network, by units computing f\nand g\n(the squares). When f\nand g\nare implemented by\nfeedforward neural networks, the encoding network is a recurrent neural network. In the unfolding network, each layer corresponds to a time instant and contains\na copy of all the units of the encoding network. Connections between layers depend on encoding network connectivity.\n2) a learning algorithm to adapt\nand\nusing examples\nfrom the training data set5;\n3) an implementation of\nand\n.\nThese aspects will be considered in turn in the following\nsections.\nB. Computation of the State\nBanach’s ﬁxed point theorem [53] does not only ensure the\nexistence and the uniqueness of the solution of (1) but it also\nsuggests the following classic iterative scheme for computing\nthe state:\n(4)\n5In other words, the parameters www are estimated using examples contained in\nthe training data set.\nwhere\ndenotes the th iteration of . The dynamical system\n(4) converges exponentially fast to the solution of (2) for any ini-\ntial value\n. We can, therefore, think of\nas the state that\nis updated by the transition function\n. In fact, (4) implements\nthe Jacobi iterative method for solving nonlinear equations [55].\nThus, the outputs and the states can be computed by iterating\n(5)\nNote that the computation described in (5) can be interpreted\nas the representation of a network consisting of units, which\ncompute\nand\n. Such a network will be called an encoding\nnetwork, following an analog terminology used for the recursive\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n\n\n66\nIEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 1, JANUARY 2009\nneural network model [17]. In order to build the encoding net-\nwork, each node of the graph is replaced by a unit computing the\nfunction\n(see Fig. 3). Each unit stores the current state\nof node\n, and, when activated, it calculates the state\nusing the node label and the information stored in the neigh-\nborhood. The simultaneous and repeated activation of the units\nproduce the behavior described in (5). The output of node\nis\nproduced by another unit, which implements\n.\nWhen\nand\nare implemented by feedforward neural net-\nworks, the encoding network turns out to be a recurrent neural\nnetwork where the connections between the neurons can be di-\nvided into internal and external connections. The internal con-\nnectivity is determined by the neural network architecture used\nto implement the unit. The external connectivity depends on the\nedges of the processed graph.\nC. The Learning Algorithm\nLearning in GNNs consists of estimating the parameter\nsuch that\napproximates the data in the learning data set\nwhere\nis the number of supervised nodes in\n. For graph-fo-\ncused tasks, one special node is used for the target (\nholds), whereas for node-focused tasks, in principle, the super-\nvision can be performed on every node. The learning task can\nbe posed as the minimization of a quadratic cost function\n(6)\nRemark 4: As common in neural network applications, the\ncost function may include a penalty term to control other prop-\nerties of the model. For example, the cost function may contain\na smoothing factor to penalize any abrupt changes of the outputs\nand to improve the generalization performance.\nThe learning algorithm is based on a gradient-descent\nstrategy and is composed of the following steps.\na) The states\nare iteratively updated by (5) until at time\nthey approach the ﬁxed point solution of (2):\n.\nb) The gradient\nis computed.\nc) The weights\nare updated according to the gradient com-\nputed in step b).\nConcerning step a), note that the hypothesis that\nis a\ncontraction map ensures the convergence to the ﬁxed point.\nStep c) is carried out within the traditional framework of gra-\ndient descent. As shown in the following, step b) can be carried\nout in a very efﬁcient way by exploiting the diffusion process\nthat takes place in GNNs. Interestingly, this diffusion process\nis very much related to the one which takes place in recurrent\nneural networks, for which the gradient computation is based\non backpropagation-through-time algorithm [17], [56], [57]. In\nthis case, the encoding network is unfolded from time\nback to\nan initial time\n. The unfolding produces the layered network\nshown in Fig. 3. Each layer corresponds to a time instant and\ncontains a copy of all the units\nof the encoding network. The\nunits of two consecutive layers are connected following graph\nconnectivity. The last layer corresponding to time\nincludes\nalso the units\nand computes the output of the network.\nBackpropagation through time consists of carrying out the\ntraditional backpropagation step on the unfolded network to\ncompute the gradient of the cost function at time\nwith respect\nto (w.r.t.) all the instances of\nand\n. Then,\nis\nobtained by summing the gradients of all instances. However,\nbackpropagation through time requires to store the states of\nevery instance of the units. When the graphs and\nare\nlarge, the memory required may be considerable.6 On the\nother hand, in our case, a more efﬁcient approach is possible,\nbased on the Almeida–Pineda algorithm [58], [59]. Since (5)\nhas reached a stable point\nbefore the gradient computation,\nwe can assume that\nholds for any\n. Thus,\nbackpropagation through time can be carried out by storing\nonly\n. The following two theorems show that such an intuitive\napproach has a formal justiﬁcation. The former theorem proves\nthat function\nis differentiable.\nTheorem 1 (Differentiability):\nLet\nand\nbe the\nglobal transition and the global output functions of a GNN,\nrespectively. If\nand\nare continuously differ-\nentiable w.r.t.\nand\n, then\nis continuously differentiable\nw.r.t.\n.\nProof: Let a function\nbe deﬁned as\nSuch a function is continuously differ-\nentiable w.r.t.\nand\n, since it is the difference of\ntwo continuously differentiable functions. Note that the\nJacobian\nmatrix\nof\nw.r.t.\nfulﬁlls\nwhere\nde-\nnotes the\n-dimensional identity matrix and\n,\nis\nthe dimension of the state. Since\nis a contraction map,\nthere exists\nsuch that\n,\nwhich implies\n. Thus, the de-\nterminant of\nis not null and we can apply the\nimplicit function theorem (see [60]) to\nand point\n. As\na consequence, there exists a function\n, which is deﬁned\nand continuously differentiable in a neighborhood of\n, such\nthat\nand\nSince this\nresult holds for any\n, it is demonstrated that\nis continu-\nously differentiable on the whole domain. Finally, note that\n, where\ndenotes the operator\nthat returns the components corresponding to node\n. Thus,\nis the composition of differentiable functions and hence is\nitself differentiable.\nIt is worth mentioning that this property does not hold for\ngeneral dynamical systems for which a slight change in the pa-\nrameters can force the transition from one ﬁxed point to another.\nThe fact that\nis differentiable in GNNs is due to the assump-\ntion that\nis a contraction map. The next theorem provides a\nmethod for an efﬁcient computation of the gradient.\nTheorem 2 (Backpropagation): Let\nand\nbe the tran-\nsition and the output functions of a GNN, respectively, and as-\nsume that\nand\nare continuously differen-\ntiable w.r.t.\nand\n. Let\nbe deﬁned by\n(7)\n6For internet applications, the graph may represent a signiﬁcant portion of\nthe web. This is an example of cases when the amount of the required memory\nstorage may play a very important role.\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n\n\nSCARSELLI et al.: THE GRAPH NEURAL NETWORK MODEL\n67\nThen, the sequence\nconverges to a vector\nand the convergence is exponential and in-\ndependent of the initial state\n. Moreover\n(8)\nholds, where\nis the stable state of the GNN.\nProof: Since\nis a contraction map, there exists\nsuch that\nholds. Thus,\n(7) converges to a stable ﬁxed point for each initial state. The\nstable ﬁxed point\nis the solution of (7) and satisﬁes\n(9)\nwhere\nholds. Moreover, let us consider again the\nfunction\ndeﬁned in the proof of Theorem 1. By the implicit\nfunction theorem\n(10)\nholds. On the other hand, since the error\ndepends on\nthe output of the network\n, the gra-\ndient\ncan be computed using the chain rule for\ndifferentiation\n(11)\nThe theorem follows by putting together (9)–(11)\nThe relationship between the gradient deﬁned by (8) and the\ngradient computed by the Almeida–Pineda algorithm can be\neasily recognized. The ﬁrst term on the right-hand side of (8)\nrepresents the contribution to the gradient due to the output func-\ntion\n. Backpropagation calculates the ﬁrst term while it is\npropagating the derivatives through the layer of the functions\n(see Fig. 3). The second term represents the contribution due to\nthe transition function\n. In fact, from (7)\nIf we assume\nand\n, for\n, it follows:\nTABLE I\nLEARNING ALGORITHM. THE FUNCTION FORWARD COMPUTES THE STATES,\nWHILE BACKWARD CALCULATES THE GRADIENT. THE PROCEDURE\nMAIN MINIMIZES THE ERROR BY CALLING ITERATIVELY\nFORWARD AND BACKWARD\nThus, (7) accumulates the\ninto the variable\n. This mechanism corresponds to backpropagate the gradients\nthrough the layers containing the\nunits.\nThe learning algorithm is detailed in Table I. It consists of a\nmain procedure and of two functions FORWARD and BACKWARD.\nFunction FORWARD takes as input the current set of parameters\nand iterates to ﬁnd the convergent point, i.e., the ﬁxed point.\nThe iteration is stopped when\nis less than\na given threshold\naccording to a given norm\n. Function\nBACKWARD computes the gradient: system (7) is iterated until\nis smaller than a threshold\n; then, the gradient\nis calculated by (8).\nThe main procedure updates the weights until the output\nreaches a desired accuracy or some other stopping criterion is\nachieved. In Table I, a predeﬁned learning rate\nis adopted,\nbut most of the common strategies based on the gradient-de-\nscent strategy can be used as well, for example, we can use a\nmomentum term and an adaptive learning rate scheme. In our\nGNN simulator, the weights are updated by the resilient back-\npropagation [61] strategy, which, according to the literature\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n\n\n68\nIEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 1, JANUARY 2009\non feedforward neural networks, is one of the most efﬁcient\nstrategies for this purpose. On the other hand, the design of\nlearning algorithms for GNNs that are not explicitly based on\ngradient is not obvious and it is a matter of future research.\nIn fact, the encoding network is only apparently similar to a\nstatic feedforward network, because the number of the layers\nis dynamically determined and the weights are partially shared\naccording to input graph topology. Thus, second-order learning\nalgorithms [62], pruning [63], and growing learning algorithms\n[64]–[66] designed for static networks cannot be directly\napplied to GNNs. Other implementation details along with\na computational cost analysis of the proposed algorithm are\nincluded in Section III.\nD. Transition and Output Function Implementations\nThe implementation of the local output function\ndoes not\nneed to fulﬁll any particular constraint. In GNNs,\nis a mul-\ntilayered feedforward neural network. On the other hand, the\nlocal transition function\nplays a crucial role in the proposed\nmodel, since its implementation determines the number and the\nexistence of the solutions of (1). The assumption behind GNN\nis that the design of\nis such that the global transition func-\ntion\nis a contraction map w.r.t. the state\n. In the following,\nwe describe two neural network models that fulﬁll this purpose\nusing different strategies. These models are based on the non-\npositional form described by (3). It can be easily observed that\nthere exist two corresponding models based on the positional\nform as well.\n1) Linear (nonpositional) GNN. Equation (3) can naturally be\nimplemented by\n(12)\nwhere the vector\nand the matrix\nare deﬁned by the output of two feedforward neural net-\nworks (FNNs), whose parameters correspond to the param-\neters of the GNN. More precisely, let us call transition net-\nwork an FNN that has to generate\nand forcing net-\nwork another FNN that has to generate\n. Moreover, let\nand\nbe the func-\ntions implemented by the transition and the forcing net-\nwork, respectively. Then, we deﬁne\n(13)\n(14)\nwhere\nand\nhold,\nand\ndenotes the operator that allocates the ele-\nments of a\n-dimensional vector into as\nmatrix. Thus,\nis obtained by arranging the outputs of the transition\nnetwork into the square matrix\nand by multiplication\nwith the factor\n. On the other hand,\nis just\na vector that contains the outputs of the forcing network.\nHere, it is further assumed that\nholds7; this can be straightforwardly veriﬁed if the output\nneurons of the transition network use an appropriately\n7The 1-norm of a matrix M\n=\nfm\ng is deﬁned as kMk\n=\nmax\njm\nj.\nbounded activation function, e.g., a hyperbolic tangent.\nNote that in this case\nwhere\nis the\nvector constructed by stacking all the\n, and\nis a block\nmatrix\n, with\nif\nis a neighbor of\nand\notherwise. Moreover, vectors\nand\nmatrices\ndo not depend on the state\n, but only on\nnode and edge labels. Thus,\n, and, by simple\nalgebra\nwhich implies that\nis a contraction map (w.r.t.\n)\nfor any set of parameters\n.\n2) Nonlinear (nonpositional) GNN. In this case,\nis real-\nized by a multilayered FNN. Since three-layered neural\nnetworks are universal approximators [67],\ncan approx-\nimate any desired function. However, not all the parameters\ncan be used, because it must be ensured that the corre-\nsponding transition function\nis a contraction map. This\ncan be achieved by adding a penalty term to (6), i.e.,\nwhere the penalty term\nis\nif\nand 0\notherwise, and the parameter\ndeﬁnes the desired\ncontraction constant of\n. More generally, the penalty\nterm can be any expression, differentiable w.r.t.\n, that\nis monotone increasing w.r.t. the norm of the Jacobian.\nFor example, in our experiments, we use the penalty term\n, where\nis the th column of\n. In fact, such an expression is an approximation\nof\n.\nE. A Comparison With Random Walks and\nRecursive Neural Networks\nGNNs turn out to be an extension of other models already pro-\nposed in the literature. In particular, recursive neural networks\n[17] are a special case of GNNs, where:\n1) the input graph is a directed acyclic graph;\n2) the inputs of\nare limited to\nand\n, where\nis the set of children of\n8;\n3) there is a supersource node\nfrom which all the other\nnodes can be reached. This node is typically used for output\n(graph-focused tasks).\nThe neural architectures, which have been suggested for real-\nizing\nand\n, include multilayered FNNs [17], [19], cascade\ncorrelation [68], and self-organizing maps [20], [69]. Note that\nthe above constraints on the processed graphs and on the inputs\nof\nexclude any sort of cyclic dependence of a state on itself.\nThus, in the recursive neural network model, the encoding net-\nworks are FNNs. This assumption simpliﬁes the computation of\n8A node u is child of n if there exists an arc from n to u. Obviously, ch[n] \u0012\nne[n] holds.\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n\n\nSCARSELLI et al.: THE GRAPH NEURAL NETWORK MODEL\n69\nTABLE II\nTIME COMPLEXITY OF THE MOST EXPENSIVE INSTRUCTIONS OF THE LEARNING ALGORITHM. FOR EACH INSTRUCTION AND EACH GNN MODEL,\nA BOUND ON THE ORDER OF FLOATING POINT OPERATIONS IS GIVEN. THE TABLE ALSO DISPLAYS\nTHE NUMBER OF TIMES PER EPOCH THAT EACH INSTRUCTION IS EXECUTED\nthe states. In fact, the states can be computed following a prede-\nﬁned ordering that is induced by the partial ordering of the input\ngraph.\nInterestingly, the GNN model captures also the random walks\non graphs when choosing\nas a linear function. Random walks\nand, more generally, Markov chain models are useful in several\napplication areas and have been recently used to develop ranking\nalgorithms for internet search engines [18], [21]. In random\nwalks on graphs, the state\nassociated with a node is a real\nvalue and is described by\n(15)\nwhere\nis the set of parents of\n, and\n,\nholds for each\n. The\nare normalized so that\n. In fact, (15) can represent a random walker\nwho is traveling on the graph. The value\nrepresents the\nprobability that the walker, when visiting node\n, decides to go\nto node . The state\nstands for the probability that the walker\nis on node\nin the steady state. When all\nare stacked into\na vector\n, (15) becomes\nwhere\nand\nis deﬁned as in (15) if\nand\notherwise. It is\neasily veriﬁed that\n. Markov chain theory suggests\nthat if there exists\nsuch that all the elements of the matrix\nare nonnull, then (15) is a contraction map [70]. Thus, provided\nthat the above condition on\nholds, random walks on graphs\nare an instance of GNNs, where\nis a constant stochastic\nmatrix instead of being generated by neural networks.\nIII. COMPUTATIONAL COMPLEXITY ISSUES\nIn this section, an accurate analysis of the computational cost\nwill be derived. The analysis will focus on three different GNN\nmodels: positional GNNs, where the functions\nand\nof (1)\nare implemented by FNNs; linear (nonpositional) GNNs; and\nnonlinear (nonpositional) GNNs.\nFirst, we will describe with more details the most complex\ninstructions involved in the learning procedure (see Table II).\nThen, the complexity of the learning algorithm will be deﬁned.\nFor the sake of simplicity, the cost is derived assuming that the\ntraining set contains just one graph\n. Such an assumption does\nnot cause any loss of generality, since the graphs of the training\nset can always be merged into a single graph. The complexity is\nmeasured by the order of ﬂoating point operations.9\nIn Table II, the notation\nis used to denote the number of\nhidden-layer neurons. For example,\nindicates the number of\nhidden-layer neurons in the implementation of function\n.\nIn the following,\n,\n, and\ndenote the number of epochs,\nthe mean number of forward iterations (of the repeat cycle in\nfunction FORWARD), and the mean number of backward itera-\ntions (of the repeat cycle in function BACKWARD), respectively.\nMoreover, we will assume that there exist two procedures\nand\n, which implement the forward phase and the backward\nphase of the backpropagation procedure [71], respectively. For-\nmally, given a function\nimplemented by an\nFNN, we have\nHere,\nis the input vector and the row vector\nis a\nsignal that suggests how the network output must be adjusted to\nimprove the cost function. In most applications, the cost func-\ntion is\nand\n),\nwhere\nand\nis the vector of the desired output cor-\nresponding to input\n. On the other hand,\nis the\ngradient of\nw.r.t. the network input and is easily computed\n9According to the common deﬁnition of time complexity, an algorithm re-\nquires O(l(a)) operations, if there exist \u000b > 0, \u0016a \u0015 0, such that c(a) \u0014 \u000b l(a)\nholds for each a \u0015 \u0016a, where c(a) is the maximal number of operations executed\nby the algorithm when the length of input is a.\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n\n\n70\nIEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 1, JANUARY 2009\nas a side product of backpropagation.10 Finally,\nand\nde-\nnote the computational complexity required by the application\nof\nand\non\n, respectively. For example, if\nis imple-\nmented by a multilayered FNN with\ninputs,\nhidden neurons,\nand\noutputs, then\nholds.\nA. Complexity of Instructions\n1) Instructions\n,\n, and\n: Since\nis a matrix having at most\nnonnull elements, the multiplication of\nby\n, and as\na consequence, the instruction\n, costs\nﬂoating points operations. Moreover, the state\nand the output vector\nare calculated by applying the local\ntransition function and the local output function to each node\n. Thus, in positional GNNs and in nonlinear GNNs, where\n,\n, and\nare directly implemented by FNNs,\nand\nare computed by running the forward phase of backpropagation\nonce for each node or edge (see Table II).\nOn the other hand, in linear GNNs,\nis calculated in\ntwo steps: the matrices\nof (13) and the vectors\n(14) are\nevaluated; then,\nis computed. The former phase, the cost\nof which is\n, is executed once for each\nepoch, whereas the latter phase, the cost of which is\n,\nis executed at every step of the cycle in the function FORWARD.\n2) Instruction\n: This instruction re-\nquires the computation of the Jacobian of\n. Note that\nis a block matrix where the block\nmeasures the\neffect of node\non node\n, if there is an arc\nfrom\nto\n, and is null otherwise. In the linear model, the matrices\ncorrespond to those displayed in (13) and used to calculate\nin the forward phase. Thus, such an instruction has no cost in\nthe backward phase in linear GNNs.\nIn nonlinear GNNs,\n,\nis computed by appropriately exploiting the backpropagation\nprocedure. More precisely, let\nbe a vector where all\nthe components are zero except for the th one, which equals\none, i.e.,\n,\n, and so on.\nNote that\n, when it is applied to\nwith\n, returns\n, i.e., the th column of the Jacobian\n. Thus,\ncan be computed by applying\non\nall the\n, i.e.,\n(16)\nwhere\nindicates that we are considering only the ﬁrst com-\nponent of the output of\n. A similar reasoning can also be used\nwith positional GNNs. The complexity of these procedures is\neasily derived and is displayed in the fourth row of Table II.\n3) Computation of\nand\n: In linear GNNs,\nthe cost function is\n, and, as a con-\nsequence,\n, if\nis a node belonging\nto the training set, and 0 otherwise. Thus,\nis easily cal-\nculated by\noperations.\n10Backpropagation\ncomputes\nfor\neach\nneuron\nv\nthe\ndelta\nvalue\n(@e =@a )(yyy) = \u000e(@l =@a )(yyy), where e\nis the cost function and a\nthe\nactivation level of neuron v. Thus, \u000e(@l =@yyy)(yyy) is just a vector stacking all\nthe delta values of the input neurons.\nIn positional and nonlinear GNNs, a penalty term\nis added\nto the cost function to force the transition function to be a con-\ntraction map. In this case, it is necessary to compute\n,\nbecause such a vector must be added to the gradient. Let\ndenote the element in position\nof the block\n. According\nto the deﬁnition of\n, we have\nwhere\n, if the sum is larger\nthan 0, and it is 0 otherwise. It follows:\nwhere\nis the sign function. Moreover, let\nbe a matrix\nwhose element in position\nis\nand let\nbe\nthe operator that takes a matrix and produce a column vector by\nstacking all its columns one on top of the other. Then\n(17)\nholds. The vector\ndepends on selected imple-\nmentation of\nor\n. For sake of simplicity, let us restrict our\nattention to nonlinear GNNs and assume that the transition net-\nwork is a three-layered FNN.\n,\n,\n, and\nare the activa-\ntion function11, the vector of the activation levels, the matrix of\nthe weights, and the thresholds of the th layer, respectively. The\nfollowing reasoning can also be extended to positional GNNs\nand networks with a different number of layers. The function\nis formally deﬁned in terms of\n,\n,\n, and\nBy the chain differentiation rule, it follows:\nwhere\nis the derivative of\n,\nis an operator that\ntransforms a vector into a diagonal matrix having such a vector\nas diagonal, and\nis the submatrix of\nthat contains only\nthe weights that connect the inputs corresponding to\nto\nthe hidden layer. The parameters\naffect four components of\n11\u001b\nis a vectorial function that takes as input the vector of the activation\nlevels of neurons in a layer and returns the vector of the outputs of the neurons\nof the same layer.\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n\n\nSCARSELLI et al.: THE GRAPH NEURAL NETWORK MODEL\n71\n, i.e.,\n,\n,\n, and\n. By properties of derivatives\nfor matrix products and the chain rule\n(18)\nholds.\nThus,\nis the sum of four con-\ntributions. In order to derive a method to compute those terms,\nlet\ndenote the\nidentity matrix. Let\nbe the Kro-\nnecker product and suppose that\nis a\nmatrix such\nthat\nfor any vector\n. By the Kro-\nnecker product properties,\nholds\nfor matrices\n,\n, and\nhaving compatible dimensions [72].\nThus, we have\nwhich implies\nSimilarly, using the properties\nand\n, it follows:\nwhere\nis the number of hidden neurons. Then, we have\n(19)\n(20)\n(21)\n(22)\nwhere the mentioned Kronecker product properties have been\nused.\nIt follows that\ncan be written\nas the sum of the four contributions represented by (19)–(22).\nThe second and the fourth term [(20) and (22)] can be computed\ndirectly using the corresponding formulas. The ﬁrst one can be\ncalculated by observing that\nlooks like the function com-\nputed by a three-layered FNN that is the same as\nexcept for\nthe activation function of the last layer. In fact, if we denote by\nsuch a network, then\n(23)\nholds, where\n. A sim-\nilar reasoning can be applied also to the third contribution.\nThe above described method includes two tasks: the matrix\nmultiplications of (19)–(22) and the backpropagation as deﬁned\nby (23). The former task consists of several matrix multiplica-\ntions. By inspection of (19)–(22), the number of ﬂoating point\noperations is approximately estimated as\n,12 where\ndenotes the number of hidden-layer neu-\nrons implementing the function\n. The second task has approx-\nimately the same cost as a backpropagation phase through the\noriginal function\n.\nThus, the complexity of computing\nis\n. Note, however, that even if the sum in (17)\nranges over all the arcs of the graph, only those arcs\nsuch\nthat\nhave to be considered. In practice,\nis a rare event, since it happens only when the columns of the\nJacobian are larger than\nand a penalty function was used\nto limit the occurrence of these cases. As a consequence, a\nbetter estimate of the complexity of computing\nis\n, where\nis the average number of\nnodes\nsuch that\nholds for some\n.\n4) Instructions\nand\n:\nThe terms\nand\ncan be\ncalculated by the backpropagation of\nthrough the\nnetwork that implements\n. Since such an operation must\nbe repeated for each node, the time complexity of instruc-\ntions\nand\nis\nfor all the GNN models.\n12Such a value is obtained by considering the following observations: for an\na \u0002 b matrix CCC and b \u0002 c matrix D\nD\nD, the multiplication CCCD\nD\nD requires approxi-\nmately 2abc operations; more precisely, abc multiplications and ac(b\u00021) sums.\nIf D\nD\nD is a diagonal b \u0002 b matrix, then CCCD\nD\nD requires 2ab operations. Moreover, if\nCCC is an a \u0002 b matrix, D\nD\nD is a b \u0002 a matrix, and PPP\nis the a \u0002 a matrix deﬁned\nabove and used in (19)–(22), then computing vec(CCCD\nD\nD)PPP\ncosts only 2ab op-\nerations provided that a sparse representation is used for P . Finally, aaa ; aaa ; aaa\nare already available, since they are computed during the forward phase of the\nlearning algorithm.\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n\n\n72\nIEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 1, JANUARY 2009\n5) Instruction\n: By deﬁnition of\n,\n, and\n, we have\n(24)\nwhere\nand\nindicates that we are\nconsidering only the ﬁrst part of the output of\n. Similarly\n(25)\nwhere\n. Thus, (24) and (25) provide a\ndirect method to compute\nin positional and nonlinear GNNs,\nrespectively.\nFor linear GNNs, let\ndenote the th output of\nand note\nthat\nholds. Here,\nand\nare the element in position\nof ma-\ntrix\nand the corresponding output of the transition network\n[see (13)], respectively, while\nis the th element of vector\n,\nis the corresponding output of the forcing network [see (14)],\nand\nis the th element of\n. Then\nwhere\n,\n,\n, and\nis\na vector that stores\nin the position corre-\nsponding to\n, that is,\n. Thus,\nin linear GNNs,\nis computed by calling the backpropagation\nprocedure on each arc and node.\nB. Time Complexity of the GNN Model\nAccording to our experiments, the application of a trained\nGNN on a graph (test phase) is relatively fast even for large\ngraphs. Formally, the complexity is easily derived from\nTable II and it is\nfor positional\nGNNs,\nfor nonlinear GNNs, and\nfor linear GNNs.\nIn practice, the cost of the test phase is mainly due to the\nrepeated computation of the state\n. The cost of each it-\neration is linear both w.r.t. the dimension of the input graph\n(the number of edges), the dimension of the employed FNNs\nand the state, with the only exception of linear GNNs, whose\nsingle iteration cost is quadratic w.r.t. to the state. The number\nof iterations required for the convergence of the state depends\non the problem at hand, but Banach’s theorem ensures that the\nconvergence is exponentially fast and experiments have shown\nthat 5–15 iterations are generally sufﬁcient to approximate the\nﬁxed point.\nIn positional and nonlinear GNNs, the transition function\nmust be activated\nand\ntimes, respectively. Even\nif such a difference may appear signiﬁcant, in practice, the\ncomplexity of the two models is similar, because the network\nthat implements the\nis larger than the one that implements\n. In fact,\nhas\ninput neurons, where\nis\nthe maximum number of neighbors for a node, whereas\nhas only\ninput neurons. An appreciable difference can\nbe noticed only for graphs where the number of neighbors\nof nodes is highly variable, since the inputs of\nmust be\nsufﬁcient to accommodate the maximal number of neighbors\nand many inputs may remain unused when\nis applied. On\nthe other hand, it is observed that in the linear model the FNNs\nare used only once for each iteration, so that the complexity\nof each iteration is\ninstead of\n. Note that\nholds, when\nis\nimplemented by a three-layered FNN with\nhidden neurons.\nIn practical cases, where\nis often larger than\n, the linear\nmodel is faster than the nonlinear model. As conﬁrmed by the\nexperiments, such an advantage is mitigated by the smaller\naccuracy that the model usually achieves.\nIn GNNs, the learning phase requires much more time than\nthe test phase, mainly due to the repetition of the forward and\nbackward phases for several epochs. The experiments have\nshown that the time spent in the forward and backward phases\nis not very different. Similarly to the forward phase, the cost\nof function BACKWARD is mainly due to the repetition of the\ninstruction that computes\n. Theorem 2 ensures that\nconverges exponentially fast and the experiments conﬁrmed\nthat\nis usually a small number.\nFormally, the cost of each learning epoch is given by the sum\nof all the instructions times the iterations in Table II. An inspec-\ntion of Table II shows that the cost of all instructions involved in\nthe learning phase are linear both with respect to the dimension\nof the input graph and of the FNNs. The only exceptions are due\nto the computation of\n,\nand\n, which depend quadratically on .\nThe most expensive instruction is apparently the computa-\ntion of\nin nonlinear GNNs, which costs\n. On the other hand, the experiments have shown that\nusually\nis a small number. In most epochs,\nis 0, since\nthe Jacobian does not violate the imposed constraint, and in\nthe other cases,\nis usually in the range 1–5. Thus, for a\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n\n\nSCARSELLI et al.: THE GRAPH NEURAL NETWORK MODEL\n73\nsmall state dimension\n, the computation of\nrequires\nfew applications of backpropagation on\nand has a small im-\npact on the global complexity of the learning process. On the\nother hand, in theory, if\nis very large, it might happen that\nand at the same time\n, causing the computation of the gradient to be very\nslow. However, it is worth mentioning that this case was never\nobserved in our experiments.\nIV. EXPERIMENTAL RESULTS\nIn this section, we present the experimental results, obtained\non a set of simple problems carried out to study the properties\nof the GNN model and to prove that the method can be ap-\nplied to relevant applications in relational domains. The prob-\nlems that we consider, viz., the subgraph matching, the mutage-\nnesis, and the web page ranking, have been selected since they\nare particularly suited to discover the properties of the model\nand are correlated to important real-world applications. From\na practical point of view, we will see that the results obtained\non some parts of mutagenesis data sets are among the best that\nare currently reported in the open literature (please see detailed\ncomparison in Section IV-B). Moreover, the subgraph matching\nproblem is relevant to several application domains. Even if the\nperformance of our method is not comparable in terms of best\naccuracy on the same problem with the most efﬁcient algorithms\nin the literature, the proposed approach is a very general tech-\nnique that can be applied on extension of the subgraph matching\nproblems [73]–[75]. Finally, the web page ranking is an inter-\nesting problem, since it is important in information retrieval and\nvery few techniques have been proposed for its solution [76]. It\nis worth mentioning that the GNN model has been already suc-\ncessfully applied on larger applications, which include image\nclassiﬁcation and object localization in images [77], [78], web\npage ranking [79], relational learning [80], and XML classiﬁca-\ntion [81].\nThe following facts hold for each experiment, unless other-\nwise speciﬁed. The experiments have been carried out with both\nlinear and nonlinear GNNs. According to existing results on re-\ncursive neural networks, the nonpositional transition function\nslightly outperforms the positional ones, hence, currently only\nnonpositional GNNs have been implemented and tested. Both\nthe (nonpositional) linear and the nonlinear model were tested.\nAll the functions involved in the two models, i.e.,\n,\n, and\nfor linear GNNs, and\nand\nfor nonlinear GNNs were\nimplemented by three-layered FNNs with sigmoidal activation\nfunctions. The presented results were averaged over ﬁve dif-\nferent runs. In each run, the data set was a collection of random\ngraphs constructed by the following procedure: each pair of\nnodes was connected with a certain probability ; the resulting\ngraph was checked to verify whether it was connected and if\nit was not, random edges were inserted until the condition was\nsatisﬁed.\nThe data set was split into a training set, a validation set, and\na test set and the validation set was used to avoid possible issues\nwith overﬁtting. For the problems where the original data is only\none single big graph\n, a training set, a validation set, and a test\nFig. 4. Two graphs GG\nG and GG\nG that contain a subgraph SSS. The numbers inside\nthe nodes represent the labels. The function \u001c to be learned is \u001c(GG\nG ; n\n) = 1,\nif n\nis a black node, and \u001c(GG\nG ; n\n) = \u00001, if n\nis a white node.\nset include different supervised nodes of\n. Otherwise, when\nseveral graphs were available, all the patterns of a graph\nwere\nassigned to only one set. In every trial, the training procedure\nperformed at most 5000 epochs and every 20 epochs the GNN\nwas evaluated on the validation set. The GNN that achieved the\nlowest cost on the validation set was considered the best model\nand was applied to the test set.\nThe performance of the model is measured by the accuracy\nin classiﬁcation problems (when\ncan take only the values\nor 1) and by the relative error in regression problems (when\nmay be any real number). More precisely, in a classiﬁ-\ncation problem, a pattern is considered correctly classiﬁed if\nand\nor if\nand\n. Thus, accuracy is deﬁned as the percentage of\npatterns correctly classiﬁed by the GNN on the test set. On\nthe other hand, in regression problems, the relative error on a\npattern is given by\n.\nThe algorithm was implemented in Matlab® 713 and the soft-\nware can be freely downloaded, together with the source and\nsome examples [82]. The experiments were carried out on a\nPower Mac G5 with a 2-GHz PowerPC processor.\nA. The Subgraph Matching Problem\nThe subgraph matching problem consists of ﬁnding the nodes\nof a given subgraph\nin a larger graph\n. More precisely, the\nfunction\nthat has to be learned is such that\nif\nbelongs to a subgraph of\n, which is isomorphic to\n, and\n, otherwise (see Fig. 4). Subgraph\nmatching has a number of practical applications, such as ob-\nject localization and detection of active parts in chemical com-\npounds [73]–[75]. This problem is a basic test to assess a method\nfor graph processing. The experiments will demonstrate that\nthe GNN model can cope with the given task. Of course, the\npresented results cannot be compared with those achievable by\nother speciﬁc methods for subgraph matching, which are faster\nand more accurate. On the other hand, the GNN model is a gen-\neral approach and can be used without any modiﬁcation to a\nvariety of extensions of the subgraph matching problem, where,\nfor example, several graphs must be detected at the same time,\nthe graphs are corrupted by noise on the structure and the labels,\n13Copyright © 1994–2006 by The MathWorks, Inc., Natick, MA.\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n\n\n74\nIEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 1, JANUARY 2009\nTABLE III\nACCURACIES ACHIEVED BY NONLINEAR MODEL (NL), LINEAR MODEL\n(L), AND A FEEDFORWARD NEURAL NETWORK\nON SUBGRAPH MATCHING PROBLEM\nand the target to be detected is unknown and provided only by\nexamples.\nIn our experiments, the data set\nconsisted of 600 connected\nrandom graphs (constructed using\n), equally divided into\na training set, a validation set, and a test set. A smaller subgraph\n, which was randomly generated in each trial, was inserted into\nevery graph of the data set. Thus, each graph\ncontained at\nleast a copy of\n, even if more copies might have been included\nby the random construction procedure. All the nodes had integer\nlabels in the range\nand, in order to deﬁne the correct tar-\ngets\n, a brute force algorithm located all the\ncopies of\nin\n. Finally, a small Gaussian noise, with zero\nmean and a standard deviation of 0.25, was added to all the la-\nbels. As a consequence, all the copies of\nin our data set were\ndifferent due to the introduced noise.\nIn all the experiments, the state dimension was\nand all\nthe neural networks involved in the GNNs had ﬁve hidden neu-\nrons. More network architectures have been tested with similar\nresults.\nIn order to evaluate the relative importance of the labels and\nthe connectivity in the subgraph localization, also a feedforward\nneural network was applied to this test. The FNN had one output,\n20 hidden, and one input units. The FNN predicted\nusing\nonly the label\nof node\n. Thus, the FNN did not use the\nconnectivity and exploited only the relative distribution of the\nlabels in\nw.r.t. the labels in graphs\n.\nTable III presents the accuracies achieved by the nonlinear\nGNN model (nonlinear), the linear GNN model (linear), and the\nFNN with several dimensions for\nand\n. The results allow to\nsingle out some of the factors that have inﬂuence on the com-\nplexity of the problem and on the performance of the models.\nObviously, the proportion of positive and negative patterns af-\nfects the performance of all the methods. The results improve\nwhen\nis close to\n, whereas when\nis about a half of\n, the performance is lower. In fact, in the latter case, the data\nset is perfectly balanced and it is more difﬁcult to guess the right\nresponse. Moreover, the dimension\n, by itself, has inﬂuence\non the performance, because the labels can assume only 11 dif-\nferent values and when\nis small most of the nodes of the sub-\ngraph can be identiﬁed by their labels. In fact, the performances\nare better for smaller\n, even if we restrict our attention to the\ncases when\nholds.\nThe results show that GNNs always outperform the FNNs,\nconﬁrming that the GNNs can exploit label contents and graph\ntopology at the same time. Moreover, the nonlinear GNN model\nachieved a slightly better performance than the linear one, prob-\nably because nonlinear GNNs implement a more general model\nthat can approximate a larger class of functions. Finally, it can\nbe observed that the total average error for FNNs is about 50%\nlarger than the GNN error (12.7 for nonlinear GNNs, 13.5 for\nlinear GNNs, and 22.8 for FNNs). Actually, the relative differ-\nence between the GNN and FNN errors, which measures the\nadvantage provided by the topology, tend to become smaller\nfor larger values of\n(see the last column of Table III). In\nfact, GNNs use an information diffusion mechanism to decide\nwhether a node belongs to the subgraph. When\nis larger, more\ninformation has to be diffused and, as a consequence, the func-\ntion to be learned is more complex.\nThe subgraph matching problem was used also to evaluate the\nperformance of the GNN model and to experimentally verify the\nﬁndings about the computational cost of the model described\nin Section III. For this purpose, some experiments have been\ncarried out varying the number of nodes, the number of edges\nin the data set, the number of hidden units in the neural networks\nimplementing the GNN, and the dimensionality of the state. In\nthe base case, the training set contained ten random graphs, each\none made of 20 nodes and 40 edges, the networks implementing\nthe GNN had ﬁve hidden neurons, and the state dimension was\n2. The GNN was trained for 1000 epochs and the results were\naveraged over ten trials. As expected, the central processing unit\n(CPU) time required by the gradient computation grows linearly\nw.r.t. the number of nodes, edges and hidden units, whereas\nthe growth is quadratic w.r.t. the state dimension. For example,\nFig. 5 depicts the CPU time spent by the gradient computation\nprocess when the nodes of each graph14 [Fig. 5(a)] and the states\nof the GNN [Fig. 5(b)] are increased, respectively.\nIt is worth mentioning that, in nonlinear GNNs, the\nquadratic growth w.r.t. the states, according to the discus-\nsion of Section III, depends on the time spent to calculate the\nJacobian\nand its derivative\n. Fig. 5\nshows how the total time spent by the gradient computation\nprocess is composed in this case: line\ndenotes the time\nrequired by the computation of\nand\n; line\nde-\nnotes that for the Jacobian\n; line\ndenotes\nthat for the derivative\n; the dotted line and the dashed\nline represent the rest of the time15 required by the FORWARD\nand the BACKWARD procedure, respectively; the continuous\nline stands for the rest of the time required by the gradient\ncomputation process.\n14More precisely, in this experiment, nodes and edges were increased keeping\nconstant to 1=2 their ratio.\n15That is, the time required by those procedures except for that already con-\nsidered in the previous points.\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n\n\nSCARSELLI et al.: THE GRAPH NEURAL NETWORK MODEL\n75\nFig. 5. Some plots about the cost of the gradient computation on GNNs. (a) and (b) CPU times required for 1000 learning epochs by nonlinear GNNs (continuous\nline) and linear GNN (dashed line), respectively, as a function of the number of nodes of the training set (a) and the dimension of the state (b). (c) Composition\nof the learning time for nonlinear GNNs: the computation of e\nand @e =@www (\u0000o\u0000); the Jacobian (@F =@xxx)(xxx; lll) (\u0000 \u0003 \u0002); the derivative @p =@w (\u0002x\u0002);\nthe rest of the FORWARD procedure (dotted line); the rest of the BACKWARD procedure (dashed line); the rest of the time learning procedure (continuous line).\n(d) Histogram of the number of the forward iterations, the backward iterations, and the number of nodes u such that RR\nR\n6= 0 [see (17)] encountered in each\nepoch of a learning session.\nFrom Fig. 5(c), we can observe that the computation of\nthat, in theory, is quadratic w.r.t. the states may have a\nsmall effect in practice. In fact, as already noticed in Section III,\nthe cost of such a computation depends on the number\nof\ncolumns of\nwhose norm is larger than the\nprescribed threshold, i.e., the number of nodes\nand\nsuch\nthat\n[see (17)]. Such a number is usually small due to\nthe effect of the penalty term\n. Fig. 5(d) shows a histogram\nof the number of nodes\nfor which\nin each epoch\nof a learning session: in practice, in this experiment, the non-\nnull\nare often zero and never exceed four in magnitude.\nAnother factor that affects the learning time is the number of\nforward and backward iterations needed to compute the stable\nstate and the gradient, respectively.16 Fig. 5(d) shows also the\n16The number of iterations depends also on the constant \u000f and \u000f of Table I,\nwhich were both set to 1e\u00023 in the experiments. However, due to the exponen-\ntial convergence of the iterative methods, these constants have a linear effect.\nhistograms of the number of required iterations, suggesting that\nalso those numbers are often small.\nB. The Mutagenesis Problem\nThe Mutagenesis data set [13] is a small data set, which is\navailable online and is often used as a benchmark in the re-\nlational learning and inductive logic programming literature.\nIt contains the descriptions of 230 nitroaromatic compounds\nthat are common intermediate subproducts of many industrial\nchemical reactions [83]. The goal of the benchmark consists of\nlearning to recognize the mutagenic compounds. The log mu-\ntagenicity was thresholded at zero, so the prediction is a bi-\nnary classiﬁcation problem. We will demonstrate that GNNs\nachieved the best result compared with those reported in the lit-\nerature on some parts of the data set.\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n\n\n76\nIEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 1, JANUARY 2009\nFig. 6. Atom-bond structure of a molecule represented by a graph with labeled\nnodes. Nodes represent atoms and edges denote atom bonds. Only one node is\nsupervised.\nIn [83], it is shown that 188 molecules out of 230 are\namenable to a linear regression analysis. This subset was called\n“regression friendly,” while the remaining 42 compounds were\ntermed “regression unfriendly.” Many different features have\nbeen used in the prediction. Apart from the atom-bond (AB)\nstructure, each compound is provided with four global features\n[83]. The ﬁrst two features are chemical measurements (C):\nthe lowest unoccupied molecule orbital and the water/octanol\npartition coefﬁcient, while the remaining two are precoded\nstructural (PS) attributes. Finally, the AB description can be\nused to deﬁne functional groups (FG), e.g., methyl groups and\nmany different rings that can be used as higher level features.\nIn our experiments, the best results were achieved using AB,\nC, and PS, without the functional groups. Probably the reason\nis that GNNs can recover the substructures that are relevant to\nthe classiﬁcation, exploiting the graphical structure contained\nin the AB description.\nIn our experiments, each molecule of the data set was trans-\nformed into a graph where nodes represent atoms and edges\nstand for ABs. The average number of nodes in a molecule is\naround 26. Node labels contain atom type, its energy state, and\nthe global properties AB, C, and PS. In each graph, there is\nonly one supervised node, the ﬁrst atom in the AB description\n(Fig. 6). The desired output is 1, if the molecule is mutagenic,\nand\n1, otherwise.\nIn Tables IV–VI, the results obtained by nonlinear GNNs17\nare compared with those achieved by other methods. The pre-\nsented results were evaluated using a tenfold cross-validation\nprocedure, i.e., the data set was randomly split into ten parts and\nthe experiments were repeated ten times, each time using a dif-\nferent part as the test set and the remaining patterns as training\nset. The results were averaged on ﬁve runs of the cross-valida-\ntion procedure.\nGNNs achieved the best accuracy on the regression-un-\nfriendly part (Table V) and on the whole data set (Table VI),\nwhile the results are close to the state of the art techniques\non the regression-friendly part (Table IV). It is worth noticing\nthat, whereas most of the approaches showed a higher level of\naccuracy when applied to the whole data set with respect to the\n17Some results were already presented in [80].\nTABLE IV\nACCURACIES ACHIEVED ON THE REGRESSION-FRIENDLY PART OF THE\nMUTAGENESIS DATA SET. THE TABLE DISPLAYS THE METHOD, THE\nFEATURES USED TO MAKE THE PREDICTION, AND A POSSIBLE\nREFERENCE TO THE PAPER WHERE THE RESULT IS DESCRIBED\nTABLE V\nACCURACIES ACHIEVED ON THE REGRESSION-UNFRIENDLY PART OF THE\nMUTAGENESIS DATA SET. THE TABLE DISPLAYS THE METHOD, THE\nFEATURES USED TO MAKE THE PREDICTION, AND A POSSIBLE\nREFERENCE TO THE PAPER WHERE THE RESULT IS DESCRIBED\nTABLE VI\nACCURACIES ACHIEVED ON THE WHOLE MUTAGENESIS DATA SET. THE TABLE\nDISPLAYS THE METHOD, THE FEATURES USED TO MAKE THE PREDICTION, AND\nA POSSIBLE REFERENCE TO THE PAPER WHERE THE RESULT IS DESCRIBED\nunfriendly part, the converse holds for GNNs. This suggests that\nGNNs can capture characteristics of the patterns that are useful\nto solve the problem but are not homogeneously distributed in\nthe two parts.\nC. Web Page Ranking\nIn this experiment, the goal is to learn the rank of a web\npage, inspired by Google’s PageRank [18]. According to\nPageRank, a page is considered authoritative if it is referred\nby many other pages and if the referring pages are authori-\ntative themselves. Formally, the PageRank\nof a page\nis\nwhere\nis the outdegree\nof\n, and\nis the damping factor [18]. In this experi-\nment, it is shown that a GNN can learn a modiﬁed version of\nPageRank, which adapts the “authority” measure according\nto the page content. For this purpose, a random web graph\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n\n\nSCARSELLI et al.: THE GRAPH NEURAL NETWORK MODEL\n77\nFig. 7. Desired function \u001c (the continuous lines) and the output of the GNN (the dotted lines) on the pages that belong to only one topic (a) and on the other pages\n(b). Horizontal axis stands for pages and vertical axis stands for scores. Pages have been sorted according to the desired value \u001c(GG\nG; n).\ncontaining 5000 nodes was generated, with\n. Training,\nvalidation, and test sets consisted of different nodes of this\ngraph. More precisely, only 50 nodes were supervised in the\ntraining set, other 50 nodes belonged to the validation set, and\nthe remaining nodes were in the test set.\nTo each node\n, a bidimensional boolean label\nis at-\ntached that represents whether the page belongs to two given\ntopics. If the page\nbelongs to both topics, then\n, while if it belongs to only one topic, then\n,\nor\n, and if it does not belong to either topics, then\n. The GNN was trained in order to produce the\nfollowing output:\nif\notherwise\nwhere\nstands for the Google’s PageRank.\nWeb page ranking algorithms are used by search engines to\nsort the URLs returned in response to user’s queries and more\ngenerally to evaluate the data returned by information retrieval\nsystems. The design of ranking algorithms capable of mixing to-\ngether the information provided by web connectivity and page\ncontent has been a matter of recent research [93]–[96]. In gen-\neral, this is an interesting and hard problem due to the difﬁculty\nin coping with structured information and large data sets. Here,\nwe present the results obtained by GNNs on a synthetic data set.\nMore results achieved on a snapshot of the web are available in\n[79].\nFor this example, only the linear model has been used, be-\ncause it is naturally suited to approximate the linear dynamics\nof the PageRank. Moreover, the transition and forcing networks\n(see Section I) were implemented by three-layered neural net-\nworks with ﬁve hidden neurons, and the dimension of the state\nwas\n. For the output function,\nis implemented as\nwhere\nis the function realized\nby a three-layered neural networks with ﬁve hidden neurons.\nFig. 7 shows the output of the GNN\nand the target function\non the test set. Fig. 7(a) displays the result for the pages that\nbelong to only one topic and Fig. 7(b) displays the result for\nthe other pages. Pages are displayed on horizontal axes and are\nsorted according to the desired output\n. The plots denote\nFig. 8. Error function on the training set (continuous line) and on the validation\n(dashed line) set during learning phase.\nthe value of function\n(continuous lines) and the value of the\nfunction implemented by the GNN (the dotted lines). The ﬁgure\nclearly suggests that GNN performs very well on this problem.\nFinally, Fig. 8 displays the error function during the learning\nprocess. The continuous line is the error on the training set,\nwhereas the dotted line is the error on the validation set. It\nis worth noting that the two curves are always very close and\nthat the error on the validation set is still decreasing after 2400\nepochs. This suggests that the GNN does not experiment over-\nﬁtting problems, despite the fact that the learning set consists of\nonly 50 pages from a graph containing 5000 nodes.\nV. CONCLUSION\nIn this paper, we introduced a novel neural network model\nthat can handle graph inputs: the graphs can be cyclic, directed,\nundirected, or a mixture of these. The model is based on in-\nformation diffusion and relaxation mechanisms. The approach\nextends into a common framework, the previous connectionist\ntechniques for processing structured data, and the methods\nbased on random walk models. A learning algorithm to esti-\nmate model parameters was provided and its computational\ncomplexity was studied, demonstrating that the method is\nsuitable also for large data sets.\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n\n\n78\nIEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 1, JANUARY 2009\nSome promising experimental results were provided to assess\nthe model. In particular, the results achieved on the whole Mu-\ntagenisis data set and on the unfriendly part of such a data set\nare the best compared with those reported in the open literature.\nMoreover, the experiments on the subgraph matching and on the\nweb page ranking show that the method can be applied to prob-\nlems that are related to important practical applications.\nThe possibility of dealing with domains where the data con-\nsists of patterns and relationships gives rise to several new topics\nof research. For example, while in this paper it is assumed that\nthe domain is static, it may happen that the input graphs change\nwith time. In this case, at least two interesting issues can be\nconsidered: ﬁrst, GNNs must be extended to cope with a dy-\nnamic domain; and second, no method exists, to the best of our\nknowledge, to model the evolution of the domain. The solution\nof the latter problem, for instance, may allow to model the evolu-\ntion of the web and, more generally, of social networks. Another\ntopic of future research is the study on how to deal with domains\nwhere the relationships, which are not known in advance, must\nbe inferred. In this case, the input contains ﬂat data and is auto-\nmatically transformed into a set of graphs in order to shed some\nlight on possible hidden relationships.\nREFERENCES\n[1] P. Baldi and G. Pollastri, “The principled design of large-scale recur-\nsive neural network architectures-dag-RNNs and the protein structure\nprediction problem,” J. Mach. Learn. Res., vol. 4, pp. 575–602, 2003.\n[2] E. Francesconi, P. Frasconi, M. Gori, S. Marinai, J. Sheng, G. Soda, and\nA. Sperduti, “Logo recognition by recursive neural networks,” in Lec-\nture Notes in Computer Science — Graphics Recognition, K. Tombre\nand A. K. Chhabra, Eds.\nBerlin, Germany: Springer-Verlag, 1997.\n[3] E. Krahmer, S. Erk, and A. Verleg, “Graph-based generation of refer-\nring expressions,” Comput. Linguist., vol. 29, no. 1, pp. 53–72, 2003.\n[4] A. Mason and E. Blake, “A graphical representation of the state spaces\nof hierarchical level-of-detail scene descriptions,” IEEE Trans. Vis.\nComput. Graphics, vol. 7, no. 1, pp. 70–75, Jan.-Mar. 2001.\n[5] L. Baresi and R. Heckel, “Tutorial introduction to graph transforma-\ntion: A software engineering perspective,” in Lecture Notes in Com-\nputer Science.\nBerlin, Germany: Springer-Verlag, 2002, vol. 2505,\npp. 402–429.\n[6] C. Collberg, S. Kobourov, J. Nagra, J. Pitts, and K. Wampler, “A system\nfor graph-based visualization of the evolution of software,” in Proc.\nACM Symp. Software Vis., 2003, pp. 77–86.\n[7] A. Bua, M. Gori, and F. Santini, “Recursive neural networks applied\nto discourse representation theory,” in Lecture Notes in Computer Sci-\nence.\nBerlin, Germany: Springer-Verlag, 2002, vol. 2415.\n[8] L. De Raedt, Logical and Relational Learning.\nNew York: Springer-\nVerlag, 2008, to be published.\n[9] T. Dietterich, L. Getoor, and K. Murphy, Eds., Proc. Int. Workshop\nStatist. Relat. Learn. Connect. Other Fields, 2004.\n[10] P. Avesani and M. Gori, Eds., Proc. Int. Workshop Sub-Symbol.\nParadigms Structured Domains, 2005.\n[11] S. Nijseen, Ed., Proc. 3rd Int. Workshop Mining Graphs Trees Se-\nquences, 2005.\n[12] T. Gaertner, G. Garriga, and T. Meini, Eds., Proc. 4th Int. Workshop\nMining Graphs Trees Sequences, 2006.\n[13] A. Srinivasan, S. Muggleton, R. King, and M. Sternberg, “Mutagenesis:\nIlp experiments in a non-determinate biological domain,” in Proc. 4th\nInt. Workshop Inductive Logic Programm., 1994, pp. 217–232.\n[14] T. Pavlidis, Structural Pattern Recognition, ser. Electrophysics.\nNew\nYork: Springer-Verlag, 1977.\n[15] M. Bianchini, M. Maggini, L. Sarti, and F. Scarselli, “Recursive neural\nnetworks learn to localize faces,” Phys. Rev. Lett., vol. 26, no. 12, pp.\n1885–1895, Sep. 2005.\n[16] S. Haykin, Neural Networks: A Comprehensive Foundation.\nNew\nYork: Prentice-Hall, 1994.\n[17] P. Frasconi, M. Gori, and A. Sperduti, “A general framework for adap-\ntive processing of data structures,” IEEE Trans. Neural Netw., vol. 9,\nno. 5, pp. 768–786, Sep. 1998.\n[18] S. Brin and L. Page, “The anatomy of a large-scale hypertextual web\nsearch engine,” in Proc. 7th World Wide Web Conf., Apr. 1998, pp.\n107–117.\n[19] A. Sperduti and A. Starita, “Supervised neural networks for the clas-\nsiﬁcation of structures,” IEEE Trans. Neural Netw., vol. 8, no. 2, pp.\n429–459, Mar. 1997.\n[20] M. Hagenbuchner, A. Sperduti, and A. C. Tsoi, “A self-organizing map\nfor adaptive processing of structured data,” IEEE Trans. Neural Netw.,\nvol. 14, no. 3, pp. 491–505, May 2003.\n[21] J. Kleinberg, “Authoritative sources in a hyperlinked environment,” J.\nACM, vol. 46, no. 5, pp. 604–632, 1999.\n[22] A. C. Tsoi, G. Morini, F. Scarselli, M. Hagenbuchner, and M. Maggini,\n“Adaptive ranking of web pages,” in Proc. 12th World Wide Web Conf.,\nBudapest, Hungary, May 2003, pp. 356–365.\n[23] M. Bianchini, P. Mazzoni, L. Sarti, and F. Scarselli, “Face spotting in\ncolor images using recursive neural networks,” in Proc. 1st Int. Work-\nshop Artif. Neural Netw. Pattern Recognit., Florence, Italy, Sep. 2003,\npp. 76–81.\n[24] M. Bianchini, M. Gori, and F. Scarselli, “Processing directed acyclic\ngraphs with recursive neural networks,” IEEE Trans. Neural Netw., vol.\n12, no. 6, pp. 1464–1470, Nov. 2001.\n[25] A. Küchler and C. Goller, “Inductive learning in symbolic domains\nusing structure-driven recurrent neural networks,” in Lecture Notes in\nComputer Science, G. Görz and S. Hölldobler, Eds.\nBerlin, Germany:\nSpringer-Verlag, Sep. 1996, vol. 1137.\n[26] T. Schmitt and C. Goller, “Relating chemical structure to activity:\nAn application of the neural folding architecture,” in Proc. Workshop\nFuzzy-Neuro Syst./Conf. Eng. Appl. Neural Netw., 1998, pp. 170–177.\n[27] M. Hagenbuchner and A. C. Tsoi, “A supervised training algorithm\nfor self-organizing maps for structures,” Pattern Recognit. Lett., vol.\n26, no. 12, pp. 1874–1884, 2005.\n[28] M. Gori, M. Maggini, E. Martinelli, and F. Scarselli, “Learning user\nproﬁles in NAUTILUS,” in Proc. Int. Conf. Adaptive Hypermedia\nAdaptive Web-Based Syst., Trento, Italy, Aug. 2000, pp. 323–326.\n[29] M. Bianchini, P. Mazzoni, L. Sarti, and F. Scarselli, “Face spotting in\ncolor images using recursive neural networks,” in Proc. Italian Work-\nshop Neural Netw., Vietri sul Mare, Italy, Jul. 2003.\n[30] B. Hammer and J. Jain, “Neural methods for non-standard data,” in\nProc. 12th Eur. Symp. Artif. Neural Netw., M. Verleysen, Ed., 2004,\npp. 281–292.\n[31] T. Gärtner, “Kernel-based learning in multi-relational data mining,”\nACM SIGKDD Explorations, vol. 5, no. 1, pp. 49–58, 2003.\n[32] T. Gärtner, J. Lloyd, and P. Flach, “Kernels and distances for structured\ndata,” Mach. Learn., vol. 57, no. 3, pp. 205–232, 2004.\n[33] R. Kondor and J. Lafferty, “Diffusion kernels on graphs and other dis-\ncrete structures,” in Proc. 19th Int. Conf. Mach. Learn., C. Sammut and\nA. e. Hoffmann, Eds., 2002, pp. 315–322.\n[34] H. Kashima, K. Tsuda, and A. Inokuchi, “Marginalized kernels\nbetween labeled graphs,” in Proc. 20th Int. Conf. Mach. Learn., T.\nFawcett and N. e. Mishra, Eds., 2003, pp. 321–328.\n[35] P. Mahé, N. Ueda, T. Akutsu, J.-L Perret, and J.-P. Vert, “Extensions\nof marginalized graph kernels,” in Proc. 21st Int. Conf. Mach. Learn.,\n2004, pp. 552–559.\n[36] M. Collins and N. Duffy, “Convolution kernels for natural language,”\nin Advances in Neural Information Processing Systems, T. G. Diet-\nterich, S. Becker, and Z. Ghahramani, Eds.\nCambridge, MA: MIT\nPress, 2002, vol. 14, pp. 625–632.\n[37] J. Suzuki, Y. Sasaki, and E. Maeda, “Kernels for structured natural\nlanguage data,” in Proc. Conf. Neural Inf. Process. Syst., 2003.\n[38] J. Suzuki, H. Isozaki, and E. Maeda, “Convolution kernels with fea-\nture selection for natural language processing tasks,” in Proc. Annu.\nMeeting Assoc. Comput. Linguistics, 2004, pp. 119–126.\n[39] J. Cho, H. Garcia-Molina, and L. Page, “Efﬁcient crawling through url\nordering,” in Proc. 7th World Wide Web Conf., Brisbane, Australia, Apr.\n1998, pp. 161–172.\n[40] A. C. Tsoi, M. Hagenbuchner, and F. Scarselli, “Computing cus-\ntomized page ranks,” ACM Trans. Internet Technol., vol. 6, no. 4, pp.\n381–414, Nov. 2006.\n[41] H. Chang, D. Cohn, and A. K. McCallum, “Learning to create cus-\ntomized authority lists,” in Proc. 17th Int. Conf. Mach. Learn., 2000,\npp. 127–134.\n[42] J. Lafferty, A. McCallum, and F. Pereira, “Conditional random ﬁelds:\nProbabilistic models for segmenting and labeling sequence data,” in\nProc. 18th Int. Conf. Mach. Learn., 2001, pp. 282–289.\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n\n\nSCARSELLI et al.: THE GRAPH NEURAL NETWORK MODEL\n79\n[43] F. V. Jensen, Introduction to Bayesian Networks.\nNew York:\nSpringer-Verlag, 1996.\n[44] L. Getoor and B. Taskar, Introduction to Statistical Relational\nLearning.\nCambridge, MA: MIT Press, 2007.\n[45] V. N. Vapnik, Statistical Learning Theory.\nNew York: Wiley, 1998.\n[46] O. Chapelle, B. Schölkopf, and A. Zien, Eds., Semi-Supervised\nLearning.\nCambridge, MA: MIT Press, 2006.\n[47] L. Chua and L. Yang, “Cellular neural networks: Theory,” IEEE Trans.\nCircuits Syst., vol. CAS-35, no. 10, pp. 1257–1272, Oct. 1988.\n[48] L. Chua and L. Yang, “Cellular neural networks: Applications,” IEEE\nTrans. Circuits Syst., vol. CAS-35, no. 10, pp. 1273–1290, Oct. 1988.\n[49] P. Kaluzny, “Counting stable equilibria of cellular neural networks-A\ngraph theoretic approach,” in Proc. Int. Workshop Cellular Neural\nNetw. Appl., 1992, pp. 112–116.\n[50] M. Ogorzatek, C. Merkwirth, and J. Wichard, “Pattern recognition\nusing ﬁnite-iteration cellular systems,” in Proc. 9th Int. Workshop\nCellular Neural Netw. Appl., 2005, pp. 57–60.\n[51] J. Hopﬁeld, “Neural networks and physical systems with emergent\ncollective computational abilities,” Proc. Nat. Acad. Sci., vol. 79, pp.\n2554–2558, 1982.\n[52] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfar-\ndini, “Computation capabilities of graph neural networks,” IEEE Trans.\nNeural Netw., vol. 20, no. 1, Jan. 2009, to be published.\n[53] M. A. Khamsi, An Introduction to Metric Spaces and Fixed Point\nTheory.\nNew York: Wiley, 2001.\n[54] M. Bianchini, M. Maggini, L. Sarti, and F. Scarselli, “Recursive neural\nnetworks for processing graphs with labelled edges: Theory and appli-\ncations,” Neural Netw., vol. 18, Special Issue on Neural Networks and\nKernel Methods for Structured Domains, no. 8, pp. 1040–1050, 2005.\n[55] M. J. D. Powell, “An efﬁcient method for ﬁnding the minimum of a\nfunction of several variables without calculating derivatives,” Comput.\nJ., vol. 7, pp. 155–162, 1964.\n[56] W. T. Miller, III, R. Sutton, and P. E. Werbos, Neural Network for\nControl.\nCamrbidge, MA: MIT Press, 1990.\n[57] A. C. Tsoi, “Adaptive processing of sequences and data structures,” in\nLecture Notes in Computer Science, C. L. Giles and M. Gori, Eds.\nBerlin, Germany: Springer-Verlag, 1998, vol. 1387, pp. 27–62.\n[58] L. Almeida, “A learning rule for asynchronous perceptrons with feed-\nback in a combinatorial environment,” in Proc. IEEE Int. Conf. Neural\nNetw., M. Caudill and C. Butler, Eds., San Diego, 1987, vol. 2, pp.\n609–618.\n[59] F. Pineda, “Generalization of back-propagation to recurrent neural net-\nworks,” Phys. Rev. Lett., vol. 59, pp. 2229–2232, 1987.\n[60] W. Rudin, Real and Complex Analysis, 3rd ed.\nNew York: McGraw-\nHill, 1987.\n[61] M. Riedmiller and H. Braun, “A direct adaptive method for faster back-\npropagation learning: The rprop algorithm,” in Proc. IEEE Int. Conf.\nNeural Netw., San Francisco, CA, 1993, pp. 586–591.\n[62] M. Bishop, Neural Networks for Pattern Recognition.\nOxford, U.K.:\nOxford Univ. Press, 1995.\n[63] R. Reed, “Pruning algorithms — A survey,” IEEE Trans. Neural Netw.,\nvol. 4, no. 5, pp. 740–747, Sep. 1993.\n[64] S. Fahlman and C. Lebiere, “The cascade-correlation learning archi-\ntecture,” in Advances in Neural Information Processing Systems, D.\nTouretzky, Ed.\nDenver, San Mateo: Morgan Kaufmann, 1989, vol. 2,\npp. 524–532.\n[65] T.-Y. Kwok and D.-Y. Yeung, “Constructive algorithms for structure\nlearning in feedforward neural networks for regression problems,”\nIEEE Trans. Neural Netw., vol. 8, no. 3, pp. 630–645, May 1997.\n[66] G.-B. Huang, L. Chen, and C. K. Siew, “Universal approximation using\nincremental constructive feedforward networks with random hidden\nnodes,” IEEE Trans. Neural Netw., vol. 17, no. 1, pp. 879–892, Jan.\n2006.\n[67] F. Scarselli and A. C. Tsoi, “Universal approximation using feedfor-\nward neural networks: A survey of some existing methods, and some\nnew results,” Neural Netw., vol. 11, no. 1, pp. 15–37, 1998.\n[68] A. M. Bianucci, A. Micheli, A. Sperduti, and A. Starita, “Analysis of\nthe internal representations developed by neural networks for structures\napplied to quantitative structure-activity relationship studies of benzo-\ndiazepines,” J. Chem. Inf. Comput. Sci., vol. 41, no. 1, pp. 202–218,\n2001.\n[69] M. Hagenbuchner, A. C. Tsoi, and A. Sperduti, “A supervised self-\norganising map for structured data,” in Advances in Self-Organising\nMaps, N. Allinson, H. Yin, L. Allinson, and J. Slack, Eds.\nBerlin,\nGermany: Springer-Verlag, 2001, pp. 21–28.\n[70] E. Seneta, Non-Negative Matrices and Markov Chains.\nNew York:\nSpringer-Verlag, 1981, ch. 4, pp. 112–158.\n[71] D. E. Rumelhart and J. McClelland, Parallel Distributed Processing:\nExplorations in the Microstructure of Cognition.\nCambridge, MA:\nPDP Research Group, MIT Press, 1986, vol. 1.\n[72] A. Graham, Kronecker Products and Matrix Calculus: With Applica-\ntions.\nNew York: Wiley, 1982.\n[73] H. Bunke, “Graph matching: Theoretical foundations, algorithms, and\napplications,” in Proc. Vis. Interface, Montreal, QC, Canada, 2000, pp.\n82–88.\n[74] D. Conte, P. Foggia, C. Sansone, and M. Vento, “Graph matching ap-\nplications in pattern recognition and image processing,” in Proc. Int.\nConf. Image Process., Sep. 2003, vol. 2, pp. 21–24.\n[75] D. Conte, P. Foggia, C. Sansone, and M. Vento, “Thirty years of graph\nmatching in pattern recognition,” Int. J. Pattern Recognit. Artif. Intell.,\nvol. 18, no. 3, pp. 265–268, 2004.\n[76] A. Agarwal, S. Chakrabarti, and S. Aggarwal, “Learning to rank net-\nworked entities,” in Proc. 12th ACM SIGKDD Int. Conf. Knowl. Disc.\nData Mining, New York, 2006, pp. 14–23.\n[77] V. Di Massa, G. Monfardini, L. Sarti, F. Scarselli, M. Maggini, and\nM. Gori, “A comparison between recursive neural networks and graph\nneural networks,” in Proc. Int. Joint Conf. Neural Netw., Jul. 2006, pp.\n778–785.\n[78] G. Monfardini, V. Di Massa, F. Scarselli, and M. Gori, “Graph neural\nnetworks for object localization,” in Proc. 17th Eur. Conf. Artif. Intell.,\nAug. 2006, pp. 665–670.\n[79] F. Scarselli, S. Yong, M. Gori, M. Hagenbuchner, A. C. Tsoi, and M.\nMaggini, “Graph neural networks for ranking web pages,” in Proc.\nIEEE/WIC/ACM Conf. Web Intelligence, 2005, pp. 666–672.\n[80] W. Uwents, G. Monfardini, H. Blockeel, F. Scarselli, and M. Gori,\n“Two connectionist models for graph processing: An experimental\ncomparison on relational data,” in Proc. Eur. Conf. Mach. Learn.,\n2006, pp. 213–220.\n[81] S. Yong, M. Hagenbuchner, F. Scarselli, A. C. Tsoi, and M. Gori, “Doc-\nument mining using graph neural networks,” in Proc. 5th Int. Work-\nshop Initiative Evaluat. XML Retrieval, N. Fuhr, M. Lalmas, and A.\nTrotman, Eds., 2007, pp. 458–472.\n[82] F. Scarselli and G. Monfardini, The GNN Toolbox, [Online]. Avail-\nable: http://airgroup.dii.unisi.it/projects/GraphNeuralNetwork/down-\nload.htm\n[83] A. K. Debnath, R. Lopex de Compandre, G. Debnath, A. Schusterman,\nand C. Hansch, “Structure-activity relationship of mutagenic aromatic\nand heteroaromatic nitro compounds. correlation with molecular or-\nbital energies and hydrophobicity,” J. Med. Chem., vol. 34, no. 2, pp.\n786–797, 1991.\n[84] S. Kramer and L. De Raedt, “Feature construction with version spaces\nfor biochemical applications,” in Proc. 18th Int. Conf. Mach. Learn.,\n2001, pp. 258–265.\n[85] J. Quinlan and R. Cameron-Jones, “FOIL: A midterm report,” in Proc.\nEur. Conf. Mach. Learn., 1993, pp. 3–20.\n[86] J. Quinlan, “Boosting ﬁrst-order learning,” in Lecture Notes in Com-\nputer Science.\nBerlin, Germany: Springer-Verlag, 1996, vol. 1160,\np. 143.\n[87] J. Ramon, “Clustering and instance based learning in ﬁrst order logic,”\nPh.D. dissertation, Dept. Comput. Sci., K.U. Leuven, Leuven, Bel-\ngium, 2002.\n[88] M. Kirsten, “Multirelational distance-based clustering,” Ph.D. disser-\ntation, Schl. Comput. Sci., Otto-von-Guericke Univ., Magdeburg, Ger-\nmany, 2002.\n[89] M. Krogel, S. Rawles, F. Zelezny, P. Flach, N. Lavrac, and S.\nWrobel, “Comparative evaluation of approaches to propositionaliza-\ntion,” in Proc. 13th Int. Conf. Inductive Logic Programm., 2003,\npp. 197–214.\n[90] S. Muggleton, “Machine learning for systems biology,” in Proc. 15th\nInt. Conf. Inductive Logic Programm., Bonn, Germany, Aug. 10 – 13,\n2005, pp. 416–423.\n[91] A. Woz´nica, A. Kalousis, and M. Hilario, “Matching based kernels for\nlabeled graphs,” in Proc. Int. Workshop Mining Learn. Graphs/ECML/\nPKDD, T. Gärtner, G. Garriga, and T. Meinl, Eds., 2006, pp. 97–108.\n[92] L. De Raedt and H. Blockeel, “Using logical decision trees for clus-\ntering,” in Lecture Notes in Artiﬁcial Intelligence.\nBerlin, Germany:\nSpringer-Verlag, 1997, vol. 1297, pp. 133–141.\n[93] M. Diligenti, M. Gori, and M. Maggini, “Web page scoring systems for\nhorizontal and vertical search,” in Proc. 11th World Wide Web Conf.,\n2002, pp. 508–516.\n[94] T. H. Haveliwala, “Topic sensitive pagerank,” in Proc. 11th World Wide\nWeb Conf., 2002, pp. 517–526.\n[95] G. Jeh and J. Widom, “Scaling personalized web search,” in Proc. 12th\nWorld Wide Web Conf., May 20–24, 2003, pp. 271–279.\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n\n\n80\nIEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 1, JANUARY 2009\n[96] F. Scarselli, A. C. Tsoi, and M. Hagenbuchner, “Computing person-\nalized pageranks,” in Proc. 12th World Wide Web Conf., 2004, pp.\n282–283.\nFranco Scarselli received the Laurea degree with\nhonors in computer science from the University of\nPisa, Pisa, Italy, in 1989 and the Ph.D. degree in\ncomputer science and automation engineering from\nthe University of Florence, Florence, Italy, in 1994.\nHe has been supported by foundations of private\nand public companies and by a postdoctoral of the\nUniversity of Florence. In 1999, he moved to the Uni-\nversity of Siena, Siena, Italy, where he was initially a\nResearch Associate and is currently an Associate Pro-\nfessor at the Department of Information Engineering.\nHe is the author of more than 50 journal and conference papers and has been\nhas been involved in several research projects, founded by public institutions\nand private companies, focused on software engineering, machine learning, and\ninformation retrieval. His current theoretical research activity is mainly in the\nﬁeld of machine learning with a particular focus on adaptive processing of data\nstructures, neural networks, and approximation theory. His research interests\ninclude also image understanding, information retrieval, and web applications.\nMarco Gori (S’88–M’91–SM’97–F’01) received the\nPh.D. degree from University di Bologna, Bologna,\nItaly, in 1990, while working in part as a visiting stu-\ndent at the School of Computer Science, McGill Uni-\nversity, Montréal, QC, Canada.\nIn 1992, he became an Associate Professor of\nComputer Science at Università di Firenze and, in\nNovember 1995, he joint the Università di Siena,\nSiena, Italy, where he is currently Full Professor of\nComputer Science. His main interests are in machine\nlearning with applications to pattern recognition,\nweb mining, and game playing. He is especially interested in the formulation of\nrelational machine learning schemes in the continuum setting. He is the leader\nof the WebCrow project for automatic solving of crosswords that outperformed\nhuman competitors in an ofﬁcial competition taken place within the 2006\nEuropean Conference on Artiﬁcial Intelligence. He is coauthor of the book\nWeb Dragons: Inside the Myths of Search Engines Technologies (San Mateo:\nMorgan Kauffman, 2006).\nDr. Gori serves (has served) as an Associate Editor of a number of technical\njournals related to his areas of expertise and he has been the recipient of best\npaper awards and keynote speakers in a number of international conferences. He\nwas the Chairman of the Italian Chapter of the IEEE Computational Intelligence\nSociety and the President of the Italian Association for Artiﬁcial Intelligence. He\nis a Fellow of the European Coordinating Committee for Artiﬁcial Intelligence.\nAh Chung Tsoi received the Higher Diploma degree\nin electronic engineering from the Hong Kong Tech-\nnical College, Hong Kong, in 1969 and the M.Sc. de-\ngree in electronic control engineering and the Ph.D.\ndegree in control engineering from University of Sal-\nford, Salford, U.K., in 1970 and 1972, respectively.\nHe was a Postdoctoral Fellow at the Inter-Univer-\nsity Institute of Engineering Control at University\nCollege of North Wales, Bangor, North Wales and a\nLecturer at Paisley College of Technology, Paisley,\nScotland. He was a Senior Lecturer in Electrical\nEngineering in the Department of Electrical Engineering, University of Auck-\nland, New Zealand, and a Senior Lecturer in Electrical Engineering, University\nCollege University of New South Wales, Australia, for ﬁve years. He then\nserved as Professor of Electrical Engineering at University of Queensland,\nAustralia; Dean, and simultaneously, Director of Information Technology\nServices, and then foundation Pro-Vice Chancellor (Information Technology\nand Communications) at University of Wollongong, before joining the Aus-\ntralian Research Council as an Executive Director, Mathematics, Information\nand Communications Sciences. He was Director, Monash e-Research Centre,\nMonash University, Melbourne, Australia. In April 2007, became a Vice\nPresident (Research and Institutional Advancement), Hong Kong Baptist\nUniversity, Hong Kong. In recent years, he has been working in the area of\nartiﬁcial intelligence in particular neural networks and fuzzy systems. He has\npublished in neural network literature. Recently, he has been working in the\napplication of neural networks to graph domains, with applications to the world\nwide web searching, and ranking problems, and subgraph matching problem.\nMarkus Hagenbuchner (M’02) received the B.Sc.\ndegree (with honors) and the Ph.D. degree in com-\nputer science from University of Wollongong, Wol-\nlongong, Australia, in 1996 and 2002, respectively.\nCurrently, he is a Lecturer at Faculty of In-\nformatics, University of Wollongong. His main\nresearch activities are in the area of machine learning\nwith special focus on supervised and unsupervised\nmethods for the graph structured domain. His\ncontribution to the development of a self-organizing\nmap for graphs led to winning the international\ncompetition on document mining on several occasions.\nGabriele Monfardini received the Laurea degree\nand the Ph.D. degree in information engineering\nfrom the University of Siena, Siena, Italy, in 2003\nand 2007, respectively.\nCurrently, he is Contract Professor at the School\nof Engineering, University of Siena. His main\nresearch interests include neural networks, adaptive\nprocessing of structured data, image analysis, and\npattern recognition.\nAuthorized licensed use limited to: ULAKBIM UASL - Firat Universitesi. Downloaded on April 14,2025 at 16:25:20 UTC from IEEE Xplore.  Restrictions apply. \n"
}