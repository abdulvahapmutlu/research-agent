{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87927ab4-8729-4e6f-bc06-abdbd24e583a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading JSON from: C:\\Users\\offic\\AGENT\\data\\text_json\n",
      "Index will be saved to: C:\\Users\\offic\\AGENT\\retriever\\faiss_index\n",
      "\n",
      "Loaded 67 documents.\n",
      "\n",
      "Splitting into ~500-token chunks (+50 overlap)\n",
      "Created 2454 token chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max tokens/chunk: 635, mean: 463.3\n",
      "\n",
      "Initializing embeddings model (CPU): sentence-transformers/all-mpnet-base-v2\n",
      "Building FAISS index (this may take a few minutes)...\n",
      "✅ Index built & saved in 1138.7s\n",
      "\n",
      "Reloading FAISS index with pickle deserialization enabled\n",
      "Index loaded. Running sample query...\n",
      "\n",
      "Top 5 chunks for query:\n",
      "1. 44 ± 0.77 60.60 ± 0.71 Prototypical Networks (Snell et al., 2017) conv (64)×4 49.42 ± 0.78 68.20 ± 0.66 MAML (Finn et al., 2017) conv (32)×4 48.70 ± 1.84 63.11 ± 0.92 R2D2 (Bertinetto et al., 2018) co…\n",
      "\n",
      "2. \u001b\u000f\u0003V \u0015 Output Input Input Output Output + Path A Path B Figure 1: The architecture of ResNet-50. The convolution kernel size, output channel size and stride size (default is 1) are illustrated, simila…\n",
      "\n",
      "3.  82.32±0.14 71.66±0.23 85.50±0.15 DMF [66] ResNet-12 12.4 M 67.76±0.46 82.71±0.31 71.89±0.52 85.96±0.35 InfoPatch [39] ResNet-12 12.4 M 67.67±0.45 82.44±0.31 - - BML [77] ResNet-12 12.4 M 67.04±0.63 8…\n",
      "\n",
      "4. ×1, 1024  ×36 conv5 x 7×7 \u0014 3×3, 512 3×3, 512 \u0015 ×2 \u0014 3×3, 512 3×3, 512 \u0015 ×3   1×1, 512 3×3, 512 1×1, 2048  ×3   1×1, 512 3×3, 512 1×1, 2048  ×3   1×1, 512 3×3, 512 1×1, 2048  ×3 1×1 aver…\n",
      "\n",
      "5. -layer ResNet with more than 40M parameters. Similar trends can be observed from the right panel, which plots the validation error as a function of the number of FLOPs: a DenseNet that requires as muc…\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "retriever_embeddings.py\n",
    "\n",
    "1) Load JSON‑extracted paper texts from data/text_json/\n",
    "2) Split into ~500‑token chunks via TokenTextSplitter(encoding_name=\"cl100k_base\")\n",
    "3) Embed chunks with sentence-transformers/all-mpnet-base-v2 (CPU)\n",
    "4) Build & save a FAISS index under retriever/faiss_index/\n",
    "5) Reload the index and run a sample similarity search\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# ─── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "\n",
    "TEXT_JSON_DIR      = r\"C:\\Users\\offic\\AGENT\\data\\text_json\"\n",
    "INDEX_DIR          = r\"C:\\Users\\offic\\AGENT\\retriever\\faiss_index\"\n",
    "EMBED_MODEL        = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "TOKEN_CHUNK_SIZE    = 500\n",
    "TOKEN_CHUNK_OVERLAP = 50\n",
    "\n",
    "EMBED_BATCH_SIZE    = 64\n",
    "\n",
    "# ─── PREPARE ───────────────────────────────────────────────────────────────────\n",
    "\n",
    "os.makedirs(INDEX_DIR, exist_ok=True)\n",
    "print(f\"Reading JSON from: {TEXT_JSON_DIR}\")\n",
    "print(f\"Index will be saved to: {INDEX_DIR}\\n\")\n",
    "\n",
    "# ─── STEP 1: LOAD DOCUMENTS ────────────────────────────────────────────────────\n",
    "\n",
    "docs = []\n",
    "for fname in sorted(os.listdir(TEXT_JSON_DIR)):\n",
    "    if fname.endswith(\".json\"):\n",
    "        path = os.path.join(TEXT_JSON_DIR, fname)\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            docs.append(data.get(\"full_text\", \"\"))\n",
    "print(f\"Loaded {len(docs)} documents.\\n\")\n",
    "\n",
    "# ─── STEP 2: TOKEN‑BASED SPLITTING ──────────────────────────────────────────────\n",
    "\n",
    "print(f\"Splitting into ~{TOKEN_CHUNK_SIZE}-token chunks (+{TOKEN_CHUNK_OVERLAP} overlap)\")\n",
    "splitter = TokenTextSplitter(\n",
    "    encoding_name=\"cl100k_base\",\n",
    "    chunk_size=TOKEN_CHUNK_SIZE,\n",
    "    chunk_overlap=TOKEN_CHUNK_OVERLAP,\n",
    ")\n",
    "\n",
    "chunks = []\n",
    "for d in docs:\n",
    "    chunks.extend(splitter.split_text(d))\n",
    "print(f\"Created {len(chunks)} token chunks.\")\n",
    "\n",
    "# (Optional) inspect chunk sizes\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL)\n",
    "lengths = [len(tokenizer.encode(c)) for c in chunks]\n",
    "print(f\"Max tokens/chunk: {max(lengths)}, mean: {sum(lengths)/len(lengths):.1f}\\n\")\n",
    "\n",
    "# ─── STEP 3: EMBEDDINGS & FAISS INDEX ──────────────────────────────────────────\n",
    "\n",
    "print(\"Initializing embeddings model (CPU):\", EMBED_MODEL)\n",
    "emb = HuggingFaceEmbeddings(\n",
    "    model_name=EMBED_MODEL,\n",
    "    encode_kwargs={\"batch_size\": EMBED_BATCH_SIZE},\n",
    ")\n",
    "\n",
    "print(\"Building FAISS index (this may take a few minutes)...\")\n",
    "t0 = time.time()\n",
    "vectorstore = FAISS.from_texts(chunks, emb)\n",
    "vectorstore.save_local(INDEX_DIR)\n",
    "print(f\"✅ Index built & saved in {time.time() - t0:.1f}s\\n\")\n",
    "\n",
    "# ─── STEP 4: LOAD & TEST RETRIEVAL ─────────────────────────────────────────────\n",
    "\n",
    "print(\"Reloading FAISS index with pickle deserialization enabled\")\n",
    "vs = FAISS.load_local(\n",
    "    INDEX_DIR,\n",
    "    emb,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "print(\"Index loaded. Running sample query...\")\n",
    "\n",
    "query = \"Which architectures used ResNet?\"\n",
    "results = vs.similarity_search(query, k=5)\n",
    "\n",
    "print(\"\\nTop 5 chunks for query:\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    snippet = doc.page_content.replace(\"\\n\", \" \")\n",
    "    print(f\"{i}. {snippet[:200]}…\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ade08a-70fe-444f-ae46-1414ba8c67e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
