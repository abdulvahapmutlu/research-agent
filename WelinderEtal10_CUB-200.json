{
  "filename": "WelinderEtal10_CUB-200.pdf",
  "num_pages": 16,
  "pages": [
    "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/46572499\nCaltech-UCSD Birds 200\nArticle · September 2010\nSource: OAI\nCITATIONS\n2,090\nREADS\n6,772\n7 authors, including:\nTakeshi Mita\nToshiba Corporation\n24 PUBLICATIONS   2,637 CITATIONS   \nSEE PROFILE\nSerge Belongie\nCornell University\n351 PUBLICATIONS   176,011 CITATIONS   \nSEE PROFILE\nAll content following this page was uploaded by Serge Belongie on 22 May 2014.\nThe user has requested enhancement of the downloaded file.\n",
    "Caltech-UCSD Birds 200\nPeter Welinder†\nSteve Branson⋆\nTakeshi Mita†\nCatherine Wah⋆\nFlorian Schroff⋆\nSerge Belongie⋆\n⋆University of California, San Diego\nPietro Perona†\n† California Institute of Technology\nAbstract\nCaltech-UCSD Birds 200 (CUB-200) is a challenging image dataset annotated\nwith 200 bird species. It was created to enable the study of subordinate catego-\nrization, which is not possible with other popular datasets that focus on basic level\ncategories (such as PASCAL VOC, Caltech-101, etc). The images were down-\nloaded from the website Flickr and ﬁltered by workers on Amazon Mechanical\nTurk. Each image is annotated with a bounding box, a rough bird segmentation,\nand a set of attribute labels.\nforehead_color\nblack\nblack\nblack\nbreast_pattern\nsolid\nsolid\nsolid\nbreast_color\nwhite\nwhite\nwhite\nhead_pattern\nplain\ncapped\nplain\nback_color\nwhite\nwhite\nblack\nwing_color\ngrey/white\ngrey\nwhite\nleg_color\norange\norange\norange\nsize\nmedium\nlarge\nmedium\nbill_shape\nneedle\ndagger\ndagger\nwing_shape\npointed\ntapered\nlong\n...\n...\n...\n...\nprimary_color\nwhite\nwhite\nwhite\nforehead_color\nred\nred\nred\nbreast_pattern\nmulti-\ncolored\nsolid\nsolid\nbreast_color\nwhite\nwhite/red\nwhite\nhead_pattern\ncapped\ncapped\ncapped\nback_color\nwhite/\nblack\nwhite/\nblack\nwhite/\nblack\nwing_color\nwhite/\nblack\nwhite/black\nwhite/black\nleg_color\nbuff\nblack\nblack\nsize\nsmall\nmedium\nmedium\nbill_shape\nall-\npurpose\ndagger\nall-\npurpose\nwing_shape\npointed\ntapered\npointed\n...\n...\n...\n...\nprimary_color\nblack, red\nwhite, black\nwhite, \nblack\nFigure 1: Images and annotations from CUB-200. Each example image is shown with a rough\noutline (segmentation) in green. To the right of each image is a table of attributes (one per row, 11\nout of a total of 25 attributes shown), and attribute-values provided by Amazon Mechanical Turk\nworkers looking at the image. The attribute-values in the three right-most columns in the tables\nare provided by different workers (across both columns and rows). The font of the attribute-value\nindicates the conﬁdence of the worker: bold font means the worker was ‘deﬁnitely’ sure of the label,\nthin means ‘probably’, and grey means ‘guessing’.\n1\nIntroduction\nLarge-scale annotated image datasets have been instrumental for driving progress in object recogni-\ntion over the last decade. Most datasets contain a wide variety of basic level classes, such as different\nkinds of animals and inanimate objects. Examples of popular such datasets include Caltech-101 and\nCaltech-256 [4, 5], LabelMe [8], PASCAL VOC [3], and ImageNet [2]. One property shared by all\nthese datasets is that an average human being would have little difﬁculty in achieving near-perfect\nclassiﬁcation accuracy. Computer vision systems, on the other hand, still do quite poorly.\nWe introduce Caltech-UCSD Birds 200 (CUB-200), a dataset aimed at subordinate category clas-\nsiﬁcation. CUB-200 includes 6,033 annotated images of birds, belonging to 200, mostly North\nAmerican, bird species. Each image is annotated with a rough segmentation, a bounding box, and\n1\n",
    "(a)\n(b)\nFigure 2: Annotations obtained from MTurk workers. (a) Screenshot of the web-based annotation\ntool used by workers. The image to be annotated is on the right (superimposed in red is the rough\noutline provided by a worker), a template image is on the left. The worker has to assess whether\nthe right image contains a bird, and if it does, whether the species of the two birds is the ‘same’,\n‘similar’, ‘different’ or ‘difﬁcult to compare’. The worker is then asked to provide either a bounding\nbox or trace the outline of the bird on the right (details in the Section 3). (b) The resulting annotations\n(the similarity label was ‘same’ and is not shown).\nbinary attribute annotations. There is only one other dataset known to us with a similar scope, the\nFlowers dataset [6] with 102 different types of ﬂowers common in the United Kingdom. In contrast\nto the datasets mentioned above, accurately classifying more than a handful of birds is something\nonly a small proportion of people can do without access to a ﬁeld guide. Moreover, since few people\ndo well on subordinate categorization tasks, it is arguably an area where a visual recognition system\nwould be useful even if it was not perfect.\nWith CUB-200 we hope to facilitate research on applications where computer vision helps people\nclassify objects that are unknown to them. For example, if an accurate bird classiﬁer were developed,\na user could submit a photo of a recently spotted bird to query a knowledge database, such as\nWikipedia [7]. Such classiﬁers could also help to automate other areas of science1.\n2\nImage Collection\nA list of 278 bird species was compiled from an online ﬁeld guide2. Next, we downloaded all images\non the corresponding Wikipedia3 page for each species. Species with no Wikipedia article, or no\nimages on their article page, were eliminated from the list. The remaining names were fed to Flickr4\nas query terms, and up to 40 images were downloaded for each species. If a name returned less than\n20 images from the Flickr search, it was removed from the list, which left 223 species with 20 or\n1An example of something that could be automated is the Great Backyard Bird Count that crowdsources the\ncounting of bird species in North America, http://www.birdsource.org/gbbc/.\n2http://www.birdfieldguide.com/\n3http://www.wikipedia.org\n4http://www.flickr.com/\n2\n",
    "more Flickr images. We manually ensured that the example images downloaded from Wikipedia\nactually contained a bird.\nAll the Flickr images were annotated with a rough segmentation by workers on Amazon Mechanical\nTurk5 (MTurk), as described in the next section. Each image was annotated by two workers per\nimage and annotation type. The workers were shown a representative image exemplar from the\nWikipedia page of the species that was used to query Flickr to ﬁnd the image to be annotated. In\naddition to providing the annotation, they were asked to rank the similarity between the image and\nthe exemplar using the following system:\n• Same: the bird in the image looks like it is of the same species as the exemplar,\n• Similar: the bird in the image and the exemplar look similar, maybe of the same species,\n• Different: the bird in image differs from the one in the exemplar,\n• Difﬁcult: chosen if occlusion or scale differences make the comparison difﬁcult.\nFrom the annotated Flickr images, we kept only images that were labeled as ‘same’ or ‘similar’ by\nboth workers, and where there was an overlap of the bounding boxes enclosing the rough outline\nannotations; the rest of the images were eliminated. The remaining images were checked by us, so\nthat each image was reviewed by a total of three different people. After excluding all species that\nhad less than 20 Flickr images remaining, 200 species were left with a total of 6,033 images. See\nthe Appendix for example images from all species.\n3\nAnnotations\nWe collected two kinds of annotations from MTurk: rough outlines and attribute annotations, see\nFigure 1. Bounding boxes were deduced from the rough outlines.\nFor the rough segmentations, the workers were asked to draw with a thick brush to touch all the\nboundary pixels of the foreground object, see Figure 2. The rough segmentation was chosen over a\nmore detailed segmentation, such as the segmentations in [8], since the former takes shorter time for\na worker to complete, thus increasing the overall throughput.\nIn addition to location information, in another task we instructed MTurk workers to provide attribute\nannotations. We used 25 visual attributes from an online bird ﬁeld guide6, listed in Table 1. We\ncreated a user interface for MTurk workers to provide attribute annotations, see Figure 3, where the\nuser was shown the query image to the left and a set of attribute values (and explanations) to the\nright. They were also asked to provide the conﬁdence of their label in three grades: ‘deﬁnitely’ sure,\n‘probably’ sure, and ‘guessing’. We obtained ﬁve annotations per image and attribute from a total\nof 1,577 workers. Figure 4 shows how the work was distributed among the workers and Figure 5 the\nsizes of the images downloaded and the obtained annotations.\n4\nBaseline Experiments\nIn order to establish a baseline performance on the dataset, we used a nearest neighbor (NN) classi-\nﬁer to classify images from a test set using different features. We chose two simple features as the\nbaseline: image sizes and color histograms. In the case of the image sizes, we represented each im-\nage by its width and height in pixels. For the color histograms, we used 10 bins per channel (making\n103 bins in total) and then applied Principal Component Analysis (PCA) and kept only the top 128\nprincipal components. Figure 6 shows how the performance of the NN classiﬁer degrades as the\nnumber of classes in the dataset is increased. The performance of the image size features are close\nto chance at 0.6% for the 200 classes, while the color histogram features increase the performance\nto 1.7%. We also compare the NN classiﬁer to the baseline method in [1], which is the ﬁrst paper to\nuse the dataset and achieves 19% classiﬁcation performance.\nOne disadvantage of searching for images on Flickr is that images returned by a query are often\ndistributed over only a few photographers. This poses a problem because it is quite common that a\n5http://www.mturk.com\n6http://www.whatbird.com\n3\n",
    "Attribute\nValues\nCrown color\nblue, black, orange, buff, brown, grey, white, red, pink, rufous, iridescent, yel-\nlow, olive, purple, green\nNape color\nwhite, black, brown, buff, grey, yellow, red, orange, iridescent, olive, green,\nblue, rufous, pink, purple\nBill shape\ncone, all-purpose, dagger, hooked seabird, hooked, curved (up or down), spatu-\nlate, needle, specialized\nHead Pattern\nmalar, eyebrow, capped, eyering, unique pattern, striped, spotted, crested,\nmasked, plain, eyeline\nBelly Pattern\nsolid, striped, spotted, multi-colored\nBelly color\ngrey, white, black, buff, yellow, brown, green, blue, iridescent, olive, orange,\nred, rufous, pink, purple\nWing shape\npointed-wings, tapered-wings, long-wings, rounded-wings, broad-wings\nShape\nperching-like, tree-clinging-like, gull-like, duck-like, swallow-like, upright-\nperching water-like, sandpiper-like, upland-ground-like, chicken-like-marsh,\npigeon-like, long-legged-like, hummingbird-like, hawk-like, owl-like\nPrimary Color\nbrown, grey, white, black, rufous, yellow, buff, red, blue, olive, iridescent,\ngreen, orange, pink, purple\nSize\nsmall (5 - 9 in), very small (3 - 5 in), medium (9 - 16 in), very large (32 - 72 in),\nlarge (16 - 32 in)\nForehead Color\ngrey, buff, red, black, orange, brown, white, blue, iridescent, rufous, green,\nyellow, pink, olive, purple\nThroat Color\nbrown, buff, black, white, orange, grey, yellow, blue, iridescent, olive, rufous,\ngreen, pink, purple, red\nEye color\nyellow, black, red, rufous, orange, white, brown, grey, olive, buff, blue, green,\npurple, pink\nUnderparts Color\ngrey, yellow, brown, white, black, buff, orange, iridescent, olive, blue, red,\ngreen, rufous, pink, purple\nBreast Pattern\nstriped, solid, spotted, multi-colored\nBreast Color\nwhite, grey, orange, yellow, buff, black, brown, rufous, green, iridescent, blue,\nred, pink, olive, purple\nUpperparts Color\nbuff, brown, grey, black, white, yellow, red, purple, olive, orange, iridescent,\ngreen, blue, rufous, pink\nBack pattern\nspotted, solid, multi-colored, striped\nBack color\nbuff, white, black, grey, brown, purple, pink, blue, iridescent, olive, rufous,\nyellow, green, red, orange\nLeg color\nwhite, blue, grey, black, orange, buff, brown, pink, yellow, red, purple, olive,\nrufous, iridescent, green\nTail pattern\nstriped, solid, spotted, multi-colored\nUnder tail color\ngrey, buff, orange, yellow, black, brown, white, rufous, olive, iridescent, blue,\ngreen, red, purple, pink\nUpper tail color\nbrown, black, grey, buff, white, yellow, rufous, olive, blue, iridescent, orange,\ngreen, red, pink, purple\nWing Pattern\nstriped, spotted, solid, multi-colored\nWing Color\nblack, buff, grey, white, brown, yellow, purple, iridescent, blue, olive, rufous,\norange, red, green, pink\nTable 1: Multi-valued bird attributes. For each image, we asked workers to select the values that\nwere most appropriate for the attribute in question.\nphotographer has taken many images of the same individual bird in a very short time period, resulting\nin near-identical images in the Flickr search results. Thus, if a large proportion of the images in a\nclass come from one photographer, a simple nearest neighbor based method will perform artiﬁcially\nwell on the classiﬁcation task. To overcome this problem, for each species we chose a date that split\nthe images into roughly equal-sized sets: the images before the date to be used as training set and the\nimages after the date to be used as test set. We strongly suggest that our dataset is always used this\nway. Different choices of the training-testing sets will likely produce vastly different classiﬁcation\nperformance ﬁgures. We have released the training/test set splits on the CUB-200 project website.\n4\n",
    "Figure 3: The interface used by MTurk workers to provide attribute labels. The query image is\nshown to the left and the choice of attribute values on the right in each diagram.\n0\n1000\n2000\n3000\n4000\n5000\n6000\n10\n0\n10\n1\n10\n2\n10\n3\nnumber of images annotated by worker\nnumber of workers\n(a) worker activity\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\nsorted workers\nnumber of images labeled\n(b) sorted worker activity\nFigure 4: The distribution of activity of the MTurk workers. (a) A histogram of the number of\nimages annotated per worker. (b) All workers sorted by the number of labels they provided.\n5\nConclusion\nCUB-2007 has a total of 6, 033 images allocated over 200 (mostly North American) bird species,\nsee Figure 5. The large number of categories should make it an interesting dataset for subordinate\ncategorization. Moreover, since it is annotated with bounding boxes, rough segmentations and at-\ntribute labels, it is also ideally suited for benchmarking systems where the users take an active part\nin the recognition process, as demonstrated in [1].\nReferences\n[1] Steve Branson, Catherine Wah, Florian Schroff, Boris Babenko, Peter Welinder, Pietro Perona,\nand Serge Belongie. Visual Recognition with Humans in the Loop. In ECCV, 2010. 3, 5, 6\n[2] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierar-\nchical Image Database. In CVPR, 2009. 1\n[3] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The Pascal Visual\nObject Classes (VOC) Challenge. International Journal of Computer Vision, 88(2):303–338,\nJune 2010. 1\n[4] L. Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object categories. IEEE Trans. Pattern\nAnalysis and Machine Intelligence, 28(4):594–611, 2006. 1\n[5] G. Grifﬁn, A. Holub, and P. Perona. Caltech-256 object category dataset. Technical Report\n7694, California Institute of Technology, 2007. 1\n7Download at http://www.vision.caltech.edu/visipedia.\n5\n",
    "20\n25\n30\n35\n0\n5\n10\n15\n20\nnumber of images per class\nnumber of classes\n(a) class size\n0.5\n1\n1.5\n2\n2.5\nx 10\n5\n0\n200\n400\n600\n800\n1000\n1200\nimage size (pixels)\nnumber of images\n(b) image size\n10\n−2\n10\n−1\n10\n0\n0\n50\n100\n150\n200\n250\nfraction of image pixels\nnumber of images\n(c) size of segmentation\nFigure 5: Distribution of images. (a) A histogram of the sizes (in number of images) per bird species.\n(b) Distribution of image sizes (in pixels) in the dataset out of 6,033 images. (c) Distribution of the\nfractions of pixels that the segmented bird occupies with respect to the total size in the image.\nMethod\nPerformance\nNN (image size)\n0.6%\nNN (color histogram)\n1.7%\nSVM (SIFT, spatial pyramid)\n19%\nFigure 6: Baseline performance on CUB-200. Left: Performance of a nearest neighbor classiﬁer\nusing image size and color histogram features as the number of classes is increased. The error bars\nshow the standard error from 10 trials where a subset of the 200 classes was randomly sampled with-\nout replacement. Also shown (labeled ‘random’) is the probability of making a correct classiﬁcation\nby chance. Right: Performance on the full dataset with 200 classes. We also compare against the\nbaseline method used in [1] which is based on a 1-vs-all SVM classiﬁer using SIFT features and a\nspatial pyramid.\n6\n",
    "[6] M-E. Nilsback and A. Zisserman.\nAutomated ﬂower classiﬁcation over a large number of\nclasses. In Proceedings of the Indian Conference on Computer Vision, Graphics and Image\nProcessing, Dec 2008. 2\n[7] P. Perona. Vision of a Visipedia. Proceedings of the IEEE, 98(8):1526 –1534, 2010. 2\n[8] B.C. Russell, A. Torralba, K.P. Murphy, and W.T. Freeman. LabelMe: A Database and Web-\nBased Tool for Image Annotation. Int. J. Comput. Vis., 77(1–3):157–173, 2008. 1, 3\n7\n",
    "Appendix: Example Images\nHere we show ﬁve random example images from each of the 200 bird categories.\nAcadian Flycatcher\nAmerican Crow\nAmerican Goldfinch\nAmerican Pipit\nAmerican Redstart\nAmerican Three toed Woodpecker\nAnna Hummingbird\nArtic Tern\nBaird Sparrow\nBaltimore Oriole\nBank Swallow\nBarn Swallow\nBay breasted Warbler\nBelted Kingfisher\nBewick Wren\nBlack and white Warbler\nBlack billed Cuckoo\nBlack capped Vireo\nBlack footed Albatross\nBlack Tern\nBlack throated Blue Warbler\nBlack throated Sparrow\nBlue Grosbeak\nBlue headed Vireo\nBlue Jay\nBlue winged Warbler\n8\n",
    "Boat tailed Grackle\nBobolink\nBohemian Waxwing\nBrandt Cormorant\nBrewer Blackbird\nBrewer Sparrow\nBronzed Cowbird\nBrown Creeper\nBrown Pelican\nBrown Thrasher\nCactus Wren\nCalifornia Gull\nCanada Warbler\nCape Glossy Starling\nCape May Warbler\nCardinal\nCarolina Wren\nCaspian Tern\nCedar Waxwing\nCerulean Warbler\nChestnut sided Warbler\nChipping Sparrow\nChuck will Widow\nClark Nutcracker\nClay colored Sparrow\nCliff Swallow\n9\n",
    "Common Raven\nCommon Tern\nCommon Yellowthroat\nCrested Auklet\nDark eyed Junco\nDowny Woodpecker\nEared Grebe\nEastern Towhee\nElegant Tern\nEuropean Goldfinch\nEvening Grosbeak\nField Sparrow\nFish Crow\nFlorida Jay\nForsters Tern\nFox Sparrow\nFrigatebird\nGadwall\nGeococcyx\nGlaucous winged Gull\nGolden winged Warbler\nGrasshopper Sparrow\nGray Catbird\nGray crowned Rosy Finch\nGray Kingbird\nGreat Crested Flycatcher\n10\n",
    "Great Grey Shrike\nGreen Jay\nGreen Kingfisher\nGreen tailed Towhee\nGreen Violetear\nGroove billed Ani\nHarris Sparrow\nHeermann Gull\nHenslow Sparrow\nHerring Gull\nHooded Merganser\nHooded Oriole\nHooded Warbler\nHorned Grebe\nHorned Lark\nHorned Puffin\nHouse Sparrow\nHouse Wren\nIndigo Bunting\nIvory Gull\nKentucky Warbler\nLaysan Albatross\nLazuli Bunting\nLe Conte Sparrow\nLeast Auklet\nLeast Flycatcher\n11\n",
    "Least Tern\nLincoln Sparrow\nLoggerhead Shrike\nLong tailed Jaeger\nLouisiana Waterthrush\nMagnolia Warbler\nMallard\nMangrove Cuckoo\nMarsh Wren\nMockingbird\nMourning Warbler\nMyrtle Warbler\nNashville Warbler\nNelson Sharp tailed Sparrow\nNighthawk\nNorthern Flicker\nNorthern Fulmar\nNorthern Waterthrush\nOlive sided Flycatcher\nOrange crowned Warbler\nOrchard Oriole\nOvenbird\nPacific Loon\nPainted Bunting\nPalm Warbler\nParakeet Auklet\n12\n",
    "Pelagic Cormorant\nPhiladelphia Vireo\nPied billed Grebe\nPied Kingfisher\nPigeon Guillemot\nPileated Woodpecker\nPine Grosbeak\nPine Warbler\nPomarine Jaeger\nPrairie Warbler\nProthonotary Warbler\nPurple Finch\nRed bellied Woodpecker\nRed breasted Merganser\nRed cockaded Woodpecker\nRed eyed Vireo\nRed faced Cormorant\nRed headed Woodpecker\nRed legged Kittiwake\nRed winged Blackbird\nRhinoceros Auklet\nRing billed Gull\nRinged Kingfisher\nRock Wren\nRose breasted Grosbeak\nRuby throated Hummingbird\n13\n",
    "Rufous Hummingbird\nRusty Blackbird\nSage Thrasher\nSavannah Sparrow\nSayornis\nScarlet Tanager\nScissor tailed Flycatcher\nScott Oriole\nSeaside Sparrow\nShiny Cowbird\nSlaty backed Gull\nSong Sparrow\nSooty Albatross\nSpotted Catbird\nSummer Tanager\nSwainson Warbler\nTennessee Warbler\nTree Sparrow\nTree Swallow\nTropical Kingbird\nVermilion Flycatcher\nVesper Sparrow\nWarbling Vireo\nWestern Grebe\nWestern Gull\nWestern Meadowlark\n14\n",
    "Western Wood Pewee\nWhip poor Will\nWhite breasted Kingfisher\nWhite breasted Nuthatch\nWhite crowned Sparrow\nWhite eyed Vireo\nWhite necked Raven\nWhite Pelican\nWhite throated Sparrow\nWilson Warbler\nWinter Wren\nWorm eating Warbler\nYellow bellied Flycatcher\nYellow billed Cuckoo\nYellow breasted Chat\nYellow headed Blackbird\nYellow throated Vireo\nYellow Warbler\n15\nView publication stats\n"
  ],
  "full_text": "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/46572499\nCaltech-UCSD Birds 200\nArticle · September 2010\nSource: OAI\nCITATIONS\n2,090\nREADS\n6,772\n7 authors, including:\nTakeshi Mita\nToshiba Corporation\n24 PUBLICATIONS   2,637 CITATIONS   \nSEE PROFILE\nSerge Belongie\nCornell University\n351 PUBLICATIONS   176,011 CITATIONS   \nSEE PROFILE\nAll content following this page was uploaded by Serge Belongie on 22 May 2014.\nThe user has requested enhancement of the downloaded file.\n\n\nCaltech-UCSD Birds 200\nPeter Welinder†\nSteve Branson⋆\nTakeshi Mita†\nCatherine Wah⋆\nFlorian Schroff⋆\nSerge Belongie⋆\n⋆University of California, San Diego\nPietro Perona†\n† California Institute of Technology\nAbstract\nCaltech-UCSD Birds 200 (CUB-200) is a challenging image dataset annotated\nwith 200 bird species. It was created to enable the study of subordinate catego-\nrization, which is not possible with other popular datasets that focus on basic level\ncategories (such as PASCAL VOC, Caltech-101, etc). The images were down-\nloaded from the website Flickr and ﬁltered by workers on Amazon Mechanical\nTurk. Each image is annotated with a bounding box, a rough bird segmentation,\nand a set of attribute labels.\nforehead_color\nblack\nblack\nblack\nbreast_pattern\nsolid\nsolid\nsolid\nbreast_color\nwhite\nwhite\nwhite\nhead_pattern\nplain\ncapped\nplain\nback_color\nwhite\nwhite\nblack\nwing_color\ngrey/white\ngrey\nwhite\nleg_color\norange\norange\norange\nsize\nmedium\nlarge\nmedium\nbill_shape\nneedle\ndagger\ndagger\nwing_shape\npointed\ntapered\nlong\n...\n...\n...\n...\nprimary_color\nwhite\nwhite\nwhite\nforehead_color\nred\nred\nred\nbreast_pattern\nmulti-\ncolored\nsolid\nsolid\nbreast_color\nwhite\nwhite/red\nwhite\nhead_pattern\ncapped\ncapped\ncapped\nback_color\nwhite/\nblack\nwhite/\nblack\nwhite/\nblack\nwing_color\nwhite/\nblack\nwhite/black\nwhite/black\nleg_color\nbuff\nblack\nblack\nsize\nsmall\nmedium\nmedium\nbill_shape\nall-\npurpose\ndagger\nall-\npurpose\nwing_shape\npointed\ntapered\npointed\n...\n...\n...\n...\nprimary_color\nblack, red\nwhite, black\nwhite, \nblack\nFigure 1: Images and annotations from CUB-200. Each example image is shown with a rough\noutline (segmentation) in green. To the right of each image is a table of attributes (one per row, 11\nout of a total of 25 attributes shown), and attribute-values provided by Amazon Mechanical Turk\nworkers looking at the image. The attribute-values in the three right-most columns in the tables\nare provided by different workers (across both columns and rows). The font of the attribute-value\nindicates the conﬁdence of the worker: bold font means the worker was ‘deﬁnitely’ sure of the label,\nthin means ‘probably’, and grey means ‘guessing’.\n1\nIntroduction\nLarge-scale annotated image datasets have been instrumental for driving progress in object recogni-\ntion over the last decade. Most datasets contain a wide variety of basic level classes, such as different\nkinds of animals and inanimate objects. Examples of popular such datasets include Caltech-101 and\nCaltech-256 [4, 5], LabelMe [8], PASCAL VOC [3], and ImageNet [2]. One property shared by all\nthese datasets is that an average human being would have little difﬁculty in achieving near-perfect\nclassiﬁcation accuracy. Computer vision systems, on the other hand, still do quite poorly.\nWe introduce Caltech-UCSD Birds 200 (CUB-200), a dataset aimed at subordinate category clas-\nsiﬁcation. CUB-200 includes 6,033 annotated images of birds, belonging to 200, mostly North\nAmerican, bird species. Each image is annotated with a rough segmentation, a bounding box, and\n1\n\n\n(a)\n(b)\nFigure 2: Annotations obtained from MTurk workers. (a) Screenshot of the web-based annotation\ntool used by workers. The image to be annotated is on the right (superimposed in red is the rough\noutline provided by a worker), a template image is on the left. The worker has to assess whether\nthe right image contains a bird, and if it does, whether the species of the two birds is the ‘same’,\n‘similar’, ‘different’ or ‘difﬁcult to compare’. The worker is then asked to provide either a bounding\nbox or trace the outline of the bird on the right (details in the Section 3). (b) The resulting annotations\n(the similarity label was ‘same’ and is not shown).\nbinary attribute annotations. There is only one other dataset known to us with a similar scope, the\nFlowers dataset [6] with 102 different types of ﬂowers common in the United Kingdom. In contrast\nto the datasets mentioned above, accurately classifying more than a handful of birds is something\nonly a small proportion of people can do without access to a ﬁeld guide. Moreover, since few people\ndo well on subordinate categorization tasks, it is arguably an area where a visual recognition system\nwould be useful even if it was not perfect.\nWith CUB-200 we hope to facilitate research on applications where computer vision helps people\nclassify objects that are unknown to them. For example, if an accurate bird classiﬁer were developed,\na user could submit a photo of a recently spotted bird to query a knowledge database, such as\nWikipedia [7]. Such classiﬁers could also help to automate other areas of science1.\n2\nImage Collection\nA list of 278 bird species was compiled from an online ﬁeld guide2. Next, we downloaded all images\non the corresponding Wikipedia3 page for each species. Species with no Wikipedia article, or no\nimages on their article page, were eliminated from the list. The remaining names were fed to Flickr4\nas query terms, and up to 40 images were downloaded for each species. If a name returned less than\n20 images from the Flickr search, it was removed from the list, which left 223 species with 20 or\n1An example of something that could be automated is the Great Backyard Bird Count that crowdsources the\ncounting of bird species in North America, http://www.birdsource.org/gbbc/.\n2http://www.birdfieldguide.com/\n3http://www.wikipedia.org\n4http://www.flickr.com/\n2\n\n\nmore Flickr images. We manually ensured that the example images downloaded from Wikipedia\nactually contained a bird.\nAll the Flickr images were annotated with a rough segmentation by workers on Amazon Mechanical\nTurk5 (MTurk), as described in the next section. Each image was annotated by two workers per\nimage and annotation type. The workers were shown a representative image exemplar from the\nWikipedia page of the species that was used to query Flickr to ﬁnd the image to be annotated. In\naddition to providing the annotation, they were asked to rank the similarity between the image and\nthe exemplar using the following system:\n• Same: the bird in the image looks like it is of the same species as the exemplar,\n• Similar: the bird in the image and the exemplar look similar, maybe of the same species,\n• Different: the bird in image differs from the one in the exemplar,\n• Difﬁcult: chosen if occlusion or scale differences make the comparison difﬁcult.\nFrom the annotated Flickr images, we kept only images that were labeled as ‘same’ or ‘similar’ by\nboth workers, and where there was an overlap of the bounding boxes enclosing the rough outline\nannotations; the rest of the images were eliminated. The remaining images were checked by us, so\nthat each image was reviewed by a total of three different people. After excluding all species that\nhad less than 20 Flickr images remaining, 200 species were left with a total of 6,033 images. See\nthe Appendix for example images from all species.\n3\nAnnotations\nWe collected two kinds of annotations from MTurk: rough outlines and attribute annotations, see\nFigure 1. Bounding boxes were deduced from the rough outlines.\nFor the rough segmentations, the workers were asked to draw with a thick brush to touch all the\nboundary pixels of the foreground object, see Figure 2. The rough segmentation was chosen over a\nmore detailed segmentation, such as the segmentations in [8], since the former takes shorter time for\na worker to complete, thus increasing the overall throughput.\nIn addition to location information, in another task we instructed MTurk workers to provide attribute\nannotations. We used 25 visual attributes from an online bird ﬁeld guide6, listed in Table 1. We\ncreated a user interface for MTurk workers to provide attribute annotations, see Figure 3, where the\nuser was shown the query image to the left and a set of attribute values (and explanations) to the\nright. They were also asked to provide the conﬁdence of their label in three grades: ‘deﬁnitely’ sure,\n‘probably’ sure, and ‘guessing’. We obtained ﬁve annotations per image and attribute from a total\nof 1,577 workers. Figure 4 shows how the work was distributed among the workers and Figure 5 the\nsizes of the images downloaded and the obtained annotations.\n4\nBaseline Experiments\nIn order to establish a baseline performance on the dataset, we used a nearest neighbor (NN) classi-\nﬁer to classify images from a test set using different features. We chose two simple features as the\nbaseline: image sizes and color histograms. In the case of the image sizes, we represented each im-\nage by its width and height in pixels. For the color histograms, we used 10 bins per channel (making\n103 bins in total) and then applied Principal Component Analysis (PCA) and kept only the top 128\nprincipal components. Figure 6 shows how the performance of the NN classiﬁer degrades as the\nnumber of classes in the dataset is increased. The performance of the image size features are close\nto chance at 0.6% for the 200 classes, while the color histogram features increase the performance\nto 1.7%. We also compare the NN classiﬁer to the baseline method in [1], which is the ﬁrst paper to\nuse the dataset and achieves 19% classiﬁcation performance.\nOne disadvantage of searching for images on Flickr is that images returned by a query are often\ndistributed over only a few photographers. This poses a problem because it is quite common that a\n5http://www.mturk.com\n6http://www.whatbird.com\n3\n\n\nAttribute\nValues\nCrown color\nblue, black, orange, buff, brown, grey, white, red, pink, rufous, iridescent, yel-\nlow, olive, purple, green\nNape color\nwhite, black, brown, buff, grey, yellow, red, orange, iridescent, olive, green,\nblue, rufous, pink, purple\nBill shape\ncone, all-purpose, dagger, hooked seabird, hooked, curved (up or down), spatu-\nlate, needle, specialized\nHead Pattern\nmalar, eyebrow, capped, eyering, unique pattern, striped, spotted, crested,\nmasked, plain, eyeline\nBelly Pattern\nsolid, striped, spotted, multi-colored\nBelly color\ngrey, white, black, buff, yellow, brown, green, blue, iridescent, olive, orange,\nred, rufous, pink, purple\nWing shape\npointed-wings, tapered-wings, long-wings, rounded-wings, broad-wings\nShape\nperching-like, tree-clinging-like, gull-like, duck-like, swallow-like, upright-\nperching water-like, sandpiper-like, upland-ground-like, chicken-like-marsh,\npigeon-like, long-legged-like, hummingbird-like, hawk-like, owl-like\nPrimary Color\nbrown, grey, white, black, rufous, yellow, buff, red, blue, olive, iridescent,\ngreen, orange, pink, purple\nSize\nsmall (5 - 9 in), very small (3 - 5 in), medium (9 - 16 in), very large (32 - 72 in),\nlarge (16 - 32 in)\nForehead Color\ngrey, buff, red, black, orange, brown, white, blue, iridescent, rufous, green,\nyellow, pink, olive, purple\nThroat Color\nbrown, buff, black, white, orange, grey, yellow, blue, iridescent, olive, rufous,\ngreen, pink, purple, red\nEye color\nyellow, black, red, rufous, orange, white, brown, grey, olive, buff, blue, green,\npurple, pink\nUnderparts Color\ngrey, yellow, brown, white, black, buff, orange, iridescent, olive, blue, red,\ngreen, rufous, pink, purple\nBreast Pattern\nstriped, solid, spotted, multi-colored\nBreast Color\nwhite, grey, orange, yellow, buff, black, brown, rufous, green, iridescent, blue,\nred, pink, olive, purple\nUpperparts Color\nbuff, brown, grey, black, white, yellow, red, purple, olive, orange, iridescent,\ngreen, blue, rufous, pink\nBack pattern\nspotted, solid, multi-colored, striped\nBack color\nbuff, white, black, grey, brown, purple, pink, blue, iridescent, olive, rufous,\nyellow, green, red, orange\nLeg color\nwhite, blue, grey, black, orange, buff, brown, pink, yellow, red, purple, olive,\nrufous, iridescent, green\nTail pattern\nstriped, solid, spotted, multi-colored\nUnder tail color\ngrey, buff, orange, yellow, black, brown, white, rufous, olive, iridescent, blue,\ngreen, red, purple, pink\nUpper tail color\nbrown, black, grey, buff, white, yellow, rufous, olive, blue, iridescent, orange,\ngreen, red, pink, purple\nWing Pattern\nstriped, spotted, solid, multi-colored\nWing Color\nblack, buff, grey, white, brown, yellow, purple, iridescent, blue, olive, rufous,\norange, red, green, pink\nTable 1: Multi-valued bird attributes. For each image, we asked workers to select the values that\nwere most appropriate for the attribute in question.\nphotographer has taken many images of the same individual bird in a very short time period, resulting\nin near-identical images in the Flickr search results. Thus, if a large proportion of the images in a\nclass come from one photographer, a simple nearest neighbor based method will perform artiﬁcially\nwell on the classiﬁcation task. To overcome this problem, for each species we chose a date that split\nthe images into roughly equal-sized sets: the images before the date to be used as training set and the\nimages after the date to be used as test set. We strongly suggest that our dataset is always used this\nway. Different choices of the training-testing sets will likely produce vastly different classiﬁcation\nperformance ﬁgures. We have released the training/test set splits on the CUB-200 project website.\n4\n\n\nFigure 3: The interface used by MTurk workers to provide attribute labels. The query image is\nshown to the left and the choice of attribute values on the right in each diagram.\n0\n1000\n2000\n3000\n4000\n5000\n6000\n10\n0\n10\n1\n10\n2\n10\n3\nnumber of images annotated by worker\nnumber of workers\n(a) worker activity\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\nsorted workers\nnumber of images labeled\n(b) sorted worker activity\nFigure 4: The distribution of activity of the MTurk workers. (a) A histogram of the number of\nimages annotated per worker. (b) All workers sorted by the number of labels they provided.\n5\nConclusion\nCUB-2007 has a total of 6, 033 images allocated over 200 (mostly North American) bird species,\nsee Figure 5. The large number of categories should make it an interesting dataset for subordinate\ncategorization. Moreover, since it is annotated with bounding boxes, rough segmentations and at-\ntribute labels, it is also ideally suited for benchmarking systems where the users take an active part\nin the recognition process, as demonstrated in [1].\nReferences\n[1] Steve Branson, Catherine Wah, Florian Schroff, Boris Babenko, Peter Welinder, Pietro Perona,\nand Serge Belongie. Visual Recognition with Humans in the Loop. In ECCV, 2010. 3, 5, 6\n[2] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierar-\nchical Image Database. In CVPR, 2009. 1\n[3] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The Pascal Visual\nObject Classes (VOC) Challenge. International Journal of Computer Vision, 88(2):303–338,\nJune 2010. 1\n[4] L. Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object categories. IEEE Trans. Pattern\nAnalysis and Machine Intelligence, 28(4):594–611, 2006. 1\n[5] G. Grifﬁn, A. Holub, and P. Perona. Caltech-256 object category dataset. Technical Report\n7694, California Institute of Technology, 2007. 1\n7Download at http://www.vision.caltech.edu/visipedia.\n5\n\n\n20\n25\n30\n35\n0\n5\n10\n15\n20\nnumber of images per class\nnumber of classes\n(a) class size\n0.5\n1\n1.5\n2\n2.5\nx 10\n5\n0\n200\n400\n600\n800\n1000\n1200\nimage size (pixels)\nnumber of images\n(b) image size\n10\n−2\n10\n−1\n10\n0\n0\n50\n100\n150\n200\n250\nfraction of image pixels\nnumber of images\n(c) size of segmentation\nFigure 5: Distribution of images. (a) A histogram of the sizes (in number of images) per bird species.\n(b) Distribution of image sizes (in pixels) in the dataset out of 6,033 images. (c) Distribution of the\nfractions of pixels that the segmented bird occupies with respect to the total size in the image.\nMethod\nPerformance\nNN (image size)\n0.6%\nNN (color histogram)\n1.7%\nSVM (SIFT, spatial pyramid)\n19%\nFigure 6: Baseline performance on CUB-200. Left: Performance of a nearest neighbor classiﬁer\nusing image size and color histogram features as the number of classes is increased. The error bars\nshow the standard error from 10 trials where a subset of the 200 classes was randomly sampled with-\nout replacement. Also shown (labeled ‘random’) is the probability of making a correct classiﬁcation\nby chance. Right: Performance on the full dataset with 200 classes. We also compare against the\nbaseline method used in [1] which is based on a 1-vs-all SVM classiﬁer using SIFT features and a\nspatial pyramid.\n6\n\n\n[6] M-E. Nilsback and A. Zisserman.\nAutomated ﬂower classiﬁcation over a large number of\nclasses. In Proceedings of the Indian Conference on Computer Vision, Graphics and Image\nProcessing, Dec 2008. 2\n[7] P. Perona. Vision of a Visipedia. Proceedings of the IEEE, 98(8):1526 –1534, 2010. 2\n[8] B.C. Russell, A. Torralba, K.P. Murphy, and W.T. Freeman. LabelMe: A Database and Web-\nBased Tool for Image Annotation. Int. J. Comput. Vis., 77(1–3):157–173, 2008. 1, 3\n7\n\n\nAppendix: Example Images\nHere we show ﬁve random example images from each of the 200 bird categories.\nAcadian Flycatcher\nAmerican Crow\nAmerican Goldfinch\nAmerican Pipit\nAmerican Redstart\nAmerican Three toed Woodpecker\nAnna Hummingbird\nArtic Tern\nBaird Sparrow\nBaltimore Oriole\nBank Swallow\nBarn Swallow\nBay breasted Warbler\nBelted Kingfisher\nBewick Wren\nBlack and white Warbler\nBlack billed Cuckoo\nBlack capped Vireo\nBlack footed Albatross\nBlack Tern\nBlack throated Blue Warbler\nBlack throated Sparrow\nBlue Grosbeak\nBlue headed Vireo\nBlue Jay\nBlue winged Warbler\n8\n\n\nBoat tailed Grackle\nBobolink\nBohemian Waxwing\nBrandt Cormorant\nBrewer Blackbird\nBrewer Sparrow\nBronzed Cowbird\nBrown Creeper\nBrown Pelican\nBrown Thrasher\nCactus Wren\nCalifornia Gull\nCanada Warbler\nCape Glossy Starling\nCape May Warbler\nCardinal\nCarolina Wren\nCaspian Tern\nCedar Waxwing\nCerulean Warbler\nChestnut sided Warbler\nChipping Sparrow\nChuck will Widow\nClark Nutcracker\nClay colored Sparrow\nCliff Swallow\n9\n\n\nCommon Raven\nCommon Tern\nCommon Yellowthroat\nCrested Auklet\nDark eyed Junco\nDowny Woodpecker\nEared Grebe\nEastern Towhee\nElegant Tern\nEuropean Goldfinch\nEvening Grosbeak\nField Sparrow\nFish Crow\nFlorida Jay\nForsters Tern\nFox Sparrow\nFrigatebird\nGadwall\nGeococcyx\nGlaucous winged Gull\nGolden winged Warbler\nGrasshopper Sparrow\nGray Catbird\nGray crowned Rosy Finch\nGray Kingbird\nGreat Crested Flycatcher\n10\n\n\nGreat Grey Shrike\nGreen Jay\nGreen Kingfisher\nGreen tailed Towhee\nGreen Violetear\nGroove billed Ani\nHarris Sparrow\nHeermann Gull\nHenslow Sparrow\nHerring Gull\nHooded Merganser\nHooded Oriole\nHooded Warbler\nHorned Grebe\nHorned Lark\nHorned Puffin\nHouse Sparrow\nHouse Wren\nIndigo Bunting\nIvory Gull\nKentucky Warbler\nLaysan Albatross\nLazuli Bunting\nLe Conte Sparrow\nLeast Auklet\nLeast Flycatcher\n11\n\n\nLeast Tern\nLincoln Sparrow\nLoggerhead Shrike\nLong tailed Jaeger\nLouisiana Waterthrush\nMagnolia Warbler\nMallard\nMangrove Cuckoo\nMarsh Wren\nMockingbird\nMourning Warbler\nMyrtle Warbler\nNashville Warbler\nNelson Sharp tailed Sparrow\nNighthawk\nNorthern Flicker\nNorthern Fulmar\nNorthern Waterthrush\nOlive sided Flycatcher\nOrange crowned Warbler\nOrchard Oriole\nOvenbird\nPacific Loon\nPainted Bunting\nPalm Warbler\nParakeet Auklet\n12\n\n\nPelagic Cormorant\nPhiladelphia Vireo\nPied billed Grebe\nPied Kingfisher\nPigeon Guillemot\nPileated Woodpecker\nPine Grosbeak\nPine Warbler\nPomarine Jaeger\nPrairie Warbler\nProthonotary Warbler\nPurple Finch\nRed bellied Woodpecker\nRed breasted Merganser\nRed cockaded Woodpecker\nRed eyed Vireo\nRed faced Cormorant\nRed headed Woodpecker\nRed legged Kittiwake\nRed winged Blackbird\nRhinoceros Auklet\nRing billed Gull\nRinged Kingfisher\nRock Wren\nRose breasted Grosbeak\nRuby throated Hummingbird\n13\n\n\nRufous Hummingbird\nRusty Blackbird\nSage Thrasher\nSavannah Sparrow\nSayornis\nScarlet Tanager\nScissor tailed Flycatcher\nScott Oriole\nSeaside Sparrow\nShiny Cowbird\nSlaty backed Gull\nSong Sparrow\nSooty Albatross\nSpotted Catbird\nSummer Tanager\nSwainson Warbler\nTennessee Warbler\nTree Sparrow\nTree Swallow\nTropical Kingbird\nVermilion Flycatcher\nVesper Sparrow\nWarbling Vireo\nWestern Grebe\nWestern Gull\nWestern Meadowlark\n14\n\n\nWestern Wood Pewee\nWhip poor Will\nWhite breasted Kingfisher\nWhite breasted Nuthatch\nWhite crowned Sparrow\nWhite eyed Vireo\nWhite necked Raven\nWhite Pelican\nWhite throated Sparrow\nWilson Warbler\nWinter Wren\nWorm eating Warbler\nYellow bellied Flycatcher\nYellow billed Cuckoo\nYellow breasted Chat\nYellow headed Blackbird\nYellow throated Vireo\nYellow Warbler\n15\nView publication stats\n"
}