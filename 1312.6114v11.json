{
  "filename": "1312.6114v11.pdf",
  "num_pages": 14,
  "pages": [
    "Auto-Encoding Variational Bayes\nDiederik P. Kingma\nMachine Learning Group\nUniversiteit van Amsterdam\ndpkingma@gmail.com\nMax Welling\nMachine Learning Group\nUniversiteit van Amsterdam\nwelling.max@gmail.com\nAbstract\nHow can we perform efﬁcient inference and learning in directed probabilistic\nmodels, in the presence of continuous latent variables with intractable posterior\ndistributions, and large datasets? We introduce a stochastic variational inference\nand learning algorithm that scales to large datasets and, under some mild differ-\nentiability conditions, even works in the intractable case. Our contributions are\ntwo-fold. First, we show that a reparameterization of the variational lower bound\nyields a lower bound estimator that can be straightforwardly optimized using stan-\ndard stochastic gradient methods. Second, we show that for i.i.d. datasets with\ncontinuous latent variables per datapoint, posterior inference can be made espe-\ncially efﬁcient by ﬁtting an approximate inference model (also called a recogni-\ntion model) to the intractable posterior using the proposed lower bound estimator.\nTheoretical advantages are reﬂected in experimental results.\n1\nIntroduction\nHow can we perform efﬁcient approximate inference and learning with directed probabilistic models\nwhose continuous latent variables and/or parameters have intractable posterior distributions? The\nvariational Bayesian (VB) approach involves the optimization of an approximation to the intractable\nposterior. Unfortunately, the common mean-ﬁeld approach requires analytical solutions of expecta-\ntions w.r.t. the approximate posterior, which are also intractable in the general case. We show how a\nreparameterization of the variational lower bound yields a simple differentiable unbiased estimator\nof the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for ef-\nﬁcient approximate posterior inference in almost any model with continuous latent variables and/or\nparameters, and is straightforward to optimize using standard stochastic gradient ascent techniques.\nFor the case of an i.i.d. dataset and continuous latent variables per datapoint, we propose the Auto-\nEncoding VB (AEVB) algorithm. In the AEVB algorithm we make inference and learning especially\nefﬁcient by using the SGVB estimator to optimize a recognition model that allows us to perform very\nefﬁcient approximate posterior inference using simple ancestral sampling, which in turn allows us\nto efﬁciently learn the model parameters, without the need of expensive iterative inference schemes\n(such as MCMC) per datapoint. The learned approximate posterior inference model can also be used\nfor a host of tasks such as recognition, denoising, representation and visualization purposes. When\na neural network is used for the recognition model, we arrive at the variational auto-encoder.\n2\nMethod\nThe strategy in this section can be used to derive a lower bound estimator (a stochastic objective\nfunction) for a variety of directed graphical models with continuous latent variables. We will restrict\nourselves here to the common case where we have an i.i.d. dataset with latent variables per datapoint,\nand where we like to perform maximum likelihood (ML) or maximum a posteriori (MAP) inference\non the (global) parameters, and variational inference on the latent variables. It is, for example,\n1\narXiv:1312.6114v11  [stat.ML]  10 Dec 2022\n",
    "x\nz\nφ\nθ\nN\nFigure 1: The type of directed graphical model under consideration. Solid lines denote the generative\nmodel pθ(z)pθ(x|z), dashed lines denote the variational approximation qφ(z|x) to the intractable\nposterior pθ(z|x). The variational parameters φ are learned jointly with the generative model pa-\nrameters θ.\nstraightforward to extend this scenario to the case where we also perform variational inference on\nthe global parameters; that algorithm is put in the appendix, but experiments with that case are left to\nfuture work. Note that our method can be applied to online, non-stationary settings, e.g. streaming\ndata, but here we assume a ﬁxed dataset for simplicity.\n2.1\nProblem scenario\nLet us consider some dataset X = {x(i)}N\ni=1 consisting of N i.i.d. samples of some continuous\nor discrete variable x. We assume that the data are generated by some random process, involving\nan unobserved continuous random variable z. The process consists of two steps: (1) a value z(i)\nis generated from some prior distribution pθ∗(z); (2) a value x(i) is generated from some condi-\ntional distribution pθ∗(x|z). We assume that the prior pθ∗(z) and likelihood pθ∗(x|z) come from\nparametric families of distributions pθ(z) and pθ(x|z), and that their PDFs are differentiable almost\neverywhere w.r.t. both θ and z. Unfortunately, a lot of this process is hidden from our view: the true\nparameters θ∗as well as the values of the latent variables z(i) are unknown to us.\nVery importantly, we do not make the common simplifying assumptions about the marginal or pos-\nterior probabilities. Conversely, we are here interested in a general algorithm that even works efﬁ-\nciently in the case of:\n1. Intractability:\nthe case where the integral of the marginal likelihood pθ(x)\n=\nR\npθ(z)pθ(x|z) dz is intractable (so we cannot evaluate or differentiate the marginal like-\nlihood), where the true posterior density pθ(z|x) = pθ(x|z)pθ(z)/pθ(x) is intractable\n(so the EM algorithm cannot be used), and where the required integrals for any reason-\nable mean-ﬁeld VB algorithm are also intractable. These intractabilities are quite common\nand appear in cases of moderately complicated likelihood functions pθ(x|z), e.g. a neural\nnetwork with a nonlinear hidden layer.\n2. A large dataset: we have so much data that batch optimization is too costly; we would like\nto make parameter updates using small minibatches or even single datapoints. Sampling-\nbased solutions, e.g. Monte Carlo EM, would in general be too slow, since it involves a\ntypically expensive sampling loop per datapoint.\nWe are interested in, and propose a solution to, three related problems in the above scenario:\n1. Efﬁcient approximate ML or MAP estimation for the parameters θ. The parameters can be\nof interest themselves, e.g. if we are analyzing some natural process. They also allow us to\nmimic the hidden random process and generate artiﬁcial data that resembles the real data.\n2. Efﬁcient approximate posterior inference of the latent variable z given an observed value x\nfor a choice of parameters θ. This is useful for coding or data representation tasks.\n3. Efﬁcient approximate marginal inference of the variable x. This allows us to perform all\nkinds of inference tasks where a prior over x is required. Common applications in computer\nvision include image denoising, inpainting and super-resolution.\n2\n",
    "For the purpose of solving the above problems, let us introduce a recognition model qφ(z|x): an\napproximation to the intractable true posterior pθ(z|x). Note that in contrast with the approximate\nposterior in mean-ﬁeld variational inference, it is not necessarily factorial and its parameters φ are\nnot computed from some closed-form expectation. Instead, we’ll introduce a method for learning\nthe recognition model parameters φ jointly with the generative model parameters θ.\nFrom a coding theory perspective, the unobserved variables z have an interpretation as a latent\nrepresentation or code. In this paper we will therefore also refer to the recognition model qφ(z|x)\nas a probabilistic encoder, since given a datapoint x it produces a distribution (e.g. a Gaussian)\nover the possible values of the code z from which the datapoint x could have been generated. In a\nsimilar vein we will refer to pθ(x|z) as a probabilistic decoder, since given a code z it produces a\ndistribution over the possible corresponding values of x.\n2.2\nThe variational bound\nThe marginal likelihood is composed of a sum over the marginal likelihoods of individual datapoints\nlog pθ(x(1), · · · , x(N)) = PN\ni=1 log pθ(x(i)), which can each be rewritten as:\nlog pθ(x(i)) = DKL(qφ(z|x(i))||pθ(z|x(i))) + L(θ, φ; x(i))\n(1)\nThe ﬁrst RHS term is the KL divergence of the approximate from the true posterior. Since this\nKL-divergence is non-negative, the second RHS term L(θ, φ; x(i)) is called the (variational) lower\nbound on the marginal likelihood of datapoint i, and can be written as:\nlog pθ(x(i)) ≥L(θ, φ; x(i)) = Eqφ(z|x) [−log qφ(z|x) + log pθ(x, z)]\n(2)\nwhich can also be written as:\nL(θ, φ; x(i)) = −DKL(qφ(z|x(i))||pθ(z)) + Eqφ(z|x(i))\nh\nlog pθ(x(i)|z)\ni\n(3)\nWe want to differentiate and optimize the lower bound L(θ, φ; x(i)) w.r.t. both the variational\nparameters φ and generative parameters θ. However, the gradient of the lower bound w.r.t. φ\nis a bit problematic. The usual (na¨ıve) Monte Carlo gradient estimator for this type of problem\nis: ∇φEqφ(z) [f(z)] = Eqφ(z)\n\u0002\nf(z)∇qφ(z) log qφ(z)\n\u0003\n≃1\nL\nPL\nl=1 f(z)∇qφ(z(l)) log qφ(z(l)) where\nz(l) ∼qφ(z|x(i)). This gradient estimator exhibits exhibits very high variance (see e.g. [BJP12])\nand is impractical for our purposes.\n2.3\nThe SGVB estimator and AEVB algorithm\nIn this section we introduce a practical estimator of the lower bound and its derivatives w.r.t. the\nparameters. We assume an approximate posterior in the form qφ(z|x), but please note that the\ntechnique can be applied to the case qφ(z), i.e. where we do not condition on x, as well. The fully\nvariational Bayesian method for inferring a posterior over the parameters is given in the appendix.\nUnder certain mild conditions outlined in section 2.4 for a chosen approximate posterior qφ(z|x) we\ncan reparameterize the random variable ez ∼qφ(z|x) using a differentiable transformation gφ(ϵ, x)\nof an (auxiliary) noise variable ϵ:\nez = gφ(ϵ, x)\nwith\nϵ ∼p(ϵ)\n(4)\nSee section 2.4 for general strategies for chosing such an approriate distribution p(ϵ) and function\ngφ(ϵ, x). We can now form Monte Carlo estimates of expectations of some function f(z) w.r.t.\nqφ(z|x) as follows:\nEqφ(z|x(i)) [f(z)] = Ep(ϵ)\nh\nf(gφ(ϵ, x(i)))\ni\n≃1\nL\nL\nX\nl=1\nf(gφ(ϵ(l), x(i)))\nwhere\nϵ(l) ∼p(ϵ) (5)\nWe apply this technique to the variational lower bound (eq. (2)), yielding our generic Stochastic\nGradient Variational Bayes (SGVB) estimator eLA(θ, φ; x(i)) ≃L(θ, φ; x(i)):\neLA(θ, φ; x(i)) = 1\nL\nL\nX\nl=1\nlog pθ(x(i), z(i,l)) −log qφ(z(i,l)|x(i))\nwhere\nz(i,l) = gφ(ϵ(i,l), x(i))\nand\nϵ(l) ∼p(ϵ)\n(6)\n3\n",
    "Algorithm 1 Minibatch version of the Auto-Encoding VB (AEVB) algorithm. Either of the two\nSGVB estimators in section 2.3 can be used. We use settings M = 100 and L = 1 in experiments.\nθ, φ ←Initialize parameters\nrepeat\nXM ←Random minibatch of M datapoints (drawn from full dataset)\nϵ ←Random samples from noise distribution p(ϵ)\ng ←∇θ,φ eLM(θ, φ; XM, ϵ) (Gradients of minibatch estimator (8))\nθ, φ ←Update parameters using gradients g (e.g. SGD or Adagrad [DHS10])\nuntil convergence of parameters (θ, φ)\nreturn θ, φ\nOften, the KL-divergence DKL(qφ(z|x(i))||pθ(z)) of eq. (3) can be integrated analytically (see\nappendix B), such that only the expected reconstruction error Eqφ(z|x(i))\n\u0002\nlog pθ(x(i)|z)\n\u0003\nrequires\nestimation by sampling. The KL-divergence term can then be interpreted as regularizing φ, encour-\naging the approximate posterior to be close to the prior pθ(z). This yields a second version of the\nSGVB estimator eLB(θ, φ; x(i)) ≃L(θ, φ; x(i)), corresponding to eq. (3), which typically has less\nvariance than the generic estimator:\neLB(θ, φ; x(i)) = −DKL(qφ(z|x(i))||pθ(z)) + 1\nL\nL\nX\nl=1\n(log pθ(x(i)|z(i,l)))\nwhere\nz(i,l) = gφ(ϵ(i,l), x(i))\nand\nϵ(l) ∼p(ϵ)\n(7)\nGiven multiple datapoints from a dataset X with N datapoints, we can construct an estimator of the\nmarginal likelihood lower bound of the full dataset, based on minibatches:\nL(θ, φ; X) ≃eLM(θ, φ; XM) = N\nM\nM\nX\ni=1\neL(θ, φ; x(i))\n(8)\nwhere the minibatch XM = {x(i)}M\ni=1 is a randomly drawn sample of M datapoints from the\nfull dataset X with N datapoints. In our experiments we found that the number of samples L\nper datapoint can be set to 1 as long as the minibatch size M was large enough, e.g. M = 100.\nDerivatives ∇θ,φ eL(θ; XM) can be taken, and the resulting gradients can be used in conjunction\nwith stochastic optimization methods such as SGD or Adagrad [DHS10]. See algorithm 1 for a\nbasic approach to compute the stochastic gradients.\nA connection with auto-encoders becomes clear when looking at the objective function given at\neq. (7). The ﬁrst term is (the KL divergence of the approximate posterior from the prior) acts as a\nregularizer, while the second term is a an expected negative reconstruction error. The function gφ(.)\nis chosen such that it maps a datapoint x(i) and a random noise vector ϵ(l) to a sample from the\napproximate posterior for that datapoint: z(i,l) = gφ(ϵ(l), x(i)) where z(i,l) ∼qφ(z|x(i)). Subse-\nquently, the sample z(i,l) is then input to function log pθ(x(i)|z(i,l)), which equals the probability\ndensity (or mass) of datapoint x(i) under the generative model, given z(i,l). This term is a negative\nreconstruction error in auto-encoder parlance.\n2.4\nThe reparameterization trick\nIn order to solve our problem we invoked an alternative method for generating samples from\nqφ(z|x). The essential parameterization trick is quite simple. Let z be a continuous random vari-\nable, and z ∼qφ(z|x) be some conditional distribution. It is then often possible to express the\nrandom variable z as a deterministic variable z = gφ(ϵ, x), where ϵ is an auxiliary variable with\nindependent marginal p(ϵ), and gφ(.) is some vector-valued function parameterized by φ.\nThis reparameterization is useful for our case since it can be used to rewrite an expectation w.r.t\nqφ(z|x) such that the Monte Carlo estimate of the expectation is differentiable w.r.t. φ. A proof\nis as follows. Given the deterministic mapping z = gφ(ϵ, x) we know that qφ(z|x) Q\ni dzi =\np(ϵ) Q\ni dϵi. Therefore1,\nR\nqφ(z|x)f(z) dz =\nR\np(ϵ)f(z) dϵ =\nR\np(ϵ)f(gφ(ϵ, x)) dϵ. It follows\n1Note that for inﬁnitesimals we use the notational convention dz = Q\ni dzi\n4\n",
    "that a differentiable estimator can be constructed:\nR\nqφ(z|x)f(z) dz ≃\n1\nL\nPL\nl=1 f(gφ(x, ϵ(l)))\nwhere ϵ(l) ∼p(ϵ). In section 2.3 we applied this trick to obtain a differentiable estimator of the\nvariational lower bound.\nTake, for example, the univariate Gaussian case: let z ∼p(z|x) = N(µ, σ2). In this case, a valid\nreparameterization is z = µ + σϵ, where ϵ is an auxiliary noise variable ϵ ∼N(0, 1). Therefore,\nEN(z;µ,σ2) [f(z)] = EN(ϵ;0,1) [f(µ + σϵ)] ≃1\nL\nPL\nl=1 f(µ + σϵ(l)) where ϵ(l) ∼N(0, 1).\nFor which qφ(z|x) can we choose such a differentiable transformation gφ(.) and auxiliary variable\nϵ ∼p(ϵ)? Three basic approaches are:\n1. Tractable inverse CDF. In this case, let ϵ ∼U(0, I), and let gφ(ϵ, x) be the inverse CDF of\nqφ(z|x). Examples: Exponential, Cauchy, Logistic, Rayleigh, Pareto, Weibull, Reciprocal,\nGompertz, Gumbel and Erlang distributions.\n2. Analogous to the Gaussian example, for any ”location-scale” family of distributions we can\nchoose the standard distribution (with location = 0, scale = 1) as the auxiliary variable\nϵ, and let g(.) = location + scale · ϵ. Examples: Laplace, Elliptical, Student’s t, Logistic,\nUniform, Triangular and Gaussian distributions.\n3. Composition: It is often possible to express random variables as different transformations\nof auxiliary variables. Examples: Log-Normal (exponentiation of normally distributed\nvariable), Gamma (a sum over exponentially distributed variables), Dirichlet (weighted\nsum of Gamma variates), Beta, Chi-Squared, and F distributions.\nWhen all three approaches fail, good approximations to the inverse CDF exist requiring computa-\ntions with time complexity comparable to the PDF (see e.g. [Dev86] for some methods).\n3\nExample: Variational Auto-Encoder\nIn this section we’ll give an example where we use a neural network for the probabilistic encoder\nqφ(z|x) (the approximation to the posterior of the generative model pθ(x, z)) and where the param-\neters φ and θ are optimized jointly with the AEVB algorithm.\nLet the prior over the latent variables be the centered isotropic multivariate Gaussian pθ(z) =\nN(z; 0, I). Note that in this case, the prior lacks parameters. We let pθ(x|z) be a multivariate\nGaussian (in case of real-valued data) or Bernoulli (in case of binary data) whose distribution pa-\nrameters are computed from z with a MLP (a fully-connected neural network with a single hidden\nlayer, see appendix C). Note the true posterior pθ(z|x) is in this case intractable. While there is\nmuch freedom in the form qφ(z|x), we’ll assume the true (but intractable) posterior takes on a ap-\nproximate Gaussian form with an approximately diagonal covariance. In this case, we can let the\nvariational approximate posterior be a multivariate Gaussian with a diagonal covariance structure2:\nlog qφ(z|x(i)) = log N(z; µ(i), σ2(i)I)\n(9)\nwhere the mean and s.d. of the approximate posterior, µ(i) and σ(i), are outputs of the encoding\nMLP, i.e. nonlinear functions of datapoint x(i) and the variational parameters φ (see appendix C).\nAs explained in section 2.4, we sample from the posterior z(i,l) ∼qφ(z|x(i)) using z(i,l) =\ngφ(x(i), ϵ(l)) = µ(i) + σ(i) ⊙ϵ(l) where ϵ(l) ∼N(0, I). With ⊙we signify an element-wise\nproduct. In this model both pθ(z) (the prior) and qφ(z|x) are Gaussian; in this case, we can use the\nestimator of eq. (7) where the KL divergence can be computed and differentiated without estimation\n(see appendix B). The resulting estimator for this model and datapoint x(i) is:\nL(θ, φ; x(i)) ≃1\n2\nJ\nX\nj=1\n\u0010\n1 + log((σ(i)\nj )2) −(µ(i)\nj )2 −(σ(i)\nj )2\u0011\n+ 1\nL\nL\nX\nl=1\nlog pθ(x(i)|z(i,l))\nwhere\nz(i,l) = µ(i) + σ(i) ⊙ϵ(l)\nand\nϵ(l) ∼N(0, I)\n(10)\nAs explained above and in appendix C, the decoding term log pθ(x(i)|z(i,l)) is a Bernoulli or Gaus-\nsian MLP, depending on the type of data we are modelling.\n2Note that this is just a (simplifying) choice, and not a limitation of our method.\n5\n",
    "4\nRelated work\nThe wake-sleep algorithm [HDFN95] is, to the best of our knowledge, the only other on-line learn-\ning method in the literature that is applicable to the same general class of continuous latent variable\nmodels. Like our method, the wake-sleep algorithm employs a recognition model that approximates\nthe true posterior. A drawback of the wake-sleep algorithm is that it requires a concurrent optimiza-\ntion of two objective functions, which together do not correspond to optimization of (a bound of)\nthe marginal likelihood. An advantage of wake-sleep is that it also applies to models with discrete\nlatent variables. Wake-Sleep has the same computational complexity as AEVB per datapoint.\nStochastic variational inference [HBWP13] has recently received increasing interest.\nRecently,\n[BJP12] introduced a control variate schemes to reduce the high variance of the na¨ıve gradient\nestimator discussed in section 2.1, and applied to exponential family approximations of the poste-\nrior. In [RGB13] some general methods, i.e. a control variate scheme, were introduced for reducing\nthe variance of the original gradient estimator. In [SK13], a similar reparameterization as in this\npaper was used in an efﬁcient version of a stochastic variational inference algorithm for learning the\nnatural parameters of exponential-family approximating distributions.\nThe AEVB algorithm exposes a connection between directed probabilistic models (trained with a\nvariational objective) and auto-encoders. A connection between linear auto-encoders and a certain\nclass of generative linear-Gaussian models has long been known. In [Row98] it was shown that PCA\ncorresponds to the maximum-likelihood (ML) solution of a special case of the linear-Gaussian model\nwith a prior p(z) = N(0, I) and a conditional distribution p(x|z) = N(x; Wz, ϵI), speciﬁcally the\ncase with inﬁnitesimally small ϵ.\nIn relevant recent work on autoencoders [VLL+10] it was shown that the training criterion of un-\nregularized autoencoders corresponds to maximization of a lower bound (see the infomax princi-\nple [Lin89]) of the mutual information between input X and latent representation Z. Maximiz-\ning (w.r.t. parameters) of the mutual information is equivalent to maximizing the conditional en-\ntropy, which is lower bounded by the expected loglikelihood of the data under the autoencoding\nmodel [VLL+10], i.e. the negative reconstrution error. However, it is well known that this recon-\nstruction criterion is in itself not sufﬁcient for learning useful representations [BCV13]. Regular-\nization techniques have been proposed to make autoencoders learn useful representations, such as\ndenoising, contractive and sparse autoencoder variants [BCV13]. The SGVB objective contains a\nregularization term dictated by the variational bound (e.g. eq. (10)), lacking the usual nuisance regu-\nlarization hyperparameter required to learn useful representations. Related are also encoder-decoder\narchitectures such as the predictive sparse decomposition (PSD) [KRL08], from which we drew\nsome inspiration. Also relevant are the recently introduced Generative Stochastic Networks [BTL13]\nwhere noisy auto-encoders learn the transition operator of a Markov chain that samples from the data\ndistribution. In [SL10] a recognition model was employed for efﬁcient learning with Deep Boltz-\nmann Machines. These methods are targeted at either unnormalized models (i.e. undirected models\nlike Boltzmann machines) or limited to sparse coding models, in contrast to our proposed algorithm\nfor learning a general class of directed probabilistic models.\nThe recently proposed DARN method [GMW13], also learns a directed probabilistic model using\nan auto-encoding structure, however their method applies to binary latent variables. Even more\nrecently, [RMW14] also make the connection between auto-encoders, directed proabilistic models\nand stochastic variational inference using the reparameterization trick we describe in this paper.\nTheir work was developed independently of ours and provides an additional perspective on AEVB.\n5\nExperiments\nWe trained generative models of images from the MNIST and Frey Face datasets3 and compared\nlearning algorithms in terms of the variational lower bound, and the estimated marginal likelihood.\nThe generative model (encoder) and variational approximation (decoder) from section 3 were used,\nwhere the described encoder and decoder have an equal number of hidden units. Since the Frey\nFace data are continuous, we used a decoder with Gaussian outputs, identical to the encoder, except\nthat the means were constrained to the interval (0, 1) using a sigmoidal activation function at the\n3Available at http://www.cs.nyu.edu/˜roweis/data.html\n6\n",
    "105\n106\n107\n108\n# Training samples evaluated\n150\n140\n130\n120\n110\n100\nL\nMNIST, Nz =3\n105\n106\n107\n108\n150\n140\n130\n120\n110\n100\nMNIST, Nz =5\n105\n106\n107\n108\n150\n140\n130\n120\n110\n100\nMNIST, Nz =10\n105\n106\n107\n108\n150\n140\n130\n120\n110\n100\nMNIST, Nz =20\n105\n106\n107\n108\n150\n140\n130\n120\n110\n100\nMNIST, Nz =200\n105\n106\n107\n108\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\nL\nFrey Face, Nz =2\nWake-Sleep (test)\nWake-Sleep (train)\nAEVB (test)\nAEVB (train)\n105\n106\n107\n108\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600 Frey Face, Nz =5\n105\n106\n107\n108\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600 Frey Face, Nz =10\n105\n106\n107\n108\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600 Frey Face, Nz =20\nFigure 2: Comparison of our AEVB method to the wake-sleep algorithm, in terms of optimizing the\nlower bound, for different dimensionality of latent space (Nz). Our method converged considerably\nfaster and reached a better solution in all experiments. Interestingly enough, more latent variables\ndoes not result in more overﬁtting, which is explained by the regularizing effect of the lower bound.\nVertical axis: the estimated average variational lower bound per datapoint. The estimator variance\nwas small (< 1) and omitted. Horizontal axis: amount of training points evaluated. Computa-\ntion took around 20-40 minutes per million training samples with a Intel Xeon CPU running at an\neffective 40 GFLOPS.\ndecoder output. Note that with hidden units we refer to the hidden layer of the neural networks of\nthe encoder and decoder.\nParameters are updated using stochastic gradient ascent where gradients are computed by differenti-\nating the lower bound estimator ∇θ,φL(θ, φ; X) (see algorithm 1), plus a small weight decay term\ncorresponding to a prior p(θ) = N(0, I). Optimization of this objective is equivalent to approxi-\nmate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower\nbound.\nWe compared performance of AEVB to the wake-sleep algorithm [HDFN95]. We employed the\nsame encoder (also called recognition model) for the wake-sleep algorithm and the variational auto-\nencoder. All parameters, both variational and generative, were initialized by random sampling from\nN(0, 0.01), and were jointly stochastically optimized using the MAP criterion. Stepsizes were\nadapted with Adagrad [DHS10]; the Adagrad global stepsize parameters were chosen from {0.01,\n0.02, 0.1} based on performance on the training set in the ﬁrst few iterations. Minibatches of size\nM = 100 were used, with L = 1 samples per datapoint.\nLikelihood lower bound\nWe trained generative models (decoders) and corresponding encoders\n(a.k.a. recognition models) having 500 hidden units in case of MNIST, and 200 hidden units in case\nof the Frey Face dataset (to prevent overﬁtting, since it is a considerably smaller dataset). The chosen\nnumber of hidden units is based on prior literature on auto-encoders, and the relative performance\nof different algorithms was not very sensitive to these choices. Figure 2 shows the results when\ncomparing the lower bounds. Interestingly, superﬂuous latent variables did not result in overﬁtting,\nwhich is explained by the regularizing nature of the variational bound.\nMarginal likelihood\nFor very low-dimensional latent space it is possible to estimate the marginal\nlikelihood of the learned generative models using an MCMC estimator. More information about the\nmarginal likelihood estimator is available in the appendix. For the encoder and decoder we again\nused neural networks, this time with 100 hidden units, and 3 latent variables; for higher dimensional\nlatent space the estimates became unreliable. Again, the MNIST dataset was used. The AEVB\nand Wake-Sleep methods were compared to Monte Carlo EM (MCEM) with a Hybrid Monte Carlo\n(HMC) [DKPR87] sampler; details are in the appendix. We compared the convergence speed for\nthe three algorithms, for a small and large training set size. Results are in ﬁgure 3.\n7\n",
    "0\n10\n20\n30\n40\n50\n60\n# Training samples evaluated (millions)\n160\n150\n140\n130\n120\n110\n100\nMarginal log-likelihood\nNtrain = 1000\n0\n10\n20\n30\n40\n50\n60\n160\n155\n150\n145\n140\n135\n130\n125\nNtrain = 50000\nWake-Sleep (train)\nWake-Sleep (test)\nMCEM (train)\nMCEM (test)\nAEVB (train)\nAEVB (test)\nFigure 3: Comparison of AEVB to the wake-sleep algorithm and Monte Carlo EM, in terms of the\nestimated marginal likelihood, for a different number of training points. Monte Carlo EM is not an\non-line algorithm, and (unlike AEVB and the wake-sleep method) can’t be applied efﬁciently for\nthe full MNIST dataset.\nVisualisation of high-dimensional data\nIf we choose a low-dimensional latent space (e.g. 2D),\nwe can use the learned encoders (recognition model) to project high-dimensional data to a low-\ndimensional manifold. See appendix A for visualisations of the 2D latent manifolds for the MNIST\nand Frey Face datasets.\n6\nConclusion\nWe have introduced a novel estimator of the variational lower bound, Stochastic Gradient VB\n(SGVB), for efﬁcient approximate inference with continuous latent variables. The proposed estima-\ntor can be straightforwardly differentiated and optimized using standard stochastic gradient meth-\nods. For the case of i.i.d. datasets and continuous latent variables per datapoint we introduce an\nefﬁcient algorithm for efﬁcient inference and learning, Auto-Encoding VB (AEVB), that learns an\napproximate inference model using the SGVB estimator. The theoretical advantages are reﬂected in\nexperimental results.\n7\nFuture work\nSince the SGVB estimator and the AEVB algorithm can be applied to almost any inference and\nlearning problem with continuous latent variables, there are plenty of future directions: (i) learning\nhierarchical generative architectures with deep neural networks (e.g. convolutional networks) used\nfor the encoders and decoders, trained jointly with AEVB; (ii) time-series models (i.e. dynamic\nBayesian networks); (iii) application of SGVB to the global parameters; (iv) supervised models\nwith latent variables, useful for learning complicated noise distributions.\n8\n",
    "References\n[BCV13]\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A re-\nview and new perspectives. 2013.\n[BJP12]\nDavid M Blei, Michael I Jordan, and John W Paisley. Variational Bayesian inference\nwith Stochastic Search. In Proceedings of the 29th International Conference on Ma-\nchine Learning (ICML-12), pages 1367–1374, 2012.\n[BTL13]\nYoshua Bengio and ´Eric Thibodeau-Laufer. Deep generative stochastic networks train-\nable by backprop. arXiv preprint arXiv:1306.1091, 2013.\n[Dev86]\nLuc Devroye. Sample-based non-uniform random variate generation. In Proceedings\nof the 18th conference on Winter simulation, pages 260–265. ACM, 1986.\n[DHS10]\nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online\nlearning and stochastic optimization. Journal of Machine Learning Research, 12:2121–\n2159, 2010.\n[DKPR87] Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. Hybrid\nmonte carlo. Physics letters B, 195(2):216–222, 1987.\n[GMW13]\nKarol Gregor, Andriy Mnih, and Daan Wierstra. Deep autoregressive networks. arXiv\npreprint arXiv:1310.8499, 2013.\n[HBWP13] Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic varia-\ntional inference. The Journal of Machine Learning Research, 14(1):1303–1347, 2013.\n[HDFN95] Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The” wake-\nsleep” algorithm for unsupervised neural networks. SCIENCE, pages 1158–1158, 1995.\n[KRL08]\nKoray Kavukcuoglu, Marc’Aurelio Ranzato, and Yann LeCun. Fast inference in sparse\ncoding algorithms with applications to object recognition. Technical Report CBLL-\nTR-2008-12-01, Computational and Biological Learning Lab, Courant Institute, NYU,\n2008.\n[Lin89]\nRalph Linsker. An application of the principle of maximum information preservation to\nlinear systems. Morgan Kaufmann Publishers Inc., 1989.\n[RGB13]\nRajesh Ranganath, Sean Gerrish, and David M Blei. Black Box Variational Inference.\narXiv preprint arXiv:1401.0118, 2013.\n[RMW14]\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.\nStochastic back-\npropagation and variational inference in deep latent gaussian models. arXiv preprint\narXiv:1401.4082, 2014.\n[Row98]\nSam Roweis. EM algorithms for PCA and SPCA. Advances in neural information\nprocessing systems, pages 626–632, 1998.\n[SK13]\nTim Salimans and David A Knowles. Fixed-form variational posterior approximation\nthrough stochastic linear regression. Bayesian Analysis, 8(4), 2013.\n[SL10]\nRuslan Salakhutdinov and Hugo Larochelle. Efﬁcient learning of deep boltzmann ma-\nchines. In International Conference on Artiﬁcial Intelligence and Statistics, pages 693–\n700, 2010.\n[VLL+10] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine\nManzagol. Stacked denoising autoencoders: Learning useful representations in a deep\nnetwork with a local denoising criterion. The Journal of Machine Learning Research,\n9999:3371–3408, 2010.\nA\nVisualisations\nSee ﬁgures 4 and 5 for visualisations of latent space and corresponding observed space of models\nlearned with SGVB.\n9\n",
    "(a) Learned Frey Face manifold\n(b) Learned MNIST manifold\nFigure 4: Visualisations of learned data manifold for generative models with two-dimensional latent\nspace, learned with AEVB. Since the prior of the latent space is Gaussian, linearly spaced coor-\ndinates on the unit square were transformed through the inverse CDF of the Gaussian to produce\nvalues of the latent variables z. For each of these values z, we plotted the corresponding generative\npθ(x|z) with the learned parameters θ.\n(a) 2-D latent space\n(b) 5-D latent space\n(c) 10-D latent space\n(d) 20-D latent space\nFigure 5: Random samples from learned generative models of MNIST for different dimensionalities\nof latent space.\nB\nSolution of −DKL(qφ(z)||pθ(z)), Gaussian case\nThe variational lower bound (the objective to be maximized) contains a KL term that can often be\nintegrated analytically. Here we give the solution when both the prior pθ(z) = N(0, I) and the\nposterior approximation qφ(z|x(i)) are Gaussian. Let J be the dimensionality of z. Let µ and σ\ndenote the variational mean and s.d. evaluated at datapoint i, and let µj and σj simply denote the\nj-th element of these vectors. Then:\nZ\nqθ(z) log p(z) dz =\nZ\nN(z; µ, σ2) log N(z; 0, I) dz\n= −J\n2 log(2π) −1\n2\nJ\nX\nj=1\n(µ2\nj + σ2\nj )\n10\n",
    "And:\nZ\nqθ(z) log qθ(z) dz =\nZ\nN(z; µ, σ2) log N(z; µ, σ2) dz\n= −J\n2 log(2π) −1\n2\nJ\nX\nj=1\n(1 + log σ2\nj )\nTherefore:\n−DKL((qφ(z)||pθ(z)) =\nZ\nqθ(z) (log pθ(z) −log qθ(z)) dz\n= 1\n2\nJ\nX\nj=1\n\u00001 + log((σj)2) −(µj)2 −(σj)2\u0001\nWhen using a recognition model qφ(z|x) then µ and s.d. σ are simply functions of x and the\nvariational parameters φ, as exempliﬁed in the text.\nC\nMLP’s as probabilistic encoders and decoders\nIn variational auto-encoders, neural networks are used as probabilistic encoders and decoders. There\nare many possible choices of encoders and decoders, depending on the type of data and model. In\nour example we used relatively simple neural networks, namely multi-layered perceptrons (MLPs).\nFor the encoder we used a MLP with Gaussian output, while for the decoder we used MLPs with\neither Gaussian or Bernoulli outputs, depending on the type of data.\nC.1\nBernoulli MLP as decoder\nIn this case let pθ(x|z) be a multivariate Bernoulli whose probabilities are computed from z with a\nfully-connected neural network with a single hidden layer:\nlog p(x|z) =\nD\nX\ni=1\nxi log yi + (1 −xi) · log(1 −yi)\nwhere y = fσ(W2 tanh(W1z + b1) + b2)\n(11)\nwhere fσ(.) is the elementwise sigmoid activation function, and where θ = {W1, W2, b1, b2} are\nthe weights and biases of the MLP.\nC.2\nGaussian MLP as encoder or decoder\nIn this case let encoder or decoder be a multivariate Gaussian with a diagonal covariance structure:\nlog p(x|z) = log N(x; µ, σ2I)\nwhere µ = W4h + b4\nlog σ2 = W5h + b5\nh = tanh(W3z + b3)\n(12)\nwhere {W3, W4, W5, b3, b4, b5} are the weights and biases of the MLP and part of θ when used\nas decoder. Note that when this network is used as an encoder qφ(z|x), then z and x are swapped,\nand the weights and biases are variational parameters φ.\nD\nMarginal likelihood estimator\nWe derived the following marginal likelihood estimator that produces good estimates of the marginal\nlikelihood as long as the dimensionality of the sampled space is low (less then 5 dimensions), and\nsufﬁcient samples are taken. Let pθ(x, z) = pθ(z)pθ(x|z) be the generative model we are sampling\nfrom, and for a given datapoint x(i) we would like to estimate the marginal likelihood pθ(x(i)).\nThe estimation process consists of three stages:\n11\n",
    "1. Sample L values {z(l)} from the posterior using gradient-based MCMC, e.g. Hybrid Monte\nCarlo, using ∇z log pθ(z|x) = ∇z log pθ(z) + ∇z log pθ(x|z).\n2. Fit a density estimator q(z) to these samples {z(l)}.\n3. Again, sample L new values from the posterior. Plug these samples, as well as the ﬁtted\nq(z), into the following estimator:\npθ(x(i)) ≃\n \n1\nL\nL\nX\nl=1\nq(z(l))\npθ(z)pθ(x(i)|z(l))\n!−1\nwhere\nz(l) ∼pθ(z|x(i))\nDerivation of the estimator:\n1\npθ(x(i)) =\nR\nq(z) dz\npθ(x(i)) =\nR\nq(z) pθ(x(i),z)\npθ(x(i),z) dz\npθ(x(i))\n=\nZ pθ(x(i), z)\npθ(x(i))\nq(z)\npθ(x(i), z) dz\n=\nZ\npθ(z|x(i))\nq(z)\npθ(x(i), z) dz\n≃1\nL\nL\nX\nl=1\nq(z(l))\npθ(z)pθ(x(i)|z(l))\nwhere\nz(l) ∼pθ(z|x(i))\nE\nMonte Carlo EM\nThe Monte Carlo EM algorithm does not employ an encoder, instead it samples from the pos-\nterior of the latent variables using gradients of the posterior computed with ∇z log pθ(z|x) =\n∇z log pθ(z) + ∇z log pθ(x|z). The Monte Carlo EM procedure consists of 10 HMC leapfrog\nsteps with an automatically tuned stepsize such that the acceptance rate was 90%, followed by 5\nweight updates steps using the acquired sample. For all algorithms the parameters were updated\nusing the Adagrad stepsizes (with accompanying annealing schedule).\nThe marginal likelihood was estimated with the ﬁrst 1000 datapoints from the train and test sets,\nfor each datapoint sampling 50 values from the posterior of the latent variables using Hybrid Monte\nCarlo with 4 leapfrog steps.\nF\nFull VB\nAs written in the paper, it is possible to perform variational inference on both the parameters θ and\nthe latent variables z, as opposed to just the latent variables as we did in the paper. Here, we’ll derive\nour estimator for that case.\nLet pα(θ) be some hyperprior for the parameters introduced above, parameterized by α. The\nmarginal likelihood can be written as:\nlog pα(X) = DKL(qφ(θ)||pα(θ|X)) + L(φ; X)\n(13)\nwhere the ﬁrst RHS term denotes a KL divergence of the approximate from the true posterior, and\nwhere L(φ; X) denotes the variational lower bound to the marginal likelihood:\nL(φ; X) =\nZ\nqφ(θ) (log pθ(X) + log pα(θ) −log qφ(θ)) dθ\n(14)\nNote that this is a lower bound since the KL divergence is non-negative; the bound equals the true\nmarginal when the approximate and true posteriors match exactly. The term log pθ(X) is composed\nof a sum over the marginal likelihoods of individual datapoints log pθ(X) = PN\ni=1 log pθ(x(i)),\nwhich can each be rewritten as:\nlog pθ(x(i)) = DKL(qφ(z|x(i))||pθ(z|x(i))) + L(θ, φ; x(i))\n(15)\n12\n",
    "where again the ﬁrst RHS term is the KL divergence of the approximate from the true posterior, and\nL(θ, φ; x) is the variational lower bound of the marginal likelihood of datapoint i:\nL(θ, φ; x(i)) =\nZ\nqφ(z|x)\n\u0010\nlog pθ(x(i)|z) + log pθ(z) −log qφ(z|x)\n\u0011\ndz\n(16)\nThe expectations on the RHS of eqs (14) and (16) can obviously be written as a sum of three separate\nexpectations, of which the second and third component can sometimes be analytically solved, e.g.\nwhen both pθ(x) and qφ(z|x) are Gaussian. For generality we will here assume that each of these\nexpectations is intractable.\nUnder certain mild conditions outlined in section (see paper) for chosen approximate posteriors\nqφ(θ) and qφ(z|x) we can reparameterize conditional samples ez ∼qφ(z|x) as\nez = gφ(ϵ, x)\nwith\nϵ ∼p(ϵ)\n(17)\nwhere we choose a prior p(ϵ) and a function gφ(ϵ, x) such that the following holds:\nL(θ, φ; x(i)) =\nZ\nqφ(z|x)\n\u0010\nlog pθ(x(i)|z) + log pθ(z) −log qφ(z|x)\n\u0011\ndz\n=\nZ\np(ϵ)\n\u0010\nlog pθ(x(i)|z) + log pθ(z) −log qφ(z|x)\n\u0011 \f\f\f\f\nz=gφ(ϵ,x(i))\ndϵ\n(18)\nThe same can be done for the approximate posterior qφ(θ):\neθ = hφ(ζ)\nwith\nζ ∼p(ζ)\n(19)\nwhere we, similarly as above, choose a prior p(ζ) and a function hφ(ζ) such that the following\nholds:\nL(φ; X) =\nZ\nqφ(θ) (log pθ(X) + log pα(θ) −log qφ(θ)) dθ\n=\nZ\np(ζ) (log pθ(X) + log pα(θ) −log qφ(θ))\n\f\f\f\f\nθ=hφ(ζ)\ndζ\n(20)\nFor notational conciseness we introduce a shorthand notation fφ(x, z, θ):\nfφ(x, z, θ) = N · (log pθ(x|z) + log pθ(z) −log qφ(z|x)) + log pα(θ) −log qφ(θ)\n(21)\nUsing equations (20) and (18), the Monte Carlo estimate of the variational lower bound, given\ndatapoint x(i), is:\nL(φ; X) ≃1\nL\nL\nX\nl=1\nfφ(x(l), gφ(ϵ(l), x(l)), hφ(ζ(l)))\n(22)\nwhere ϵ(l) ∼p(ϵ) and ζ(l) ∼p(ζ). The estimator only depends on samples from p(ϵ) and p(ζ)\nwhich are obviously not inﬂuenced by φ, therefore the estimator can be differentiated w.r.t. φ.\nThe resulting stochastic gradients can be used in conjunction with stochastic optimization methods\nsuch as SGD or Adagrad [DHS10]. See algorithm 1 for a basic approach to computing stochastic\ngradients.\nF.1\nExample\nLet the prior over the parameters and latent variables be the centered isotropic Gaussian pα(θ) =\nN(z; 0, I) and pθ(z) = N(z; 0, I). Note that in this case, the prior lacks parameters. Let’s also\nassume that the true posteriors are approximatily Gaussian with an approximately diagonal covari-\nance. In this case, we can let the variational approximate posteriors be multivariate Gaussians with\na diagonal covariance structure:\nlog qφ(θ) = log N(θ; µθ, σ2\nθI)\nlog qφ(z|x) = log N(z; µz, σ2\nzI)\n(23)\n13\n",
    "Algorithm 2 Pseudocode for computing a stochastic gradient using our estimator. See text for\nmeaning of the functions fφ, gφ and hφ.\nRequire: φ (Current value of variational parameters)\ng ←0\nfor l is 1 to L do\nx ←Random draw from dataset X\nϵ ←Random draw from prior p(ϵ)\nζ ←Random draw from prior p(ζ)\ng ←g + 1\nL∇φfφ(x, gφ(ϵ, x), hφ(ζ))\nend for\nreturn g\nwhere µz and σz are yet unspeciﬁed functions of x. Since they are Gaussian, we can parameterize\nthe variational approximate posteriors:\nqφ(θ)\nas\neθ = µθ + σθ ⊙ζ\nwhere\nζ ∼N(0, I)\nqφ(z|x)\nas\nez = µz + σz ⊙ϵ\nwhere\nϵ ∼N(0, I)\nWith ⊙we signify an element-wise product. These can be plugged into the lower bound deﬁned\nabove (eqs (21) and (22)).\nIn this case it is possible to construct an alternative estimator with a lower variance, since in this\nmodel pα(θ), pθ(z), qφ(θ) and qφ(z|x) are Gaussian, and therefore four terms of fφ can be solved\nanalytically. The resulting estimator is:\nL(φ; X) ≃1\nL\nL\nX\nl=1\nN ·\n\n1\n2\nJ\nX\nj=1\n\u0010\n1 + log((σ(l)\nz,j)2) −(µ(l)\nz,j)2 −(σ(l)\nz,j)2\u0011\n+ log pθ(x(i)z(i))\n\n\n+ 1\n2\nJ\nX\nj=1\n\u0010\n1 + log((σ(l)\nθ,j)2) −(µ(l)\nθ,j)2 −(σ(l)\nθ,j)2\u0011\n(24)\nµ(i)\nj\nand σ(i)\nj\nsimply denote the j-th element of vectors µ(i) and σ(i).\n14\n"
  ],
  "full_text": "Auto-Encoding Variational Bayes\nDiederik P. Kingma\nMachine Learning Group\nUniversiteit van Amsterdam\ndpkingma@gmail.com\nMax Welling\nMachine Learning Group\nUniversiteit van Amsterdam\nwelling.max@gmail.com\nAbstract\nHow can we perform efﬁcient inference and learning in directed probabilistic\nmodels, in the presence of continuous latent variables with intractable posterior\ndistributions, and large datasets? We introduce a stochastic variational inference\nand learning algorithm that scales to large datasets and, under some mild differ-\nentiability conditions, even works in the intractable case. Our contributions are\ntwo-fold. First, we show that a reparameterization of the variational lower bound\nyields a lower bound estimator that can be straightforwardly optimized using stan-\ndard stochastic gradient methods. Second, we show that for i.i.d. datasets with\ncontinuous latent variables per datapoint, posterior inference can be made espe-\ncially efﬁcient by ﬁtting an approximate inference model (also called a recogni-\ntion model) to the intractable posterior using the proposed lower bound estimator.\nTheoretical advantages are reﬂected in experimental results.\n1\nIntroduction\nHow can we perform efﬁcient approximate inference and learning with directed probabilistic models\nwhose continuous latent variables and/or parameters have intractable posterior distributions? The\nvariational Bayesian (VB) approach involves the optimization of an approximation to the intractable\nposterior. Unfortunately, the common mean-ﬁeld approach requires analytical solutions of expecta-\ntions w.r.t. the approximate posterior, which are also intractable in the general case. We show how a\nreparameterization of the variational lower bound yields a simple differentiable unbiased estimator\nof the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for ef-\nﬁcient approximate posterior inference in almost any model with continuous latent variables and/or\nparameters, and is straightforward to optimize using standard stochastic gradient ascent techniques.\nFor the case of an i.i.d. dataset and continuous latent variables per datapoint, we propose the Auto-\nEncoding VB (AEVB) algorithm. In the AEVB algorithm we make inference and learning especially\nefﬁcient by using the SGVB estimator to optimize a recognition model that allows us to perform very\nefﬁcient approximate posterior inference using simple ancestral sampling, which in turn allows us\nto efﬁciently learn the model parameters, without the need of expensive iterative inference schemes\n(such as MCMC) per datapoint. The learned approximate posterior inference model can also be used\nfor a host of tasks such as recognition, denoising, representation and visualization purposes. When\na neural network is used for the recognition model, we arrive at the variational auto-encoder.\n2\nMethod\nThe strategy in this section can be used to derive a lower bound estimator (a stochastic objective\nfunction) for a variety of directed graphical models with continuous latent variables. We will restrict\nourselves here to the common case where we have an i.i.d. dataset with latent variables per datapoint,\nand where we like to perform maximum likelihood (ML) or maximum a posteriori (MAP) inference\non the (global) parameters, and variational inference on the latent variables. It is, for example,\n1\narXiv:1312.6114v11  [stat.ML]  10 Dec 2022\n\n\nx\nz\nφ\nθ\nN\nFigure 1: The type of directed graphical model under consideration. Solid lines denote the generative\nmodel pθ(z)pθ(x|z), dashed lines denote the variational approximation qφ(z|x) to the intractable\nposterior pθ(z|x). The variational parameters φ are learned jointly with the generative model pa-\nrameters θ.\nstraightforward to extend this scenario to the case where we also perform variational inference on\nthe global parameters; that algorithm is put in the appendix, but experiments with that case are left to\nfuture work. Note that our method can be applied to online, non-stationary settings, e.g. streaming\ndata, but here we assume a ﬁxed dataset for simplicity.\n2.1\nProblem scenario\nLet us consider some dataset X = {x(i)}N\ni=1 consisting of N i.i.d. samples of some continuous\nor discrete variable x. We assume that the data are generated by some random process, involving\nan unobserved continuous random variable z. The process consists of two steps: (1) a value z(i)\nis generated from some prior distribution pθ∗(z); (2) a value x(i) is generated from some condi-\ntional distribution pθ∗(x|z). We assume that the prior pθ∗(z) and likelihood pθ∗(x|z) come from\nparametric families of distributions pθ(z) and pθ(x|z), and that their PDFs are differentiable almost\neverywhere w.r.t. both θ and z. Unfortunately, a lot of this process is hidden from our view: the true\nparameters θ∗as well as the values of the latent variables z(i) are unknown to us.\nVery importantly, we do not make the common simplifying assumptions about the marginal or pos-\nterior probabilities. Conversely, we are here interested in a general algorithm that even works efﬁ-\nciently in the case of:\n1. Intractability:\nthe case where the integral of the marginal likelihood pθ(x)\n=\nR\npθ(z)pθ(x|z) dz is intractable (so we cannot evaluate or differentiate the marginal like-\nlihood), where the true posterior density pθ(z|x) = pθ(x|z)pθ(z)/pθ(x) is intractable\n(so the EM algorithm cannot be used), and where the required integrals for any reason-\nable mean-ﬁeld VB algorithm are also intractable. These intractabilities are quite common\nand appear in cases of moderately complicated likelihood functions pθ(x|z), e.g. a neural\nnetwork with a nonlinear hidden layer.\n2. A large dataset: we have so much data that batch optimization is too costly; we would like\nto make parameter updates using small minibatches or even single datapoints. Sampling-\nbased solutions, e.g. Monte Carlo EM, would in general be too slow, since it involves a\ntypically expensive sampling loop per datapoint.\nWe are interested in, and propose a solution to, three related problems in the above scenario:\n1. Efﬁcient approximate ML or MAP estimation for the parameters θ. The parameters can be\nof interest themselves, e.g. if we are analyzing some natural process. They also allow us to\nmimic the hidden random process and generate artiﬁcial data that resembles the real data.\n2. Efﬁcient approximate posterior inference of the latent variable z given an observed value x\nfor a choice of parameters θ. This is useful for coding or data representation tasks.\n3. Efﬁcient approximate marginal inference of the variable x. This allows us to perform all\nkinds of inference tasks where a prior over x is required. Common applications in computer\nvision include image denoising, inpainting and super-resolution.\n2\n\n\nFor the purpose of solving the above problems, let us introduce a recognition model qφ(z|x): an\napproximation to the intractable true posterior pθ(z|x). Note that in contrast with the approximate\nposterior in mean-ﬁeld variational inference, it is not necessarily factorial and its parameters φ are\nnot computed from some closed-form expectation. Instead, we’ll introduce a method for learning\nthe recognition model parameters φ jointly with the generative model parameters θ.\nFrom a coding theory perspective, the unobserved variables z have an interpretation as a latent\nrepresentation or code. In this paper we will therefore also refer to the recognition model qφ(z|x)\nas a probabilistic encoder, since given a datapoint x it produces a distribution (e.g. a Gaussian)\nover the possible values of the code z from which the datapoint x could have been generated. In a\nsimilar vein we will refer to pθ(x|z) as a probabilistic decoder, since given a code z it produces a\ndistribution over the possible corresponding values of x.\n2.2\nThe variational bound\nThe marginal likelihood is composed of a sum over the marginal likelihoods of individual datapoints\nlog pθ(x(1), · · · , x(N)) = PN\ni=1 log pθ(x(i)), which can each be rewritten as:\nlog pθ(x(i)) = DKL(qφ(z|x(i))||pθ(z|x(i))) + L(θ, φ; x(i))\n(1)\nThe ﬁrst RHS term is the KL divergence of the approximate from the true posterior. Since this\nKL-divergence is non-negative, the second RHS term L(θ, φ; x(i)) is called the (variational) lower\nbound on the marginal likelihood of datapoint i, and can be written as:\nlog pθ(x(i)) ≥L(θ, φ; x(i)) = Eqφ(z|x) [−log qφ(z|x) + log pθ(x, z)]\n(2)\nwhich can also be written as:\nL(θ, φ; x(i)) = −DKL(qφ(z|x(i))||pθ(z)) + Eqφ(z|x(i))\nh\nlog pθ(x(i)|z)\ni\n(3)\nWe want to differentiate and optimize the lower bound L(θ, φ; x(i)) w.r.t. both the variational\nparameters φ and generative parameters θ. However, the gradient of the lower bound w.r.t. φ\nis a bit problematic. The usual (na¨ıve) Monte Carlo gradient estimator for this type of problem\nis: ∇φEqφ(z) [f(z)] = Eqφ(z)\n\u0002\nf(z)∇qφ(z) log qφ(z)\n\u0003\n≃1\nL\nPL\nl=1 f(z)∇qφ(z(l)) log qφ(z(l)) where\nz(l) ∼qφ(z|x(i)). This gradient estimator exhibits exhibits very high variance (see e.g. [BJP12])\nand is impractical for our purposes.\n2.3\nThe SGVB estimator and AEVB algorithm\nIn this section we introduce a practical estimator of the lower bound and its derivatives w.r.t. the\nparameters. We assume an approximate posterior in the form qφ(z|x), but please note that the\ntechnique can be applied to the case qφ(z), i.e. where we do not condition on x, as well. The fully\nvariational Bayesian method for inferring a posterior over the parameters is given in the appendix.\nUnder certain mild conditions outlined in section 2.4 for a chosen approximate posterior qφ(z|x) we\ncan reparameterize the random variable ez ∼qφ(z|x) using a differentiable transformation gφ(ϵ, x)\nof an (auxiliary) noise variable ϵ:\nez = gφ(ϵ, x)\nwith\nϵ ∼p(ϵ)\n(4)\nSee section 2.4 for general strategies for chosing such an approriate distribution p(ϵ) and function\ngφ(ϵ, x). We can now form Monte Carlo estimates of expectations of some function f(z) w.r.t.\nqφ(z|x) as follows:\nEqφ(z|x(i)) [f(z)] = Ep(ϵ)\nh\nf(gφ(ϵ, x(i)))\ni\n≃1\nL\nL\nX\nl=1\nf(gφ(ϵ(l), x(i)))\nwhere\nϵ(l) ∼p(ϵ) (5)\nWe apply this technique to the variational lower bound (eq. (2)), yielding our generic Stochastic\nGradient Variational Bayes (SGVB) estimator eLA(θ, φ; x(i)) ≃L(θ, φ; x(i)):\neLA(θ, φ; x(i)) = 1\nL\nL\nX\nl=1\nlog pθ(x(i), z(i,l)) −log qφ(z(i,l)|x(i))\nwhere\nz(i,l) = gφ(ϵ(i,l), x(i))\nand\nϵ(l) ∼p(ϵ)\n(6)\n3\n\n\nAlgorithm 1 Minibatch version of the Auto-Encoding VB (AEVB) algorithm. Either of the two\nSGVB estimators in section 2.3 can be used. We use settings M = 100 and L = 1 in experiments.\nθ, φ ←Initialize parameters\nrepeat\nXM ←Random minibatch of M datapoints (drawn from full dataset)\nϵ ←Random samples from noise distribution p(ϵ)\ng ←∇θ,φ eLM(θ, φ; XM, ϵ) (Gradients of minibatch estimator (8))\nθ, φ ←Update parameters using gradients g (e.g. SGD or Adagrad [DHS10])\nuntil convergence of parameters (θ, φ)\nreturn θ, φ\nOften, the KL-divergence DKL(qφ(z|x(i))||pθ(z)) of eq. (3) can be integrated analytically (see\nappendix B), such that only the expected reconstruction error Eqφ(z|x(i))\n\u0002\nlog pθ(x(i)|z)\n\u0003\nrequires\nestimation by sampling. The KL-divergence term can then be interpreted as regularizing φ, encour-\naging the approximate posterior to be close to the prior pθ(z). This yields a second version of the\nSGVB estimator eLB(θ, φ; x(i)) ≃L(θ, φ; x(i)), corresponding to eq. (3), which typically has less\nvariance than the generic estimator:\neLB(θ, φ; x(i)) = −DKL(qφ(z|x(i))||pθ(z)) + 1\nL\nL\nX\nl=1\n(log pθ(x(i)|z(i,l)))\nwhere\nz(i,l) = gφ(ϵ(i,l), x(i))\nand\nϵ(l) ∼p(ϵ)\n(7)\nGiven multiple datapoints from a dataset X with N datapoints, we can construct an estimator of the\nmarginal likelihood lower bound of the full dataset, based on minibatches:\nL(θ, φ; X) ≃eLM(θ, φ; XM) = N\nM\nM\nX\ni=1\neL(θ, φ; x(i))\n(8)\nwhere the minibatch XM = {x(i)}M\ni=1 is a randomly drawn sample of M datapoints from the\nfull dataset X with N datapoints. In our experiments we found that the number of samples L\nper datapoint can be set to 1 as long as the minibatch size M was large enough, e.g. M = 100.\nDerivatives ∇θ,φ eL(θ; XM) can be taken, and the resulting gradients can be used in conjunction\nwith stochastic optimization methods such as SGD or Adagrad [DHS10]. See algorithm 1 for a\nbasic approach to compute the stochastic gradients.\nA connection with auto-encoders becomes clear when looking at the objective function given at\neq. (7). The ﬁrst term is (the KL divergence of the approximate posterior from the prior) acts as a\nregularizer, while the second term is a an expected negative reconstruction error. The function gφ(.)\nis chosen such that it maps a datapoint x(i) and a random noise vector ϵ(l) to a sample from the\napproximate posterior for that datapoint: z(i,l) = gφ(ϵ(l), x(i)) where z(i,l) ∼qφ(z|x(i)). Subse-\nquently, the sample z(i,l) is then input to function log pθ(x(i)|z(i,l)), which equals the probability\ndensity (or mass) of datapoint x(i) under the generative model, given z(i,l). This term is a negative\nreconstruction error in auto-encoder parlance.\n2.4\nThe reparameterization trick\nIn order to solve our problem we invoked an alternative method for generating samples from\nqφ(z|x). The essential parameterization trick is quite simple. Let z be a continuous random vari-\nable, and z ∼qφ(z|x) be some conditional distribution. It is then often possible to express the\nrandom variable z as a deterministic variable z = gφ(ϵ, x), where ϵ is an auxiliary variable with\nindependent marginal p(ϵ), and gφ(.) is some vector-valued function parameterized by φ.\nThis reparameterization is useful for our case since it can be used to rewrite an expectation w.r.t\nqφ(z|x) such that the Monte Carlo estimate of the expectation is differentiable w.r.t. φ. A proof\nis as follows. Given the deterministic mapping z = gφ(ϵ, x) we know that qφ(z|x) Q\ni dzi =\np(ϵ) Q\ni dϵi. Therefore1,\nR\nqφ(z|x)f(z) dz =\nR\np(ϵ)f(z) dϵ =\nR\np(ϵ)f(gφ(ϵ, x)) dϵ. It follows\n1Note that for inﬁnitesimals we use the notational convention dz = Q\ni dzi\n4\n\n\nthat a differentiable estimator can be constructed:\nR\nqφ(z|x)f(z) dz ≃\n1\nL\nPL\nl=1 f(gφ(x, ϵ(l)))\nwhere ϵ(l) ∼p(ϵ). In section 2.3 we applied this trick to obtain a differentiable estimator of the\nvariational lower bound.\nTake, for example, the univariate Gaussian case: let z ∼p(z|x) = N(µ, σ2). In this case, a valid\nreparameterization is z = µ + σϵ, where ϵ is an auxiliary noise variable ϵ ∼N(0, 1). Therefore,\nEN(z;µ,σ2) [f(z)] = EN(ϵ;0,1) [f(µ + σϵ)] ≃1\nL\nPL\nl=1 f(µ + σϵ(l)) where ϵ(l) ∼N(0, 1).\nFor which qφ(z|x) can we choose such a differentiable transformation gφ(.) and auxiliary variable\nϵ ∼p(ϵ)? Three basic approaches are:\n1. Tractable inverse CDF. In this case, let ϵ ∼U(0, I), and let gφ(ϵ, x) be the inverse CDF of\nqφ(z|x). Examples: Exponential, Cauchy, Logistic, Rayleigh, Pareto, Weibull, Reciprocal,\nGompertz, Gumbel and Erlang distributions.\n2. Analogous to the Gaussian example, for any ”location-scale” family of distributions we can\nchoose the standard distribution (with location = 0, scale = 1) as the auxiliary variable\nϵ, and let g(.) = location + scale · ϵ. Examples: Laplace, Elliptical, Student’s t, Logistic,\nUniform, Triangular and Gaussian distributions.\n3. Composition: It is often possible to express random variables as different transformations\nof auxiliary variables. Examples: Log-Normal (exponentiation of normally distributed\nvariable), Gamma (a sum over exponentially distributed variables), Dirichlet (weighted\nsum of Gamma variates), Beta, Chi-Squared, and F distributions.\nWhen all three approaches fail, good approximations to the inverse CDF exist requiring computa-\ntions with time complexity comparable to the PDF (see e.g. [Dev86] for some methods).\n3\nExample: Variational Auto-Encoder\nIn this section we’ll give an example where we use a neural network for the probabilistic encoder\nqφ(z|x) (the approximation to the posterior of the generative model pθ(x, z)) and where the param-\neters φ and θ are optimized jointly with the AEVB algorithm.\nLet the prior over the latent variables be the centered isotropic multivariate Gaussian pθ(z) =\nN(z; 0, I). Note that in this case, the prior lacks parameters. We let pθ(x|z) be a multivariate\nGaussian (in case of real-valued data) or Bernoulli (in case of binary data) whose distribution pa-\nrameters are computed from z with a MLP (a fully-connected neural network with a single hidden\nlayer, see appendix C). Note the true posterior pθ(z|x) is in this case intractable. While there is\nmuch freedom in the form qφ(z|x), we’ll assume the true (but intractable) posterior takes on a ap-\nproximate Gaussian form with an approximately diagonal covariance. In this case, we can let the\nvariational approximate posterior be a multivariate Gaussian with a diagonal covariance structure2:\nlog qφ(z|x(i)) = log N(z; µ(i), σ2(i)I)\n(9)\nwhere the mean and s.d. of the approximate posterior, µ(i) and σ(i), are outputs of the encoding\nMLP, i.e. nonlinear functions of datapoint x(i) and the variational parameters φ (see appendix C).\nAs explained in section 2.4, we sample from the posterior z(i,l) ∼qφ(z|x(i)) using z(i,l) =\ngφ(x(i), ϵ(l)) = µ(i) + σ(i) ⊙ϵ(l) where ϵ(l) ∼N(0, I). With ⊙we signify an element-wise\nproduct. In this model both pθ(z) (the prior) and qφ(z|x) are Gaussian; in this case, we can use the\nestimator of eq. (7) where the KL divergence can be computed and differentiated without estimation\n(see appendix B). The resulting estimator for this model and datapoint x(i) is:\nL(θ, φ; x(i)) ≃1\n2\nJ\nX\nj=1\n\u0010\n1 + log((σ(i)\nj )2) −(µ(i)\nj )2 −(σ(i)\nj )2\u0011\n+ 1\nL\nL\nX\nl=1\nlog pθ(x(i)|z(i,l))\nwhere\nz(i,l) = µ(i) + σ(i) ⊙ϵ(l)\nand\nϵ(l) ∼N(0, I)\n(10)\nAs explained above and in appendix C, the decoding term log pθ(x(i)|z(i,l)) is a Bernoulli or Gaus-\nsian MLP, depending on the type of data we are modelling.\n2Note that this is just a (simplifying) choice, and not a limitation of our method.\n5\n\n\n4\nRelated work\nThe wake-sleep algorithm [HDFN95] is, to the best of our knowledge, the only other on-line learn-\ning method in the literature that is applicable to the same general class of continuous latent variable\nmodels. Like our method, the wake-sleep algorithm employs a recognition model that approximates\nthe true posterior. A drawback of the wake-sleep algorithm is that it requires a concurrent optimiza-\ntion of two objective functions, which together do not correspond to optimization of (a bound of)\nthe marginal likelihood. An advantage of wake-sleep is that it also applies to models with discrete\nlatent variables. Wake-Sleep has the same computational complexity as AEVB per datapoint.\nStochastic variational inference [HBWP13] has recently received increasing interest.\nRecently,\n[BJP12] introduced a control variate schemes to reduce the high variance of the na¨ıve gradient\nestimator discussed in section 2.1, and applied to exponential family approximations of the poste-\nrior. In [RGB13] some general methods, i.e. a control variate scheme, were introduced for reducing\nthe variance of the original gradient estimator. In [SK13], a similar reparameterization as in this\npaper was used in an efﬁcient version of a stochastic variational inference algorithm for learning the\nnatural parameters of exponential-family approximating distributions.\nThe AEVB algorithm exposes a connection between directed probabilistic models (trained with a\nvariational objective) and auto-encoders. A connection between linear auto-encoders and a certain\nclass of generative linear-Gaussian models has long been known. In [Row98] it was shown that PCA\ncorresponds to the maximum-likelihood (ML) solution of a special case of the linear-Gaussian model\nwith a prior p(z) = N(0, I) and a conditional distribution p(x|z) = N(x; Wz, ϵI), speciﬁcally the\ncase with inﬁnitesimally small ϵ.\nIn relevant recent work on autoencoders [VLL+10] it was shown that the training criterion of un-\nregularized autoencoders corresponds to maximization of a lower bound (see the infomax princi-\nple [Lin89]) of the mutual information between input X and latent representation Z. Maximiz-\ning (w.r.t. parameters) of the mutual information is equivalent to maximizing the conditional en-\ntropy, which is lower bounded by the expected loglikelihood of the data under the autoencoding\nmodel [VLL+10], i.e. the negative reconstrution error. However, it is well known that this recon-\nstruction criterion is in itself not sufﬁcient for learning useful representations [BCV13]. Regular-\nization techniques have been proposed to make autoencoders learn useful representations, such as\ndenoising, contractive and sparse autoencoder variants [BCV13]. The SGVB objective contains a\nregularization term dictated by the variational bound (e.g. eq. (10)), lacking the usual nuisance regu-\nlarization hyperparameter required to learn useful representations. Related are also encoder-decoder\narchitectures such as the predictive sparse decomposition (PSD) [KRL08], from which we drew\nsome inspiration. Also relevant are the recently introduced Generative Stochastic Networks [BTL13]\nwhere noisy auto-encoders learn the transition operator of a Markov chain that samples from the data\ndistribution. In [SL10] a recognition model was employed for efﬁcient learning with Deep Boltz-\nmann Machines. These methods are targeted at either unnormalized models (i.e. undirected models\nlike Boltzmann machines) or limited to sparse coding models, in contrast to our proposed algorithm\nfor learning a general class of directed probabilistic models.\nThe recently proposed DARN method [GMW13], also learns a directed probabilistic model using\nan auto-encoding structure, however their method applies to binary latent variables. Even more\nrecently, [RMW14] also make the connection between auto-encoders, directed proabilistic models\nand stochastic variational inference using the reparameterization trick we describe in this paper.\nTheir work was developed independently of ours and provides an additional perspective on AEVB.\n5\nExperiments\nWe trained generative models of images from the MNIST and Frey Face datasets3 and compared\nlearning algorithms in terms of the variational lower bound, and the estimated marginal likelihood.\nThe generative model (encoder) and variational approximation (decoder) from section 3 were used,\nwhere the described encoder and decoder have an equal number of hidden units. Since the Frey\nFace data are continuous, we used a decoder with Gaussian outputs, identical to the encoder, except\nthat the means were constrained to the interval (0, 1) using a sigmoidal activation function at the\n3Available at http://www.cs.nyu.edu/˜roweis/data.html\n6\n\n\n105\n106\n107\n108\n# Training samples evaluated\n150\n140\n130\n120\n110\n100\nL\nMNIST, Nz =3\n105\n106\n107\n108\n150\n140\n130\n120\n110\n100\nMNIST, Nz =5\n105\n106\n107\n108\n150\n140\n130\n120\n110\n100\nMNIST, Nz =10\n105\n106\n107\n108\n150\n140\n130\n120\n110\n100\nMNIST, Nz =20\n105\n106\n107\n108\n150\n140\n130\n120\n110\n100\nMNIST, Nz =200\n105\n106\n107\n108\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\nL\nFrey Face, Nz =2\nWake-Sleep (test)\nWake-Sleep (train)\nAEVB (test)\nAEVB (train)\n105\n106\n107\n108\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600 Frey Face, Nz =5\n105\n106\n107\n108\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600 Frey Face, Nz =10\n105\n106\n107\n108\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600 Frey Face, Nz =20\nFigure 2: Comparison of our AEVB method to the wake-sleep algorithm, in terms of optimizing the\nlower bound, for different dimensionality of latent space (Nz). Our method converged considerably\nfaster and reached a better solution in all experiments. Interestingly enough, more latent variables\ndoes not result in more overﬁtting, which is explained by the regularizing effect of the lower bound.\nVertical axis: the estimated average variational lower bound per datapoint. The estimator variance\nwas small (< 1) and omitted. Horizontal axis: amount of training points evaluated. Computa-\ntion took around 20-40 minutes per million training samples with a Intel Xeon CPU running at an\neffective 40 GFLOPS.\ndecoder output. Note that with hidden units we refer to the hidden layer of the neural networks of\nthe encoder and decoder.\nParameters are updated using stochastic gradient ascent where gradients are computed by differenti-\nating the lower bound estimator ∇θ,φL(θ, φ; X) (see algorithm 1), plus a small weight decay term\ncorresponding to a prior p(θ) = N(0, I). Optimization of this objective is equivalent to approxi-\nmate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower\nbound.\nWe compared performance of AEVB to the wake-sleep algorithm [HDFN95]. We employed the\nsame encoder (also called recognition model) for the wake-sleep algorithm and the variational auto-\nencoder. All parameters, both variational and generative, were initialized by random sampling from\nN(0, 0.01), and were jointly stochastically optimized using the MAP criterion. Stepsizes were\nadapted with Adagrad [DHS10]; the Adagrad global stepsize parameters were chosen from {0.01,\n0.02, 0.1} based on performance on the training set in the ﬁrst few iterations. Minibatches of size\nM = 100 were used, with L = 1 samples per datapoint.\nLikelihood lower bound\nWe trained generative models (decoders) and corresponding encoders\n(a.k.a. recognition models) having 500 hidden units in case of MNIST, and 200 hidden units in case\nof the Frey Face dataset (to prevent overﬁtting, since it is a considerably smaller dataset). The chosen\nnumber of hidden units is based on prior literature on auto-encoders, and the relative performance\nof different algorithms was not very sensitive to these choices. Figure 2 shows the results when\ncomparing the lower bounds. Interestingly, superﬂuous latent variables did not result in overﬁtting,\nwhich is explained by the regularizing nature of the variational bound.\nMarginal likelihood\nFor very low-dimensional latent space it is possible to estimate the marginal\nlikelihood of the learned generative models using an MCMC estimator. More information about the\nmarginal likelihood estimator is available in the appendix. For the encoder and decoder we again\nused neural networks, this time with 100 hidden units, and 3 latent variables; for higher dimensional\nlatent space the estimates became unreliable. Again, the MNIST dataset was used. The AEVB\nand Wake-Sleep methods were compared to Monte Carlo EM (MCEM) with a Hybrid Monte Carlo\n(HMC) [DKPR87] sampler; details are in the appendix. We compared the convergence speed for\nthe three algorithms, for a small and large training set size. Results are in ﬁgure 3.\n7\n\n\n0\n10\n20\n30\n40\n50\n60\n# Training samples evaluated (millions)\n160\n150\n140\n130\n120\n110\n100\nMarginal log-likelihood\nNtrain = 1000\n0\n10\n20\n30\n40\n50\n60\n160\n155\n150\n145\n140\n135\n130\n125\nNtrain = 50000\nWake-Sleep (train)\nWake-Sleep (test)\nMCEM (train)\nMCEM (test)\nAEVB (train)\nAEVB (test)\nFigure 3: Comparison of AEVB to the wake-sleep algorithm and Monte Carlo EM, in terms of the\nestimated marginal likelihood, for a different number of training points. Monte Carlo EM is not an\non-line algorithm, and (unlike AEVB and the wake-sleep method) can’t be applied efﬁciently for\nthe full MNIST dataset.\nVisualisation of high-dimensional data\nIf we choose a low-dimensional latent space (e.g. 2D),\nwe can use the learned encoders (recognition model) to project high-dimensional data to a low-\ndimensional manifold. See appendix A for visualisations of the 2D latent manifolds for the MNIST\nand Frey Face datasets.\n6\nConclusion\nWe have introduced a novel estimator of the variational lower bound, Stochastic Gradient VB\n(SGVB), for efﬁcient approximate inference with continuous latent variables. The proposed estima-\ntor can be straightforwardly differentiated and optimized using standard stochastic gradient meth-\nods. For the case of i.i.d. datasets and continuous latent variables per datapoint we introduce an\nefﬁcient algorithm for efﬁcient inference and learning, Auto-Encoding VB (AEVB), that learns an\napproximate inference model using the SGVB estimator. The theoretical advantages are reﬂected in\nexperimental results.\n7\nFuture work\nSince the SGVB estimator and the AEVB algorithm can be applied to almost any inference and\nlearning problem with continuous latent variables, there are plenty of future directions: (i) learning\nhierarchical generative architectures with deep neural networks (e.g. convolutional networks) used\nfor the encoders and decoders, trained jointly with AEVB; (ii) time-series models (i.e. dynamic\nBayesian networks); (iii) application of SGVB to the global parameters; (iv) supervised models\nwith latent variables, useful for learning complicated noise distributions.\n8\n\n\nReferences\n[BCV13]\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A re-\nview and new perspectives. 2013.\n[BJP12]\nDavid M Blei, Michael I Jordan, and John W Paisley. Variational Bayesian inference\nwith Stochastic Search. In Proceedings of the 29th International Conference on Ma-\nchine Learning (ICML-12), pages 1367–1374, 2012.\n[BTL13]\nYoshua Bengio and ´Eric Thibodeau-Laufer. Deep generative stochastic networks train-\nable by backprop. arXiv preprint arXiv:1306.1091, 2013.\n[Dev86]\nLuc Devroye. Sample-based non-uniform random variate generation. In Proceedings\nof the 18th conference on Winter simulation, pages 260–265. ACM, 1986.\n[DHS10]\nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online\nlearning and stochastic optimization. Journal of Machine Learning Research, 12:2121–\n2159, 2010.\n[DKPR87] Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. Hybrid\nmonte carlo. Physics letters B, 195(2):216–222, 1987.\n[GMW13]\nKarol Gregor, Andriy Mnih, and Daan Wierstra. Deep autoregressive networks. arXiv\npreprint arXiv:1310.8499, 2013.\n[HBWP13] Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic varia-\ntional inference. The Journal of Machine Learning Research, 14(1):1303–1347, 2013.\n[HDFN95] Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The” wake-\nsleep” algorithm for unsupervised neural networks. SCIENCE, pages 1158–1158, 1995.\n[KRL08]\nKoray Kavukcuoglu, Marc’Aurelio Ranzato, and Yann LeCun. Fast inference in sparse\ncoding algorithms with applications to object recognition. Technical Report CBLL-\nTR-2008-12-01, Computational and Biological Learning Lab, Courant Institute, NYU,\n2008.\n[Lin89]\nRalph Linsker. An application of the principle of maximum information preservation to\nlinear systems. Morgan Kaufmann Publishers Inc., 1989.\n[RGB13]\nRajesh Ranganath, Sean Gerrish, and David M Blei. Black Box Variational Inference.\narXiv preprint arXiv:1401.0118, 2013.\n[RMW14]\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.\nStochastic back-\npropagation and variational inference in deep latent gaussian models. arXiv preprint\narXiv:1401.4082, 2014.\n[Row98]\nSam Roweis. EM algorithms for PCA and SPCA. Advances in neural information\nprocessing systems, pages 626–632, 1998.\n[SK13]\nTim Salimans and David A Knowles. Fixed-form variational posterior approximation\nthrough stochastic linear regression. Bayesian Analysis, 8(4), 2013.\n[SL10]\nRuslan Salakhutdinov and Hugo Larochelle. Efﬁcient learning of deep boltzmann ma-\nchines. In International Conference on Artiﬁcial Intelligence and Statistics, pages 693–\n700, 2010.\n[VLL+10] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine\nManzagol. Stacked denoising autoencoders: Learning useful representations in a deep\nnetwork with a local denoising criterion. The Journal of Machine Learning Research,\n9999:3371–3408, 2010.\nA\nVisualisations\nSee ﬁgures 4 and 5 for visualisations of latent space and corresponding observed space of models\nlearned with SGVB.\n9\n\n\n(a) Learned Frey Face manifold\n(b) Learned MNIST manifold\nFigure 4: Visualisations of learned data manifold for generative models with two-dimensional latent\nspace, learned with AEVB. Since the prior of the latent space is Gaussian, linearly spaced coor-\ndinates on the unit square were transformed through the inverse CDF of the Gaussian to produce\nvalues of the latent variables z. For each of these values z, we plotted the corresponding generative\npθ(x|z) with the learned parameters θ.\n(a) 2-D latent space\n(b) 5-D latent space\n(c) 10-D latent space\n(d) 20-D latent space\nFigure 5: Random samples from learned generative models of MNIST for different dimensionalities\nof latent space.\nB\nSolution of −DKL(qφ(z)||pθ(z)), Gaussian case\nThe variational lower bound (the objective to be maximized) contains a KL term that can often be\nintegrated analytically. Here we give the solution when both the prior pθ(z) = N(0, I) and the\nposterior approximation qφ(z|x(i)) are Gaussian. Let J be the dimensionality of z. Let µ and σ\ndenote the variational mean and s.d. evaluated at datapoint i, and let µj and σj simply denote the\nj-th element of these vectors. Then:\nZ\nqθ(z) log p(z) dz =\nZ\nN(z; µ, σ2) log N(z; 0, I) dz\n= −J\n2 log(2π) −1\n2\nJ\nX\nj=1\n(µ2\nj + σ2\nj )\n10\n\n\nAnd:\nZ\nqθ(z) log qθ(z) dz =\nZ\nN(z; µ, σ2) log N(z; µ, σ2) dz\n= −J\n2 log(2π) −1\n2\nJ\nX\nj=1\n(1 + log σ2\nj )\nTherefore:\n−DKL((qφ(z)||pθ(z)) =\nZ\nqθ(z) (log pθ(z) −log qθ(z)) dz\n= 1\n2\nJ\nX\nj=1\n\u00001 + log((σj)2) −(µj)2 −(σj)2\u0001\nWhen using a recognition model qφ(z|x) then µ and s.d. σ are simply functions of x and the\nvariational parameters φ, as exempliﬁed in the text.\nC\nMLP’s as probabilistic encoders and decoders\nIn variational auto-encoders, neural networks are used as probabilistic encoders and decoders. There\nare many possible choices of encoders and decoders, depending on the type of data and model. In\nour example we used relatively simple neural networks, namely multi-layered perceptrons (MLPs).\nFor the encoder we used a MLP with Gaussian output, while for the decoder we used MLPs with\neither Gaussian or Bernoulli outputs, depending on the type of data.\nC.1\nBernoulli MLP as decoder\nIn this case let pθ(x|z) be a multivariate Bernoulli whose probabilities are computed from z with a\nfully-connected neural network with a single hidden layer:\nlog p(x|z) =\nD\nX\ni=1\nxi log yi + (1 −xi) · log(1 −yi)\nwhere y = fσ(W2 tanh(W1z + b1) + b2)\n(11)\nwhere fσ(.) is the elementwise sigmoid activation function, and where θ = {W1, W2, b1, b2} are\nthe weights and biases of the MLP.\nC.2\nGaussian MLP as encoder or decoder\nIn this case let encoder or decoder be a multivariate Gaussian with a diagonal covariance structure:\nlog p(x|z) = log N(x; µ, σ2I)\nwhere µ = W4h + b4\nlog σ2 = W5h + b5\nh = tanh(W3z + b3)\n(12)\nwhere {W3, W4, W5, b3, b4, b5} are the weights and biases of the MLP and part of θ when used\nas decoder. Note that when this network is used as an encoder qφ(z|x), then z and x are swapped,\nand the weights and biases are variational parameters φ.\nD\nMarginal likelihood estimator\nWe derived the following marginal likelihood estimator that produces good estimates of the marginal\nlikelihood as long as the dimensionality of the sampled space is low (less then 5 dimensions), and\nsufﬁcient samples are taken. Let pθ(x, z) = pθ(z)pθ(x|z) be the generative model we are sampling\nfrom, and for a given datapoint x(i) we would like to estimate the marginal likelihood pθ(x(i)).\nThe estimation process consists of three stages:\n11\n\n\n1. Sample L values {z(l)} from the posterior using gradient-based MCMC, e.g. Hybrid Monte\nCarlo, using ∇z log pθ(z|x) = ∇z log pθ(z) + ∇z log pθ(x|z).\n2. Fit a density estimator q(z) to these samples {z(l)}.\n3. Again, sample L new values from the posterior. Plug these samples, as well as the ﬁtted\nq(z), into the following estimator:\npθ(x(i)) ≃\n \n1\nL\nL\nX\nl=1\nq(z(l))\npθ(z)pθ(x(i)|z(l))\n!−1\nwhere\nz(l) ∼pθ(z|x(i))\nDerivation of the estimator:\n1\npθ(x(i)) =\nR\nq(z) dz\npθ(x(i)) =\nR\nq(z) pθ(x(i),z)\npθ(x(i),z) dz\npθ(x(i))\n=\nZ pθ(x(i), z)\npθ(x(i))\nq(z)\npθ(x(i), z) dz\n=\nZ\npθ(z|x(i))\nq(z)\npθ(x(i), z) dz\n≃1\nL\nL\nX\nl=1\nq(z(l))\npθ(z)pθ(x(i)|z(l))\nwhere\nz(l) ∼pθ(z|x(i))\nE\nMonte Carlo EM\nThe Monte Carlo EM algorithm does not employ an encoder, instead it samples from the pos-\nterior of the latent variables using gradients of the posterior computed with ∇z log pθ(z|x) =\n∇z log pθ(z) + ∇z log pθ(x|z). The Monte Carlo EM procedure consists of 10 HMC leapfrog\nsteps with an automatically tuned stepsize such that the acceptance rate was 90%, followed by 5\nweight updates steps using the acquired sample. For all algorithms the parameters were updated\nusing the Adagrad stepsizes (with accompanying annealing schedule).\nThe marginal likelihood was estimated with the ﬁrst 1000 datapoints from the train and test sets,\nfor each datapoint sampling 50 values from the posterior of the latent variables using Hybrid Monte\nCarlo with 4 leapfrog steps.\nF\nFull VB\nAs written in the paper, it is possible to perform variational inference on both the parameters θ and\nthe latent variables z, as opposed to just the latent variables as we did in the paper. Here, we’ll derive\nour estimator for that case.\nLet pα(θ) be some hyperprior for the parameters introduced above, parameterized by α. The\nmarginal likelihood can be written as:\nlog pα(X) = DKL(qφ(θ)||pα(θ|X)) + L(φ; X)\n(13)\nwhere the ﬁrst RHS term denotes a KL divergence of the approximate from the true posterior, and\nwhere L(φ; X) denotes the variational lower bound to the marginal likelihood:\nL(φ; X) =\nZ\nqφ(θ) (log pθ(X) + log pα(θ) −log qφ(θ)) dθ\n(14)\nNote that this is a lower bound since the KL divergence is non-negative; the bound equals the true\nmarginal when the approximate and true posteriors match exactly. The term log pθ(X) is composed\nof a sum over the marginal likelihoods of individual datapoints log pθ(X) = PN\ni=1 log pθ(x(i)),\nwhich can each be rewritten as:\nlog pθ(x(i)) = DKL(qφ(z|x(i))||pθ(z|x(i))) + L(θ, φ; x(i))\n(15)\n12\n\n\nwhere again the ﬁrst RHS term is the KL divergence of the approximate from the true posterior, and\nL(θ, φ; x) is the variational lower bound of the marginal likelihood of datapoint i:\nL(θ, φ; x(i)) =\nZ\nqφ(z|x)\n\u0010\nlog pθ(x(i)|z) + log pθ(z) −log qφ(z|x)\n\u0011\ndz\n(16)\nThe expectations on the RHS of eqs (14) and (16) can obviously be written as a sum of three separate\nexpectations, of which the second and third component can sometimes be analytically solved, e.g.\nwhen both pθ(x) and qφ(z|x) are Gaussian. For generality we will here assume that each of these\nexpectations is intractable.\nUnder certain mild conditions outlined in section (see paper) for chosen approximate posteriors\nqφ(θ) and qφ(z|x) we can reparameterize conditional samples ez ∼qφ(z|x) as\nez = gφ(ϵ, x)\nwith\nϵ ∼p(ϵ)\n(17)\nwhere we choose a prior p(ϵ) and a function gφ(ϵ, x) such that the following holds:\nL(θ, φ; x(i)) =\nZ\nqφ(z|x)\n\u0010\nlog pθ(x(i)|z) + log pθ(z) −log qφ(z|x)\n\u0011\ndz\n=\nZ\np(ϵ)\n\u0010\nlog pθ(x(i)|z) + log pθ(z) −log qφ(z|x)\n\u0011 \f\f\f\f\nz=gφ(ϵ,x(i))\ndϵ\n(18)\nThe same can be done for the approximate posterior qφ(θ):\neθ = hφ(ζ)\nwith\nζ ∼p(ζ)\n(19)\nwhere we, similarly as above, choose a prior p(ζ) and a function hφ(ζ) such that the following\nholds:\nL(φ; X) =\nZ\nqφ(θ) (log pθ(X) + log pα(θ) −log qφ(θ)) dθ\n=\nZ\np(ζ) (log pθ(X) + log pα(θ) −log qφ(θ))\n\f\f\f\f\nθ=hφ(ζ)\ndζ\n(20)\nFor notational conciseness we introduce a shorthand notation fφ(x, z, θ):\nfφ(x, z, θ) = N · (log pθ(x|z) + log pθ(z) −log qφ(z|x)) + log pα(θ) −log qφ(θ)\n(21)\nUsing equations (20) and (18), the Monte Carlo estimate of the variational lower bound, given\ndatapoint x(i), is:\nL(φ; X) ≃1\nL\nL\nX\nl=1\nfφ(x(l), gφ(ϵ(l), x(l)), hφ(ζ(l)))\n(22)\nwhere ϵ(l) ∼p(ϵ) and ζ(l) ∼p(ζ). The estimator only depends on samples from p(ϵ) and p(ζ)\nwhich are obviously not inﬂuenced by φ, therefore the estimator can be differentiated w.r.t. φ.\nThe resulting stochastic gradients can be used in conjunction with stochastic optimization methods\nsuch as SGD or Adagrad [DHS10]. See algorithm 1 for a basic approach to computing stochastic\ngradients.\nF.1\nExample\nLet the prior over the parameters and latent variables be the centered isotropic Gaussian pα(θ) =\nN(z; 0, I) and pθ(z) = N(z; 0, I). Note that in this case, the prior lacks parameters. Let’s also\nassume that the true posteriors are approximatily Gaussian with an approximately diagonal covari-\nance. In this case, we can let the variational approximate posteriors be multivariate Gaussians with\na diagonal covariance structure:\nlog qφ(θ) = log N(θ; µθ, σ2\nθI)\nlog qφ(z|x) = log N(z; µz, σ2\nzI)\n(23)\n13\n\n\nAlgorithm 2 Pseudocode for computing a stochastic gradient using our estimator. See text for\nmeaning of the functions fφ, gφ and hφ.\nRequire: φ (Current value of variational parameters)\ng ←0\nfor l is 1 to L do\nx ←Random draw from dataset X\nϵ ←Random draw from prior p(ϵ)\nζ ←Random draw from prior p(ζ)\ng ←g + 1\nL∇φfφ(x, gφ(ϵ, x), hφ(ζ))\nend for\nreturn g\nwhere µz and σz are yet unspeciﬁed functions of x. Since they are Gaussian, we can parameterize\nthe variational approximate posteriors:\nqφ(θ)\nas\neθ = µθ + σθ ⊙ζ\nwhere\nζ ∼N(0, I)\nqφ(z|x)\nas\nez = µz + σz ⊙ϵ\nwhere\nϵ ∼N(0, I)\nWith ⊙we signify an element-wise product. These can be plugged into the lower bound deﬁned\nabove (eqs (21) and (22)).\nIn this case it is possible to construct an alternative estimator with a lower variance, since in this\nmodel pα(θ), pθ(z), qφ(θ) and qφ(z|x) are Gaussian, and therefore four terms of fφ can be solved\nanalytically. The resulting estimator is:\nL(φ; X) ≃1\nL\nL\nX\nl=1\nN ·\n\n1\n2\nJ\nX\nj=1\n\u0010\n1 + log((σ(l)\nz,j)2) −(µ(l)\nz,j)2 −(σ(l)\nz,j)2\u0011\n+ log pθ(x(i)z(i))\n\n\n+ 1\n2\nJ\nX\nj=1\n\u0010\n1 + log((σ(l)\nθ,j)2) −(µ(l)\nθ,j)2 −(σ(l)\nθ,j)2\u0011\n(24)\nµ(i)\nj\nand σ(i)\nj\nsimply denote the j-th element of vectors µ(i) and σ(i).\n14\n"
}