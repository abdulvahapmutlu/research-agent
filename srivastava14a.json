{
  "filename": "srivastava14a.pdf",
  "num_pages": 30,
  "pages": [
    "Journal of Machine Learning Research 15 (2014) 1929-1958\nSubmitted 11/13; Published 6/14\nDropout: A Simple Way to Prevent Neural Networks from\nOverﬁtting\nNitish Srivastava\nnitish@cs.toronto.edu\nGeoﬀrey Hinton\nhinton@cs.toronto.edu\nAlex Krizhevsky\nkriz@cs.toronto.edu\nIlya Sutskever\nilya@cs.toronto.edu\nRuslan Salakhutdinov\nrsalakhu@cs.toronto.edu\nDepartment of Computer Science\nUniversity of Toronto\n10 Kings College Road, Rm 3302\nToronto, Ontario, M5S 3G4, Canada.\nEditor: Yoshua Bengio\nAbstract\nDeep neural nets with a large number of parameters are very powerful machine learning\nsystems. However, overﬁtting is a serious problem in such networks. Large networks are also\nslow to use, making it diﬃcult to deal with overﬁtting by combining the predictions of many\ndiﬀerent large neural nets at test time. Dropout is a technique for addressing this problem.\nThe key idea is to randomly drop units (along with their connections) from the neural\nnetwork during training. This prevents units from co-adapting too much. During training,\ndropout samples from an exponential number of diﬀerent “thinned” networks. At test time,\nit is easy to approximate the eﬀect of averaging the predictions of all these thinned networks\nby simply using a single unthinned network that has smaller weights. This signiﬁcantly\nreduces overﬁtting and gives major improvements over other regularization methods. We\nshow that dropout improves the performance of neural networks on supervised learning\ntasks in vision, speech recognition, document classiﬁcation and computational biology,\nobtaining state-of-the-art results on many benchmark data sets.\nKeywords:\nneural networks, regularization, model combination, deep learning\n1. Introduction\nDeep neural networks contain multiple non-linear hidden layers and this makes them very\nexpressive models that can learn very complicated relationships between their inputs and\noutputs.\nWith limited training data, however, many of these complicated relationships\nwill be the result of sampling noise, so they will exist in the training set but not in real\ntest data even if it is drawn from the same distribution. This leads to overﬁtting and many\nmethods have been developed for reducing it. These include stopping the training as soon as\nperformance on a validation set starts to get worse, introducing weight penalties of various\nkinds such as L1 and L2 regularization and soft weight sharing (Nowlan and Hinton, 1992).\nWith unlimited computation, the best way to “regularize” a ﬁxed-sized model is to\naverage the predictions of all possible settings of the parameters, weighting each setting by\nc⃝2014 Nitish Srivastava, Geoﬀrey Hinton, Alex Krizhevsky, Ilya Sutskever and Ruslan Salakhutdinov.\n",
    "Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\n(a) Standard Neural Net\n(b) After applying dropout.\nFigure 1: Dropout Neural Net Model. Left: A standard neural net with 2 hidden layers. Right:\nAn example of a thinned net produced by applying dropout to the network on the left.\nCrossed units have been dropped.\nits posterior probability given the training data. This can sometimes be approximated quite\nwell for simple or small models (Xiong et al., 2011; Salakhutdinov and Mnih, 2008), but we\nwould like to approach the performance of the Bayesian gold standard using considerably\nless computation. We propose to do this by approximating an equally weighted geometric\nmean of the predictions of an exponential number of learned models that share parameters.\nModel combination nearly always improves the performance of machine learning meth-\nods. With large neural networks, however, the obvious idea of averaging the outputs of\nmany separately trained nets is prohibitively expensive. Combining several models is most\nhelpful when the individual models are diﬀerent from each other and in order to make\nneural net models diﬀerent, they should either have diﬀerent architectures or be trained\non diﬀerent data. Training many diﬀerent architectures is hard because ﬁnding optimal\nhyperparameters for each architecture is a daunting task and training each large network\nrequires a lot of computation. Moreover, large networks normally require large amounts of\ntraining data and there may not be enough data available to train diﬀerent networks on\ndiﬀerent subsets of the data. Even if one was able to train many diﬀerent large networks,\nusing them all at test time is infeasible in applications where it is important to respond\nquickly.\nDropout is a technique that addresses both these issues. It prevents overﬁtting and\nprovides a way of approximately combining exponentially many diﬀerent neural network\narchitectures eﬃciently.\nThe term “dropout” refers to dropping out units (hidden and\nvisible) in a neural network. By dropping a unit out, we mean temporarily removing it from\nthe network, along with all its incoming and outgoing connections, as shown in Figure 1.\nThe choice of which units to drop is random. In the simplest case, each unit is retained with\na ﬁxed probability p independent of other units, where p can be chosen using a validation\nset or can simply be set at 0.5, which seems to be close to optimal for a wide range of\nnetworks and tasks. For the input units, however, the optimal probability of retention is\nusually closer to 1 than to 0.5.\n1930\n",
    "Dropout\nPresent with\nprobability p\nw\n-\n(a) At training time\nAlways\npresent\npw\n-\n(b) At test time\nFigure 2: Left: A unit at training time that is present with probability p and is connected to units\nin the next layer with weights w. Right: At test time, the unit is always present and\nthe weights are multiplied by p. The output at test time is same as the expected output\nat training time.\nApplying dropout to a neural network amounts to sampling a “thinned” network from\nit. The thinned network consists of all the units that survived dropout (Figure 1b). A\nneural net with n units, can be seen as a collection of 2n possible thinned neural networks.\nThese networks all share weights so that the total number of parameters is still O(n2), or\nless. For each presentation of each training case, a new thinned network is sampled and\ntrained. So training a neural network with dropout can be seen as training a collection of 2n\nthinned networks with extensive weight sharing, where each thinned network gets trained\nvery rarely, if at all.\nAt test time, it is not feasible to explicitly average the predictions from exponentially\nmany thinned models. However, a very simple approximate averaging method works well in\npractice. The idea is to use a single neural net at test time without dropout. The weights\nof this network are scaled-down versions of the trained weights. If a unit is retained with\nprobability p during training, the outgoing weights of that unit are multiplied by p at test\ntime as shown in Figure 2. This ensures that for any hidden unit the expected output (under\nthe distribution used to drop units at training time) is the same as the actual output at\ntest time. By doing this scaling, 2n networks with shared weights can be combined into\na single neural network to be used at test time. We found that training a network with\ndropout and using this approximate averaging method at test time leads to signiﬁcantly\nlower generalization error on a wide variety of classiﬁcation problems compared to training\nwith other regularization methods.\nThe idea of dropout is not limited to feed-forward neural nets. It can be more generally\napplied to graphical models such as Boltzmann Machines.\nIn this paper, we introduce\nthe dropout Restricted Boltzmann Machine model and compare it to standard Restricted\nBoltzmann Machines (RBM). Our experiments show that dropout RBMs are better than\nstandard RBMs in certain respects.\nThis paper is structured as follows. Section 2 describes the motivation for this idea.\nSection 3 describes relevant previous work. Section 4 formally describes the dropout model.\nSection 5 gives an algorithm for training dropout networks. In Section 6, we present our\nexperimental results where we apply dropout to problems in diﬀerent domains and compare\nit with other forms of regularization and model combination. Section 7 analyzes the eﬀect of\ndropout on diﬀerent properties of a neural network and describes how dropout interacts with\nthe network’s hyperparameters. Section 8 describes the Dropout RBM model. In Section 9\nwe explore the idea of marginalizing dropout. In Appendix A we present a practical guide\n1931\n",
    "Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\nfor training dropout nets. This includes a detailed analysis of the practical considerations\ninvolved in choosing hyperparameters when training dropout networks.\n2. Motivation\nA motivation for dropout comes from a theory of the role of sex in evolution (Livnat et al.,\n2010). Sexual reproduction involves taking half the genes of one parent and half of the\nother, adding a very small amount of random mutation, and combining them to produce an\noﬀspring. The asexual alternative is to create an oﬀspring with a slightly mutated copy of\nthe parent’s genes. It seems plausible that asexual reproduction should be a better way to\noptimize individual ﬁtness because a good set of genes that have come to work well together\ncan be passed on directly to the oﬀspring. On the other hand, sexual reproduction is likely\nto break up these co-adapted sets of genes, especially if these sets are large and, intuitively,\nthis should decrease the ﬁtness of organisms that have already evolved complicated co-\nadaptations. However, sexual reproduction is the way most advanced organisms evolved.\nOne possible explanation for the superiority of sexual reproduction is that, over the long\nterm, the criterion for natural selection may not be individual ﬁtness but rather mix-ability\nof genes. The ability of a set of genes to be able to work well with another random set of\ngenes makes them more robust. Since a gene cannot rely on a large set of partners to be\npresent at all times, it must learn to do something useful on its own or in collaboration with\na small number of other genes. According to this theory, the role of sexual reproduction\nis not just to allow useful new genes to spread throughout the population, but also to\nfacilitate this process by reducing complex co-adaptations that would reduce the chance of\na new gene improving the ﬁtness of an individual. Similarly, each hidden unit in a neural\nnetwork trained with dropout must learn to work with a randomly chosen sample of other\nunits. This should make each hidden unit more robust and drive it towards creating useful\nfeatures on its own without relying on other hidden units to correct its mistakes. However,\nthe hidden units within a layer will still learn to do diﬀerent things from each other. One\nmight imagine that the net would become robust against dropout by making many copies\nof each hidden unit, but this is a poor solution for exactly the same reason as replica codes\nare a poor way to deal with a noisy channel.\nA closely related, but slightly diﬀerent motivation for dropout comes from thinking\nabout successful conspiracies.\nTen conspiracies each involving ﬁve people is probably a\nbetter way to create havoc than one big conspiracy that requires ﬁfty people to all play\ntheir parts correctly. If conditions do not change and there is plenty of time for rehearsal, a\nbig conspiracy can work well, but with non-stationary conditions, the smaller the conspiracy\nthe greater its chance of still working. Complex co-adaptations can be trained to work well\non a training set, but on novel test data they are far more likely to fail than multiple simpler\nco-adaptations that achieve the same thing.\n3. Related Work\nDropout can be interpreted as a way of regularizing a neural network by adding noise to\nits hidden units. The idea of adding noise to the states of units has previously been used in\nthe context of Denoising Autoencoders (DAEs) by Vincent et al. (2008, 2010) where noise\n1932\n",
    "Dropout\nis added to the input units of an autoencoder and the network is trained to reconstruct the\nnoise-free input. Our work extends this idea by showing that dropout can be eﬀectively\napplied in the hidden layers as well and that it can be interpreted as a form of model\naveraging.\nWe also show that adding noise is not only useful for unsupervised feature\nlearning but can also be extended to supervised learning problems. In fact, our method can\nbe applied to other neuron-based architectures, for example, Boltzmann Machines. While\n5% noise typically works best for DAEs, we found that our weight scaling procedure applied\nat test time enables us to use much higher noise levels. Dropping out 20% of the input units\nand 50% of the hidden units was often found to be optimal.\nSince dropout can be seen as a stochastic regularization technique, it is natural to\nconsider its deterministic counterpart which is obtained by marginalizing out the noise. In\nthis paper, we show that, in simple cases, dropout can be analytically marginalized out\nto obtain deterministic regularization methods.\nRecently, van der Maaten et al. (2013)\nalso explored deterministic regularizers corresponding to diﬀerent exponential-family noise\ndistributions, including dropout (which they refer to as “blankout noise”). However, they\napply noise to the inputs and only explore models with no hidden layers. Wang and Manning\n(2013) proposed a method for speeding up dropout by marginalizing dropout noise. Chen\net al. (2012) explored marginalization in the context of denoising autoencoders.\nIn dropout, we minimize the loss function stochastically under a noise distribution.\nThis can be seen as minimizing an expected loss function. Previous work of Globerson and\nRoweis (2006); Dekel et al. (2010) explored an alternate setting where the loss is minimized\nwhen an adversary gets to pick which units to drop. Here, instead of a noise distribution,\nthe maximum number of units that can be dropped is ﬁxed. However, this work also does\nnot explore models with hidden units.\n4. Model Description\nThis section describes the dropout neural network model. Consider a neural network with\nL hidden layers. Let l ∈{1, . . . , L} index the hidden layers of the network. Let z(l) denote\nthe vector of inputs into layer l, y(l) denote the vector of outputs from layer l (y(0) = x is\nthe input). W (l) and b(l) are the weights and biases at layer l. The feed-forward operation\nof a standard neural network (Figure 3a) can be described as (for l ∈{0, . . . , L −1} and\nany hidden unit i)\nz(l+1)\ni\n=\nw(l+1)\ni\nyl + b(l+1)\ni\n,\ny(l+1)\ni\n=\nf(z(l+1)\ni\n),\nwhere f is any activation function, for example, f(x) = 1/ (1 + exp(−x)).\nWith dropout, the feed-forward operation becomes (Figure 3b)\nr(l)\nj\n∼\nBernoulli(p),\ney(l)\n=\nr(l) ∗y(l),\nz(l+1)\ni\n=\nw(l+1)\ni\neyl + b(l+1)\ni\n,\ny(l+1)\ni\n=\nf(z(l+1)\ni\n).\n1933\n",
    "Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\n+1\ny(l)\n1\ny(l)\n2\ny(l)\n3\nz(l+1)\ni\ny(l+1)\ni\nw(l+1)\ni\nb(l+1)\ni\nf\n(a) Standard network\n+1\ney(l)\n1\ney(l)\n2\ney(l)\n3\nz(l+1)\ni\ny(l+1)\ni\ny(l)\n1\ny(l)\n2\ny(l)\n3\nr(l)\n1\nr(l)\n2\nr(l)\n3\nw(l+1)\ni\nb(l+1)\ni\nf\n(b) Dropout network\nFigure 3: Comparison of the basic operations of a standard and dropout network.\nHere ∗denotes an element-wise product. For any layer l, r(l) is a vector of independent\nBernoulli random variables each of which has probability p of being 1.\nThis vector is\nsampled and multiplied element-wise with the outputs of that layer, y(l), to create the\nthinned outputs ey(l). The thinned outputs are then used as input to the next layer. This\nprocess is applied at each layer. This amounts to sampling a sub-network from a larger\nnetwork. For learning, the derivatives of the loss function are backpropagated through the\nsub-network. At test time, the weights are scaled as W (l)\ntest = pW (l) as shown in Figure 2.\nThe resulting neural network is used without dropout.\n5. Learning Dropout Nets\nThis section describes a procedure for training dropout neural nets.\n5.1 Backpropagation\nDropout neural networks can be trained using stochastic gradient descent in a manner simi-\nlar to standard neural nets. The only diﬀerence is that for each training case in a mini-batch,\nwe sample a thinned network by dropping out units. Forward and backpropagation for that\ntraining case are done only on this thinned network. The gradients for each parameter are\naveraged over the training cases in each mini-batch. Any training case which does not use a\nparameter contributes a gradient of zero for that parameter. Many methods have been used\nto improve stochastic gradient descent such as momentum, annealed learning rates and L2\nweight decay. Those were found to be useful for dropout neural networks as well.\nOne particular form of regularization was found to be especially useful for dropout—\nconstraining the norm of the incoming weight vector at each hidden unit to be upper\nbounded by a ﬁxed constant c. In other words, if w represents the vector of weights incident\non any hidden unit, the neural network was optimized under the constraint ||w||2 ≤c. This\nconstraint was imposed during optimization by projecting w onto the surface of a ball of\nradius c, whenever w went out of it. This is also called max-norm regularization since it\nimplies that the maximum value that the norm of any weight can take is c. The constant\n1934\n",
    "Dropout\nc is a tunable hyperparameter, which is determined using a validation set.\nMax-norm\nregularization has been previously used in the context of collaborative ﬁltering (Srebro and\nShraibman, 2005).\nIt typically improves the performance of stochastic gradient descent\ntraining of deep neural nets, even when no dropout is used.\nAlthough dropout alone gives signiﬁcant improvements, using dropout along with max-\nnorm regularization, large decaying learning rates and high momentum provides a signiﬁcant\nboost over just using dropout. A possible justiﬁcation is that constraining weight vectors\nto lie inside a ball of ﬁxed radius makes it possible to use a huge learning rate without the\npossibility of weights blowing up. The noise provided by dropout then allows the optimiza-\ntion process to explore diﬀerent regions of the weight space that would have otherwise been\ndiﬃcult to reach. As the learning rate decays, the optimization takes shorter steps, thereby\ndoing less exploration and eventually settles into a minimum.\n5.2 Unsupervised Pretraining\nNeural networks can be pretrained using stacks of RBMs (Hinton and Salakhutdinov, 2006),\nautoencoders (Vincent et al., 2010) or Deep Boltzmann Machines (Salakhutdinov and Hin-\nton, 2009). Pretraining is an eﬀective way of making use of unlabeled data. Pretraining\nfollowed by ﬁnetuning with backpropagation has been shown to give signiﬁcant performance\nboosts over ﬁnetuning from random initializations in certain cases.\nDropout can be applied to ﬁnetune nets that have been pretrained using these tech-\nniques. The pretraining procedure stays the same. The weights obtained from pretraining\nshould be scaled up by a factor of 1/p. This makes sure that for each unit, the expected\noutput from it under random dropout will be the same as the output during pretraining.\nWe were initially concerned that the stochastic nature of dropout might wipe out the in-\nformation in the pretrained weights. This did happen when the learning rates used during\nﬁnetuning were comparable to the best learning rates for randomly initialized nets. How-\never, when the learning rates were chosen to be smaller, the information in the pretrained\nweights seemed to be retained and we were able to get improvements in terms of the ﬁnal\ngeneralization error compared to not using dropout when ﬁnetuning.\n6. Experimental Results\nWe trained dropout neural networks for classiﬁcation problems on data sets in diﬀerent\ndomains. We found that dropout improved generalization performance on all data sets\ncompared to neural networks that did not use dropout. Table 1 gives a brief description of\nthe data sets. The data sets are\n• MNIST : A standard toy data set of handwritten digits.\n• TIMIT : A standard speech benchmark for clean speech recognition.\n• CIFAR-10 and CIFAR-100 : Tiny natural images (Krizhevsky, 2009).\n• Street View House Numbers data set (SVHN) : Images of house numbers collected by\nGoogle Street View (Netzer et al., 2011).\n• ImageNet : A large collection of natural images.\n• Reuters-RCV1 : A collection of Reuters newswire articles.\n1935\n",
    "Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\n• Alternative Splicing data set: RNA features for predicting alternative gene splicing\n(Xiong et al., 2011).\nWe chose a diverse set of data sets to demonstrate that dropout is a general technique\nfor improving neural nets and is not speciﬁc to any particular application domain. In this\nsection, we present some key results that show the eﬀectiveness of dropout. A more detailed\ndescription of all the experiments and data sets is provided in Appendix B.\nData Set\nDomain\nDimensionality\nTraining Set\nTest Set\nMNIST\nVision\n784 (28 × 28 grayscale)\n60K\n10K\nSVHN\nVision\n3072 (32 × 32 color)\n600K\n26K\nCIFAR-10/100\nVision\n3072 (32 × 32 color)\n60K\n10K\nImageNet (ILSVRC-2012)\nVision\n65536 (256 × 256 color)\n1.2M\n150K\nTIMIT\nSpeech\n2520 (120-dim, 21 frames)\n1.1M frames\n58K frames\nReuters-RCV1\nText\n2000\n200K\n200K\nAlternative Splicing\nGenetics\n1014\n2932\n733\nTable 1: Overview of the data sets used in this paper.\n6.1 Results on Image Data Sets\nWe used ﬁve image data sets to evaluate dropout—MNIST, SVHN, CIFAR-10, CIFAR-100\nand ImageNet. These data sets include diﬀerent image types and training set sizes. Models\nwhich achieve state-of-the-art results on all of these data sets use dropout.\n6.1.1 MNIST\nMethod\nUnit\nType\nArchitecture\nError\n%\nStandard Neural Net (Simard et al., 2003)\nLogistic\n2 layers, 800 units\n1.60\nSVM Gaussian kernel\nNA\nNA\n1.40\nDropout NN\nLogistic\n3 layers, 1024 units\n1.35\nDropout NN\nReLU\n3 layers, 1024 units\n1.25\nDropout NN + max-norm constraint\nReLU\n3 layers, 1024 units\n1.06\nDropout NN + max-norm constraint\nReLU\n3 layers, 2048 units\n1.04\nDropout NN + max-norm constraint\nReLU\n2 layers, 4096 units\n1.01\nDropout NN + max-norm constraint\nReLU\n2 layers, 8192 units\n0.95\nDropout NN + max-norm constraint (Goodfellow\net al., 2013)\nMaxout\n2 layers, (5 × 240)\nunits\n0.94\nDBN + ﬁnetuning (Hinton and Salakhutdinov, 2006)\nLogistic\n500-500-2000\n1.18\nDBM + ﬁnetuning (Salakhutdinov and Hinton, 2009)\nLogistic\n500-500-2000\n0.96\nDBN + dropout ﬁnetuning\nLogistic\n500-500-2000\n0.92\nDBM + dropout ﬁnetuning\nLogistic\n500-500-2000\n0.79\nTable 2: Comparison of diﬀerent models on MNIST.\nThe MNIST data set consists of 28 × 28 pixel handwritten digit images. The task is\nto classify the images into 10 digit classes. Table 2 compares the performance of dropout\nwith other techniques. The best performing neural networks for the permutation invariant\n1936\n",
    "Dropout\nsetting that do not use dropout or unsupervised pretraining achieve an error of about\n1.60% (Simard et al., 2003). With dropout the error reduces to 1.35%. Replacing logistic\nunits with rectiﬁed linear units (ReLUs) (Jarrett et al., 2009) further reduces the error to\n1.25%. Adding max-norm regularization again reduces it to 1.06%. Increasing the size of\nthe network leads to better results. A neural net with 2 layers and 8192 units per layer\ngets down to 0.95% error. Note that this network has more than 65 million parameters and\nis being trained on a data set of size 60,000. Training a network of this size to give good\ngeneralization error is very hard with standard regularization methods and early stopping.\nDropout, on the other hand, prevents overﬁtting, even in this case. It does not even need\nearly stopping. Goodfellow et al. (2013) showed that results can be further improved to\n0.94% by replacing ReLU units with maxout units. All dropout nets use p = 0.5 for hidden\nunits and p = 0.8 for input units. More experimental details can be found in Appendix B.1.\nDropout nets pretrained with stacks of RBMs and Deep Boltzmann Machines also give\nimprovements as shown in Table 2. DBM—pretrained dropout nets achieve a test error of\n0.79% which is the best performance ever reported for the permutation invariant setting.\nWe note that it possible to obtain better results by using 2-D spatial information and\naugmenting the training set with distorted versions of images from the standard training\nset. We demonstrate the eﬀectiveness of dropout in that setting on more interesting data\nsets.\n0\n200000\n400000\n600000\n800000\n1000000\nNumber of weight updates\n1.0\n1.5\n2.0\n2.5\nClassification Error %\nWith dropout\nWithout dropout\n@\nR\n@\n@\nR\nFigure 4: Test error for diﬀerent architectures\nwith and without dropout.\nThe net-\nworks have 2 to 4 hidden layers each\nwith 1024 to 2048 units.\nIn\norder\nto\ntest\nthe\nrobustness\nof\ndropout,\nclassiﬁcation experiments were\ndone with networks of many diﬀerent ar-\nchitectures keeping all hyperparameters, in-\ncluding p, ﬁxed.\nFigure 4 shows the test\nerror rates obtained for these diﬀerent ar-\nchitectures as training progresses.\nThe\nsame architectures trained with and with-\nout dropout have drastically diﬀerent test\nerrors as seen as by the two separate clus-\nters of trajectories. Dropout gives a huge\nimprovement across all architectures, with-\nout using hyperparameters that were tuned\nspeciﬁcally for each architecture.\n6.1.2 Street View House Numbers\nThe Street View House Numbers (SVHN)\nData Set (Netzer et al., 2011) consists of\ncolor images of house numbers collected by\nGoogle Street View. Figure 5a shows some examples of images from this data set. The\npart of the data set that we use in our experiments consists of 32 × 32 color images roughly\ncentered on a digit in a house number. The task is to identify that digit.\nFor this data set, we applied dropout to convolutional neural networks (LeCun et al.,\n1989). The best architecture that we found has three convolutional layers followed by 2\nfully connected hidden layers. All hidden units were ReLUs. Each convolutional layer was\n1937\n",
    "Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\nMethod\nError %\nBinary Features (WDCH) (Netzer et al., 2011)\n36.7\nHOG (Netzer et al., 2011)\n15.0\nStacked Sparse Autoencoders (Netzer et al., 2011)\n10.3\nKMeans (Netzer et al., 2011)\n9.4\nMulti-stage Conv Net with average pooling (Sermanet et al., 2012)\n9.06\nMulti-stage Conv Net + L2 pooling (Sermanet et al., 2012)\n5.36\nMulti-stage Conv Net + L4 pooling + padding (Sermanet et al., 2012)\n4.90\nConv Net + max-pooling\n3.95\nConv Net + max pooling + dropout in fully connected layers\n3.02\nConv Net + stochastic pooling (Zeiler and Fergus, 2013)\n2.80\nConv Net + max pooling + dropout in all layers\n2.55\nConv Net + maxout (Goodfellow et al., 2013)\n2.47\nHuman Performance\n2.0\nTable 3: Results on the Street View House Numbers data set.\nfollowed by a max-pooling layer. Appendix B.2 describes the architecture in more detail.\nDropout was applied to all the layers of the network with the probability of retaining a hid-\nden unit being p = (0.9, 0.75, 0.75, 0.5, 0.5, 0.5) for the diﬀerent layers of the network (going\nfrom input to convolutional layers to fully connected layers). Max-norm regularization was\nused for weights in both convolutional and fully connected layers. Table 3 compares the\nresults obtained by diﬀerent methods. We ﬁnd that convolutional nets outperform other\nmethods. The best performing convolutional nets that do not use dropout achieve an error\nrate of 3.95%. Adding dropout only to the fully connected layers reduces the error to 3.02%.\nAdding dropout to the convolutional layers as well further reduces the error to 2.55%. Even\nmore gains can be obtained by using maxout units.\nThe additional gain in performance obtained by adding dropout in the convolutional\nlayers (3.02% to 2.55%) is worth noting. One may have presumed that since the convo-\nlutional layers don’t have a lot of parameters, overﬁtting is not a problem and therefore\ndropout would not have much eﬀect. However, dropout in the lower layers still helps be-\ncause it provides noisy inputs for the higher fully connected layers which prevents them\nfrom overﬁtting.\n6.1.3 CIFAR-10 and CIFAR-100\nThe CIFAR-10 and CIFAR-100 data sets consist of 32 × 32 color images drawn from 10\nand 100 categories respectively. Figure 5b shows some examples of images from this data\nset. A detailed description of the data sets, input preprocessing, network architectures and\nother experimental details is given in Appendix B.3. Table 4 shows the error rate obtained\nby diﬀerent methods on these data sets. Without any data augmentation, Snoek et al.\n(2012) used Bayesian hyperparameter optimization to obtained an error rate of 14.98% on\nCIFAR-10. Using dropout in the fully connected layers reduces that to 14.32% and adding\ndropout in every layer further reduces the error to 12.61%. Goodfellow et al. (2013) showed\nthat the error is further reduced to 11.68% by replacing ReLU units with maxout units. On\nCIFAR-100, dropout reduces the error from 43.48% to 37.20% which is a huge improvement.\nNo data augmentation was used for either data set (apart from the input dropout).\n1938\n",
    "Dropout\n(a) Street View House Numbers (SVHN)\n(b) CIFAR-10\nFigure 5: Samples from image data sets. Each row corresponds to a diﬀerent category.\nMethod\nCIFAR-10\nCIFAR-100\nConv Net + max pooling (hand tuned)\n15.60\n43.48\nConv Net + stochastic pooling (Zeiler and Fergus, 2013)\n15.13\n42.51\nConv Net + max pooling (Snoek et al., 2012)\n14.98\n-\nConv Net + max pooling + dropout fully connected layers\n14.32\n41.26\nConv Net + max pooling + dropout in all layers\n12.61\n37.20\nConv Net + maxout (Goodfellow et al., 2013)\n11.68\n38.57\nTable 4: Error rates on CIFAR-10 and CIFAR-100.\n6.1.4 ImageNet\nImageNet is a data set of over 15 million labeled high-resolution images belonging to roughly\n22,000 categories. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual\ncompetition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has\nbeen held. A subset of ImageNet with roughly 1000 images in each of 1000 categories is\nused in this challenge. Since the number of categories is rather large, it is conventional to\nreport two error rates: top-1 and top-5, where the top-5 error rate is the fraction of test\nimages for which the correct label is not among the ﬁve labels considered most probable by\nthe model. Figure 6 shows some predictions made by our model on a few test images.\nILSVRC-2010 is the only version of ILSVRC for which the test set labels are available, so\nmost of our experiments were performed on this data set. Table 5 compares the performance\nof diﬀerent methods. Convolutional nets with dropout outperform other methods by a large\nmargin. The architecture and implementation details are described in detail in Krizhevsky\net al. (2012).\n1939\n",
    "Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\nFigure 6: Some ImageNet test cases with the 4 most probable labels as predicted by our model.\nThe length of the horizontal bars is proportional to the probability assigned to the labels\nby the model. Pink indicates ground truth.\nModel\nTop-1\nTop-5\nSparse Coding (Lin et al., 2010)\n47.1\n28.2\nSIFT + Fisher Vectors (Sanchez and Perronnin, 2011)\n45.7\n25.7\nConv Net + dropout (Krizhevsky et al., 2012)\n37.5\n17.0\nTable 5: Results on the ILSVRC-2010 test set.\nModel\nTop-1\n(val)\nTop-5\n(val)\nTop-5\n(test)\nSVM on Fisher Vectors of Dense SIFT and Color Statistics\n-\n-\n27.3\nAvg of classiﬁers over FVs of SIFT, LBP, GIST and CSIFT\n-\n-\n26.2\nConv Net + dropout (Krizhevsky et al., 2012)\n40.7\n18.2\n-\nAvg of 5 Conv Nets + dropout (Krizhevsky et al., 2012)\n38.1\n16.4\n16.4\nTable 6: Results on the ILSVRC-2012 validation/test set.\nOur model based on convolutional nets and dropout won the ILSVRC-2012 competition.\nSince the labels for the test set are not available, we report our results on the test set for\nthe ﬁnal submission and include the validation set results for diﬀerent variations of our\nmodel. Table 6 shows the results from the competition. While the best methods based on\nstandard vision features achieve a top-5 error rate of about 26%, convolutional nets with\ndropout achieve a test error of about 16% which is a staggering diﬀerence. Figure 6 shows\nsome examples of predictions made by our model. We can see that the model makes very\nreasonable predictions, even when its best guess is not correct.\n6.2 Results on TIMIT\nNext, we applied dropout to a speech recognition task. We use the TIMIT data set which\nconsists of recordings from 680 speakers covering 8 major dialects of American English\nreading ten phonetically-rich sentences in a controlled noise-free environment.\nDropout\nneural networks were trained on windows of 21 log-ﬁlter bank frames to predict the label\nof the central frame.\nNo speaker dependent operations were performed.\nAppendix B.4\ndescribes the data preprocessing and training details. Table 7 compares dropout neural\n1940\n",
    "Dropout\nnets with other models. A 6-layer net gives a phone error rate of 23.4%. Dropout further\nimproves it to 21.8%. We also trained dropout nets starting from pretrained weights. A\n4-layer net pretrained with a stack of RBMs get a phone error rate of 22.7%. With dropout,\nthis reduces to 19.7%. Similarly, for an 8-layer net the error reduces from 20.5% to 19.7%.\nMethod\nPhone Error Rate%\nNN (6 layers) (Mohamed et al., 2010)\n23.4\nDropout NN (6 layers)\n21.8\nDBN-pretrained NN (4 layers)\n22.7\nDBN-pretrained NN (6 layers) (Mohamed et al., 2010)\n22.4\nDBN-pretrained NN (8 layers) (Mohamed et al., 2010)\n20.7\nmcRBM-DBN-pretrained NN (5 layers) (Dahl et al., 2010)\n20.5\nDBN-pretrained NN (4 layers) + dropout\n19.7\nDBN-pretrained NN (8 layers) + dropout\n19.7\nTable 7: Phone error rate on the TIMIT core test set.\n6.3 Results on a Text Data Set\nTo test the usefulness of dropout in the text domain, we used dropout networks to train a\ndocument classiﬁer. We used a subset of the Reuters-RCV1 data set which is a collection of\nover 800,000 newswire articles from Reuters. These articles cover a variety of topics. The\ntask is to take a bag of words representation of a document and classify it into 50 disjoint\ntopics. Appendix B.5 describes the setup in more detail. Our best neural net which did\nnot use dropout obtained an error rate of 31.05%. Adding dropout reduced the error to\n29.62%. We found that the improvement was much smaller compared to that for the vision\nand speech data sets.\n6.4 Comparison with Bayesian Neural Networks\nDropout can be seen as a way of doing an equally-weighted averaging of exponentially many\nmodels with shared weights. On the other hand, Bayesian neural networks (Neal, 1996) are\nthe proper way of doing model averaging over the space of neural network structures and\nparameters.\nIn dropout, each model is weighted equally, whereas in a Bayesian neural\nnetwork each model is weighted taking into account the prior and how well the model ﬁts\nthe data, which is the more correct approach. Bayesian neural nets are extremely useful for\nsolving problems in domains where data is scarce such as medical diagnosis, genetics, drug\ndiscovery and other computational biology applications. However, Bayesian neural nets are\nslow to train and diﬃcult to scale to very large network sizes. Besides, it is expensive to\nget predictions from many large nets at test time. On the other hand, dropout neural nets\nare much faster to train and use at test time. In this section, we report experiments that\ncompare Bayesian neural nets with dropout neural nets on a small data set where Bayesian\nneural networks are known to perform well and obtain state-of-the-art results. The aim is\nto analyze how much does dropout lose compared to Bayesian neural nets.\nThe data set that we use (Xiong et al., 2011) comes from the domain of genetics. The\ntask is to predict the occurrence of alternative splicing based on RNA features. Alternative\nsplicing is a signiﬁcant cause of cellular diversity in mammalian tissues. Predicting the\n1941\n",
    "Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\nMethod\nCode Quality (bits)\nNeural Network (early stopping) (Xiong et al., 2011)\n440\nRegression, PCA (Xiong et al., 2011)\n463\nSVM, PCA (Xiong et al., 2011)\n487\nNeural Network with dropout\n567\nBayesian Neural Network (Xiong et al., 2011)\n623\nTable 8: Results on the Alternative Splicing Data Set.\noccurrence of alternate splicing in certain tissues under diﬀerent conditions is important for\nunderstanding many human diseases. Given the RNA features, the task is to predict the\nprobability of three splicing related events that biologists care about. The evaluation metric\nis Code Quality which is a measure of the negative KL divergence between the target and\nthe predicted probability distributions (higher is better). Appendix B.6 includes a detailed\ndescription of the data set and this performance metric.\nTable 8 summarizes the performance of diﬀerent models on this data set. Xiong et al.\n(2011) used Bayesian neural nets for this task. As expected, we found that Bayesian neural\nnets perform better than dropout. However, we see that dropout improves signiﬁcantly\nupon the performance of standard neural nets and outperforms all other methods. The\nchallenge in this data set is to prevent overﬁtting since the size of the training set is small.\nOne way to prevent overﬁtting is to reduce the input dimensionality using PCA. Thereafter,\nstandard techniques such as SVMs or logistic regression can be used. However, with dropout\nwe were able to prevent overﬁtting without the need to do dimensionality reduction. The\ndropout nets are very large (1000s of hidden units) compared to a few tens of units in the\nBayesian network. This shows that dropout has a strong regularizing eﬀect.\n6.5 Comparison with Standard Regularizers\nSeveral regularization methods have been proposed for preventing overﬁtting in neural net-\nworks. These include L2 weight decay (more generally Tikhonov regularization (Tikhonov,\n1943)), lasso (Tibshirani, 1996), KL-sparsity and max-norm regularization. Dropout can\nbe seen as another way of regularizing neural networks. In this section we compare dropout\nwith some of these regularization methods using the MNIST data set.\nThe same network architecture (784-1024-1024-2048-10) with ReLUs was trained us-\ning stochastic gradient descent with diﬀerent regularizations. Table 9 shows the results.\nThe values of diﬀerent hyperparameters associated with each kind of regularization (decay\nconstants, target sparsity, dropout rate, max-norm upper bound) were obtained using a\nvalidation set. We found that dropout combined with max-norm regularization gives the\nlowest generalization error.\n7. Salient Features\nThe experiments described in the previous section provide strong evidence that dropout\nis a useful technique for improving neural networks. In this section, we closely examine\nhow dropout aﬀects a neural network. We analyze the eﬀect of dropout on the quality of\nfeatures produced. We see how dropout aﬀects the sparsity of hidden unit activations. We\n1942\n",
    "Dropout\nMethod\nTest Classiﬁcation error %\nL2\n1.62\nL2 + L1 applied towards the end of training\n1.60\nL2 + KL-sparsity\n1.55\nMax-norm\n1.35\nDropout + L2\n1.25\nDropout + Max-norm\n1.05\nTable 9: Comparison of diﬀerent regularization methods on MNIST.\nalso see how the advantages obtained from dropout vary with the probability of retaining\nunits, size of the network and the size of the training set. These observations give some\ninsight into why dropout works so well.\n7.1 Eﬀect on Features\n(a) Without dropout\n(b) Dropout with p = 0.5.\nFigure 7: Features learned on MNIST with one hidden layer autoencoders having 256 rectiﬁed\nlinear units.\nIn a standard neural network, the derivative received by each parameter tells it how it\nshould change so the ﬁnal loss function is reduced, given what all other units are doing.\nTherefore, units may change in a way that they ﬁx up the mistakes of the other units.\nThis may lead to complex co-adaptations. This in turn leads to overﬁtting because these\nco-adaptations do not generalize to unseen data. We hypothesize that for each hidden unit,\ndropout prevents co-adaptation by making the presence of other hidden units unreliable.\nTherefore, a hidden unit cannot rely on other speciﬁc units to correct its mistakes. It must\nperform well in a wide variety of diﬀerent contexts provided by the other hidden units. To\nobserve this eﬀect directly, we look at the ﬁrst level features learned by neural networks\ntrained on visual tasks with and without dropout.\n1943\n",
    "Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\nFigure 7a shows features learned by an autoencoder on MNIST with a single hidden\nlayer of 256 rectiﬁed linear units without dropout. Figure 7b shows the features learned by\nan identical autoencoder which used dropout in the hidden layer with p = 0.5. Both au-\ntoencoders had similar test reconstruction errors. However, it is apparent that the features\nshown in Figure 7a have co-adapted in order to produce good reconstructions. Each hidden\nunit on its own does not seem to be detecting a meaningful feature. On the other hand, in\nFigure 7b, the hidden units seem to detect edges, strokes and spots in diﬀerent parts of the\nimage. This shows that dropout does break up co-adaptations, which is probably the main\nreason why it leads to lower generalization errors.\n7.2 Eﬀect on Sparsity\n(a) Without dropout\n(b) Dropout with p = 0.5.\nFigure 8: Eﬀect of dropout on sparsity. ReLUs were used for both models. Left: The histogram\nof mean activations shows that most units have a mean activation of about 2.0. The\nhistogram of activations shows a huge mode away from zero. Clearly, a large fraction of\nunits have high activation. Right: The histogram of mean activations shows that most\nunits have a smaller mean mean activation of about 0.7. The histogram of activations\nshows a sharp peak at zero. Very few units have high activation.\nWe found that as a side-eﬀect of doing dropout, the activations of the hidden units\nbecome sparse, even when no sparsity inducing regularizers are present. Thus, dropout au-\ntomatically leads to sparse representations. To observe this eﬀect, we take the autoencoders\ntrained in the previous section and look at the sparsity of hidden unit activations on a ran-\ndom mini-batch taken from the test set. Figure 8a and Figure 8b compare the sparsity for\nthe two models. In a good sparse model, there should only be a few highly activated units\nfor any data case. Moreover, the average activation of any unit across data cases should\nbe low. To assess both of these qualities, we plot two histograms for each model. For each\nmodel, the histogram on the left shows the distribution of mean activations of hidden units\nacross the minibatch. The histogram on the right shows the distribution of activations of\nthe hidden units.\nComparing the histograms of activations we can see that fewer hidden units have high\nactivations in Figure 8b compared to Figure 8a, as seen by the signiﬁcant mass away from\n1944\n",
    "Dropout\nzero for the net that does not use dropout. The mean activations are also smaller for the\ndropout net. The overall mean activation of hidden units is close to 2.0 for the autoencoder\nwithout dropout but drops to around 0.7 when dropout is used.\n7.3 Eﬀect of Dropout Rate\nDropout has a tunable hyperparameter p (the probability of retaining a unit in the network).\nIn this section, we explore the eﬀect of varying this hyperparameter. The comparison is\ndone in two situations.\n1. The number of hidden units is held constant.\n2. The number of hidden units is changed so that the expected number of hidden units\nthat will be retained after dropout is held constant.\nIn the ﬁrst case, we train the same network architecture with diﬀerent amounts of\ndropout. We use a 784-2048-2048-2048-10 architecture. No input dropout was used. Fig-\nure 9a shows the test error obtained as a function of p. If the architecture is held constant,\nhaving a small p means very few units will turn on during training. It can be seen that this\nhas led to underﬁtting since the training error is also high. We see that as p increases, the\nerror goes down. It becomes ﬂat when 0.4 ≤p ≤0.8 and then increases as p becomes close\nto 1.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProbability of retaining a unit (p)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nClassification Error %\nTest Error\nTraining Error\n(a) Keeping n ﬁxed.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProbability of retaining a unit (p)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nClassification Error %\nTest Error\nTraining Error\n(b) Keeping pn ﬁxed.\nFigure 9: Eﬀect of changing dropout rates on MNIST.\nAnother interesting setting is the second case in which the quantity pn is held constant\nwhere n is the number of hidden units in any particular layer. This means that networks\nthat have small p will have a large number of hidden units.\nTherefore, after applying\ndropout, the expected number of units that are present will be the same across diﬀerent\narchitectures. However, the test networks will be of diﬀerent sizes. In our experiments,\nwe set pn = 256 for the ﬁrst two hidden layers and pn = 512 for the last hidden layer.\nFigure 9b shows the test error obtained as a function of p. We notice that the magnitude\nof errors for small values of p has reduced by a lot compared to Figure 9a (for p = 0.1 it fell\nfrom 2.7% to 1.7%). Values of p that are close to 0.6 seem to perform best for this choice\nof pn but our usual default value of 0.5 is close to optimal.\n1945\n",
    "Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\n7.4 Eﬀect of Data Set Size\nOne test of a good regularizer is that it should make it possible to get good generalization\nerror from models with a large number of parameters trained on small data sets. This\nsection explores the eﬀect of changing the data set size when dropout is used with feed-\nforward networks. Huge neural networks trained in the standard way overﬁt massively on\nsmall data sets. To see if dropout can help, we run classiﬁcation experiments on MNIST\nand vary the amount of data given to the network.\n10\n2\n10\n3\n10\n4\n10\n5\nDataset size\n0\n5\n10\n15\n20\n25\n30\nClassification Error %\nWith dropout\nWithout dropout\nFigure 10: Eﬀect of varying data set size.\nThe results of these experiments are\nshown in Figure 10. The network was given\ndata sets of size 100, 500, 1K, 5K, 10K\nand 50K chosen randomly from the MNIST\ntraining set.\nThe same network architec-\nture (784-1024-1024-2048-10) was used for\nall data sets. Dropout with p = 0.5 was per-\nformed at all the hidden layers and p = 0.8\nat the input layer. It can be observed that\nfor extremely small data sets (100, 500)\ndropout does not give any improvements.\nThe model has enough parameters that it\ncan overﬁt on the training data, even with\nall the noise coming from dropout. As the\nsize of the data set is increased, the gain\nfrom doing dropout increases up to a point and then declines. This suggests that for any\ngiven architecture and dropout rate, there is a “sweet spot” corresponding to some amount\nof data that is large enough to not be memorized in spite of the noise but not so large that\noverﬁtting is not a problem anyways.\n7.5 Monte-Carlo Model Averaging vs. Weight Scaling\n0\n20\n40\n60\n80\n100\n120\nNumber of samples used for Monte-Carlo averaging (k)\n1.00\n1.05\n1.10\n1.15\n1.20\n1.25\n1.30\n1.35\nTest Classification error %\nMonte-Carlo Model Averaging\nApproximate averaging by weight scaling\nFigure 11: Monte-Carlo\nmodel\naveraging\nvs.\nweight scaling.\nThe eﬃcient test time procedure that we\npropose is to do an approximate model com-\nbination by scaling down the weights of the\ntrained neural network. An expensive but\nmore correct way of averaging the models\nis to sample k neural nets using dropout for\neach test case and average their predictions.\nAs k →∞, this Monte-Carlo model average\ngets close to the true model average. It is in-\nteresting to see empirically how many sam-\nples k are needed to match the performance\nof the approximate averaging method. By\ncomputing the error for diﬀerent values of k\nwe can see how quickly the error rate of the\nﬁnite-sample average approaches the error\nrate of the true model average.\n1946\n",
    "Dropout\nWe again use the MNIST data set and do classiﬁcation by averaging the predictions\nof k randomly sampled neural networks. Figure 11 shows the test error rate obtained for\ndiﬀerent values of k. This is compared with the error obtained using the weight scaling\nmethod (shown as a horizontal line). It can be seen that around k = 50, the Monte-Carlo\nmethod becomes as good as the approximate method. Thereafter, the Monte-Carlo method\nis slightly better than the approximate method but well within one standard deviation of\nit. This suggests that the weight scaling method is a fairly good approximation of the true\nmodel average.\n8. Dropout Restricted Boltzmann Machines\nBesides feed-forward neural networks, dropout can also be applied to Restricted Boltzmann\nMachines (RBM). In this section, we formally describe this model and show some results\nto illustrate its key properties.\n8.1 Model Description\nConsider an RBM with visible units v ∈{0, 1}D and hidden units h ∈{0, 1}F . It deﬁnes\nthe following probability distribution\nP(h, v; θ) =\n1\nZ(θ) exp(v⊤Wh + a⊤h + b⊤v).\nWhere θ = {W, a, b} represents the model parameters and Z is the partition function.\nDropout RBMs are RBMs augmented with a vector of binary random variables r ∈\n{0, 1}F .\nEach random variable rj takes the value 1 with probability p, independent of\nothers. If rj takes the value 1, the hidden unit hj is retained, otherwise it is dropped from\nthe model. The joint distribution deﬁned by a Dropout RBM can be expressed as\nP(r, h, v; p, θ)\n=\nP(r; p)P(h, v|r; θ),\nP(r; p)\n=\nF\nY\nj=1\nprj(1 −p)1−rj,\nP(h, v|r; θ)\n=\n1\nZ′(θ, r) exp(v⊤Wh + a⊤h + b⊤v)\nF\nY\nj=1\ng(hj, rj),\ng(hj, rj)\n=\n1(rj = 1) + 1(rj = 0)1(hj = 0).\nZ′(θ, r) is the normalization constant. g(hj, rj) imposes the constraint that if rj = 0,\nhj must be 0. The distribution over h, conditioned on v and r is factorial\nP(h|r, v)\n=\nF\nY\nj=1\nP(hj|rj, v),\nP(hj = 1|rj, v)\n=\n1(rj = 1)σ\n \nbj +\nX\ni\nWijvi\n!\n.\n1947\n",
    "Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\n(a) Without dropout\n(b) Dropout with p = 0.5.\nFigure 12: Features learned on MNIST by 256 hidden unit RBMs. The features are ordered by L2\nnorm.\nThe distribution over v conditioned on h is same as that of an RBM\nP(v|h)\n=\nD\nY\ni=1\nP(vi|h),\nP(vi = 1|h)\n=\nσ\n\nai +\nX\nj\nWijhj\n\n.\nConditioned on r, the distribution over {v, h} is same as the distribution that an RBM\nwould impose, except that the units for which rj = 0 are dropped from h. Therefore, the\nDropout RBM model can be seen as a mixture of exponentially many RBMs with shared\nweights each using a diﬀerent subset of h.\n8.2 Learning Dropout RBMs\nLearning algorithms developed for RBMs such as Contrastive Divergence (Hinton et al.,\n2006) can be directly applied for learning Dropout RBMs. The only diﬀerence is that r is\nﬁrst sampled and only the hidden units that are retained are used for training. Similar to\ndropout neural networks, a diﬀerent r is sampled for each training case in every minibatch.\nIn our experiments, we use CD-1 for training dropout RBMs.\n8.3 Eﬀect on Features\nDropout in feed-forward networks improved the quality of features by reducing co-adaptations.\nThis section explores whether this eﬀect transfers to Dropout RBMs as well.\nFigure 12a shows features learned by a binary RBM with 256 hidden units. Figure 12b\nshows features learned by a dropout RBM with the same number of hidden units. Features\n1948\n",
    "Dropout\n(a) Without dropout\n(b) Dropout with p = 0.5.\nFigure 13: Eﬀect of dropout on sparsity. Left: The activation histogram shows that a large num-\nber of units have activations away from zero. Right: A large number of units have\nactivations close to zero and very few units have high activation.\nlearned by the dropout RBM appear qualitatively diﬀerent in the sense that they seem to\ncapture features that are coarser compared to the sharply deﬁned stroke-like features in the\nstandard RBM. There seem to be very few dead units in the dropout RBM relative to the\nstandard RBM.\n8.4 Eﬀect on Sparsity\nNext, we investigate the eﬀect of dropout RBM training on sparsity of the hidden unit\nactivations. Figure 13a shows the histograms of hidden unit activations and their means on\na test mini-batch after training an RBM. Figure 13b shows the same for dropout RBMs.\nThe histograms clearly indicate that the dropout RBMs learn much sparser representations\nthan standard RBMs even when no additional sparsity inducing regularizer is present.\n9. Marginalizing Dropout\nDropout can be seen as a way of adding noise to the states of hidden units in a neural\nnetwork. In this section, we explore the class of models that arise as a result of marginalizing\nthis noise. These models can be seen as deterministic versions of dropout. In contrast to\nstandard (“Monte-Carlo”) dropout, these models do not need random bits and it is possible\nto get gradients for the marginalized loss functions. In this section, we brieﬂy explore these\nmodels.\nDeterministic algorithms have been proposed that try to learn models that are robust to\nfeature deletion at test time (Globerson and Roweis, 2006). Marginalization in the context\nof denoising autoencoders has been explored previously (Chen et al., 2012). The marginal-\nization of dropout noise in the context of linear regression was discussed in Srivastava (2013).\nWang and Manning (2013) further explored the idea of marginalizing dropout to speed-up\ntraining. van der Maaten et al. (2013) investigated diﬀerent input noise distributions and\n1949\n",
    "Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\nthe regularizers obtained by marginalizing this noise. Wager et al. (2013) describes how\ndropout can be seen as an adaptive regularizer.\n9.1 Linear Regression\nFirst we explore a very simple case of applying dropout to the classical problem of linear\nregression. Let X ∈RN×D be a data matrix of N data points. y ∈RN be a vector of\ntargets. Linear regression tries to ﬁnd a w ∈RD that minimizes\n||y −Xw||2.\nWhen the input X is dropped out such that any input dimension is retained with\nprobability p, the input can be expressed as R∗X where R ∈{0, 1}N×D is a random matrix\nwith Rij ∼Bernoulli(p) and ∗denotes an element-wise product. Marginalizing the noise,\nthe objective function becomes\nminimize\nw\nER∼Bernoulli(p)\n\u0002\n||y −(R ∗X)w||2\u0003\n.\nThis reduces to\nminimize\nw\n||y −pXw||2 + p(1 −p)||Γw||2,\nwhere Γ = (diag(X⊤X))1/2.\nTherefore, dropout with linear regression is equivalent, in\nexpectation, to ridge regression with a particular form for Γ. This form of Γ essentially\nscales the weight cost for weight wi by the standard deviation of the ith dimension of the\ndata. If a particular data dimension varies a lot, the regularizer tries to squeeze its weight\nmore.\nAnother interesting way to look at this objective is to absorb the factor of p into w.\nThis leads to the following form\nminimize\nw\n||y −X ew||2 + 1 −p\np\n||Γew||2,\nwhere ew = pw. This makes the dependence of the regularization constant on p explicit.\nFor p close to 1, all the inputs are retained and the regularization constant is small. As\nmore dropout is done (by decreasing p), the regularization constant grows larger.\n9.2 Logistic Regression and Deep Networks\nFor logistic regression and deep neural nets, it is hard to obtain a closed form marginalized\nmodel. However, Wang and Manning (2013) showed that in the context of dropout applied\nto logistic regression, the corresponding marginalized model can be trained approximately.\nUnder reasonable assumptions, the distributions over the inputs to the logistic unit and over\nthe gradients of the marginalized model are Gaussian. Their means and variances can be\ncomputed eﬃciently. This approximate marginalization outperforms Monte-Carlo dropout\nin terms of training time and generalization performance.\nHowever, the assumptions involved in this technique become successively weaker as more\nlayers are added. Therefore, the results are not directly applicable to deep networks.\n1950\n",
    "Dropout\nData Set\nArchitecture\nBernoulli dropout\nGaussian dropout\nMNIST\n2 layers, 1024 units each\n1.08 ± 0.04\n0.95 ± 0.04\nCIFAR-10\n3 conv + 2 fully connected layers\n12.6 ± 0.1\n12.5 ± 0.1\nTable 10: Comparison of classiﬁcation error % with Bernoulli and Gaussian dropout. For MNIST,\nthe Bernoulli model uses p = 0.5 for the hidden units and p = 0.8 for the input units.\nFor CIFAR-10, we use p = (0.9, 0.75, 0.75, 0.5, 0.5, 0.5) going from the input layer to the\ntop. The value of σ for the Gaussian dropout models was set to be\nq\n1−p\np . Results were\naveraged over 10 diﬀerent random seeds.\n10. Multiplicative Gaussian Noise\nDropout involves multiplying hidden activations by Bernoulli distributed random variables\nwhich take the value 1 with probability p and 0 otherwise. This idea can be generalized\nby multiplying the activations with random variables drawn from other distributions. We\nrecently discovered that multiplying by a random variable drawn from N(1, 1) works just\nas well, or perhaps better than using Bernoulli noise. This new form of dropout amounts\nto adding a Gaussian distributed random variable with zero mean and standard deviation\nequal to the activation of the unit.\nThat is, each hidden activation hi is perturbed to\nhi + hir where r ∼N(0, 1), or equivalently hir′ where r′ ∼N(1, 1). We can generalize\nthis to r′ ∼N(1, σ2) where σ becomes an additional hyperparameter to tune, just like p\nwas in the standard (Bernoulli) dropout. The expected value of the activations remains\nunchanged, therefore no weight scaling is required at test time.\nIn this paper, we described dropout as a method where we retain units with probability p\nat training time and scale down the weights by multiplying them by a factor of p at test time.\nAnother way to achieve the same eﬀect is to scale up the retained activations by multiplying\nby 1/p at training time and not modifying the weights at test time. These methods are\nequivalent with appropriate scaling of the learning rate and weight initializations at each\nlayer.\nTherefore, dropout can be seen as multiplying hi by a Bernoulli random variable rb that\ntakes the value 1/p with probability p and 0 otherwise. E[rb] = 1 and V ar[rb] = (1 −p)/p.\nFor the Gaussian multiplicative noise, if we set σ2 = (1 −p)/p, we end up multiplying\nhi by a random variable rg, where E[rg] = 1 and V ar[rg] = (1 −p)/p. Therefore, both\nforms of dropout can be set up so that the random variable being multiplied by has the\nsame mean and variance. However, given these ﬁrst and second order moments, rg has the\nhighest entropy and rb has the lowest. Both these extremes work well, although preliminary\nexperimental results shown in Table 10 suggest that the high entropy case might work\nslightly better. For each layer, the value of σ in the Gaussian model was set to be\nq\n1−p\np\nusing the p from the corresponding layer in the Bernoulli model.\n11. Conclusion\nDropout is a technique for improving neural networks by reducing overﬁtting. Standard\nbackpropagation learning builds up brittle co-adaptations that work for the training data\nbut do not generalize to unseen data. Random dropout breaks up these co-adaptations by\n1951\n",
    "Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\nmaking the presence of any particular hidden unit unreliable. This technique was found\nto improve the performance of neural nets in a wide variety of application domains includ-\ning object classiﬁcation, digit recognition, speech recognition, document classiﬁcation and\nanalysis of computational biology data. This suggests that dropout is a general technique\nand is not speciﬁc to any domain. Methods that use dropout achieve state-of-the-art re-\nsults on SVHN, ImageNet, CIFAR-100 and MNIST. Dropout considerably improved the\nperformance of standard neural nets on other data sets as well.\nThis idea can be extended to Restricted Boltzmann Machines and other graphical mod-\nels. The central idea of dropout is to take a large model that overﬁts easily and repeatedly\nsample and train smaller sub-models from it. RBMs easily ﬁt into this framework. We de-\nveloped Dropout RBMs and empirically showed that they have certain desirable properties.\nOne of the drawbacks of dropout is that it increases training time. A dropout network\ntypically takes 2-3 times longer to train than a standard neural network of the same ar-\nchitecture. A major cause of this increase is that the parameter updates are very noisy.\nEach training case eﬀectively tries to train a diﬀerent random architecture. Therefore, the\ngradients that are being computed are not gradients of the ﬁnal architecture that will be\nused at test time. Therefore, it is not surprising that training takes a long time. However,\nit is likely that this stochasticity prevents overﬁtting. This creates a trade-oﬀbetween over-\nﬁtting and training time. With more training time, one can use high dropout and suﬀer less\noverﬁtting. However, one way to obtain some of the beneﬁts of dropout without stochas-\nticity is to marginalize the noise to obtain a regularizer that does the same thing as the\ndropout procedure, in expectation. We showed that for linear regression this regularizer is\na modiﬁed form of L2 regularization. For more complicated models, it is not obvious how to\nobtain an equivalent regularizer. Speeding up dropout is an interesting direction for future\nwork.\nAcknowledgments\nThis research was supported by OGS, NSERC and an Early Researcher Award.\nAppendix A. A Practical Guide for Training Dropout Networks\nNeural networks are infamous for requiring extensive hyperparameter tuning.\nDropout\nnetworks are no exception. In this section, we describe heuristics that might be useful for\napplying dropout.\nA.1 Network Size\nIt is to be expected that dropping units will reduce the capacity of a neural network. If\nn is the number of hidden units in any layer and p is the probability of retaining a unit,\nthen instead of n hidden units, only pn units will be present after dropout, in expectation.\nMoreover, this set of pn units will be diﬀerent each time and the units are not allowed to\nbuild co-adaptations freely. Therefore, if an n-sized layer is optimal for a standard neural\nnet on any given task, a good dropout net should have at least n/p units. We found this to\nbe a useful heuristic for setting the number of hidden units in both convolutional and fully\nconnected networks.\n1952\n",
    "Dropout\nA.2 Learning Rate and Momentum\nDropout introduces a signiﬁcant amount of noise in the gradients compared to standard\nstochastic gradient descent. Therefore, a lot of gradients tend to cancel each other. In\norder to make up for this, a dropout net should typically use 10-100 times the learning rate\nthat was optimal for a standard neural net. Another way to reduce the eﬀect the noise is\nto use a high momentum. While momentum values of 0.9 are common for standard nets,\nwith dropout we found that values around 0.95 to 0.99 work quite a lot better. Using high\nlearning rate and/or momentum signiﬁcantly speed up learning.\nA.3 Max-norm Regularization\nThough large momentum and learning rate speed up learning, they sometimes cause the\nnetwork weights to grow very large. To prevent this, we can use max-norm regularization.\nThis constrains the norm of the vector of incoming weights at each hidden unit to be bound\nby a constant c. Typical values of c range from 3 to 4.\nA.4 Dropout Rate\nDropout introduces an extra hyperparameter—the probability of retaining a unit p. This\nhyperparameter controls the intensity of dropout. p = 1, implies no dropout and low values\nof p mean more dropout. Typical values of p for hidden units are in the range 0.5 to 0.8.\nFor input layers, the choice depends on the kind of input. For real-valued inputs (image\npatches or speech frames), a typical value is 0.8. For hidden layers, the choice of p is coupled\nwith the choice of number of hidden units n. Smaller p requires big n which slows down\nthe training and leads to underﬁtting. Large p may not produce enough dropout to prevent\noverﬁtting.\nAppendix B. Detailed Description of Experiments and Data Sets\n.\nThis section describes the network architectures and training details for the experimental\nresults reported in this paper. The code for reproducing these results can be obtained from\nhttp://www.cs.toronto.edu/~nitish/dropout. The implementation is GPU-based. We\nused the excellent CUDA libraries—cudamat (Mnih, 2009) and cuda-convnet (Krizhevsky\net al., 2012) to implement our networks.\nB.1 MNIST\nThe MNIST data set consists of 60,000 training and 10,000 test examples each representing\na 28×28 digit image. We held out 10,000 random training images for validation. Hyperpa-\nrameters were tuned on the validation set such that the best validation error was produced\nafter 1 million weight updates. The validation set was then combined with the training set\nand training was done for 1 million weight updates. This net was used to evaluate the per-\nformance on the test set. This way of using the validation set was chosen because we found\nthat it was easy to set up hyperparameters so that early stopping was not required at all.\nTherefore, once the hyperparameters were ﬁxed, it made sense to combine the validation\nand training sets and train for a very long time.\n1953\n",
    "Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\nThe architectures shown in Figure 4 include all combinations of 2, 3, and 4 layer networks\nwith 1024 and 2048 units in each layer. Thus, there are six architectures in all. For all the\narchitectures (including the ones reported in Table 2), we used p = 0.5 in all hidden layers\nand p = 0.8 in the input layer. A ﬁnal momentum of 0.95 and weight constraints with c = 2\nwas used in all the layers.\nTo test the limits of dropout’s regularization power, we also experimented with 2 and 3\nlayer nets having 4096 and 8192 units. 2 layer nets gave improvements as shown in Table 2.\nHowever, the three layer nets performed slightly worse than 2 layer ones with the same\nlevel of dropout. When we increased dropout, performance improved but not enough to\noutperform the 2 layer nets.\nB.2 SVHN\nThe SVHN data set consists of approximately 600,000 training images and 26,000 test\nimages. The training set consists of two parts—A standard labeled training set and another\nset of labeled examples that are easy. A validation set was constructed by taking examples\nfrom both the parts. Two-thirds of it were taken from the standard set (400 per class) and\none-third from the extra set (200 per class), a total of 6000 samples. This same process\nis used by Sermanet et al. (2012). The inputs were RGB pixels normalized to have zero\nmean and unit variance. Other preprocessing techniques such as global or local contrast\nnormalization or ZCA whitening did not give any noticeable improvements.\nThe best architecture that we found uses three convolutional layers each followed by\na max-pooling layer. The convolutional layers have 96, 128 and 256 ﬁlters respectively.\nEach convolutional layer has a 5 × 5 receptive ﬁeld applied with a stride of 1 pixel. Each\nmax pooling layer pools 3 × 3 regions at strides of 2 pixels. The convolutional layers are\nfollowed by two fully connected hidden layers having 2048 units each. All units use the\nrectiﬁed linear activation function. Dropout was applied to all the layers of the network\nwith the probability of retaining the unit being p = (0.9, 0.75, 0.75, 0.5, 0.5, 0.5) for the\ndiﬀerent layers of the network (going from input to convolutional layers to fully connected\nlayers). In addition, the max-norm constraint with c = 4 was used for all the weights. A\nmomentum of 0.95 was used in all the layers. These hyperparameters were tuned using a\nvalidation set. Since the training set was quite large, we did not combine the validation\nset with the training set for ﬁnal training. We reported test error of the model that had\nsmallest validation error.\nB.3 CIFAR-10 and CIFAR-100\nThe CIFAR-10 and CIFAR-100 data sets consists of 50,000 training and 10,000 test images\neach. They have 10 and 100 image categories respectively. These are 32 × 32 color images.\nWe used 5,000 of the training images for validation. We followed the procedure similar\nto MNIST, where we found the best hyperparameters using the validation set and then\ncombined it with the training set. The images were preprocessed by doing global contrast\nnormalization in each color channel followed by ZCA whitening. Global contrast normal-\nization means that for image and each color channel in that image, we compute the mean\nof the pixel intensities and subtract it from the channel. ZCA whitening means that we\nmean center the data, rotate it onto its principle components, normalize each component\n1954\n",
    "Dropout\nand then rotate it back. The network architecture and dropout rates are same as that for\nSVHN, except the learning rates for the input layer which had to be set to smaller values.\nB.4 TIMIT\nThe open source Kaldi toolkit (Povey et al., 2011) was used to preprocess the data into log-\nﬁlter banks. A monophone system was trained to do a forced alignment and to get labels for\nspeech frames. Dropout neural networks were trained on windows of 21 consecutive frames\nto predict the label of the central frame. No speaker dependent operations were performed.\nThe inputs were mean centered and normalized to have unit variance.\nWe used probability of retention p = 0.8 in the input layers and 0.5 in the hidden layers.\nMax-norm constraint with c = 4 was used in all the layers. A momentum of 0.95 with a\nhigh learning rate of 0.1 was used. The learning rate was decayed as ϵ0(1 + t/T)−1. For\nDBN pretraining, we trained RBMs using CD-1. The variance of each input unit for the\nGaussian RBM was ﬁxed to 1. For ﬁnetuning the DBN with dropout, we found that in\norder to get the best results it was important to use a smaller learning rate (about 0.01).\nAdding max-norm constraints did not give any improvements.\nB.5 Reuters\nThe Reuters RCV1 corpus contains more than 800,000 documents categorized into 103\nclasses. These classes are arranged in a tree hierarchy. We created a subset of this data set\nconsisting of 402,738 articles and a vocabulary of 2000 words comprising of 50 categories\nin which each document belongs to exactly one class. The data was split into equal sized\ntraining and test sets. We tried many network architectures and found that dropout gave\nimprovements in classiﬁcation accuracy over all of them. However, the improvement was\nnot as signiﬁcant as that for the image and speech data sets. This might be explained by\nthe fact that this data set is quite big (more than 200,000 training examples) and overﬁtting\nis not a very serious problem.\nB.6 Alternative Splicing\nThe alternative splicing data set consists of data for 3665 cassette exons, 1014 RNA features\nand 4 tissue types derived from 27 mouse tissues. For each input, the target consists of 4\nsoftmax units (one for tissue type). Each softmax unit has 3 states (inc, exc, nc) which are\nof the biological importance. For each softmax unit, the aim is to predict a distribution over\nthese 3 states that matches the observed distribution from wet lab experiments as closely\nas possible. The evaluation metric is Code Quality which is deﬁned as\n|data points|\nX\ni=1\nX\nt∈tissue types\nX\ns∈{inc, exc, nc}\nps\ni,t log(qs\nt (ri)\n¯ps\n),\nwhere, ps\ni,t is the target probability for state s and tissue type t in input i; qs\nt (ri) is the\npredicted probability for state s in tissue type t for input ri and ¯ps is the average of ps\ni,t\nover i and t.\nA two layer dropout network with 1024 units in each layer was trained on this data set.\nA value of p = 0.5 was used for the hidden layer and p = 0.7 for the input layer. Max-norm\nregularization with high decaying learning rates was used. Results were averaged across the\nsame 5 folds used by Xiong et al. (2011).\n1955\n",
    "Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\nReferences\nM. Chen, Z. Xu, K. Weinberger, and F. Sha.\nMarginalized denoising autoencoders for\ndomain adaptation.\nIn Proceedings of the 29th International Conference on Machine\nLearning, pages 767–774. ACM, 2012.\nG. E. Dahl, M. Ranzato, A. Mohamed, and G. E. Hinton. Phone recognition with the mean-\ncovariance restricted Boltzmann machine. In Advances in Neural Information Processing\nSystems 23, pages 469–477, 2010.\nO. Dekel, O. Shamir, and L. Xiao. Learning to classify with missing and corrupted features.\nMachine Learning, 81(2):149–178, 2010.\nA. Globerson and S. Roweis. Nightmare at test time: robust learning by feature deletion. In\nProceedings of the 23rd International Conference on Machine Learning, pages 353–360.\nACM, 2006.\nI. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio. Maxout networks.\nIn Proceedings of the 30th International Conference on Machine Learning, pages 1319–\n1327. ACM, 2013.\nG. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks.\nScience, 313(5786):504 – 507, 2006.\nG. E. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep belief nets.\nNeural Computation, 18:1527–1554, 2006.\nK. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun. What is the best multi-stage\narchitecture for object recognition?\nIn Proceedings of the International Conference on\nComputer Vision (ICCV’09). IEEE, 2009.\nA. Krizhevsky. Learning multiple layers of features from tiny images. Technical report,\nUniversity of Toronto, 2009.\nA. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolu-\ntional neural networks. In Advances in Neural Information Processing Systems 25, pages\n1106–1114, 2012.\nY. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D.\nJackel. Backpropagation applied to handwritten zip code recognition. Neural Computa-\ntion, 1(4):541–551, 1989.\nY. Lin, F. Lv, S. Zhu, M. Yang, T. Cour, K. Yu, L. Cao, Z. Li, M.-H. Tsai, X. Zhou,\nT. Huang, and T. Zhang. Imagenet classiﬁcation: fast descriptor coding and large-scale\nsvm training. Large scale visual recognition challenge, 2010.\nA. Livnat, C. Papadimitriou, N. Pippenger, and M. W. Feldman.\nSex, mixability, and\nmodularity. Proceedings of the National Academy of Sciences, 107(4):1452–1457, 2010.\nV. Mnih. CUDAMat: a CUDA-based matrix class for Python. Technical Report UTML\nTR 2009-004, Department of Computer Science, University of Toronto, November 2009.\n1956\n",
    "Dropout\nA. Mohamed, G. E. Dahl, and G. E. Hinton. Acoustic modeling using deep belief networks.\nIEEE Transactions on Audio, Speech, and Language Processing, 2010.\nR. M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag New York, Inc., 1996.\nY. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in\nnatural images with unsupervised feature learning. In NIPS Workshop on Deep Learning\nand Unsupervised Feature Learning 2011, 2011.\nS. J. Nowlan and G. E. Hinton. Simplifying neural networks by soft weight-sharing. Neural\nComputation, 4(4), 1992.\nD. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann,\nP. Motlicek, Y. Qian, P. Schwarz, J. Silovsky, G. Stemmer, and K. Vesely. The Kaldi\nSpeech Recognition Toolkit. In IEEE 2011 Workshop on Automatic Speech Recognition\nand Understanding. IEEE Signal Processing Society, 2011.\nR. Salakhutdinov and G. Hinton. Deep Boltzmann machines. In Proceedings of the Inter-\nnational Conference on Artiﬁcial Intelligence and Statistics, volume 5, pages 448–455,\n2009.\nR. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using Markov\nchain Monte Carlo.\nIn Proceedings of the 25th International Conference on Machine\nLearning. ACM, 2008.\nJ. Sanchez and F. Perronnin. High-dimensional signature compression for large-scale image\nclassiﬁcation.\nIn Proceedings of the 2011 IEEE Conference on Computer Vision and\nPattern Recognition, pages 1665–1672, 2011.\nP. Sermanet, S. Chintala, and Y. LeCun. Convolutional neural networks applied to house\nnumbers digit classiﬁcation. In International Conference on Pattern Recognition (ICPR\n2012), 2012.\nP. Simard, D. Steinkraus, and J. Platt. Best practices for convolutional neural networks ap-\nplied to visual document analysis. In Proceedings of the Seventh International Conference\non Document Analysis and Recognition, volume 2, pages 958–962, 2003.\nJ. Snoek, H. Larochelle, and R. Adams. Practical Bayesian optimization of machine learning\nalgorithms. In Advances in Neural Information Processing Systems 25, pages 2960–2968,\n2012.\nN. Srebro and A. Shraibman. Rank, trace-norm and max-norm. In Proceedings of the 18th\nannual conference on Learning Theory, COLT’05, pages 545–560. Springer-Verlag, 2005.\nN. Srivastava. Improving Neural Networks with Dropout. Master’s thesis, University of\nToronto, January 2013.\nR. Tibshirani.\nRegression shrinkage and selection via the lasso.\nJournal of the Royal\nStatistical Society. Series B. Methodological, 58(1):267–288, 1996.\n1957\n",
    "Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\nA. N. Tikhonov. On the stability of inverse problems. Doklady Akademii Nauk SSSR, 39(5):\n195–198, 1943.\nL. van der Maaten, M. Chen, S. Tyree, and K. Q. Weinberger. Learning with marginalized\ncorrupted features.\nIn Proceedings of the 30th International Conference on Machine\nLearning, pages 410–418. ACM, 2013.\nP. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust\nfeatures with denoising autoencoders. In Proceedings of the 25th International Conference\non Machine Learning, pages 1096–1103. ACM, 2008.\nP. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol. Stacked denoising\nautoencoders: Learning useful representations in a deep network with a local denoising\ncriterion. In Proceedings of the 27th International Conference on Machine Learning, pages\n3371–3408. ACM, 2010.\nS. Wager, S. Wang, and P. Liang. Dropout training as adaptive regularization. In Advances\nin Neural Information Processing Systems 26, pages 351–359, 2013.\nS. Wang and C. D. Manning. Fast dropout training. In Proceedings of the 30th International\nConference on Machine Learning, pages 118–126. ACM, 2013.\nH. Y. Xiong, Y. Barash, and B. J. Frey. Bayesian prediction of tissue-regulated splicing\nusing RNA sequence and cellular context. Bioinformatics, 27(18):2554–2562, 2011.\nM. D. Zeiler and R. Fergus. Stochastic pooling for regularization of deep convolutional\nneural networks. CoRR, abs/1301.3557, 2013.\n1958\n"
  ],
  "full_text": "Journal of Machine Learning Research 15 (2014) 1929-1958\nSubmitted 11/13; Published 6/14\nDropout: A Simple Way to Prevent Neural Networks from\nOverﬁtting\nNitish Srivastava\nnitish@cs.toronto.edu\nGeoﬀrey Hinton\nhinton@cs.toronto.edu\nAlex Krizhevsky\nkriz@cs.toronto.edu\nIlya Sutskever\nilya@cs.toronto.edu\nRuslan Salakhutdinov\nrsalakhu@cs.toronto.edu\nDepartment of Computer Science\nUniversity of Toronto\n10 Kings College Road, Rm 3302\nToronto, Ontario, M5S 3G4, Canada.\nEditor: Yoshua Bengio\nAbstract\nDeep neural nets with a large number of parameters are very powerful machine learning\nsystems. However, overﬁtting is a serious problem in such networks. Large networks are also\nslow to use, making it diﬃcult to deal with overﬁtting by combining the predictions of many\ndiﬀerent large neural nets at test time. Dropout is a technique for addressing this problem.\nThe key idea is to randomly drop units (along with their connections) from the neural\nnetwork during training. This prevents units from co-adapting too much. During training,\ndropout samples from an exponential number of diﬀerent “thinned” networks. At test time,\nit is easy to approximate the eﬀect of averaging the predictions of all these thinned networks\nby simply using a single unthinned network that has smaller weights. This signiﬁcantly\nreduces overﬁtting and gives major improvements over other regularization methods. We\nshow that dropout improves the performance of neural networks on supervised learning\ntasks in vision, speech recognition, document classiﬁcation and computational biology,\nobtaining state-of-the-art results on many benchmark data sets.\nKeywords:\nneural networks, regularization, model combination, deep learning\n1. Introduction\nDeep neural networks contain multiple non-linear hidden layers and this makes them very\nexpressive models that can learn very complicated relationships between their inputs and\noutputs.\nWith limited training data, however, many of these complicated relationships\nwill be the result of sampling noise, so they will exist in the training set but not in real\ntest data even if it is drawn from the same distribution. This leads to overﬁtting and many\nmethods have been developed for reducing it. These include stopping the training as soon as\nperformance on a validation set starts to get worse, introducing weight penalties of various\nkinds such as L1 and L2 regularization and soft weight sharing (Nowlan and Hinton, 1992).\nWith unlimited computation, the best way to “regularize” a ﬁxed-sized model is to\naverage the predictions of all possible settings of the parameters, weighting each setting by\nc⃝2014 Nitish Srivastava, Geoﬀrey Hinton, Alex Krizhevsky, Ilya Sutskever and Ruslan Salakhutdinov.\n\n\nSrivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\n(a) Standard Neural Net\n(b) After applying dropout.\nFigure 1: Dropout Neural Net Model. Left: A standard neural net with 2 hidden layers. Right:\nAn example of a thinned net produced by applying dropout to the network on the left.\nCrossed units have been dropped.\nits posterior probability given the training data. This can sometimes be approximated quite\nwell for simple or small models (Xiong et al., 2011; Salakhutdinov and Mnih, 2008), but we\nwould like to approach the performance of the Bayesian gold standard using considerably\nless computation. We propose to do this by approximating an equally weighted geometric\nmean of the predictions of an exponential number of learned models that share parameters.\nModel combination nearly always improves the performance of machine learning meth-\nods. With large neural networks, however, the obvious idea of averaging the outputs of\nmany separately trained nets is prohibitively expensive. Combining several models is most\nhelpful when the individual models are diﬀerent from each other and in order to make\nneural net models diﬀerent, they should either have diﬀerent architectures or be trained\non diﬀerent data. Training many diﬀerent architectures is hard because ﬁnding optimal\nhyperparameters for each architecture is a daunting task and training each large network\nrequires a lot of computation. Moreover, large networks normally require large amounts of\ntraining data and there may not be enough data available to train diﬀerent networks on\ndiﬀerent subsets of the data. Even if one was able to train many diﬀerent large networks,\nusing them all at test time is infeasible in applications where it is important to respond\nquickly.\nDropout is a technique that addresses both these issues. It prevents overﬁtting and\nprovides a way of approximately combining exponentially many diﬀerent neural network\narchitectures eﬃciently.\nThe term “dropout” refers to dropping out units (hidden and\nvisible) in a neural network. By dropping a unit out, we mean temporarily removing it from\nthe network, along with all its incoming and outgoing connections, as shown in Figure 1.\nThe choice of which units to drop is random. In the simplest case, each unit is retained with\na ﬁxed probability p independent of other units, where p can be chosen using a validation\nset or can simply be set at 0.5, which seems to be close to optimal for a wide range of\nnetworks and tasks. For the input units, however, the optimal probability of retention is\nusually closer to 1 than to 0.5.\n1930\n\n\nDropout\nPresent with\nprobability p\nw\n-\n(a) At training time\nAlways\npresent\npw\n-\n(b) At test time\nFigure 2: Left: A unit at training time that is present with probability p and is connected to units\nin the next layer with weights w. Right: At test time, the unit is always present and\nthe weights are multiplied by p. The output at test time is same as the expected output\nat training time.\nApplying dropout to a neural network amounts to sampling a “thinned” network from\nit. The thinned network consists of all the units that survived dropout (Figure 1b). A\nneural net with n units, can be seen as a collection of 2n possible thinned neural networks.\nThese networks all share weights so that the total number of parameters is still O(n2), or\nless. For each presentation of each training case, a new thinned network is sampled and\ntrained. So training a neural network with dropout can be seen as training a collection of 2n\nthinned networks with extensive weight sharing, where each thinned network gets trained\nvery rarely, if at all.\nAt test time, it is not feasible to explicitly average the predictions from exponentially\nmany thinned models. However, a very simple approximate averaging method works well in\npractice. The idea is to use a single neural net at test time without dropout. The weights\nof this network are scaled-down versions of the trained weights. If a unit is retained with\nprobability p during training, the outgoing weights of that unit are multiplied by p at test\ntime as shown in Figure 2. This ensures that for any hidden unit the expected output (under\nthe distribution used to drop units at training time) is the same as the actual output at\ntest time. By doing this scaling, 2n networks with shared weights can be combined into\na single neural network to be used at test time. We found that training a network with\ndropout and using this approximate averaging method at test time leads to signiﬁcantly\nlower generalization error on a wide variety of classiﬁcation problems compared to training\nwith other regularization methods.\nThe idea of dropout is not limited to feed-forward neural nets. It can be more generally\napplied to graphical models such as Boltzmann Machines.\nIn this paper, we introduce\nthe dropout Restricted Boltzmann Machine model and compare it to standard Restricted\nBoltzmann Machines (RBM). Our experiments show that dropout RBMs are better than\nstandard RBMs in certain respects.\nThis paper is structured as follows. Section 2 describes the motivation for this idea.\nSection 3 describes relevant previous work. Section 4 formally describes the dropout model.\nSection 5 gives an algorithm for training dropout networks. In Section 6, we present our\nexperimental results where we apply dropout to problems in diﬀerent domains and compare\nit with other forms of regularization and model combination. Section 7 analyzes the eﬀect of\ndropout on diﬀerent properties of a neural network and describes how dropout interacts with\nthe network’s hyperparameters. Section 8 describes the Dropout RBM model. In Section 9\nwe explore the idea of marginalizing dropout. In Appendix A we present a practical guide\n1931\n\n\nSrivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\nfor training dropout nets. This includes a detailed analysis of the practical considerations\ninvolved in choosing hyperparameters when training dropout networks.\n2. Motivation\nA motivation for dropout comes from a theory of the role of sex in evolution (Livnat et al.,\n2010). Sexual reproduction involves taking half the genes of one parent and half of the\nother, adding a very small amount of random mutation, and combining them to produce an\noﬀspring. The asexual alternative is to create an oﬀspring with a slightly mutated copy of\nthe parent’s genes. It seems plausible that asexual reproduction should be a better way to\noptimize individual ﬁtness because a good set of genes that have come to work well together\ncan be passed on directly to the oﬀspring. On the other hand, sexual reproduction is likely\nto break up these co-adapted sets of genes, especially if these sets are large and, intuitively,\nthis should decrease the ﬁtness of organisms that have already evolved complicated co-\nadaptations. However, sexual reproduction is the way most advanced organisms evolved.\nOne possible explanation for the superiority of sexual reproduction is that, over the long\nterm, the criterion for natural selection may not be individual ﬁtness but rather mix-ability\nof genes. The ability of a set of genes to be able to work well with another random set of\ngenes makes them more robust. Since a gene cannot rely on a large set of partners to be\npresent at all times, it must learn to do something useful on its own or in collaboration with\na small number of other genes. According to this theory, the role of sexual reproduction\nis not just to allow useful new genes to spread throughout the population, but also to\nfacilitate this process by reducing complex co-adaptations that would reduce the chance of\na new gene improving the ﬁtness of an individual. Similarly, each hidden unit in a neural\nnetwork trained with dropout must learn to work with a randomly chosen sample of other\nunits. This should make each hidden unit more robust and drive it towards creating useful\nfeatures on its own without relying on other hidden units to correct its mistakes. However,\nthe hidden units within a layer will still learn to do diﬀerent things from each other. One\nmight imagine that the net would become robust against dropout by making many copies\nof each hidden unit, but this is a poor solution for exactly the same reason as replica codes\nare a poor way to deal with a noisy channel.\nA closely related, but slightly diﬀerent motivation for dropout comes from thinking\nabout successful conspiracies.\nTen conspiracies each involving ﬁve people is probably a\nbetter way to create havoc than one big conspiracy that requires ﬁfty people to all play\ntheir parts correctly. If conditions do not change and there is plenty of time for rehearsal, a\nbig conspiracy can work well, but with non-stationary conditions, the smaller the conspiracy\nthe greater its chance of still working. Complex co-adaptations can be trained to work well\non a training set, but on novel test data they are far more likely to fail than multiple simpler\nco-adaptations that achieve the same thing.\n3. Related Work\nDropout can be interpreted as a way of regularizing a neural network by adding noise to\nits hidden units. The idea of adding noise to the states of units has previously been used in\nthe context of Denoising Autoencoders (DAEs) by Vincent et al. (2008, 2010) where noise\n1932\n\n\nDropout\nis added to the input units of an autoencoder and the network is trained to reconstruct the\nnoise-free input. Our work extends this idea by showing that dropout can be eﬀectively\napplied in the hidden layers as well and that it can be interpreted as a form of model\naveraging.\nWe also show that adding noise is not only useful for unsupervised feature\nlearning but can also be extended to supervised learning problems. In fact, our method can\nbe applied to other neuron-based architectures, for example, Boltzmann Machines. While\n5% noise typically works best for DAEs, we found that our weight scaling procedure applied\nat test time enables us to use much higher noise levels. Dropping out 20% of the input units\nand 50% of the hidden units was often found to be optimal.\nSince dropout can be seen as a stochastic regularization technique, it is natural to\nconsider its deterministic counterpart which is obtained by marginalizing out the noise. In\nthis paper, we show that, in simple cases, dropout can be analytically marginalized out\nto obtain deterministic regularization methods.\nRecently, van der Maaten et al. (2013)\nalso explored deterministic regularizers corresponding to diﬀerent exponential-family noise\ndistributions, including dropout (which they refer to as “blankout noise”). However, they\napply noise to the inputs and only explore models with no hidden layers. Wang and Manning\n(2013) proposed a method for speeding up dropout by marginalizing dropout noise. Chen\net al. (2012) explored marginalization in the context of denoising autoencoders.\nIn dropout, we minimize the loss function stochastically under a noise distribution.\nThis can be seen as minimizing an expected loss function. Previous work of Globerson and\nRoweis (2006); Dekel et al. (2010) explored an alternate setting where the loss is minimized\nwhen an adversary gets to pick which units to drop. Here, instead of a noise distribution,\nthe maximum number of units that can be dropped is ﬁxed. However, this work also does\nnot explore models with hidden units.\n4. Model Description\nThis section describes the dropout neural network model. Consider a neural network with\nL hidden layers. Let l ∈{1, . . . , L} index the hidden layers of the network. Let z(l) denote\nthe vector of inputs into layer l, y(l) denote the vector of outputs from layer l (y(0) = x is\nthe input). W (l) and b(l) are the weights and biases at layer l. The feed-forward operation\nof a standard neural network (Figure 3a) can be described as (for l ∈{0, . . . , L −1} and\nany hidden unit i)\nz(l+1)\ni\n=\nw(l+1)\ni\nyl + b(l+1)\ni\n,\ny(l+1)\ni\n=\nf(z(l+1)\ni\n),\nwhere f is any activation function, for example, f(x) = 1/ (1 + exp(−x)).\nWith dropout, the feed-forward operation becomes (Figure 3b)\nr(l)\nj\n∼\nBernoulli(p),\ney(l)\n=\nr(l) ∗y(l),\nz(l+1)\ni\n=\nw(l+1)\ni\neyl + b(l+1)\ni\n,\ny(l+1)\ni\n=\nf(z(l+1)\ni\n).\n1933\n\n\nSrivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\n+1\ny(l)\n1\ny(l)\n2\ny(l)\n3\nz(l+1)\ni\ny(l+1)\ni\nw(l+1)\ni\nb(l+1)\ni\nf\n(a) Standard network\n+1\ney(l)\n1\ney(l)\n2\ney(l)\n3\nz(l+1)\ni\ny(l+1)\ni\ny(l)\n1\ny(l)\n2\ny(l)\n3\nr(l)\n1\nr(l)\n2\nr(l)\n3\nw(l+1)\ni\nb(l+1)\ni\nf\n(b) Dropout network\nFigure 3: Comparison of the basic operations of a standard and dropout network.\nHere ∗denotes an element-wise product. For any layer l, r(l) is a vector of independent\nBernoulli random variables each of which has probability p of being 1.\nThis vector is\nsampled and multiplied element-wise with the outputs of that layer, y(l), to create the\nthinned outputs ey(l). The thinned outputs are then used as input to the next layer. This\nprocess is applied at each layer. This amounts to sampling a sub-network from a larger\nnetwork. For learning, the derivatives of the loss function are backpropagated through the\nsub-network. At test time, the weights are scaled as W (l)\ntest = pW (l) as shown in Figure 2.\nThe resulting neural network is used without dropout.\n5. Learning Dropout Nets\nThis section describes a procedure for training dropout neural nets.\n5.1 Backpropagation\nDropout neural networks can be trained using stochastic gradient descent in a manner simi-\nlar to standard neural nets. The only diﬀerence is that for each training case in a mini-batch,\nwe sample a thinned network by dropping out units. Forward and backpropagation for that\ntraining case are done only on this thinned network. The gradients for each parameter are\naveraged over the training cases in each mini-batch. Any training case which does not use a\nparameter contributes a gradient of zero for that parameter. Many methods have been used\nto improve stochastic gradient descent such as momentum, annealed learning rates and L2\nweight decay. Those were found to be useful for dropout neural networks as well.\nOne particular form of regularization was found to be especially useful for dropout—\nconstraining the norm of the incoming weight vector at each hidden unit to be upper\nbounded by a ﬁxed constant c. In other words, if w represents the vector of weights incident\non any hidden unit, the neural network was optimized under the constraint ||w||2 ≤c. This\nconstraint was imposed during optimization by projecting w onto the surface of a ball of\nradius c, whenever w went out of it. This is also called max-norm regularization since it\nimplies that the maximum value that the norm of any weight can take is c. The constant\n1934\n\n\nDropout\nc is a tunable hyperparameter, which is determined using a validation set.\nMax-norm\nregularization has been previously used in the context of collaborative ﬁltering (Srebro and\nShraibman, 2005).\nIt typically improves the performance of stochastic gradient descent\ntraining of deep neural nets, even when no dropout is used.\nAlthough dropout alone gives signiﬁcant improvements, using dropout along with max-\nnorm regularization, large decaying learning rates and high momentum provides a signiﬁcant\nboost over just using dropout. A possible justiﬁcation is that constraining weight vectors\nto lie inside a ball of ﬁxed radius makes it possible to use a huge learning rate without the\npossibility of weights blowing up. The noise provided by dropout then allows the optimiza-\ntion process to explore diﬀerent regions of the weight space that would have otherwise been\ndiﬃcult to reach. As the learning rate decays, the optimization takes shorter steps, thereby\ndoing less exploration and eventually settles into a minimum.\n5.2 Unsupervised Pretraining\nNeural networks can be pretrained using stacks of RBMs (Hinton and Salakhutdinov, 2006),\nautoencoders (Vincent et al., 2010) or Deep Boltzmann Machines (Salakhutdinov and Hin-\nton, 2009). Pretraining is an eﬀective way of making use of unlabeled data. Pretraining\nfollowed by ﬁnetuning with backpropagation has been shown to give signiﬁcant performance\nboosts over ﬁnetuning from random initializations in certain cases.\nDropout can be applied to ﬁnetune nets that have been pretrained using these tech-\nniques. The pretraining procedure stays the same. The weights obtained from pretraining\nshould be scaled up by a factor of 1/p. This makes sure that for each unit, the expected\noutput from it under random dropout will be the same as the output during pretraining.\nWe were initially concerned that the stochastic nature of dropout might wipe out the in-\nformation in the pretrained weights. This did happen when the learning rates used during\nﬁnetuning were comparable to the best learning rates for randomly initialized nets. How-\never, when the learning rates were chosen to be smaller, the information in the pretrained\nweights seemed to be retained and we were able to get improvements in terms of the ﬁnal\ngeneralization error compared to not using dropout when ﬁnetuning.\n6. Experimental Results\nWe trained dropout neural networks for classiﬁcation problems on data sets in diﬀerent\ndomains. We found that dropout improved generalization performance on all data sets\ncompared to neural networks that did not use dropout. Table 1 gives a brief description of\nthe data sets. The data sets are\n• MNIST : A standard toy data set of handwritten digits.\n• TIMIT : A standard speech benchmark for clean speech recognition.\n• CIFAR-10 and CIFAR-100 : Tiny natural images (Krizhevsky, 2009).\n• Street View House Numbers data set (SVHN) : Images of house numbers collected by\nGoogle Street View (Netzer et al., 2011).\n• ImageNet : A large collection of natural images.\n• Reuters-RCV1 : A collection of Reuters newswire articles.\n1935\n\n\nSrivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\n• Alternative Splicing data set: RNA features for predicting alternative gene splicing\n(Xiong et al., 2011).\nWe chose a diverse set of data sets to demonstrate that dropout is a general technique\nfor improving neural nets and is not speciﬁc to any particular application domain. In this\nsection, we present some key results that show the eﬀectiveness of dropout. A more detailed\ndescription of all the experiments and data sets is provided in Appendix B.\nData Set\nDomain\nDimensionality\nTraining Set\nTest Set\nMNIST\nVision\n784 (28 × 28 grayscale)\n60K\n10K\nSVHN\nVision\n3072 (32 × 32 color)\n600K\n26K\nCIFAR-10/100\nVision\n3072 (32 × 32 color)\n60K\n10K\nImageNet (ILSVRC-2012)\nVision\n65536 (256 × 256 color)\n1.2M\n150K\nTIMIT\nSpeech\n2520 (120-dim, 21 frames)\n1.1M frames\n58K frames\nReuters-RCV1\nText\n2000\n200K\n200K\nAlternative Splicing\nGenetics\n1014\n2932\n733\nTable 1: Overview of the data sets used in this paper.\n6.1 Results on Image Data Sets\nWe used ﬁve image data sets to evaluate dropout—MNIST, SVHN, CIFAR-10, CIFAR-100\nand ImageNet. These data sets include diﬀerent image types and training set sizes. Models\nwhich achieve state-of-the-art results on all of these data sets use dropout.\n6.1.1 MNIST\nMethod\nUnit\nType\nArchitecture\nError\n%\nStandard Neural Net (Simard et al., 2003)\nLogistic\n2 layers, 800 units\n1.60\nSVM Gaussian kernel\nNA\nNA\n1.40\nDropout NN\nLogistic\n3 layers, 1024 units\n1.35\nDropout NN\nReLU\n3 layers, 1024 units\n1.25\nDropout NN + max-norm constraint\nReLU\n3 layers, 1024 units\n1.06\nDropout NN + max-norm constraint\nReLU\n3 layers, 2048 units\n1.04\nDropout NN + max-norm constraint\nReLU\n2 layers, 4096 units\n1.01\nDropout NN + max-norm constraint\nReLU\n2 layers, 8192 units\n0.95\nDropout NN + max-norm constraint (Goodfellow\net al., 2013)\nMaxout\n2 layers, (5 × 240)\nunits\n0.94\nDBN + ﬁnetuning (Hinton and Salakhutdinov, 2006)\nLogistic\n500-500-2000\n1.18\nDBM + ﬁnetuning (Salakhutdinov and Hinton, 2009)\nLogistic\n500-500-2000\n0.96\nDBN + dropout ﬁnetuning\nLogistic\n500-500-2000\n0.92\nDBM + dropout ﬁnetuning\nLogistic\n500-500-2000\n0.79\nTable 2: Comparison of diﬀerent models on MNIST.\nThe MNIST data set consists of 28 × 28 pixel handwritten digit images. The task is\nto classify the images into 10 digit classes. Table 2 compares the performance of dropout\nwith other techniques. The best performing neural networks for the permutation invariant\n1936\n\n\nDropout\nsetting that do not use dropout or unsupervised pretraining achieve an error of about\n1.60% (Simard et al., 2003). With dropout the error reduces to 1.35%. Replacing logistic\nunits with rectiﬁed linear units (ReLUs) (Jarrett et al., 2009) further reduces the error to\n1.25%. Adding max-norm regularization again reduces it to 1.06%. Increasing the size of\nthe network leads to better results. A neural net with 2 layers and 8192 units per layer\ngets down to 0.95% error. Note that this network has more than 65 million parameters and\nis being trained on a data set of size 60,000. Training a network of this size to give good\ngeneralization error is very hard with standard regularization methods and early stopping.\nDropout, on the other hand, prevents overﬁtting, even in this case. It does not even need\nearly stopping. Goodfellow et al. (2013) showed that results can be further improved to\n0.94% by replacing ReLU units with maxout units. All dropout nets use p = 0.5 for hidden\nunits and p = 0.8 for input units. More experimental details can be found in Appendix B.1.\nDropout nets pretrained with stacks of RBMs and Deep Boltzmann Machines also give\nimprovements as shown in Table 2. DBM—pretrained dropout nets achieve a test error of\n0.79% which is the best performance ever reported for the permutation invariant setting.\nWe note that it possible to obtain better results by using 2-D spatial information and\naugmenting the training set with distorted versions of images from the standard training\nset. We demonstrate the eﬀectiveness of dropout in that setting on more interesting data\nsets.\n0\n200000\n400000\n600000\n800000\n1000000\nNumber of weight updates\n1.0\n1.5\n2.0\n2.5\nClassification Error %\nWith dropout\nWithout dropout\n@\nR\n@\n@\nR\nFigure 4: Test error for diﬀerent architectures\nwith and without dropout.\nThe net-\nworks have 2 to 4 hidden layers each\nwith 1024 to 2048 units.\nIn\norder\nto\ntest\nthe\nrobustness\nof\ndropout,\nclassiﬁcation experiments were\ndone with networks of many diﬀerent ar-\nchitectures keeping all hyperparameters, in-\ncluding p, ﬁxed.\nFigure 4 shows the test\nerror rates obtained for these diﬀerent ar-\nchitectures as training progresses.\nThe\nsame architectures trained with and with-\nout dropout have drastically diﬀerent test\nerrors as seen as by the two separate clus-\nters of trajectories. Dropout gives a huge\nimprovement across all architectures, with-\nout using hyperparameters that were tuned\nspeciﬁcally for each architecture.\n6.1.2 Street View House Numbers\nThe Street View House Numbers (SVHN)\nData Set (Netzer et al., 2011) consists of\ncolor images of house numbers collected by\nGoogle Street View. Figure 5a shows some examples of images from this data set. The\npart of the data set that we use in our experiments consists of 32 × 32 color images roughly\ncentered on a digit in a house number. The task is to identify that digit.\nFor this data set, we applied dropout to convolutional neural networks (LeCun et al.,\n1989). The best architecture that we found has three convolutional layers followed by 2\nfully connected hidden layers. All hidden units were ReLUs. Each convolutional layer was\n1937\n\n\nSrivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\nMethod\nError %\nBinary Features (WDCH) (Netzer et al., 2011)\n36.7\nHOG (Netzer et al., 2011)\n15.0\nStacked Sparse Autoencoders (Netzer et al., 2011)\n10.3\nKMeans (Netzer et al., 2011)\n9.4\nMulti-stage Conv Net with average pooling (Sermanet et al., 2012)\n9.06\nMulti-stage Conv Net + L2 pooling (Sermanet et al., 2012)\n5.36\nMulti-stage Conv Net + L4 pooling + padding (Sermanet et al., 2012)\n4.90\nConv Net + max-pooling\n3.95\nConv Net + max pooling + dropout in fully connected layers\n3.02\nConv Net + stochastic pooling (Zeiler and Fergus, 2013)\n2.80\nConv Net + max pooling + dropout in all layers\n2.55\nConv Net + maxout (Goodfellow et al., 2013)\n2.47\nHuman Performance\n2.0\nTable 3: Results on the Street View House Numbers data set.\nfollowed by a max-pooling layer. Appendix B.2 describes the architecture in more detail.\nDropout was applied to all the layers of the network with the probability of retaining a hid-\nden unit being p = (0.9, 0.75, 0.75, 0.5, 0.5, 0.5) for the diﬀerent layers of the network (going\nfrom input to convolutional layers to fully connected layers). Max-norm regularization was\nused for weights in both convolutional and fully connected layers. Table 3 compares the\nresults obtained by diﬀerent methods. We ﬁnd that convolutional nets outperform other\nmethods. The best performing convolutional nets that do not use dropout achieve an error\nrate of 3.95%. Adding dropout only to the fully connected layers reduces the error to 3.02%.\nAdding dropout to the convolutional layers as well further reduces the error to 2.55%. Even\nmore gains can be obtained by using maxout units.\nThe additional gain in performance obtained by adding dropout in the convolutional\nlayers (3.02% to 2.55%) is worth noting. One may have presumed that since the convo-\nlutional layers don’t have a lot of parameters, overﬁtting is not a problem and therefore\ndropout would not have much eﬀect. However, dropout in the lower layers still helps be-\ncause it provides noisy inputs for the higher fully connected layers which prevents them\nfrom overﬁtting.\n6.1.3 CIFAR-10 and CIFAR-100\nThe CIFAR-10 and CIFAR-100 data sets consist of 32 × 32 color images drawn from 10\nand 100 categories respectively. Figure 5b shows some examples of images from this data\nset. A detailed description of the data sets, input preprocessing, network architectures and\nother experimental details is given in Appendix B.3. Table 4 shows the error rate obtained\nby diﬀerent methods on these data sets. Without any data augmentation, Snoek et al.\n(2012) used Bayesian hyperparameter optimization to obtained an error rate of 14.98% on\nCIFAR-10. Using dropout in the fully connected layers reduces that to 14.32% and adding\ndropout in every layer further reduces the error to 12.61%. Goodfellow et al. (2013) showed\nthat the error is further reduced to 11.68% by replacing ReLU units with maxout units. On\nCIFAR-100, dropout reduces the error from 43.48% to 37.20% which is a huge improvement.\nNo data augmentation was used for either data set (apart from the input dropout).\n1938\n\n\nDropout\n(a) Street View House Numbers (SVHN)\n(b) CIFAR-10\nFigure 5: Samples from image data sets. Each row corresponds to a diﬀerent category.\nMethod\nCIFAR-10\nCIFAR-100\nConv Net + max pooling (hand tuned)\n15.60\n43.48\nConv Net + stochastic pooling (Zeiler and Fergus, 2013)\n15.13\n42.51\nConv Net + max pooling (Snoek et al., 2012)\n14.98\n-\nConv Net + max pooling + dropout fully connected layers\n14.32\n41.26\nConv Net + max pooling + dropout in all layers\n12.61\n37.20\nConv Net + maxout (Goodfellow et al., 2013)\n11.68\n38.57\nTable 4: Error rates on CIFAR-10 and CIFAR-100.\n6.1.4 ImageNet\nImageNet is a data set of over 15 million labeled high-resolution images belonging to roughly\n22,000 categories. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual\ncompetition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has\nbeen held. A subset of ImageNet with roughly 1000 images in each of 1000 categories is\nused in this challenge. Since the number of categories is rather large, it is conventional to\nreport two error rates: top-1 and top-5, where the top-5 error rate is the fraction of test\nimages for which the correct label is not among the ﬁve labels considered most probable by\nthe model. Figure 6 shows some predictions made by our model on a few test images.\nILSVRC-2010 is the only version of ILSVRC for which the test set labels are available, so\nmost of our experiments were performed on this data set. Table 5 compares the performance\nof diﬀerent methods. Convolutional nets with dropout outperform other methods by a large\nmargin. The architecture and implementation details are described in detail in Krizhevsky\net al. (2012).\n1939\n\n\nSrivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\nFigure 6: Some ImageNet test cases with the 4 most probable labels as predicted by our model.\nThe length of the horizontal bars is proportional to the probability assigned to the labels\nby the model. Pink indicates ground truth.\nModel\nTop-1\nTop-5\nSparse Coding (Lin et al., 2010)\n47.1\n28.2\nSIFT + Fisher Vectors (Sanchez and Perronnin, 2011)\n45.7\n25.7\nConv Net + dropout (Krizhevsky et al., 2012)\n37.5\n17.0\nTable 5: Results on the ILSVRC-2010 test set.\nModel\nTop-1\n(val)\nTop-5\n(val)\nTop-5\n(test)\nSVM on Fisher Vectors of Dense SIFT and Color Statistics\n-\n-\n27.3\nAvg of classiﬁers over FVs of SIFT, LBP, GIST and CSIFT\n-\n-\n26.2\nConv Net + dropout (Krizhevsky et al., 2012)\n40.7\n18.2\n-\nAvg of 5 Conv Nets + dropout (Krizhevsky et al., 2012)\n38.1\n16.4\n16.4\nTable 6: Results on the ILSVRC-2012 validation/test set.\nOur model based on convolutional nets and dropout won the ILSVRC-2012 competition.\nSince the labels for the test set are not available, we report our results on the test set for\nthe ﬁnal submission and include the validation set results for diﬀerent variations of our\nmodel. Table 6 shows the results from the competition. While the best methods based on\nstandard vision features achieve a top-5 error rate of about 26%, convolutional nets with\ndropout achieve a test error of about 16% which is a staggering diﬀerence. Figure 6 shows\nsome examples of predictions made by our model. We can see that the model makes very\nreasonable predictions, even when its best guess is not correct.\n6.2 Results on TIMIT\nNext, we applied dropout to a speech recognition task. We use the TIMIT data set which\nconsists of recordings from 680 speakers covering 8 major dialects of American English\nreading ten phonetically-rich sentences in a controlled noise-free environment.\nDropout\nneural networks were trained on windows of 21 log-ﬁlter bank frames to predict the label\nof the central frame.\nNo speaker dependent operations were performed.\nAppendix B.4\ndescribes the data preprocessing and training details. Table 7 compares dropout neural\n1940\n\n\nDropout\nnets with other models. A 6-layer net gives a phone error rate of 23.4%. Dropout further\nimproves it to 21.8%. We also trained dropout nets starting from pretrained weights. A\n4-layer net pretrained with a stack of RBMs get a phone error rate of 22.7%. With dropout,\nthis reduces to 19.7%. Similarly, for an 8-layer net the error reduces from 20.5% to 19.7%.\nMethod\nPhone Error Rate%\nNN (6 layers) (Mohamed et al., 2010)\n23.4\nDropout NN (6 layers)\n21.8\nDBN-pretrained NN (4 layers)\n22.7\nDBN-pretrained NN (6 layers) (Mohamed et al., 2010)\n22.4\nDBN-pretrained NN (8 layers) (Mohamed et al., 2010)\n20.7\nmcRBM-DBN-pretrained NN (5 layers) (Dahl et al., 2010)\n20.5\nDBN-pretrained NN (4 layers) + dropout\n19.7\nDBN-pretrained NN (8 layers) + dropout\n19.7\nTable 7: Phone error rate on the TIMIT core test set.\n6.3 Results on a Text Data Set\nTo test the usefulness of dropout in the text domain, we used dropout networks to train a\ndocument classiﬁer. We used a subset of the Reuters-RCV1 data set which is a collection of\nover 800,000 newswire articles from Reuters. These articles cover a variety of topics. The\ntask is to take a bag of words representation of a document and classify it into 50 disjoint\ntopics. Appendix B.5 describes the setup in more detail. Our best neural net which did\nnot use dropout obtained an error rate of 31.05%. Adding dropout reduced the error to\n29.62%. We found that the improvement was much smaller compared to that for the vision\nand speech data sets.\n6.4 Comparison with Bayesian Neural Networks\nDropout can be seen as a way of doing an equally-weighted averaging of exponentially many\nmodels with shared weights. On the other hand, Bayesian neural networks (Neal, 1996) are\nthe proper way of doing model averaging over the space of neural network structures and\nparameters.\nIn dropout, each model is weighted equally, whereas in a Bayesian neural\nnetwork each model is weighted taking into account the prior and how well the model ﬁts\nthe data, which is the more correct approach. Bayesian neural nets are extremely useful for\nsolving problems in domains where data is scarce such as medical diagnosis, genetics, drug\ndiscovery and other computational biology applications. However, Bayesian neural nets are\nslow to train and diﬃcult to scale to very large network sizes. Besides, it is expensive to\nget predictions from many large nets at test time. On the other hand, dropout neural nets\nare much faster to train and use at test time. In this section, we report experiments that\ncompare Bayesian neural nets with dropout neural nets on a small data set where Bayesian\nneural networks are known to perform well and obtain state-of-the-art results. The aim is\nto analyze how much does dropout lose compared to Bayesian neural nets.\nThe data set that we use (Xiong et al., 2011) comes from the domain of genetics. The\ntask is to predict the occurrence of alternative splicing based on RNA features. Alternative\nsplicing is a signiﬁcant cause of cellular diversity in mammalian tissues. Predicting the\n1941\n\n\nSrivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\nMethod\nCode Quality (bits)\nNeural Network (early stopping) (Xiong et al., 2011)\n440\nRegression, PCA (Xiong et al., 2011)\n463\nSVM, PCA (Xiong et al., 2011)\n487\nNeural Network with dropout\n567\nBayesian Neural Network (Xiong et al., 2011)\n623\nTable 8: Results on the Alternative Splicing Data Set.\noccurrence of alternate splicing in certain tissues under diﬀerent conditions is important for\nunderstanding many human diseases. Given the RNA features, the task is to predict the\nprobability of three splicing related events that biologists care about. The evaluation metric\nis Code Quality which is a measure of the negative KL divergence between the target and\nthe predicted probability distributions (higher is better). Appendix B.6 includes a detailed\ndescription of the data set and this performance metric.\nTable 8 summarizes the performance of diﬀerent models on this data set. Xiong et al.\n(2011) used Bayesian neural nets for this task. As expected, we found that Bayesian neural\nnets perform better than dropout. However, we see that dropout improves signiﬁcantly\nupon the performance of standard neural nets and outperforms all other methods. The\nchallenge in this data set is to prevent overﬁtting since the size of the training set is small.\nOne way to prevent overﬁtting is to reduce the input dimensionality using PCA. Thereafter,\nstandard techniques such as SVMs or logistic regression can be used. However, with dropout\nwe were able to prevent overﬁtting without the need to do dimensionality reduction. The\ndropout nets are very large (1000s of hidden units) compared to a few tens of units in the\nBayesian network. This shows that dropout has a strong regularizing eﬀect.\n6.5 Comparison with Standard Regularizers\nSeveral regularization methods have been proposed for preventing overﬁtting in neural net-\nworks. These include L2 weight decay (more generally Tikhonov regularization (Tikhonov,\n1943)), lasso (Tibshirani, 1996), KL-sparsity and max-norm regularization. Dropout can\nbe seen as another way of regularizing neural networks. In this section we compare dropout\nwith some of these regularization methods using the MNIST data set.\nThe same network architecture (784-1024-1024-2048-10) with ReLUs was trained us-\ning stochastic gradient descent with diﬀerent regularizations. Table 9 shows the results.\nThe values of diﬀerent hyperparameters associated with each kind of regularization (decay\nconstants, target sparsity, dropout rate, max-norm upper bound) were obtained using a\nvalidation set. We found that dropout combined with max-norm regularization gives the\nlowest generalization error.\n7. Salient Features\nThe experiments described in the previous section provide strong evidence that dropout\nis a useful technique for improving neural networks. In this section, we closely examine\nhow dropout aﬀects a neural network. We analyze the eﬀect of dropout on the quality of\nfeatures produced. We see how dropout aﬀects the sparsity of hidden unit activations. We\n1942\n\n\nDropout\nMethod\nTest Classiﬁcation error %\nL2\n1.62\nL2 + L1 applied towards the end of training\n1.60\nL2 + KL-sparsity\n1.55\nMax-norm\n1.35\nDropout + L2\n1.25\nDropout + Max-norm\n1.05\nTable 9: Comparison of diﬀerent regularization methods on MNIST.\nalso see how the advantages obtained from dropout vary with the probability of retaining\nunits, size of the network and the size of the training set. These observations give some\ninsight into why dropout works so well.\n7.1 Eﬀect on Features\n(a) Without dropout\n(b) Dropout with p = 0.5.\nFigure 7: Features learned on MNIST with one hidden layer autoencoders having 256 rectiﬁed\nlinear units.\nIn a standard neural network, the derivative received by each parameter tells it how it\nshould change so the ﬁnal loss function is reduced, given what all other units are doing.\nTherefore, units may change in a way that they ﬁx up the mistakes of the other units.\nThis may lead to complex co-adaptations. This in turn leads to overﬁtting because these\nco-adaptations do not generalize to unseen data. We hypothesize that for each hidden unit,\ndropout prevents co-adaptation by making the presence of other hidden units unreliable.\nTherefore, a hidden unit cannot rely on other speciﬁc units to correct its mistakes. It must\nperform well in a wide variety of diﬀerent contexts provided by the other hidden units. To\nobserve this eﬀect directly, we look at the ﬁrst level features learned by neural networks\ntrained on visual tasks with and without dropout.\n1943\n\n\nSrivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\nFigure 7a shows features learned by an autoencoder on MNIST with a single hidden\nlayer of 256 rectiﬁed linear units without dropout. Figure 7b shows the features learned by\nan identical autoencoder which used dropout in the hidden layer with p = 0.5. Both au-\ntoencoders had similar test reconstruction errors. However, it is apparent that the features\nshown in Figure 7a have co-adapted in order to produce good reconstructions. Each hidden\nunit on its own does not seem to be detecting a meaningful feature. On the other hand, in\nFigure 7b, the hidden units seem to detect edges, strokes and spots in diﬀerent parts of the\nimage. This shows that dropout does break up co-adaptations, which is probably the main\nreason why it leads to lower generalization errors.\n7.2 Eﬀect on Sparsity\n(a) Without dropout\n(b) Dropout with p = 0.5.\nFigure 8: Eﬀect of dropout on sparsity. ReLUs were used for both models. Left: The histogram\nof mean activations shows that most units have a mean activation of about 2.0. The\nhistogram of activations shows a huge mode away from zero. Clearly, a large fraction of\nunits have high activation. Right: The histogram of mean activations shows that most\nunits have a smaller mean mean activation of about 0.7. The histogram of activations\nshows a sharp peak at zero. Very few units have high activation.\nWe found that as a side-eﬀect of doing dropout, the activations of the hidden units\nbecome sparse, even when no sparsity inducing regularizers are present. Thus, dropout au-\ntomatically leads to sparse representations. To observe this eﬀect, we take the autoencoders\ntrained in the previous section and look at the sparsity of hidden unit activations on a ran-\ndom mini-batch taken from the test set. Figure 8a and Figure 8b compare the sparsity for\nthe two models. In a good sparse model, there should only be a few highly activated units\nfor any data case. Moreover, the average activation of any unit across data cases should\nbe low. To assess both of these qualities, we plot two histograms for each model. For each\nmodel, the histogram on the left shows the distribution of mean activations of hidden units\nacross the minibatch. The histogram on the right shows the distribution of activations of\nthe hidden units.\nComparing the histograms of activations we can see that fewer hidden units have high\nactivations in Figure 8b compared to Figure 8a, as seen by the signiﬁcant mass away from\n1944\n\n\nDropout\nzero for the net that does not use dropout. The mean activations are also smaller for the\ndropout net. The overall mean activation of hidden units is close to 2.0 for the autoencoder\nwithout dropout but drops to around 0.7 when dropout is used.\n7.3 Eﬀect of Dropout Rate\nDropout has a tunable hyperparameter p (the probability of retaining a unit in the network).\nIn this section, we explore the eﬀect of varying this hyperparameter. The comparison is\ndone in two situations.\n1. The number of hidden units is held constant.\n2. The number of hidden units is changed so that the expected number of hidden units\nthat will be retained after dropout is held constant.\nIn the ﬁrst case, we train the same network architecture with diﬀerent amounts of\ndropout. We use a 784-2048-2048-2048-10 architecture. No input dropout was used. Fig-\nure 9a shows the test error obtained as a function of p. If the architecture is held constant,\nhaving a small p means very few units will turn on during training. It can be seen that this\nhas led to underﬁtting since the training error is also high. We see that as p increases, the\nerror goes down. It becomes ﬂat when 0.4 ≤p ≤0.8 and then increases as p becomes close\nto 1.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProbability of retaining a unit (p)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nClassification Error %\nTest Error\nTraining Error\n(a) Keeping n ﬁxed.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProbability of retaining a unit (p)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nClassification Error %\nTest Error\nTraining Error\n(b) Keeping pn ﬁxed.\nFigure 9: Eﬀect of changing dropout rates on MNIST.\nAnother interesting setting is the second case in which the quantity pn is held constant\nwhere n is the number of hidden units in any particular layer. This means that networks\nthat have small p will have a large number of hidden units.\nTherefore, after applying\ndropout, the expected number of units that are present will be the same across diﬀerent\narchitectures. However, the test networks will be of diﬀerent sizes. In our experiments,\nwe set pn = 256 for the ﬁrst two hidden layers and pn = 512 for the last hidden layer.\nFigure 9b shows the test error obtained as a function of p. We notice that the magnitude\nof errors for small values of p has reduced by a lot compared to Figure 9a (for p = 0.1 it fell\nfrom 2.7% to 1.7%). Values of p that are close to 0.6 seem to perform best for this choice\nof pn but our usual default value of 0.5 is close to optimal.\n1945\n\n\nSrivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\n7.4 Eﬀect of Data Set Size\nOne test of a good regularizer is that it should make it possible to get good generalization\nerror from models with a large number of parameters trained on small data sets. This\nsection explores the eﬀect of changing the data set size when dropout is used with feed-\nforward networks. Huge neural networks trained in the standard way overﬁt massively on\nsmall data sets. To see if dropout can help, we run classiﬁcation experiments on MNIST\nand vary the amount of data given to the network.\n10\n2\n10\n3\n10\n4\n10\n5\nDataset size\n0\n5\n10\n15\n20\n25\n30\nClassification Error %\nWith dropout\nWithout dropout\nFigure 10: Eﬀect of varying data set size.\nThe results of these experiments are\nshown in Figure 10. The network was given\ndata sets of size 100, 500, 1K, 5K, 10K\nand 50K chosen randomly from the MNIST\ntraining set.\nThe same network architec-\nture (784-1024-1024-2048-10) was used for\nall data sets. Dropout with p = 0.5 was per-\nformed at all the hidden layers and p = 0.8\nat the input layer. It can be observed that\nfor extremely small data sets (100, 500)\ndropout does not give any improvements.\nThe model has enough parameters that it\ncan overﬁt on the training data, even with\nall the noise coming from dropout. As the\nsize of the data set is increased, the gain\nfrom doing dropout increases up to a point and then declines. This suggests that for any\ngiven architecture and dropout rate, there is a “sweet spot” corresponding to some amount\nof data that is large enough to not be memorized in spite of the noise but not so large that\noverﬁtting is not a problem anyways.\n7.5 Monte-Carlo Model Averaging vs. Weight Scaling\n0\n20\n40\n60\n80\n100\n120\nNumber of samples used for Monte-Carlo averaging (k)\n1.00\n1.05\n1.10\n1.15\n1.20\n1.25\n1.30\n1.35\nTest Classification error %\nMonte-Carlo Model Averaging\nApproximate averaging by weight scaling\nFigure 11: Monte-Carlo\nmodel\naveraging\nvs.\nweight scaling.\nThe eﬃcient test time procedure that we\npropose is to do an approximate model com-\nbination by scaling down the weights of the\ntrained neural network. An expensive but\nmore correct way of averaging the models\nis to sample k neural nets using dropout for\neach test case and average their predictions.\nAs k →∞, this Monte-Carlo model average\ngets close to the true model average. It is in-\nteresting to see empirically how many sam-\nples k are needed to match the performance\nof the approximate averaging method. By\ncomputing the error for diﬀerent values of k\nwe can see how quickly the error rate of the\nﬁnite-sample average approaches the error\nrate of the true model average.\n1946\n\n\nDropout\nWe again use the MNIST data set and do classiﬁcation by averaging the predictions\nof k randomly sampled neural networks. Figure 11 shows the test error rate obtained for\ndiﬀerent values of k. This is compared with the error obtained using the weight scaling\nmethod (shown as a horizontal line). It can be seen that around k = 50, the Monte-Carlo\nmethod becomes as good as the approximate method. Thereafter, the Monte-Carlo method\nis slightly better than the approximate method but well within one standard deviation of\nit. This suggests that the weight scaling method is a fairly good approximation of the true\nmodel average.\n8. Dropout Restricted Boltzmann Machines\nBesides feed-forward neural networks, dropout can also be applied to Restricted Boltzmann\nMachines (RBM). In this section, we formally describe this model and show some results\nto illustrate its key properties.\n8.1 Model Description\nConsider an RBM with visible units v ∈{0, 1}D and hidden units h ∈{0, 1}F . It deﬁnes\nthe following probability distribution\nP(h, v; θ) =\n1\nZ(θ) exp(v⊤Wh + a⊤h + b⊤v).\nWhere θ = {W, a, b} represents the model parameters and Z is the partition function.\nDropout RBMs are RBMs augmented with a vector of binary random variables r ∈\n{0, 1}F .\nEach random variable rj takes the value 1 with probability p, independent of\nothers. If rj takes the value 1, the hidden unit hj is retained, otherwise it is dropped from\nthe model. The joint distribution deﬁned by a Dropout RBM can be expressed as\nP(r, h, v; p, θ)\n=\nP(r; p)P(h, v|r; θ),\nP(r; p)\n=\nF\nY\nj=1\nprj(1 −p)1−rj,\nP(h, v|r; θ)\n=\n1\nZ′(θ, r) exp(v⊤Wh + a⊤h + b⊤v)\nF\nY\nj=1\ng(hj, rj),\ng(hj, rj)\n=\n1(rj = 1) + 1(rj = 0)1(hj = 0).\nZ′(θ, r) is the normalization constant. g(hj, rj) imposes the constraint that if rj = 0,\nhj must be 0. The distribution over h, conditioned on v and r is factorial\nP(h|r, v)\n=\nF\nY\nj=1\nP(hj|rj, v),\nP(hj = 1|rj, v)\n=\n1(rj = 1)σ\n \nbj +\nX\ni\nWijvi\n!\n.\n1947\n\n\nSrivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\n(a) Without dropout\n(b) Dropout with p = 0.5.\nFigure 12: Features learned on MNIST by 256 hidden unit RBMs. The features are ordered by L2\nnorm.\nThe distribution over v conditioned on h is same as that of an RBM\nP(v|h)\n=\nD\nY\ni=1\nP(vi|h),\nP(vi = 1|h)\n=\nσ\n\nai +\nX\nj\nWijhj\n\n.\nConditioned on r, the distribution over {v, h} is same as the distribution that an RBM\nwould impose, except that the units for which rj = 0 are dropped from h. Therefore, the\nDropout RBM model can be seen as a mixture of exponentially many RBMs with shared\nweights each using a diﬀerent subset of h.\n8.2 Learning Dropout RBMs\nLearning algorithms developed for RBMs such as Contrastive Divergence (Hinton et al.,\n2006) can be directly applied for learning Dropout RBMs. The only diﬀerence is that r is\nﬁrst sampled and only the hidden units that are retained are used for training. Similar to\ndropout neural networks, a diﬀerent r is sampled for each training case in every minibatch.\nIn our experiments, we use CD-1 for training dropout RBMs.\n8.3 Eﬀect on Features\nDropout in feed-forward networks improved the quality of features by reducing co-adaptations.\nThis section explores whether this eﬀect transfers to Dropout RBMs as well.\nFigure 12a shows features learned by a binary RBM with 256 hidden units. Figure 12b\nshows features learned by a dropout RBM with the same number of hidden units. Features\n1948\n\n\nDropout\n(a) Without dropout\n(b) Dropout with p = 0.5.\nFigure 13: Eﬀect of dropout on sparsity. Left: The activation histogram shows that a large num-\nber of units have activations away from zero. Right: A large number of units have\nactivations close to zero and very few units have high activation.\nlearned by the dropout RBM appear qualitatively diﬀerent in the sense that they seem to\ncapture features that are coarser compared to the sharply deﬁned stroke-like features in the\nstandard RBM. There seem to be very few dead units in the dropout RBM relative to the\nstandard RBM.\n8.4 Eﬀect on Sparsity\nNext, we investigate the eﬀect of dropout RBM training on sparsity of the hidden unit\nactivations. Figure 13a shows the histograms of hidden unit activations and their means on\na test mini-batch after training an RBM. Figure 13b shows the same for dropout RBMs.\nThe histograms clearly indicate that the dropout RBMs learn much sparser representations\nthan standard RBMs even when no additional sparsity inducing regularizer is present.\n9. Marginalizing Dropout\nDropout can be seen as a way of adding noise to the states of hidden units in a neural\nnetwork. In this section, we explore the class of models that arise as a result of marginalizing\nthis noise. These models can be seen as deterministic versions of dropout. In contrast to\nstandard (“Monte-Carlo”) dropout, these models do not need random bits and it is possible\nto get gradients for the marginalized loss functions. In this section, we brieﬂy explore these\nmodels.\nDeterministic algorithms have been proposed that try to learn models that are robust to\nfeature deletion at test time (Globerson and Roweis, 2006). Marginalization in the context\nof denoising autoencoders has been explored previously (Chen et al., 2012). The marginal-\nization of dropout noise in the context of linear regression was discussed in Srivastava (2013).\nWang and Manning (2013) further explored the idea of marginalizing dropout to speed-up\ntraining. van der Maaten et al. (2013) investigated diﬀerent input noise distributions and\n1949\n\n\nSrivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\nthe regularizers obtained by marginalizing this noise. Wager et al. (2013) describes how\ndropout can be seen as an adaptive regularizer.\n9.1 Linear Regression\nFirst we explore a very simple case of applying dropout to the classical problem of linear\nregression. Let X ∈RN×D be a data matrix of N data points. y ∈RN be a vector of\ntargets. Linear regression tries to ﬁnd a w ∈RD that minimizes\n||y −Xw||2.\nWhen the input X is dropped out such that any input dimension is retained with\nprobability p, the input can be expressed as R∗X where R ∈{0, 1}N×D is a random matrix\nwith Rij ∼Bernoulli(p) and ∗denotes an element-wise product. Marginalizing the noise,\nthe objective function becomes\nminimize\nw\nER∼Bernoulli(p)\n\u0002\n||y −(R ∗X)w||2\u0003\n.\nThis reduces to\nminimize\nw\n||y −pXw||2 + p(1 −p)||Γw||2,\nwhere Γ = (diag(X⊤X))1/2.\nTherefore, dropout with linear regression is equivalent, in\nexpectation, to ridge regression with a particular form for Γ. This form of Γ essentially\nscales the weight cost for weight wi by the standard deviation of the ith dimension of the\ndata. If a particular data dimension varies a lot, the regularizer tries to squeeze its weight\nmore.\nAnother interesting way to look at this objective is to absorb the factor of p into w.\nThis leads to the following form\nminimize\nw\n||y −X ew||2 + 1 −p\np\n||Γew||2,\nwhere ew = pw. This makes the dependence of the regularization constant on p explicit.\nFor p close to 1, all the inputs are retained and the regularization constant is small. As\nmore dropout is done (by decreasing p), the regularization constant grows larger.\n9.2 Logistic Regression and Deep Networks\nFor logistic regression and deep neural nets, it is hard to obtain a closed form marginalized\nmodel. However, Wang and Manning (2013) showed that in the context of dropout applied\nto logistic regression, the corresponding marginalized model can be trained approximately.\nUnder reasonable assumptions, the distributions over the inputs to the logistic unit and over\nthe gradients of the marginalized model are Gaussian. Their means and variances can be\ncomputed eﬃciently. This approximate marginalization outperforms Monte-Carlo dropout\nin terms of training time and generalization performance.\nHowever, the assumptions involved in this technique become successively weaker as more\nlayers are added. Therefore, the results are not directly applicable to deep networks.\n1950\n\n\nDropout\nData Set\nArchitecture\nBernoulli dropout\nGaussian dropout\nMNIST\n2 layers, 1024 units each\n1.08 ± 0.04\n0.95 ± 0.04\nCIFAR-10\n3 conv + 2 fully connected layers\n12.6 ± 0.1\n12.5 ± 0.1\nTable 10: Comparison of classiﬁcation error % with Bernoulli and Gaussian dropout. For MNIST,\nthe Bernoulli model uses p = 0.5 for the hidden units and p = 0.8 for the input units.\nFor CIFAR-10, we use p = (0.9, 0.75, 0.75, 0.5, 0.5, 0.5) going from the input layer to the\ntop. The value of σ for the Gaussian dropout models was set to be\nq\n1−p\np . Results were\naveraged over 10 diﬀerent random seeds.\n10. Multiplicative Gaussian Noise\nDropout involves multiplying hidden activations by Bernoulli distributed random variables\nwhich take the value 1 with probability p and 0 otherwise. This idea can be generalized\nby multiplying the activations with random variables drawn from other distributions. We\nrecently discovered that multiplying by a random variable drawn from N(1, 1) works just\nas well, or perhaps better than using Bernoulli noise. This new form of dropout amounts\nto adding a Gaussian distributed random variable with zero mean and standard deviation\nequal to the activation of the unit.\nThat is, each hidden activation hi is perturbed to\nhi + hir where r ∼N(0, 1), or equivalently hir′ where r′ ∼N(1, 1). We can generalize\nthis to r′ ∼N(1, σ2) where σ becomes an additional hyperparameter to tune, just like p\nwas in the standard (Bernoulli) dropout. The expected value of the activations remains\nunchanged, therefore no weight scaling is required at test time.\nIn this paper, we described dropout as a method where we retain units with probability p\nat training time and scale down the weights by multiplying them by a factor of p at test time.\nAnother way to achieve the same eﬀect is to scale up the retained activations by multiplying\nby 1/p at training time and not modifying the weights at test time. These methods are\nequivalent with appropriate scaling of the learning rate and weight initializations at each\nlayer.\nTherefore, dropout can be seen as multiplying hi by a Bernoulli random variable rb that\ntakes the value 1/p with probability p and 0 otherwise. E[rb] = 1 and V ar[rb] = (1 −p)/p.\nFor the Gaussian multiplicative noise, if we set σ2 = (1 −p)/p, we end up multiplying\nhi by a random variable rg, where E[rg] = 1 and V ar[rg] = (1 −p)/p. Therefore, both\nforms of dropout can be set up so that the random variable being multiplied by has the\nsame mean and variance. However, given these ﬁrst and second order moments, rg has the\nhighest entropy and rb has the lowest. Both these extremes work well, although preliminary\nexperimental results shown in Table 10 suggest that the high entropy case might work\nslightly better. For each layer, the value of σ in the Gaussian model was set to be\nq\n1−p\np\nusing the p from the corresponding layer in the Bernoulli model.\n11. Conclusion\nDropout is a technique for improving neural networks by reducing overﬁtting. Standard\nbackpropagation learning builds up brittle co-adaptations that work for the training data\nbut do not generalize to unseen data. Random dropout breaks up these co-adaptations by\n1951\n\n\nSrivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\nmaking the presence of any particular hidden unit unreliable. This technique was found\nto improve the performance of neural nets in a wide variety of application domains includ-\ning object classiﬁcation, digit recognition, speech recognition, document classiﬁcation and\nanalysis of computational biology data. This suggests that dropout is a general technique\nand is not speciﬁc to any domain. Methods that use dropout achieve state-of-the-art re-\nsults on SVHN, ImageNet, CIFAR-100 and MNIST. Dropout considerably improved the\nperformance of standard neural nets on other data sets as well.\nThis idea can be extended to Restricted Boltzmann Machines and other graphical mod-\nels. The central idea of dropout is to take a large model that overﬁts easily and repeatedly\nsample and train smaller sub-models from it. RBMs easily ﬁt into this framework. We de-\nveloped Dropout RBMs and empirically showed that they have certain desirable properties.\nOne of the drawbacks of dropout is that it increases training time. A dropout network\ntypically takes 2-3 times longer to train than a standard neural network of the same ar-\nchitecture. A major cause of this increase is that the parameter updates are very noisy.\nEach training case eﬀectively tries to train a diﬀerent random architecture. Therefore, the\ngradients that are being computed are not gradients of the ﬁnal architecture that will be\nused at test time. Therefore, it is not surprising that training takes a long time. However,\nit is likely that this stochasticity prevents overﬁtting. This creates a trade-oﬀbetween over-\nﬁtting and training time. With more training time, one can use high dropout and suﬀer less\noverﬁtting. However, one way to obtain some of the beneﬁts of dropout without stochas-\nticity is to marginalize the noise to obtain a regularizer that does the same thing as the\ndropout procedure, in expectation. We showed that for linear regression this regularizer is\na modiﬁed form of L2 regularization. For more complicated models, it is not obvious how to\nobtain an equivalent regularizer. Speeding up dropout is an interesting direction for future\nwork.\nAcknowledgments\nThis research was supported by OGS, NSERC and an Early Researcher Award.\nAppendix A. A Practical Guide for Training Dropout Networks\nNeural networks are infamous for requiring extensive hyperparameter tuning.\nDropout\nnetworks are no exception. In this section, we describe heuristics that might be useful for\napplying dropout.\nA.1 Network Size\nIt is to be expected that dropping units will reduce the capacity of a neural network. If\nn is the number of hidden units in any layer and p is the probability of retaining a unit,\nthen instead of n hidden units, only pn units will be present after dropout, in expectation.\nMoreover, this set of pn units will be diﬀerent each time and the units are not allowed to\nbuild co-adaptations freely. Therefore, if an n-sized layer is optimal for a standard neural\nnet on any given task, a good dropout net should have at least n/p units. We found this to\nbe a useful heuristic for setting the number of hidden units in both convolutional and fully\nconnected networks.\n1952\n\n\nDropout\nA.2 Learning Rate and Momentum\nDropout introduces a signiﬁcant amount of noise in the gradients compared to standard\nstochastic gradient descent. Therefore, a lot of gradients tend to cancel each other. In\norder to make up for this, a dropout net should typically use 10-100 times the learning rate\nthat was optimal for a standard neural net. Another way to reduce the eﬀect the noise is\nto use a high momentum. While momentum values of 0.9 are common for standard nets,\nwith dropout we found that values around 0.95 to 0.99 work quite a lot better. Using high\nlearning rate and/or momentum signiﬁcantly speed up learning.\nA.3 Max-norm Regularization\nThough large momentum and learning rate speed up learning, they sometimes cause the\nnetwork weights to grow very large. To prevent this, we can use max-norm regularization.\nThis constrains the norm of the vector of incoming weights at each hidden unit to be bound\nby a constant c. Typical values of c range from 3 to 4.\nA.4 Dropout Rate\nDropout introduces an extra hyperparameter—the probability of retaining a unit p. This\nhyperparameter controls the intensity of dropout. p = 1, implies no dropout and low values\nof p mean more dropout. Typical values of p for hidden units are in the range 0.5 to 0.8.\nFor input layers, the choice depends on the kind of input. For real-valued inputs (image\npatches or speech frames), a typical value is 0.8. For hidden layers, the choice of p is coupled\nwith the choice of number of hidden units n. Smaller p requires big n which slows down\nthe training and leads to underﬁtting. Large p may not produce enough dropout to prevent\noverﬁtting.\nAppendix B. Detailed Description of Experiments and Data Sets\n.\nThis section describes the network architectures and training details for the experimental\nresults reported in this paper. The code for reproducing these results can be obtained from\nhttp://www.cs.toronto.edu/~nitish/dropout. The implementation is GPU-based. We\nused the excellent CUDA libraries—cudamat (Mnih, 2009) and cuda-convnet (Krizhevsky\net al., 2012) to implement our networks.\nB.1 MNIST\nThe MNIST data set consists of 60,000 training and 10,000 test examples each representing\na 28×28 digit image. We held out 10,000 random training images for validation. Hyperpa-\nrameters were tuned on the validation set such that the best validation error was produced\nafter 1 million weight updates. The validation set was then combined with the training set\nand training was done for 1 million weight updates. This net was used to evaluate the per-\nformance on the test set. This way of using the validation set was chosen because we found\nthat it was easy to set up hyperparameters so that early stopping was not required at all.\nTherefore, once the hyperparameters were ﬁxed, it made sense to combine the validation\nand training sets and train for a very long time.\n1953\n\n\nSrivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\nThe architectures shown in Figure 4 include all combinations of 2, 3, and 4 layer networks\nwith 1024 and 2048 units in each layer. Thus, there are six architectures in all. For all the\narchitectures (including the ones reported in Table 2), we used p = 0.5 in all hidden layers\nand p = 0.8 in the input layer. A ﬁnal momentum of 0.95 and weight constraints with c = 2\nwas used in all the layers.\nTo test the limits of dropout’s regularization power, we also experimented with 2 and 3\nlayer nets having 4096 and 8192 units. 2 layer nets gave improvements as shown in Table 2.\nHowever, the three layer nets performed slightly worse than 2 layer ones with the same\nlevel of dropout. When we increased dropout, performance improved but not enough to\noutperform the 2 layer nets.\nB.2 SVHN\nThe SVHN data set consists of approximately 600,000 training images and 26,000 test\nimages. The training set consists of two parts—A standard labeled training set and another\nset of labeled examples that are easy. A validation set was constructed by taking examples\nfrom both the parts. Two-thirds of it were taken from the standard set (400 per class) and\none-third from the extra set (200 per class), a total of 6000 samples. This same process\nis used by Sermanet et al. (2012). The inputs were RGB pixels normalized to have zero\nmean and unit variance. Other preprocessing techniques such as global or local contrast\nnormalization or ZCA whitening did not give any noticeable improvements.\nThe best architecture that we found uses three convolutional layers each followed by\na max-pooling layer. The convolutional layers have 96, 128 and 256 ﬁlters respectively.\nEach convolutional layer has a 5 × 5 receptive ﬁeld applied with a stride of 1 pixel. Each\nmax pooling layer pools 3 × 3 regions at strides of 2 pixels. The convolutional layers are\nfollowed by two fully connected hidden layers having 2048 units each. All units use the\nrectiﬁed linear activation function. Dropout was applied to all the layers of the network\nwith the probability of retaining the unit being p = (0.9, 0.75, 0.75, 0.5, 0.5, 0.5) for the\ndiﬀerent layers of the network (going from input to convolutional layers to fully connected\nlayers). In addition, the max-norm constraint with c = 4 was used for all the weights. A\nmomentum of 0.95 was used in all the layers. These hyperparameters were tuned using a\nvalidation set. Since the training set was quite large, we did not combine the validation\nset with the training set for ﬁnal training. We reported test error of the model that had\nsmallest validation error.\nB.3 CIFAR-10 and CIFAR-100\nThe CIFAR-10 and CIFAR-100 data sets consists of 50,000 training and 10,000 test images\neach. They have 10 and 100 image categories respectively. These are 32 × 32 color images.\nWe used 5,000 of the training images for validation. We followed the procedure similar\nto MNIST, where we found the best hyperparameters using the validation set and then\ncombined it with the training set. The images were preprocessed by doing global contrast\nnormalization in each color channel followed by ZCA whitening. Global contrast normal-\nization means that for image and each color channel in that image, we compute the mean\nof the pixel intensities and subtract it from the channel. ZCA whitening means that we\nmean center the data, rotate it onto its principle components, normalize each component\n1954\n\n\nDropout\nand then rotate it back. The network architecture and dropout rates are same as that for\nSVHN, except the learning rates for the input layer which had to be set to smaller values.\nB.4 TIMIT\nThe open source Kaldi toolkit (Povey et al., 2011) was used to preprocess the data into log-\nﬁlter banks. A monophone system was trained to do a forced alignment and to get labels for\nspeech frames. Dropout neural networks were trained on windows of 21 consecutive frames\nto predict the label of the central frame. No speaker dependent operations were performed.\nThe inputs were mean centered and normalized to have unit variance.\nWe used probability of retention p = 0.8 in the input layers and 0.5 in the hidden layers.\nMax-norm constraint with c = 4 was used in all the layers. A momentum of 0.95 with a\nhigh learning rate of 0.1 was used. The learning rate was decayed as ϵ0(1 + t/T)−1. For\nDBN pretraining, we trained RBMs using CD-1. The variance of each input unit for the\nGaussian RBM was ﬁxed to 1. For ﬁnetuning the DBN with dropout, we found that in\norder to get the best results it was important to use a smaller learning rate (about 0.01).\nAdding max-norm constraints did not give any improvements.\nB.5 Reuters\nThe Reuters RCV1 corpus contains more than 800,000 documents categorized into 103\nclasses. These classes are arranged in a tree hierarchy. We created a subset of this data set\nconsisting of 402,738 articles and a vocabulary of 2000 words comprising of 50 categories\nin which each document belongs to exactly one class. The data was split into equal sized\ntraining and test sets. We tried many network architectures and found that dropout gave\nimprovements in classiﬁcation accuracy over all of them. However, the improvement was\nnot as signiﬁcant as that for the image and speech data sets. This might be explained by\nthe fact that this data set is quite big (more than 200,000 training examples) and overﬁtting\nis not a very serious problem.\nB.6 Alternative Splicing\nThe alternative splicing data set consists of data for 3665 cassette exons, 1014 RNA features\nand 4 tissue types derived from 27 mouse tissues. For each input, the target consists of 4\nsoftmax units (one for tissue type). Each softmax unit has 3 states (inc, exc, nc) which are\nof the biological importance. For each softmax unit, the aim is to predict a distribution over\nthese 3 states that matches the observed distribution from wet lab experiments as closely\nas possible. The evaluation metric is Code Quality which is deﬁned as\n|data points|\nX\ni=1\nX\nt∈tissue types\nX\ns∈{inc, exc, nc}\nps\ni,t log(qs\nt (ri)\n¯ps\n),\nwhere, ps\ni,t is the target probability for state s and tissue type t in input i; qs\nt (ri) is the\npredicted probability for state s in tissue type t for input ri and ¯ps is the average of ps\ni,t\nover i and t.\nA two layer dropout network with 1024 units in each layer was trained on this data set.\nA value of p = 0.5 was used for the hidden layer and p = 0.7 for the input layer. Max-norm\nregularization with high decaying learning rates was used. Results were averaged across the\nsame 5 folds used by Xiong et al. (2011).\n1955\n\n\nSrivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\nReferences\nM. Chen, Z. Xu, K. Weinberger, and F. Sha.\nMarginalized denoising autoencoders for\ndomain adaptation.\nIn Proceedings of the 29th International Conference on Machine\nLearning, pages 767–774. ACM, 2012.\nG. E. Dahl, M. Ranzato, A. Mohamed, and G. E. Hinton. Phone recognition with the mean-\ncovariance restricted Boltzmann machine. In Advances in Neural Information Processing\nSystems 23, pages 469–477, 2010.\nO. Dekel, O. Shamir, and L. Xiao. Learning to classify with missing and corrupted features.\nMachine Learning, 81(2):149–178, 2010.\nA. Globerson and S. Roweis. Nightmare at test time: robust learning by feature deletion. In\nProceedings of the 23rd International Conference on Machine Learning, pages 353–360.\nACM, 2006.\nI. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio. Maxout networks.\nIn Proceedings of the 30th International Conference on Machine Learning, pages 1319–\n1327. ACM, 2013.\nG. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks.\nScience, 313(5786):504 – 507, 2006.\nG. E. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep belief nets.\nNeural Computation, 18:1527–1554, 2006.\nK. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun. What is the best multi-stage\narchitecture for object recognition?\nIn Proceedings of the International Conference on\nComputer Vision (ICCV’09). IEEE, 2009.\nA. Krizhevsky. Learning multiple layers of features from tiny images. Technical report,\nUniversity of Toronto, 2009.\nA. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolu-\ntional neural networks. In Advances in Neural Information Processing Systems 25, pages\n1106–1114, 2012.\nY. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D.\nJackel. Backpropagation applied to handwritten zip code recognition. Neural Computa-\ntion, 1(4):541–551, 1989.\nY. Lin, F. Lv, S. Zhu, M. Yang, T. Cour, K. Yu, L. Cao, Z. Li, M.-H. Tsai, X. Zhou,\nT. Huang, and T. Zhang. Imagenet classiﬁcation: fast descriptor coding and large-scale\nsvm training. Large scale visual recognition challenge, 2010.\nA. Livnat, C. Papadimitriou, N. Pippenger, and M. W. Feldman.\nSex, mixability, and\nmodularity. Proceedings of the National Academy of Sciences, 107(4):1452–1457, 2010.\nV. Mnih. CUDAMat: a CUDA-based matrix class for Python. Technical Report UTML\nTR 2009-004, Department of Computer Science, University of Toronto, November 2009.\n1956\n\n\nDropout\nA. Mohamed, G. E. Dahl, and G. E. Hinton. Acoustic modeling using deep belief networks.\nIEEE Transactions on Audio, Speech, and Language Processing, 2010.\nR. M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag New York, Inc., 1996.\nY. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in\nnatural images with unsupervised feature learning. In NIPS Workshop on Deep Learning\nand Unsupervised Feature Learning 2011, 2011.\nS. J. Nowlan and G. E. Hinton. Simplifying neural networks by soft weight-sharing. Neural\nComputation, 4(4), 1992.\nD. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann,\nP. Motlicek, Y. Qian, P. Schwarz, J. Silovsky, G. Stemmer, and K. Vesely. The Kaldi\nSpeech Recognition Toolkit. In IEEE 2011 Workshop on Automatic Speech Recognition\nand Understanding. IEEE Signal Processing Society, 2011.\nR. Salakhutdinov and G. Hinton. Deep Boltzmann machines. In Proceedings of the Inter-\nnational Conference on Artiﬁcial Intelligence and Statistics, volume 5, pages 448–455,\n2009.\nR. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using Markov\nchain Monte Carlo.\nIn Proceedings of the 25th International Conference on Machine\nLearning. ACM, 2008.\nJ. Sanchez and F. Perronnin. High-dimensional signature compression for large-scale image\nclassiﬁcation.\nIn Proceedings of the 2011 IEEE Conference on Computer Vision and\nPattern Recognition, pages 1665–1672, 2011.\nP. Sermanet, S. Chintala, and Y. LeCun. Convolutional neural networks applied to house\nnumbers digit classiﬁcation. In International Conference on Pattern Recognition (ICPR\n2012), 2012.\nP. Simard, D. Steinkraus, and J. Platt. Best practices for convolutional neural networks ap-\nplied to visual document analysis. In Proceedings of the Seventh International Conference\non Document Analysis and Recognition, volume 2, pages 958–962, 2003.\nJ. Snoek, H. Larochelle, and R. Adams. Practical Bayesian optimization of machine learning\nalgorithms. In Advances in Neural Information Processing Systems 25, pages 2960–2968,\n2012.\nN. Srebro and A. Shraibman. Rank, trace-norm and max-norm. In Proceedings of the 18th\nannual conference on Learning Theory, COLT’05, pages 545–560. Springer-Verlag, 2005.\nN. Srivastava. Improving Neural Networks with Dropout. Master’s thesis, University of\nToronto, January 2013.\nR. Tibshirani.\nRegression shrinkage and selection via the lasso.\nJournal of the Royal\nStatistical Society. Series B. Methodological, 58(1):267–288, 1996.\n1957\n\n\nSrivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\nA. N. Tikhonov. On the stability of inverse problems. Doklady Akademii Nauk SSSR, 39(5):\n195–198, 1943.\nL. van der Maaten, M. Chen, S. Tyree, and K. Q. Weinberger. Learning with marginalized\ncorrupted features.\nIn Proceedings of the 30th International Conference on Machine\nLearning, pages 410–418. ACM, 2013.\nP. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust\nfeatures with denoising autoencoders. In Proceedings of the 25th International Conference\non Machine Learning, pages 1096–1103. ACM, 2008.\nP. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol. Stacked denoising\nautoencoders: Learning useful representations in a deep network with a local denoising\ncriterion. In Proceedings of the 27th International Conference on Machine Learning, pages\n3371–3408. ACM, 2010.\nS. Wager, S. Wang, and P. Liang. Dropout training as adaptive regularization. In Advances\nin Neural Information Processing Systems 26, pages 351–359, 2013.\nS. Wang and C. D. Manning. Fast dropout training. In Proceedings of the 30th International\nConference on Machine Learning, pages 118–126. ACM, 2013.\nH. Y. Xiong, Y. Barash, and B. J. Frey. Bayesian prediction of tissue-regulated splicing\nusing RNA sequence and cellular context. Bioinformatics, 27(18):2554–2562, 2011.\nM. D. Zeiler and R. Fergus. Stochastic pooling for regularization of deep convolutional\nneural networks. CoRR, abs/1301.3557, 2013.\n1958\n"
}