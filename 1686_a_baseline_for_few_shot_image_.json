{
  "filename": "1686_a_baseline_for_few_shot_image_.pdf",
  "num_pages": 20,
  "pages": [
    "Published as a conference paper at ICLR 2020\nA BASELINE FOR FEW-SHOT\nIMAGE CLASSIFICATION\nGuneet S. Dhillon1, Pratik Chaudhari2∗, Avinash Ravichandran1, Stefano Soatto1,3\n1Amazon Web Services, 2University of Pennsylvania, 3University of California, Los Angeles\n{guneetsd, ravinash, soattos}@amazon.com, pratikac@seas.upenn.edu\nABSTRACT\nFine-tuning a deep network trained with the standard cross-entropy loss is a strong\nbaseline for few-shot learning. When ﬁne-tuned transductively, this outperforms\nthe current state-of-the-art on standard datasets such as Mini-ImageNet, Tiered-\nImageNet, CIFAR-FS and FC-100 with the same hyper-parameters. The simplicity\nof this approach enables us to demonstrate the ﬁrst few-shot learning results on\nthe ImageNet-21k dataset. We ﬁnd that using a large number of meta-training\nclasses results in high few-shot accuracies even for a large number of few-shot\nclasses. We do not advocate our approach as the solution for few-shot learning, but\nsimply use the results to highlight limitations of current benchmarks and few-shot\nprotocols. We perform extensive studies on benchmark datasets to propose a metric\nthat quantiﬁes the “hardness” of a few-shot episode. This metric can be used to\nreport the performance of few-shot algorithms in a more systematic way.\n1\nINTRODUCTION\nPrototypical\nNetworks [2017]\nMAML\n[2017]\nLEO\n[2018]\nMetaOpt\nSVM [2019]\nTransductive\nFine-Tuning\n0\n20\n40\n60\n80\n100\n1-shot, 5-way accuracy on Mini-Imagenet (%)\nFigure 1: Are we making progress? The box-plot illustrates the performance of state-of-the-art few-shot\nalgorithms on the Mini-ImageNet (Vinyals et al., 2016) dataset for the 1-shot 5-way protocol. The boxes\nshow the ± 25% quantiles of the accuracy while the notches indicate the median and its 95% conﬁdence\ninterval. Whiskers denote the 1.5× interquartile range which captures 99.3% of the probability mass for a\nnormal distribution. The spread of the box-plots are large, indicating that the standard deviations of the few-shot\naccuracies is large too. This suggests that progress may be illusory, especially considering that none outperform\nthe simple transductive ﬁne-tuning baseline discussed in this paper (rightmost).\nAs image classiﬁcation systems begin to tackle more and more classes, the cost of annotating a\nmassive number of images and the difﬁculty of procuring images of rare categories increases. This\nhas fueled interest in few-shot learning, where only few labeled samples per class are available for\ntraining. Fig. 1 displays a snapshot of the state-of-the-art. We estimated this plot by using published\n∗Work done while at Amazon Web Services\n1\n",
    "Published as a conference paper at ICLR 2020\nnumbers for the estimate of the mean accuracy, the 95% conﬁdence interval of this estimate and the\nnumber of few-shot episodes. For MAML (Finn et al., 2017) and MetaOpt SVM (Lee et al., 2019),\nwe use the number of episodes in the author’s Github implementation.\nThe ﬁeld appears to be progressing steadily albeit slowly based on Fig. 1. However, the variance of the\nestimate of the mean accuracy is not the same as the variance of the accuracy. The former can be zero\n(e.g., asymptotically for an unbiased estimator), yet the latter could be arbitrarily large. The variance\nof the accuracies is extremely large in Fig. 1. This suggests that progress in the past few years may\nbe less signiﬁcant than it seems if one only looks at the mean accuracies. To compound the problem,\nmany algorithms report results using different models for different number of ways (classes) and\nshots (number of labeled samples per class), with aggressive hyper-parameter optimization.1 Our goal\nis to develop a simple baseline for few-shot learning, one that does not require specialized training\ndepending on the number of ways or shots, nor hyper-parameter tuning for different protocols.\nThe simplest baseline we can think of is to pre-train a model on the meta-training dataset using the\nstandard cross-entropy loss, and then ﬁne-tune on the few-shot dataset. Although this approach is\nbasic and has been considered before (Vinyals et al., 2016; Chen et al., 2018), it has gone unnoticed\nthat it outperforms many sophisticated few-shot algorithms. Indeed, with a small twist of performing\nﬁne-tuning transductively, this baseline outperforms all state-of-the-art algorithms on all standard\nbenchmarks and few-shot protocols (cf. Table 1).\nOur contribution is to develop a transductive ﬁne-tuning baseline for few-shot learning, our approach\nworks even for a single labeled example and a single test datum per class. Our baseline outperforms\nthe state-of-the-art on a variety of benchmark datasets such as Mini-ImageNet (Vinyals et al., 2016),\nTiered-ImageNet (Ren et al., 2018), CIFAR-FS (Bertinetto et al., 2018) and FC-100 (Oreshkin et al.,\n2018), all with the same hyper-parameters. Current approaches to few-shot learning are hard to\nscale to large datasets. We report the ﬁrst few-shot learning results on the ImageNet-21k dataset\n(Deng et al., 2009) which contains 14.2 million images across 21,814 classes. The rare classes in\nImageNet-21k form a natural benchmark for few-shot learning.\nThe empirical performance of this baseline, should not be understood as us suggesting that this is the\nright way of performing few-shot learning. We believe that sophisticated meta-training, understanding\ntaxonomies and meronomies, transfer learning, and domain adaptation are necessary for effective\nfew-shot learning. The performance of the simple baseline however indicates that we need to interpret\nexisting results2 with a grain of salt, and be wary of methods that tailor to the benchmark. To facilitate\nthat, we propose a metric to quantify the hardness of few-shot episodes and a way to systematically\nreport performance for different few-shot protocols.\n2\nPROBLEM DEFINITION AND RELATED WORK\nWe ﬁrst introduce some notation and formalize the few-shot image classiﬁcation problem. Let\n(x, y) denote an image and its ground-truth label respectively. The training and test datasets are\nDs = {(xi, yi)}Ns\ni=1 and Dq = {(xi, yi)}Nq\ni=1 respectively, where yi ∈Ct for some set of classes Ct.\nIn the few-shot learning literature, training and test datasets are referred to as support and query\ndatasets respectively, and are collectively called a few-shot episode. The number of ways, or classes,\nis |Ct|. The set {xi | yi = k, (xi, yi) ∈Ds} is the support of class k and its cardinality is s support\nshots (this is non-zero and is generally shortened to shots). The number s is small in the few-shot\nsetting. The set {xi | yi = k, (xi, yi) ∈Dq} is the query of class k and its cardinality is q query shots.\nThe goal is to learn a function F to exploit the training set Ds to predict the label of a test datum x,\n1For instance, Rusu et al. (2018) tune for different few-shot protocols, with parameters changing by up to six\norders of magnitude; Oreshkin et al. (2018) use a different query shot for different few-shot protocols.\n2For instance, Vinyals et al. (2016); Ravi & Larochelle (2016) use different versions of Mini-ImageNet;\nOreshkin et al. (2018) report results for meta-training on the training set while Qiao et al. (2018) use both the\ntraining and validation sets; Chen et al. (2018) use full-sized images from the parent ImageNet-1k dataset (Deng\net al., 2009); Snell et al. (2017); Finn et al. (2017); Oreshkin et al. (2018); Rusu et al. (2018) use different model\narchitectures of varying sizes, which makes it difﬁcult to disentangle the effect of their algorithmic contributions.\n2\n",
    "Published as a conference paper at ICLR 2020\nwhere (x, y) ∈Dq, by\nˆy = F(x; Ds).\n(1)\nTypical approaches for supervised learning replace Ds above with a statistic, θ∗= θ∗(Ds) that is,\nideally, sufﬁcient to classify Ds, as measured by, say, the cross-entropy loss\nθ∗(Ds) = arg min\nθ\n1\nNs\nX\n(x,y)∈Ds\n−log pθ(y|x),\n(2)\nwhere pθ(·|x) is the probability distribution on Ct as predicted by the model in response to input x.\nWhen presented with a test datum, the classiﬁcation rule is typically chosen to be of the form\nFθ∗(x; Ds) ≜arg max\nk\npθ∗(k|x),\n(3)\nwhere Ds is represented by θ∗. This form of the classiﬁer entails a loss of generality unless θ∗is a\nsufﬁcient statistic, pθ∗(y|x) = p(y|x), which is of course never the case, especially given few labeled\ndata in Ds. However, it conveniently separates training and inference phases, never having to revisit\nthe training set. This might be desirable in ordinary image classiﬁcation, but not in few-shot learning.\nWe therefore adopt the more general form of F in (1).\nIf we call the test datum x = xNs+1, then we can obtain the general form of the classiﬁer by\nˆy = F(x; Ds) = arg min\nyNs+1\nmin\nθ\n1\nNs + 1\nNs+1\nX\ni=1\n−log pθ(yi|xi).\n(4)\nIn addition to the training set, one typically also has a meta-training set, Dm = {(xi, yi)}Nm\ni=1,\nwhere yi\n∈\nCm, with set of classes Cm disjoint from Ct.\nThe goal of meta-training\nis to use Dm to infer the parameters of the few-shot learning model:\nˆθ(Dm; (Ds, Dq)) =\narg minθ\n1\nNm\nP\n(x,y)∈Dm ℓ(y, Fθ(x; (Ds, Dq))), where meta-training loss ℓdepends on the method.\n2.1\nRELATED WORK\nLearning to learn: The meta-training loss is designed to make few-shot training efﬁcient (Utgoff,\n1986; Schmidhuber, 1987; Baxter, 1995; Thrun, 1998). This approach partitions the problem into a\nbase-level that performs standard supervised learning and a meta-level that accrues information from\nthe base-level. Two main approaches have emerged to do so.\nGradient-based approaches: These approaches treat the updates of the base-level as a learnable\nmapping (Bengio et al., 1992). This mapping can be learnt using temporal models (Hochreiter\net al., 2001; Ravi & Larochelle, 2016), or one can back-propagate the gradients across the base-level\nupdates (Maclaurin et al., 2015; Finn et al., 2017). It is challenging to perform this dual or bi-level\noptimization, respectively. These approaches have not been shown to be competitive on large datasets.\nRecent approaches learn the base-level in closed-form using SVMs (Bertinetto et al., 2018; Lee et al.,\n2019) which restricts the capacity of the base-level although it alleviates the optimization problem.\nMetric-based approaches: A majority of the state-of-the-art algorithms are metric-based approaches.\nThese approaches learn an embedding that can be used to compare (Bromley et al., 1994; Chopra\net al., 2005) or cluster (Vinyals et al., 2016; Snell et al., 2017) query samples. Recent approaches build\nupon this idea with increasing levels of sophistication in learning the embedding (Vinyals et al., 2016;\nGidaris & Komodakis, 2018; Oreshkin et al., 2018), creating exemplars from the support set and\npicking a metric for the embedding (Gidaris & Komodakis, 2018; Allen et al., 2018; Ravichandran\net al., 2019). There are numerous hyper-parameters involved in implementing these approaches which\nmakes it hard to evaluate them systematically (Chen et al., 2018).\nTransductive learning: This approach is more efﬁcient at using few labeled data than supervised\nlearning (Joachims, 1999; Zhou et al., 2004; Vapnik, 2013). The idea is to use information from\nthe test datum x to restrict the hypothesis space while searching for the classiﬁer F(x, Ds) at test\ntime. Our approach is closest to this line of work. We train a model on the meta-training set Dm and\n3\n",
    "Published as a conference paper at ICLR 2020\ninitialize a classiﬁer using the support set Ds. The parameters are then ﬁne-tuned to adapt to the new\ntest datum x.\nThere are recent papers in few-shot learning such as Nichol et al. (2018); Liu et al. (2018a) that\nare motivated from transductive learning and exploit the unlabeled query samples. The former\nupdates batch-normalization parameters using query samples while the latter uses label propagation\nto estimate labels of all query samples at once.\nSemi-supervised learning: We penalize the Shannon Entropy of the predictions on the query\nsamples at test time. This is a simple technique in the semi-supervised learning literature, closest to\nGrandvalet & Bengio (2005). Modern augmentation techniques such as Miyato et al. (2015); Sajjadi\net al. (2016); Dai et al. (2017) or graph-based approaches (Kipf & Welling, 2016) can also be used\nwith our approach; we used the entropic penalty for the sake of simplicity.\nSemi-supervised few-shot learning is typically formulated as having access to extra unlabeled data\nduring meta-training or few-shot training (Garcia & Bruna, 2017; Ren et al., 2018). This is different\nfrom our approach which uses the unlabeled query samples for transductive learning.\nInitialization for ﬁne-tuning: We use recent ideas from the deep metric learning literature (Hu et al.,\n2015; Movshovitz-Attias et al., 2017; Qi et al., 2018; Chen et al., 2018; Gidaris & Komodakis, 2018)\nto initialize the meta-trained model for ﬁne-tuning. These works connect the softmax cross-entropy\nloss with cosine distance and are discussed further in Section 3.1.\n3\nAPPROACH\nThe simplest form of meta-training is pre-training with the cross-entropy loss, which yields\nˆθ = arg min\nθ\n1\nNm\nX\n(x,y)∈Dm\n−log pθ(y|x) + R(θ),\n(5)\nwhere the second term denotes a regularizer, say weight decay R(θ) = ∥θ∥2/2. The model predicts\nlogits zk(x; θ) for k ∈Cm and the distribution pθ(·|x) is computed from these logits using the\nsoftmax operator. This loss is typically minimized by stochastic gradient descent-based algorithms.\nIf few-shot training is performed according to the general form in (4), then the optimization is\nidentical to that above and amounts to ﬁne-tuning the pre-trained model. However, the model needs\nto be modiﬁed to account for the new classes. Careful initialization can make this process efﬁcient.\n3.1\nSUPPORT-BASED INITIALIZATION\nGiven the pre-trained model (called the “backbone”), pθ (dropping the hat from ˆθ), we append a\nnew fully-connected “classiﬁer” layer that takes the logits of the backbone as input and predicts the\nlabels in Ct. For a support sample (x, y), denote the logits of the backbone by z(x; θ) ∈R|Cm|; the\nweights and biases of the classiﬁer by w ∈R|Ct|×|Cm| and b ∈R|Ct| respectively; and the kth row of\nw and b by wk and bk respectively. The ReLU non-linearity is denoted by (·)+.\nIf the classiﬁer’s logits are z′\n= wz(x; θ)+ + b, the ﬁrst term in the cross-entropy loss:\n−log pΘ(y|x) = −wyz(x; θ)+ −by + log P\nk ewkz(x;θ)++bk would be the cosine distance between\nwy and z(x; θ)+ if both were normalized to unit ℓ2 norm and bias by = 0. This suggests\nwy =\nz(x; θ)+\n∥z(x; θ)+∥\nand\nby = 0\n(6)\nas a candidate for initializing the classiﬁer, along with normalizing z(x; θ)+ to unit ℓ2 norm. It is\neasy to see that this maximizes the cosine similarity between features z(x; θ)+ and weights wy. For\nmultiple support samples per class, we take the Euclidean average of features z(x; θ)+ for each class\nin Ct, before ℓ2 normalization in (6). The logits of the classiﬁer are thus given by\nR|Ct| ∋z(x; Θ) = w z(x; θ)+\n∥z(x; θ)+∥+ b,\n(7)\n4\n",
    "Published as a conference paper at ICLR 2020\nwhere Θ = {θ, w, b}, the combined parameters of the backbone and the classiﬁer. Note that we have\nadded a ReLU non-linearity between the backbone and the classiﬁer, before the ℓ2 normalization. All\nthe parameters Θ are trainable in the ﬁne-tuning phase.\nRemark 1 (Relation to weight imprinting). The support-based initialization is motivated from\nprevious papers (Hu et al., 2015; Movshovitz-Attias et al., 2017; Chen et al., 2018; Gidaris &\nKomodakis, 2018). In particular, Qi et al. (2018) use a similar technique, with minor differences, to\nexpand the size of the ﬁnal fully-connected layer (classiﬁer) for low-shot continual learning. The\nauthors call their technique “weight imprinting” because wk can be thought of as a template for class\nk. In our case, we are only interested in performing well on the few-shot classes.\nRemark 2 (Using logits of the backbone instead of features as input to the classiﬁer). A natural\nway to adapt the backbone to predict new classes is to re-initialize its ﬁnal fully-connected layer\n(classiﬁer). We instead append a new classiﬁer after the logits of the backbone. This is motivated from\nFrosst et al. (2019) who show that for a trained backbone, outputs of all layers are entangled, without\nclass-speciﬁc clusters; but the logits are peaked on the correct class, and are therefore well-clustered.\nThe logits are thus better inputs to the classiﬁer as compared to the features. We explore this choice\nvia an experiment in Appendix C.6.\n3.2\nTRANSDUCTIVE FINE-TUNING\nIn (4), we assumed that there is a single query sample. However, we can also process multiple query\nsamples together, and perform the minimization over all unknown query labels. We introduce a\nregularizer, similar to Grandvalet & Bengio (2005), as we seek outputs with a peaked posterior, or\nlow Shannon Entropy H. So the transductive ﬁne-tuning phase solves for\nΘ∗= arg min\nΘ\n1\nNs\nX\n(x,y)∈Ds\n−log pΘ (y | x) + 1\nNq\nX\n(x,y)∈Dq\nH(pΘ(· | x)).\n(8)\nNote that the data ﬁtting term uses the labeled support samples whereas the regularizer uses the\nunlabeled query samples. The two terms can be highly imbalanced (due to the varying range of values\nfor the two quantities, or due to the variance in their estimates which depend on Ns and Nq). To allow\nﬁner control on this imbalance, one can use a coefﬁcient for the entropic term and/or a temperature\nin the softmax distribution of the query samples. Tuning these hyper-parameters per dataset and\nfew-shot protocol leads to uniform improvements in the results in Section 4 by 1-2%. However, we\nwish to keep in line with our goal of developing a simple baseline and refrain from optimizing these\nhyper-parameters, and set them equal to 1 for all experiments on benchmark datasets.\n4\nEXPERIMENTAL RESULTS\nWe show results of transductive ﬁne-tuning on benchmark datasets in few-shot learning, namely\nMini-ImageNet (Vinyals et al., 2016), Tiered-ImageNet (Ren et al., 2018), CIFAR-FS (Bertinetto\net al., 2018) and FC-100 (Oreshkin et al., 2018), in Section 4.1. We also show large-scale experiments\non the ImageNet-21k dataset (Deng et al., 2009) in Section 4.2. Along with the analysis in Section 4.3,\nthese help us design a metric that measures the hardness of an episode in Section 4.4. We sketch key\npoints of the experimental setup here; see Appendix A for details.\nPre-training: We use the WRN-28-10 (Zagoruyko & Komodakis, 2016) model as the backbone. We\npre-train using standard data augmentation, cross-entropy loss with label smoothing (Szegedy et al.,\n2016) of ϵ=0.1, mixup regularization (Zhang et al., 2017) of α=0.25, SGD with batch-size of 256,\nNesterov’s momentum of 0.9, weight-decay of 10−4 and no dropout. We use batch-normalization\n(Ioffe & Szegedy, 2015) but exclude its parameters from weight decay (Jia et al., 2018). We use\ncyclic learning rates (Smith, 2017) and half-precision distributed training on 8 GPUs (Howard et al.,\n2018) to reduce training time.\nEach dataset has a training, validation and test set consisting of disjoint sets of classes. Some\nalgorithms use only the training set as the meta-training set (Snell et al., 2017; Oreshkin et al., 2018),\nwhile others use both training and validation sets (Rusu et al., 2018). For completeness we report\n5\n",
    "Published as a conference paper at ICLR 2020\nresults using both methodologies; the former is denoted as (train) while the latter is denoted as (train\n+ val). All experiments in Sections 4.3 and 4.4 use the (train + val) setting.\nFine-tuning: We perform ﬁne-tuning on one GPU in full-precision for 25 epochs and a ﬁxed learning\nrate of 5 × 10−5 with Adam (Kingma & Ba, 2014) without any regularization. We make two weight\nupdates in each epoch: one for the cross-entropy term using support samples and one for the Shannon\nEntropy term using query samples (cf. (8)).\nHyper-parameters: We used images from ImageNet-1k belonging to the training classes of Mini-\nImageNet as the validation set for pre-training the backbone for Mini-ImageNet. We used the\nvalidation set of Mini-ImageNet to choose hyper-parameters for ﬁne-tuning. All hyper-parameters\nare kept constant for experiments on benchmark datasets.\nEvaluation: Few-shot episodes contain classes sampled uniformly from classes in the test sets of the\nrespective datasets; support and query samples are further sampled uniformly for each class; the query\nshot is ﬁxed to 15 for all experiments unless noted otherwise. All networks are evaluated over 1,000\nfew-shot episodes unless noted otherwise. To enable easy comparison with existing literature, we\nreport an estimate of the mean accuracy and the 95% conﬁdence interval of this estimate. However,\nwe encourage reporting the standard deviation in light of Section 1 and Fig. 1.\n4.1\nRESULTS ON BENCHMARK DATASETS\nTable 1: Few-shot accuracies on benchmark datasets for 5-way few-shot episodes. The notation conv\n(64k)×4 denotes a CNN with 4 layers and 64k channels in the kth layer. Best results in each column are shown\nin bold. Results where the support-based initialization is better than or comparable to existing algorithms\nare denoted by †. The notation (train + val) indicates that the backbone was pre-trained on both training and\nvalidation sets of the datasets; the backbone is trained only on the training set otherwise. (Lee et al., 2019) uses a\n1.25× wider ResNet-12 which we denote as ResNet-12 ∗.\nMini-ImageNet\nTiered-ImageNet\nCIFAR-FS\nFC-100\nAlgorithm\nArchitecture\n1-shot (%)\n5-shot (%)\n1-shot (%)\n5-shot (%)\n1-shot (%)\n5-shot (%)\n1-shot (%)\n5-shot (%)\nMatching networks (Vinyals et al., 2016)\nconv (64)×4\n46.6\n60\nLSTM meta-learner (Ravi & Larochelle,\n2016)\nconv (64)×4 43.44 ± 0.77 60.60 ± 0.71\nPrototypical Networks (Snell et al., 2017)\nconv (64)×4 49.42 ± 0.78 68.20 ± 0.66\nMAML (Finn et al., 2017)\nconv (32)×4 48.70 ± 1.84 63.11 ± 0.92\nR2D2 (Bertinetto et al., 2018)\nconv (96k)×4\n51.8 ± 0.2\n68.4 ± 0.2\n65.4 ± 0.2\n79.4 ± 0.2\nTADAM (Oreshkin et al., 2018)\nResNet-12\n58.5 ± 0.3\n76.7 ± 0.3\n40.1 ± 0.4\n56.1 ± 0.4\nTransductive\nPropagation\n(Liu\net\nal.,\n2018b)\nconv (64)×4 55.51 ± 0.86 69.86 ± 0.65\n59.91 ± 0.94\n73.30 ± 0.75\nTransductive\nPropagation\n(Liu\net\nal.,\n2018b)\nResNet-12\n59.46\n75.64\nMetaOpt SVM (Lee et al., 2019)\nResNet-12 ∗62.64 ± 0.61 78.63 ± 0.46\n65.99 ± 0.72\n81.56 ± 0.53\n72.0 ± 0.7\n84.2 ± 0.5\n41.1 ± 0.6\n55.5 ± 0.6\nSupport-based initialization (train)\nWRN-28-10 56.17 ± 0.64 73.31 ± 0.53 67.45 ± 0.70† 82.88 ± 0.53†\n70.26 ± 0.70 83.82 ± 0.49† 36.82 ± 0.51 49.72 ± 0.55\nFine-tuning (train)\nWRN-28-10 57.73 ± 0.62 78.17 ± 0.49\n66.58 ± 0.70\n85.55 ± 0.48\n68.72 ± 0.67\n86.11 ± 0.47 38.25 ± 0.52 57.19 ± 0.57\nTransductive ﬁne-tuning (train)\nWRN-28-10 65.73 ± 0.68 78.40 ± 0.52\n73.34 ± 0.71\n85.50 ± 0.50\n76.58 ± 0.68\n85.79 ± 0.50 43.16 ± 0.59 57.57 ± 0.55\nActivation to Parameter (Qiao et al., 2018)\n(train + val)\nWRN-28-10 59.60 ± 0.41 73.74 ± 0.19\nLEO (Rusu et al., 2018) (train + val)\nWRN-28-10 61.76 ± 0.08 77.59 ± 0.12\n66.33 ± 0.05\n81.44 ± 0.09\nMetaOpt SVM (Lee et al., 2019) (train +\nval)\nResNet-12 ∗64.09 ± 0.62 80.00 ± 0.45\n65.81 ± 0.74\n81.75 ± 0.53\n72.8 ± 0.7\n85.0 ± 0.5\n47.2 ± 0.6\n62.5 ± 0.6\nSupport-based initialization (train + val)\nWRN-28-10 58.47 ± 0.66 75.56 ± 0.52 67.34 ± 0.69† 83.32 ± 0.51† 72.14 ± 0.69† 85.21 ± 0.49† 45.08 ± 0.61 60.05 ± 0.60\nFine-tuning (train + val)\nWRN-28-10 59.62 ± 0.66 79.93 ± 0.47\n66.23 ± 0.68\n86.08 ± 0.47\n70.07 ± 0.67\n87.26 ± 0.45 43.80 ± 0.58 64.40 ± 0.58\nTransductive ﬁne-tuning (train + val)\nWRN-28-10 68.11 ± 0.69 80.36 ± 0.50\n72.87 ± 0.71\n86.15 ± 0.50\n78.36 ± 0.70\n87.54 ± 0.49 50.44 ± 0.68 65.74 ± 0.60\nTable 1 shows the results of transductive ﬁne-tuning on benchmark datasets for standard few-shot\nprotocols. We see that this simple baseline is uniformly better than state-of-the-art algorithms. We\ninclude results for support-based initialization, which does no ﬁne-tuning; and for ﬁne-tuning, which\ninvolves optimizing only the cross-entropy term in (8) using the labeled support samples.\nThe support-based initialization is sometimes better than or comparable to state-of-the-art\nalgorithms (marked †). The few-shot literature has gravitated towards larger backbones (Rusu et al.,\n2018). Our results indicate that for large backbones even standard cross-entropy pre-training and\nsupport-based initialization work well, similar to observation made by Chen et al. (2018).\n6\n",
    "Published as a conference paper at ICLR 2020\nFor the 1-shot 5-way setting, ﬁne-tuning using only the labeled support examples leads to minor\nimprovement over the initialization, and sometimes marginal degradation. However, for the 5-shot\n5-way setting non-transductive ﬁne-tuning is better than the state-of-the-art.\nIn both (train) and (train + val) settings, transductive ﬁne-tuning leads to 2-7% improvement for\n1-shot 5-way setting over the state-of-the-art for all datasets. It results in an increase of 1.5-4% for\nthe 5-shot 5-way setting except for the Mini-ImageNet dataset, where the performance is matched.\nThis suggests that the use of the unlabeled query samples is vital for the few-shot setting.\nFor the Mini-ImageNet, CIFAR-FS and FC-100 datasets, using additional data from the valida-\ntion set to pre-train the backbone results in 2-8% improvements; the improvement is smaller for\nTiered-ImageNet. This suggests that having more pre-training classes leads to improved few-shot\nperformance as a consequence of a better embedding. See Appendix C.5 for more experiments.\n4.2\nLARGE-SCALE FEW-SHOT LEARNING\nThe ImageNet-21k dataset (Deng et al., 2009) with 14.2M images across 21,814 classes is an ideal\nlarge-scale few-shot learning benchmark due to the high class imbalance. The simplicity of our\napproach allows us to present the ﬁrst few-shot learning results on this large dataset. We use the 7,491\nclasses having more than 1,000 images each as the meta-training set and the next 13,007 classes with\nat least 10 images each for constructing few-shot episodes. See Appendix B for details.\nTable 2: Accuracy (%) on the few-shot data of ImageNet-21k. The conﬁdence intervals are large because we\ncompute statistics only over 80 few-shot episodes so as to test for large number of ways.\nWay\nAlgorithm\nModel\nShot\n5\n10\n20\n40\n80\n160\nSupport-based initialization\nWRN-28-10\n1\n87.20 ± 1.72\n78.71 ± 1.63\n69.48 ± 1.30\n60.55 ± 1.03\n49.15 ± 0.68\n40.57 ± 0.42\nTransductive ﬁne-tuning\nWRN-28-10\n1\n89.00 ± 1.86\n79.88 ± 1.70\n69.66 ± 1.30\n60.72 ± 1.04\n48.88 ± 0.66\n40.46 ± 0.44\nSupport-based initialization\nWRN-28-10\n5\n95.73 ± 0.84\n91.00 ± 1.09\n84.77 ± 1.04\n78.10 ± 0.79\n70.09 ± 0.71\n61.93 ± 0.45\nTransductive ﬁne-tuning\nWRN-28-10\n5\n95.20 ± 0.94\n90.61 ± 1.03\n84.21 ± 1.09\n77.13 ± 0.82\n68.94 ± 0.75\n60.11 ± 0.48\nTable 2 shows the mean accuracy of transductive ﬁne-tuning evaluated over 80 few-shot episodes on\nImageNet-21k. The accuracy is extremely high as compared to corresponding results in Table 1 even\nfor large way. E.g., the 1-shot 5-way accuracy on Tiered-ImageNet is 72.87 ± 0.71% while it is 89 ±\n1.86% here. This corroborates the results in Section 4.1 and indicates that pre-training with a large\nnumber of classes may be an effective strategy to build large-scale few-shot learning systems.\nThe improvements of transductive ﬁne-tuning are minor for ImageNet-21k because the support-based\ninitialization accuracies are extremely high. We noticed a slight degradation of accuracies due to\ntransductive ﬁne-tuning at high ways because the entropic term in (8) is much larger than the the\ncross-entropy loss. The experiments for ImageNet-21k therefore scale down the entropic term by\nlog |Ct| and forego the ReLU in (6) and (7). This reduces the difference in accuracies at high ways.\n4.3\nANALYSIS\nThis section presents a comprehensive analysis of transductive ﬁne-tuning on the Mini-ImageNet,\nTiered-ImageNet and ImageNet-21k datasets.\nRobustness of transductive ﬁne-tuning to query shot: Fig. 2a shows the effect of changing the\nquery shot on the mean accuracy. For the 1-shot 5-way setting, the entropic penalty in (8) helps as the\nquery shot increases. This effect is minor in the 5-shot 5-way setting as more labeled data is available.\nQuery shot of 1 achieves a relatively high mean accuracy because transductive ﬁne-tuning can adapt\nto those few queries. One query shot is enough to beneﬁt from transductive ﬁne-tuning: for\nMini-ImageNet, the 1-shot 5-way accuracy with query shot of 1 is 66.94 ± 1.55% which is better\nthan non-transductive ﬁne-tuning (59.62 ± 0.66% in Table 1) and higher than other approaches.\nPerformance for different way and support shot: A few-shot system should be able to robustly\nhandle different few-shot scenarios. Figs. 2b and 2c, show the performance of transductive ﬁne-tuning\n7\n",
    "Published as a conference paper at ICLR 2020\n1\n5\n10\n15\n20\nQuery shot\n65\n75\n85\n95\nMean accuracy (%)\n1 shot 5-way Mini-Imagenet\n5 shot         \" \n1 shot 5-way Tiered-Imagenet\n5 shot         \" \n(a)\n10\n1\n10\n2\nWay\n0\n20\n40\n60\n80\n100\nMean accuracy (%)\n 1 shot   Tiered-ImageNet\n 5 shot       \" \n10 shot       \" \n 1 shot   ImageNet-21k\n 5 shot       \" \n10 shot       \" \n(b)\n10\n0\n10\n1\nSupport Shot\n20\n40\n60\n80\n100\nMean accuracy (%)\n 5 way Tiered-Imagenet\n20 way       \"\n80 way       \"\n160 way       \"\n(c)\nFigure 2: Mean accuracy of transductive ﬁne-tuning for different query shot, way and support shot.\nFig. 2a shows that the mean accuracy improves with query shot if the support shot is low; this effect is minor for\nTiered-ImageNet. The mean accuracy for query shot of 1 is high because transductive ﬁne-tuning can specialize\nto those queries. Fig. 2b shows that the mean accuracy degrades logarithmically with way for ﬁxed support shot\nand query shot (15). Fig. 2c suggests that the mean accuracy improves logarithmically with the support shot for\nﬁxed way and query shot (15). These trends suggest thumb rules for building few-shot systems.\nwith changing way and support shot. The mean accuracy changes logarithmically with the way\nand support shot which provides thumb rules for building few-shot systems.\nDifferent backbone architectures: We include experiments using conv (64)×4 (Vinyals et al., 2016)\nand ResNet-12 (He et al., 2016a; Oreshkin et al., 2018) in Table 3, in order to facilitate comparisons\nfor different backbone architectures. The results for transductive ﬁne-tuning are comparable or better\nthan state-of-the-art for a given backbone architecture, except for those in Liu et al. (2018b) who\nuse a more sophisticated transductive algorithm using graph propagation, with conv (64)×4. In line\nwith our goal for simplicity, we kept the hyper-parameters for pre-training and ﬁne-tuning the\nsame as the ones used for WRN-28-10 (cf. Sections 3 and 4). These results show that transductive\nﬁne-tuning is a sound baseline for a variety of backbone architectures.\nComputational complexity: There is no free lunch and our advocated baseline has its limitations.\nIt performs gradient updates during the ﬁne-tuning phase which makes it slow at inference time.\nSpeciﬁcally, transductive ﬁne-tuning is about 300× slower (20.8 vs. 0.07 seconds) for a 1-shot\n5-way episode with 15 query shot as compared to Snell et al. (2017) with the same backbone\narchitecture (prototypical networks (Snell et al., 2017) do not update model parameters at inference\ntime). The latency factor reduces with higher support shot. Interestingly, for a single query shot,\nthe former takes 4 seconds vs. 0.07 seconds. This is a more reasonable factor of 50×, especially\nconsidering that the mean accuracy of the former is 66.2% compared to about 58% of the latter in our\nimplementation. Experiments in Appendix C.3 suggest that using a smaller backbone architecture\npartially compensates for the latency with some degradation of accuracy. A number of approaches\nsuch as Ravi & Larochelle (2016); Finn et al. (2017); Rusu et al. (2018); Lee et al. (2019) also perform\nadditional processing at inference time and are expected to be slow, along with other transductive\napproaches (Nichol et al., 2018; Liu et al., 2018b). Additionally, support-based initialization has the\nsame inference time as Snell et al. (2017).\n4.4\nA PROPOSAL FOR REPORTING FEW-SHOT CLASSIFICATION PERFORMANCE\nAs discussed in Section 1, we need better metrics to report the performance of few-shot algorithms.\nThere are two main issues: (i) standard deviation of the few-shot accuracy across different sampled\nepisodes for a given algorithm, dataset and few-shot protocol is very high (cf. Fig. 1), and (ii)\ndifferent models and hyper-parameters for different few-shot protocols makes evaluating algorithmic\ncontributions difﬁcult (cf. Table 1). This section takes a step towards resolving these issues.\nHardness of an episode: Classiﬁcation performance on a few-shot episode is determined by the\nrelative location of the features corresponding to labeled and unlabeled samples. If the unlabeled\n8\n",
    "Published as a conference paper at ICLR 2020\nfeatures are close to the labeled features from the same class, the classiﬁer can distinguish between\nthe classes easily to obtain a high accuracy. Otherwise, the accuracy would be low. The following\ndeﬁnition characterizes this intuition.\nFor training (support) set Ds and test (query) set Dq, we will deﬁne the hardness Ωϕ as the average\nlog-odds of a test datum being classiﬁed incorrectly. More precisely,\nΩϕ(Dq; Ds) = 1\nNq\nX\n(x,y)∈Dq\nlog 1 −p(y | x)\np(y | x)\n,\n(9)\nwhere p(·| x) is a softmax distribution with logits zy = wϕ(x). w is the weight matrix constructed\nusing (6) and Ds; and ϕ is the ℓ2 normalized logits computed using a rich-enough feature generator,\nsay a deep network trained for standard image classiﬁcation. This is a clustering loss where the\nlabeled support samples form class-speciﬁc cluster centers. The cluster afﬁnities are calculated using\ncosine-similarities, followed by the softmax operator to get the probability distribution p(·| x).\nNote that Ωϕ does not depend on the few-shot learner and gives a measure of how difﬁcult the\nclassiﬁcation problem is for any few-shot episode, using a generic feature extractor.\n1\n2\n3\n4\n5\nHardness\n20\n40\n60\n80\n100\nAccuracy (%)\nCIFAR-FS\nFC-100\nTiered-Imagenet\nMini-Imagenet\nImagenet-21k\nFigure 3: Comparing the accuracy of transductive ﬁne-tuning (solid lines) vs. support-based initialization\n(dotted lines) for different datasets, ways (5, 10, 20, 40, 80 and 160) and support shots (1 and 5). Abscissae\nare computed using (9) and a Resnet-152 (He et al., 2016b) network trained for standard image classiﬁcation on\nthe ImageNet-1k dataset. Each marker indicates the accuracy of transductive ﬁne-tuning on a few-shot episode;\nmarkers for support-based initialization are hidden to avoid clutter. Shape of the markers denotes different\nways; ways increase from left to right (5, 10, 20, 40, 80 and 160). Size of the markers denotes different support\nshot (1 and 5); it increases from the bottom to the top. E.g., the ellipse contains accuracies of different 5-shot\n10-way episodes for ImageNet-21k. Regression lines are drawn for each algorithm and dataset by combining\nthe episodes of all few-shot protocols. This plot is akin to a precision-recall curve and allows comparing two\nalgorithms for different few-shot scenarios. The areas in the ﬁrst quadrant under the ﬁtted regression lines are\n295 vs. 284 (CIFAR-FS), 167 vs. 149 (FC-100), 208 vs. 194 (Mini-ImageNet), 280 vs. 270 (Tiered-ImageNet)\nand 475 vs. 484 (ImageNet-21k) for transductive ﬁne-tuning and support-based initialization.\n9\n",
    "Published as a conference paper at ICLR 2020\nFig. 3 demonstrates how to use the hardness metric. Few-shot accuracy degrades linearly with\nhardness. Performance for all hardness can thus be estimated by testing for two different ways. We\nadvocate selecting hyper-parameters using the area under the ﬁtted curve as a metric instead\nof tuning them speciﬁcally for each few-shot protocol. The advantage of such a test methodology is\nthat it predicts the performance of the model across multiple few-shot protocols systematically.\nDifferent algorithms can be compared directly, e.g., transductive ﬁne-tuning (solid lines) and\nsupport-based initialization (dotted lines). For instance, the former leads to large improvements on\neasy episodes, the performance is similar for hard episodes, especially for Tiered-ImageNet and\nImageNet-21k.\nThe high standard deviation of accuracy of few-shot learning algorithms in Fig. 1 can be seen as\nthe spread of the cluster corresponding to each few-shot protocol, e.g., the ellipse in Fig. 3 denotes\nthe 5-shot 10-way protocol for ImageNet-21k. It is the nature of few-shot learning that episodes have\nvarying hardness even if the way and shot are ﬁxed. However, episodes within the ellipse lie on a\ndifferent line (with a large negative slope) which indicates that given a few-shot protocol, hardness is\na good indicator of accuracy.\nFig. 3 also shows that due to fewer test classes, CIFAR-FS, FC-100 and Mini-ImageNet have less\ndiversity in the hardness of episodes while Tiered-ImageNet and ImageNet-21k allow sampling of\nboth very hard and very easy diverse episodes. For a given few-shot protocol, the hardness of episodes\nin the former three is almost the same as that of the latter two datasets. This indicates that CIFAR-FS,\nFC-100 and Mini-ImageNet may be good benchmarks for applications with few classes.\nThe hardness metric in (9) naturally builds upon existing ideas in deep metric learning (Qi et al.,\n2018). We propose it as a means to evaluate few-shot learning algorithms uniformly across different\nfew-shot protocols for different datasets; ascertaining its efﬁcacy and comparisons to other metrics\nwill be part of future work.\n5\nDISCUSSION\nOur aim is to provide grounding to the practice of few-shot learning. The current literature is in the\nspirit of increasingly sophisticated approaches for modest improvements in mean accuracy using\nan inadequate evaluation methodology. This is why we set out to establish a baseline, namely\ntransductive ﬁne-tuning, and a systematic evaluation methodology, namely the hardness metric. We\nwould like to emphasize that our advocated baseline, namely transductive ﬁne-tuning, is not novel and\nyet performs better than existing algorithms on all standard benchmarks. This is indeed surprising\nand indicates that we need to take a step back and re-evaluate the status quo in few-shot learning. We\nhope to use the results in this paper as guidelines for the development of new algorithms.\nREFERENCES\nKelsey R Allen, Hanul Shin, Evan Shelhamer, and Josh B Tenenbaum. Variadic learning by bayesian nonpara-\nmetric deep embedding. 2018.\nJonathan Baxter. Learning internal representations. Flinders University of S. Aust., 1995.\nSamy Bengio, Yoshua Bengio, Jocelyn Cloutier, and Jan Gecsei. On the optimization of a synaptic learning rule.\nIn Preprints Conf. Optimality in Artiﬁcial and Biological Neural Networks, pp. 6–8. Univ. of Texas, 1992.\nLuca Bertinetto, Jo˜ao F Henriques, Philip HS Torr, and Andrea Vedaldi. Meta-learning with differentiable\nclosed-form solvers. arXiv:1805.08136, 2018.\nJane Bromley, Isabelle Guyon, Yann LeCun, Eduard S¨ackinger, and Roopak Shah. Signature veriﬁcation using\na” siamese” time delay neural network. In Advances in neural information processing systems, pp. 737–744,\n1994.\nWei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look at few-shot\nclassiﬁcation. 2018.\nSumit Chopra, Raia Hadsell, Yann LeCun, et al. Learning a similarity metric discriminatively, with application\nto face veriﬁcation. In CVPR (1), pp. 539–546, 2005.\n10\n",
    "Published as a conference paper at ICLR 2020\nZihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Ruslan R Salakhutdinov. Good semi-supervised\nlearning that requires a bad gan. In Advances in neural information processing systems, pp. 6510–6520, 2017.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee,\n2009.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep\nnetworks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1126–\n1135. JMLR. org, 2017.\nNicholas Frosst, Nicolas Papernot, and Geoffrey Hinton. Analyzing and improving representations with the soft\nnearest neighbor loss. arXiv:1902.01889, 2019.\nVictor Garcia and Joan Bruna. Few-shot learning with graph neural networks. arXiv:1711.04043, 2017.\nSpyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, pp. 4367–4375, 2018.\nYves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In Advances in neural\ninformation processing systems, pp. 529–536, 2005.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\nThe IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016a.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks.\narXiv:1603.05027, 2016b.\nSepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent. In\nInternational Conference on Artiﬁcial Neural Networks, pp. 87–94. Springer, 2001.\nJeremy Howard et al. fastai. https://github.com/fastai/fastai, 2018.\nJunlin Hu, Jiwen Lu, and Yap-Peng Tan. Deep transfer metric learning. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pp. 325–333, 2015.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. arXiv:1502.03167, 2015.\nXianyan Jia, Shutao Song, Wei He, Yangzihao Wang, Haidong Rong, Feihu Zhou, Liqiang Xie, Zhenyu Guo,\nYuanzhou Yang, Liwei Yu, et al. Highly scalable deep learning training system with mixed-precision: Training\nimagenet in four minutes. arXiv:1807.11205, 2018.\nThorsten Joachims. Transductive inference for text classiﬁcation using support vector machines. In Icml,\nvolume 99, pp. 200–209, 1999.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014.\nThomas N Kipf and Max Welling.\nSemi-supervised classiﬁcation with graph convolutional networks.\narXiv:1609.02907, 2016.\nAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report,\nCiteseer, 2009.\nKwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with differentiable\nconvex optimization. arXiv:1904.03758, 2019.\nYanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sung Ju Hwang, and Yi Yang. Learning to\npropagate labels: Transductive propagation network for few-shot learning. 2018a.\nYanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, and Yi Yang. Transductive propagation network for few-shot\nlearning. arXiv:1805.10002, 2018b.\nIlya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv:1608.03983,\n2016.\nLaurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of machine learning\nresearch, 9(Nov):2579–2605, 2008.\nDougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization through\nreversible learning. In International Conference on Machine Learning, pp. 2113–2122, 2015.\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg,\nMichael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv:1710.03740,\n2017.\n11\n",
    "Published as a conference paper at ICLR 2020\nTakeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, and Shin Ishii. Distributional smoothing with\nvirtual adversarial training. arXiv:1507.00677, 2015.\nYair Movshovitz-Attias, Alexander Toshev, Thomas K Leung, Sergey Ioffe, and Saurabh Singh. No fuss distance\nmetric learning using proxies. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n360–368, 2017.\nAlex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-order meta-learning algorithms. arXiv:1803.02999,\n2018.\nBoris Oreshkin, Pau Rodr´ıguez L´opez, and Alexandre Lacoste. Tadam: Task dependent adaptive metric for\nimproved few-shot learning. In Advances in Neural Information Processing Systems, pp. 719–729, 2018.\nHang Qi, Matthew Brown, and David G Lowe. Low-shot learning with imprinted weights. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, pp. 5822–5830, 2018.\nSiyuan Qiao, Chenxi Liu, Wei Shen, and Alan L Yuille. Few-shot image recognition by predicting parameters\nfrom activations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n7229–7238, 2018.\nSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. 2016.\nAvinash Ravichandran, Rahul Bhotika, and Stefano Soatto. Few-shot learning with embedded class models and\nshot-free meta training, 2019.\nMengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum,\nHugo Larochelle, and Richard S Zemel.\nMeta-learning for semi-supervised few-shot classiﬁcation.\narXiv:1803.00676, 2018.\nAndrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia\nHadsell. Meta-learning with latent embedding optimization. arXiv:1807.05960, 2018.\nMehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transformations and\nperturbations for deep semi-supervised learning. In Advances in Neural Information Processing Systems, pp.\n1163–1171, 2016.\nJurgen Schmidhuber. Evolutionary principles in self-referential learning. On learning how to learn: The\nmeta-meta-... hook.) Diploma thesis, Institut f. Informatik, Tech. Univ. Munich, 1987.\nLeslie N Smith. Cyclical learning rates for training neural networks. In 2017 IEEE Winter Conference on\nApplications of Computer Vision (WACV), pp. 464–472. IEEE, 2017.\nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Advances in\nNeural Information Processing Systems, pp. 4077–4087, 2017.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception\narchitecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 2818–2826, 2016.\nSebastian Thrun. Lifelong learning algorithms. In Learning to learn, pp. 181–209. Springer, 1998.\nEleni Triantaﬁllou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Kelvin Xu, Ross Goroshin, Carles Gelada,\nKevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle. Meta-dataset: A dataset of datasets for\nlearning to learn from few examples. arXiv preprint arXiv:1903.03096, 2019.\nPaul E Utgoff. Shift of bias for inductive concept learning. Machine learning: An artiﬁcial intelligence approach,\n2:107–148, 1986.\nVladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 2013.\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot\nlearning. In Advances in neural information processing systems, pp. 3630–3638, 2016.\nJunyuan Xie, Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, and Mu Li. Bag of tricks for image\nclassiﬁcation with convolutional neural networks. arXiv:1812.01187, 2018.\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv:1605.07146, 2016.\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk\nminimization. arXiv:1710.09412, 2017.\nDengyong Zhou, Olivier Bousquet, Thomas N Lal, Jason Weston, and Bernhard Sch¨olkopf. Learning with local\nand global consistency. In Advances in neural information processing systems, pp. 321–328, 2004.\n12\n",
    "Published as a conference paper at ICLR 2020\nA\nSETUP\nA.1\nDATASETS\nWe use the following datasets for our benchmarking experiments.\n• The Mini-ImageNet dataset (Vinyals et al., 2016) which is a subset of ImageNet-1k (Deng\net al., 2009) and consists of 84 × 84 sized images with 600 images per class. There are 64\ntraining, 16 validation and 20 test classes. There are multiple versions of this dataset in the\nliterature; we obtained the dataset from the authors of Gidaris & Komodakis (2018)3.\n• The Tiered-ImageNet dataset (Ren et al., 2018) is a larger subset of ImageNet-1k with 608\nclasses split as 351 training, 97 validation and 160 testing classes, each with about 1300\nimages of size 84 × 84. This dataset ensures that training, validation and test classes do not\nhave a semantic overlap and is a potentially harder few-shot learning dataset.\n• We also consider two smaller CIFAR-100 (Krizhevsky & Hinton, 2009) derivatives, both\nwith 32 × 32 sized images and 600 images per class. The ﬁrst is the CIFAR-FS dataset\n(Bertinetto et al., 2018) which splits classes randomly into 64 training, 16 validation and 20\ntest. The second is the FC-100 dataset (Oreshkin et al., 2018) which splits CIFAR-100 into\n60 training, 20 validation and 20 test classes with minimal semantic overlap.\nEach dataset has a training, validation and test set. The set of classes for each of these sets are disjoint\nfrom each other. For meta-training, we ran two sets of experiments: the ﬁrst, where we only use\nthe training set as the meta-training dataset, denoted by (train); the second, where we use both the\ntraining and validation sets as the meta-training dataset, denoted by (train + val). We use the test set\nto construct few-shot episodes.\nA.2\nPRE-TRAINING\nWe use a wide residual network (Zagoruyko & Komodakis, 2016; Qiao et al., 2018; Rusu et al.,\n2018) with a widening factor of 10 and a depth of 28 which we denote as WRN-28-10. The smaller\nnetworks: conv (64)×4 (Vinyals et al., 2016; Snell et al., 2017), ResNet-12 (He et al., 2016a; Oreshkin\net al., 2018; Lee et al., 2019) and WRN-16-4 (Zagoruyko & Komodakis, 2016), are used for analysis\nin Appendix C. All networks are trained using SGD with a batch-size of 256, Nesterov’s momentum\nset to 0.9, no dropout, weight decay of 10−4. We use batch-normalization (Ioffe & Szegedy, 2015).\nWe use two-cycles of learning rate annealing (Smith, 2017), these are 40 and 80 epochs each for all\ndatasets except ImageNet-21k, which uses cycles of 8 and 16 epochs each. The learning rate is set to\n10−i at the beginning of the ith cycle and decreased to 10−6 by the end of that cycle with a cosine\nschedule (Loshchilov & Hutter, 2016). We use data parallelism across 8 Nvidia V100 GPUs and\nhalf-precision training using techniques from Micikevicius et al. (2017); Howard et al. (2018).\nWe use the following regularization techniques that have been discovered in the non-few-shot,\nstandard image classiﬁcation literature (Xie et al., 2018) for pre-training the backbone.\n• Mixup (Zhang et al., 2017): This augments data by a linear interpolation between input\nimages and their one-hot labels. If (x1, y1), (x2, y2) ∈D are two samples, mixup creates a\nnew sample (˜x, ˜y) where ˜x = λx1 +(1−λ)x2 and its label ˜y = λey1 +(1−λ)ey2; here ek\nis the one-hot vector with a non-zero kth entry and λ ∈[0, 1] is sampled from Beta(α, α)\nfor a hyper-parameter α.\n• Label smoothing (Szegedy et al., 2016): When using a softmax operator, the logits can\nincrease or decrease in an unbounded manner causing numerical instabilities while training.\nLabel smoothing sets pθ(k|x) = 1 −ϵ if k = y and ϵ/(K −1) otherwise, for a small\nconstant ϵ > 0 and number of classes K. The ratio between the largest and smallest output\nneuron is thus ﬁxed which helps large-scale training.\n• We exclude the batch-normalization parameters from weight-decay (Jia et al., 2018).\n3https://github.com/gidariss/FewShotWithoutForgetting\n13\n",
    "Published as a conference paper at ICLR 2020\nWe set ϵ=0.1 for label smoothing cross-entroy loss and α=0.25 for mixup regularization for all our\nexperiments.\nA.3\nFINE-TUNING HYPER-PARAMETERS\nWe used 1-shot 5-way episodes on the validation set of Mini-ImageNet to manually tune hyper-\nparameters. Fine-tuning is done for 25 epochs with a ﬁxed learning rate of 5 × 10−5 with Adam\n(Kingma & Ba, 2014). Adam is used here as it is more robust to large changes in the magnitude\nof the loss and gradients which occurs if the number of classes in the few-shot episode (ways) is\nlarge. We do not use any regularization (weight-decay, mixup, dropout, or label smoothing) in the\nﬁne-tuning phase. These hyper-parameters are kept constant on all benchmark datasets, namely\nMini-ImageNet, Tiered-ImageNet, CIFAR-FS and FC-100.\nAll ﬁne-tuning and evaluation is performed on a single GPU in full-precision. We update the\nparameters sequentially by computing the gradient of the two terms in (8) independently. This\nupdates both the weights of the model and the batch-normalization parameters.\nA.4\nDATA AUGMENTATION\nInput images are normalized using the mean and standard-deviation computed on ImageNet-1k. Our\nData augmentation consists of left-right ﬂips with probability of 0.5, padding the image with 4px\nand adding brightness and contrast changes of ± 40%. The augmentation is kept the same for both\npre-training and ﬁne-tuning.\nWe explored augmentation using afﬁne transforms of the images but found that adding this has minor\neffect with no particular trend on the numerical results.\nA.5\nEVALUATION PROCEDURE\nThe few-shot episode contains classes that are uniformly sampled from the test classes of correspond-\ning datasets. Support and query samples are further uniformly sampled for each class. The query\nshot is ﬁxed to 15 for all experiments unless noted otherwise. We evaluate all networks over 1,000\nepisodes unless noted otherwise. For ease of comparison, we report the mean accuracy and the 95%\nconﬁdence interval of the estimate of the mean accuracy.\nB\nSETUP FOR IMAGENET-21K\nThe ImageNet-21k dataset (Deng et al., 2009) has 14.2M images across 21,814 classes. The blue\nregion in Fig. 4 denotes our meta-training set with 7,491 classes, each with more than 1,000 images.\nThe green region shows 13,007 classes with at least 10 images each, the set used to construct few-shot\nepisodes. We do not use the red region consisting of 1,343 classes with less than 10 images each.\nWe train the same backbone (WRN-28-10) with the same procedure as that in Appendix A on 84 ×\n84 resized images, albeit for only 24 epochs. Since we use the same hyper-parameters as the other\nbenchmark datasets, we did not create validation sets for pre-training or the ﬁne-tuning phases. The\nfew-shot episodes are constructed in the same way as Appendix A. We evaluate using fewer few-shot\nepisodes (80) on this dataset because we would like to demonstrate the performance across a large\nnumber of different ways.\nC\nADDITIONAL ANALYSIS\nThis section contains additional experiments and analysis, complementing Section 4.3. All ex-\nperiments use the (train + val) setting, pre-training on both the training and validation sets of the\ncorresponding datasets, unless noted otherwise.\n14\n",
    "Published as a conference paper at ICLR 2020\n0\n5000\n10000\n15000\n20000\nclasses\n10\n0\n10\n1\n10\n2\n10\n3\nimages per class\nFigure 4: ImageNet-21k is a highly imbalanced dataset. The most frequent class has about 3K images while\nthe rarest class has a single image.\nFigure 5: t-SNE (Maaten & Hinton, 2008) embedding of the logits for 1-shot 5-way few-shot episode of\nMini-ImageNet. Colors denote the ground-truth labels; crosses denote the support samples; circles denote the\nquery samples; translucent markers and opaque markers denote the embeddings before and after transductive\nﬁne-tuning respectively. Even though query samples are far away from their respective supports in the beginning,\nthey move towards the supports by the end of transductive ﬁne-tuning. Logits of support samples are relatively\nunchanged which suggests that the support-based initialization is effective.\nC.1\nTRANSDUCTIVE FINE-TUNING CHANGES THE EMBEDDING DRAMATICALLY\nFig. 5 demonstrates this effect. The logits for query samples are far from those of their respective\nsupport samples and metric-based loss functions, e.g., those for prototypical networks (Snell et al.,\n2017) would have a high loss on this episode; indeed the accuracy after the support-based initialization\nis 64%. Logits for the query samples change dramatically during transductive ﬁne-tuning and majority\nof the query samples cluster around their respective supports. The post transductive ﬁne-tuning\naccuracy of this episode is 73.3%. This suggests that modifying the embedding using the query\nsamples is crucial to obtaining good performance on new classes. This example also demonstrates\nthat the support-based initialization is efﬁcient, logits of the support samples are relatively unchanged\nduring the transductive ﬁne-tuning phase.\n15\n",
    "Published as a conference paper at ICLR 2020\nC.2\nLARGE VS. SMALL BACKBONES\nThe expressive power of the backbone plays an important role in the efﬁcacy of ﬁne-tuning. We\nobserved that a WRN-16-4 architecture (2.7M parameters) performs worse than WRN-28-10 (36M\nparameters). The former obtains 63.28 ± 0.68% and 77.39 ± 0.5% accuracy on Mini-ImageNet and\n69.04 ± 0.69% and 83.55 ± 0.51% accuracy on Tiered-ImageNet on 1-shot 5-way and 5-shot 5-way\nprotocols respectively. While these numbers are comparable to those of state-of-the-art algorithms,\nthey are lower than their counterparts for WRN-28-10 in Table 1. This suggests that a larger network\nis effective in learning richer features from the meta-training classes, and ﬁne-tuning is effective in\ntaking advantage of this to further improve performance on samples belonging to few-shot classes.\nC.3\nLATENCY WITH A SMALLER BACKBONES\nThe WRN-16-4 architecture (2.7M parameters) is much smaller than WRN-28-10 (36M parameters)\nand transductive ﬁne-tuning on the former is much faster. As compared to our implementation of\nSnell et al. (2017) with the same backbone, WRN-16-4 is 20-70× slower (0.87 vs. 0.04 seconds for\na query shot of 1, and 2.85 vs. 0.04 seconds for a query shot of 15) for the 1-shot 5-way scenario.\nCompare this to the computational complexity experiment in Section 4.3.\nAs discussed in Appendix C.2, the accuracy of WRN-16-4 is 63.28 ± 0.68% and 77.39 ± 0.5%\nfor 1-shot 5-way and 5-shot 5-way on Mini-ImageNet respectively. As compared to this, our\nimplementation of (Snell et al., 2017) using a WRN-16-4 backbone obtains 57.29 ± 0.40% and 75.34\n± 0.32% accuracies for the same settings respectively; the former number in particular is signiﬁcantly\nworse than its transductive ﬁne-tuning counterpart.\nC.4\nCOMPARISONS AGAINST BACKBONES IN THE CURRENT LITERATURE\nWe include experiments using conv (64)×4 and ResNet-12 in Table 3, in addition to WRN-28-10\nin Section 4, in order to facilitate comparisons of the proposed baseline for different backbone\narchitectures. Our results are comparable or better than existing results for a given backbone\narchitecture, except for those in Liu et al. (2018b) who use a graph-based transduction algorithm, for\nconv (64)×4 on Mini-ImageNet. In line with our goal for simplicity, we kept the hyper-parameters\nfor pre-training and ﬁne-tuning the same as the ones used for WRN-28-10 (cf. Sections 3 and 4).\nThese results suggest that transductive ﬁne-tuning is a sound baseline for a variety of backbone\narchitectures.\nC.5\nUSING MORE META-TRAINING CLASSES\nIn Section 4.1 we observed that having more pre-training classes improves few-shot performance.\nBut since we append a classiﬁer on top of a pre-trained backbone and use the logits of the backbone\nas inputs to the classiﬁer, a backbone pre-trained on more classes would also have more parameters\nas compared to one pre-trained on fewer classes. However, this difference is not large: WRN-28-10\nfor Mini-ImageNet has 0.03% more parameters for (train + val) as compared to (train). However,\nin order to facilitate a fair comparison, we ran an experiment where we use the features of the\nbackbone, instead of the logits, as inputs to the classiﬁer. By doing so, the number of parameters\nin the pre-trained backbone that are used for few-shot classiﬁcation remain the same for both the\n(train) and (train + val) settings. For Mini-ImageNet, (train + val) obtains 64.20 ± 0.65% and 81.26\n± 0.45%, and (train) obtains 62.55 ± 0.65% and 78.89 ± 0.46%, for 1-shot 5-way and 5-shot 5-way\nrespectively. These results corroborate the original statement that more pre-training classes improves\nfew-shot performance.\nC.6\nUSING FEATURES OF THE BACKBONE AS INPUT TO THE CLASSIFIER\nInstead of re-initializing the ﬁnal fully-connected layer of the backbone to classify new classes, we\nsimply append the classiﬁer on top of it. We implemented the former, more common, approach\nand found that it achieves an accuracy of 64.20 ± 0.65% and 81.26 ± 0.45% for 1-shot 5-way and\n5-shot 5-way respectively on Mini-ImageNet, while the accuracy on Tiered-ImageNet is 67.14 ±\n16\n",
    "Published as a conference paper at ICLR 2020\nTable 3: Few-shot accuracies on benchmark datasets for 5-way few-shot episodes. The notation conv\n(64k)×4 denotes a CNN with 4 layers and 64k channels in the kth layer. The rows are grouped by the backbone\narchitectures. Best results in each column and for a given backbone architecture are shown in bold. Results\nwhere the support-based initialization is better than or comparable to existing algorithms are denoted by †.\nThe notation (train + val) indicates that the backbone was pre-trained on both training and validation sets of\nthe datasets; the backbone is trained only on the training set otherwise. (Lee et al., 2019) uses a 1.25× wider\nResNet-12 which we denote as ResNet-12 ∗.\nMini-ImageNet\nTiered-ImageNet\nCIFAR-FS\nFC-100\nAlgorithm\nArchitecture\n1-shot (%)\n5-shot (%)\n1-shot (%)\n5-shot (%)\n1-shot (%)\n5-shot (%)\n1-shot (%)\n5-shot (%)\nMAML (Finn et al., 2017)\nconv (32)×4 48.70 ± 1.84 63.11 ± 0.92\nMatching networks (Vinyals et al., 2016)\nconv (64)×4\n46.6\n60\nLSTM meta-learner (Ravi & Larochelle,\n2016)\nconv (64)×4 43.44 ± 0.77 60.60 ± 0.71\nPrototypical Networks (Snell et al., 2017)\nconv (64)×4 49.42 ± 0.78 68.20 ± 0.66\nTransductive\nPropagation\n(Liu\net\nal.,\n2018b)\nconv (64)×4 55.51 ± 0.86 69.86 ± 0.65\n59.91 ± 0.94\n73.30 ± 0.75\nSupport-based initialization (train)\nconv (64)×4 50.69 ± 0.63 66.07 ± 0.53\n58.42 ± 0.69 73.98 ± 0.58† 61.77 ± 0.73 76.40 ± 0.54 36.07 ± 0.54 48.72 ± 0.57\nFine-tuning (train)\nconv (64)×4 49.43 ± 0.62 66.42 ± 0.53\n57.45 ± 0.68\n73.96 ± 0.56 59.74 ± 0.72 76.37 ± 0.53 35.46 ± 0.53 49.43 ± 0.57\nTransductive ﬁne-tuning (train)\nconv (64)×4 50.46 ± 0.62 66.68 ± 0.52\n58.05 ± 0.68\n74.24 ± 0.56 61.73 ± 0.72 76.92 ± 0.52 36.62 ± 0.55 50.24 ± 0.58\nR2D2 (Bertinetto et al., 2018)\nconv (96k)×4\n51.8 ± 0.2\n68.4 ± 0.2\n65.4 ± 0.2\n79.4 ± 0.2\nTADAM (Oreshkin et al., 2018)\nResNet-12\n58.5 ± 0.3\n76.7 ± 0.3\n40.1 ± 0.4\n56.1 ± 0.4\nTransductive\nPropagation\n(Liu\net\nal.,\n2018b)\nResNet-12\n59.46\n75.64\nSupport-based initialization (train)\nResNet-12\n54.21 ± 0.64 70.58 ± 0.54\n66.39 ± 0.73\n81.93 ± 0.54 65.69 ± 0.72 79.95 ± 0.51 35.51 ± 0.53 48.26 ± 0.54\nFine-tuning (train)\nResNet-12\n56.67 ± 0.62 74.80 ± 0.51\n64.45 ± 0.70\n83.59 ± 0.51 64.66 ± 0.73 82.13 ± 0.50 37.52 ± 0.53 55.39 ± 0.57\nTransductive ﬁne-tuning (train)\nResNet-12\n62.35 ± 0.66 74.53 ± 0.54\n68.41 ± 0.73\n83.41 ± 0.52 70.76 ± 0.74 81.56 ± 0.53 41.89 ± 0.59 54.96 ± 0.55\nMetaOpt SVM (Lee et al., 2019)\nResNet-12 ∗62.64 ± 0.61 78.63 ± 0.46\n65.99 ± 0.72\n81.56 ± 0.53\n72.0 ± 0.7\n84.2 ± 0.5\n41.1 ± 0.6\n55.5 ± 0.6\nSupport-based initialization (train)\nWRN-28-10 56.17 ± 0.64 73.31 ± 0.53\n67.45 ± 0.70\n82.88 ± 0.53 70.26 ± 0.70 83.82 ± 0.49 36.82 ± 0.51 49.72 ± 0.55\nFine-tuning (train)\nWRN-28-10 57.73 ± 0.62 78.17 ± 0.49\n66.58 ± 0.70\n85.55 ± 0.48 68.72 ± 0.67 86.11 ± 0.47 38.25 ± 0.52 57.19 ± 0.57\nTransductive ﬁne-tuning (train)\nWRN-28-10 65.73 ± 0.68 78.40 ± 0.52\n73.34 ± 0.71\n85.50 ± 0.50 76.58 ± 0.68 85.79 ± 0.50 43.16 ± 0.59 57.57 ± 0.55\nSupport-based initialization (train + val)\nconv (64)×4 52.77 ± 0.64 68.29 ± 0.54\n59.08 ± 0.70\n74.62 ± 0.57 64.01 ± 0.71 78.46 ± 0.53 40.25 ± 0.56 54.53 ± 0.57\nFine-tuning (train + val)\nconv (64)×4 51.40 ± 0.61 68.58 ± 0.52\n58.04 ± 0.68\n74.48 ± 0.56 62.12 ± 0.71 77.98 ± 0.52 39.09 ± 0.55 54.83 ± 0.55\nTransductive ﬁne-tuning (train + val)\nconv (64)×4 52.30 ± 0.61 68.78 ± 0.53\n58.81 ± 0.69\n74.71 ± 0.56 63.89 ± 0.71 78.48 ± 0.52 40.33 ± 0.56 55.60 ± 0.56\nSupport-based initialization (train + val)\nResNet-12\n56.79 ± 0.65 72.94 ± 0.55\n67.60 ± 0.71\n83.09 ± 0.53 69.39 ± 0.71 83.27 ± 0.50 43.11 ± 0.58 58.16 ± 0.57\nFine-tuning (train + val)\nResNet-12\n58.64 ± 0.64 76.83 ± 0.50\n65.55 ± 0.70\n84.51 ± 0.50 68.11 ± 0.70 85.19 ± 0.48 42.84 ± 0.57 63.10 ± 0.57\nTransductive ﬁne-tuning (train + val)\nResNet-12\n64.50 ± 0.68 76.92 ± 0.55\n69.48 ± 0.73\n84.37 ± 0.51 74.35 ± 0.71 84.57 ± 0.53 48.29 ± 0.63 63.38 ± 0.58\nMetaOpt SVM (Lee et al., 2019) (train +\nval)\nResNet-12 ∗64.09 ± 0.62 80.00 ± 0.45\n65.81 ± 0.74\n81.75 ± 0.53\n72.8 ± 0.7\n85.0 ± 0.5\n47.2 ± 0.6\n62.5 ± 0.6\nActivation to Parameter (Qiao et al., 2018)\n(train + val)\nWRN-28-10 59.60 ± 0.41 73.74 ± 0.19\nLEO (Rusu et al., 2018) (train + val)\nWRN-28-10 61.76 ± 0.08 77.59 ± 0.12\n66.33 ± 0.05\n81.44 ± 0.09\nSupport-based initialization (train + val)\nWRN-28-10 58.47 ± 0.66 75.56 ± 0.52 67.34 ± 0.69† 83.32 ± 0.51† 72.14 ± 0.69 85.21 ± 0.49 45.08 ± 0.61 60.05 ± 0.60\nFine-tuning (train + val)\nWRN-28-10 59.62 ± 0.66 79.93 ± 0.47\n66.23 ± 0.68\n86.08 ± 0.47 70.07 ± 0.67 87.26 ± 0.45 43.80 ± 0.58 64.40 ± 0.58\nTransductive ﬁne-tuning (train + val)\nWRN-28-10 68.11 ± 0.69 80.36 ± 0.50\n72.87 ± 0.71\n86.15 ± 0.50 78.36 ± 0.70 87.54 ± 0.49 50.44 ± 0.68 65.74 ± 0.60\n0.74% and 86.67 ± 0.46% for 1-shot 5-way and 5-shot 5-way respectively. These numbers are\nsigniﬁcantly lower for the 1-shot 5-way protocol on both datasets compared to their counterparts in\nTable 1. However, the 5-shot 5-way accuracy is marginally higher in this experiment than that in\nTable 1. As noted in Remark 2, logits of the backbone are well-clustered and that is why they work\nbetter for few-shot scenarios.\n17\n",
    "Published as a conference paper at ICLR 2020\nC.7\nFREEZING THE BACKBONE RESTRICTS PERFORMANCE\nThe previous observation suggests that the network changes a lot in the ﬁne-tuning phase. Freezing\nthe backbone severely restricts the changes in the network to only changes to the classiﬁer. As\na consequence, the accuracy of freezing the backbone is 58.38 ± 0.66 % and 75.46 ± 0.52% on\nMini-ImageNet and 67.06 ± 0.69% and 83.20 ± 0.51% on Tiered-ImageNet for 1-shot 5-way and\n5-shot 5-way respectively. While the 1-shot 5-way accuracies are much lower than their counterparts\nin Table 1, the gap in the 5-shot 5-way scenario is smaller.\nC.8\nUSING MIXUP DURING PRE-TRAINING\nMixup improves the few-shot accuracy by about 1%; the accuracy for WRN-28-10 trained without\nmixup is 67.06 ± 0.71% and 79.29 ± 0.51% on Mini-ImageNet for 1-shot 5-way and 5-shot 5-way\nrespectively.\nC.9\nMORE FEW-SHOT EPISODES\nFig. 1 suggests that the standard deviation of the accuracies achieved by few-shot algorithms is high.\nConsidering this randomness, evaluations were done over 10,000 few-shot episodes as well. The\naccuracies on Mini-ImageNet are 67.77 ± 0.21 % and 80.24 ± 0.16 % and on Tiered-ImageNet are\n72.36 ± 0.23 % and 85.70 ± 0.16 % for 1-shot 5-way and 5-shot 5-way respectively. The numbers\nare consistent with the ones for 1,000 few-shot episodes in Table 1, though the conﬁdence intervals\ndecreased as the number of episodes sampled increased.\nC.10\nEVALUATION ON META-DATASET\nTable 4: Few-shot accuracies on Meta-Dataset: Best results in each row are shown in bold. 600 few-shot\nepisodes were used to compare to the results reported in Triantaﬁllou et al. (2019).\nDataset\nBest performance in\nTransductive\nRank for Transductive\nTriantaﬁllou et al. (2019)\nFine-tuning\nFine-tuning (based on\nTriantaﬁllou et al. (2019))\nImageNet-1k (ILSVRC)\n51.01 ± 1.05\n55.57 ± 1.02\n1\nOmniglot\n63.00 ± 1.35\n79.59 ± 0.98\n1\nAircraft\n68.69 ± 1.26\n67.26 ± 0.98\n1.5\nBirds\n68.79 ± 1.01\n74.26 ± 0.82\n1\nTextures\n69.05 ± 0.90\n77.35 ± 0.74\n1\nVGG Flowers\n86.86 ± 0.75\n88.14 ± 0.63\n1.5\nTrafﬁc Signs\n66.79 ± 1.31\n55.98 ± 1.32\n2\nMSCOCO\n43.41 ± 1.06\n40.62 ± 0.98\n2.5\nAverage Rank\n1.4375\nWe ran experiments on Meta-Dataset (Triantaﬁllou et al., 2019), and compared the performance of\ntransductive ﬁne-tuning for meta-training done on ImageNet-1k (ILSVRC) in Table 4. Transductive\nﬁne-tuning is better, most times signiﬁcantly, than state-of-the-art on 6 out of 8 tasks in Meta-Dataset;\nits average rank across all tasks is 1.4375 (calculated using the results reported in Triantaﬁllou et al.\n(2019)). The Fungi and Quick Draw datasets were not included because of issues with getting the\ndata; the link to access the dataset for the former does not seem to work and the latter requires certain\nlegal conditions which we are working on obtaining.\nThe few-shot episode sampling was done the same way as described in Triantaﬁllou et al. (2019);\nexcept for the few-shot class sampling for ImageNet-1k (ILSVRC) and Omniglot, which was done\nuniformly over all few-shot classes (Triantaﬁllou et al. (2019) use a hierarchical sampling technique to\nsample classes that are far from each other in the hierarchy, and hence easier to distinguish between).\nThe hyper-parameters used for meta-training and few-shot ﬁne-tuning are kept the same as the ones\nin Section 4 and are not tuned for these experiments.\n18\n",
    "Published as a conference paper at ICLR 2020\nD\nFREQUENTLY ASKED QUESTIONS\n1. Why has it not been noticed yet that this simple approach works so well?\nNon-transductive ﬁne-tuning as a baseline has been considered before (Vinyals et al., 2016; Chen\net al., 2018). The fact that this is comparable to state-of-the-art has probably gone unnoticed\nbecause of the following reasons:\n• Given that there are only a few labeled support samples provided in the few-shot setting,\ninitializing the classiﬁer becomes important. The support-based initialization (cf. Section 3.1)\nmotivated from the deep metric learning literature (Hu et al., 2015; Movshovitz-Attias et al.,\n2017; Qi et al., 2018; Gidaris & Komodakis, 2018) classiﬁes support samples correctly (for\na support shot of 1, this may not be true for higher support shots). This initialization, as\nopposed to initializing the weights of the classiﬁer randomly, was critical to performance in\nour experiments.\n• In our experience, existing meta-training methods, both gradient-based ones and metric-\nbased ones, are difﬁcult to tune for larger architectures. We speculate that this is the reason a\nlarge part of the existing literature focuses on smaller backbone architectures. The few-shot\nlearning literature has only recently started to move towards bigger backbone architectures\n(Oreshkin et al., 2018; Rusu et al., 2018). From Table 3 we see that non-tranductive ﬁne-\ntuning gets better with a deeper backbone architecture. A similar observation was made\nby (Chen et al., 2018). The observation that we can use “simple” well-understood training\ntechniques from standard supervised learning that scale up to large backbone architectures\nfor few-shot classiﬁcation is a key contribution of our paper.\nTransductive methods have recently started to become popular in the few-shot learning literature\n(Nichol et al., 2018; Liu et al., 2018a). Because of the scarcity of labeled support samples, it is\ncrucial to make use of the unlabeled query samples in the few-shot regime.\nOur advocated baseline makes use of both a good initialization and transduction, relatively new in\nthe few-shot learning literature, which makes this simplistic approach go unrecognized till now.\n2. Transductive ﬁne-tuning works better than existing algorithms because of a big backbone\narchitecture. One should compare on the same backbone architectures as the existing algo-\nrithms for a fair comparison.\nThe current literature is in the spirit of increasingly sophisticated approaches for modest perfor-\nmance gains, often with different architectures (cf. Table 1). This is why we set out to establish a\nbaseline. Our simple baseline is comparable or better than existing approaches. The backbone\nwe have used is common in the recent few-shot learning literature (Rusu et al., 2018; Qiao et al.,\n2018) (cf. Table 1). Additionally, we have included results on smaller common backbone architec-\ntures, namely conv (64)×4 and ResNet-12 in Appendix C.4, and some additional experiments in\nAppendix C.2. These experiments suggest that transductive ﬁne-tuning is a sound baseline for a\nvariety of different backbone architectures. This indicates that we should take results on existing\nbenchmarks with a grain of salt. Also see the response to question 1 above.\n3. There are missing entries in Tables 1 and 3. Is it still a fair comparison?\nTables 1 and 3 show all relevant published results by the original authors. Re-implementing\nexisting algorithms to ﬁll missing entries without access to original code is impractical and often\nyields results inferior to those published, which may be judged as unfair. The purpose of a\nbenchmark is to enable others to test their method easily. This does not exist today due to myriad\nperformance-critical design choices often not detailed in the papers. In fact, missing entries in\nthe table indicate the inadequate state of the current literature. Our work enables benchmarking\nrelative to a simple, systematic baseline.\n4. Fine-tuning for few-shot learning is not novel.\nWe do not claim novelty in this paper. Transductive ﬁne-tuning is our advocated baseline for\nfew-shot classiﬁcation. It is a combination of different techniques that are not novel. Yet, it\nperforms better than existing algorithms on all few-shot protocols with ﬁxed hyper-parameters.\nWe emphasize that this indicates the need to re-interpret existing results on benchmarks and\nre-evaluate the status quo in the literature.\n19\n",
    "Published as a conference paper at ICLR 2020\n5. Transductive ﬁne-tuning has a very high latency at inference time, this is not practical.\nOur goal is to establish a systematic baseline for accuracy, which might help judge the accuracy of\nfew-shot learning algorithms in the future. The question of test-time latency is indeed important\nbut we have not focused on it in this paper. Appendix C.3 provides results using a smaller backbone\nwhere we see that the WRN-16-4 network is about 20-70x slower than metric-based approaches\nemploying the same backbone while having signiﬁcantly better accuracy. The latencies with\nWRN-28-10 are larger (see the computational complexity section in Section 4.3) but with a bigger\nadvantage in terms of accuracy.\nThere are other transductive methods used for few-shot classiﬁcation (Nichol et al., 2018; Liu\net al., 2018a), that are expected to be slow as well.\n6. Transductive ﬁne-tuning does not make sense in the online setting when query samples are\nshown in a sequence.\nTransductive ﬁne-tuning can be performed even with a single test datum. Indeed, the network can\nspecialize itself completely to classify this one datum. We explore a similar scenario in Section 4.3\nand Fig. 2a, which discuss the performance of transductive ﬁne-tuning with a query shot of 1 (this\nmeans 5 query samples one from each class for 5-way evaluation). Note that the loss function in\n(8) leverages multiple query samples when available. It does not require that the query samples be\nbalanced in terms of their ground-truth classes. In particular, the loss function in (8) is well-deﬁned\neven for a single test datum. For concerns about latency, see the question 5 above.\n7. Having transductive approaches will incentivize hacking the query set.\nThere are already published methods that use transductive methods (Nichol et al., 2018; Liu et al.,\n2018a), and it is a fundamental property of the transductive paradigm to be dependent on the query\nset, in addition to the support set. In order to prevent query set hacking, we will make the test\nepisodes public which will enable consistent benchmarking, even for transductive methods.\n8. Why is having the same hyper-parameters for different few-shot protocols so important?\nA practical few-shot learning algorithm should be able to handle any few-shot protocol. Having\none model for each different scenario is unreasonable in the real-world, as the number of different\nscenarios is, in principle, inﬁnite. Current algorithms do not handle this well. A single model\nwhich can handle any few-shot scenario is thus desirable.\n9. Is this over-ﬁtting to the test datum?\nNo, label of the test datum is not used in the loss function.\n10. Can you give some intuition about the hardness metric? How did you come up with the\nformula?\nThe hardness metric is the clustering loss where the labeled support samples form the centers of\nthe class-speciﬁc clusters. The special form, namely, E(x,y)∈Dq log 1−p(y|x)\np(y|x)\n(cf. (9)) allows an\ninterpretation of log-odds. We used this form because it is sensitive to the number of few-shot\nclasses (cf. Fig. 3). Similar metrics, e.g., E(x,y)∈Dq [−log p(y|x)] can also be used but they\ncome with a few caveats. Note that it is easier for p(y|x) to be large for small way because the\nnormalization constant in softmax has fewer terms. For large way, p(y|x) could be smaller. This\neffect is better captured by our metric.\n11. How does Fig. 3 look for algorithm X, Y, Z?\nWe compared two algorithms in Fig. 3, namely transductive ﬁne-tuning and support-based initial-\nization. Section 4.4 and the caption of Fig. 3 explains how the former algorithm is better. We will\nconsider adding comparisons to other algorithms to this plot in the future.\n20\n"
  ],
  "full_text": "Published as a conference paper at ICLR 2020\nA BASELINE FOR FEW-SHOT\nIMAGE CLASSIFICATION\nGuneet S. Dhillon1, Pratik Chaudhari2∗, Avinash Ravichandran1, Stefano Soatto1,3\n1Amazon Web Services, 2University of Pennsylvania, 3University of California, Los Angeles\n{guneetsd, ravinash, soattos}@amazon.com, pratikac@seas.upenn.edu\nABSTRACT\nFine-tuning a deep network trained with the standard cross-entropy loss is a strong\nbaseline for few-shot learning. When ﬁne-tuned transductively, this outperforms\nthe current state-of-the-art on standard datasets such as Mini-ImageNet, Tiered-\nImageNet, CIFAR-FS and FC-100 with the same hyper-parameters. The simplicity\nof this approach enables us to demonstrate the ﬁrst few-shot learning results on\nthe ImageNet-21k dataset. We ﬁnd that using a large number of meta-training\nclasses results in high few-shot accuracies even for a large number of few-shot\nclasses. We do not advocate our approach as the solution for few-shot learning, but\nsimply use the results to highlight limitations of current benchmarks and few-shot\nprotocols. We perform extensive studies on benchmark datasets to propose a metric\nthat quantiﬁes the “hardness” of a few-shot episode. This metric can be used to\nreport the performance of few-shot algorithms in a more systematic way.\n1\nINTRODUCTION\nPrototypical\nNetworks [2017]\nMAML\n[2017]\nLEO\n[2018]\nMetaOpt\nSVM [2019]\nTransductive\nFine-Tuning\n0\n20\n40\n60\n80\n100\n1-shot, 5-way accuracy on Mini-Imagenet (%)\nFigure 1: Are we making progress? The box-plot illustrates the performance of state-of-the-art few-shot\nalgorithms on the Mini-ImageNet (Vinyals et al., 2016) dataset for the 1-shot 5-way protocol. The boxes\nshow the ± 25% quantiles of the accuracy while the notches indicate the median and its 95% conﬁdence\ninterval. Whiskers denote the 1.5× interquartile range which captures 99.3% of the probability mass for a\nnormal distribution. The spread of the box-plots are large, indicating that the standard deviations of the few-shot\naccuracies is large too. This suggests that progress may be illusory, especially considering that none outperform\nthe simple transductive ﬁne-tuning baseline discussed in this paper (rightmost).\nAs image classiﬁcation systems begin to tackle more and more classes, the cost of annotating a\nmassive number of images and the difﬁculty of procuring images of rare categories increases. This\nhas fueled interest in few-shot learning, where only few labeled samples per class are available for\ntraining. Fig. 1 displays a snapshot of the state-of-the-art. We estimated this plot by using published\n∗Work done while at Amazon Web Services\n1\n\n\nPublished as a conference paper at ICLR 2020\nnumbers for the estimate of the mean accuracy, the 95% conﬁdence interval of this estimate and the\nnumber of few-shot episodes. For MAML (Finn et al., 2017) and MetaOpt SVM (Lee et al., 2019),\nwe use the number of episodes in the author’s Github implementation.\nThe ﬁeld appears to be progressing steadily albeit slowly based on Fig. 1. However, the variance of the\nestimate of the mean accuracy is not the same as the variance of the accuracy. The former can be zero\n(e.g., asymptotically for an unbiased estimator), yet the latter could be arbitrarily large. The variance\nof the accuracies is extremely large in Fig. 1. This suggests that progress in the past few years may\nbe less signiﬁcant than it seems if one only looks at the mean accuracies. To compound the problem,\nmany algorithms report results using different models for different number of ways (classes) and\nshots (number of labeled samples per class), with aggressive hyper-parameter optimization.1 Our goal\nis to develop a simple baseline for few-shot learning, one that does not require specialized training\ndepending on the number of ways or shots, nor hyper-parameter tuning for different protocols.\nThe simplest baseline we can think of is to pre-train a model on the meta-training dataset using the\nstandard cross-entropy loss, and then ﬁne-tune on the few-shot dataset. Although this approach is\nbasic and has been considered before (Vinyals et al., 2016; Chen et al., 2018), it has gone unnoticed\nthat it outperforms many sophisticated few-shot algorithms. Indeed, with a small twist of performing\nﬁne-tuning transductively, this baseline outperforms all state-of-the-art algorithms on all standard\nbenchmarks and few-shot protocols (cf. Table 1).\nOur contribution is to develop a transductive ﬁne-tuning baseline for few-shot learning, our approach\nworks even for a single labeled example and a single test datum per class. Our baseline outperforms\nthe state-of-the-art on a variety of benchmark datasets such as Mini-ImageNet (Vinyals et al., 2016),\nTiered-ImageNet (Ren et al., 2018), CIFAR-FS (Bertinetto et al., 2018) and FC-100 (Oreshkin et al.,\n2018), all with the same hyper-parameters. Current approaches to few-shot learning are hard to\nscale to large datasets. We report the ﬁrst few-shot learning results on the ImageNet-21k dataset\n(Deng et al., 2009) which contains 14.2 million images across 21,814 classes. The rare classes in\nImageNet-21k form a natural benchmark for few-shot learning.\nThe empirical performance of this baseline, should not be understood as us suggesting that this is the\nright way of performing few-shot learning. We believe that sophisticated meta-training, understanding\ntaxonomies and meronomies, transfer learning, and domain adaptation are necessary for effective\nfew-shot learning. The performance of the simple baseline however indicates that we need to interpret\nexisting results2 with a grain of salt, and be wary of methods that tailor to the benchmark. To facilitate\nthat, we propose a metric to quantify the hardness of few-shot episodes and a way to systematically\nreport performance for different few-shot protocols.\n2\nPROBLEM DEFINITION AND RELATED WORK\nWe ﬁrst introduce some notation and formalize the few-shot image classiﬁcation problem. Let\n(x, y) denote an image and its ground-truth label respectively. The training and test datasets are\nDs = {(xi, yi)}Ns\ni=1 and Dq = {(xi, yi)}Nq\ni=1 respectively, where yi ∈Ct for some set of classes Ct.\nIn the few-shot learning literature, training and test datasets are referred to as support and query\ndatasets respectively, and are collectively called a few-shot episode. The number of ways, or classes,\nis |Ct|. The set {xi | yi = k, (xi, yi) ∈Ds} is the support of class k and its cardinality is s support\nshots (this is non-zero and is generally shortened to shots). The number s is small in the few-shot\nsetting. The set {xi | yi = k, (xi, yi) ∈Dq} is the query of class k and its cardinality is q query shots.\nThe goal is to learn a function F to exploit the training set Ds to predict the label of a test datum x,\n1For instance, Rusu et al. (2018) tune for different few-shot protocols, with parameters changing by up to six\norders of magnitude; Oreshkin et al. (2018) use a different query shot for different few-shot protocols.\n2For instance, Vinyals et al. (2016); Ravi & Larochelle (2016) use different versions of Mini-ImageNet;\nOreshkin et al. (2018) report results for meta-training on the training set while Qiao et al. (2018) use both the\ntraining and validation sets; Chen et al. (2018) use full-sized images from the parent ImageNet-1k dataset (Deng\net al., 2009); Snell et al. (2017); Finn et al. (2017); Oreshkin et al. (2018); Rusu et al. (2018) use different model\narchitectures of varying sizes, which makes it difﬁcult to disentangle the effect of their algorithmic contributions.\n2\n\n\nPublished as a conference paper at ICLR 2020\nwhere (x, y) ∈Dq, by\nˆy = F(x; Ds).\n(1)\nTypical approaches for supervised learning replace Ds above with a statistic, θ∗= θ∗(Ds) that is,\nideally, sufﬁcient to classify Ds, as measured by, say, the cross-entropy loss\nθ∗(Ds) = arg min\nθ\n1\nNs\nX\n(x,y)∈Ds\n−log pθ(y|x),\n(2)\nwhere pθ(·|x) is the probability distribution on Ct as predicted by the model in response to input x.\nWhen presented with a test datum, the classiﬁcation rule is typically chosen to be of the form\nFθ∗(x; Ds) ≜arg max\nk\npθ∗(k|x),\n(3)\nwhere Ds is represented by θ∗. This form of the classiﬁer entails a loss of generality unless θ∗is a\nsufﬁcient statistic, pθ∗(y|x) = p(y|x), which is of course never the case, especially given few labeled\ndata in Ds. However, it conveniently separates training and inference phases, never having to revisit\nthe training set. This might be desirable in ordinary image classiﬁcation, but not in few-shot learning.\nWe therefore adopt the more general form of F in (1).\nIf we call the test datum x = xNs+1, then we can obtain the general form of the classiﬁer by\nˆy = F(x; Ds) = arg min\nyNs+1\nmin\nθ\n1\nNs + 1\nNs+1\nX\ni=1\n−log pθ(yi|xi).\n(4)\nIn addition to the training set, one typically also has a meta-training set, Dm = {(xi, yi)}Nm\ni=1,\nwhere yi\n∈\nCm, with set of classes Cm disjoint from Ct.\nThe goal of meta-training\nis to use Dm to infer the parameters of the few-shot learning model:\nˆθ(Dm; (Ds, Dq)) =\narg minθ\n1\nNm\nP\n(x,y)∈Dm ℓ(y, Fθ(x; (Ds, Dq))), where meta-training loss ℓdepends on the method.\n2.1\nRELATED WORK\nLearning to learn: The meta-training loss is designed to make few-shot training efﬁcient (Utgoff,\n1986; Schmidhuber, 1987; Baxter, 1995; Thrun, 1998). This approach partitions the problem into a\nbase-level that performs standard supervised learning and a meta-level that accrues information from\nthe base-level. Two main approaches have emerged to do so.\nGradient-based approaches: These approaches treat the updates of the base-level as a learnable\nmapping (Bengio et al., 1992). This mapping can be learnt using temporal models (Hochreiter\net al., 2001; Ravi & Larochelle, 2016), or one can back-propagate the gradients across the base-level\nupdates (Maclaurin et al., 2015; Finn et al., 2017). It is challenging to perform this dual or bi-level\noptimization, respectively. These approaches have not been shown to be competitive on large datasets.\nRecent approaches learn the base-level in closed-form using SVMs (Bertinetto et al., 2018; Lee et al.,\n2019) which restricts the capacity of the base-level although it alleviates the optimization problem.\nMetric-based approaches: A majority of the state-of-the-art algorithms are metric-based approaches.\nThese approaches learn an embedding that can be used to compare (Bromley et al., 1994; Chopra\net al., 2005) or cluster (Vinyals et al., 2016; Snell et al., 2017) query samples. Recent approaches build\nupon this idea with increasing levels of sophistication in learning the embedding (Vinyals et al., 2016;\nGidaris & Komodakis, 2018; Oreshkin et al., 2018), creating exemplars from the support set and\npicking a metric for the embedding (Gidaris & Komodakis, 2018; Allen et al., 2018; Ravichandran\net al., 2019). There are numerous hyper-parameters involved in implementing these approaches which\nmakes it hard to evaluate them systematically (Chen et al., 2018).\nTransductive learning: This approach is more efﬁcient at using few labeled data than supervised\nlearning (Joachims, 1999; Zhou et al., 2004; Vapnik, 2013). The idea is to use information from\nthe test datum x to restrict the hypothesis space while searching for the classiﬁer F(x, Ds) at test\ntime. Our approach is closest to this line of work. We train a model on the meta-training set Dm and\n3\n\n\nPublished as a conference paper at ICLR 2020\ninitialize a classiﬁer using the support set Ds. The parameters are then ﬁne-tuned to adapt to the new\ntest datum x.\nThere are recent papers in few-shot learning such as Nichol et al. (2018); Liu et al. (2018a) that\nare motivated from transductive learning and exploit the unlabeled query samples. The former\nupdates batch-normalization parameters using query samples while the latter uses label propagation\nto estimate labels of all query samples at once.\nSemi-supervised learning: We penalize the Shannon Entropy of the predictions on the query\nsamples at test time. This is a simple technique in the semi-supervised learning literature, closest to\nGrandvalet & Bengio (2005). Modern augmentation techniques such as Miyato et al. (2015); Sajjadi\net al. (2016); Dai et al. (2017) or graph-based approaches (Kipf & Welling, 2016) can also be used\nwith our approach; we used the entropic penalty for the sake of simplicity.\nSemi-supervised few-shot learning is typically formulated as having access to extra unlabeled data\nduring meta-training or few-shot training (Garcia & Bruna, 2017; Ren et al., 2018). This is different\nfrom our approach which uses the unlabeled query samples for transductive learning.\nInitialization for ﬁne-tuning: We use recent ideas from the deep metric learning literature (Hu et al.,\n2015; Movshovitz-Attias et al., 2017; Qi et al., 2018; Chen et al., 2018; Gidaris & Komodakis, 2018)\nto initialize the meta-trained model for ﬁne-tuning. These works connect the softmax cross-entropy\nloss with cosine distance and are discussed further in Section 3.1.\n3\nAPPROACH\nThe simplest form of meta-training is pre-training with the cross-entropy loss, which yields\nˆθ = arg min\nθ\n1\nNm\nX\n(x,y)∈Dm\n−log pθ(y|x) + R(θ),\n(5)\nwhere the second term denotes a regularizer, say weight decay R(θ) = ∥θ∥2/2. The model predicts\nlogits zk(x; θ) for k ∈Cm and the distribution pθ(·|x) is computed from these logits using the\nsoftmax operator. This loss is typically minimized by stochastic gradient descent-based algorithms.\nIf few-shot training is performed according to the general form in (4), then the optimization is\nidentical to that above and amounts to ﬁne-tuning the pre-trained model. However, the model needs\nto be modiﬁed to account for the new classes. Careful initialization can make this process efﬁcient.\n3.1\nSUPPORT-BASED INITIALIZATION\nGiven the pre-trained model (called the “backbone”), pθ (dropping the hat from ˆθ), we append a\nnew fully-connected “classiﬁer” layer that takes the logits of the backbone as input and predicts the\nlabels in Ct. For a support sample (x, y), denote the logits of the backbone by z(x; θ) ∈R|Cm|; the\nweights and biases of the classiﬁer by w ∈R|Ct|×|Cm| and b ∈R|Ct| respectively; and the kth row of\nw and b by wk and bk respectively. The ReLU non-linearity is denoted by (·)+.\nIf the classiﬁer’s logits are z′\n= wz(x; θ)+ + b, the ﬁrst term in the cross-entropy loss:\n−log pΘ(y|x) = −wyz(x; θ)+ −by + log P\nk ewkz(x;θ)++bk would be the cosine distance between\nwy and z(x; θ)+ if both were normalized to unit ℓ2 norm and bias by = 0. This suggests\nwy =\nz(x; θ)+\n∥z(x; θ)+∥\nand\nby = 0\n(6)\nas a candidate for initializing the classiﬁer, along with normalizing z(x; θ)+ to unit ℓ2 norm. It is\neasy to see that this maximizes the cosine similarity between features z(x; θ)+ and weights wy. For\nmultiple support samples per class, we take the Euclidean average of features z(x; θ)+ for each class\nin Ct, before ℓ2 normalization in (6). The logits of the classiﬁer are thus given by\nR|Ct| ∋z(x; Θ) = w z(x; θ)+\n∥z(x; θ)+∥+ b,\n(7)\n4\n\n\nPublished as a conference paper at ICLR 2020\nwhere Θ = {θ, w, b}, the combined parameters of the backbone and the classiﬁer. Note that we have\nadded a ReLU non-linearity between the backbone and the classiﬁer, before the ℓ2 normalization. All\nthe parameters Θ are trainable in the ﬁne-tuning phase.\nRemark 1 (Relation to weight imprinting). The support-based initialization is motivated from\nprevious papers (Hu et al., 2015; Movshovitz-Attias et al., 2017; Chen et al., 2018; Gidaris &\nKomodakis, 2018). In particular, Qi et al. (2018) use a similar technique, with minor differences, to\nexpand the size of the ﬁnal fully-connected layer (classiﬁer) for low-shot continual learning. The\nauthors call their technique “weight imprinting” because wk can be thought of as a template for class\nk. In our case, we are only interested in performing well on the few-shot classes.\nRemark 2 (Using logits of the backbone instead of features as input to the classiﬁer). A natural\nway to adapt the backbone to predict new classes is to re-initialize its ﬁnal fully-connected layer\n(classiﬁer). We instead append a new classiﬁer after the logits of the backbone. This is motivated from\nFrosst et al. (2019) who show that for a trained backbone, outputs of all layers are entangled, without\nclass-speciﬁc clusters; but the logits are peaked on the correct class, and are therefore well-clustered.\nThe logits are thus better inputs to the classiﬁer as compared to the features. We explore this choice\nvia an experiment in Appendix C.6.\n3.2\nTRANSDUCTIVE FINE-TUNING\nIn (4), we assumed that there is a single query sample. However, we can also process multiple query\nsamples together, and perform the minimization over all unknown query labels. We introduce a\nregularizer, similar to Grandvalet & Bengio (2005), as we seek outputs with a peaked posterior, or\nlow Shannon Entropy H. So the transductive ﬁne-tuning phase solves for\nΘ∗= arg min\nΘ\n1\nNs\nX\n(x,y)∈Ds\n−log pΘ (y | x) + 1\nNq\nX\n(x,y)∈Dq\nH(pΘ(· | x)).\n(8)\nNote that the data ﬁtting term uses the labeled support samples whereas the regularizer uses the\nunlabeled query samples. The two terms can be highly imbalanced (due to the varying range of values\nfor the two quantities, or due to the variance in their estimates which depend on Ns and Nq). To allow\nﬁner control on this imbalance, one can use a coefﬁcient for the entropic term and/or a temperature\nin the softmax distribution of the query samples. Tuning these hyper-parameters per dataset and\nfew-shot protocol leads to uniform improvements in the results in Section 4 by 1-2%. However, we\nwish to keep in line with our goal of developing a simple baseline and refrain from optimizing these\nhyper-parameters, and set them equal to 1 for all experiments on benchmark datasets.\n4\nEXPERIMENTAL RESULTS\nWe show results of transductive ﬁne-tuning on benchmark datasets in few-shot learning, namely\nMini-ImageNet (Vinyals et al., 2016), Tiered-ImageNet (Ren et al., 2018), CIFAR-FS (Bertinetto\net al., 2018) and FC-100 (Oreshkin et al., 2018), in Section 4.1. We also show large-scale experiments\non the ImageNet-21k dataset (Deng et al., 2009) in Section 4.2. Along with the analysis in Section 4.3,\nthese help us design a metric that measures the hardness of an episode in Section 4.4. We sketch key\npoints of the experimental setup here; see Appendix A for details.\nPre-training: We use the WRN-28-10 (Zagoruyko & Komodakis, 2016) model as the backbone. We\npre-train using standard data augmentation, cross-entropy loss with label smoothing (Szegedy et al.,\n2016) of ϵ=0.1, mixup regularization (Zhang et al., 2017) of α=0.25, SGD with batch-size of 256,\nNesterov’s momentum of 0.9, weight-decay of 10−4 and no dropout. We use batch-normalization\n(Ioffe & Szegedy, 2015) but exclude its parameters from weight decay (Jia et al., 2018). We use\ncyclic learning rates (Smith, 2017) and half-precision distributed training on 8 GPUs (Howard et al.,\n2018) to reduce training time.\nEach dataset has a training, validation and test set consisting of disjoint sets of classes. Some\nalgorithms use only the training set as the meta-training set (Snell et al., 2017; Oreshkin et al., 2018),\nwhile others use both training and validation sets (Rusu et al., 2018). For completeness we report\n5\n\n\nPublished as a conference paper at ICLR 2020\nresults using both methodologies; the former is denoted as (train) while the latter is denoted as (train\n+ val). All experiments in Sections 4.3 and 4.4 use the (train + val) setting.\nFine-tuning: We perform ﬁne-tuning on one GPU in full-precision for 25 epochs and a ﬁxed learning\nrate of 5 × 10−5 with Adam (Kingma & Ba, 2014) without any regularization. We make two weight\nupdates in each epoch: one for the cross-entropy term using support samples and one for the Shannon\nEntropy term using query samples (cf. (8)).\nHyper-parameters: We used images from ImageNet-1k belonging to the training classes of Mini-\nImageNet as the validation set for pre-training the backbone for Mini-ImageNet. We used the\nvalidation set of Mini-ImageNet to choose hyper-parameters for ﬁne-tuning. All hyper-parameters\nare kept constant for experiments on benchmark datasets.\nEvaluation: Few-shot episodes contain classes sampled uniformly from classes in the test sets of the\nrespective datasets; support and query samples are further sampled uniformly for each class; the query\nshot is ﬁxed to 15 for all experiments unless noted otherwise. All networks are evaluated over 1,000\nfew-shot episodes unless noted otherwise. To enable easy comparison with existing literature, we\nreport an estimate of the mean accuracy and the 95% conﬁdence interval of this estimate. However,\nwe encourage reporting the standard deviation in light of Section 1 and Fig. 1.\n4.1\nRESULTS ON BENCHMARK DATASETS\nTable 1: Few-shot accuracies on benchmark datasets for 5-way few-shot episodes. The notation conv\n(64k)×4 denotes a CNN with 4 layers and 64k channels in the kth layer. Best results in each column are shown\nin bold. Results where the support-based initialization is better than or comparable to existing algorithms\nare denoted by †. The notation (train + val) indicates that the backbone was pre-trained on both training and\nvalidation sets of the datasets; the backbone is trained only on the training set otherwise. (Lee et al., 2019) uses a\n1.25× wider ResNet-12 which we denote as ResNet-12 ∗.\nMini-ImageNet\nTiered-ImageNet\nCIFAR-FS\nFC-100\nAlgorithm\nArchitecture\n1-shot (%)\n5-shot (%)\n1-shot (%)\n5-shot (%)\n1-shot (%)\n5-shot (%)\n1-shot (%)\n5-shot (%)\nMatching networks (Vinyals et al., 2016)\nconv (64)×4\n46.6\n60\nLSTM meta-learner (Ravi & Larochelle,\n2016)\nconv (64)×4 43.44 ± 0.77 60.60 ± 0.71\nPrototypical Networks (Snell et al., 2017)\nconv (64)×4 49.42 ± 0.78 68.20 ± 0.66\nMAML (Finn et al., 2017)\nconv (32)×4 48.70 ± 1.84 63.11 ± 0.92\nR2D2 (Bertinetto et al., 2018)\nconv (96k)×4\n51.8 ± 0.2\n68.4 ± 0.2\n65.4 ± 0.2\n79.4 ± 0.2\nTADAM (Oreshkin et al., 2018)\nResNet-12\n58.5 ± 0.3\n76.7 ± 0.3\n40.1 ± 0.4\n56.1 ± 0.4\nTransductive\nPropagation\n(Liu\net\nal.,\n2018b)\nconv (64)×4 55.51 ± 0.86 69.86 ± 0.65\n59.91 ± 0.94\n73.30 ± 0.75\nTransductive\nPropagation\n(Liu\net\nal.,\n2018b)\nResNet-12\n59.46\n75.64\nMetaOpt SVM (Lee et al., 2019)\nResNet-12 ∗62.64 ± 0.61 78.63 ± 0.46\n65.99 ± 0.72\n81.56 ± 0.53\n72.0 ± 0.7\n84.2 ± 0.5\n41.1 ± 0.6\n55.5 ± 0.6\nSupport-based initialization (train)\nWRN-28-10 56.17 ± 0.64 73.31 ± 0.53 67.45 ± 0.70† 82.88 ± 0.53†\n70.26 ± 0.70 83.82 ± 0.49† 36.82 ± 0.51 49.72 ± 0.55\nFine-tuning (train)\nWRN-28-10 57.73 ± 0.62 78.17 ± 0.49\n66.58 ± 0.70\n85.55 ± 0.48\n68.72 ± 0.67\n86.11 ± 0.47 38.25 ± 0.52 57.19 ± 0.57\nTransductive ﬁne-tuning (train)\nWRN-28-10 65.73 ± 0.68 78.40 ± 0.52\n73.34 ± 0.71\n85.50 ± 0.50\n76.58 ± 0.68\n85.79 ± 0.50 43.16 ± 0.59 57.57 ± 0.55\nActivation to Parameter (Qiao et al., 2018)\n(train + val)\nWRN-28-10 59.60 ± 0.41 73.74 ± 0.19\nLEO (Rusu et al., 2018) (train + val)\nWRN-28-10 61.76 ± 0.08 77.59 ± 0.12\n66.33 ± 0.05\n81.44 ± 0.09\nMetaOpt SVM (Lee et al., 2019) (train +\nval)\nResNet-12 ∗64.09 ± 0.62 80.00 ± 0.45\n65.81 ± 0.74\n81.75 ± 0.53\n72.8 ± 0.7\n85.0 ± 0.5\n47.2 ± 0.6\n62.5 ± 0.6\nSupport-based initialization (train + val)\nWRN-28-10 58.47 ± 0.66 75.56 ± 0.52 67.34 ± 0.69† 83.32 ± 0.51† 72.14 ± 0.69† 85.21 ± 0.49† 45.08 ± 0.61 60.05 ± 0.60\nFine-tuning (train + val)\nWRN-28-10 59.62 ± 0.66 79.93 ± 0.47\n66.23 ± 0.68\n86.08 ± 0.47\n70.07 ± 0.67\n87.26 ± 0.45 43.80 ± 0.58 64.40 ± 0.58\nTransductive ﬁne-tuning (train + val)\nWRN-28-10 68.11 ± 0.69 80.36 ± 0.50\n72.87 ± 0.71\n86.15 ± 0.50\n78.36 ± 0.70\n87.54 ± 0.49 50.44 ± 0.68 65.74 ± 0.60\nTable 1 shows the results of transductive ﬁne-tuning on benchmark datasets for standard few-shot\nprotocols. We see that this simple baseline is uniformly better than state-of-the-art algorithms. We\ninclude results for support-based initialization, which does no ﬁne-tuning; and for ﬁne-tuning, which\ninvolves optimizing only the cross-entropy term in (8) using the labeled support samples.\nThe support-based initialization is sometimes better than or comparable to state-of-the-art\nalgorithms (marked †). The few-shot literature has gravitated towards larger backbones (Rusu et al.,\n2018). Our results indicate that for large backbones even standard cross-entropy pre-training and\nsupport-based initialization work well, similar to observation made by Chen et al. (2018).\n6\n\n\nPublished as a conference paper at ICLR 2020\nFor the 1-shot 5-way setting, ﬁne-tuning using only the labeled support examples leads to minor\nimprovement over the initialization, and sometimes marginal degradation. However, for the 5-shot\n5-way setting non-transductive ﬁne-tuning is better than the state-of-the-art.\nIn both (train) and (train + val) settings, transductive ﬁne-tuning leads to 2-7% improvement for\n1-shot 5-way setting over the state-of-the-art for all datasets. It results in an increase of 1.5-4% for\nthe 5-shot 5-way setting except for the Mini-ImageNet dataset, where the performance is matched.\nThis suggests that the use of the unlabeled query samples is vital for the few-shot setting.\nFor the Mini-ImageNet, CIFAR-FS and FC-100 datasets, using additional data from the valida-\ntion set to pre-train the backbone results in 2-8% improvements; the improvement is smaller for\nTiered-ImageNet. This suggests that having more pre-training classes leads to improved few-shot\nperformance as a consequence of a better embedding. See Appendix C.5 for more experiments.\n4.2\nLARGE-SCALE FEW-SHOT LEARNING\nThe ImageNet-21k dataset (Deng et al., 2009) with 14.2M images across 21,814 classes is an ideal\nlarge-scale few-shot learning benchmark due to the high class imbalance. The simplicity of our\napproach allows us to present the ﬁrst few-shot learning results on this large dataset. We use the 7,491\nclasses having more than 1,000 images each as the meta-training set and the next 13,007 classes with\nat least 10 images each for constructing few-shot episodes. See Appendix B for details.\nTable 2: Accuracy (%) on the few-shot data of ImageNet-21k. The conﬁdence intervals are large because we\ncompute statistics only over 80 few-shot episodes so as to test for large number of ways.\nWay\nAlgorithm\nModel\nShot\n5\n10\n20\n40\n80\n160\nSupport-based initialization\nWRN-28-10\n1\n87.20 ± 1.72\n78.71 ± 1.63\n69.48 ± 1.30\n60.55 ± 1.03\n49.15 ± 0.68\n40.57 ± 0.42\nTransductive ﬁne-tuning\nWRN-28-10\n1\n89.00 ± 1.86\n79.88 ± 1.70\n69.66 ± 1.30\n60.72 ± 1.04\n48.88 ± 0.66\n40.46 ± 0.44\nSupport-based initialization\nWRN-28-10\n5\n95.73 ± 0.84\n91.00 ± 1.09\n84.77 ± 1.04\n78.10 ± 0.79\n70.09 ± 0.71\n61.93 ± 0.45\nTransductive ﬁne-tuning\nWRN-28-10\n5\n95.20 ± 0.94\n90.61 ± 1.03\n84.21 ± 1.09\n77.13 ± 0.82\n68.94 ± 0.75\n60.11 ± 0.48\nTable 2 shows the mean accuracy of transductive ﬁne-tuning evaluated over 80 few-shot episodes on\nImageNet-21k. The accuracy is extremely high as compared to corresponding results in Table 1 even\nfor large way. E.g., the 1-shot 5-way accuracy on Tiered-ImageNet is 72.87 ± 0.71% while it is 89 ±\n1.86% here. This corroborates the results in Section 4.1 and indicates that pre-training with a large\nnumber of classes may be an effective strategy to build large-scale few-shot learning systems.\nThe improvements of transductive ﬁne-tuning are minor for ImageNet-21k because the support-based\ninitialization accuracies are extremely high. We noticed a slight degradation of accuracies due to\ntransductive ﬁne-tuning at high ways because the entropic term in (8) is much larger than the the\ncross-entropy loss. The experiments for ImageNet-21k therefore scale down the entropic term by\nlog |Ct| and forego the ReLU in (6) and (7). This reduces the difference in accuracies at high ways.\n4.3\nANALYSIS\nThis section presents a comprehensive analysis of transductive ﬁne-tuning on the Mini-ImageNet,\nTiered-ImageNet and ImageNet-21k datasets.\nRobustness of transductive ﬁne-tuning to query shot: Fig. 2a shows the effect of changing the\nquery shot on the mean accuracy. For the 1-shot 5-way setting, the entropic penalty in (8) helps as the\nquery shot increases. This effect is minor in the 5-shot 5-way setting as more labeled data is available.\nQuery shot of 1 achieves a relatively high mean accuracy because transductive ﬁne-tuning can adapt\nto those few queries. One query shot is enough to beneﬁt from transductive ﬁne-tuning: for\nMini-ImageNet, the 1-shot 5-way accuracy with query shot of 1 is 66.94 ± 1.55% which is better\nthan non-transductive ﬁne-tuning (59.62 ± 0.66% in Table 1) and higher than other approaches.\nPerformance for different way and support shot: A few-shot system should be able to robustly\nhandle different few-shot scenarios. Figs. 2b and 2c, show the performance of transductive ﬁne-tuning\n7\n\n\nPublished as a conference paper at ICLR 2020\n1\n5\n10\n15\n20\nQuery shot\n65\n75\n85\n95\nMean accuracy (%)\n1 shot 5-way Mini-Imagenet\n5 shot         \" \n1 shot 5-way Tiered-Imagenet\n5 shot         \" \n(a)\n10\n1\n10\n2\nWay\n0\n20\n40\n60\n80\n100\nMean accuracy (%)\n 1 shot   Tiered-ImageNet\n 5 shot       \" \n10 shot       \" \n 1 shot   ImageNet-21k\n 5 shot       \" \n10 shot       \" \n(b)\n10\n0\n10\n1\nSupport Shot\n20\n40\n60\n80\n100\nMean accuracy (%)\n 5 way Tiered-Imagenet\n20 way       \"\n80 way       \"\n160 way       \"\n(c)\nFigure 2: Mean accuracy of transductive ﬁne-tuning for different query shot, way and support shot.\nFig. 2a shows that the mean accuracy improves with query shot if the support shot is low; this effect is minor for\nTiered-ImageNet. The mean accuracy for query shot of 1 is high because transductive ﬁne-tuning can specialize\nto those queries. Fig. 2b shows that the mean accuracy degrades logarithmically with way for ﬁxed support shot\nand query shot (15). Fig. 2c suggests that the mean accuracy improves logarithmically with the support shot for\nﬁxed way and query shot (15). These trends suggest thumb rules for building few-shot systems.\nwith changing way and support shot. The mean accuracy changes logarithmically with the way\nand support shot which provides thumb rules for building few-shot systems.\nDifferent backbone architectures: We include experiments using conv (64)×4 (Vinyals et al., 2016)\nand ResNet-12 (He et al., 2016a; Oreshkin et al., 2018) in Table 3, in order to facilitate comparisons\nfor different backbone architectures. The results for transductive ﬁne-tuning are comparable or better\nthan state-of-the-art for a given backbone architecture, except for those in Liu et al. (2018b) who\nuse a more sophisticated transductive algorithm using graph propagation, with conv (64)×4. In line\nwith our goal for simplicity, we kept the hyper-parameters for pre-training and ﬁne-tuning the\nsame as the ones used for WRN-28-10 (cf. Sections 3 and 4). These results show that transductive\nﬁne-tuning is a sound baseline for a variety of backbone architectures.\nComputational complexity: There is no free lunch and our advocated baseline has its limitations.\nIt performs gradient updates during the ﬁne-tuning phase which makes it slow at inference time.\nSpeciﬁcally, transductive ﬁne-tuning is about 300× slower (20.8 vs. 0.07 seconds) for a 1-shot\n5-way episode with 15 query shot as compared to Snell et al. (2017) with the same backbone\narchitecture (prototypical networks (Snell et al., 2017) do not update model parameters at inference\ntime). The latency factor reduces with higher support shot. Interestingly, for a single query shot,\nthe former takes 4 seconds vs. 0.07 seconds. This is a more reasonable factor of 50×, especially\nconsidering that the mean accuracy of the former is 66.2% compared to about 58% of the latter in our\nimplementation. Experiments in Appendix C.3 suggest that using a smaller backbone architecture\npartially compensates for the latency with some degradation of accuracy. A number of approaches\nsuch as Ravi & Larochelle (2016); Finn et al. (2017); Rusu et al. (2018); Lee et al. (2019) also perform\nadditional processing at inference time and are expected to be slow, along with other transductive\napproaches (Nichol et al., 2018; Liu et al., 2018b). Additionally, support-based initialization has the\nsame inference time as Snell et al. (2017).\n4.4\nA PROPOSAL FOR REPORTING FEW-SHOT CLASSIFICATION PERFORMANCE\nAs discussed in Section 1, we need better metrics to report the performance of few-shot algorithms.\nThere are two main issues: (i) standard deviation of the few-shot accuracy across different sampled\nepisodes for a given algorithm, dataset and few-shot protocol is very high (cf. Fig. 1), and (ii)\ndifferent models and hyper-parameters for different few-shot protocols makes evaluating algorithmic\ncontributions difﬁcult (cf. Table 1). This section takes a step towards resolving these issues.\nHardness of an episode: Classiﬁcation performance on a few-shot episode is determined by the\nrelative location of the features corresponding to labeled and unlabeled samples. If the unlabeled\n8\n\n\nPublished as a conference paper at ICLR 2020\nfeatures are close to the labeled features from the same class, the classiﬁer can distinguish between\nthe classes easily to obtain a high accuracy. Otherwise, the accuracy would be low. The following\ndeﬁnition characterizes this intuition.\nFor training (support) set Ds and test (query) set Dq, we will deﬁne the hardness Ωϕ as the average\nlog-odds of a test datum being classiﬁed incorrectly. More precisely,\nΩϕ(Dq; Ds) = 1\nNq\nX\n(x,y)∈Dq\nlog 1 −p(y | x)\np(y | x)\n,\n(9)\nwhere p(·| x) is a softmax distribution with logits zy = wϕ(x). w is the weight matrix constructed\nusing (6) and Ds; and ϕ is the ℓ2 normalized logits computed using a rich-enough feature generator,\nsay a deep network trained for standard image classiﬁcation. This is a clustering loss where the\nlabeled support samples form class-speciﬁc cluster centers. The cluster afﬁnities are calculated using\ncosine-similarities, followed by the softmax operator to get the probability distribution p(·| x).\nNote that Ωϕ does not depend on the few-shot learner and gives a measure of how difﬁcult the\nclassiﬁcation problem is for any few-shot episode, using a generic feature extractor.\n1\n2\n3\n4\n5\nHardness\n20\n40\n60\n80\n100\nAccuracy (%)\nCIFAR-FS\nFC-100\nTiered-Imagenet\nMini-Imagenet\nImagenet-21k\nFigure 3: Comparing the accuracy of transductive ﬁne-tuning (solid lines) vs. support-based initialization\n(dotted lines) for different datasets, ways (5, 10, 20, 40, 80 and 160) and support shots (1 and 5). Abscissae\nare computed using (9) and a Resnet-152 (He et al., 2016b) network trained for standard image classiﬁcation on\nthe ImageNet-1k dataset. Each marker indicates the accuracy of transductive ﬁne-tuning on a few-shot episode;\nmarkers for support-based initialization are hidden to avoid clutter. Shape of the markers denotes different\nways; ways increase from left to right (5, 10, 20, 40, 80 and 160). Size of the markers denotes different support\nshot (1 and 5); it increases from the bottom to the top. E.g., the ellipse contains accuracies of different 5-shot\n10-way episodes for ImageNet-21k. Regression lines are drawn for each algorithm and dataset by combining\nthe episodes of all few-shot protocols. This plot is akin to a precision-recall curve and allows comparing two\nalgorithms for different few-shot scenarios. The areas in the ﬁrst quadrant under the ﬁtted regression lines are\n295 vs. 284 (CIFAR-FS), 167 vs. 149 (FC-100), 208 vs. 194 (Mini-ImageNet), 280 vs. 270 (Tiered-ImageNet)\nand 475 vs. 484 (ImageNet-21k) for transductive ﬁne-tuning and support-based initialization.\n9\n\n\nPublished as a conference paper at ICLR 2020\nFig. 3 demonstrates how to use the hardness metric. Few-shot accuracy degrades linearly with\nhardness. Performance for all hardness can thus be estimated by testing for two different ways. We\nadvocate selecting hyper-parameters using the area under the ﬁtted curve as a metric instead\nof tuning them speciﬁcally for each few-shot protocol. The advantage of such a test methodology is\nthat it predicts the performance of the model across multiple few-shot protocols systematically.\nDifferent algorithms can be compared directly, e.g., transductive ﬁne-tuning (solid lines) and\nsupport-based initialization (dotted lines). For instance, the former leads to large improvements on\neasy episodes, the performance is similar for hard episodes, especially for Tiered-ImageNet and\nImageNet-21k.\nThe high standard deviation of accuracy of few-shot learning algorithms in Fig. 1 can be seen as\nthe spread of the cluster corresponding to each few-shot protocol, e.g., the ellipse in Fig. 3 denotes\nthe 5-shot 10-way protocol for ImageNet-21k. It is the nature of few-shot learning that episodes have\nvarying hardness even if the way and shot are ﬁxed. However, episodes within the ellipse lie on a\ndifferent line (with a large negative slope) which indicates that given a few-shot protocol, hardness is\na good indicator of accuracy.\nFig. 3 also shows that due to fewer test classes, CIFAR-FS, FC-100 and Mini-ImageNet have less\ndiversity in the hardness of episodes while Tiered-ImageNet and ImageNet-21k allow sampling of\nboth very hard and very easy diverse episodes. For a given few-shot protocol, the hardness of episodes\nin the former three is almost the same as that of the latter two datasets. This indicates that CIFAR-FS,\nFC-100 and Mini-ImageNet may be good benchmarks for applications with few classes.\nThe hardness metric in (9) naturally builds upon existing ideas in deep metric learning (Qi et al.,\n2018). We propose it as a means to evaluate few-shot learning algorithms uniformly across different\nfew-shot protocols for different datasets; ascertaining its efﬁcacy and comparisons to other metrics\nwill be part of future work.\n5\nDISCUSSION\nOur aim is to provide grounding to the practice of few-shot learning. The current literature is in the\nspirit of increasingly sophisticated approaches for modest improvements in mean accuracy using\nan inadequate evaluation methodology. This is why we set out to establish a baseline, namely\ntransductive ﬁne-tuning, and a systematic evaluation methodology, namely the hardness metric. We\nwould like to emphasize that our advocated baseline, namely transductive ﬁne-tuning, is not novel and\nyet performs better than existing algorithms on all standard benchmarks. This is indeed surprising\nand indicates that we need to take a step back and re-evaluate the status quo in few-shot learning. We\nhope to use the results in this paper as guidelines for the development of new algorithms.\nREFERENCES\nKelsey R Allen, Hanul Shin, Evan Shelhamer, and Josh B Tenenbaum. Variadic learning by bayesian nonpara-\nmetric deep embedding. 2018.\nJonathan Baxter. Learning internal representations. Flinders University of S. Aust., 1995.\nSamy Bengio, Yoshua Bengio, Jocelyn Cloutier, and Jan Gecsei. On the optimization of a synaptic learning rule.\nIn Preprints Conf. Optimality in Artiﬁcial and Biological Neural Networks, pp. 6–8. Univ. of Texas, 1992.\nLuca Bertinetto, Jo˜ao F Henriques, Philip HS Torr, and Andrea Vedaldi. Meta-learning with differentiable\nclosed-form solvers. arXiv:1805.08136, 2018.\nJane Bromley, Isabelle Guyon, Yann LeCun, Eduard S¨ackinger, and Roopak Shah. Signature veriﬁcation using\na” siamese” time delay neural network. In Advances in neural information processing systems, pp. 737–744,\n1994.\nWei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look at few-shot\nclassiﬁcation. 2018.\nSumit Chopra, Raia Hadsell, Yann LeCun, et al. Learning a similarity metric discriminatively, with application\nto face veriﬁcation. In CVPR (1), pp. 539–546, 2005.\n10\n\n\nPublished as a conference paper at ICLR 2020\nZihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Ruslan R Salakhutdinov. Good semi-supervised\nlearning that requires a bad gan. In Advances in neural information processing systems, pp. 6510–6520, 2017.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee,\n2009.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep\nnetworks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1126–\n1135. JMLR. org, 2017.\nNicholas Frosst, Nicolas Papernot, and Geoffrey Hinton. Analyzing and improving representations with the soft\nnearest neighbor loss. arXiv:1902.01889, 2019.\nVictor Garcia and Joan Bruna. Few-shot learning with graph neural networks. arXiv:1711.04043, 2017.\nSpyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, pp. 4367–4375, 2018.\nYves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In Advances in neural\ninformation processing systems, pp. 529–536, 2005.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\nThe IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016a.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks.\narXiv:1603.05027, 2016b.\nSepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent. In\nInternational Conference on Artiﬁcial Neural Networks, pp. 87–94. Springer, 2001.\nJeremy Howard et al. fastai. https://github.com/fastai/fastai, 2018.\nJunlin Hu, Jiwen Lu, and Yap-Peng Tan. Deep transfer metric learning. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pp. 325–333, 2015.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. arXiv:1502.03167, 2015.\nXianyan Jia, Shutao Song, Wei He, Yangzihao Wang, Haidong Rong, Feihu Zhou, Liqiang Xie, Zhenyu Guo,\nYuanzhou Yang, Liwei Yu, et al. Highly scalable deep learning training system with mixed-precision: Training\nimagenet in four minutes. arXiv:1807.11205, 2018.\nThorsten Joachims. Transductive inference for text classiﬁcation using support vector machines. In Icml,\nvolume 99, pp. 200–209, 1999.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014.\nThomas N Kipf and Max Welling.\nSemi-supervised classiﬁcation with graph convolutional networks.\narXiv:1609.02907, 2016.\nAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report,\nCiteseer, 2009.\nKwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with differentiable\nconvex optimization. arXiv:1904.03758, 2019.\nYanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sung Ju Hwang, and Yi Yang. Learning to\npropagate labels: Transductive propagation network for few-shot learning. 2018a.\nYanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, and Yi Yang. Transductive propagation network for few-shot\nlearning. arXiv:1805.10002, 2018b.\nIlya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv:1608.03983,\n2016.\nLaurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of machine learning\nresearch, 9(Nov):2579–2605, 2008.\nDougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization through\nreversible learning. In International Conference on Machine Learning, pp. 2113–2122, 2015.\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg,\nMichael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv:1710.03740,\n2017.\n11\n\n\nPublished as a conference paper at ICLR 2020\nTakeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, and Shin Ishii. Distributional smoothing with\nvirtual adversarial training. arXiv:1507.00677, 2015.\nYair Movshovitz-Attias, Alexander Toshev, Thomas K Leung, Sergey Ioffe, and Saurabh Singh. No fuss distance\nmetric learning using proxies. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n360–368, 2017.\nAlex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-order meta-learning algorithms. arXiv:1803.02999,\n2018.\nBoris Oreshkin, Pau Rodr´ıguez L´opez, and Alexandre Lacoste. Tadam: Task dependent adaptive metric for\nimproved few-shot learning. In Advances in Neural Information Processing Systems, pp. 719–729, 2018.\nHang Qi, Matthew Brown, and David G Lowe. Low-shot learning with imprinted weights. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, pp. 5822–5830, 2018.\nSiyuan Qiao, Chenxi Liu, Wei Shen, and Alan L Yuille. Few-shot image recognition by predicting parameters\nfrom activations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n7229–7238, 2018.\nSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. 2016.\nAvinash Ravichandran, Rahul Bhotika, and Stefano Soatto. Few-shot learning with embedded class models and\nshot-free meta training, 2019.\nMengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum,\nHugo Larochelle, and Richard S Zemel.\nMeta-learning for semi-supervised few-shot classiﬁcation.\narXiv:1803.00676, 2018.\nAndrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia\nHadsell. Meta-learning with latent embedding optimization. arXiv:1807.05960, 2018.\nMehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transformations and\nperturbations for deep semi-supervised learning. In Advances in Neural Information Processing Systems, pp.\n1163–1171, 2016.\nJurgen Schmidhuber. Evolutionary principles in self-referential learning. On learning how to learn: The\nmeta-meta-... hook.) Diploma thesis, Institut f. Informatik, Tech. Univ. Munich, 1987.\nLeslie N Smith. Cyclical learning rates for training neural networks. In 2017 IEEE Winter Conference on\nApplications of Computer Vision (WACV), pp. 464–472. IEEE, 2017.\nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Advances in\nNeural Information Processing Systems, pp. 4077–4087, 2017.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception\narchitecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 2818–2826, 2016.\nSebastian Thrun. Lifelong learning algorithms. In Learning to learn, pp. 181–209. Springer, 1998.\nEleni Triantaﬁllou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Kelvin Xu, Ross Goroshin, Carles Gelada,\nKevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle. Meta-dataset: A dataset of datasets for\nlearning to learn from few examples. arXiv preprint arXiv:1903.03096, 2019.\nPaul E Utgoff. Shift of bias for inductive concept learning. Machine learning: An artiﬁcial intelligence approach,\n2:107–148, 1986.\nVladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 2013.\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot\nlearning. In Advances in neural information processing systems, pp. 3630–3638, 2016.\nJunyuan Xie, Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, and Mu Li. Bag of tricks for image\nclassiﬁcation with convolutional neural networks. arXiv:1812.01187, 2018.\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv:1605.07146, 2016.\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk\nminimization. arXiv:1710.09412, 2017.\nDengyong Zhou, Olivier Bousquet, Thomas N Lal, Jason Weston, and Bernhard Sch¨olkopf. Learning with local\nand global consistency. In Advances in neural information processing systems, pp. 321–328, 2004.\n12\n\n\nPublished as a conference paper at ICLR 2020\nA\nSETUP\nA.1\nDATASETS\nWe use the following datasets for our benchmarking experiments.\n• The Mini-ImageNet dataset (Vinyals et al., 2016) which is a subset of ImageNet-1k (Deng\net al., 2009) and consists of 84 × 84 sized images with 600 images per class. There are 64\ntraining, 16 validation and 20 test classes. There are multiple versions of this dataset in the\nliterature; we obtained the dataset from the authors of Gidaris & Komodakis (2018)3.\n• The Tiered-ImageNet dataset (Ren et al., 2018) is a larger subset of ImageNet-1k with 608\nclasses split as 351 training, 97 validation and 160 testing classes, each with about 1300\nimages of size 84 × 84. This dataset ensures that training, validation and test classes do not\nhave a semantic overlap and is a potentially harder few-shot learning dataset.\n• We also consider two smaller CIFAR-100 (Krizhevsky & Hinton, 2009) derivatives, both\nwith 32 × 32 sized images and 600 images per class. The ﬁrst is the CIFAR-FS dataset\n(Bertinetto et al., 2018) which splits classes randomly into 64 training, 16 validation and 20\ntest. The second is the FC-100 dataset (Oreshkin et al., 2018) which splits CIFAR-100 into\n60 training, 20 validation and 20 test classes with minimal semantic overlap.\nEach dataset has a training, validation and test set. The set of classes for each of these sets are disjoint\nfrom each other. For meta-training, we ran two sets of experiments: the ﬁrst, where we only use\nthe training set as the meta-training dataset, denoted by (train); the second, where we use both the\ntraining and validation sets as the meta-training dataset, denoted by (train + val). We use the test set\nto construct few-shot episodes.\nA.2\nPRE-TRAINING\nWe use a wide residual network (Zagoruyko & Komodakis, 2016; Qiao et al., 2018; Rusu et al.,\n2018) with a widening factor of 10 and a depth of 28 which we denote as WRN-28-10. The smaller\nnetworks: conv (64)×4 (Vinyals et al., 2016; Snell et al., 2017), ResNet-12 (He et al., 2016a; Oreshkin\net al., 2018; Lee et al., 2019) and WRN-16-4 (Zagoruyko & Komodakis, 2016), are used for analysis\nin Appendix C. All networks are trained using SGD with a batch-size of 256, Nesterov’s momentum\nset to 0.9, no dropout, weight decay of 10−4. We use batch-normalization (Ioffe & Szegedy, 2015).\nWe use two-cycles of learning rate annealing (Smith, 2017), these are 40 and 80 epochs each for all\ndatasets except ImageNet-21k, which uses cycles of 8 and 16 epochs each. The learning rate is set to\n10−i at the beginning of the ith cycle and decreased to 10−6 by the end of that cycle with a cosine\nschedule (Loshchilov & Hutter, 2016). We use data parallelism across 8 Nvidia V100 GPUs and\nhalf-precision training using techniques from Micikevicius et al. (2017); Howard et al. (2018).\nWe use the following regularization techniques that have been discovered in the non-few-shot,\nstandard image classiﬁcation literature (Xie et al., 2018) for pre-training the backbone.\n• Mixup (Zhang et al., 2017): This augments data by a linear interpolation between input\nimages and their one-hot labels. If (x1, y1), (x2, y2) ∈D are two samples, mixup creates a\nnew sample (˜x, ˜y) where ˜x = λx1 +(1−λ)x2 and its label ˜y = λey1 +(1−λ)ey2; here ek\nis the one-hot vector with a non-zero kth entry and λ ∈[0, 1] is sampled from Beta(α, α)\nfor a hyper-parameter α.\n• Label smoothing (Szegedy et al., 2016): When using a softmax operator, the logits can\nincrease or decrease in an unbounded manner causing numerical instabilities while training.\nLabel smoothing sets pθ(k|x) = 1 −ϵ if k = y and ϵ/(K −1) otherwise, for a small\nconstant ϵ > 0 and number of classes K. The ratio between the largest and smallest output\nneuron is thus ﬁxed which helps large-scale training.\n• We exclude the batch-normalization parameters from weight-decay (Jia et al., 2018).\n3https://github.com/gidariss/FewShotWithoutForgetting\n13\n\n\nPublished as a conference paper at ICLR 2020\nWe set ϵ=0.1 for label smoothing cross-entroy loss and α=0.25 for mixup regularization for all our\nexperiments.\nA.3\nFINE-TUNING HYPER-PARAMETERS\nWe used 1-shot 5-way episodes on the validation set of Mini-ImageNet to manually tune hyper-\nparameters. Fine-tuning is done for 25 epochs with a ﬁxed learning rate of 5 × 10−5 with Adam\n(Kingma & Ba, 2014). Adam is used here as it is more robust to large changes in the magnitude\nof the loss and gradients which occurs if the number of classes in the few-shot episode (ways) is\nlarge. We do not use any regularization (weight-decay, mixup, dropout, or label smoothing) in the\nﬁne-tuning phase. These hyper-parameters are kept constant on all benchmark datasets, namely\nMini-ImageNet, Tiered-ImageNet, CIFAR-FS and FC-100.\nAll ﬁne-tuning and evaluation is performed on a single GPU in full-precision. We update the\nparameters sequentially by computing the gradient of the two terms in (8) independently. This\nupdates both the weights of the model and the batch-normalization parameters.\nA.4\nDATA AUGMENTATION\nInput images are normalized using the mean and standard-deviation computed on ImageNet-1k. Our\nData augmentation consists of left-right ﬂips with probability of 0.5, padding the image with 4px\nand adding brightness and contrast changes of ± 40%. The augmentation is kept the same for both\npre-training and ﬁne-tuning.\nWe explored augmentation using afﬁne transforms of the images but found that adding this has minor\neffect with no particular trend on the numerical results.\nA.5\nEVALUATION PROCEDURE\nThe few-shot episode contains classes that are uniformly sampled from the test classes of correspond-\ning datasets. Support and query samples are further uniformly sampled for each class. The query\nshot is ﬁxed to 15 for all experiments unless noted otherwise. We evaluate all networks over 1,000\nepisodes unless noted otherwise. For ease of comparison, we report the mean accuracy and the 95%\nconﬁdence interval of the estimate of the mean accuracy.\nB\nSETUP FOR IMAGENET-21K\nThe ImageNet-21k dataset (Deng et al., 2009) has 14.2M images across 21,814 classes. The blue\nregion in Fig. 4 denotes our meta-training set with 7,491 classes, each with more than 1,000 images.\nThe green region shows 13,007 classes with at least 10 images each, the set used to construct few-shot\nepisodes. We do not use the red region consisting of 1,343 classes with less than 10 images each.\nWe train the same backbone (WRN-28-10) with the same procedure as that in Appendix A on 84 ×\n84 resized images, albeit for only 24 epochs. Since we use the same hyper-parameters as the other\nbenchmark datasets, we did not create validation sets for pre-training or the ﬁne-tuning phases. The\nfew-shot episodes are constructed in the same way as Appendix A. We evaluate using fewer few-shot\nepisodes (80) on this dataset because we would like to demonstrate the performance across a large\nnumber of different ways.\nC\nADDITIONAL ANALYSIS\nThis section contains additional experiments and analysis, complementing Section 4.3. All ex-\nperiments use the (train + val) setting, pre-training on both the training and validation sets of the\ncorresponding datasets, unless noted otherwise.\n14\n\n\nPublished as a conference paper at ICLR 2020\n0\n5000\n10000\n15000\n20000\nclasses\n10\n0\n10\n1\n10\n2\n10\n3\nimages per class\nFigure 4: ImageNet-21k is a highly imbalanced dataset. The most frequent class has about 3K images while\nthe rarest class has a single image.\nFigure 5: t-SNE (Maaten & Hinton, 2008) embedding of the logits for 1-shot 5-way few-shot episode of\nMini-ImageNet. Colors denote the ground-truth labels; crosses denote the support samples; circles denote the\nquery samples; translucent markers and opaque markers denote the embeddings before and after transductive\nﬁne-tuning respectively. Even though query samples are far away from their respective supports in the beginning,\nthey move towards the supports by the end of transductive ﬁne-tuning. Logits of support samples are relatively\nunchanged which suggests that the support-based initialization is effective.\nC.1\nTRANSDUCTIVE FINE-TUNING CHANGES THE EMBEDDING DRAMATICALLY\nFig. 5 demonstrates this effect. The logits for query samples are far from those of their respective\nsupport samples and metric-based loss functions, e.g., those for prototypical networks (Snell et al.,\n2017) would have a high loss on this episode; indeed the accuracy after the support-based initialization\nis 64%. Logits for the query samples change dramatically during transductive ﬁne-tuning and majority\nof the query samples cluster around their respective supports. The post transductive ﬁne-tuning\naccuracy of this episode is 73.3%. This suggests that modifying the embedding using the query\nsamples is crucial to obtaining good performance on new classes. This example also demonstrates\nthat the support-based initialization is efﬁcient, logits of the support samples are relatively unchanged\nduring the transductive ﬁne-tuning phase.\n15\n\n\nPublished as a conference paper at ICLR 2020\nC.2\nLARGE VS. SMALL BACKBONES\nThe expressive power of the backbone plays an important role in the efﬁcacy of ﬁne-tuning. We\nobserved that a WRN-16-4 architecture (2.7M parameters) performs worse than WRN-28-10 (36M\nparameters). The former obtains 63.28 ± 0.68% and 77.39 ± 0.5% accuracy on Mini-ImageNet and\n69.04 ± 0.69% and 83.55 ± 0.51% accuracy on Tiered-ImageNet on 1-shot 5-way and 5-shot 5-way\nprotocols respectively. While these numbers are comparable to those of state-of-the-art algorithms,\nthey are lower than their counterparts for WRN-28-10 in Table 1. This suggests that a larger network\nis effective in learning richer features from the meta-training classes, and ﬁne-tuning is effective in\ntaking advantage of this to further improve performance on samples belonging to few-shot classes.\nC.3\nLATENCY WITH A SMALLER BACKBONES\nThe WRN-16-4 architecture (2.7M parameters) is much smaller than WRN-28-10 (36M parameters)\nand transductive ﬁne-tuning on the former is much faster. As compared to our implementation of\nSnell et al. (2017) with the same backbone, WRN-16-4 is 20-70× slower (0.87 vs. 0.04 seconds for\na query shot of 1, and 2.85 vs. 0.04 seconds for a query shot of 15) for the 1-shot 5-way scenario.\nCompare this to the computational complexity experiment in Section 4.3.\nAs discussed in Appendix C.2, the accuracy of WRN-16-4 is 63.28 ± 0.68% and 77.39 ± 0.5%\nfor 1-shot 5-way and 5-shot 5-way on Mini-ImageNet respectively. As compared to this, our\nimplementation of (Snell et al., 2017) using a WRN-16-4 backbone obtains 57.29 ± 0.40% and 75.34\n± 0.32% accuracies for the same settings respectively; the former number in particular is signiﬁcantly\nworse than its transductive ﬁne-tuning counterpart.\nC.4\nCOMPARISONS AGAINST BACKBONES IN THE CURRENT LITERATURE\nWe include experiments using conv (64)×4 and ResNet-12 in Table 3, in addition to WRN-28-10\nin Section 4, in order to facilitate comparisons of the proposed baseline for different backbone\narchitectures. Our results are comparable or better than existing results for a given backbone\narchitecture, except for those in Liu et al. (2018b) who use a graph-based transduction algorithm, for\nconv (64)×4 on Mini-ImageNet. In line with our goal for simplicity, we kept the hyper-parameters\nfor pre-training and ﬁne-tuning the same as the ones used for WRN-28-10 (cf. Sections 3 and 4).\nThese results suggest that transductive ﬁne-tuning is a sound baseline for a variety of backbone\narchitectures.\nC.5\nUSING MORE META-TRAINING CLASSES\nIn Section 4.1 we observed that having more pre-training classes improves few-shot performance.\nBut since we append a classiﬁer on top of a pre-trained backbone and use the logits of the backbone\nas inputs to the classiﬁer, a backbone pre-trained on more classes would also have more parameters\nas compared to one pre-trained on fewer classes. However, this difference is not large: WRN-28-10\nfor Mini-ImageNet has 0.03% more parameters for (train + val) as compared to (train). However,\nin order to facilitate a fair comparison, we ran an experiment where we use the features of the\nbackbone, instead of the logits, as inputs to the classiﬁer. By doing so, the number of parameters\nin the pre-trained backbone that are used for few-shot classiﬁcation remain the same for both the\n(train) and (train + val) settings. For Mini-ImageNet, (train + val) obtains 64.20 ± 0.65% and 81.26\n± 0.45%, and (train) obtains 62.55 ± 0.65% and 78.89 ± 0.46%, for 1-shot 5-way and 5-shot 5-way\nrespectively. These results corroborate the original statement that more pre-training classes improves\nfew-shot performance.\nC.6\nUSING FEATURES OF THE BACKBONE AS INPUT TO THE CLASSIFIER\nInstead of re-initializing the ﬁnal fully-connected layer of the backbone to classify new classes, we\nsimply append the classiﬁer on top of it. We implemented the former, more common, approach\nand found that it achieves an accuracy of 64.20 ± 0.65% and 81.26 ± 0.45% for 1-shot 5-way and\n5-shot 5-way respectively on Mini-ImageNet, while the accuracy on Tiered-ImageNet is 67.14 ±\n16\n\n\nPublished as a conference paper at ICLR 2020\nTable 3: Few-shot accuracies on benchmark datasets for 5-way few-shot episodes. The notation conv\n(64k)×4 denotes a CNN with 4 layers and 64k channels in the kth layer. The rows are grouped by the backbone\narchitectures. Best results in each column and for a given backbone architecture are shown in bold. Results\nwhere the support-based initialization is better than or comparable to existing algorithms are denoted by †.\nThe notation (train + val) indicates that the backbone was pre-trained on both training and validation sets of\nthe datasets; the backbone is trained only on the training set otherwise. (Lee et al., 2019) uses a 1.25× wider\nResNet-12 which we denote as ResNet-12 ∗.\nMini-ImageNet\nTiered-ImageNet\nCIFAR-FS\nFC-100\nAlgorithm\nArchitecture\n1-shot (%)\n5-shot (%)\n1-shot (%)\n5-shot (%)\n1-shot (%)\n5-shot (%)\n1-shot (%)\n5-shot (%)\nMAML (Finn et al., 2017)\nconv (32)×4 48.70 ± 1.84 63.11 ± 0.92\nMatching networks (Vinyals et al., 2016)\nconv (64)×4\n46.6\n60\nLSTM meta-learner (Ravi & Larochelle,\n2016)\nconv (64)×4 43.44 ± 0.77 60.60 ± 0.71\nPrototypical Networks (Snell et al., 2017)\nconv (64)×4 49.42 ± 0.78 68.20 ± 0.66\nTransductive\nPropagation\n(Liu\net\nal.,\n2018b)\nconv (64)×4 55.51 ± 0.86 69.86 ± 0.65\n59.91 ± 0.94\n73.30 ± 0.75\nSupport-based initialization (train)\nconv (64)×4 50.69 ± 0.63 66.07 ± 0.53\n58.42 ± 0.69 73.98 ± 0.58† 61.77 ± 0.73 76.40 ± 0.54 36.07 ± 0.54 48.72 ± 0.57\nFine-tuning (train)\nconv (64)×4 49.43 ± 0.62 66.42 ± 0.53\n57.45 ± 0.68\n73.96 ± 0.56 59.74 ± 0.72 76.37 ± 0.53 35.46 ± 0.53 49.43 ± 0.57\nTransductive ﬁne-tuning (train)\nconv (64)×4 50.46 ± 0.62 66.68 ± 0.52\n58.05 ± 0.68\n74.24 ± 0.56 61.73 ± 0.72 76.92 ± 0.52 36.62 ± 0.55 50.24 ± 0.58\nR2D2 (Bertinetto et al., 2018)\nconv (96k)×4\n51.8 ± 0.2\n68.4 ± 0.2\n65.4 ± 0.2\n79.4 ± 0.2\nTADAM (Oreshkin et al., 2018)\nResNet-12\n58.5 ± 0.3\n76.7 ± 0.3\n40.1 ± 0.4\n56.1 ± 0.4\nTransductive\nPropagation\n(Liu\net\nal.,\n2018b)\nResNet-12\n59.46\n75.64\nSupport-based initialization (train)\nResNet-12\n54.21 ± 0.64 70.58 ± 0.54\n66.39 ± 0.73\n81.93 ± 0.54 65.69 ± 0.72 79.95 ± 0.51 35.51 ± 0.53 48.26 ± 0.54\nFine-tuning (train)\nResNet-12\n56.67 ± 0.62 74.80 ± 0.51\n64.45 ± 0.70\n83.59 ± 0.51 64.66 ± 0.73 82.13 ± 0.50 37.52 ± 0.53 55.39 ± 0.57\nTransductive ﬁne-tuning (train)\nResNet-12\n62.35 ± 0.66 74.53 ± 0.54\n68.41 ± 0.73\n83.41 ± 0.52 70.76 ± 0.74 81.56 ± 0.53 41.89 ± 0.59 54.96 ± 0.55\nMetaOpt SVM (Lee et al., 2019)\nResNet-12 ∗62.64 ± 0.61 78.63 ± 0.46\n65.99 ± 0.72\n81.56 ± 0.53\n72.0 ± 0.7\n84.2 ± 0.5\n41.1 ± 0.6\n55.5 ± 0.6\nSupport-based initialization (train)\nWRN-28-10 56.17 ± 0.64 73.31 ± 0.53\n67.45 ± 0.70\n82.88 ± 0.53 70.26 ± 0.70 83.82 ± 0.49 36.82 ± 0.51 49.72 ± 0.55\nFine-tuning (train)\nWRN-28-10 57.73 ± 0.62 78.17 ± 0.49\n66.58 ± 0.70\n85.55 ± 0.48 68.72 ± 0.67 86.11 ± 0.47 38.25 ± 0.52 57.19 ± 0.57\nTransductive ﬁne-tuning (train)\nWRN-28-10 65.73 ± 0.68 78.40 ± 0.52\n73.34 ± 0.71\n85.50 ± 0.50 76.58 ± 0.68 85.79 ± 0.50 43.16 ± 0.59 57.57 ± 0.55\nSupport-based initialization (train + val)\nconv (64)×4 52.77 ± 0.64 68.29 ± 0.54\n59.08 ± 0.70\n74.62 ± 0.57 64.01 ± 0.71 78.46 ± 0.53 40.25 ± 0.56 54.53 ± 0.57\nFine-tuning (train + val)\nconv (64)×4 51.40 ± 0.61 68.58 ± 0.52\n58.04 ± 0.68\n74.48 ± 0.56 62.12 ± 0.71 77.98 ± 0.52 39.09 ± 0.55 54.83 ± 0.55\nTransductive ﬁne-tuning (train + val)\nconv (64)×4 52.30 ± 0.61 68.78 ± 0.53\n58.81 ± 0.69\n74.71 ± 0.56 63.89 ± 0.71 78.48 ± 0.52 40.33 ± 0.56 55.60 ± 0.56\nSupport-based initialization (train + val)\nResNet-12\n56.79 ± 0.65 72.94 ± 0.55\n67.60 ± 0.71\n83.09 ± 0.53 69.39 ± 0.71 83.27 ± 0.50 43.11 ± 0.58 58.16 ± 0.57\nFine-tuning (train + val)\nResNet-12\n58.64 ± 0.64 76.83 ± 0.50\n65.55 ± 0.70\n84.51 ± 0.50 68.11 ± 0.70 85.19 ± 0.48 42.84 ± 0.57 63.10 ± 0.57\nTransductive ﬁne-tuning (train + val)\nResNet-12\n64.50 ± 0.68 76.92 ± 0.55\n69.48 ± 0.73\n84.37 ± 0.51 74.35 ± 0.71 84.57 ± 0.53 48.29 ± 0.63 63.38 ± 0.58\nMetaOpt SVM (Lee et al., 2019) (train +\nval)\nResNet-12 ∗64.09 ± 0.62 80.00 ± 0.45\n65.81 ± 0.74\n81.75 ± 0.53\n72.8 ± 0.7\n85.0 ± 0.5\n47.2 ± 0.6\n62.5 ± 0.6\nActivation to Parameter (Qiao et al., 2018)\n(train + val)\nWRN-28-10 59.60 ± 0.41 73.74 ± 0.19\nLEO (Rusu et al., 2018) (train + val)\nWRN-28-10 61.76 ± 0.08 77.59 ± 0.12\n66.33 ± 0.05\n81.44 ± 0.09\nSupport-based initialization (train + val)\nWRN-28-10 58.47 ± 0.66 75.56 ± 0.52 67.34 ± 0.69† 83.32 ± 0.51† 72.14 ± 0.69 85.21 ± 0.49 45.08 ± 0.61 60.05 ± 0.60\nFine-tuning (train + val)\nWRN-28-10 59.62 ± 0.66 79.93 ± 0.47\n66.23 ± 0.68\n86.08 ± 0.47 70.07 ± 0.67 87.26 ± 0.45 43.80 ± 0.58 64.40 ± 0.58\nTransductive ﬁne-tuning (train + val)\nWRN-28-10 68.11 ± 0.69 80.36 ± 0.50\n72.87 ± 0.71\n86.15 ± 0.50 78.36 ± 0.70 87.54 ± 0.49 50.44 ± 0.68 65.74 ± 0.60\n0.74% and 86.67 ± 0.46% for 1-shot 5-way and 5-shot 5-way respectively. These numbers are\nsigniﬁcantly lower for the 1-shot 5-way protocol on both datasets compared to their counterparts in\nTable 1. However, the 5-shot 5-way accuracy is marginally higher in this experiment than that in\nTable 1. As noted in Remark 2, logits of the backbone are well-clustered and that is why they work\nbetter for few-shot scenarios.\n17\n\n\nPublished as a conference paper at ICLR 2020\nC.7\nFREEZING THE BACKBONE RESTRICTS PERFORMANCE\nThe previous observation suggests that the network changes a lot in the ﬁne-tuning phase. Freezing\nthe backbone severely restricts the changes in the network to only changes to the classiﬁer. As\na consequence, the accuracy of freezing the backbone is 58.38 ± 0.66 % and 75.46 ± 0.52% on\nMini-ImageNet and 67.06 ± 0.69% and 83.20 ± 0.51% on Tiered-ImageNet for 1-shot 5-way and\n5-shot 5-way respectively. While the 1-shot 5-way accuracies are much lower than their counterparts\nin Table 1, the gap in the 5-shot 5-way scenario is smaller.\nC.8\nUSING MIXUP DURING PRE-TRAINING\nMixup improves the few-shot accuracy by about 1%; the accuracy for WRN-28-10 trained without\nmixup is 67.06 ± 0.71% and 79.29 ± 0.51% on Mini-ImageNet for 1-shot 5-way and 5-shot 5-way\nrespectively.\nC.9\nMORE FEW-SHOT EPISODES\nFig. 1 suggests that the standard deviation of the accuracies achieved by few-shot algorithms is high.\nConsidering this randomness, evaluations were done over 10,000 few-shot episodes as well. The\naccuracies on Mini-ImageNet are 67.77 ± 0.21 % and 80.24 ± 0.16 % and on Tiered-ImageNet are\n72.36 ± 0.23 % and 85.70 ± 0.16 % for 1-shot 5-way and 5-shot 5-way respectively. The numbers\nare consistent with the ones for 1,000 few-shot episodes in Table 1, though the conﬁdence intervals\ndecreased as the number of episodes sampled increased.\nC.10\nEVALUATION ON META-DATASET\nTable 4: Few-shot accuracies on Meta-Dataset: Best results in each row are shown in bold. 600 few-shot\nepisodes were used to compare to the results reported in Triantaﬁllou et al. (2019).\nDataset\nBest performance in\nTransductive\nRank for Transductive\nTriantaﬁllou et al. (2019)\nFine-tuning\nFine-tuning (based on\nTriantaﬁllou et al. (2019))\nImageNet-1k (ILSVRC)\n51.01 ± 1.05\n55.57 ± 1.02\n1\nOmniglot\n63.00 ± 1.35\n79.59 ± 0.98\n1\nAircraft\n68.69 ± 1.26\n67.26 ± 0.98\n1.5\nBirds\n68.79 ± 1.01\n74.26 ± 0.82\n1\nTextures\n69.05 ± 0.90\n77.35 ± 0.74\n1\nVGG Flowers\n86.86 ± 0.75\n88.14 ± 0.63\n1.5\nTrafﬁc Signs\n66.79 ± 1.31\n55.98 ± 1.32\n2\nMSCOCO\n43.41 ± 1.06\n40.62 ± 0.98\n2.5\nAverage Rank\n1.4375\nWe ran experiments on Meta-Dataset (Triantaﬁllou et al., 2019), and compared the performance of\ntransductive ﬁne-tuning for meta-training done on ImageNet-1k (ILSVRC) in Table 4. Transductive\nﬁne-tuning is better, most times signiﬁcantly, than state-of-the-art on 6 out of 8 tasks in Meta-Dataset;\nits average rank across all tasks is 1.4375 (calculated using the results reported in Triantaﬁllou et al.\n(2019)). The Fungi and Quick Draw datasets were not included because of issues with getting the\ndata; the link to access the dataset for the former does not seem to work and the latter requires certain\nlegal conditions which we are working on obtaining.\nThe few-shot episode sampling was done the same way as described in Triantaﬁllou et al. (2019);\nexcept for the few-shot class sampling for ImageNet-1k (ILSVRC) and Omniglot, which was done\nuniformly over all few-shot classes (Triantaﬁllou et al. (2019) use a hierarchical sampling technique to\nsample classes that are far from each other in the hierarchy, and hence easier to distinguish between).\nThe hyper-parameters used for meta-training and few-shot ﬁne-tuning are kept the same as the ones\nin Section 4 and are not tuned for these experiments.\n18\n\n\nPublished as a conference paper at ICLR 2020\nD\nFREQUENTLY ASKED QUESTIONS\n1. Why has it not been noticed yet that this simple approach works so well?\nNon-transductive ﬁne-tuning as a baseline has been considered before (Vinyals et al., 2016; Chen\net al., 2018). The fact that this is comparable to state-of-the-art has probably gone unnoticed\nbecause of the following reasons:\n• Given that there are only a few labeled support samples provided in the few-shot setting,\ninitializing the classiﬁer becomes important. The support-based initialization (cf. Section 3.1)\nmotivated from the deep metric learning literature (Hu et al., 2015; Movshovitz-Attias et al.,\n2017; Qi et al., 2018; Gidaris & Komodakis, 2018) classiﬁes support samples correctly (for\na support shot of 1, this may not be true for higher support shots). This initialization, as\nopposed to initializing the weights of the classiﬁer randomly, was critical to performance in\nour experiments.\n• In our experience, existing meta-training methods, both gradient-based ones and metric-\nbased ones, are difﬁcult to tune for larger architectures. We speculate that this is the reason a\nlarge part of the existing literature focuses on smaller backbone architectures. The few-shot\nlearning literature has only recently started to move towards bigger backbone architectures\n(Oreshkin et al., 2018; Rusu et al., 2018). From Table 3 we see that non-tranductive ﬁne-\ntuning gets better with a deeper backbone architecture. A similar observation was made\nby (Chen et al., 2018). The observation that we can use “simple” well-understood training\ntechniques from standard supervised learning that scale up to large backbone architectures\nfor few-shot classiﬁcation is a key contribution of our paper.\nTransductive methods have recently started to become popular in the few-shot learning literature\n(Nichol et al., 2018; Liu et al., 2018a). Because of the scarcity of labeled support samples, it is\ncrucial to make use of the unlabeled query samples in the few-shot regime.\nOur advocated baseline makes use of both a good initialization and transduction, relatively new in\nthe few-shot learning literature, which makes this simplistic approach go unrecognized till now.\n2. Transductive ﬁne-tuning works better than existing algorithms because of a big backbone\narchitecture. One should compare on the same backbone architectures as the existing algo-\nrithms for a fair comparison.\nThe current literature is in the spirit of increasingly sophisticated approaches for modest perfor-\nmance gains, often with different architectures (cf. Table 1). This is why we set out to establish a\nbaseline. Our simple baseline is comparable or better than existing approaches. The backbone\nwe have used is common in the recent few-shot learning literature (Rusu et al., 2018; Qiao et al.,\n2018) (cf. Table 1). Additionally, we have included results on smaller common backbone architec-\ntures, namely conv (64)×4 and ResNet-12 in Appendix C.4, and some additional experiments in\nAppendix C.2. These experiments suggest that transductive ﬁne-tuning is a sound baseline for a\nvariety of different backbone architectures. This indicates that we should take results on existing\nbenchmarks with a grain of salt. Also see the response to question 1 above.\n3. There are missing entries in Tables 1 and 3. Is it still a fair comparison?\nTables 1 and 3 show all relevant published results by the original authors. Re-implementing\nexisting algorithms to ﬁll missing entries without access to original code is impractical and often\nyields results inferior to those published, which may be judged as unfair. The purpose of a\nbenchmark is to enable others to test their method easily. This does not exist today due to myriad\nperformance-critical design choices often not detailed in the papers. In fact, missing entries in\nthe table indicate the inadequate state of the current literature. Our work enables benchmarking\nrelative to a simple, systematic baseline.\n4. Fine-tuning for few-shot learning is not novel.\nWe do not claim novelty in this paper. Transductive ﬁne-tuning is our advocated baseline for\nfew-shot classiﬁcation. It is a combination of different techniques that are not novel. Yet, it\nperforms better than existing algorithms on all few-shot protocols with ﬁxed hyper-parameters.\nWe emphasize that this indicates the need to re-interpret existing results on benchmarks and\nre-evaluate the status quo in the literature.\n19\n\n\nPublished as a conference paper at ICLR 2020\n5. Transductive ﬁne-tuning has a very high latency at inference time, this is not practical.\nOur goal is to establish a systematic baseline for accuracy, which might help judge the accuracy of\nfew-shot learning algorithms in the future. The question of test-time latency is indeed important\nbut we have not focused on it in this paper. Appendix C.3 provides results using a smaller backbone\nwhere we see that the WRN-16-4 network is about 20-70x slower than metric-based approaches\nemploying the same backbone while having signiﬁcantly better accuracy. The latencies with\nWRN-28-10 are larger (see the computational complexity section in Section 4.3) but with a bigger\nadvantage in terms of accuracy.\nThere are other transductive methods used for few-shot classiﬁcation (Nichol et al., 2018; Liu\net al., 2018a), that are expected to be slow as well.\n6. Transductive ﬁne-tuning does not make sense in the online setting when query samples are\nshown in a sequence.\nTransductive ﬁne-tuning can be performed even with a single test datum. Indeed, the network can\nspecialize itself completely to classify this one datum. We explore a similar scenario in Section 4.3\nand Fig. 2a, which discuss the performance of transductive ﬁne-tuning with a query shot of 1 (this\nmeans 5 query samples one from each class for 5-way evaluation). Note that the loss function in\n(8) leverages multiple query samples when available. It does not require that the query samples be\nbalanced in terms of their ground-truth classes. In particular, the loss function in (8) is well-deﬁned\neven for a single test datum. For concerns about latency, see the question 5 above.\n7. Having transductive approaches will incentivize hacking the query set.\nThere are already published methods that use transductive methods (Nichol et al., 2018; Liu et al.,\n2018a), and it is a fundamental property of the transductive paradigm to be dependent on the query\nset, in addition to the support set. In order to prevent query set hacking, we will make the test\nepisodes public which will enable consistent benchmarking, even for transductive methods.\n8. Why is having the same hyper-parameters for different few-shot protocols so important?\nA practical few-shot learning algorithm should be able to handle any few-shot protocol. Having\none model for each different scenario is unreasonable in the real-world, as the number of different\nscenarios is, in principle, inﬁnite. Current algorithms do not handle this well. A single model\nwhich can handle any few-shot scenario is thus desirable.\n9. Is this over-ﬁtting to the test datum?\nNo, label of the test datum is not used in the loss function.\n10. Can you give some intuition about the hardness metric? How did you come up with the\nformula?\nThe hardness metric is the clustering loss where the labeled support samples form the centers of\nthe class-speciﬁc clusters. The special form, namely, E(x,y)∈Dq log 1−p(y|x)\np(y|x)\n(cf. (9)) allows an\ninterpretation of log-odds. We used this form because it is sensitive to the number of few-shot\nclasses (cf. Fig. 3). Similar metrics, e.g., E(x,y)∈Dq [−log p(y|x)] can also be used but they\ncome with a few caveats. Note that it is easier for p(y|x) to be large for small way because the\nnormalization constant in softmax has fewer terms. For large way, p(y|x) could be smaller. This\neffect is better captured by our metric.\n11. How does Fig. 3 look for algorithm X, Y, Z?\nWe compared two algorithms in Fig. 3, namely transductive ﬁne-tuning and support-based initial-\nization. Section 4.4 and the caption of Fig. 3 explains how the former algorithm is better. We will\nconsider adding comparisons to other algorithms to this plot in the future.\n20\n"
}