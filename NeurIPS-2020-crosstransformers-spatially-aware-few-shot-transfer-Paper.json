{
  "filename": "NeurIPS-2020-crosstransformers-spatially-aware-few-shot-transfer-Paper.pdf",
  "num_pages": 13,
  "pages": [
    "CrossTransformers: spatially-aware few-shot transfer\nCarl Doersch∗\nAnkush Gupta∗\nAndrew Zisserman∗†\n∗DeepMind, London\n† VGG, Department of Engineering Science, University of Oxford\nAbstract\nGiven new tasks with very little data—such as new classes in a classiﬁcation\nproblem or a domain shift in the input—performance of modern vision systems\ndegrades remarkably quickly. In this work, we illustrate how the neural network\nrepresentations which underpin modern vision systems are subject to supervision\ncollapse, whereby they lose any information that is not necessary for performing\nthe training task, including information that may be necessary for transfer to new\ntasks or domains. We then propose two methods to mitigate this problem. First, we\nemploy self-supervised learning to encourage general-purpose features that transfer\nbetter. Second, we propose a novel Transformer based neural network architecture\ncalled CrossTransformers, which can take a small number of labeled images and\nan unlabeled query, ﬁnd coarse spatial correspondence between the query and the\nlabeled images, and then infer class membership by computing distances between\nspatially-corresponding features. The result is a classiﬁer that is more robust to\ntask and domain shift, which we demonstrate via state-of-the-art performance on\nMeta-Dataset, a recent dataset for evaluating transfer from ImageNet to many other\nvision datasets. Code available at: https://github.com/google-research/\nmeta-dataset.\n1\nIntroduction\nGeneral-purpose vision systems must be adaptable. Home robots must be able to operate in new,\nunseen homes; photo-organizing software must recognize unseen objects (e.g., to ﬁnd examples of\n“my sixth-grade son’s abstract art project”); industrial quality-assurance systems must spot defects in\nnew products. Deep neural network representations can bring some visual knowledge from datasets\nlike ImageNet [68] to bear on different tasks beyond ImageNet [15, 32, 62], but empirically, this\nrequires a non-trivial amount of labeled data in the new task. With too little labeled data, or for a\nlarge change in distribution, such systems empirically perform poorly.\nResearch on meta-learning directly benchmarks adaptability. At training time, the algorithm receives\na large amount of data and accompanying supervision (e.g., labels). At test time, however, the\nalgorithm receives a series of episodes, each of which consists of a small number of datapoints from a\ndifferent distribution than the training set (e.g., a different domain or different classes). Only a subset\nof this data has the accompanying supervision (called the support set); the algorithm must make\npredictions about the rest (the query set). Meta-Dataset [86] is particularly relevant for vision, as the\nchallenge is few-shot ﬁne-grained image classiﬁcation. The training data is a subset of ImageNet\nclasses. At test time, each episode either contains images from the other ImageNet classes, or from\none of nine other visually distinct ﬁne-grained recognition datasets. The algorithm must rapidly adapt\nits representations to the new classes and domains.\nSimple centroid-based algorithms like Prototypical Nets [17, 76] are near state-of-the-art on Meta-\nDataset, achieving around 50% accuracy on the held-out ImageNet classes in Meta-Dataset’s valida-\ntion set (chance is roughly 1 in 20). An equivalent classiﬁer trained on those validation classes can\nachieve roughly 84% accuracy on the same challenge. What accounts for the enormous discrepancy\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\n",
    "Query\nNearest Neighbors\ngila monster\ngila monster hognose snake gila monster\ngila monster\ngila monster\nnight snake\nhognose snake gila monster\nknot\nletter opener\nletter opener\nscrewdriver\nletter opener\nballpoint\nfountain pen\nbuckle\nletter opener\nletter opener\nbassoon\nhammer\nhammer\nmicrophone\nhammer\nscrew\nhammer\nsyringe\nshovel\nhook\nscrew\nbuckeye\nbuckeye\nbuckeye\nhip\npomegranate\nsunglasses\nbuckeye\nbuckeye\nbuckeye\nscrew\n  : in test split\nFigure 1: Illustration of supervision collapse with nearest neighbors. In each row, the leftmost\nimage is a query taken from the Meta-Dataset ImageNet test classes, and the rest are the top 9\nnearest neighbors from both training and test support set classes, using the embedding learned by a\nPrototypical Net (training details in the extended version [1]). Images belonging to the test split are\nindicated by a\nnear the bottom left corner; rest are from the training split. For a simple classiﬁer\nto work well on these test classes, semantically similar images should have similar representations,\nand so we hope the nearest neighbors would come from the same—or semantically similar—classes.\nInstead, we observe that only 5% of matches for test-set queries are from the same class as the query.\nFurthermore, many matches are all from the same incorrect training class (highlighted in red). We\nsee a knot is matched with several gila monsters (and other reptiles); a bassoon with letter openers\n(and pens); a screw with hammers; another screw with buckeyes. The errors within that wrong class\noften have widely different appearance: for example, the bottom-most screw is matched with single\nbuckeyes and also a pile of buckeyes. One interpretation is that the network picks up on image\npatterns during training that allow images of each class to be tightly grouped in the feature space,\nminimizing other ways that the image might be similar to other classes in preparation for a conﬁdent\nclassiﬁcation. For out-of-domain samples, the network can then overemphasize a spurious image\npattern that suggests membership in one training-set class. This is the consequence of supervision\ncollapse, where image patterns that might help make the correct associations are lost.\nbetween performance on within-distribution samples and out-of-distribution samples? We hypothe-\nsize that because the neural network backbone of Prototypical Nets is designed for classiﬁcation, they\ndo just this: represent only an image’s (training-set) class, and discard information that might help\nwith out-of-distribution classes. Doing so minimizes the losses for many meta-learning algorithms,\nincluding Prototypical Nets. We call this problem supervision collapse, and illustrate it in Figure 1.\nOur ﬁrst contribution is to explore using self-supervision to overcome supervision collapse. We\nemploy SimCLR [16], which learns embeddings that discriminate between every image in the dataset\nwhile maintaining invariance to transformations (e.g., cropping and color shifts), thus capturing more\nthan just classes. However, rather than treat SimCLR as an auxiliary loss, we reformulate SimCLR as\n“episodes” that can be classiﬁed in the same manner as a training episode.\nOur second contribution is a novel architecture called CrossTransformers, which extends Trans-\nformers [88] to few-shot ﬁne-grained classiﬁcation. Our key insight is that objects and scenes are\ngenerally composed of smaller parts, with local appearance that may be similar to what has been seen\nat training time. The classical example of this is the centaur that appeared in several early papers on\nvisual representation [12, 41, 89], where the parts from the human and horse composed the centaur.\nCrossTransformers operationalize this insight of (i) local part-based comparisons, and (ii) accounting\nfor spatial alignment, resulting in a procedure for comparing images which is more agnostic to the\nunderlying classes. In more detail, ﬁrst a coarse alignment between geometric or functional parts in\nthe query- and support-set images is established using attention as in Transformers. Then, given this\nalignment, distances between corresponding local features are computed to inform classiﬁcation. We\ndemonstrate this improves generalization to unseen classes and domains.\n2\n",
    "In summary, our contributions in this paper are: (i) We improve the robustness of our local features\nwith a self-supervised technique, modifying the state-of-the-art SimCLR [16] algorithm. (ii) We\npropose the CrossTransformer, a network architecture that is spatially aware and performs few-shot\nclassiﬁcation using more local features, which improves transfer. Finally, (iii) we evaluate and\nablate how the choices in these algorithms impact Meta-Dataset [86] performance, and demonstrate\nstate-of-the-art results on nearly every dataset within it, often by large margins.\n2\nRelated Work\nFew-shot image classiﬁcation.\nFew-shot learning [26, 36, 47, 54] has recently been primarily\naddressed in the meta-learning framework [58, 72, 81], where a model learns an update rule for\nthe parameters of a base-learner model [6, 7, 73] through a sequence of training episodes [80, 90].\nThe meta-learner either learns to produce new parameters directly from the new data [9, 34, 57, 63,\n65, 73, 74], or learns to produce an update rule to iteratively optimize the base learner to ﬁt the\nnew data [2, 6, 8, 39, 64, 99]. [28, 52, 60] do not use any explicit meta-learner model, but instead\nunroll the base-learner gradient updates and optimize for model initializations which generalize\nwell on novel tasks. Matching-based methods [29, 76, 79, 91] instead learn representations for\nsimilarity functions [11, 17, 18, 45, 83], in the hope that the similarities will generalize to new\ndata. CrossTransformers fall in this category, and share much of their architecture with Prototypical\nNets [76].\nAttention for few-shot learning.\nCrossTransformers attend [4] individually over each class’s\nsupport set to establish local correspondences, whereas Matching Networks [91] attend over the\nwhole support set to “point to” matching instances. [55] extend this idea to larger contexts using\ntemporally dilated convolutions [87]. In the limit, attention over long-term experiences accumulated\nin memories [44, 57, 70, 77] can augment more traditional learning.\nCorrespondences for visual recognition.\nCrossTransformers perform classiﬁcation by matching\nmore local parts. Discriminative parts [5, 10, 23, 35, 43, 84] and visual words [75, 102] have a\nrich history, and have found applications in deformable-parts models [27], classiﬁcation [71, 102],\nand retrieval [13, 85]. Part-based correspondences for recognition [105] have been particularly\nsuccessful in ﬁne-grained face retrieval and recognition [49, 97]. CrossTransformers establish\nsoft correspondences between pixels in the query- and support-set images; such dense pairwise\ninteractions [94] have recently proved useful for generative networks [101], semantic matching [67]\nand tracking [92]. [50] learns spatially dense classiﬁers for few-shot classiﬁcation, but pools the\nspatial dimensions of the prototypes, and hence does not have a notion of correspondence.\nSelf-supervised learning for few-shot.\nOur work on SimCLR episodes inherits from a line of\nself-supervised learning research, which typically deal with transfer from pretext tasks to semantic\nones and must therefore represent more than their training data [3, 14, 16, 22, 25, 31, 37, 48,\n61, 103, 104]. Some recent works [30, 78] demonstrate that this can improve few-shot learning,\nalthough these use self-supervised auxiliary losses rather than integrating self-supervised instance\ndiscrimination [16, 25, 56, 82, 96] into episodic training. Also particularly relevant are methods\nthat use self-supervision for correspondence [42, 51, 92], which may in future work improve the\ncorrespondences that CrossTransformers use.\n3\nStopping Collapse: SimCLR Episodes and CrossTransformers\nWe take a two-pronged approach to dealing with the supervision collapse problem. Modern ap-\nproaches to few-shot learning typically involve learning an embedding for each image, followed by a\nclassiﬁer that aggregates information across an episode’s support set in order to classify the episode’s\nqueries. Our ﬁrst step aims to use self-supervised learning to improve the embedding so it expresses\ninformation beyond the classes, in a way that is as algorithm-agnostic as possible. Once we have these\nembeddings, we build a classiﬁer called a CrossTransformer. CrossTransformers use Prototypical\nNets [76] as a blueprint, chosen due to their simplicity and strong performance; the main modiﬁcation\nis to aggregate information in a spatially-aware way. We begin by reviewing Prototypical Nets, and\nthen describe the two approaches.\n3\n",
    "Prototypical Nets are episodic learners, which means training is performed on the same kind of\nepisodes that will be presented at test time: a query set Q of images, and a support set S which can\nbe partitioned into classes c ∈{1, 2, . . . , C}: each Sc = {xc\ni}N\ni=1 is composed of N example images\nxc\ni. Prototypical Nets learn a distance function between the query and each subset Sc. Both the\nquery- and support-set images are ﬁrst encoded into a D-dimensional representation Φ(x), using a\nshared ConvNet Φ : RH×W ×3 7→RD, where H, W are the height and width respectively. Then a\n“prototype” tc ∈RD for the class c is obtained by averaging the representations of the support set Sc,\ntc =\n1\n|Sc|\nP\nx∈Sc Φ(x). Finally, a distribution of classes is obtained using softmax over the distances\nbetween the query image and class prototypes: p(y = c|xq) =\nexp(−d(Φ(xq),tc))\nPC\nc′=1 exp(−d(Φ(xq),tc′)). In practice,\nthe distance function d is ﬁxed to be the squared Euclidean distance d(xq, Sc) = ||Φ(xq) −tc||2\n2.\nThe learning objective is to train the embedding network Φ to maximize the probability of the correct\nclass for each query.\n3.1\nSelf-supervised training with SimCLR\nOur ﬁrst challenge is to improve the neural network embedding Φ: after all, if these features have\ncollapsed to represent little information beyond the classes, then a subsequent classiﬁer cannot can\nrecover this information. But how can we train features to represent things beyond labels when our\nonly supervision is the labels? Our solution is self-supervised learning, which invents “pretext tasks”\nthat train representations without labels [22, 25], and better yet, has a reputation for representations\nthat transfer beyond this pretext task. Speciﬁcally we use SimCLR [16], which uses “instance\ndiscrimination” as a pretext task. It works by applying random image transformations (e.g., cropping\nor color shifts) twice to the same image, thus generating two “views” of that image. Then it trains the\nnetwork so that representations of the two views of the same image are more similar to each other\nthan they are to those of different images. Empirically, networks trained in this way become sensitive\nto semantic information, but also learn to discriminate between different images within a single class,\nwhich is useful for combating supervision collapse.\nWhile we could treat SimCLR as an auxiliary loss on the embedding, we instead reformulate SimCLR\nas episodic learning, so that the technique can be applied to all episodic learners with minimal\nhyper-parameters. To do this, we randomly convert 50% of the training episodes into what we call\nSimCLR episodes, by treating every image as its own class. For clarity, we will call the original\nepisodes that have not been converted SimCLR episodes MD-categorization episodes, to emphasize\nthat they use the original categories from Meta-Dataset. Speciﬁcally, let ρ(·) be SimCLR’s (random)\nimage transformation function, and let S = {xi}|S|\ni=1 be a training support set. We generate a\nSimCLR episode by sampling a new support set, transforming each image in the original support\nset S′ = {ρ(xi)}|S|\ni=1, and then generating query images by sampling other transformations from the\nsame support set: Q′ = {ρ(random_sample(S))}|Q|\ni=1, where random_sample just takes a random\nimage from the set.1 The original query set Q is discarded. The label for an image in the SimCLR\nepisode is its index in the original support set, resulting in an |S|-way classiﬁcation for each query.\nNote that for a SimCLR episode, the ‘prototypes’ in Prototypical Nets average over just a single\nimage, and therefore the Prototypical Net loss can be written as\nexp(−d(Φ(ρ(xq)),Φ(ρ(xq))))\nPn\ni=1 exp(−d(Φ(ρ(xq)),Φ(ρ(xi))). If we\ndeﬁne d as the cosine distance rather than Euclidean, this loss is identical to the one used in SimCLR.\n3.2\nCrossTransformers\nGiven a query image xq and a support set Sc = {xc\ni}N\ni=1 for the class c, CrossTransformers aim to\nbuild a representation which enables local part-based comparisons between them.\nCrossTransformers start by making the image representation a spatial tensor, and then assemble\nquery-aligned class prototypes by putting the support-set images Sc in correspondence with the\nquery image. The distance between the query image and the query-aligned prototype for each class\nis then computed and used in a similar way to Prototypical Nets. In practice, we establish soft\ncorrespondences using attention [4] based Transformers [88]. In contrast, Prototypical Nets use ﬂat\nvector representations which lose the location of image features, and have a ﬁxed class prototype\nwhich is independent of the query image.\n1We enforce that the sampled queries have the same class distribution as Q, and have no repeats.\n4\n",
    "p\nΓ\nΓ\nΩ\nm\nn\nDot-product query features at location \np against all support set (Sc) spatial \nfeatures in class\nΛ\nΛ\nSoftmax across all\nspatial features in class\np\nKey Heads\nValue Heads\nQuery-aligned\nprototype (tc)\nweighted sum\nSoftmax normalized\nattention weights\nSupport-set (Sc) image \nfeatures for the category c\nQuery Head\ndv\ndv\ndk\ndk\nQuery image \nfeatures\nm\nn\na2\n    c\na1\n    c\n~a2\n    c\n~a1\n    c\nKeys (kc)\nValues (vc)\nQueries (q)\nFigure 2: CrossTransformers. Construction of query-aligned class prototype vector tc\np for the class\nc and the query image xq, focusing on the spatial location p in xq. The query vector qp is compared\nagainst keys kc from all spatial locations in the support set Sc to obtain attention scores ac, which are\nsoftmax normalized before being used to aggregate the values vc for the aligned prototype vector tc\np.\nConcretely, CrossTransformers remove the ﬁnal spatial pooling in Prototypical Nets’ embedding\nnetwork Φ(·), such that the spatial dimensions H′, W ′ are preserved: Φ(x) ∈RH′×W ′×D. Following\nTransformers, key-value pairs are then generated for each image in the support set using two\nindependent linear maps: the key-head Γ : RD 7→Rdk, and the value-head Λ : RD 7→Rdv\nrespectively. Similarly, the query image features Φ(xq) are embedded using the query-head Ω:\nRD 7→Rdk. Dot-product attention scores are then obtained between keys and queries, followed by\nsoftmax normalization across all the images and locations in Sc. This attention serves as our coarse\ncorrespondence (see example attention visualizations in Figure 3 and the extended version [1]), and\nis used to aggregate the support-set features into alignment with the query. This process is visualized\nin Figure 2.\nMathematically, let kc\njm = Γ · Φ(xc\nj)m be the key for the jth image in the support set for class c at\nspatial position m (index over the two dimensions H′, W ′), and similarly let qp = Ω· Φ(xq)p be the\nquery vector at spatial position p in the query image xq. The attention ˜ac\njmp ∈R between the two is\nthen obtained as:\n˜ac\njmp =\nexp(ac\njmp/τ)\nP\ni,n exp(ac\ninp/τ),\nwhere\nac\njmp = kc\njm · qp,\nand\nτ =\np\ndk.\n(1)\nNext, the aligned prototype vector tc\np corresponding to spatial location p in the query is obtained by\naggregating the support-set values vc\njm = Λ · Φ(xc\nj)m using the attention weights above:\ntc\np =\nX\njm\n˜ac\njmpvc\njm\n(2)\nFinally, squared Euclidean distances between aligned local features from the above prototype and\ncorresponding query image values wp = Λ · Φ(xq)p are aggregated as below. This scalar distance\nacts as a negative logit for a distribution over classes as in Prototypical Nets.\nd(xq, Sc) =\n1\nH′W ′\nX\np\n||tc\np −wp||2\n2\n(3)\nNote we apply the same value-head Λ to both the query and support-set images. This ensures that\nthe CrossTransformer behaves somewhat like a distance. That is, imagine a trivial case where, for\none class, all images in Sc are identical to xq. We would want d(xq, Sc) to approach 0 even if the\nnetwork is untrained, or if these images are highly dissimilar from those used for training. Sharing Λ\nbetween the support and query sets helps accomplish this: in fact, if ˜ac\njmp is 1 where p = m and 0\nelsewhere for all j, then d(xq, Sc) will be identically 0 under this architecture, no matter the network\n5\n",
    "Query\nCorrespondence in support set\nQuery\nCorrespondence in support set\nFigure 3: Visualization of the attention ˜a. We show four query images, along with three support-set\nimages for each. Within each query image, we choose three spatial locations (red, green, and blue\nsquares), and plot the CrossTransformer attention weights for each one in the corresponding color\n(brighter colors mean higher weight). The four examples are from Aircraft, CU-Birds, VGG Flowers,\nand ImNet test sets respectively (clockwise, starting from top-left). No matter which dataset, the\nattention masks are semantically relevant, even when the correspondence is not one-to-one. More\nvisualizations are given in the extended version [1].\nweights. To encourage this behavior for the attention ˜a, we also set Γ = Ω, i.e., the key and query\nheads are the same. This way, in our trivial case, the attention is likely to be maximal for spatial\nlocations that correspond, because kc\njm and qp will be the same for p = m.\nFor one experiment, we also augment the CrossTransformer with a global feature, which can help for\nsome datasets like DTD (Describable Textures Dataset) with less spatial structure.\n4\nExperiments\nWe evaluate on Meta-Dataset [86], speciﬁcally the setting where the training is performed on the\nImageNet train split only, which is 712 classes (plus 158 classes for validation, which are not used\nfor training but only to perform early stopping). We then test on the remaining 130 held-out classes\nfrom ImageNet, as well as 9 other image datasets. Note that this is in contrast to another popular (and\neasier) setting, where the training also uses a subset of categories from more of these datasets: usually\nall datasets except Trafﬁc Signs and COCO. For clarity, we’ll use “Meta-Dataset Train-on-ILSVRC”\nto denote training on ImageNet only, and “Meta-Dataset Train-on-all” to denote when training occurs\non more datasets. Test time consists of a series of episodes, each of which contains: (1) a support set\nbetween 50 and 500 labeled images which come from between 5 and 50 classes; and (2) an unlabeled\nquery set with 10 images per class. Meta-Dataset aims for ﬁne-grained recognition, so the classes in\neach episode are mutually similar: one episode may contain only musical instruments, another may\ncontain only birds, etc.\nMeta-Dataset is useful for studying transfer because different test datasets encapsulate different\nkinds of transfer challenges. For test datasets like CU-Birds [93], there are numerous similar classes\nin ImageNet train (20 bird classes in ImageNet train, versus 100 in the CU-birds test dataset). In\ncontrast, for test datasets like Aircraft [53], there is just a single corresponding class in ImageNet\ntrain; therefore, algorithms which don’t represent the intra-class variability for this class will be\npenalized. The ImageNet test set has images in a similar domain to the ImageNet train set but with\ndifferent classes, while test datasets like COCO contain many similar classes to ImageNet, but with\ndomain shift (in COCO, instances are generally not the subject of their photographs, and may be\nlow-resolution or occluded). Finally, test datasets like OmniGlot combine these challenges, i.e.,\ndifferent classes in a substantially different domain.\n4.1\nImplementation details\nTo ensure comparability, we followed the public implementation of Prototypical Nets for Meta-\nDataset [86] wherever possible. This includes using the same hyperparameters, unless otherwise\nnoted. For the hyperparameters that were chosen with a sweep on the validation set (learning rate\nschedule and weight decay), we simply used the best values discovered for Prototypical Nets for\nall the experiments in this paper. See the extended version [1] for details of the CrossTransformer\n6\n",
    "architecture. We use no pretraining for CrossTransformers, although to be consistent with prior\nwork [86] we use it for the experiments involving Prototypical Nets.\nWe incorporate two improvements from Meta-Baseline [17], which at test time is similar to Proto-\ntypical Nets (though it isn’t trained as an episodic learner). The ﬁrst is to keep exponential moving\naverages for Batch Norm statistics during training, and use those for Batch Norm at test time. Second,\nwe note that Meta-Baseline does not train on ﬁne-grained episodes sampled from the ImageNet hier-\narchy, as Prototypical Nets does, but rather on batches with uniformly-sampled classes. Empirically,\nPrototypical Nets trained only on ﬁne-grained episodes struggle to do coarse-grained recognition, as\nrequired for datasets like COCO. Therefore, we only use the ImageNet hierarchy to make 50% of\nepisodes ﬁne-grained; the rest have categories sampled uniformly.\nChoice of network.\nPrior implementations of networks like Prototypical Nets use relatively small\nnetworks (e.g., ResNet-18) with small input images (e.g. 126×126 pixels), and report that measures\nto increase capacity (e.g., Wide ResNets [100]) provide minimal beneﬁts. This is surprising given that\nstandard networks show improvements for increasing capacity (e.g., ResNet-34 outperforms ResNet-\n18 by 3% on ImageNet [38]). Making our networks spatially-aware requires higher-resolution, and\nalso higher-capacity networks are especially important in self-supervised learning [24, 46]. Therefore,\nour experiments increase resolution to the standard 224×224 and use ResNet-34, and we also use\nnormalized stochastic gradient descent [19, 59], which we found improved stability when ﬁne-tuning\nmore complex networks. Table 1 compares the Prototypical Nets performance of this network to that\nof using a ResNet-18. Increased capacity leads to only slight performance improvements, which are\nmore pronounced for datasets that are similar to ImageNet; it harms, e.g., OmniGlot. Further details\nin the extended version [1].\nFor experiments with CrossTransformers, we also increased the resolution of the convolutional feature\nmap by setting the stride of ﬁnal block of the ResNet to 1, and using dilated convolutions to preserve\nthe feature computation [33, 40]. This turns the usual 7×7 feature map for a 224×224 image into a\n14×14 feature map. We ablate this choice in the extended version [1].\nAugmenting CTX with a global feature.\nRecent works have also shown beneﬁts for applying\nlogistic regression (LR) at test time [83]. In practice, it is too expensive to apply LR to our query-\naligned prototypes (as this would involve a separate classiﬁer for every query). Therefore, we instead\napply logistic regression to a globally-pooled feature and average the logits with those produced by\nthe CrossTransformer. See the extended version [1] for details.\nAugmentation.\nWhile most experiments use no augmentation (apart from SimCLR episodes) to be\nconsistent with prior work [86], more recent work [17, 69, 83] showed that stronger data augmentation\nis effective. Therefore, for two experiments, we employ augmentation using the settings discovered\nin BOHB [69] (via Auto-Augment [20] on the validation set), with an extra stage that randomly\ndownsamples and then upsamples images, which we ﬁnd helpful as our network operates at higher\nresolution than many of the test datasets. This BOHB augmentation is only applied to the “MD-\ncategorization” episodes, and not to the SimCLR episodes. Note this BOHB augmentation is different\nfrom SimCLR-style augmentation, which is used in SimCLR Episodes as well as in the ablation\n(SC-Aug) in Table 1. See the extended version [1] for details.\n4.2\nResults for self-supervised learning with SimCLR on Prototypical Nets\nWe ﬁrst analyze the impact of SimCLR Episodes and other architectural choices in Table 1. For\nbaseline Prototypical Nets, SimCLR Episodes generally improve performance, but this depends on\narchitectural choices. Improvements are largest for datasets that are more distant from ImageNet,\ne.g., OmniGlot and Quickdraw, and datasets which require distinguishing between sub-categories\nImageNet categories, e.g., Aircraft and Trafﬁc Signs. In ImageNet, all commercial airplanes fall\nin a single ImageNet class; therefore, the success of SimCLR Episodes here suggests they recover\nfeatures which are lost due to supervision collapse. Strangely, however, SimCLR Episodes interact\nwith Batch Norm: we ﬁnd more robust improvements when computing Batch Norm statistics from\nthe test-time support set, but not when using exponential moving averages (EMA) as suggested by\n[17]. One possible interpretation is that the network has learned to use Batch Norm to communicate\ninformation across the batch: e.g., to distinguish between SimCLR Episodes and MD-categorization\nepisodes. Using EMA at test time may prevent this, which may confuse the network. Interestingly,\n7\n",
    "Table 1: Effects of architecture and SimCLR Episodes on Prototypical Nets, for Meta-Dataset\nTrain-on-ILSVRC. We ablate architectural choices: use of Exponential Moving Averages (EMA) at\ntest time for Batch Norm (versus computing Batch Norm statistics on the support set at test time),\nimage resolution (224, versus the baseline’s 126), ResNet-34 (R34) replacing ResNet-18, SimCLR-\nstyle augmentation (SC-Aug), and the addition of 50% SimCLR Episodes (SC-Eps). The test datasets\nfrom Meta-Dataset are ImNet: Meta-Dataset’s ImageNet Test classes; Omni: OmniGlot drawn\ncharacters; Acraft: Aircraft; Bird: CU-Birds; DTD: Textures; QDraw: Quick Draw drawings; Fungi:\nFGVCx fungi challenge; Flower: VGG Flowers; COCO: Microsoft COCO cropped objects. The best\nnumber in each column is bolded, along with others that are within a conﬁdence interval [86]. Rank\nis the average rank for each method. Using SimCLR Episodes provides improvements on almost\nall datasets, and provides especially large boosts for datasets which are dissimilar from ImageNet,\nsuch as OmniGlot. However, simply using SimCLR transformations without instance discrimination\n(SC-Aug) harms results on almost all datasets. Increased capacity provides small beneﬁts on some\ndatasets, especially the more realistic and ImageNet-like datasets (e.g., birds), but actually harm\nothers like OmniGlot. Note that in this table, QuickDraw uses the split from the original paper [86]\nrather than the (somewhat easier) split published for that paper’s public benchmark. For all other\ntables, we use the split from the published benchmark.\n224 R34 SC-Aug SC-Eps EMA ImNet Omni Acraft\nBird DTD QDraw Fungi Flower\nSign COCO Rank\n\u0013\n49.10 59.27\n49.31 68.43 66.70\n45.83 38.48\n85.34 49.49\n42.88\n5.55\n49.77 55.70\n52.06 68.58 67.27\n49.86 37.68\n84.32 50.27\n41.92\n5.20\n\u0013\n\u0013\n\u0013\n51.66 57.22\n51.63 71.73 69.72\n47.31 42.07\n87.29 47.45\n44.38\n4.35\n\u0013\n\u0013\n52.51 49.87\n56.47 72.81 68.45\n51.41 42.16\n87.92 54.40\n40.60\n3.30\n\u0013\n\u0013\n\u0013\n\u0013\n47.58 55.73\n46.93 57.75 54.88\n42.91 37.42\n83.82 46.88\n43.36\n7.55\n\u0013\n\u0013\n\u0013\n47.94 51.79\n54.58 62.84 58.64\n46.36 36.06\n76.88 48.35\n38.77\n7.45\n\u0013\n\u0013\n\u0013\n\u0013\n49.67 65.21\n54.46 60.94 63.96\n50.64 37.84\n88.70 51.61\n42.97\n4.35\n\u0013\n\u0013\n\u0013\n53.69 68.50\n58.04 74.07 68.76\n53.30 40.73\n86.96 58.11\n41.70\n1.90\nProtoNets [86]\n50.50 59.98\n53.10 68.79 66.56\n48.96 39.71\n85.27 47.12\n41.00\n5.35\nwe will show later that SimCLR Episodes don’t harm CrossTransformers as they harm Prototypical\nNets when using EMA at test time, suggesting the two architectures solve the problem differently.\nRecall that converting an MD-categorization episode into a SimCLR episode makes two changes to\nthe episode: it 1) applies data augmentation, and 2) converts the classiﬁcation problem to “instance\ndiscrimination,” by selecting images from the support set as a new query set, and requiring the\nnetwork to predict the selected indices. To ensure that we are not simply seeing the effect of data\naugmentation, we also implemented a baseline (SC-Aug) that does 1 but not 2 to the input MD-\ncategorization episodes, and does this augmentation for all episodes (rather than 50%, which is\nthe fraction of MD-categorization episodes that are converted to SimCLR episodes for SC-Eps\nexperiments). Indeed, we see no improvements for this change, and in fact non-trivial performance\nloss from this, mirroring the result for supervised learning in the original paper [16]. This reinforces\nthat SimCLR was designed for self-supervised learning, and so the transformations are more severe\nthan is usually optimal for supervised learning.\nFinally, we see small improvements from using larger networks and higher resolution for the baseline\nmodel. While our baseline is overall better than the baseline Prototypical Nets implementation [86],\nit is still below the state-of-the-art for centroid-based methods which rely more heavily on pretraining,\nand use no episodic training [17].\n4.3\nCrossTransformers results\nGiven these performant features, we next turn to CrossTransformers. Table 2 compares CrossTrans-\nformers (CTX) with and without SimCLR episodes to several state-of-the-art methods, including the\nPrototypical Nets on which they are based. We see that CrossTransformers provide strong perfor-\nmance on their own, including having a better average rank than all baselines. With SimCLR episodes\nproviding more versatile features, we see further improvements, with performance on-par or better\nthan the best methods on almost every dataset. We note particularly large improvements on OmniGlot,\nwhich has a large domain gap relative to the training data. We also see strong improvements on Street\nSigns, Aircraft, and Flowers, where multiple test-time categories map to few training-time categories,\nand often exhibit well-deﬁned spatial correspondence.\n8\n",
    "Table 2: CrossTransformers (CTX) comparison to state-of-the-art. We compare four versions\nof CrossTransformers to several state-of-the-art methods, which are the best performers among\nthose evaluated for Meta-Dataset Train-on-ILSVRC. We see that CTX alone has better average rank\nthan any baseline. Adding SimCLR episodes (+SimCLR Eps) and data augmentation inspired by\nBOHB [69] (+Aug) further improves results. Our full model is on-par or above prior methods on all\nbut one dataset, sometimes with large gaps over the best baseline (e.g., +5% on OmniGlot, +13% on\nAircraft, +5% on Signs), and furthermore, each prior method has some datasets where we outperform\nby a larger margin (the next best average rank [83], performs 19% worse on Aircraft and 17% worse\non OmniGlot). Finally, adding a test-time Logistic Regression classiﬁer inspired by Tian et al. [83]\nimproves performance on the one dataset—DTD textures—that was otherwise lacking. Note that\nmost of these methods [17, 69, 83] are unpublished concurrent work.\nImNet Omni Acraft\nBird\nDTD QDraw Fungi Flower\nSign COCO\nRank\nFinetuning [86]\n45.78 60.85\n68.69 57.31 69.05\n42.60 38.20\n85.51 66.79\n34.86 12.20\nProtoNets [86]\n50.50 59.98\n53.10 68.79 66.56\n48.96 39.71\n85.27 47.12\n41.00 12.65\nProtoNets+MAML [86]\n49.53 63.37\n55.95 68.66 66.49\n51.52 39.96\n87.15 48.83\n43.74 11.55\nCNAPS [66]\n50.60 45.20\n36.00 60.70 67.50\n42.30 30.10\n70.70 53.30\n45.20 13.55\nBOHB-L [69]\n50.60 64.09\n57.36 67.68 70.38\n46.26 33.82\n85.51 55.17\n41.58 11.50\nBOHB-NC [69]\n51.92 67.57\n54.12 70.69 68.34\n50.33 41.38\n87.34 51.80\n48.03 10.15\nBOHB-NC Ensemble [69]\n55.39 77.45\n60.85 73.56 72.86\n61.16 44.54\n90.62 57.53\n51.86\n7.45\nDhillon et al. [21]\n-\n-\n68.69 74.26 77.35\n-\n-\n88.14 55.98\n40.62\n-\nMeta-Baseline [17]\n59.20 69.10\n54.10 77.30 76.00\n57.30 45.40\n89.60 66.20\n55.70\n7.20\nTian et al. LR [83]\n60.14 64.92\n63.12 77.69 78.59\n62.48 47.12\n91.60 77.51\n57.00\n5.50\nTian et al. LR-distill [83]\n61.58 64.31\n62.32 79.47 79.28\n60.83 48.53\n91.00 76.33\n59.28\n4.60\nProtoNets (Our implementation)\n51.66 57.22\n51.63 71.73 69.72\n53.81 42.07\n87.29 47.45\n44.38 11.10\nCTX\n61.94 76.52\n79.65 84.06 76.26\n65.67 52.53\n94.11 70.47\n53.51\n3.85\nCTX+SimCLR Eps\n63.79 80.83\n82.05 82.01 75.76\n68.84 52.01\n94.62 75.01\n52.76\n3.05\nCTX+SimCLR Eps+Aug\n62.76 82.21\n79.49 80.63 75.57\n72.68 51.58\n95.34 82.65\n59.90\n2.25\nCTX+SimCLR Eps+Aug+LR\n62.25 82.03\n77.41 76.66 80.29\n72.24 49.39\n93.05 75.25\n60.35\n3.40\nDTD, however, is more challenging for basic CTX, which is unsurprising since textures have little of\nthe kind of spatial correspondence that CTX attempts to ﬁnd. COCO is also challenging, likely due\nto its extremely large intra-class variation (e.g., occlusion) and the fact that many categories overlap\nwith ImageNet-train categories, meaning that simply memorizing categories from the training set\nmay be more useful than using test-time appearance. To explore this trade-off, we applied logistic\nregression at test time to a globally pooled feature (see the extended version [1]), which provides\nadditional logits that are averaged with the CTX logits. We see non-trivial improvements on DTD\nby using this, although we sacriﬁce some performance on other datasets, such as Signs and Aircraft.\nThis implies that there’s a fundamental tension between learning categories based on global features,\nand decomposing the task into local features. More work is needed to better combine these two ideas.\nFinally, Figure 3 depicts the correspondence inferred by the CrossTransformer. The attention is often\nsemantically meaningful: object parts are well matched, including heads, bodies, feet, engines, and\nstrings. The attention is often not one-to-one either: for the ﬂower, the single query ﬂower is matched\nto multiple ﬂowers in some of the support images. Furthermore, the matching works even when\nthe ﬁne-grained classes are not the same, such as the different species of birds, suggesting that the\nattention is indeed a coarse-grained matching that has not overﬁt to the training-set classes.\n5\nConclusion\nWithin a single domain, deep networks have a remarkable ability to compose and reuse features in or-\nder to achieve statistical efﬁciency. However, this work shows the hidden problem with such systems:\nthe networks compose features in a way that conﬂates images which have different appearance but the\nsame label, i.e., it loses information about intra-class variation that may be necessary to understand\nnovel classes. We propose two techniques that help resolve this problem: self-supervised learning,\nwhich prevents features from losing that intra-class variation, and CrossTransformers, which help\nneural networks classify images using local features that are more likely to generalize. However,\nthis problem is far from resolved. In particular, our algorithm provides less beneﬁt when less spatial\nstructure is available, when knowledge of train-time categories can be useful (as in, e.g., COCO), or\nwhen higher-level reasoning is required (e.g., ﬁnding conjunctions of multiple objects). Allowing this\nalgorithm to use spatial structure only where relevant remains an open problem.\n9\n",
    "Broader Impact\nThe algorithm presented in this paper most directly applies to few-shot recognition, which has\nnumerous uses in industry, including vision systems for robotics that must adapt to new objects,\nand photo-organizing software which must infer the presence of new classes of objects on-the-ﬂy.\nUnfamiliar objects are ubiquitous in many real-world vision applications due to the so-called ‘long\ntail’ [95] of objects that occur in real scenes, and therefore we expect our algorithm to improve\nthe robustness of visual recognition systems. While our current work only addresses classiﬁcation,\nmany other tasks in computer vision, such as object detection and segmentation, use neural network\nrepresentations that can likewise be made more robust using the kind of architectures presented here.\nOur algorithm attempts to build representations which factorize the object recognition problem\ninto sub-problems (feature correspondence and feature comparison) that will each transfer correctly\nto new datasets. We hope that further research in this direction may help address dataset biases,\nincluding biases regarding race, gender, or other attributes [98], by helping to disentangle the truly\nmeaningful traits from the spurious correlations. Finally, while this algorithm presents an advance\nto state-of-the-art in understanding rare objects, the general performance of such systems is still far\nbelow human performance. For safety-critical applications (e.g., surgery or self-driving cars), relying\non the ability of vision systems to correctly interpret unusual situations is risky with current systems,\neven with the advances presented here.\nFunding Disclosure\nThis work was funded by DeepMind.\nAcknowledgments\nThe authors would like to thank Pascal Lamblin for help with Meta-Dataset, Olivier Hénaff for\nhelp with SimCLR, Yonglong Tian for help in reproducing baselines, and Relja Arandjelovi´c for\ninvaluable advice on the paper. They are also grateful to Jean-Baptiste Alayrac, Joao Carreira,\nMateusz Malinowski, Viorica P˘atr˘aucean, Adria Recasens, and Lucas Smaira for helpful discussions,\nsupport, and feedback on the project.\nReferences\n[1] Supplementary material (appendix) for the paper. http://arxiv.org/abs/2007.11498.\n[2] M. Andrychowicz, M. Denil, S. Gomez, M. W. Hoffman, D. Pfau, T. Schaul, B. Shillingford, and\nN. De Freitas. Learning to learn by gradient descent by gradient descent. In NeurIPS, 2016.\n[3] P. Bachman, R. D. Hjelm, and W. Buchwalter. Learning representations by maximizing mutual information\nacross views. In NeurIPS, 2019.\n[4] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate.\nIn Proc. ICLR, 2015.\n[5] E. Bart and S. Ullman. Cross-generalization: Learning novel classes from a single example by feature\nreplacement. In Proc. CVPR, 2005.\n[6] S. Bengio, Y. Bengio, J. Cloutier, and J. Gecsei. On the optimization of a synaptic learning rule. In\nPreprints Conf. Optimality in Artiﬁcial and Biological Neural Networks, volume 2. Univ. of Texas, 1992.\n[7] Y. Bengio, S. Bengio, and J. Cloutier. Learning a synaptic learning rule. Citeseer, 1990.\n[8] L. Bertinetto, J. F. Henriques, P. H. Torr, and A. Vedaldi. Meta-learning with differentiable closed-form\nsolvers. In Proc. ICLR, 2019.\n[9] L. Bertinetto, J. F. Henriques, J. Valmadre, P. Torr, and A. Vedaldi. Learning feed-forward one-shot\nlearners. In NeurIPS, 2016.\n[10] L. Bourdev and J. Malik. Poselets: Body part detectors trained using 3d human pose annotations. In Proc.\nICCV, 2009.\n[11] J. Bromley, I. Guyon, Y. LeCun, E. Säckinger, and R. Shah. Signature veriﬁcation using a\" siamese\" time\ndelay neural network. In NeurIPS, 1994.\n[12] A. M. Bronstein, M. M. Bronstein, A. M. Bruckstein, and R. Kimmel. Partial similarity of objects, or\nhow to compare a centaur to a horse. Proc. ICCV, 2009.\n[13] B. Cao, A. Araujo, and J. Sim. Unifying deep local and global features for image search. In Proc. ECCV,\n2020.\n[14] M. Caron, P. Bojanowski, A. Joulin, and M. Douze. Deep clustering for unsupervised learning of visual\nfeatures. In Proc. ECCV, 2018.\n10\n",
    "[15] K. Chatﬁeld, K. Simonyan, and A. Zisserman. Efﬁcient on-the-ﬂy category retrieval using convnets and\ngpus. In Asian Conference on Computer Vision, 2014.\n[16] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning of visual\nrepresentations. In Proc. ICML, 2020.\n[17] Y. Chen, X. Wang, Z. Liu, H. Xu, and T. Darrell. A new meta-baseline for few-shot learning. arXiv\npreprint arXiv:2003.04390, 2020.\n[18] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity metric discriminatively, with application to\nface veriﬁcation. In Proc. CVPR, 2005.\n[19] J. Cortés. Finite-time convergent gradient ﬂows with applications to network consensus. Automatica,\n42(11), 2006.\n[20] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le. Autoaugment: Learning augmentation\nstrategies from data. In Proc. CVPR, 2019.\n[21] G. S. Dhillon et al. A baseline for few-shot image classiﬁcation. Proc. ICLR, 2020.\n[22] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised visual representation learning by context prediction.\nIn Proc. ICCV, 2015.\n[23] C. Doersch, S. Singh, A. Gupta, J. Sivic, and A. A. Efros. What makes paris look like paris? Proc. ACM\nSIGGRAPH, 31(4), 2012.\n[24] C. Doersch and A. Zisserman. Multi-task self-supervised visual learning. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pages 2051–2060, 2017.\n[25] A. Dosovitskiy, J. T. Springenberg, M. Riedmiller, and T. Brox. Discriminative unsupervised feature\nlearning with convolutional neural networks. In NeurIPS. 2014.\n[26] L. Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object categories. IEEE PAMI, 2006.\n[27] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively\ntrained part-based models. IEEE PAMI, 32(9), 2009.\n[28] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In\nProc. ICML, 2017.\n[29] V. Garcia and J. Bruna. Few-shot learning with graph neural networks. In Proc. ICLR, 2018.\n[30] S. Gidaris, A. Bursuc, N. Komodakis, P. Perez, and M. Cord. Boosting few-shot visual learning with\nself-supervision. In The IEEE International Conference on Computer Vision (ICCV), October 2019.\n[31] S. Gidaris, P. Singh, and N. Komodakis. Unsupervised representation learning by predicting image\nrotations. In Proc. ICLR, 2018.\n[32] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection\nand semantic segmentation. In Proc. CVPR, 2014.\n[33] A. Giusti, D. C. Cire¸san, J. Masci, L. M. Gambardella, and J. Schmidhuber. Fast image scanning with\ndeep max-pooling convolutional neural networks. In Intl. Conf. Image Proc., 2013.\n[34] D. Ha, A. Dai, and Q. V. Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016.\n[35] X. Han, T. Leung, Y. Jia, R. Sukthankar, and A. C. Berg. Matchnet: Unifying feature and metric learning\nfor patch-based matching. In Proc. CVPR, 2015.\n[36] B. Hariharan and R. Girshick. Low-shot visual recognition by shrinking and hallucinating features. In\nProc. CVPR, 2017.\n[37] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation\nlearning. In Proc. CVPR, 2020.\n[38] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proc. CVPR, 2016.\n[39] S. Hochreiter, A. S. Younger, and P. R. Conwell. Learning to learn using gradient descent. In International\nConference on Artiﬁcial Neural Networks. Springer, 2001.\n[40] M. Holschneider, R. Kronland-Martinet, J. Morlet, and P. Tchamitchian. A real-time algorithm for signal\nanalysis with the help of the wavelet transform. In Wavelets, pages 286–297. 1990.\n[41] D. Jacobs, D. Weinshall, and Y. Gdalyahu. Class representation and image retrieval with non-metric\ndistances. IEEE PAMI, 22(6):583–600, 2000.\n[42] J. Y. Jason, A. W. Harley, and K. G. Derpanis. Back to basics: Unsupervised learning of optical ﬂow via\nbrightness constancy and motion smoothness. In Proc. CVPR, 2016.\n[43] M. Juneja, A. Vedaldi, C. Jawahar, and A. Zisserman. Blocks that shout: Distinctive parts for scene\nclassiﬁcation. In Proc. CVPR, 2013.\n[44] Ł. Kaiser, O. Nachum, A. Roy, and S. Bengio. Learning to remember rare events. In Proc. ICLR, 2017.\n[45] G. Koch, R. Zemel, and R. Salakhutdinov. Siamese neural networks for one-shot image recognition. In\nICML deep learning workshop, volume 2. Lille, 2015.\n[46] A. Kolesnikov, X. Zhai, and L. Beyer. Revisiting self-supervised visual representation learning. In Proc.\nCVPR, 2019.\n[47] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level concept learning through probabilistic\nprogram induction. Science, 350, 2015.\n[48] G. Larsson, M. Maire, and G. Shakhnarovich. Learning representations for automatic colorization. In\nProc. ECCV, 2016.\n[49] H. Li, G. Hua, Z. Lin, J. Brandt, and J. Yang. Probabilistic elastic matching for pose variant face\nveriﬁcation. In Proc. CVPR, 2013.\n[50] Y. Lifchitz, Y. Avrithis, S. Picard, and A. Bursuc. Dense classiﬁcation and implanting for few-shot\nlearning. In Proc. CVPR, 2019.\n[51] P. Liu, M. Lyu, I. King, and J. Xu. Selﬂow: Self-supervised learning of optical ﬂow. In Proc. CVPR,\n2019.\n11\n",
    "[52] D. Maclaurin, D. Duvenaud, and R. Adams. Gradient-based hyperparameter optimization through\nreversible learning. In International Conference on Machine Learning, 2015.\n[53] S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi. Fine-grained visual classiﬁcation of aircraft.\narXiv preprint arXiv:1306.5151, 2013.\n[54] E. G. Miller, N. E. Matsakis, and P. A. Viola. Learning from one example through shared densities on\ntransforms. In Proc. CVPR, 2000.\n[55] N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel. A simple neural attentive meta-learner. In Proc.\nICLR, 2017.\n[56] I. Misra and L. v. d. Maaten. Self-supervised learning of pretext-invariant representations. In Proc. CVPR,\n2020.\n[57] T. Munkhdalai and H. Yu. Meta networks. In Proc. ICML, 2017.\n[58] D. K. Naik and R. J. Mammone. Meta-neural networks that learn by learning. In [Proceedings 1992]\nIJCNN International Joint Conference on Neural Networks, volume 1. IEEE, 1992.\n[59] Y. E. Nesterov. Minimization methods for nonsmooth convex and quasiconvex functions. Matekon, 29,\n1984.\n[60] J. Nichol, Alex any Andrychowicz ed Achiam and J. Schulman. On ﬁrst-order meta-learning algorithms.\narXiv preprint arXiv:1803.02999, 2018.\n[61] M. Noroozi and P. Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In\nProc. ECCV, 2016.\n[62] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning and transferring mid-level image representations\nusing convolutional neural networks. In Proc. CVPR, 2014.\n[63] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville. Film: Visual reasoning with a general\nconditioning layer. In Proc. AAAI, 2018.\n[64] S. Ravi and H. Larochelle. Optimization as a model for few-shot learning. In Proc. ICLR, 2017.\n[65] S.-A. Rebufﬁ, H. Bilen, and A. Vedaldi. Learning multiple visual domains with residual adapters. In\nNeurIPS, 2017.\n[66] J. Requeima, J. Gordon, J. Bronskill, S. Nowozin, and R. E. Turner. Fast and ﬂexible multi-task\nclassiﬁcation using conditional neural adaptive processes. In NeurIPS, 2019.\n[67] I. Rocco, M. Cimpoi, R. Arandjelovi´c, A. Torii, T. Pajdla, and J. Sivic. Neighbourhood consensus\nnetworks. In NeurIPS, pages 1651–1662, 2018.\n[68] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla,\nM. Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 115(3), 2015.\n[69] T. Saikia, T. Brox, and C. Schmid. Optimized generic feature learning for few-shot classiﬁcation across\ndomains. arXiv preprint arXiv:2001.07926, 2020.\n[70] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and T. Lillicrap. Meta-learning with memory-\naugmented neural networks. In Proc. ICML, 2016.\n[71] S. Savarese, J. Winn, and A. Criminisi. Discriminative object class models of appearance and shape by\ncorrelatons. In Proc. CVPR, 2006.\n[72] J. Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: the\nmeta-meta-... hook. PhD thesis, Technische Universität München, 1987.\n[73] J. Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks.\nNeural Computation, 4(1):131–139, 1992.\n[74] J. Schmidhuber. A neural network that embeds its own meta-levels. In IEEE International Conference on\nNeural Networks, pages 407–412. IEEE, 1993.\n[75] J. Sivic and A. Zisserman. Video Google: A text retrieval approach to object matching in videos. In Proc.\nICCV, 2003.\n[76] J. Snell, K. Swersky, and R. Zemel. Prototypical networks for few-shot learning. In NeurIPS, 2017.\n[77] P. Sprechmann, S. M. Jayakumar, J. W. Rae, A. Pritzel, A. P. Badia, B. Uria, O. Vinyals, D. Hassabis,\nR. Pascanu, and C. Blundell. Memory-based parameter adaptation. In Proc. ICLR, 2018.\n[78] J.-C. Su, S. Maji, and B. Hariharan. When does self-supervision improve few-shot learning? In Proc.\nECCV, 2020.\n[79] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and T. M. Hospedales. Learning to compare: Relation\nnetwork for few-shot learning. In Proc. CVPR, 2018.\n[80] S. Thrun. Lifelong learning algorithms. In Learning to learn. Springer, 1998.\n[81] S. Thrun and L. Pratt. Learning to learn. Springer Science & Business Media, 1998.\n[82] Y. Tian, D. Krishnan, and P. Isola. Contrastive multiview coding. arXiv preprint arXiv:1906.05849, 2019.\n[83] Y. Tian, Y. Wang, D. Krishnan, J. B. Tenenbaum, and P. Isola. Rethinking few-shot image classiﬁcation:\na good embedding is all you need? In Proc. ECCV, 2020.\n[84] P. Tokmakov, Y.-X. Wang, and M. Hebert. Learning compositional representations for few-shot recogni-\ntion. In Proc. ICCV, 2019.\n[85] G. Tolias, T. Jenicek, and O. Chum. Learning and aggregating deep local descriptors for instance-level\nrecognition. In Proc. ECCV, 2020.\n[86] E. Triantaﬁllou, T. Zhu, V. Dumoulin, P. Lamblin, U. Evci, K. Xu, R. Goroshin, C. Gelada, K. J. Swersky,\nP.-A. Manzagol, and H. Larochelle. Meta-dataset: A dataset of datasets for learning to learn from few\nexamples. In Proc. ICLR, 2020.\n[87] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior,\nand K. Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499,\n2016.\n12\n",
    "[88] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.\nAttention is all you need. In NeurIPS, 2017.\n[89] R. C. Veltkamp. Shape matching: Similarity measures and algorithms. In International Conference on\nShape Modeling and Applications, 2001.\n[90] R. Vilalta and Y. Drissi. A perspective view and survey of meta-learning. Artiﬁcial intelligence review,\n18, 2002.\n[91] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and D. Wierstra. Matching networks for one shot\nlearning. In NeurIPS, 2016.\n[92] C. Vondrick, A. Shrivastava, A. Fathi, S. Guadarrama, and K. Murphy. Tracking emerges by colorizing\nvideos. In Proc. ECCV, 2018.\n[93] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 dataset.\n2011.\n[94] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural networks. In Proc. CVPR, 2018.\n[95] Y.-X. Wang, D. Ramanan, and M. Hebert. Learning to model the tail. In NeurIPS, 2017.\n[96] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin. Unsupervised feature learning via non-parametric instance\ndiscrimination. In Proc. CVPR, 2018.\n[97] W. Xie, L. Shen, and A. Zisserman. Comparator networks. In Proc. ECCV, 2018.\n[98] K. Yang, K. Qinami, L. Fei-Fei, J. Deng, and O. Russakovsky. Towards fairer datasets: Filtering and\nbalancing the distribution of the people subtree in the imagenet hierarchy. In Conference on Fairness,\nAccountability, and Transparency, 2020.\n[99] A. S. Younger, S. Hochreiter, and P. R. Conwell. Meta-learning with backpropagation. In IJCNN’01.\nInternational Joint Conference on Neural Networks. Proceedings (Cat. No. 01CH37222), volume 3. IEEE,\n2001.\n[100] S. Zagoruyko and N. Komodakis. Wide residual networks. In Proc. BMVC., 2016.\n[101] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena. Self-attention generative adversarial networks. In\nProc. ICML, 2019.\n[102] J. Zhang, M. Marszałek, S. Lazebnik, and C. Schmid. Local features and kernels for classiﬁcation of\ntexture and object categories: A comprehensive study. IJCV, 73(2), 2007.\n[103] R. Zhang, P. Isola, and A. A. Efros. Colorful image colorization. In Proc. ECCV. Springer, 2016.\n[104] R. Zhang, P. Isola, and A. A. Efros. Split-brain autoencoders: Unsupervised learning by cross-channel\nprediction. In Proc. CVPR, 2017.\n[105] H. Zheng, J. Fu, T. Mei, and J. Luo. Learning multi-attention convolutional neural network for ﬁne-grained\nimage recognition. In Proc. ICCV, 2017.\n13\n"
  ],
  "full_text": "CrossTransformers: spatially-aware few-shot transfer\nCarl Doersch∗\nAnkush Gupta∗\nAndrew Zisserman∗†\n∗DeepMind, London\n† VGG, Department of Engineering Science, University of Oxford\nAbstract\nGiven new tasks with very little data—such as new classes in a classiﬁcation\nproblem or a domain shift in the input—performance of modern vision systems\ndegrades remarkably quickly. In this work, we illustrate how the neural network\nrepresentations which underpin modern vision systems are subject to supervision\ncollapse, whereby they lose any information that is not necessary for performing\nthe training task, including information that may be necessary for transfer to new\ntasks or domains. We then propose two methods to mitigate this problem. First, we\nemploy self-supervised learning to encourage general-purpose features that transfer\nbetter. Second, we propose a novel Transformer based neural network architecture\ncalled CrossTransformers, which can take a small number of labeled images and\nan unlabeled query, ﬁnd coarse spatial correspondence between the query and the\nlabeled images, and then infer class membership by computing distances between\nspatially-corresponding features. The result is a classiﬁer that is more robust to\ntask and domain shift, which we demonstrate via state-of-the-art performance on\nMeta-Dataset, a recent dataset for evaluating transfer from ImageNet to many other\nvision datasets. Code available at: https://github.com/google-research/\nmeta-dataset.\n1\nIntroduction\nGeneral-purpose vision systems must be adaptable. Home robots must be able to operate in new,\nunseen homes; photo-organizing software must recognize unseen objects (e.g., to ﬁnd examples of\n“my sixth-grade son’s abstract art project”); industrial quality-assurance systems must spot defects in\nnew products. Deep neural network representations can bring some visual knowledge from datasets\nlike ImageNet [68] to bear on different tasks beyond ImageNet [15, 32, 62], but empirically, this\nrequires a non-trivial amount of labeled data in the new task. With too little labeled data, or for a\nlarge change in distribution, such systems empirically perform poorly.\nResearch on meta-learning directly benchmarks adaptability. At training time, the algorithm receives\na large amount of data and accompanying supervision (e.g., labels). At test time, however, the\nalgorithm receives a series of episodes, each of which consists of a small number of datapoints from a\ndifferent distribution than the training set (e.g., a different domain or different classes). Only a subset\nof this data has the accompanying supervision (called the support set); the algorithm must make\npredictions about the rest (the query set). Meta-Dataset [86] is particularly relevant for vision, as the\nchallenge is few-shot ﬁne-grained image classiﬁcation. The training data is a subset of ImageNet\nclasses. At test time, each episode either contains images from the other ImageNet classes, or from\none of nine other visually distinct ﬁne-grained recognition datasets. The algorithm must rapidly adapt\nits representations to the new classes and domains.\nSimple centroid-based algorithms like Prototypical Nets [17, 76] are near state-of-the-art on Meta-\nDataset, achieving around 50% accuracy on the held-out ImageNet classes in Meta-Dataset’s valida-\ntion set (chance is roughly 1 in 20). An equivalent classiﬁer trained on those validation classes can\nachieve roughly 84% accuracy on the same challenge. What accounts for the enormous discrepancy\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\n\n\nQuery\nNearest Neighbors\ngila monster\ngila monster hognose snake gila monster\ngila monster\ngila monster\nnight snake\nhognose snake gila monster\nknot\nletter opener\nletter opener\nscrewdriver\nletter opener\nballpoint\nfountain pen\nbuckle\nletter opener\nletter opener\nbassoon\nhammer\nhammer\nmicrophone\nhammer\nscrew\nhammer\nsyringe\nshovel\nhook\nscrew\nbuckeye\nbuckeye\nbuckeye\nhip\npomegranate\nsunglasses\nbuckeye\nbuckeye\nbuckeye\nscrew\n  : in test split\nFigure 1: Illustration of supervision collapse with nearest neighbors. In each row, the leftmost\nimage is a query taken from the Meta-Dataset ImageNet test classes, and the rest are the top 9\nnearest neighbors from both training and test support set classes, using the embedding learned by a\nPrototypical Net (training details in the extended version [1]). Images belonging to the test split are\nindicated by a\nnear the bottom left corner; rest are from the training split. For a simple classiﬁer\nto work well on these test classes, semantically similar images should have similar representations,\nand so we hope the nearest neighbors would come from the same—or semantically similar—classes.\nInstead, we observe that only 5% of matches for test-set queries are from the same class as the query.\nFurthermore, many matches are all from the same incorrect training class (highlighted in red). We\nsee a knot is matched with several gila monsters (and other reptiles); a bassoon with letter openers\n(and pens); a screw with hammers; another screw with buckeyes. The errors within that wrong class\noften have widely different appearance: for example, the bottom-most screw is matched with single\nbuckeyes and also a pile of buckeyes. One interpretation is that the network picks up on image\npatterns during training that allow images of each class to be tightly grouped in the feature space,\nminimizing other ways that the image might be similar to other classes in preparation for a conﬁdent\nclassiﬁcation. For out-of-domain samples, the network can then overemphasize a spurious image\npattern that suggests membership in one training-set class. This is the consequence of supervision\ncollapse, where image patterns that might help make the correct associations are lost.\nbetween performance on within-distribution samples and out-of-distribution samples? We hypothe-\nsize that because the neural network backbone of Prototypical Nets is designed for classiﬁcation, they\ndo just this: represent only an image’s (training-set) class, and discard information that might help\nwith out-of-distribution classes. Doing so minimizes the losses for many meta-learning algorithms,\nincluding Prototypical Nets. We call this problem supervision collapse, and illustrate it in Figure 1.\nOur ﬁrst contribution is to explore using self-supervision to overcome supervision collapse. We\nemploy SimCLR [16], which learns embeddings that discriminate between every image in the dataset\nwhile maintaining invariance to transformations (e.g., cropping and color shifts), thus capturing more\nthan just classes. However, rather than treat SimCLR as an auxiliary loss, we reformulate SimCLR as\n“episodes” that can be classiﬁed in the same manner as a training episode.\nOur second contribution is a novel architecture called CrossTransformers, which extends Trans-\nformers [88] to few-shot ﬁne-grained classiﬁcation. Our key insight is that objects and scenes are\ngenerally composed of smaller parts, with local appearance that may be similar to what has been seen\nat training time. The classical example of this is the centaur that appeared in several early papers on\nvisual representation [12, 41, 89], where the parts from the human and horse composed the centaur.\nCrossTransformers operationalize this insight of (i) local part-based comparisons, and (ii) accounting\nfor spatial alignment, resulting in a procedure for comparing images which is more agnostic to the\nunderlying classes. In more detail, ﬁrst a coarse alignment between geometric or functional parts in\nthe query- and support-set images is established using attention as in Transformers. Then, given this\nalignment, distances between corresponding local features are computed to inform classiﬁcation. We\ndemonstrate this improves generalization to unseen classes and domains.\n2\n\n\nIn summary, our contributions in this paper are: (i) We improve the robustness of our local features\nwith a self-supervised technique, modifying the state-of-the-art SimCLR [16] algorithm. (ii) We\npropose the CrossTransformer, a network architecture that is spatially aware and performs few-shot\nclassiﬁcation using more local features, which improves transfer. Finally, (iii) we evaluate and\nablate how the choices in these algorithms impact Meta-Dataset [86] performance, and demonstrate\nstate-of-the-art results on nearly every dataset within it, often by large margins.\n2\nRelated Work\nFew-shot image classiﬁcation.\nFew-shot learning [26, 36, 47, 54] has recently been primarily\naddressed in the meta-learning framework [58, 72, 81], where a model learns an update rule for\nthe parameters of a base-learner model [6, 7, 73] through a sequence of training episodes [80, 90].\nThe meta-learner either learns to produce new parameters directly from the new data [9, 34, 57, 63,\n65, 73, 74], or learns to produce an update rule to iteratively optimize the base learner to ﬁt the\nnew data [2, 6, 8, 39, 64, 99]. [28, 52, 60] do not use any explicit meta-learner model, but instead\nunroll the base-learner gradient updates and optimize for model initializations which generalize\nwell on novel tasks. Matching-based methods [29, 76, 79, 91] instead learn representations for\nsimilarity functions [11, 17, 18, 45, 83], in the hope that the similarities will generalize to new\ndata. CrossTransformers fall in this category, and share much of their architecture with Prototypical\nNets [76].\nAttention for few-shot learning.\nCrossTransformers attend [4] individually over each class’s\nsupport set to establish local correspondences, whereas Matching Networks [91] attend over the\nwhole support set to “point to” matching instances. [55] extend this idea to larger contexts using\ntemporally dilated convolutions [87]. In the limit, attention over long-term experiences accumulated\nin memories [44, 57, 70, 77] can augment more traditional learning.\nCorrespondences for visual recognition.\nCrossTransformers perform classiﬁcation by matching\nmore local parts. Discriminative parts [5, 10, 23, 35, 43, 84] and visual words [75, 102] have a\nrich history, and have found applications in deformable-parts models [27], classiﬁcation [71, 102],\nand retrieval [13, 85]. Part-based correspondences for recognition [105] have been particularly\nsuccessful in ﬁne-grained face retrieval and recognition [49, 97]. CrossTransformers establish\nsoft correspondences between pixels in the query- and support-set images; such dense pairwise\ninteractions [94] have recently proved useful for generative networks [101], semantic matching [67]\nand tracking [92]. [50] learns spatially dense classiﬁers for few-shot classiﬁcation, but pools the\nspatial dimensions of the prototypes, and hence does not have a notion of correspondence.\nSelf-supervised learning for few-shot.\nOur work on SimCLR episodes inherits from a line of\nself-supervised learning research, which typically deal with transfer from pretext tasks to semantic\nones and must therefore represent more than their training data [3, 14, 16, 22, 25, 31, 37, 48,\n61, 103, 104]. Some recent works [30, 78] demonstrate that this can improve few-shot learning,\nalthough these use self-supervised auxiliary losses rather than integrating self-supervised instance\ndiscrimination [16, 25, 56, 82, 96] into episodic training. Also particularly relevant are methods\nthat use self-supervision for correspondence [42, 51, 92], which may in future work improve the\ncorrespondences that CrossTransformers use.\n3\nStopping Collapse: SimCLR Episodes and CrossTransformers\nWe take a two-pronged approach to dealing with the supervision collapse problem. Modern ap-\nproaches to few-shot learning typically involve learning an embedding for each image, followed by a\nclassiﬁer that aggregates information across an episode’s support set in order to classify the episode’s\nqueries. Our ﬁrst step aims to use self-supervised learning to improve the embedding so it expresses\ninformation beyond the classes, in a way that is as algorithm-agnostic as possible. Once we have these\nembeddings, we build a classiﬁer called a CrossTransformer. CrossTransformers use Prototypical\nNets [76] as a blueprint, chosen due to their simplicity and strong performance; the main modiﬁcation\nis to aggregate information in a spatially-aware way. We begin by reviewing Prototypical Nets, and\nthen describe the two approaches.\n3\n\n\nPrototypical Nets are episodic learners, which means training is performed on the same kind of\nepisodes that will be presented at test time: a query set Q of images, and a support set S which can\nbe partitioned into classes c ∈{1, 2, . . . , C}: each Sc = {xc\ni}N\ni=1 is composed of N example images\nxc\ni. Prototypical Nets learn a distance function between the query and each subset Sc. Both the\nquery- and support-set images are ﬁrst encoded into a D-dimensional representation Φ(x), using a\nshared ConvNet Φ : RH×W ×3 7→RD, where H, W are the height and width respectively. Then a\n“prototype” tc ∈RD for the class c is obtained by averaging the representations of the support set Sc,\ntc =\n1\n|Sc|\nP\nx∈Sc Φ(x). Finally, a distribution of classes is obtained using softmax over the distances\nbetween the query image and class prototypes: p(y = c|xq) =\nexp(−d(Φ(xq),tc))\nPC\nc′=1 exp(−d(Φ(xq),tc′)). In practice,\nthe distance function d is ﬁxed to be the squared Euclidean distance d(xq, Sc) = ||Φ(xq) −tc||2\n2.\nThe learning objective is to train the embedding network Φ to maximize the probability of the correct\nclass for each query.\n3.1\nSelf-supervised training with SimCLR\nOur ﬁrst challenge is to improve the neural network embedding Φ: after all, if these features have\ncollapsed to represent little information beyond the classes, then a subsequent classiﬁer cannot can\nrecover this information. But how can we train features to represent things beyond labels when our\nonly supervision is the labels? Our solution is self-supervised learning, which invents “pretext tasks”\nthat train representations without labels [22, 25], and better yet, has a reputation for representations\nthat transfer beyond this pretext task. Speciﬁcally we use SimCLR [16], which uses “instance\ndiscrimination” as a pretext task. It works by applying random image transformations (e.g., cropping\nor color shifts) twice to the same image, thus generating two “views” of that image. Then it trains the\nnetwork so that representations of the two views of the same image are more similar to each other\nthan they are to those of different images. Empirically, networks trained in this way become sensitive\nto semantic information, but also learn to discriminate between different images within a single class,\nwhich is useful for combating supervision collapse.\nWhile we could treat SimCLR as an auxiliary loss on the embedding, we instead reformulate SimCLR\nas episodic learning, so that the technique can be applied to all episodic learners with minimal\nhyper-parameters. To do this, we randomly convert 50% of the training episodes into what we call\nSimCLR episodes, by treating every image as its own class. For clarity, we will call the original\nepisodes that have not been converted SimCLR episodes MD-categorization episodes, to emphasize\nthat they use the original categories from Meta-Dataset. Speciﬁcally, let ρ(·) be SimCLR’s (random)\nimage transformation function, and let S = {xi}|S|\ni=1 be a training support set. We generate a\nSimCLR episode by sampling a new support set, transforming each image in the original support\nset S′ = {ρ(xi)}|S|\ni=1, and then generating query images by sampling other transformations from the\nsame support set: Q′ = {ρ(random_sample(S))}|Q|\ni=1, where random_sample just takes a random\nimage from the set.1 The original query set Q is discarded. The label for an image in the SimCLR\nepisode is its index in the original support set, resulting in an |S|-way classiﬁcation for each query.\nNote that for a SimCLR episode, the ‘prototypes’ in Prototypical Nets average over just a single\nimage, and therefore the Prototypical Net loss can be written as\nexp(−d(Φ(ρ(xq)),Φ(ρ(xq))))\nPn\ni=1 exp(−d(Φ(ρ(xq)),Φ(ρ(xi))). If we\ndeﬁne d as the cosine distance rather than Euclidean, this loss is identical to the one used in SimCLR.\n3.2\nCrossTransformers\nGiven a query image xq and a support set Sc = {xc\ni}N\ni=1 for the class c, CrossTransformers aim to\nbuild a representation which enables local part-based comparisons between them.\nCrossTransformers start by making the image representation a spatial tensor, and then assemble\nquery-aligned class prototypes by putting the support-set images Sc in correspondence with the\nquery image. The distance between the query image and the query-aligned prototype for each class\nis then computed and used in a similar way to Prototypical Nets. In practice, we establish soft\ncorrespondences using attention [4] based Transformers [88]. In contrast, Prototypical Nets use ﬂat\nvector representations which lose the location of image features, and have a ﬁxed class prototype\nwhich is independent of the query image.\n1We enforce that the sampled queries have the same class distribution as Q, and have no repeats.\n4\n\n\np\nΓ\nΓ\nΩ\nm\nn\nDot-product query features at location \np against all support set (Sc) spatial \nfeatures in class\nΛ\nΛ\nSoftmax across all\nspatial features in class\np\nKey Heads\nValue Heads\nQuery-aligned\nprototype (tc)\nweighted sum\nSoftmax normalized\nattention weights\nSupport-set (Sc) image \nfeatures for the category c\nQuery Head\ndv\ndv\ndk\ndk\nQuery image \nfeatures\nm\nn\na2\n    c\na1\n    c\n~a2\n    c\n~a1\n    c\nKeys (kc)\nValues (vc)\nQueries (q)\nFigure 2: CrossTransformers. Construction of query-aligned class prototype vector tc\np for the class\nc and the query image xq, focusing on the spatial location p in xq. The query vector qp is compared\nagainst keys kc from all spatial locations in the support set Sc to obtain attention scores ac, which are\nsoftmax normalized before being used to aggregate the values vc for the aligned prototype vector tc\np.\nConcretely, CrossTransformers remove the ﬁnal spatial pooling in Prototypical Nets’ embedding\nnetwork Φ(·), such that the spatial dimensions H′, W ′ are preserved: Φ(x) ∈RH′×W ′×D. Following\nTransformers, key-value pairs are then generated for each image in the support set using two\nindependent linear maps: the key-head Γ : RD 7→Rdk, and the value-head Λ : RD 7→Rdv\nrespectively. Similarly, the query image features Φ(xq) are embedded using the query-head Ω:\nRD 7→Rdk. Dot-product attention scores are then obtained between keys and queries, followed by\nsoftmax normalization across all the images and locations in Sc. This attention serves as our coarse\ncorrespondence (see example attention visualizations in Figure 3 and the extended version [1]), and\nis used to aggregate the support-set features into alignment with the query. This process is visualized\nin Figure 2.\nMathematically, let kc\njm = Γ · Φ(xc\nj)m be the key for the jth image in the support set for class c at\nspatial position m (index over the two dimensions H′, W ′), and similarly let qp = Ω· Φ(xq)p be the\nquery vector at spatial position p in the query image xq. The attention ˜ac\njmp ∈R between the two is\nthen obtained as:\n˜ac\njmp =\nexp(ac\njmp/τ)\nP\ni,n exp(ac\ninp/τ),\nwhere\nac\njmp = kc\njm · qp,\nand\nτ =\np\ndk.\n(1)\nNext, the aligned prototype vector tc\np corresponding to spatial location p in the query is obtained by\naggregating the support-set values vc\njm = Λ · Φ(xc\nj)m using the attention weights above:\ntc\np =\nX\njm\n˜ac\njmpvc\njm\n(2)\nFinally, squared Euclidean distances between aligned local features from the above prototype and\ncorresponding query image values wp = Λ · Φ(xq)p are aggregated as below. This scalar distance\nacts as a negative logit for a distribution over classes as in Prototypical Nets.\nd(xq, Sc) =\n1\nH′W ′\nX\np\n||tc\np −wp||2\n2\n(3)\nNote we apply the same value-head Λ to both the query and support-set images. This ensures that\nthe CrossTransformer behaves somewhat like a distance. That is, imagine a trivial case where, for\none class, all images in Sc are identical to xq. We would want d(xq, Sc) to approach 0 even if the\nnetwork is untrained, or if these images are highly dissimilar from those used for training. Sharing Λ\nbetween the support and query sets helps accomplish this: in fact, if ˜ac\njmp is 1 where p = m and 0\nelsewhere for all j, then d(xq, Sc) will be identically 0 under this architecture, no matter the network\n5\n\n\nQuery\nCorrespondence in support set\nQuery\nCorrespondence in support set\nFigure 3: Visualization of the attention ˜a. We show four query images, along with three support-set\nimages for each. Within each query image, we choose three spatial locations (red, green, and blue\nsquares), and plot the CrossTransformer attention weights for each one in the corresponding color\n(brighter colors mean higher weight). The four examples are from Aircraft, CU-Birds, VGG Flowers,\nand ImNet test sets respectively (clockwise, starting from top-left). No matter which dataset, the\nattention masks are semantically relevant, even when the correspondence is not one-to-one. More\nvisualizations are given in the extended version [1].\nweights. To encourage this behavior for the attention ˜a, we also set Γ = Ω, i.e., the key and query\nheads are the same. This way, in our trivial case, the attention is likely to be maximal for spatial\nlocations that correspond, because kc\njm and qp will be the same for p = m.\nFor one experiment, we also augment the CrossTransformer with a global feature, which can help for\nsome datasets like DTD (Describable Textures Dataset) with less spatial structure.\n4\nExperiments\nWe evaluate on Meta-Dataset [86], speciﬁcally the setting where the training is performed on the\nImageNet train split only, which is 712 classes (plus 158 classes for validation, which are not used\nfor training but only to perform early stopping). We then test on the remaining 130 held-out classes\nfrom ImageNet, as well as 9 other image datasets. Note that this is in contrast to another popular (and\neasier) setting, where the training also uses a subset of categories from more of these datasets: usually\nall datasets except Trafﬁc Signs and COCO. For clarity, we’ll use “Meta-Dataset Train-on-ILSVRC”\nto denote training on ImageNet only, and “Meta-Dataset Train-on-all” to denote when training occurs\non more datasets. Test time consists of a series of episodes, each of which contains: (1) a support set\nbetween 50 and 500 labeled images which come from between 5 and 50 classes; and (2) an unlabeled\nquery set with 10 images per class. Meta-Dataset aims for ﬁne-grained recognition, so the classes in\neach episode are mutually similar: one episode may contain only musical instruments, another may\ncontain only birds, etc.\nMeta-Dataset is useful for studying transfer because different test datasets encapsulate different\nkinds of transfer challenges. For test datasets like CU-Birds [93], there are numerous similar classes\nin ImageNet train (20 bird classes in ImageNet train, versus 100 in the CU-birds test dataset). In\ncontrast, for test datasets like Aircraft [53], there is just a single corresponding class in ImageNet\ntrain; therefore, algorithms which don’t represent the intra-class variability for this class will be\npenalized. The ImageNet test set has images in a similar domain to the ImageNet train set but with\ndifferent classes, while test datasets like COCO contain many similar classes to ImageNet, but with\ndomain shift (in COCO, instances are generally not the subject of their photographs, and may be\nlow-resolution or occluded). Finally, test datasets like OmniGlot combine these challenges, i.e.,\ndifferent classes in a substantially different domain.\n4.1\nImplementation details\nTo ensure comparability, we followed the public implementation of Prototypical Nets for Meta-\nDataset [86] wherever possible. This includes using the same hyperparameters, unless otherwise\nnoted. For the hyperparameters that were chosen with a sweep on the validation set (learning rate\nschedule and weight decay), we simply used the best values discovered for Prototypical Nets for\nall the experiments in this paper. See the extended version [1] for details of the CrossTransformer\n6\n\n\narchitecture. We use no pretraining for CrossTransformers, although to be consistent with prior\nwork [86] we use it for the experiments involving Prototypical Nets.\nWe incorporate two improvements from Meta-Baseline [17], which at test time is similar to Proto-\ntypical Nets (though it isn’t trained as an episodic learner). The ﬁrst is to keep exponential moving\naverages for Batch Norm statistics during training, and use those for Batch Norm at test time. Second,\nwe note that Meta-Baseline does not train on ﬁne-grained episodes sampled from the ImageNet hier-\narchy, as Prototypical Nets does, but rather on batches with uniformly-sampled classes. Empirically,\nPrototypical Nets trained only on ﬁne-grained episodes struggle to do coarse-grained recognition, as\nrequired for datasets like COCO. Therefore, we only use the ImageNet hierarchy to make 50% of\nepisodes ﬁne-grained; the rest have categories sampled uniformly.\nChoice of network.\nPrior implementations of networks like Prototypical Nets use relatively small\nnetworks (e.g., ResNet-18) with small input images (e.g. 126×126 pixels), and report that measures\nto increase capacity (e.g., Wide ResNets [100]) provide minimal beneﬁts. This is surprising given that\nstandard networks show improvements for increasing capacity (e.g., ResNet-34 outperforms ResNet-\n18 by 3% on ImageNet [38]). Making our networks spatially-aware requires higher-resolution, and\nalso higher-capacity networks are especially important in self-supervised learning [24, 46]. Therefore,\nour experiments increase resolution to the standard 224×224 and use ResNet-34, and we also use\nnormalized stochastic gradient descent [19, 59], which we found improved stability when ﬁne-tuning\nmore complex networks. Table 1 compares the Prototypical Nets performance of this network to that\nof using a ResNet-18. Increased capacity leads to only slight performance improvements, which are\nmore pronounced for datasets that are similar to ImageNet; it harms, e.g., OmniGlot. Further details\nin the extended version [1].\nFor experiments with CrossTransformers, we also increased the resolution of the convolutional feature\nmap by setting the stride of ﬁnal block of the ResNet to 1, and using dilated convolutions to preserve\nthe feature computation [33, 40]. This turns the usual 7×7 feature map for a 224×224 image into a\n14×14 feature map. We ablate this choice in the extended version [1].\nAugmenting CTX with a global feature.\nRecent works have also shown beneﬁts for applying\nlogistic regression (LR) at test time [83]. In practice, it is too expensive to apply LR to our query-\naligned prototypes (as this would involve a separate classiﬁer for every query). Therefore, we instead\napply logistic regression to a globally-pooled feature and average the logits with those produced by\nthe CrossTransformer. See the extended version [1] for details.\nAugmentation.\nWhile most experiments use no augmentation (apart from SimCLR episodes) to be\nconsistent with prior work [86], more recent work [17, 69, 83] showed that stronger data augmentation\nis effective. Therefore, for two experiments, we employ augmentation using the settings discovered\nin BOHB [69] (via Auto-Augment [20] on the validation set), with an extra stage that randomly\ndownsamples and then upsamples images, which we ﬁnd helpful as our network operates at higher\nresolution than many of the test datasets. This BOHB augmentation is only applied to the “MD-\ncategorization” episodes, and not to the SimCLR episodes. Note this BOHB augmentation is different\nfrom SimCLR-style augmentation, which is used in SimCLR Episodes as well as in the ablation\n(SC-Aug) in Table 1. See the extended version [1] for details.\n4.2\nResults for self-supervised learning with SimCLR on Prototypical Nets\nWe ﬁrst analyze the impact of SimCLR Episodes and other architectural choices in Table 1. For\nbaseline Prototypical Nets, SimCLR Episodes generally improve performance, but this depends on\narchitectural choices. Improvements are largest for datasets that are more distant from ImageNet,\ne.g., OmniGlot and Quickdraw, and datasets which require distinguishing between sub-categories\nImageNet categories, e.g., Aircraft and Trafﬁc Signs. In ImageNet, all commercial airplanes fall\nin a single ImageNet class; therefore, the success of SimCLR Episodes here suggests they recover\nfeatures which are lost due to supervision collapse. Strangely, however, SimCLR Episodes interact\nwith Batch Norm: we ﬁnd more robust improvements when computing Batch Norm statistics from\nthe test-time support set, but not when using exponential moving averages (EMA) as suggested by\n[17]. One possible interpretation is that the network has learned to use Batch Norm to communicate\ninformation across the batch: e.g., to distinguish between SimCLR Episodes and MD-categorization\nepisodes. Using EMA at test time may prevent this, which may confuse the network. Interestingly,\n7\n\n\nTable 1: Effects of architecture and SimCLR Episodes on Prototypical Nets, for Meta-Dataset\nTrain-on-ILSVRC. We ablate architectural choices: use of Exponential Moving Averages (EMA) at\ntest time for Batch Norm (versus computing Batch Norm statistics on the support set at test time),\nimage resolution (224, versus the baseline’s 126), ResNet-34 (R34) replacing ResNet-18, SimCLR-\nstyle augmentation (SC-Aug), and the addition of 50% SimCLR Episodes (SC-Eps). The test datasets\nfrom Meta-Dataset are ImNet: Meta-Dataset’s ImageNet Test classes; Omni: OmniGlot drawn\ncharacters; Acraft: Aircraft; Bird: CU-Birds; DTD: Textures; QDraw: Quick Draw drawings; Fungi:\nFGVCx fungi challenge; Flower: VGG Flowers; COCO: Microsoft COCO cropped objects. The best\nnumber in each column is bolded, along with others that are within a conﬁdence interval [86]. Rank\nis the average rank for each method. Using SimCLR Episodes provides improvements on almost\nall datasets, and provides especially large boosts for datasets which are dissimilar from ImageNet,\nsuch as OmniGlot. However, simply using SimCLR transformations without instance discrimination\n(SC-Aug) harms results on almost all datasets. Increased capacity provides small beneﬁts on some\ndatasets, especially the more realistic and ImageNet-like datasets (e.g., birds), but actually harm\nothers like OmniGlot. Note that in this table, QuickDraw uses the split from the original paper [86]\nrather than the (somewhat easier) split published for that paper’s public benchmark. For all other\ntables, we use the split from the published benchmark.\n224 R34 SC-Aug SC-Eps EMA ImNet Omni Acraft\nBird DTD QDraw Fungi Flower\nSign COCO Rank\n\u0013\n49.10 59.27\n49.31 68.43 66.70\n45.83 38.48\n85.34 49.49\n42.88\n5.55\n49.77 55.70\n52.06 68.58 67.27\n49.86 37.68\n84.32 50.27\n41.92\n5.20\n\u0013\n\u0013\n\u0013\n51.66 57.22\n51.63 71.73 69.72\n47.31 42.07\n87.29 47.45\n44.38\n4.35\n\u0013\n\u0013\n52.51 49.87\n56.47 72.81 68.45\n51.41 42.16\n87.92 54.40\n40.60\n3.30\n\u0013\n\u0013\n\u0013\n\u0013\n47.58 55.73\n46.93 57.75 54.88\n42.91 37.42\n83.82 46.88\n43.36\n7.55\n\u0013\n\u0013\n\u0013\n47.94 51.79\n54.58 62.84 58.64\n46.36 36.06\n76.88 48.35\n38.77\n7.45\n\u0013\n\u0013\n\u0013\n\u0013\n49.67 65.21\n54.46 60.94 63.96\n50.64 37.84\n88.70 51.61\n42.97\n4.35\n\u0013\n\u0013\n\u0013\n53.69 68.50\n58.04 74.07 68.76\n53.30 40.73\n86.96 58.11\n41.70\n1.90\nProtoNets [86]\n50.50 59.98\n53.10 68.79 66.56\n48.96 39.71\n85.27 47.12\n41.00\n5.35\nwe will show later that SimCLR Episodes don’t harm CrossTransformers as they harm Prototypical\nNets when using EMA at test time, suggesting the two architectures solve the problem differently.\nRecall that converting an MD-categorization episode into a SimCLR episode makes two changes to\nthe episode: it 1) applies data augmentation, and 2) converts the classiﬁcation problem to “instance\ndiscrimination,” by selecting images from the support set as a new query set, and requiring the\nnetwork to predict the selected indices. To ensure that we are not simply seeing the effect of data\naugmentation, we also implemented a baseline (SC-Aug) that does 1 but not 2 to the input MD-\ncategorization episodes, and does this augmentation for all episodes (rather than 50%, which is\nthe fraction of MD-categorization episodes that are converted to SimCLR episodes for SC-Eps\nexperiments). Indeed, we see no improvements for this change, and in fact non-trivial performance\nloss from this, mirroring the result for supervised learning in the original paper [16]. This reinforces\nthat SimCLR was designed for self-supervised learning, and so the transformations are more severe\nthan is usually optimal for supervised learning.\nFinally, we see small improvements from using larger networks and higher resolution for the baseline\nmodel. While our baseline is overall better than the baseline Prototypical Nets implementation [86],\nit is still below the state-of-the-art for centroid-based methods which rely more heavily on pretraining,\nand use no episodic training [17].\n4.3\nCrossTransformers results\nGiven these performant features, we next turn to CrossTransformers. Table 2 compares CrossTrans-\nformers (CTX) with and without SimCLR episodes to several state-of-the-art methods, including the\nPrototypical Nets on which they are based. We see that CrossTransformers provide strong perfor-\nmance on their own, including having a better average rank than all baselines. With SimCLR episodes\nproviding more versatile features, we see further improvements, with performance on-par or better\nthan the best methods on almost every dataset. We note particularly large improvements on OmniGlot,\nwhich has a large domain gap relative to the training data. We also see strong improvements on Street\nSigns, Aircraft, and Flowers, where multiple test-time categories map to few training-time categories,\nand often exhibit well-deﬁned spatial correspondence.\n8\n\n\nTable 2: CrossTransformers (CTX) comparison to state-of-the-art. We compare four versions\nof CrossTransformers to several state-of-the-art methods, which are the best performers among\nthose evaluated for Meta-Dataset Train-on-ILSVRC. We see that CTX alone has better average rank\nthan any baseline. Adding SimCLR episodes (+SimCLR Eps) and data augmentation inspired by\nBOHB [69] (+Aug) further improves results. Our full model is on-par or above prior methods on all\nbut one dataset, sometimes with large gaps over the best baseline (e.g., +5% on OmniGlot, +13% on\nAircraft, +5% on Signs), and furthermore, each prior method has some datasets where we outperform\nby a larger margin (the next best average rank [83], performs 19% worse on Aircraft and 17% worse\non OmniGlot). Finally, adding a test-time Logistic Regression classiﬁer inspired by Tian et al. [83]\nimproves performance on the one dataset—DTD textures—that was otherwise lacking. Note that\nmost of these methods [17, 69, 83] are unpublished concurrent work.\nImNet Omni Acraft\nBird\nDTD QDraw Fungi Flower\nSign COCO\nRank\nFinetuning [86]\n45.78 60.85\n68.69 57.31 69.05\n42.60 38.20\n85.51 66.79\n34.86 12.20\nProtoNets [86]\n50.50 59.98\n53.10 68.79 66.56\n48.96 39.71\n85.27 47.12\n41.00 12.65\nProtoNets+MAML [86]\n49.53 63.37\n55.95 68.66 66.49\n51.52 39.96\n87.15 48.83\n43.74 11.55\nCNAPS [66]\n50.60 45.20\n36.00 60.70 67.50\n42.30 30.10\n70.70 53.30\n45.20 13.55\nBOHB-L [69]\n50.60 64.09\n57.36 67.68 70.38\n46.26 33.82\n85.51 55.17\n41.58 11.50\nBOHB-NC [69]\n51.92 67.57\n54.12 70.69 68.34\n50.33 41.38\n87.34 51.80\n48.03 10.15\nBOHB-NC Ensemble [69]\n55.39 77.45\n60.85 73.56 72.86\n61.16 44.54\n90.62 57.53\n51.86\n7.45\nDhillon et al. [21]\n-\n-\n68.69 74.26 77.35\n-\n-\n88.14 55.98\n40.62\n-\nMeta-Baseline [17]\n59.20 69.10\n54.10 77.30 76.00\n57.30 45.40\n89.60 66.20\n55.70\n7.20\nTian et al. LR [83]\n60.14 64.92\n63.12 77.69 78.59\n62.48 47.12\n91.60 77.51\n57.00\n5.50\nTian et al. LR-distill [83]\n61.58 64.31\n62.32 79.47 79.28\n60.83 48.53\n91.00 76.33\n59.28\n4.60\nProtoNets (Our implementation)\n51.66 57.22\n51.63 71.73 69.72\n53.81 42.07\n87.29 47.45\n44.38 11.10\nCTX\n61.94 76.52\n79.65 84.06 76.26\n65.67 52.53\n94.11 70.47\n53.51\n3.85\nCTX+SimCLR Eps\n63.79 80.83\n82.05 82.01 75.76\n68.84 52.01\n94.62 75.01\n52.76\n3.05\nCTX+SimCLR Eps+Aug\n62.76 82.21\n79.49 80.63 75.57\n72.68 51.58\n95.34 82.65\n59.90\n2.25\nCTX+SimCLR Eps+Aug+LR\n62.25 82.03\n77.41 76.66 80.29\n72.24 49.39\n93.05 75.25\n60.35\n3.40\nDTD, however, is more challenging for basic CTX, which is unsurprising since textures have little of\nthe kind of spatial correspondence that CTX attempts to ﬁnd. COCO is also challenging, likely due\nto its extremely large intra-class variation (e.g., occlusion) and the fact that many categories overlap\nwith ImageNet-train categories, meaning that simply memorizing categories from the training set\nmay be more useful than using test-time appearance. To explore this trade-off, we applied logistic\nregression at test time to a globally pooled feature (see the extended version [1]), which provides\nadditional logits that are averaged with the CTX logits. We see non-trivial improvements on DTD\nby using this, although we sacriﬁce some performance on other datasets, such as Signs and Aircraft.\nThis implies that there’s a fundamental tension between learning categories based on global features,\nand decomposing the task into local features. More work is needed to better combine these two ideas.\nFinally, Figure 3 depicts the correspondence inferred by the CrossTransformer. The attention is often\nsemantically meaningful: object parts are well matched, including heads, bodies, feet, engines, and\nstrings. The attention is often not one-to-one either: for the ﬂower, the single query ﬂower is matched\nto multiple ﬂowers in some of the support images. Furthermore, the matching works even when\nthe ﬁne-grained classes are not the same, such as the different species of birds, suggesting that the\nattention is indeed a coarse-grained matching that has not overﬁt to the training-set classes.\n5\nConclusion\nWithin a single domain, deep networks have a remarkable ability to compose and reuse features in or-\nder to achieve statistical efﬁciency. However, this work shows the hidden problem with such systems:\nthe networks compose features in a way that conﬂates images which have different appearance but the\nsame label, i.e., it loses information about intra-class variation that may be necessary to understand\nnovel classes. We propose two techniques that help resolve this problem: self-supervised learning,\nwhich prevents features from losing that intra-class variation, and CrossTransformers, which help\nneural networks classify images using local features that are more likely to generalize. However,\nthis problem is far from resolved. In particular, our algorithm provides less beneﬁt when less spatial\nstructure is available, when knowledge of train-time categories can be useful (as in, e.g., COCO), or\nwhen higher-level reasoning is required (e.g., ﬁnding conjunctions of multiple objects). Allowing this\nalgorithm to use spatial structure only where relevant remains an open problem.\n9\n\n\nBroader Impact\nThe algorithm presented in this paper most directly applies to few-shot recognition, which has\nnumerous uses in industry, including vision systems for robotics that must adapt to new objects,\nand photo-organizing software which must infer the presence of new classes of objects on-the-ﬂy.\nUnfamiliar objects are ubiquitous in many real-world vision applications due to the so-called ‘long\ntail’ [95] of objects that occur in real scenes, and therefore we expect our algorithm to improve\nthe robustness of visual recognition systems. While our current work only addresses classiﬁcation,\nmany other tasks in computer vision, such as object detection and segmentation, use neural network\nrepresentations that can likewise be made more robust using the kind of architectures presented here.\nOur algorithm attempts to build representations which factorize the object recognition problem\ninto sub-problems (feature correspondence and feature comparison) that will each transfer correctly\nto new datasets. We hope that further research in this direction may help address dataset biases,\nincluding biases regarding race, gender, or other attributes [98], by helping to disentangle the truly\nmeaningful traits from the spurious correlations. Finally, while this algorithm presents an advance\nto state-of-the-art in understanding rare objects, the general performance of such systems is still far\nbelow human performance. For safety-critical applications (e.g., surgery or self-driving cars), relying\non the ability of vision systems to correctly interpret unusual situations is risky with current systems,\neven with the advances presented here.\nFunding Disclosure\nThis work was funded by DeepMind.\nAcknowledgments\nThe authors would like to thank Pascal Lamblin for help with Meta-Dataset, Olivier Hénaff for\nhelp with SimCLR, Yonglong Tian for help in reproducing baselines, and Relja Arandjelovi´c for\ninvaluable advice on the paper. They are also grateful to Jean-Baptiste Alayrac, Joao Carreira,\nMateusz Malinowski, Viorica P˘atr˘aucean, Adria Recasens, and Lucas Smaira for helpful discussions,\nsupport, and feedback on the project.\nReferences\n[1] Supplementary material (appendix) for the paper. http://arxiv.org/abs/2007.11498.\n[2] M. Andrychowicz, M. Denil, S. Gomez, M. W. Hoffman, D. Pfau, T. Schaul, B. Shillingford, and\nN. De Freitas. Learning to learn by gradient descent by gradient descent. In NeurIPS, 2016.\n[3] P. Bachman, R. D. Hjelm, and W. Buchwalter. Learning representations by maximizing mutual information\nacross views. In NeurIPS, 2019.\n[4] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate.\nIn Proc. ICLR, 2015.\n[5] E. Bart and S. Ullman. Cross-generalization: Learning novel classes from a single example by feature\nreplacement. In Proc. CVPR, 2005.\n[6] S. Bengio, Y. Bengio, J. Cloutier, and J. Gecsei. On the optimization of a synaptic learning rule. In\nPreprints Conf. Optimality in Artiﬁcial and Biological Neural Networks, volume 2. Univ. of Texas, 1992.\n[7] Y. Bengio, S. Bengio, and J. Cloutier. Learning a synaptic learning rule. Citeseer, 1990.\n[8] L. Bertinetto, J. F. Henriques, P. H. Torr, and A. Vedaldi. Meta-learning with differentiable closed-form\nsolvers. In Proc. ICLR, 2019.\n[9] L. Bertinetto, J. F. Henriques, J. Valmadre, P. Torr, and A. Vedaldi. Learning feed-forward one-shot\nlearners. In NeurIPS, 2016.\n[10] L. Bourdev and J. Malik. Poselets: Body part detectors trained using 3d human pose annotations. In Proc.\nICCV, 2009.\n[11] J. Bromley, I. Guyon, Y. LeCun, E. Säckinger, and R. Shah. Signature veriﬁcation using a\" siamese\" time\ndelay neural network. In NeurIPS, 1994.\n[12] A. M. Bronstein, M. M. Bronstein, A. M. Bruckstein, and R. Kimmel. Partial similarity of objects, or\nhow to compare a centaur to a horse. Proc. ICCV, 2009.\n[13] B. Cao, A. Araujo, and J. Sim. Unifying deep local and global features for image search. In Proc. ECCV,\n2020.\n[14] M. Caron, P. Bojanowski, A. Joulin, and M. Douze. Deep clustering for unsupervised learning of visual\nfeatures. In Proc. ECCV, 2018.\n10\n\n\n[15] K. Chatﬁeld, K. Simonyan, and A. Zisserman. Efﬁcient on-the-ﬂy category retrieval using convnets and\ngpus. In Asian Conference on Computer Vision, 2014.\n[16] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning of visual\nrepresentations. In Proc. ICML, 2020.\n[17] Y. Chen, X. Wang, Z. Liu, H. Xu, and T. Darrell. A new meta-baseline for few-shot learning. arXiv\npreprint arXiv:2003.04390, 2020.\n[18] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity metric discriminatively, with application to\nface veriﬁcation. In Proc. CVPR, 2005.\n[19] J. Cortés. Finite-time convergent gradient ﬂows with applications to network consensus. Automatica,\n42(11), 2006.\n[20] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le. Autoaugment: Learning augmentation\nstrategies from data. In Proc. CVPR, 2019.\n[21] G. S. Dhillon et al. A baseline for few-shot image classiﬁcation. Proc. ICLR, 2020.\n[22] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised visual representation learning by context prediction.\nIn Proc. ICCV, 2015.\n[23] C. Doersch, S. Singh, A. Gupta, J. Sivic, and A. A. Efros. What makes paris look like paris? Proc. ACM\nSIGGRAPH, 31(4), 2012.\n[24] C. Doersch and A. Zisserman. Multi-task self-supervised visual learning. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pages 2051–2060, 2017.\n[25] A. Dosovitskiy, J. T. Springenberg, M. Riedmiller, and T. Brox. Discriminative unsupervised feature\nlearning with convolutional neural networks. In NeurIPS. 2014.\n[26] L. Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object categories. IEEE PAMI, 2006.\n[27] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively\ntrained part-based models. IEEE PAMI, 32(9), 2009.\n[28] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In\nProc. ICML, 2017.\n[29] V. Garcia and J. Bruna. Few-shot learning with graph neural networks. In Proc. ICLR, 2018.\n[30] S. Gidaris, A. Bursuc, N. Komodakis, P. Perez, and M. Cord. Boosting few-shot visual learning with\nself-supervision. In The IEEE International Conference on Computer Vision (ICCV), October 2019.\n[31] S. Gidaris, P. Singh, and N. Komodakis. Unsupervised representation learning by predicting image\nrotations. In Proc. ICLR, 2018.\n[32] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection\nand semantic segmentation. In Proc. CVPR, 2014.\n[33] A. Giusti, D. C. Cire¸san, J. Masci, L. M. Gambardella, and J. Schmidhuber. Fast image scanning with\ndeep max-pooling convolutional neural networks. In Intl. Conf. Image Proc., 2013.\n[34] D. Ha, A. Dai, and Q. V. Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016.\n[35] X. Han, T. Leung, Y. Jia, R. Sukthankar, and A. C. Berg. Matchnet: Unifying feature and metric learning\nfor patch-based matching. In Proc. CVPR, 2015.\n[36] B. Hariharan and R. Girshick. Low-shot visual recognition by shrinking and hallucinating features. In\nProc. CVPR, 2017.\n[37] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation\nlearning. In Proc. CVPR, 2020.\n[38] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proc. CVPR, 2016.\n[39] S. Hochreiter, A. S. Younger, and P. R. Conwell. Learning to learn using gradient descent. In International\nConference on Artiﬁcial Neural Networks. Springer, 2001.\n[40] M. Holschneider, R. Kronland-Martinet, J. Morlet, and P. Tchamitchian. A real-time algorithm for signal\nanalysis with the help of the wavelet transform. In Wavelets, pages 286–297. 1990.\n[41] D. Jacobs, D. Weinshall, and Y. Gdalyahu. Class representation and image retrieval with non-metric\ndistances. IEEE PAMI, 22(6):583–600, 2000.\n[42] J. Y. Jason, A. W. Harley, and K. G. Derpanis. Back to basics: Unsupervised learning of optical ﬂow via\nbrightness constancy and motion smoothness. In Proc. CVPR, 2016.\n[43] M. Juneja, A. Vedaldi, C. Jawahar, and A. Zisserman. Blocks that shout: Distinctive parts for scene\nclassiﬁcation. In Proc. CVPR, 2013.\n[44] Ł. Kaiser, O. Nachum, A. Roy, and S. Bengio. Learning to remember rare events. In Proc. ICLR, 2017.\n[45] G. Koch, R. Zemel, and R. Salakhutdinov. Siamese neural networks for one-shot image recognition. In\nICML deep learning workshop, volume 2. Lille, 2015.\n[46] A. Kolesnikov, X. Zhai, and L. Beyer. Revisiting self-supervised visual representation learning. In Proc.\nCVPR, 2019.\n[47] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level concept learning through probabilistic\nprogram induction. Science, 350, 2015.\n[48] G. Larsson, M. Maire, and G. Shakhnarovich. Learning representations for automatic colorization. In\nProc. ECCV, 2016.\n[49] H. Li, G. Hua, Z. Lin, J. Brandt, and J. Yang. Probabilistic elastic matching for pose variant face\nveriﬁcation. In Proc. CVPR, 2013.\n[50] Y. Lifchitz, Y. Avrithis, S. Picard, and A. Bursuc. Dense classiﬁcation and implanting for few-shot\nlearning. In Proc. CVPR, 2019.\n[51] P. Liu, M. Lyu, I. King, and J. Xu. Selﬂow: Self-supervised learning of optical ﬂow. In Proc. CVPR,\n2019.\n11\n\n\n[52] D. Maclaurin, D. Duvenaud, and R. Adams. Gradient-based hyperparameter optimization through\nreversible learning. In International Conference on Machine Learning, 2015.\n[53] S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi. Fine-grained visual classiﬁcation of aircraft.\narXiv preprint arXiv:1306.5151, 2013.\n[54] E. G. Miller, N. E. Matsakis, and P. A. Viola. Learning from one example through shared densities on\ntransforms. In Proc. CVPR, 2000.\n[55] N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel. A simple neural attentive meta-learner. In Proc.\nICLR, 2017.\n[56] I. Misra and L. v. d. Maaten. Self-supervised learning of pretext-invariant representations. In Proc. CVPR,\n2020.\n[57] T. Munkhdalai and H. Yu. Meta networks. In Proc. ICML, 2017.\n[58] D. K. Naik and R. J. Mammone. Meta-neural networks that learn by learning. In [Proceedings 1992]\nIJCNN International Joint Conference on Neural Networks, volume 1. IEEE, 1992.\n[59] Y. E. Nesterov. Minimization methods for nonsmooth convex and quasiconvex functions. Matekon, 29,\n1984.\n[60] J. Nichol, Alex any Andrychowicz ed Achiam and J. Schulman. On ﬁrst-order meta-learning algorithms.\narXiv preprint arXiv:1803.02999, 2018.\n[61] M. Noroozi and P. Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In\nProc. ECCV, 2016.\n[62] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning and transferring mid-level image representations\nusing convolutional neural networks. In Proc. CVPR, 2014.\n[63] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville. Film: Visual reasoning with a general\nconditioning layer. In Proc. AAAI, 2018.\n[64] S. Ravi and H. Larochelle. Optimization as a model for few-shot learning. In Proc. ICLR, 2017.\n[65] S.-A. Rebufﬁ, H. Bilen, and A. Vedaldi. Learning multiple visual domains with residual adapters. In\nNeurIPS, 2017.\n[66] J. Requeima, J. Gordon, J. Bronskill, S. Nowozin, and R. E. Turner. Fast and ﬂexible multi-task\nclassiﬁcation using conditional neural adaptive processes. In NeurIPS, 2019.\n[67] I. Rocco, M. Cimpoi, R. Arandjelovi´c, A. Torii, T. Pajdla, and J. Sivic. Neighbourhood consensus\nnetworks. In NeurIPS, pages 1651–1662, 2018.\n[68] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla,\nM. Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 115(3), 2015.\n[69] T. Saikia, T. Brox, and C. Schmid. Optimized generic feature learning for few-shot classiﬁcation across\ndomains. arXiv preprint arXiv:2001.07926, 2020.\n[70] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and T. Lillicrap. Meta-learning with memory-\naugmented neural networks. In Proc. ICML, 2016.\n[71] S. Savarese, J. Winn, and A. Criminisi. Discriminative object class models of appearance and shape by\ncorrelatons. In Proc. CVPR, 2006.\n[72] J. Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: the\nmeta-meta-... hook. PhD thesis, Technische Universität München, 1987.\n[73] J. Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks.\nNeural Computation, 4(1):131–139, 1992.\n[74] J. Schmidhuber. A neural network that embeds its own meta-levels. In IEEE International Conference on\nNeural Networks, pages 407–412. IEEE, 1993.\n[75] J. Sivic and A. Zisserman. Video Google: A text retrieval approach to object matching in videos. In Proc.\nICCV, 2003.\n[76] J. Snell, K. Swersky, and R. Zemel. Prototypical networks for few-shot learning. In NeurIPS, 2017.\n[77] P. Sprechmann, S. M. Jayakumar, J. W. Rae, A. Pritzel, A. P. Badia, B. Uria, O. Vinyals, D. Hassabis,\nR. Pascanu, and C. Blundell. Memory-based parameter adaptation. In Proc. ICLR, 2018.\n[78] J.-C. Su, S. Maji, and B. Hariharan. When does self-supervision improve few-shot learning? In Proc.\nECCV, 2020.\n[79] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and T. M. Hospedales. Learning to compare: Relation\nnetwork for few-shot learning. In Proc. CVPR, 2018.\n[80] S. Thrun. Lifelong learning algorithms. In Learning to learn. Springer, 1998.\n[81] S. Thrun and L. Pratt. Learning to learn. Springer Science & Business Media, 1998.\n[82] Y. Tian, D. Krishnan, and P. Isola. Contrastive multiview coding. arXiv preprint arXiv:1906.05849, 2019.\n[83] Y. Tian, Y. Wang, D. Krishnan, J. B. Tenenbaum, and P. Isola. Rethinking few-shot image classiﬁcation:\na good embedding is all you need? In Proc. ECCV, 2020.\n[84] P. Tokmakov, Y.-X. Wang, and M. Hebert. Learning compositional representations for few-shot recogni-\ntion. In Proc. ICCV, 2019.\n[85] G. Tolias, T. Jenicek, and O. Chum. Learning and aggregating deep local descriptors for instance-level\nrecognition. In Proc. ECCV, 2020.\n[86] E. Triantaﬁllou, T. Zhu, V. Dumoulin, P. Lamblin, U. Evci, K. Xu, R. Goroshin, C. Gelada, K. J. Swersky,\nP.-A. Manzagol, and H. Larochelle. Meta-dataset: A dataset of datasets for learning to learn from few\nexamples. In Proc. ICLR, 2020.\n[87] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior,\nand K. Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499,\n2016.\n12\n\n\n[88] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.\nAttention is all you need. In NeurIPS, 2017.\n[89] R. C. Veltkamp. Shape matching: Similarity measures and algorithms. In International Conference on\nShape Modeling and Applications, 2001.\n[90] R. Vilalta and Y. Drissi. A perspective view and survey of meta-learning. Artiﬁcial intelligence review,\n18, 2002.\n[91] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and D. Wierstra. Matching networks for one shot\nlearning. In NeurIPS, 2016.\n[92] C. Vondrick, A. Shrivastava, A. Fathi, S. Guadarrama, and K. Murphy. Tracking emerges by colorizing\nvideos. In Proc. ECCV, 2018.\n[93] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 dataset.\n2011.\n[94] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural networks. In Proc. CVPR, 2018.\n[95] Y.-X. Wang, D. Ramanan, and M. Hebert. Learning to model the tail. In NeurIPS, 2017.\n[96] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin. Unsupervised feature learning via non-parametric instance\ndiscrimination. In Proc. CVPR, 2018.\n[97] W. Xie, L. Shen, and A. Zisserman. Comparator networks. In Proc. ECCV, 2018.\n[98] K. Yang, K. Qinami, L. Fei-Fei, J. Deng, and O. Russakovsky. Towards fairer datasets: Filtering and\nbalancing the distribution of the people subtree in the imagenet hierarchy. In Conference on Fairness,\nAccountability, and Transparency, 2020.\n[99] A. S. Younger, S. Hochreiter, and P. R. Conwell. Meta-learning with backpropagation. In IJCNN’01.\nInternational Joint Conference on Neural Networks. Proceedings (Cat. No. 01CH37222), volume 3. IEEE,\n2001.\n[100] S. Zagoruyko and N. Komodakis. Wide residual networks. In Proc. BMVC., 2016.\n[101] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena. Self-attention generative adversarial networks. In\nProc. ICML, 2019.\n[102] J. Zhang, M. Marszałek, S. Lazebnik, and C. Schmid. Local features and kernels for classiﬁcation of\ntexture and object categories: A comprehensive study. IJCV, 73(2), 2007.\n[103] R. Zhang, P. Isola, and A. A. Efros. Colorful image colorization. In Proc. ECCV. Springer, 2016.\n[104] R. Zhang, P. Isola, and A. A. Efros. Split-brain autoencoders: Unsupervised learning by cross-channel\nprediction. In Proc. CVPR, 2017.\n[105] H. Zheng, J. Fu, T. Mei, and J. Luo. Learning multi-attention convolutional neural network for ﬁne-grained\nimage recognition. In Proc. ICCV, 2017.\n13\n"
}