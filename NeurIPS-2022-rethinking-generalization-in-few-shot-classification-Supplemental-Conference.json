{
  "filename": "NeurIPS-2022-rethinking-generalization-in-few-shot-classification-Supplemental-Conference.pdf",
  "num_pages": 8,
  "pages": [
    "Rethinking Generalization in Few-Shot Classification\nSupplementary Material\nMarkus Hiller∗1\nRongkai Ma∗2\nMehrtash Harandi2\nTom Drummond1\n1School of Computing and Information Systems, The University of Melbourne\n2Department of Electrical and Computer Systems Engineering, Monash University\nmarkus.hiller@student.unimelb.edu.au\n{rongkai.ma, mehrtash.harandi}@monash.edu\ntom.drummond@unimelb.edu.au\nA\nSelecting helpful patches at inference time in 1-shot scenarios\nFigure 6 in the main paper demonstrates that our approach is able to successfully learn at inference\ntime which image regions should be considered to classify the unknown query images in a 5-way\n5-shot scenario. We additionally present the visualization of the token importance weights for the\nquery images of a 5-way 1-shot scenario in Figure A1. It can be clearly observed that the brighter\nregions representing higher importance of the respective image patches strongly relate to the actual\nobjects that are to be classified, even in the case of smaller objects (2nd and 4th from the right). While\nour method only has access to significantly less information in the here presented 1-shot than in the\ncase of 5-shot scenarios (see details in Section 2.4), our proposed way of masking the neighborhood\nof each pixel during the online optimization procedure still enables selection of the most helpful areas\ncharacteristic for the respective classes.\nFigure A1: Learning token importance at inference time. Visualized importance weights learnt\nvia online optimization for support set samples in a 5-way 1-shot task on the miniImageNet test set.\nB\nDiscussion on model size and performance\nRelated works have shown that model size seems to not be a good indicator for few-shot performance,\nmost likely since training datasets are comparably small (e.g. 38.4K images in miniImageNet [20] vs.\nstandard ImageNet with 1.28M [16]) and big networks are thus much more prone to overfit. Chen et\nal. [2] demonstrate in Figure 3 of their paper that the performance gains due to larger backbones\nplateau across all methods for backbones bigger than ResNet10 in their experiments and only offer\ndiminishing gains (if any at all). The investigations of Mangla et al. [11] yielded similar results,\n∗Joint first authorship\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n",
    "where the performance on the miniImageNet and tieredImageNet datasets even decreased by around\n0.5-1% when scaling up from ResNet18 to ResNet34 (Table 2). We thus conclude that increased\nnumber of parameters on its own does not lead to better few-shot performance, and the tendency\nof many recent works to choose the established ResNet12 (12.4M) over bigger backbones is highly\nlikely a result of this.\nTo gauge the influence of model size in FewTURE, we additionally investigate the use of the\nsignificantly smaller ViT-tiny architecture with only 5M parameters [19]. Results in Table A1 show\nthat our method achieves a competitive accuracy of 81.10% on the miniImageNet test dataset with\nless than one seventh of the number of parameters of a WRN-28-10, but is (in contrast to many other\nmethods like e.g. [22]) able to leverage increased model sizes to further boost performance.\nTable A1: Investigating model size and performance. Average classification accuracy on the mini-\nImageNet test set, evaluated in a 5-way 5-shot scenario with a ViT-small backbone.\nMethod\nBackbone\n#Params\nTest Accuracy\nProtoNet [18]\nResNet-12\n≈12.4M\n79.46±0.48\nFEAT [22]\nResNet-12\n≈12.4M\n82.05±0.14\nDeepEMD [23]\nResNet-12\n≈12.4M\n82.41±0.56\nCOSOC [10]\nResNet-12\n≈12.4M\n85.16±0.42\nMeta DeepBDC [21]\nResNet-12\n≈12.4M\n84.46±0.28\nLEO [17]\nWRN-28-10\n≈36.5M\n77.59±0.12\nCC+rot [7]\nWRN-28-10\n≈36.5M\n79.87±0.33\nFEAT [22]\nWRN-28-10\n≈36.5M\n81.11±0.14\nPSST [4]\nWRN-28-10\n≈36.5M\n80.64±0.32\nMetaQDA [24]\nWRN-28-10\n≈36.5M\n84.28±0.69\nOM [13]\nWRN-28-10\n≈36.5M\n85.29±0.41\nFewTURE (ours)\nViT-Tiny\n≈5.0M\n81.10±0.61\nFewTURE (ours)\nViT-Small\n≈22.0M\n84.51±0.53\nFewTURE (ours)\nSwin-Tiny\n≈29.0M\n86.38±0.49\nC\nDiscussion on self-supervised vs. supervised pretraining\nPerformance in few-shot learning. We demonstrate in Figure 4 of the main paper that self-\nsupervised pretraining with masked image modelling as pretext task provides a significant advantage\nover supervised pretraining for our approach – a finding that differs from prior non-few-shot literature\nwhere self-supervised methods only moderately outperform their supervised counterparts [25] or\neven perform worse in some cases [3]. We provide our interpretation and insights regarding this in\nthe following.\nFew-shot classification is distinctively different from ‘conventional’ classification (like investigated\nin [3]) in one important aspect: novel previously unseen classes are encountered at test time. As such,\nsupervised learning induces a tendency of the representation space to overfit to the structure of the\nclasses observed during training. In other words, the representation space is created and condensed to\neasily separate observed training classes, but at the expense of distorting other dimensions that might\nbe crucial to correctly distinguish yet unseen classes. This is known in the few-shot literature as\n‘supervision collapse’ [5]. Since no class labels are provided during the self-supervised pretraining,\nwe expect the method to create a more general/less distorted representation space that is significantly\nbetter suited to generalize to yet unseen classes and avoid collapse. These intuitions are supported by\nthe results we have obtained (Fig 4.). We further observe that self-supervised training is helpful to\nprevent early overfitting when learning from small few-shot datasets (e.g. 38.4K miniImageNet [20]\nvs. 1.2M ImageNet1K [16]).\nTraining details of supervised pretraining. For adequate comparison to related work in few-shot\nlearning, we follow the widely adopted pretraining scheme used in FEAT [22] and other works (e.g.\n2\n",
    "DeepEMD [23]) for our supervised pretraining. In detail, we train the network with a cross-entropy\nloss on the training set of the respective dataset to solve a standard classification task (e.g. for\nminiImageNet: 64 classes) – i.e., using the exact same data we use for self-supervised pretraining.\nLike [22] we use the representations of the penultimate layer (before the classifier) to evaluate the\nperformance and quality of the embeddings. To judge suitability of the encoder for few-shot tasks,\nan N-way 1-shot task is commonly solved (e.g. N=16 for miniImageNet due to the 16 classes in the\nvalidation set) – and we tried three different variants here:\n1. & 2. One sample per class is encoded to produce a class-embedding (‘prototype’), and classifica-\ntion performance is evaluated using 15 queries per class (as used in recent related works).\nTo retrieve one embedding per sample, we use the average over all patch tokens produced\nby the Transformer architecture. For fairness regarding metrics, we evaluate both:\n1. embedding distance (MSE) and\n2. embedding similarity (cosine) to perform classification.\n3. We additionally use our own patch-based classifier to evaluate the few-shot setting using all\npatch embeddings (as we later do during fine-tuning & evaluation).\nWe perform validation over 200 such few-shot tasks after every epoch during training and pick the\nbest-performing model regarding highest average validation accuracy. We encountered clear signs of\noverfitting during this type of supervised training, with the training accuracy consistently improving\nto convergence, but validation accuracy plateauing (or decreasing) rather early on (∼350-500ep),\nindependent of the variant we used to evaluate on the validation set.\nD\nAblation studies on components of FewTURE\nIn this section, we provide further insights into our approach and the design choices we made.\nD.1\nAblation on inner loop token reweighting\nA more detailed version of the average classification test accuracies achieved with a meta fine-tuned\nViT backbone on the miniImageNet dataset used for the visualization of the contribution for different\nnumbers of token reweighting steps during online optimisation (main paper, Figure 7) is presented\nin Table A2, including the respective 95% confidence intervals. As discussed in the main paper,\nwe observed a strong initial increase of 1.15% when using our proposed adaptation via online\noptimization (steps> 0). While a higher number of inner-loop updates seems to still lead to increased\naccuracy across all our test runs, this benefit brings along higher computational cost as can be seen\nin the second row of Table A2. We generally found settings between 5 and 15 steps to be a good\naccuracy vs. inference-time trade-off. Our experiments were conducted using an Nvidia-2080ti GPU\nand the stated inferences times have been averaged over 1800 query sample classifications. It is to\nbe noted that the code has not been specifically optimized for fast inference times, and these values\nshould rather be interpreted in a relative manner.\nTable A2: Average classification accuracy and inference times on the mini-ImageNet test set for\nvarying inner loop optimization steps, evaluated in a 5-way 5-shot scenario with a ViT-small backbone\nand SDG with 0.1 as learning rate. Experiments were conducted using an Nvidia-2080ti and runtimes\nwere averaged over 1800 query sample classifications.\n0 steps\n5 steps\n10 steps\n15 steps\n20 steps\nAccuracy\n82.68±0.59\n83.83±0.59\n83.89±0.57\n84.05±0.55\n84.51±0.53\nInference time [ms]\n156.86±2.16\n159.86±2.12\n162.11±2.11\n165.62±2.06\n168.62±2.22\nD.2\nAblation on token aggregation and similarity metrics\nAs discussed in the main paper, we use the logsumexp operation to aggregate our similarity logits as\nit poses a rigorous and numerically stable way of combining individual class probabilities (one for\neach token) to a valid overall probability distribution over classes for each image, independent of how\nthe individual token (log) probability scores are obtained. Table A3 (a) shows the results of additional\n3\n",
    "experiments (training and testing) using our method (ViT-small) and 15 token reweighting steps with\nthe only change being aggregation of the logtis via mean, and we found it to underperform our chosen\nlogsumexp method of aggregation. Direct addition without normalization (i.e. just summing up all\nlogits) proved unstable due to large logit values and was thus not included in this table.\nWe further investigated the use of alternate metrics to compute the similarity between different tokens.\nBoth the use of the negative Euclidean distance and unscaled dot-product yielded inferior results\ncompared to the temperature-scaled cosine distance we use in FewTURE (Table A3 (b)).\nTable A3: Ablation on token aggregation method and similarity metric. Reported are the average\nclassification accuracies on the miniImageNet test set evaluated in a 5-way 5-shot scenario with a\nViT-small backbone.\n(a) Token aggregation\nAggregation method\nTest Accuracy\nlogsumexp\n84.05 ± 0.53\nmean logits\n80.13 ± 0.60\n(b) Similarity metrics\nMetric\nTest Accuracy\ncosine similarity\n84.05 ± 0.53\nneg. Euclidean dist.\n81.85 ± 0.58\nunscaled dot-prod.\n37.60 ± 0.64\nD.3\nAblation regarding temperature scaling of embedding similarity logits\nAs reported in the main paper, we use the temperature τS to rescale the logits of our task-specific\nsimilarity matrix ˜S via division (or the original similarity matrix S in case no task-specific adaptation\nshall be used). We investigate two different ways of temperature scaling: (i) the possibility of using a\nfixed temperature defined as 1/\n√\nd where d is the dimension of the patch embeddings of the respective\narchitecture, and (ii) learning the appropriate temperature during the meta fine-tuning procedure. In\npractice, we learn log(τS) to ensure τS ≥0.\nWe observe throughout our 1-shot experiments depicted in Figure A2 (a) and (b) that the temperature\nconverges towards our default values of 1/\n√\nd shown as a dashed horizontal line. This is independent\nof the initial value of the temperature parameter τ init\nS\n. For the 5-way 5-shot experiments presented in\nFigure A2 (c) and (d) however, we observe that while our default value still achieves good results, the\nlearned temperature converges to a slightly lower value across all experiments.\nD.4\nDevelopment over the course of pretraining\nWe further present insights into the development of the accuracy during self-supervised pretraining.\nSince our pretraining procedure is entirely unsupervised and does hence not include any labels,\nwe investigate models trained for a variety of different epochs and evaluate these on the test set\nusing the proposed similarity-based classification method with (‘5 steps’ and ‘15 steps’) and without\n(‘None’) and present the results in Table A4. Note that no meta fine-tuning was employed here. We\nobserve that while the performance significantly increases over the first 50 epochs, there seems to be\nsome saturation and even slight decrease in performance until above 500 epochs where the accuracy\nincreases again and (mostly) achieves highest results in this study.\nTable A4: Development of test accuracy in self-supervised pretraining. Results obtained for a\n5-way 5-shot scenario on the tieredImageNet test set using our proposed classifier with a ViT-small\nbackbone. For online optimisation (i.e., steps> 0), we use SGD with 0.1 as learning rate.\nReweighting\nEpochs\nsteps\n1\n50\n100\n250\n500\n800\nNone\n39.20±0.69\n73.30±0.75\n73.63±0.73\n72.84±0.72\n71.51±0.72\n73.83±0.74\n5 steps\n39.34±0.69\n73.59±0.74\n74.03±0.73\n73.10±0.73\n71.82±0.72\n74.16±0.73\n15 steps\n39.43±0.69\n73.86±0.73\n74.48±0.74\n73.41±0.75\n72.16±0.73\n74.42±0.74\n4\n",
    "0\n200\n400\n600\n800\n1000\n1200\nIterations of meta fine-tuning\n0.02\n0.04\n0.06\n0.08\n0.10\nTemperature \nS\ninit\nS\n= 0.02\ninit\nS\n= 0.05\ninit\nS\n= 0.10\n(a)\n0\n200\n400\n600\n800\n1000\nIterations of meta fine-tuning\n0.04\n0.06\n0.07\n0.09\nTemperature \nS\ninit\nS\n= 0.03\ninit\nS\n= 0.05\ninit\nS\n= 0.10\n(b)\n0\n200\n400\n600\n800\n1000\n1200\nIterations of meta fine-tuning\n0.02\n0.04\n0.06\n0.08\n0.10\nTemperature \nS\ninit\nS\n= 0.01\ninit\nS\n= 0.05\ninit\nS\n= 0.10\n(c)\n0\n200\n400\n600\n800\n1000\n1200\nIterations of meta fine-tuning\n0.02\n0.04\n0.06\n0.08\n0.10\nTemperature \nS\ninit\nS\n= 0.02\ninit\nS\n= 0.05\ninit\nS\n= 0.10\n(d)\nFigure A2: Temperature for rescaling similarity logits. (a) and (b) show the learned temperatures\nfor 5-way 1-shot scenarios on miniImageNet and tieredImageNet, respectively. The corresponding\n5-way 5-shot results are depicted in (c) and (d). All experiments have been conducted using a\nViT-small architecture.\nE\nFurther visualization of instance embeddings\nFigure 5 in the main paper depicts instance and class embeddings visualized via PCA projection to\nthe three dominant dimensions. Figure A3 additionally depicts a comparison of projected views of the\ntokens of 5 instances from a novel class in embedding space for different ways of meta training. While\nthe representations obtained from the network meta fine-tuned by using common averaging over the\nembeddings (‘average’) do not exhibit any clear separation of the instances, the embeddings obtained\nwith our classifier seem to retain the instance information (‘w/o v’) and separation is improved\nwhen using token importance reweighting (‘w/ v’). These results indicate that our similarity-based\nclassifier coupled with task-specific token reweighting is able to better disentangle the embeddings of\ndifferent instances from the same class, which further prevents the network from supervision collapse\nand helps to achieve the higher performance observed on the benchmarks.\nF\nDatasets used for evaluation\nWe train and evaluate our approach presented in the main paper on the following few-shot image\nclassification datasets:\nminiImageNet. The miniImageNet dataset has been initially proposed by [20] with follow-up\nmodifications by [14] and consists of a specific 100 class subset of ImageNet [16] with 600 images\nfor each class. The data is split into 64 training, 16 validation and 20 test classes.\ntieredImageNet. Similar to the previous dataset, the tieredImageNet [15] is a subset of classes\nselected form the bigger ImageNet [16] dataset, however with a substantially larger set of classes and\n5\n",
    "pre-trained\naverage\nw/o \nw/ \nFigure A3: Instance embeddings after meta fine-tuning. Visualized are the projected tokens of 5\ninstances of the same novel support set class for different meta fine-tuning (M-FT) methods (after\nself-supervised pretraining). From left to right: self-supervised pretraining only, M-FT using an\naverage embedding per class, M-FT using our classifier but without task-specific token reweighting,\nM-FT using our classifier with 15 reweighting steps. (Projection via PCA to main dimensions.)\ndifferent structure in mind. It comprises a selection of 34 super-classes with a total of 608 categories,\ntotalling in 779,165 images that are split into 20,6 and 8 super-classes to achieve better separation\nbetween training, validation and testing, respectively.\nCIFAR-FS. The CIFAR-FS dataset [1] contains the 100 categories with 600 images per category\nfrom the CIFAR100 [8] dataset which are split into 64 training, 16 validation and 20 test classes.\nFC-100. The FC-100 dataset [12] is also derived from CIFAR100 [8] but follows a splitting strategy\nsimilar to tieredImageNet to increase difficulty through higher separation, resulting in 60 training, 20\nvalidation and 20 test classes.\nG\nImplementation details\nWe present further details regarding our implementation and used hyperparameters in the following.\nG.1\nPretraining\nGPU usage. We pretrain our models with the use of 4 Nvidia A100 GPUs with 40GB each for our\nViT [6, 19] and 8 such GPUs for our Swin [9] variants.\nHyperparameter choice. We follow the strategy introduced by [25] to pretrain our Transformer\nbackbones and mostly stick to the hyperparameter settings reported in their work. We generally use\ntwo global crops and 10 local crops with crop scales of (0.4, 1.0) and (0.05, 0.4), respectively. We\nfurther use a patch size of 16 for our ViT models and a window size of 7 for Swin, corresponding to\nthe default sizes for ViT-small [6, 19] and Swin-tiny [9]. We use an output dimension of 8192 for the\nprojection heads across all models, and employ random Masked Image Modelling with prediction\nratios (0, 0.3) and variances (0, 0.2). Our ViT and Swin architectures are trained with an image size\nof 224 × 224 arranged in batches of size 512 samples for 1600 and 800 epochs, respectively, using\na linearly ramped-up learning rate (over first 10 epochs) of 5e−4 × batchsize/256. For detailed\ninformation, we would like to refer the interested reader to the work by Zhou et al. [25] where\nmore background information regarding the influence and justification of these hyperparameters is\nprovided.\nG.2\nMeta fine-tuning\nGPU usage. During the meta fine-tuning (M-FT) stage, we use 1 and 2 Nvidia 2080-ti GPUs for\nViT-small and Swin-tiny, respectively, across all 4 datasets.\nHyperparameters. We fix the input image size as 224 × 224 for all datasets. We use the SGD\noptimizer along with a learning rate of 2e−4, 0.9 as the momentum value and 5e−4 as the weight\ndecay. Additionally, we employ a learning rate scheduler with cosine annealing for 5,000 iterations\nas one cycle, ramping down to 5e−5 at the end of each cycle.\nOnline optimization. During the online learning of the token importance reweighting vectors, we\nadopt the SGD optimizer with 0.1 as the learning rate. For online update steps, we generally choose\n6\n",
    "a default value of 15 steps across all datasets. For further details regarding the temperature scaling\nprocedure used to rescale our task-specific similarity logits, please refer to Section D.3.\nReferences\n[1] Luca Bertinetto, Joao F Henriques, Philip Torr, and Andrea Vedaldi. Meta-learning with differentiable\nclosed-form solvers. In International Conference on Learning Representations, 2019.\n[2] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Wang, and Jia-Bin Huang. A closer look at few-shot\nclassification. In International Conference on Learning Representations, 2019.\n[3] Xinlei Chen, Saining Xie, and Kaiming He.\nAn empirical study of training self-supervised vision\ntransformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages\n9640–9649, 2021.\n[4] Zhengyu Chen, Jixie Ge, Heshen Zhan, Siteng Huang, and Donglin Wang. Pareto self-supervised training\nfor few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13663–13672, 2021.\n[5] Carl Doersch, Ankush Gupta, and Andrew Zisserman. Crosstransformers: spatially-aware few-shot transfer.\nAdvances in Neural Information Processing Systems, 33:21981–21993, 2020.\n[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[7] Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick Pérez, and Matthieu Cord. Boosting few-shot\nvisual learning with self-supervision. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 8059–8068, 2019.\n[8] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n[9] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 10012–10022, 2021.\n[10] Xu Luo, Longhui Wei, Liangjian Wen, Jinrong Yang, Lingxi Xie, Zenglin Xu, and Qi Tian. Rectifying\nthe shortcut learning of background for few-shot learning. Advances in Neural Information Processing\nSystems, 34, 2021.\n[11] Puneet Mangla, Nupur Kumari, Abhishek Sinha, Mayank Singh, Balaji Krishnamurthy, and Vineeth N\nBalasubramanian. Charting the right manifold: Manifold mixup for few-shot learning. In Proceedings of\nthe IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2218–2227, 2020.\n[12] Boris Oreshkin, Pau Rodríguez López, and Alexandre Lacoste. Tadam: Task dependent adaptive metric for\nimproved few-shot learning. Advances in Neural Information Processing Systems, 31, 2018.\n[13] Guodong Qi, Huimin Yu, Zhaohui Lu, and Shuzhao Li. Transductive few-shot classification on the\noblique manifold. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages\n8412–8422, 2021.\n[14] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International\nConference on Learning Representations, 2017.\n[15] Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum, Hugo\nLarochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classification. arXiv preprint\narXiv:1803.00676, 2018.\n[16] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.\nInternational Journal of Computer Vision, 115(3):211–252, 2015.\n[17] Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, and\nRaia Hadsell. Meta-learning with latent embedding optimization. In International Conference on Learning\nRepresentations, 2018.\n[18] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in\nNeural Information Processing Systems, 30, 2017.\n[19] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou.\nTraining data-efficient image transformers & distillation through attention. In International Conference on\nMachine Learning, pages 10347–10357. PMLR, 2021.\n[20] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot\nlearning. Advances in Neural Information Processing Systems, 29:3630–3638, 2016.\n[21] Jiangtao Xie, Fei Long, Jiaming Lv, Qilong Wang, and Peihua Li. Joint distribution matters: Deep brownian\ndistance covariance for few-shot classification. In IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2022.\n[22] Han-Jia Ye, Hexiang Hu, De-Chuan Zhan, and Fei Sha. Few-shot learning via embedding adaptation with\nset-to-set functions. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages\n8808–8817, 2020.\n[23] Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen. Deepemd: Few-shot image classification with\ndifferentiable earth mover’s distance and structured classifiers. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2020.\n[24] Xueting Zhang, Debin Meng, Henry Gouk, and Timothy M Hospedales. Shallow bayesian meta learning for\nreal-world few-shot recognition. In Proceedings of the IEEE/CVF International Conference on Computer\n7\n",
    "Vision, pages 651–660, 2021.\n[25] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image\nbert pre-training with online tokenizer. International Conference on Learning Representations (ICLR),\n2022.\n8\n"
  ],
  "full_text": "Rethinking Generalization in Few-Shot Classification\nSupplementary Material\nMarkus Hiller∗1\nRongkai Ma∗2\nMehrtash Harandi2\nTom Drummond1\n1School of Computing and Information Systems, The University of Melbourne\n2Department of Electrical and Computer Systems Engineering, Monash University\nmarkus.hiller@student.unimelb.edu.au\n{rongkai.ma, mehrtash.harandi}@monash.edu\ntom.drummond@unimelb.edu.au\nA\nSelecting helpful patches at inference time in 1-shot scenarios\nFigure 6 in the main paper demonstrates that our approach is able to successfully learn at inference\ntime which image regions should be considered to classify the unknown query images in a 5-way\n5-shot scenario. We additionally present the visualization of the token importance weights for the\nquery images of a 5-way 1-shot scenario in Figure A1. It can be clearly observed that the brighter\nregions representing higher importance of the respective image patches strongly relate to the actual\nobjects that are to be classified, even in the case of smaller objects (2nd and 4th from the right). While\nour method only has access to significantly less information in the here presented 1-shot than in the\ncase of 5-shot scenarios (see details in Section 2.4), our proposed way of masking the neighborhood\nof each pixel during the online optimization procedure still enables selection of the most helpful areas\ncharacteristic for the respective classes.\nFigure A1: Learning token importance at inference time. Visualized importance weights learnt\nvia online optimization for support set samples in a 5-way 1-shot task on the miniImageNet test set.\nB\nDiscussion on model size and performance\nRelated works have shown that model size seems to not be a good indicator for few-shot performance,\nmost likely since training datasets are comparably small (e.g. 38.4K images in miniImageNet [20] vs.\nstandard ImageNet with 1.28M [16]) and big networks are thus much more prone to overfit. Chen et\nal. [2] demonstrate in Figure 3 of their paper that the performance gains due to larger backbones\nplateau across all methods for backbones bigger than ResNet10 in their experiments and only offer\ndiminishing gains (if any at all). The investigations of Mangla et al. [11] yielded similar results,\n∗Joint first authorship\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n\nwhere the performance on the miniImageNet and tieredImageNet datasets even decreased by around\n0.5-1% when scaling up from ResNet18 to ResNet34 (Table 2). We thus conclude that increased\nnumber of parameters on its own does not lead to better few-shot performance, and the tendency\nof many recent works to choose the established ResNet12 (12.4M) over bigger backbones is highly\nlikely a result of this.\nTo gauge the influence of model size in FewTURE, we additionally investigate the use of the\nsignificantly smaller ViT-tiny architecture with only 5M parameters [19]. Results in Table A1 show\nthat our method achieves a competitive accuracy of 81.10% on the miniImageNet test dataset with\nless than one seventh of the number of parameters of a WRN-28-10, but is (in contrast to many other\nmethods like e.g. [22]) able to leverage increased model sizes to further boost performance.\nTable A1: Investigating model size and performance. Average classification accuracy on the mini-\nImageNet test set, evaluated in a 5-way 5-shot scenario with a ViT-small backbone.\nMethod\nBackbone\n#Params\nTest Accuracy\nProtoNet [18]\nResNet-12\n≈12.4M\n79.46±0.48\nFEAT [22]\nResNet-12\n≈12.4M\n82.05±0.14\nDeepEMD [23]\nResNet-12\n≈12.4M\n82.41±0.56\nCOSOC [10]\nResNet-12\n≈12.4M\n85.16±0.42\nMeta DeepBDC [21]\nResNet-12\n≈12.4M\n84.46±0.28\nLEO [17]\nWRN-28-10\n≈36.5M\n77.59±0.12\nCC+rot [7]\nWRN-28-10\n≈36.5M\n79.87±0.33\nFEAT [22]\nWRN-28-10\n≈36.5M\n81.11±0.14\nPSST [4]\nWRN-28-10\n≈36.5M\n80.64±0.32\nMetaQDA [24]\nWRN-28-10\n≈36.5M\n84.28±0.69\nOM [13]\nWRN-28-10\n≈36.5M\n85.29±0.41\nFewTURE (ours)\nViT-Tiny\n≈5.0M\n81.10±0.61\nFewTURE (ours)\nViT-Small\n≈22.0M\n84.51±0.53\nFewTURE (ours)\nSwin-Tiny\n≈29.0M\n86.38±0.49\nC\nDiscussion on self-supervised vs. supervised pretraining\nPerformance in few-shot learning. We demonstrate in Figure 4 of the main paper that self-\nsupervised pretraining with masked image modelling as pretext task provides a significant advantage\nover supervised pretraining for our approach – a finding that differs from prior non-few-shot literature\nwhere self-supervised methods only moderately outperform their supervised counterparts [25] or\neven perform worse in some cases [3]. We provide our interpretation and insights regarding this in\nthe following.\nFew-shot classification is distinctively different from ‘conventional’ classification (like investigated\nin [3]) in one important aspect: novel previously unseen classes are encountered at test time. As such,\nsupervised learning induces a tendency of the representation space to overfit to the structure of the\nclasses observed during training. In other words, the representation space is created and condensed to\neasily separate observed training classes, but at the expense of distorting other dimensions that might\nbe crucial to correctly distinguish yet unseen classes. This is known in the few-shot literature as\n‘supervision collapse’ [5]. Since no class labels are provided during the self-supervised pretraining,\nwe expect the method to create a more general/less distorted representation space that is significantly\nbetter suited to generalize to yet unseen classes and avoid collapse. These intuitions are supported by\nthe results we have obtained (Fig 4.). We further observe that self-supervised training is helpful to\nprevent early overfitting when learning from small few-shot datasets (e.g. 38.4K miniImageNet [20]\nvs. 1.2M ImageNet1K [16]).\nTraining details of supervised pretraining. For adequate comparison to related work in few-shot\nlearning, we follow the widely adopted pretraining scheme used in FEAT [22] and other works (e.g.\n2\n\n\nDeepEMD [23]) for our supervised pretraining. In detail, we train the network with a cross-entropy\nloss on the training set of the respective dataset to solve a standard classification task (e.g. for\nminiImageNet: 64 classes) – i.e., using the exact same data we use for self-supervised pretraining.\nLike [22] we use the representations of the penultimate layer (before the classifier) to evaluate the\nperformance and quality of the embeddings. To judge suitability of the encoder for few-shot tasks,\nan N-way 1-shot task is commonly solved (e.g. N=16 for miniImageNet due to the 16 classes in the\nvalidation set) – and we tried three different variants here:\n1. & 2. One sample per class is encoded to produce a class-embedding (‘prototype’), and classifica-\ntion performance is evaluated using 15 queries per class (as used in recent related works).\nTo retrieve one embedding per sample, we use the average over all patch tokens produced\nby the Transformer architecture. For fairness regarding metrics, we evaluate both:\n1. embedding distance (MSE) and\n2. embedding similarity (cosine) to perform classification.\n3. We additionally use our own patch-based classifier to evaluate the few-shot setting using all\npatch embeddings (as we later do during fine-tuning & evaluation).\nWe perform validation over 200 such few-shot tasks after every epoch during training and pick the\nbest-performing model regarding highest average validation accuracy. We encountered clear signs of\noverfitting during this type of supervised training, with the training accuracy consistently improving\nto convergence, but validation accuracy plateauing (or decreasing) rather early on (∼350-500ep),\nindependent of the variant we used to evaluate on the validation set.\nD\nAblation studies on components of FewTURE\nIn this section, we provide further insights into our approach and the design choices we made.\nD.1\nAblation on inner loop token reweighting\nA more detailed version of the average classification test accuracies achieved with a meta fine-tuned\nViT backbone on the miniImageNet dataset used for the visualization of the contribution for different\nnumbers of token reweighting steps during online optimisation (main paper, Figure 7) is presented\nin Table A2, including the respective 95% confidence intervals. As discussed in the main paper,\nwe observed a strong initial increase of 1.15% when using our proposed adaptation via online\noptimization (steps> 0). While a higher number of inner-loop updates seems to still lead to increased\naccuracy across all our test runs, this benefit brings along higher computational cost as can be seen\nin the second row of Table A2. We generally found settings between 5 and 15 steps to be a good\naccuracy vs. inference-time trade-off. Our experiments were conducted using an Nvidia-2080ti GPU\nand the stated inferences times have been averaged over 1800 query sample classifications. It is to\nbe noted that the code has not been specifically optimized for fast inference times, and these values\nshould rather be interpreted in a relative manner.\nTable A2: Average classification accuracy and inference times on the mini-ImageNet test set for\nvarying inner loop optimization steps, evaluated in a 5-way 5-shot scenario with a ViT-small backbone\nand SDG with 0.1 as learning rate. Experiments were conducted using an Nvidia-2080ti and runtimes\nwere averaged over 1800 query sample classifications.\n0 steps\n5 steps\n10 steps\n15 steps\n20 steps\nAccuracy\n82.68±0.59\n83.83±0.59\n83.89±0.57\n84.05±0.55\n84.51±0.53\nInference time [ms]\n156.86±2.16\n159.86±2.12\n162.11±2.11\n165.62±2.06\n168.62±2.22\nD.2\nAblation on token aggregation and similarity metrics\nAs discussed in the main paper, we use the logsumexp operation to aggregate our similarity logits as\nit poses a rigorous and numerically stable way of combining individual class probabilities (one for\neach token) to a valid overall probability distribution over classes for each image, independent of how\nthe individual token (log) probability scores are obtained. Table A3 (a) shows the results of additional\n3\n\n\nexperiments (training and testing) using our method (ViT-small) and 15 token reweighting steps with\nthe only change being aggregation of the logtis via mean, and we found it to underperform our chosen\nlogsumexp method of aggregation. Direct addition without normalization (i.e. just summing up all\nlogits) proved unstable due to large logit values and was thus not included in this table.\nWe further investigated the use of alternate metrics to compute the similarity between different tokens.\nBoth the use of the negative Euclidean distance and unscaled dot-product yielded inferior results\ncompared to the temperature-scaled cosine distance we use in FewTURE (Table A3 (b)).\nTable A3: Ablation on token aggregation method and similarity metric. Reported are the average\nclassification accuracies on the miniImageNet test set evaluated in a 5-way 5-shot scenario with a\nViT-small backbone.\n(a) Token aggregation\nAggregation method\nTest Accuracy\nlogsumexp\n84.05 ± 0.53\nmean logits\n80.13 ± 0.60\n(b) Similarity metrics\nMetric\nTest Accuracy\ncosine similarity\n84.05 ± 0.53\nneg. Euclidean dist.\n81.85 ± 0.58\nunscaled dot-prod.\n37.60 ± 0.64\nD.3\nAblation regarding temperature scaling of embedding similarity logits\nAs reported in the main paper, we use the temperature τS to rescale the logits of our task-specific\nsimilarity matrix ˜S via division (or the original similarity matrix S in case no task-specific adaptation\nshall be used). We investigate two different ways of temperature scaling: (i) the possibility of using a\nfixed temperature defined as 1/\n√\nd where d is the dimension of the patch embeddings of the respective\narchitecture, and (ii) learning the appropriate temperature during the meta fine-tuning procedure. In\npractice, we learn log(τS) to ensure τS ≥0.\nWe observe throughout our 1-shot experiments depicted in Figure A2 (a) and (b) that the temperature\nconverges towards our default values of 1/\n√\nd shown as a dashed horizontal line. This is independent\nof the initial value of the temperature parameter τ init\nS\n. For the 5-way 5-shot experiments presented in\nFigure A2 (c) and (d) however, we observe that while our default value still achieves good results, the\nlearned temperature converges to a slightly lower value across all experiments.\nD.4\nDevelopment over the course of pretraining\nWe further present insights into the development of the accuracy during self-supervised pretraining.\nSince our pretraining procedure is entirely unsupervised and does hence not include any labels,\nwe investigate models trained for a variety of different epochs and evaluate these on the test set\nusing the proposed similarity-based classification method with (‘5 steps’ and ‘15 steps’) and without\n(‘None’) and present the results in Table A4. Note that no meta fine-tuning was employed here. We\nobserve that while the performance significantly increases over the first 50 epochs, there seems to be\nsome saturation and even slight decrease in performance until above 500 epochs where the accuracy\nincreases again and (mostly) achieves highest results in this study.\nTable A4: Development of test accuracy in self-supervised pretraining. Results obtained for a\n5-way 5-shot scenario on the tieredImageNet test set using our proposed classifier with a ViT-small\nbackbone. For online optimisation (i.e., steps> 0), we use SGD with 0.1 as learning rate.\nReweighting\nEpochs\nsteps\n1\n50\n100\n250\n500\n800\nNone\n39.20±0.69\n73.30±0.75\n73.63±0.73\n72.84±0.72\n71.51±0.72\n73.83±0.74\n5 steps\n39.34±0.69\n73.59±0.74\n74.03±0.73\n73.10±0.73\n71.82±0.72\n74.16±0.73\n15 steps\n39.43±0.69\n73.86±0.73\n74.48±0.74\n73.41±0.75\n72.16±0.73\n74.42±0.74\n4\n\n\n0\n200\n400\n600\n800\n1000\n1200\nIterations of meta fine-tuning\n0.02\n0.04\n0.06\n0.08\n0.10\nTemperature \nS\ninit\nS\n= 0.02\ninit\nS\n= 0.05\ninit\nS\n= 0.10\n(a)\n0\n200\n400\n600\n800\n1000\nIterations of meta fine-tuning\n0.04\n0.06\n0.07\n0.09\nTemperature \nS\ninit\nS\n= 0.03\ninit\nS\n= 0.05\ninit\nS\n= 0.10\n(b)\n0\n200\n400\n600\n800\n1000\n1200\nIterations of meta fine-tuning\n0.02\n0.04\n0.06\n0.08\n0.10\nTemperature \nS\ninit\nS\n= 0.01\ninit\nS\n= 0.05\ninit\nS\n= 0.10\n(c)\n0\n200\n400\n600\n800\n1000\n1200\nIterations of meta fine-tuning\n0.02\n0.04\n0.06\n0.08\n0.10\nTemperature \nS\ninit\nS\n= 0.02\ninit\nS\n= 0.05\ninit\nS\n= 0.10\n(d)\nFigure A2: Temperature for rescaling similarity logits. (a) and (b) show the learned temperatures\nfor 5-way 1-shot scenarios on miniImageNet and tieredImageNet, respectively. The corresponding\n5-way 5-shot results are depicted in (c) and (d). All experiments have been conducted using a\nViT-small architecture.\nE\nFurther visualization of instance embeddings\nFigure 5 in the main paper depicts instance and class embeddings visualized via PCA projection to\nthe three dominant dimensions. Figure A3 additionally depicts a comparison of projected views of the\ntokens of 5 instances from a novel class in embedding space for different ways of meta training. While\nthe representations obtained from the network meta fine-tuned by using common averaging over the\nembeddings (‘average’) do not exhibit any clear separation of the instances, the embeddings obtained\nwith our classifier seem to retain the instance information (‘w/o v’) and separation is improved\nwhen using token importance reweighting (‘w/ v’). These results indicate that our similarity-based\nclassifier coupled with task-specific token reweighting is able to better disentangle the embeddings of\ndifferent instances from the same class, which further prevents the network from supervision collapse\nand helps to achieve the higher performance observed on the benchmarks.\nF\nDatasets used for evaluation\nWe train and evaluate our approach presented in the main paper on the following few-shot image\nclassification datasets:\nminiImageNet. The miniImageNet dataset has been initially proposed by [20] with follow-up\nmodifications by [14] and consists of a specific 100 class subset of ImageNet [16] with 600 images\nfor each class. The data is split into 64 training, 16 validation and 20 test classes.\ntieredImageNet. Similar to the previous dataset, the tieredImageNet [15] is a subset of classes\nselected form the bigger ImageNet [16] dataset, however with a substantially larger set of classes and\n5\n\n\npre-trained\naverage\nw/o \nw/ \nFigure A3: Instance embeddings after meta fine-tuning. Visualized are the projected tokens of 5\ninstances of the same novel support set class for different meta fine-tuning (M-FT) methods (after\nself-supervised pretraining). From left to right: self-supervised pretraining only, M-FT using an\naverage embedding per class, M-FT using our classifier but without task-specific token reweighting,\nM-FT using our classifier with 15 reweighting steps. (Projection via PCA to main dimensions.)\ndifferent structure in mind. It comprises a selection of 34 super-classes with a total of 608 categories,\ntotalling in 779,165 images that are split into 20,6 and 8 super-classes to achieve better separation\nbetween training, validation and testing, respectively.\nCIFAR-FS. The CIFAR-FS dataset [1] contains the 100 categories with 600 images per category\nfrom the CIFAR100 [8] dataset which are split into 64 training, 16 validation and 20 test classes.\nFC-100. The FC-100 dataset [12] is also derived from CIFAR100 [8] but follows a splitting strategy\nsimilar to tieredImageNet to increase difficulty through higher separation, resulting in 60 training, 20\nvalidation and 20 test classes.\nG\nImplementation details\nWe present further details regarding our implementation and used hyperparameters in the following.\nG.1\nPretraining\nGPU usage. We pretrain our models with the use of 4 Nvidia A100 GPUs with 40GB each for our\nViT [6, 19] and 8 such GPUs for our Swin [9] variants.\nHyperparameter choice. We follow the strategy introduced by [25] to pretrain our Transformer\nbackbones and mostly stick to the hyperparameter settings reported in their work. We generally use\ntwo global crops and 10 local crops with crop scales of (0.4, 1.0) and (0.05, 0.4), respectively. We\nfurther use a patch size of 16 for our ViT models and a window size of 7 for Swin, corresponding to\nthe default sizes for ViT-small [6, 19] and Swin-tiny [9]. We use an output dimension of 8192 for the\nprojection heads across all models, and employ random Masked Image Modelling with prediction\nratios (0, 0.3) and variances (0, 0.2). Our ViT and Swin architectures are trained with an image size\nof 224 × 224 arranged in batches of size 512 samples for 1600 and 800 epochs, respectively, using\na linearly ramped-up learning rate (over first 10 epochs) of 5e−4 × batchsize/256. For detailed\ninformation, we would like to refer the interested reader to the work by Zhou et al. [25] where\nmore background information regarding the influence and justification of these hyperparameters is\nprovided.\nG.2\nMeta fine-tuning\nGPU usage. During the meta fine-tuning (M-FT) stage, we use 1 and 2 Nvidia 2080-ti GPUs for\nViT-small and Swin-tiny, respectively, across all 4 datasets.\nHyperparameters. We fix the input image size as 224 × 224 for all datasets. We use the SGD\noptimizer along with a learning rate of 2e−4, 0.9 as the momentum value and 5e−4 as the weight\ndecay. Additionally, we employ a learning rate scheduler with cosine annealing for 5,000 iterations\nas one cycle, ramping down to 5e−5 at the end of each cycle.\nOnline optimization. During the online learning of the token importance reweighting vectors, we\nadopt the SGD optimizer with 0.1 as the learning rate. For online update steps, we generally choose\n6\n\n\na default value of 15 steps across all datasets. For further details regarding the temperature scaling\nprocedure used to rescale our task-specific similarity logits, please refer to Section D.3.\nReferences\n[1] Luca Bertinetto, Joao F Henriques, Philip Torr, and Andrea Vedaldi. Meta-learning with differentiable\nclosed-form solvers. In International Conference on Learning Representations, 2019.\n[2] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Wang, and Jia-Bin Huang. A closer look at few-shot\nclassification. In International Conference on Learning Representations, 2019.\n[3] Xinlei Chen, Saining Xie, and Kaiming He.\nAn empirical study of training self-supervised vision\ntransformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages\n9640–9649, 2021.\n[4] Zhengyu Chen, Jixie Ge, Heshen Zhan, Siteng Huang, and Donglin Wang. Pareto self-supervised training\nfor few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13663–13672, 2021.\n[5] Carl Doersch, Ankush Gupta, and Andrew Zisserman. Crosstransformers: spatially-aware few-shot transfer.\nAdvances in Neural Information Processing Systems, 33:21981–21993, 2020.\n[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[7] Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick Pérez, and Matthieu Cord. Boosting few-shot\nvisual learning with self-supervision. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 8059–8068, 2019.\n[8] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n[9] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 10012–10022, 2021.\n[10] Xu Luo, Longhui Wei, Liangjian Wen, Jinrong Yang, Lingxi Xie, Zenglin Xu, and Qi Tian. Rectifying\nthe shortcut learning of background for few-shot learning. Advances in Neural Information Processing\nSystems, 34, 2021.\n[11] Puneet Mangla, Nupur Kumari, Abhishek Sinha, Mayank Singh, Balaji Krishnamurthy, and Vineeth N\nBalasubramanian. Charting the right manifold: Manifold mixup for few-shot learning. In Proceedings of\nthe IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2218–2227, 2020.\n[12] Boris Oreshkin, Pau Rodríguez López, and Alexandre Lacoste. Tadam: Task dependent adaptive metric for\nimproved few-shot learning. Advances in Neural Information Processing Systems, 31, 2018.\n[13] Guodong Qi, Huimin Yu, Zhaohui Lu, and Shuzhao Li. Transductive few-shot classification on the\noblique manifold. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages\n8412–8422, 2021.\n[14] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International\nConference on Learning Representations, 2017.\n[15] Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum, Hugo\nLarochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classification. arXiv preprint\narXiv:1803.00676, 2018.\n[16] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.\nInternational Journal of Computer Vision, 115(3):211–252, 2015.\n[17] Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, and\nRaia Hadsell. Meta-learning with latent embedding optimization. In International Conference on Learning\nRepresentations, 2018.\n[18] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in\nNeural Information Processing Systems, 30, 2017.\n[19] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou.\nTraining data-efficient image transformers & distillation through attention. In International Conference on\nMachine Learning, pages 10347–10357. PMLR, 2021.\n[20] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot\nlearning. Advances in Neural Information Processing Systems, 29:3630–3638, 2016.\n[21] Jiangtao Xie, Fei Long, Jiaming Lv, Qilong Wang, and Peihua Li. Joint distribution matters: Deep brownian\ndistance covariance for few-shot classification. In IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2022.\n[22] Han-Jia Ye, Hexiang Hu, De-Chuan Zhan, and Fei Sha. Few-shot learning via embedding adaptation with\nset-to-set functions. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages\n8808–8817, 2020.\n[23] Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen. Deepemd: Few-shot image classification with\ndifferentiable earth mover’s distance and structured classifiers. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2020.\n[24] Xueting Zhang, Debin Meng, Henry Gouk, and Timothy M Hospedales. Shallow bayesian meta learning for\nreal-world few-shot recognition. In Proceedings of the IEEE/CVF International Conference on Computer\n7\n\n\nVision, pages 651–660, 2021.\n[25] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image\nbert pre-training with online tokenizer. International Conference on Learning Representations (ICLR),\n2022.\n8\n"
}