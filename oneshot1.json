{
  "filename": "oneshot1.pdf",
  "num_pages": 8,
  "pages": [
    "Siamese Neural Networks for One-shot Image Recognition\nGregory Koch\nGKOCH@CS.TORONTO.EDU\nRichard Zemel\nZEMEL@CS.TORONTO.EDU\nRuslan Salakhutdinov\nRSALAKHU@CS.TORONTO.EDU\nDepartment of Computer Science, University of Toronto. Toronto, Ontario, Canada.\nAbstract\nThe process of learning good features for ma-\nchine learning applications can be very compu-\ntationally expensive and may prove difﬁcult in\ncases where little data is available. A prototyp-\nical example of this is the one-shot learning set-\nting, in which we must correctly make predic-\ntions given only a single example of each new\nclass.\nIn this paper, we explore a method for\nlearning siamese neural networks which employ\na unique structure to naturally rank similarity be-\ntween inputs. Once a network has been tuned,\nwe can then capitalize on powerful discrimina-\ntive features to generalize the predictive power of\nthe network not just to new data, but to entirely\nnew classes from unknown distributions. Using a\nconvolutional architecture, we are able to achieve\nstrong results which exceed those of other deep\nlearning models with near state-of-the-art perfor-\nmance on one-shot classiﬁcation tasks.\nHumans exhibit a strong ability to acquire and recognize\nnew patterns. In particular, we observe that when presented\nwith stimuli, people seem to be able to understand new\nconcepts quickly and then recognize variations on these\nconcepts in future percepts (Lake et al., 2011). Machine\nlearning has been successfully used to achieve state-of-\nthe-art performance in a variety of applications such as\nweb search, spam detection, caption generation, and speech\nand image recognition. However, these algorithms often\nbreak down when forced to make predictions about data for\nwhich little supervised information is available. We desire\nto generalize to these unfamiliar categories without neces-\nsitating extensive retraining which may be either expensive\nor impossible due to limited data or in an online prediction\nsetting, such as web retrieval.\nProceedings of the 32 nd International Conference on Machine\nLearning, Lille, France, 2015. JMLR: W&CP volume 37. Copy-\nright 2015 by the author(s).\nFigure 1. Example of a 20-way one-shot classiﬁcation task using\nthe Omniglot dataset. The lone test image is shown above the grid\nof 20 images representing the possible unseen classes that we can\nchoose for the test image. These 20 images are our only known\nexamples of each of those classes.\nOne particularly interesting task is classiﬁcation under the\nrestriction that we may only observe a single example of\neach possible class before making a prediction about a test\ninstance. This is called one-shot learning and it is the pri-\nmary focus of our model presented in this work (Fei-Fei\net al., 2006; Lake et al., 2011). This should be distinguished\nfrom zero-shot learning, in which the model cannot look\nat any examples from the target classes (Palatucci et al.,\n2009).\nOne-shot learning can be directly addressed by develop-\ning domain-speciﬁc features or inference procedures which\npossess highly discriminative properties for the target task.\nAs a result, systems which incorporate these methods tend\nto excel at similar instances but fail to offer robust solutions\nthat may be applied to other types of problems. In this pa-\nper, we present a novel approach which limits assumptions\non the structure of the inputs while automatically acquir-\ning features which enable the model to generalize success-\nfully from few examples. We build upon the deep learn-\n",
    "Siamese Neural Networks for One-shot Image Recognition\nFigure 2. Our general strategy. 1) Train a model to discriminate\nbetween a collection of same/different pairs. 2) Generalize to\nevaluate new categories based on learned feature mappings for\nveriﬁcation.\ning framework, which uses many layers of non-linearities\nto capture invariances to transformation in the input space,\nusually by leveraging a model with many parameters and\nthen using a large amount of data to prevent overﬁtting\n(Bengio, 2009; Hinton et al., 2006). These features are\nvery powerful because we are able to learn them without\nimposing strong priors, although the cost of the learning\nalgorithm itself may be considerable.\n1. Approach\nIn general, we learn image representations via a supervised\nmetric-based approach with siamese neural networks, then\nreuse that network’s features for one-shot learning without\nany retraining.\nIn our experiments, we restrict our attention to character\nrecognition, although the basic approach can be replicated\nfor almost any modality (Figure 2). For this domain, we\nemploy large siamese convolutional neural networks which\na) are capable of learning generic image features useful\nfor making predictions about unknown class distributions\neven when very few examples from these new distribu-\ntions are available; b) are easily trained using standard\noptimization techniques on pairs sampled from the source\ndata; and c) provide a competitive approach that does not\nrely upon domain-speciﬁc knowledge by instead exploiting\ndeep learning techniques.\nTo develop a model for one-shot image classiﬁcation, we\naim to ﬁrst learn a neural network that can discriminate\nbetween the class-identity of image pairs, which is the\nstandard veriﬁcation task for image recognition. We hy-\npothesize that networks which do well at at veriﬁcation\nshould generalize to one-shot classiﬁcation. The veriﬁca-\ntion model learns to identify input pairs according to the\nprobability that they belong to the same class or differ-\nent classes. This model can then be used to evaluate new\nimages, exactly one per novel class, in a pairwise manner\nagainst the test image. The pairing with the highest score\naccording to the veriﬁcation network is then awarded the\nhighest probability for the one-shot task. If the features\nlearned by the veriﬁcation model are sufﬁcient to conﬁrm\nor deny the identity of characters from one set of alpha-\nbets, then they ought to be sufﬁcient for other alphabets,\nprovided that the model has been exposed to a variety of\nalphabets to encourage variance amongst the learned fea-\ntures.\n2. Related Work\nOverall, research into one-shot learning algorithms is fairly\nimmature and has received limited attention by the machine\nlearning community. There are nevertheless a few key lines\nof work which precede this paper.\nThe seminal work towards one-shot learning dates back to\nthe early 2000’s with work by Li Fei-Fei et al. The au-\nthors developed a variational Bayesian framework for one-\nshot image classiﬁcation using the premise that previously\nlearned classes can be leveraged to help forecast future\nones when very few examples are available from a given\nclass (Fe-Fei et al., 2003; Fei-Fei et al., 2006). More re-\ncently, Lake et al.\napproached the problem of one-shot\nlearning from the point of view of cognitive science, ad-\ndressing one-shot learning for character recognition with\na method called Hierarchical Bayesian Program Learning\n(HBPL) (2013). In a series of several papers, the authors\nmodeled the process of drawing characters generatively to\ndecompose the image into small pieces (Lake et al., 2011;\n2012). The goal of HBPL is to determine a structural ex-\nplanation for the observed pixels. However, inference un-\nder HBPL is difﬁcult since the joint parameter space is very\nlarge, leading to an intractable integration problem.\nSome researchers have considered other modalities or\ntransfer learning approaches. Lake et al. have some very\nrecent work which uses a generative Hierarchical Hid-\nden Markov model for speech primitives combined with\na Bayesian inference procedure to recognize new words by\nunknown speakers (2014). Maas and Kemp have some of\nthe only published work using Bayesian networks to pre-\ndict attributes for Ellis Island passenger data (2009). Wu\nand Dennis address one-shot learning in the context of path\nplanning algorithms for robotic actuation (2012). Lim fo-\ncuses on how to “borrow” examples from other classes in\nthe training set by adapting a measure of how much each\ncategory should be weighted by each training exemplar in\nthe loss function (2012). This idea can be useful for data\n",
    "Siamese Neural Networks for One-shot Image Recognition\nFigure 3. A simple 2 hidden layer siamese network for binary\nclassiﬁcation with logistic prediction p. The structure of the net-\nwork is replicated across the top and bottom sections to form twin\nnetworks, with shared weight matrices at each layer.\nsets where very few examples exist for some classes, pro-\nviding a ﬂexible and continuous means of incorporating\ninter-class information into the model.\n3. Deep Siamese Networks for Image\nVeriﬁcation\nSiamese nets were ﬁrst introduced in the early 1990s by\nBromley and LeCun to solve signature veriﬁcation as an\nimage matching problem (Bromley et al., 1993). A siamese\nneural network consists of twin networks which accept dis-\ntinct inputs but are joined by an energy function at the top.\nThis function computes some metric between the highest-\nlevel feature representation on each side (Figure 3). The\nparameters between the twin networks are tied. Weight ty-\ning guarantees that two extremely similar images could not\npossibly be mapped by their respective networks to very\ndifferent locations in feature space because each network\ncomputes the same function. Also, the network is symmet-\nric, so that whenever we present two distinct images to the\ntwin networks, the top conjoining layer will compute the\nsame metric as if we were to we present the same two im-\nages but to the opposite twins.\nIn LeCun et al., the authors used a contrastive energy func-\ntion which contained dual terms to decrease the energy of\nlike pairs and increase the energy of unlike pairs (2005).\nHowever, in this paper we use the weighted L1 distance\nbetween the twin feature vectors h1 and h2 combined with\na sigmoid activation, which maps onto the interval [0, 1].\nThus a cross-entropy objective is a natural choice for train-\ning the network. Note that in LeCun et al., they directly\nlearned the similarity metric, which was implictly deﬁned\nby the energy loss, whereas we ﬁx the metric as speciﬁed\nabove, following the approach in Facebook’s DeepFace pa-\nper (Taigman et al., 2014).\nOur best-performing models use multiple convolutional\nlayers before the fully-connected layers and top-level\nenergy function.\nConvolutional neural networks have\nachieved exceptional results in many large-scale computer\nvision applications, particularly in image recognition tasks\n(Bengio, 2009; Krizhevsky et al., 2012; Simonyan & Zis-\nserman, 2014; Srivastava, 2013).\nSeveral factors make convolutional networks especially ap-\npealing. Local connectivity can greatly reduce the num-\nber of parameters in the model, which inherently provides\nsome form of built-in regularization, although convolu-\ntional layers are computationally more expensive than stan-\ndard nonlinearities. Also, the convolution operation used in\nthese networks has a direct ﬁltering interpretation, where\neach feature map is convolved against input features to\nidentify patterns as groupings of pixels.\nThus, the out-\nputs of each convolutional layer correspond to important\nspatial features in the original input space and offer some\nrobustness to simple transforms. Finally, very fast CUDA\nlibraries are now available in order to build large convolu-\ntional networks without an unacceptable amount of train-\ning time (Mnih, 2009; Krizhevsky et al., 2012; Simonyan\n& Zisserman, 2014).\nWe now detail both the structure of the siamese nets and the\nspeciﬁcs of the learning algorithm used in our experiments.\n3.1. Model\nOur standard model is a siamese convolutional neural net-\nwork with L layers each with Nl units, where h1,l repre-\nsents the hidden vector in layer l for the ﬁrst twin, and h2,l\ndenotes the same for the second twin. We use exclusively\nrectiﬁed linear (ReLU) units in the ﬁrst L −2 layers and\nsigmoidal units in the remaining layers.\nThe model consists of a sequence of convolutional layers,\neach of which uses a single channel with ﬁlters of varying\nsize and a ﬁxed stride of 1. The number of convolutional\nﬁlters is speciﬁed as a multiple of 16 to optimize perfor-\nmance. The network applies a ReLU activation function\nto the output feature maps, optionally followed by max-\npooling with a ﬁlter size and stride of 2. Thus the kth ﬁlter\nmap in each layer takes the following form:\na(k)\n1,m = max-pool(max(0, W(k)\nl−1,l ⋆h1,(l−1) + bl), 2)\na(k)\n2,m = max-pool(max(0, W(k)\nl−1,l ⋆h2,(l−1) + bl), 2)\nwhere Wl−1,l is the 3-dimensional tensor representing the\nfeature maps for layer l and we have taken ⋆to be the\nvalid convolutional operation corresponding to returning\n",
    "Siamese Neural Networks for One-shot Image Recognition\nFigure 4. Best convolutional architecture selected for veriﬁcation task. Siamese twin is not depicted, but joins immediately after the\n4096 unit fully-connected layer where the L1 component-wise distance between vectors is computed.\nonly those output units which were the result of complete\noverlap between each convolutional ﬁlter and the input fea-\nture maps.\nThe units in the ﬁnal convolutional layer are ﬂattened into\na single vector. This convolutional layer is followed by\na fully-connected layer, and then one more layer com-\nputing the induced distance metric between each siamese\ntwin, which is given to a single sigmoidal output unit.\nMore precisely, the prediction vector is given as p =\nσ(P\nj αj|h(j)\n1,L−1 −h(j)\n2,L−1|), where σ is the sigmoidal\nactivation function. This ﬁnal layer induces a metric on\nthe learned feature space of the (L −1)th hidden layer\nand scores the similarity between the two feature vec-\ntors.\nThe αj are additional parameters that are learned\nby the model during training, weighting the importance\nof the component-wise distance. This deﬁnes a ﬁnal Lth\nfully-connected layer for the network which joins the two\nsiamese twins.\nWe depict one example above (Figure 4), which shows the\nlargest version of our model that we considered. This net-\nwork also gave the best result for any network on the veri-\nﬁcation task.\n3.2. Learning\nLoss function. Let M represent the minibatch size, where\ni indexes the ith minibatch.\nNow let y(x(i)\n1 , x(i)\n2 ) be a\nlength-M vector which contains the labels for the mini-\nbatch, where we assume y(x(i)\n1 , x(i)\n2 ) = 1 whenever x1 and\nx2 are from the same character class and y(x(i)\n1 , x(i)\n2 ) = 0\notherwise. We impose a regularized cross-entropy objec-\ntive on our binary classiﬁer of the following form:\nL(x(i)\n1 , x(i)\n2 ) = y(x(i)\n1 , x(i)\n2 ) log p(x(i)\n1 , x(i)\n2 )+\n(1 −y(x(i)\n1 , x(i)\n2 )) log (1 −p(x(i)\n1 , x(i)\n2 )) + λT |w|2\nOptimization. This objective is combined with standard\nbackpropagation algorithm, where the gradient is additive\nacross the twin networks due to the tied weights. We ﬁx\na minibatch size of 128 with learning rate ηj, momentum\nµj, and L2 regularization weights λj deﬁned layer-wise, so\nthat our update rule at epoch T is as follows:\nw(T )\nkj (x(i)\n1 , x(i)\n2 ) = w(T )\nkj + ∆w(T )\nkj (x(i)\n1 , x(i)\n2 ) + 2λj|wkj|\n∆w(T )\nkj (x(i)\n1 , x(i)\n2 ) = −ηj∇w(T )\nkj + µj∆w(T −1)\nkj\nwhere ∇wkj is the partial derivative with respect to the\nweight between the jth neuron in some layer and the kth\nneuron in the successive layer.\nWeight initialization. We initialized all network weights\nin the convolutional layers from a normal distribution with\nzero-mean and a standard deviation of 10−2. Biases were\nalso initialized from a normal distribution, but with mean\n0.5 and standard deviation 10−2. In the fully-connected\nlayers, the biases were initialized in the same way as the\nconvolutional layers, but the weights were drawn from a\nmuch wider normal distribution with zero-mean and stan-\ndard deviation 2 × 10−1.\nLearning schedule. Although we allowed for a different\nlearning rate for each layer, learning rates were decayed\nuniformly across the network by 1 percent per epoch, so\nthat η(T )\nj\n= 0.99η(T −1)\nj\n. We found that by annealing the\nlearning rate, the network was able to converge to local\nminima more easily without getting stuck in the error sur-\nface. We ﬁxed momentum to start at 0.5 in every layer,\nincreasing linearly each epoch until reaching the value µj,\nthe individual momentum term for the jth layer.\nWe trained each network for a maximum of 200 epochs, but\nmonitored one-shot validation error on a set of 320 one-\nshot learning tasks generated randomly from the alphabets\nand drawers in the validation set. When the validation error\ndid not decrease for 20 epochs, we stopped and used the\nparameters of the model at the best epoch according to the\none-shot validation error. If the validation error continued\nto decrease for the entire learning schedule, we saved the\nﬁnal state of the model generated by this procedure.\nHyperparameter optimization.\nWe used the beta ver-\nsion of Whetlab, a Bayesian optimization framework, to\n",
    "Siamese Neural Networks for One-shot Image Recognition\nFigure 5. A sample of random afﬁne distortions generated for a\nsingle character in the Omniglot data set.\nperform hyperparameter selection.\nFor learning sched-\nule and regularization hyperparameters, we set the layer-\nwise learning rate ηj ∈[10−4, 10−1], layer-wise momen-\ntum µj ∈[0, 1], and layer-wise L2 regularization penalty\nλj ∈[0, 0.1]. For network hyperparameters, we let the size\nof convolutional ﬁlters vary from 3x3 to 20x20, while the\nnumber of convolutional ﬁlters in each layer varied from\n16 to 256 using multiples of 16. Fully-connected layers\nranged from 128 to 4096 units, also in multiples of 16. We\nset the optimizer to maximize one-shot validation set accu-\nracy. The score assigned to a single Whetlab iteration was\nthe highest value of this metric found during any epoch.\nAfﬁne distortions. In addition, we augmented the train-\ning set with small afﬁne distortions (Figure 5). For each\nimage pair x1, x2, we generated a pair of afﬁne trans-\nformations T1, T2 to yield x′\n1 = T1(x1), x′\n2 = T2(x2),\nwhere T1, T2 are determined stochastically by a multi-\ndimensional uniform distribution. So for an arbitrary trans-\nform T, we have T = (θ, ρx, ρy, sx, sy, tx, tx), with θ ∈\n[−10.0, 10.0], ρx, ρy ∈[−0.3, 0.3], sx, sy ∈[0.8, 1.2], and\ntx, ty ∈[−2, 2]. Each of these components of the transfor-\nmation is included with probability 0.5.\n4. Experiments\nWe trained our model on a subset of the Omniglot data set,\nwhich we ﬁrst describe. We then provide details with re-\nspect to veriﬁcation and one-shot performance.\n4.1. The Omniglot Dataset\nThe Omniglot data set was collected by Brenden Lake and\nhis collaborators at MIT via Amazon’s Mechanical Turk to\nproduce a standard benchmark for learning from few exam-\nples in the handwritten character recognition domain (Lake\net al., 2011).1 Omniglot contains examples from 50 alpha-\nbets ranging from well-established international languages\n1The complete data set can be obtained from Brenden Lake by\nrequest (brenden@cs.nyu.edu). Each character in Omniglot\nis a 105x105 binary-valued image which was drawn by hand on\nan online canvas. The stroke trajectories were collected alongside\nthe composite images, so it is possible to incorporate temporal\nand structural information into models trained on Omniglot.\nFigure 6. The Omniglot dataset contains a variety of different im-\nages from alphabets across the world.\nlike Latin and Korean to lesser known local dialects. It also\nincludes some ﬁctitious character sets such as Aurek-Besh\nand Klingon (Figure 6).\nThe number of letters in each alphabet varies considerably\nfrom about 15 to upwards of 40 characters. All charac-\nters across these alphabets are produced a single time by\neach of 20 drawers Lake split the data into a 40 alpha-\nbet background set and a 10 alphabet evaluation set. We\npreserve these two terms in order to distinguish from the\nnormal training, validation, and test sets that can be gener-\nated from the background set in order to tune models for\nveriﬁcation. The background set is used for developing a\nmodel by learning hyperparameters and feature mappings.\nConversely, the evaluation set is used only to measure the\none-shot classiﬁcation performance.\n4.2. Veriﬁcation\nTo train our veriﬁcation network, we put together three dif-\nferent data set sizes with 30,000, 90,000, and 150,000 train-\ning examples by sampling random same and different pairs.\nWe set aside sixty percent of the total data for training: 30\nalphabets out of 50 and 12 drawers out of 20.\nWe ﬁxed a uniform number of training examples per alpha-\nbet so that each alphabet receives equal representation dur-\ning optimization, although this is not guaranteed to the in-\ndividual character classes within each alphabet. By adding\nTable 1. Accuracy on Omniglot veriﬁcation task (siamese convo-\nlutional neural net)\nMethod\nTest\n30k training\nno distortions\n90.61\nafﬁne distortions x8\n91.90\n90k training\nno distortions\n91.54\nafﬁne distortions x8\n93.15\n150k training\nno distortions\n91.63\nafﬁne distortions x8\n93.42\n",
    "Siamese Neural Networks for One-shot Image Recognition\nFigure 7. Examples of ﬁrst-layer convolutional ﬁlters learned by\nthe siamese network. Notice ﬁlters adapt different roles: some\nlook for very small point-wise features whereas others function\nlike larger-scale edge detectors.\nafﬁne distortions, we also produced an additional copy of\nthe data set corresponding to the augmented version of each\nof these sizes. We added eight transforms for each train-\ning example, so the corresponding data sets have 270,000,\n810,000, and 1,350,000 effective examples.\nTo monitor performance during training, we used two\nstrategies. First, we created a validation set for veriﬁcation\nwith 10,000 example pairs taken from 10 alphabets and 4\nadditional drawers. We reserved the last 10 alphabets and\n4 drawers for testing, where we constrained these to be the\nsame ones used in Lake et al. (Lake et al., 2013). Our\nother strategy leveraged the same alphabets and drawers\nto generate a set of 320 one-shot recognition trials for the\nvalidation set which mimic the target task on the evaluation\nset. In practice, this second method of determining when to\nstop was at least as effective as the validation error for the\nveriﬁcation task so we used it as our termination criterion.\nIn the table below (Table 1), we list the ﬁnal veriﬁcation\nresults for each of the six possible training sets, where the\nlisted test accuracy is reported at the best validation check-\npoint and threshold. We report results across six different\ntraining runs, varying the training set size and toggling dis-\ntortions.\nIn Figure 7, we have extracted the ﬁrst 32 ﬁlters from both\nof our top two performing networks on the veriﬁcation task,\nwhich were trained on the 90k and 150k data sets with\nafﬁne distortions and the architecture shown in Figure 3.\nWhile there is some co-adaptation between ﬁlters, it is easy\nto see that some of the ﬁlters have assumed different roles\nwith respect to the original input space.\n4.3. One-shot Learning\nOnce we have optimized a siamese network to master the\nveriﬁcation task, we are ready to demonstrate the discrimi-\nnative potential of our learned features at one-shot learning.\nSuppose we are given a test image x, some column vector\nwhich we wish to classify into one of C categories. We\nare also given some other images {xc}C\nc=1, a set of col-\nTable 2. Comparing best one-shot accuracy from each type of\nnetwork against baselines.\nMethod\nTest\nHumans\n95.5\nHierarchical Bayesian Program Learning\n95.2\nAfﬁne model\n81.8\nHierarchical Deep\n65.2\nDeep Boltzmann Machine\n62.0\nSimple Stroke\n35.2\n1-Nearest Neighbor\n21.7\nSiamese Neural Net\n58.3\nConvolutional Siamese Net\n92.0\numn vectors representing examples of each of those C cat-\negories. We can now query the network using x, xc as our\ninput for a range of c = 1, . . . , C.2 Then predict the class\ncorresponding to the maximum similarity.\nC∗= argmaxcp(c)\nTo empirically evaluate one-shot learning performance,\nLake developed a 20-way within-alphabet classiﬁcation\ntask in which an alphabet is ﬁrst chosen from among those\nreserved for the evaluation set, along with twenty charac-\nters taken uniformly at random. Two of the twenty drawers\nare also selected from among the pool of evaluation draw-\ners. These two drawers then produce a sample of the twenty\ncharacters. Each one of the characters produced by the ﬁrst\ndrawer are denoted as test images and individually com-\npared against all twenty characters from the second drawer,\nwith the goal of predicting the class corresponding to the\ntest image from among all of the second drawer’s charac-\nters. An individual example of a one-shot learning trial is\ndepicted in Figure 7. This process is repeated twice for\nall alphabets, so that there are 40 one-shot learning trials\nfor each of the ten evaluation alphabets. This constitutes a\ntotal of 400 one-shot learning trials, from which the classi-\nﬁcation accuracy is calculated.\nThe one-shot results are given in Table 2.\nWe borrow\nthe baseline results from (Lake et al., 2013) for compari-\nson to our method. We also include results from a non-\nconvolutional siamese network with two fully-connected\nlayers.\nAt 92 percent our convolutional method is stronger than\nany model except HBPL itself. which is only slightly be-\nhind human error rates. While HBPL exhibits stronger re-\nsults overall, our top-performing convolutional network did\n2This can be processed efﬁciently by appending C copies of\nx into a single matrix X and stacking xT\nc in rows to form another\nmatrix XC so that we can perform just one feedforward pass with\nminibatch size C using input X, XC.\n",
    "Siamese Neural Networks for One-shot Image Recognition\nTable 3. Results from MNIST 10-versus-1 one-shot classiﬁcation\ntask.\nMethod\nTest\n1-Nearest Neighbor\n26.5\nConvolutional Siamese Net\n70.3\nnot include any extra prior knowledge about characters or\nstrokes such as generative information about the drawing\nprocess. This is the primary advantage of our model.\n4.4. MNIST One-shot Trial\nThe Omniglot data set contains a small handful of samples\nfor every possible class of letter; for this reason, the original\nauthors refer to it as a sort of “MNIST transpose”, where\nthe number of classes far exceeds the number of training\ninstances (Lake et al., 2013). We thought it would be in-\nteresting to monitor how well a model trained on Omniglot\ncan generalize to MNIST, where we treat the 10 digits in\nMNIST as an alphabet and then evaluate a 10-way one-\nshot classiﬁcation task. We followed a similar procedure\nto Omniglot, generating 400 one-shot trials on the MNIST\ntest set, but excluding any ﬁne tuning on the training set.\nAll 28x28 images were upsampled to 35x35, then given to\na reduced version of our model trained on 35x35 images\nfrom Omniglot which were downsampled by a factor of\n3. We also evaluated the nearest-neighbor baseline on this\ntask.\nTable 3 shows the results from this experiment. The near-\nest neighbor baseline provides similar performance to Om-\nniglot, while the performance of the convolutional network\ndrops by a more signiﬁcant amount. However, we are still\nable to achieve reasonable generalization from the features\nlearned on Ominglot without training at all on MNIST.\n5. Conclusions\nWe have presented a strategy for performing one-shot clas-\nsiﬁcation by ﬁrst learning deep convolutional siamese neu-\nral networks for veriﬁcation.\nWe outlined new results\ncomparing the performance of our networks to an exist-\ning state-of-the-art classiﬁer developed for the Omniglot\ndata set. Our networks outperform all available baselines\nby a signiﬁcant margin and come close to the best num-\nbers achieved by the previous authors. We have argued that\nthe strong performance of these networks on this task indi-\ncate not only that human-level accuracy is possible with our\nmetric learning approach, but that this approach should ex-\ntend to one-shot learning tasks in other domains, especially\nfor image classiﬁcation.\nIn this paper, we only considered training for the veriﬁca-\nFigure 8. Two sets of stroke distortions for different characters\nfrom Omniglot. Columns depict characters sampled from differ-\nent drawers. Row 1: original images. Row 2: global afﬁne trans-\nforms. Row 3: afﬁne transforms on strokes. Row 4: global afﬁne\ntransforms layered on top of stroke transforms. Notice how stroke\ndistortions can add noise and affect the spatial relations between\nindividual strokes.\ntion task by processing image pairs and their distortions\nusing a global afﬁne transform.\nWe have been experi-\nmenting with an extended algorithm that exploits the data\nabout the individual stroke trajectories to produce ﬁnal\ncomputed distortions (Figure 8). By imposing local afﬁne\ntransformations on the strokes and overlaying them into\na composite image, we are hopeful that we can learn\nfeatures which are better adapted to the variations that are\ncommonly seen in new examples.\nReferences\nBengio, Yoshua. Learning deep architectures for ai. Foun-\ndations and Trends in Machine Learning, 2(1):1–127,\n2009.\nBromley, Jane, Bentz, James W, Bottou, L´eon, Guyon,\nIsabelle, LeCun, Yann, Moore, Cliff, S¨ackinger, Ed-\nuard, and Shah, Roopak. Signature veriﬁcation using a\nsiamese time delay neural network. International Jour-\nnal of Pattern Recognition and Artiﬁcial Intelligence, 7\n(04):669–688, 1993.\nChopra, Sumit, Hadsell, Raia, and LeCun, Yann. Learning\na similarity metric discriminatively, with application to\nface veriﬁcation. In Computer Vision and Pattern Recog-\nnition, 2005. CVPR 2005. IEEE Computer Society Con-\nference on, volume 1, pp. 539–546. IEEE, 2005.\n",
    "Siamese Neural Networks for One-shot Image Recognition\nFe-Fei, Li, Fergus, Robert, and Perona, Pietro. A bayesian\napproach to unsupervised one-shot learning of object\ncategories.\nIn Computer Vision, 2003. Proceedings.\nNinth IEEE International Conference on, pp. 1134–\n1141. IEEE, 2003.\nFei-Fei, Li, Fergus, Robert, and Perona, Pietro. One-shot\nlearning of object categories. Pattern Analysis and Ma-\nchine Intelligence, IEEE Transactions on, 28(4):594–\n611, 2006.\nHinton, Geoffrey, Osindero, Simon, and Teh, Yee-Whye.\nA fast learning algorithm for deep belief nets. Neural\ncomputation, 18(7):1527–1554, 2006.\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.\nImagenet classiﬁcation with deep convolutional neural\nnetworks. In Advances in neural information processing\nsystems, pp. 1097–1105, 2012.\nLake, Brenden M, Salakhutdinov, Ruslan, Gross, Jason,\nand Tenenbaum, Joshua B. One shot learning of simple\nvisual concepts. In Proceedings of the 33rd Annual Con-\nference of the Cognitive Science Society, volume 172,\n2011.\nLake, Brenden M, Salakhutdinov, Ruslan, and Tenenbaum,\nJoshua B. Concept learning as motor program induction:\nA large-scale empirical study. In Proceedings of the 34th\nAnnual Conference of the Cognitive Science Society, pp.\n659–664, 2012.\nLake, Brenden M, Salakhutdinov, Ruslan R, and Tenen-\nbaum, Josh. One-shot learning by inverting a composi-\ntional causal process. In Advances in neural information\nprocessing systems, pp. 2526–2534, 2013.\nLake, Brenden M, Lee, Chia-ying, Glass, James R, and\nTenenbaum, Joshua B. One-shot learning of generative\nspeech concepts. Cognitive Science Society, 2014.\nLim, Joseph Jaewhan. Transfer learning by borrowing ex-\namples for multiclass object detection. Master’s thesis,\nMassachusetts Institute of Technology, 2012.\nMaas, Andrew and Kemp, Charles. One-shot learning with\nbayesian networks. Cognitive Science Society, 2009.\nMnih, Volodymyr. Cudamat: a cuda-based matrix class for\npython. 2009.\nPalatucci, Mark, Pomerleau, Dean, Hinton, Geoffrey E,\nand Mitchell, Tom M. Zero-shot learning with seman-\ntic output codes. In Advances in neural information pro-\ncessing systems, pp. 1410–1418, 2009.\nSimonyan, Karen and Zisserman, Andrew. Very deep con-\nvolutional networks for large-scale image recognition.\narXiv preprint arXiv:1409.1556, 2014.\nSrivastava, Nitish.\nImproving neural networks with\ndropout. Master’s thesis, University of Toronto, 2013.\nTaigman, Yaniv, Yang, Ming, Ranzato, Marc’Aurelio, and\nWolf, Lior. Deepface: Closing the gap to human-level\nperformance in face veriﬁcation. In Computer Vision and\nPattern Recognition (CVPR), 2014 IEEE Conference on,\npp. 1701–1708. IEEE, 2014.\nWu, Di, Zhu, Fan, and Shao, Ling.\nOne shot learning\ngesture recognition from rgbd images.\nIn Computer\nVision and Pattern Recognition Workshops (CVPRW),\n2012 IEEE Computer Society Conference on, pp. 7–12.\nIEEE, 2012.\n"
  ],
  "full_text": "Siamese Neural Networks for One-shot Image Recognition\nGregory Koch\nGKOCH@CS.TORONTO.EDU\nRichard Zemel\nZEMEL@CS.TORONTO.EDU\nRuslan Salakhutdinov\nRSALAKHU@CS.TORONTO.EDU\nDepartment of Computer Science, University of Toronto. Toronto, Ontario, Canada.\nAbstract\nThe process of learning good features for ma-\nchine learning applications can be very compu-\ntationally expensive and may prove difﬁcult in\ncases where little data is available. A prototyp-\nical example of this is the one-shot learning set-\nting, in which we must correctly make predic-\ntions given only a single example of each new\nclass.\nIn this paper, we explore a method for\nlearning siamese neural networks which employ\na unique structure to naturally rank similarity be-\ntween inputs. Once a network has been tuned,\nwe can then capitalize on powerful discrimina-\ntive features to generalize the predictive power of\nthe network not just to new data, but to entirely\nnew classes from unknown distributions. Using a\nconvolutional architecture, we are able to achieve\nstrong results which exceed those of other deep\nlearning models with near state-of-the-art perfor-\nmance on one-shot classiﬁcation tasks.\nHumans exhibit a strong ability to acquire and recognize\nnew patterns. In particular, we observe that when presented\nwith stimuli, people seem to be able to understand new\nconcepts quickly and then recognize variations on these\nconcepts in future percepts (Lake et al., 2011). Machine\nlearning has been successfully used to achieve state-of-\nthe-art performance in a variety of applications such as\nweb search, spam detection, caption generation, and speech\nand image recognition. However, these algorithms often\nbreak down when forced to make predictions about data for\nwhich little supervised information is available. We desire\nto generalize to these unfamiliar categories without neces-\nsitating extensive retraining which may be either expensive\nor impossible due to limited data or in an online prediction\nsetting, such as web retrieval.\nProceedings of the 32 nd International Conference on Machine\nLearning, Lille, France, 2015. JMLR: W&CP volume 37. Copy-\nright 2015 by the author(s).\nFigure 1. Example of a 20-way one-shot classiﬁcation task using\nthe Omniglot dataset. The lone test image is shown above the grid\nof 20 images representing the possible unseen classes that we can\nchoose for the test image. These 20 images are our only known\nexamples of each of those classes.\nOne particularly interesting task is classiﬁcation under the\nrestriction that we may only observe a single example of\neach possible class before making a prediction about a test\ninstance. This is called one-shot learning and it is the pri-\nmary focus of our model presented in this work (Fei-Fei\net al., 2006; Lake et al., 2011). This should be distinguished\nfrom zero-shot learning, in which the model cannot look\nat any examples from the target classes (Palatucci et al.,\n2009).\nOne-shot learning can be directly addressed by develop-\ning domain-speciﬁc features or inference procedures which\npossess highly discriminative properties for the target task.\nAs a result, systems which incorporate these methods tend\nto excel at similar instances but fail to offer robust solutions\nthat may be applied to other types of problems. In this pa-\nper, we present a novel approach which limits assumptions\non the structure of the inputs while automatically acquir-\ning features which enable the model to generalize success-\nfully from few examples. We build upon the deep learn-\n\n\nSiamese Neural Networks for One-shot Image Recognition\nFigure 2. Our general strategy. 1) Train a model to discriminate\nbetween a collection of same/different pairs. 2) Generalize to\nevaluate new categories based on learned feature mappings for\nveriﬁcation.\ning framework, which uses many layers of non-linearities\nto capture invariances to transformation in the input space,\nusually by leveraging a model with many parameters and\nthen using a large amount of data to prevent overﬁtting\n(Bengio, 2009; Hinton et al., 2006). These features are\nvery powerful because we are able to learn them without\nimposing strong priors, although the cost of the learning\nalgorithm itself may be considerable.\n1. Approach\nIn general, we learn image representations via a supervised\nmetric-based approach with siamese neural networks, then\nreuse that network’s features for one-shot learning without\nany retraining.\nIn our experiments, we restrict our attention to character\nrecognition, although the basic approach can be replicated\nfor almost any modality (Figure 2). For this domain, we\nemploy large siamese convolutional neural networks which\na) are capable of learning generic image features useful\nfor making predictions about unknown class distributions\neven when very few examples from these new distribu-\ntions are available; b) are easily trained using standard\noptimization techniques on pairs sampled from the source\ndata; and c) provide a competitive approach that does not\nrely upon domain-speciﬁc knowledge by instead exploiting\ndeep learning techniques.\nTo develop a model for one-shot image classiﬁcation, we\naim to ﬁrst learn a neural network that can discriminate\nbetween the class-identity of image pairs, which is the\nstandard veriﬁcation task for image recognition. We hy-\npothesize that networks which do well at at veriﬁcation\nshould generalize to one-shot classiﬁcation. The veriﬁca-\ntion model learns to identify input pairs according to the\nprobability that they belong to the same class or differ-\nent classes. This model can then be used to evaluate new\nimages, exactly one per novel class, in a pairwise manner\nagainst the test image. The pairing with the highest score\naccording to the veriﬁcation network is then awarded the\nhighest probability for the one-shot task. If the features\nlearned by the veriﬁcation model are sufﬁcient to conﬁrm\nor deny the identity of characters from one set of alpha-\nbets, then they ought to be sufﬁcient for other alphabets,\nprovided that the model has been exposed to a variety of\nalphabets to encourage variance amongst the learned fea-\ntures.\n2. Related Work\nOverall, research into one-shot learning algorithms is fairly\nimmature and has received limited attention by the machine\nlearning community. There are nevertheless a few key lines\nof work which precede this paper.\nThe seminal work towards one-shot learning dates back to\nthe early 2000’s with work by Li Fei-Fei et al. The au-\nthors developed a variational Bayesian framework for one-\nshot image classiﬁcation using the premise that previously\nlearned classes can be leveraged to help forecast future\nones when very few examples are available from a given\nclass (Fe-Fei et al., 2003; Fei-Fei et al., 2006). More re-\ncently, Lake et al.\napproached the problem of one-shot\nlearning from the point of view of cognitive science, ad-\ndressing one-shot learning for character recognition with\na method called Hierarchical Bayesian Program Learning\n(HBPL) (2013). In a series of several papers, the authors\nmodeled the process of drawing characters generatively to\ndecompose the image into small pieces (Lake et al., 2011;\n2012). The goal of HBPL is to determine a structural ex-\nplanation for the observed pixels. However, inference un-\nder HBPL is difﬁcult since the joint parameter space is very\nlarge, leading to an intractable integration problem.\nSome researchers have considered other modalities or\ntransfer learning approaches. Lake et al. have some very\nrecent work which uses a generative Hierarchical Hid-\nden Markov model for speech primitives combined with\na Bayesian inference procedure to recognize new words by\nunknown speakers (2014). Maas and Kemp have some of\nthe only published work using Bayesian networks to pre-\ndict attributes for Ellis Island passenger data (2009). Wu\nand Dennis address one-shot learning in the context of path\nplanning algorithms for robotic actuation (2012). Lim fo-\ncuses on how to “borrow” examples from other classes in\nthe training set by adapting a measure of how much each\ncategory should be weighted by each training exemplar in\nthe loss function (2012). This idea can be useful for data\n\n\nSiamese Neural Networks for One-shot Image Recognition\nFigure 3. A simple 2 hidden layer siamese network for binary\nclassiﬁcation with logistic prediction p. The structure of the net-\nwork is replicated across the top and bottom sections to form twin\nnetworks, with shared weight matrices at each layer.\nsets where very few examples exist for some classes, pro-\nviding a ﬂexible and continuous means of incorporating\ninter-class information into the model.\n3. Deep Siamese Networks for Image\nVeriﬁcation\nSiamese nets were ﬁrst introduced in the early 1990s by\nBromley and LeCun to solve signature veriﬁcation as an\nimage matching problem (Bromley et al., 1993). A siamese\nneural network consists of twin networks which accept dis-\ntinct inputs but are joined by an energy function at the top.\nThis function computes some metric between the highest-\nlevel feature representation on each side (Figure 3). The\nparameters between the twin networks are tied. Weight ty-\ning guarantees that two extremely similar images could not\npossibly be mapped by their respective networks to very\ndifferent locations in feature space because each network\ncomputes the same function. Also, the network is symmet-\nric, so that whenever we present two distinct images to the\ntwin networks, the top conjoining layer will compute the\nsame metric as if we were to we present the same two im-\nages but to the opposite twins.\nIn LeCun et al., the authors used a contrastive energy func-\ntion which contained dual terms to decrease the energy of\nlike pairs and increase the energy of unlike pairs (2005).\nHowever, in this paper we use the weighted L1 distance\nbetween the twin feature vectors h1 and h2 combined with\na sigmoid activation, which maps onto the interval [0, 1].\nThus a cross-entropy objective is a natural choice for train-\ning the network. Note that in LeCun et al., they directly\nlearned the similarity metric, which was implictly deﬁned\nby the energy loss, whereas we ﬁx the metric as speciﬁed\nabove, following the approach in Facebook’s DeepFace pa-\nper (Taigman et al., 2014).\nOur best-performing models use multiple convolutional\nlayers before the fully-connected layers and top-level\nenergy function.\nConvolutional neural networks have\nachieved exceptional results in many large-scale computer\nvision applications, particularly in image recognition tasks\n(Bengio, 2009; Krizhevsky et al., 2012; Simonyan & Zis-\nserman, 2014; Srivastava, 2013).\nSeveral factors make convolutional networks especially ap-\npealing. Local connectivity can greatly reduce the num-\nber of parameters in the model, which inherently provides\nsome form of built-in regularization, although convolu-\ntional layers are computationally more expensive than stan-\ndard nonlinearities. Also, the convolution operation used in\nthese networks has a direct ﬁltering interpretation, where\neach feature map is convolved against input features to\nidentify patterns as groupings of pixels.\nThus, the out-\nputs of each convolutional layer correspond to important\nspatial features in the original input space and offer some\nrobustness to simple transforms. Finally, very fast CUDA\nlibraries are now available in order to build large convolu-\ntional networks without an unacceptable amount of train-\ning time (Mnih, 2009; Krizhevsky et al., 2012; Simonyan\n& Zisserman, 2014).\nWe now detail both the structure of the siamese nets and the\nspeciﬁcs of the learning algorithm used in our experiments.\n3.1. Model\nOur standard model is a siamese convolutional neural net-\nwork with L layers each with Nl units, where h1,l repre-\nsents the hidden vector in layer l for the ﬁrst twin, and h2,l\ndenotes the same for the second twin. We use exclusively\nrectiﬁed linear (ReLU) units in the ﬁrst L −2 layers and\nsigmoidal units in the remaining layers.\nThe model consists of a sequence of convolutional layers,\neach of which uses a single channel with ﬁlters of varying\nsize and a ﬁxed stride of 1. The number of convolutional\nﬁlters is speciﬁed as a multiple of 16 to optimize perfor-\nmance. The network applies a ReLU activation function\nto the output feature maps, optionally followed by max-\npooling with a ﬁlter size and stride of 2. Thus the kth ﬁlter\nmap in each layer takes the following form:\na(k)\n1,m = max-pool(max(0, W(k)\nl−1,l ⋆h1,(l−1) + bl), 2)\na(k)\n2,m = max-pool(max(0, W(k)\nl−1,l ⋆h2,(l−1) + bl), 2)\nwhere Wl−1,l is the 3-dimensional tensor representing the\nfeature maps for layer l and we have taken ⋆to be the\nvalid convolutional operation corresponding to returning\n\n\nSiamese Neural Networks for One-shot Image Recognition\nFigure 4. Best convolutional architecture selected for veriﬁcation task. Siamese twin is not depicted, but joins immediately after the\n4096 unit fully-connected layer where the L1 component-wise distance between vectors is computed.\nonly those output units which were the result of complete\noverlap between each convolutional ﬁlter and the input fea-\nture maps.\nThe units in the ﬁnal convolutional layer are ﬂattened into\na single vector. This convolutional layer is followed by\na fully-connected layer, and then one more layer com-\nputing the induced distance metric between each siamese\ntwin, which is given to a single sigmoidal output unit.\nMore precisely, the prediction vector is given as p =\nσ(P\nj αj|h(j)\n1,L−1 −h(j)\n2,L−1|), where σ is the sigmoidal\nactivation function. This ﬁnal layer induces a metric on\nthe learned feature space of the (L −1)th hidden layer\nand scores the similarity between the two feature vec-\ntors.\nThe αj are additional parameters that are learned\nby the model during training, weighting the importance\nof the component-wise distance. This deﬁnes a ﬁnal Lth\nfully-connected layer for the network which joins the two\nsiamese twins.\nWe depict one example above (Figure 4), which shows the\nlargest version of our model that we considered. This net-\nwork also gave the best result for any network on the veri-\nﬁcation task.\n3.2. Learning\nLoss function. Let M represent the minibatch size, where\ni indexes the ith minibatch.\nNow let y(x(i)\n1 , x(i)\n2 ) be a\nlength-M vector which contains the labels for the mini-\nbatch, where we assume y(x(i)\n1 , x(i)\n2 ) = 1 whenever x1 and\nx2 are from the same character class and y(x(i)\n1 , x(i)\n2 ) = 0\notherwise. We impose a regularized cross-entropy objec-\ntive on our binary classiﬁer of the following form:\nL(x(i)\n1 , x(i)\n2 ) = y(x(i)\n1 , x(i)\n2 ) log p(x(i)\n1 , x(i)\n2 )+\n(1 −y(x(i)\n1 , x(i)\n2 )) log (1 −p(x(i)\n1 , x(i)\n2 )) + λT |w|2\nOptimization. This objective is combined with standard\nbackpropagation algorithm, where the gradient is additive\nacross the twin networks due to the tied weights. We ﬁx\na minibatch size of 128 with learning rate ηj, momentum\nµj, and L2 regularization weights λj deﬁned layer-wise, so\nthat our update rule at epoch T is as follows:\nw(T )\nkj (x(i)\n1 , x(i)\n2 ) = w(T )\nkj + ∆w(T )\nkj (x(i)\n1 , x(i)\n2 ) + 2λj|wkj|\n∆w(T )\nkj (x(i)\n1 , x(i)\n2 ) = −ηj∇w(T )\nkj + µj∆w(T −1)\nkj\nwhere ∇wkj is the partial derivative with respect to the\nweight between the jth neuron in some layer and the kth\nneuron in the successive layer.\nWeight initialization. We initialized all network weights\nin the convolutional layers from a normal distribution with\nzero-mean and a standard deviation of 10−2. Biases were\nalso initialized from a normal distribution, but with mean\n0.5 and standard deviation 10−2. In the fully-connected\nlayers, the biases were initialized in the same way as the\nconvolutional layers, but the weights were drawn from a\nmuch wider normal distribution with zero-mean and stan-\ndard deviation 2 × 10−1.\nLearning schedule. Although we allowed for a different\nlearning rate for each layer, learning rates were decayed\nuniformly across the network by 1 percent per epoch, so\nthat η(T )\nj\n= 0.99η(T −1)\nj\n. We found that by annealing the\nlearning rate, the network was able to converge to local\nminima more easily without getting stuck in the error sur-\nface. We ﬁxed momentum to start at 0.5 in every layer,\nincreasing linearly each epoch until reaching the value µj,\nthe individual momentum term for the jth layer.\nWe trained each network for a maximum of 200 epochs, but\nmonitored one-shot validation error on a set of 320 one-\nshot learning tasks generated randomly from the alphabets\nand drawers in the validation set. When the validation error\ndid not decrease for 20 epochs, we stopped and used the\nparameters of the model at the best epoch according to the\none-shot validation error. If the validation error continued\nto decrease for the entire learning schedule, we saved the\nﬁnal state of the model generated by this procedure.\nHyperparameter optimization.\nWe used the beta ver-\nsion of Whetlab, a Bayesian optimization framework, to\n\n\nSiamese Neural Networks for One-shot Image Recognition\nFigure 5. A sample of random afﬁne distortions generated for a\nsingle character in the Omniglot data set.\nperform hyperparameter selection.\nFor learning sched-\nule and regularization hyperparameters, we set the layer-\nwise learning rate ηj ∈[10−4, 10−1], layer-wise momen-\ntum µj ∈[0, 1], and layer-wise L2 regularization penalty\nλj ∈[0, 0.1]. For network hyperparameters, we let the size\nof convolutional ﬁlters vary from 3x3 to 20x20, while the\nnumber of convolutional ﬁlters in each layer varied from\n16 to 256 using multiples of 16. Fully-connected layers\nranged from 128 to 4096 units, also in multiples of 16. We\nset the optimizer to maximize one-shot validation set accu-\nracy. The score assigned to a single Whetlab iteration was\nthe highest value of this metric found during any epoch.\nAfﬁne distortions. In addition, we augmented the train-\ning set with small afﬁne distortions (Figure 5). For each\nimage pair x1, x2, we generated a pair of afﬁne trans-\nformations T1, T2 to yield x′\n1 = T1(x1), x′\n2 = T2(x2),\nwhere T1, T2 are determined stochastically by a multi-\ndimensional uniform distribution. So for an arbitrary trans-\nform T, we have T = (θ, ρx, ρy, sx, sy, tx, tx), with θ ∈\n[−10.0, 10.0], ρx, ρy ∈[−0.3, 0.3], sx, sy ∈[0.8, 1.2], and\ntx, ty ∈[−2, 2]. Each of these components of the transfor-\nmation is included with probability 0.5.\n4. Experiments\nWe trained our model on a subset of the Omniglot data set,\nwhich we ﬁrst describe. We then provide details with re-\nspect to veriﬁcation and one-shot performance.\n4.1. The Omniglot Dataset\nThe Omniglot data set was collected by Brenden Lake and\nhis collaborators at MIT via Amazon’s Mechanical Turk to\nproduce a standard benchmark for learning from few exam-\nples in the handwritten character recognition domain (Lake\net al., 2011).1 Omniglot contains examples from 50 alpha-\nbets ranging from well-established international languages\n1The complete data set can be obtained from Brenden Lake by\nrequest (brenden@cs.nyu.edu). Each character in Omniglot\nis a 105x105 binary-valued image which was drawn by hand on\nan online canvas. The stroke trajectories were collected alongside\nthe composite images, so it is possible to incorporate temporal\nand structural information into models trained on Omniglot.\nFigure 6. The Omniglot dataset contains a variety of different im-\nages from alphabets across the world.\nlike Latin and Korean to lesser known local dialects. It also\nincludes some ﬁctitious character sets such as Aurek-Besh\nand Klingon (Figure 6).\nThe number of letters in each alphabet varies considerably\nfrom about 15 to upwards of 40 characters. All charac-\nters across these alphabets are produced a single time by\neach of 20 drawers Lake split the data into a 40 alpha-\nbet background set and a 10 alphabet evaluation set. We\npreserve these two terms in order to distinguish from the\nnormal training, validation, and test sets that can be gener-\nated from the background set in order to tune models for\nveriﬁcation. The background set is used for developing a\nmodel by learning hyperparameters and feature mappings.\nConversely, the evaluation set is used only to measure the\none-shot classiﬁcation performance.\n4.2. Veriﬁcation\nTo train our veriﬁcation network, we put together three dif-\nferent data set sizes with 30,000, 90,000, and 150,000 train-\ning examples by sampling random same and different pairs.\nWe set aside sixty percent of the total data for training: 30\nalphabets out of 50 and 12 drawers out of 20.\nWe ﬁxed a uniform number of training examples per alpha-\nbet so that each alphabet receives equal representation dur-\ning optimization, although this is not guaranteed to the in-\ndividual character classes within each alphabet. By adding\nTable 1. Accuracy on Omniglot veriﬁcation task (siamese convo-\nlutional neural net)\nMethod\nTest\n30k training\nno distortions\n90.61\nafﬁne distortions x8\n91.90\n90k training\nno distortions\n91.54\nafﬁne distortions x8\n93.15\n150k training\nno distortions\n91.63\nafﬁne distortions x8\n93.42\n\n\nSiamese Neural Networks for One-shot Image Recognition\nFigure 7. Examples of ﬁrst-layer convolutional ﬁlters learned by\nthe siamese network. Notice ﬁlters adapt different roles: some\nlook for very small point-wise features whereas others function\nlike larger-scale edge detectors.\nafﬁne distortions, we also produced an additional copy of\nthe data set corresponding to the augmented version of each\nof these sizes. We added eight transforms for each train-\ning example, so the corresponding data sets have 270,000,\n810,000, and 1,350,000 effective examples.\nTo monitor performance during training, we used two\nstrategies. First, we created a validation set for veriﬁcation\nwith 10,000 example pairs taken from 10 alphabets and 4\nadditional drawers. We reserved the last 10 alphabets and\n4 drawers for testing, where we constrained these to be the\nsame ones used in Lake et al. (Lake et al., 2013). Our\nother strategy leveraged the same alphabets and drawers\nto generate a set of 320 one-shot recognition trials for the\nvalidation set which mimic the target task on the evaluation\nset. In practice, this second method of determining when to\nstop was at least as effective as the validation error for the\nveriﬁcation task so we used it as our termination criterion.\nIn the table below (Table 1), we list the ﬁnal veriﬁcation\nresults for each of the six possible training sets, where the\nlisted test accuracy is reported at the best validation check-\npoint and threshold. We report results across six different\ntraining runs, varying the training set size and toggling dis-\ntortions.\nIn Figure 7, we have extracted the ﬁrst 32 ﬁlters from both\nof our top two performing networks on the veriﬁcation task,\nwhich were trained on the 90k and 150k data sets with\nafﬁne distortions and the architecture shown in Figure 3.\nWhile there is some co-adaptation between ﬁlters, it is easy\nto see that some of the ﬁlters have assumed different roles\nwith respect to the original input space.\n4.3. One-shot Learning\nOnce we have optimized a siamese network to master the\nveriﬁcation task, we are ready to demonstrate the discrimi-\nnative potential of our learned features at one-shot learning.\nSuppose we are given a test image x, some column vector\nwhich we wish to classify into one of C categories. We\nare also given some other images {xc}C\nc=1, a set of col-\nTable 2. Comparing best one-shot accuracy from each type of\nnetwork against baselines.\nMethod\nTest\nHumans\n95.5\nHierarchical Bayesian Program Learning\n95.2\nAfﬁne model\n81.8\nHierarchical Deep\n65.2\nDeep Boltzmann Machine\n62.0\nSimple Stroke\n35.2\n1-Nearest Neighbor\n21.7\nSiamese Neural Net\n58.3\nConvolutional Siamese Net\n92.0\numn vectors representing examples of each of those C cat-\negories. We can now query the network using x, xc as our\ninput for a range of c = 1, . . . , C.2 Then predict the class\ncorresponding to the maximum similarity.\nC∗= argmaxcp(c)\nTo empirically evaluate one-shot learning performance,\nLake developed a 20-way within-alphabet classiﬁcation\ntask in which an alphabet is ﬁrst chosen from among those\nreserved for the evaluation set, along with twenty charac-\nters taken uniformly at random. Two of the twenty drawers\nare also selected from among the pool of evaluation draw-\ners. These two drawers then produce a sample of the twenty\ncharacters. Each one of the characters produced by the ﬁrst\ndrawer are denoted as test images and individually com-\npared against all twenty characters from the second drawer,\nwith the goal of predicting the class corresponding to the\ntest image from among all of the second drawer’s charac-\nters. An individual example of a one-shot learning trial is\ndepicted in Figure 7. This process is repeated twice for\nall alphabets, so that there are 40 one-shot learning trials\nfor each of the ten evaluation alphabets. This constitutes a\ntotal of 400 one-shot learning trials, from which the classi-\nﬁcation accuracy is calculated.\nThe one-shot results are given in Table 2.\nWe borrow\nthe baseline results from (Lake et al., 2013) for compari-\nson to our method. We also include results from a non-\nconvolutional siamese network with two fully-connected\nlayers.\nAt 92 percent our convolutional method is stronger than\nany model except HBPL itself. which is only slightly be-\nhind human error rates. While HBPL exhibits stronger re-\nsults overall, our top-performing convolutional network did\n2This can be processed efﬁciently by appending C copies of\nx into a single matrix X and stacking xT\nc in rows to form another\nmatrix XC so that we can perform just one feedforward pass with\nminibatch size C using input X, XC.\n\n\nSiamese Neural Networks for One-shot Image Recognition\nTable 3. Results from MNIST 10-versus-1 one-shot classiﬁcation\ntask.\nMethod\nTest\n1-Nearest Neighbor\n26.5\nConvolutional Siamese Net\n70.3\nnot include any extra prior knowledge about characters or\nstrokes such as generative information about the drawing\nprocess. This is the primary advantage of our model.\n4.4. MNIST One-shot Trial\nThe Omniglot data set contains a small handful of samples\nfor every possible class of letter; for this reason, the original\nauthors refer to it as a sort of “MNIST transpose”, where\nthe number of classes far exceeds the number of training\ninstances (Lake et al., 2013). We thought it would be in-\nteresting to monitor how well a model trained on Omniglot\ncan generalize to MNIST, where we treat the 10 digits in\nMNIST as an alphabet and then evaluate a 10-way one-\nshot classiﬁcation task. We followed a similar procedure\nto Omniglot, generating 400 one-shot trials on the MNIST\ntest set, but excluding any ﬁne tuning on the training set.\nAll 28x28 images were upsampled to 35x35, then given to\na reduced version of our model trained on 35x35 images\nfrom Omniglot which were downsampled by a factor of\n3. We also evaluated the nearest-neighbor baseline on this\ntask.\nTable 3 shows the results from this experiment. The near-\nest neighbor baseline provides similar performance to Om-\nniglot, while the performance of the convolutional network\ndrops by a more signiﬁcant amount. However, we are still\nable to achieve reasonable generalization from the features\nlearned on Ominglot without training at all on MNIST.\n5. Conclusions\nWe have presented a strategy for performing one-shot clas-\nsiﬁcation by ﬁrst learning deep convolutional siamese neu-\nral networks for veriﬁcation.\nWe outlined new results\ncomparing the performance of our networks to an exist-\ning state-of-the-art classiﬁer developed for the Omniglot\ndata set. Our networks outperform all available baselines\nby a signiﬁcant margin and come close to the best num-\nbers achieved by the previous authors. We have argued that\nthe strong performance of these networks on this task indi-\ncate not only that human-level accuracy is possible with our\nmetric learning approach, but that this approach should ex-\ntend to one-shot learning tasks in other domains, especially\nfor image classiﬁcation.\nIn this paper, we only considered training for the veriﬁca-\nFigure 8. Two sets of stroke distortions for different characters\nfrom Omniglot. Columns depict characters sampled from differ-\nent drawers. Row 1: original images. Row 2: global afﬁne trans-\nforms. Row 3: afﬁne transforms on strokes. Row 4: global afﬁne\ntransforms layered on top of stroke transforms. Notice how stroke\ndistortions can add noise and affect the spatial relations between\nindividual strokes.\ntion task by processing image pairs and their distortions\nusing a global afﬁne transform.\nWe have been experi-\nmenting with an extended algorithm that exploits the data\nabout the individual stroke trajectories to produce ﬁnal\ncomputed distortions (Figure 8). By imposing local afﬁne\ntransformations on the strokes and overlaying them into\na composite image, we are hopeful that we can learn\nfeatures which are better adapted to the variations that are\ncommonly seen in new examples.\nReferences\nBengio, Yoshua. Learning deep architectures for ai. Foun-\ndations and Trends in Machine Learning, 2(1):1–127,\n2009.\nBromley, Jane, Bentz, James W, Bottou, L´eon, Guyon,\nIsabelle, LeCun, Yann, Moore, Cliff, S¨ackinger, Ed-\nuard, and Shah, Roopak. Signature veriﬁcation using a\nsiamese time delay neural network. International Jour-\nnal of Pattern Recognition and Artiﬁcial Intelligence, 7\n(04):669–688, 1993.\nChopra, Sumit, Hadsell, Raia, and LeCun, Yann. Learning\na similarity metric discriminatively, with application to\nface veriﬁcation. In Computer Vision and Pattern Recog-\nnition, 2005. CVPR 2005. IEEE Computer Society Con-\nference on, volume 1, pp. 539–546. IEEE, 2005.\n\n\nSiamese Neural Networks for One-shot Image Recognition\nFe-Fei, Li, Fergus, Robert, and Perona, Pietro. A bayesian\napproach to unsupervised one-shot learning of object\ncategories.\nIn Computer Vision, 2003. Proceedings.\nNinth IEEE International Conference on, pp. 1134–\n1141. IEEE, 2003.\nFei-Fei, Li, Fergus, Robert, and Perona, Pietro. One-shot\nlearning of object categories. Pattern Analysis and Ma-\nchine Intelligence, IEEE Transactions on, 28(4):594–\n611, 2006.\nHinton, Geoffrey, Osindero, Simon, and Teh, Yee-Whye.\nA fast learning algorithm for deep belief nets. Neural\ncomputation, 18(7):1527–1554, 2006.\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.\nImagenet classiﬁcation with deep convolutional neural\nnetworks. In Advances in neural information processing\nsystems, pp. 1097–1105, 2012.\nLake, Brenden M, Salakhutdinov, Ruslan, Gross, Jason,\nand Tenenbaum, Joshua B. One shot learning of simple\nvisual concepts. In Proceedings of the 33rd Annual Con-\nference of the Cognitive Science Society, volume 172,\n2011.\nLake, Brenden M, Salakhutdinov, Ruslan, and Tenenbaum,\nJoshua B. Concept learning as motor program induction:\nA large-scale empirical study. In Proceedings of the 34th\nAnnual Conference of the Cognitive Science Society, pp.\n659–664, 2012.\nLake, Brenden M, Salakhutdinov, Ruslan R, and Tenen-\nbaum, Josh. One-shot learning by inverting a composi-\ntional causal process. In Advances in neural information\nprocessing systems, pp. 2526–2534, 2013.\nLake, Brenden M, Lee, Chia-ying, Glass, James R, and\nTenenbaum, Joshua B. One-shot learning of generative\nspeech concepts. Cognitive Science Society, 2014.\nLim, Joseph Jaewhan. Transfer learning by borrowing ex-\namples for multiclass object detection. Master’s thesis,\nMassachusetts Institute of Technology, 2012.\nMaas, Andrew and Kemp, Charles. One-shot learning with\nbayesian networks. Cognitive Science Society, 2009.\nMnih, Volodymyr. Cudamat: a cuda-based matrix class for\npython. 2009.\nPalatucci, Mark, Pomerleau, Dean, Hinton, Geoffrey E,\nand Mitchell, Tom M. Zero-shot learning with seman-\ntic output codes. In Advances in neural information pro-\ncessing systems, pp. 1410–1418, 2009.\nSimonyan, Karen and Zisserman, Andrew. Very deep con-\nvolutional networks for large-scale image recognition.\narXiv preprint arXiv:1409.1556, 2014.\nSrivastava, Nitish.\nImproving neural networks with\ndropout. Master’s thesis, University of Toronto, 2013.\nTaigman, Yaniv, Yang, Ming, Ranzato, Marc’Aurelio, and\nWolf, Lior. Deepface: Closing the gap to human-level\nperformance in face veriﬁcation. In Computer Vision and\nPattern Recognition (CVPR), 2014 IEEE Conference on,\npp. 1701–1708. IEEE, 2014.\nWu, Di, Zhu, Fan, and Shao, Ling.\nOne shot learning\ngesture recognition from rgbd images.\nIn Computer\nVision and Pattern Recognition Workshops (CVPRW),\n2012 IEEE Computer Society Conference on, pp. 7–12.\nIEEE, 2012.\n"
}