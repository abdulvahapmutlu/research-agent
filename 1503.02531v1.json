{
  "filename": "1503.02531v1.pdf",
  "num_pages": 9,
  "pages": [
    "arXiv:1503.02531v1  [stat.ML]  9 Mar 2015\nDistilling the Knowledge in a Neural Network\nGeoffrey Hinton∗†\nGoogle Inc.\nMountain View\ngeoffhinton@google.com\nOriol Vinyals†\nGoogle Inc.\nMountain View\nvinyals@google.com\nJeff Dean\nGoogle Inc.\nMountain View\njeff@google.com\nAbstract\nA very simple way to improve the performance of almost any machine learning\nalgorithm is to train many different models on the same data and then to average\ntheir predictions [3]. Unfortunately, making predictions using a whole ensemble\nof models is cumbersome and may be too computationally expensive to allow de-\nployment to a large number of users, especially if the individual models are large\nneural nets. Caruana and his collaborators [1] have shown that it is possible to\ncompress the knowledge in an ensemble into a single model which is much eas-\nier to deploy and we develop this approach further using a different compression\ntechnique. We achieve some surprising results on MNIST and we show that we\ncan signiﬁcantly improve the acoustic model of a heavily used commercial system\nby distilling the knowledge in an ensemble of models into a single model. We also\nintroduce a new type of ensemble composed of one or more full models and many\nspecialist models which learn to distinguish ﬁne-grained classes that the full mod-\nels confuse. Unlike a mixture of experts, these specialist models can be trained\nrapidly and in parallel.\n1\nIntroduction\nMany insects have a larval form that is optimized for extracting energy and nutrients from the envi-\nronment and a completely different adult form that is optimized for the very different requirements\nof traveling and reproduction. In large-scale machine learning, we typically use very similar models\nfor the training stage and the deployment stage despite their very different requirements: For tasks\nlike speech and object recognition, training must extract structure from very large, highly redundant\ndatasets but it does not need to operate in real time and it can use a huge amount of computation.\nDeployment to a large number of users, however, has much more stringent requirements on latency\nand computational resources. The analogy with insects suggests that we should be willing to train\nvery cumbersome models if that makes it easier to extract structure from the data. The cumbersome\nmodel could be an ensemble of separately trained models or a single very large model trained with\na very strong regularizer such as dropout [9]. Once the cumbersome model has been trained, we\ncan then use a different kind of training, which we call “distillation” to transfer the knowledge from\nthe cumbersome model to a small model that is more suitable for deployment. A version of this\nstrategy has already been pioneered by Rich Caruana and his collaborators [1]. In their important\npaper they demonstrate convincingly that the knowledge acquired by a large ensemble of models\ncan be transferred to a single small model.\nA conceptual block that may have prevented more investigation of this very promising approach is\nthat we tend to identify the knowledge in a trained model with the learned parameter values and this\nmakes it hard to see how we can change the form of the model but keep the same knowledge. A more\nabstract view of the knowledge, that frees it from any particular instantiation, is that it is a learned\n∗Also afﬁliated with the University of Toronto and the Canadian Institute for Advanced Research.\n†Equal contribution.\n1\n",
    "mapping from input vectors to output vectors. For cumbersome models that learn to discriminate\nbetween a large number of classes, the normal training objective is to maximize the average log\nprobability of the correct answer, but a side-effect of the learning is that the trained model assigns\nprobabilities to all of the incorrect answers and even when these probabilities are very small, some\nof them are much larger than others. The relative probabilities of incorrect answers tell us a lot about\nhow the cumbersome model tends to generalize. An image of a BMW, for example, may only have\na very small chance of being mistaken for a garbage truck, but that mistake is still many times more\nprobable than mistaking it for a carrot.\nIt is generally accepted that the objective function used for training should reﬂect the true objective\nof the user as closely as possible. Despite this, models are usually trained to optimize performance\non the training data when the real objective is to generalize well to new data. It would clearly\nbe better to train models to generalize well, but this requires information about the correct way to\ngeneralize and this information is not normally available. When we are distilling the knowledge\nfrom a large model into a small one, however, we can train the small model to generalize in the same\nway as the large model. If the cumbersome model generalizes well because, for example, it is the\naverage of a large ensemble of different models, a small model trained to generalize in the same way\nwill typically do much better on test data than a small model that is trained in the normal way on the\nsame training set as was used to train the ensemble.\nAn obvious way to transfer the generalization ability of the cumbersome model to a small model is\nto use the class probabilities produced by the cumbersome model as “soft targets” for training the\nsmall model. For this transfer stage, we could use the same training set or a separate “transfer” set.\nWhen the cumbersome model is a large ensemble of simpler models, we can use an arithmetic or\ngeometric mean of their individual predictive distributions as the soft targets. When the soft targets\nhave high entropy, they provide much more information per training case than hard targets and much\nless variance in the gradient between training cases, so the small model can often be trained on much\nless data than the original cumbersome model and using a much higher learning rate.\nFor tasks like MNIST in which the cumbersome model almost always produces the correct answer\nwith very high conﬁdence, much of the information about the learned function resides in the ratios\nof very small probabilities in the soft targets. For example, one version of a 2 may be given a\nprobability of 10−6 of being a 3 and 10−9 of being a 7 whereas for another version it may be the\nother way around. This is valuable information that deﬁnes a rich similarity structure over the data\n(i. e. it says which 2’s look like 3’s and which look like 7’s) but it has very little inﬂuence on the\ncross-entropy cost function during the transfer stage because the probabilities are so close to zero.\nCaruana and his collaborators circumvent this problem by using the logits (the inputs to the ﬁnal\nsoftmax) rather than the probabilities produced by the softmax as the targets for learning the small\nmodel and they minimize the squared difference between the logits produced by the cumbersome\nmodel and the logits produced by the small model. Our more general solution, called “distillation”,\nis to raise the temperature of the ﬁnal softmax until the cumbersome model produces a suitably soft\nset of targets. We then use the same high temperature when training the small model to match these\nsoft targets. We show later that matching the logits of the cumbersome model is actually a special\ncase of distillation.\nThe transfer set that is used to train the small model could consist entirely of unlabeled data [1]\nor we could use the original training set. We have found that using the original training set works\nwell, especially if we add a small term to the objective function that encourages the small model\nto predict the true targets as well as matching the soft targets provided by the cumbersome model.\nTypically, the small model cannot exactly match the soft targets and erring in the direction of the\ncorrect answer turns out to be helpful.\n2\nDistillation\nNeural networks typically produce class probabilities by using a “softmax” output layer that converts\nthe logit, zi, computed for each class into a probability, qi, by comparing zi with the other logits.\nqi =\nexp(zi/T )\nP\nj exp(zj/T )\n(1)\n2\n",
    "where T is a temperature that is normally set to 1. Using a higher value for T produces a softer\nprobability distribution over classes.\nIn the simplest form of distillation, knowledge is transferred to the distilled model by training it on\na transfer set and using a soft target distribution for each case in the transfer set that is produced by\nusing the cumbersome model with a high temperature in its softmax. The same high temperature is\nused when training the distilled model, but after it has been trained it uses a temperature of 1.\nWhen the correct labels are known for all or some of the transfer set, this method can be signiﬁcantly\nimproved by also training the distilled model to produce the correct labels. One way to do this is\nto use the correct labels to modify the soft targets, but we found that a better way is to simply use\na weighted average of two different objective functions. The ﬁrst objective function is the cross\nentropy with the soft targets and this cross entropy is computed using the same high temperature in\nthe softmax of the distilled model as was used for generating the soft targets from the cumbersome\nmodel. The second objective function is the cross entropy with the correct labels. This is computed\nusing exactly the same logits in softmax of the distilled model but at a temperature of 1. We found\nthat the best results were generally obtained by using a condiderably lower weight on the second\nobjective function. Since the magnitudes of the gradients produced by the soft targets scale as 1/T 2\nit is important to multiply them by T 2 when using both hard and soft targets. This ensures that the\nrelative contributions of the hard and soft targets remain roughly unchanged if the temperature used\nfor distillation is changed while experimenting with meta-parameters.\n2.1\nMatching logits is a special case of distillation\nEach case in the transfer set contributes a cross-entropy gradient, dC/dzi, with respect to each\nlogit, zi of the distilled model. If the cumbersome model has logits vi which produce soft target\nprobabilities pi and the transfer training is done at a temperature of T , this gradient is given by:\n∂C\n∂zi\n= 1\nT (qi −pi) = 1\nT\n \nezi/T\nP\nj ezj/T −\nevi/T\nP\nj evj/T\n!\n(2)\nIf the temperature is high compared with the magnitude of the logits, we can approximate:\n∂C\n∂zi\n≈1\nT\n \n1 + zi/T\nN + P\nj zj/T −\n1 + vi/T\nN + P\nj vj/T\n!\n(3)\nIf we now assume that the logits have been zero-meaned separately for each transfer case so that\nP\nj zj = P\nj vj = 0 Eq. 3 simpliﬁes to:\n∂C\n∂zi\n≈\n1\nNT 2 (zi −vi)\n(4)\nSo in the high temperature limit, distillation is equivalent to minimizing 1/2(zi −vi)2, provided the\nlogits are zero-meaned separately for each transfer case. At lower temperatures, distillation pays\nmuch less attention to matching logits that are much more negative than the average. This is poten-\ntially advantageous because these logits are almost completely unconstrained by the cost function\nused for training the cumbersome model so they could be very noisy. On the other hand, the very\nnegative logits may convey useful information about the knowledge acquired by the cumbersome\nmodel. Which of these effects dominates is an empirical question. We show that when the distilled\nmodel is much too small to capture all of the knowledege in the cumbersome model, intermedi-\nate temperatures work best which strongly suggests that ignoring the large negative logits can be\nhelpful.\n3\nPreliminary experiments on MNIST\nTo see how well distillation works, we trained a single large neural net with two hidden layers\nof 1200 rectiﬁed linear hidden units on all 60,000 training cases. The net was strongly regularized\nusing dropout and weight-constraints as described in [5]. Dropout can be viewed as a way of training\nan exponentially large ensemble of models that share weights. In addition, the input images were\n3\n",
    "jittered by up to two pixels in any direction. This net achieved 67 test errors whereas a smaller\nnet with two hidden layers of 800 rectiﬁed linear hidden units and no regularization achieved 146\nerrors. But if the smaller net was regularized solely by adding the additional task of matching the soft\ntargets produced by the large net at a temperature of 20, it achieved 74 test errors. This shows that\nsoft targets can transfer a great deal of knowledge to the distilled model, including the knowledge\nabout how to generalize that is learned from translated training data even though the transfer set does\nnot contain any translations.\nWhen the distilled net had 300 or more units in each of its two hidden layers, all temperatures above\n8 gave fairly similar results. But when this was radically reduced to 30 units per layer, temperatures\nin the range 2.5 to 4 worked signiﬁcantly better than higher or lower temperatures.\nWe then tried omitting all examples of the digit 3 from the transfer set. So from the perspective\nof the distilled model, 3 is a mythical digit that it has never seen. Despite this, the distilled model\nonly makes 206 test errors of which 133 are on the 1010 threes in the test set. Most of the errors\nare caused by the fact that the learned bias for the 3 class is much too low. If this bias is increased\nby 3.5 (which optimizes overall performance on the test set), the distilled model makes 109 errors\nof which 14 are on 3s. So with the right bias, the distilled model gets 98.6% of the test 3s correct\ndespite never having seen a 3 during training. If the transfer set contains only the 7s and 8s from the\ntraining set, the distilled model makes 47.3% test errors, but when the biases for 7 and 8 are reduced\nby 7.6 to optimize test performance, this falls to 13.2% test errors.\n4\nExperiments on speech recognition\nIn this section, we investigate the effects of ensembling Deep Neural Network (DNN) acoustic\nmodels that are used in Automatic Speech Recognition (ASR). We show that the distillation strategy\nthat we propose in this paper achieves the desired effect of distilling an ensemble of models into a\nsingle model that works signiﬁcantly better than a model of the same size that is learned directly\nfrom the same training data.\nState-of-the-art ASR systems currently use DNNs to map a (short) temporal context of features\nderived from the waveform to a probability distribution over the discrete states of a Hidden Markov\nModel (HMM) [4]. More speciﬁcally, the DNN produces a probability distribution over clusters of\ntri-phone states at each time and a decoder then ﬁnds a path through the HMM states that is the best\ncompromise between using high probability states and producing a transcription that is probable\nunder the language model.\nAlthough it is possible (and desirable) to train the DNN in such a way that the decoder (and, thus,\nthe language model) is taken into account by marginalizing over all possible paths, it is common to\ntrain the DNN to perform frame-by-frame classiﬁcation by (locally) minimizing the cross entropy\nbetween the predictions made by the net and the labels given by a forced alignment with the ground\ntruth sequence of states for each observation:\nθ = arg max\nθ′ P(ht|st; θ′)\nwhere θ are the parameters of our acoustic model P which maps acoustic observations at time t,\nst, to a probability, P(ht|st; θ′) , of the “correct” HMM state ht, which is determined by a forced\nalignment with the correct sequence of words. The model is trained with a distributed stochastic\ngradient descent approach.\nWe use an architecture with 8 hidden layers each containing 2560 rectiﬁed linear units and a ﬁnal\nsoftmax layer with 14,000 labels (HMM targets ht). The input is 26 frames of 40 Mel-scaled ﬁlter-\nbank coefﬁcients with a 10ms advance per frame and we predict the HMM state of 21st frame. The\ntotal number of parameters is about 85M. This is a slightly outdated version of the acoustic model\nused by Android voice search, and should be considered as a very strong baseline. To train the DNN\nacoustic model we use about 2000 hours of spoken English data, which yields about 700M training\nexamples. This system achieves a frame accuracy of 58.9%, and a Word Error Rate (WER) of 10.9%\non our development set.\n4\n",
    "System\nTest Frame Accuracy\nWER\nBaseline\n58.9%\n10.9%\n10xEnsemble\n61.1%\n10.7%\nDistilled Single model\n60.8%\n10.7%\nTable 1: Frame classiﬁcation accuracy and WER showing that the distilled single model performs\nabout as well as the averaged predictions of 10 models that were used to create the soft targets.\n4.1\nResults\nWe trained 10 separate models to predict P(ht|st; θ), using exactly the same architecture and train-\ning procedure as the baseline. The models are randomly initialized with different initial parameter\nvalues and we ﬁnd that this creates sufﬁcient diversity in the trained models to allow the averaged\npredictions of the ensemble to signiﬁcantly outperform the individual models. We have explored\nadding diversity to the models by varying the sets of data that each model sees, but we found this\nto not signiﬁcantly change our results, so we opted for the simpler approach. For the distillation we\ntried temperatures of [1, 2, 5, 10] and used a relative weight of 0.5 on the cross-entropy for the hard\ntargets, where bold font indicates the best value that was used for table 1 .\nTable 1 shows that, indeed, our distillation approach is able to extract more useful information from\nthe training set than simply using the hard labels to train a single model. More than 80% of the\nimprovement in frame classiﬁcation accuracy achieved by using an ensemble of 10 models is trans-\nferred to the distilled model which is similar to the improvement we observed in our preliminary\nexperiments on MNIST. The ensemble gives a smaller improvement on the ultimate objective of\nWER (on a 23K-word test set) due to the mismatch in the objective function, but again, the im-\nprovement in WER achieved by the ensemble is transferred to the distilled model.\nWe have recently become aware of related work on learning a small acoustic model by matching\nthe class probabilities of an already trained larger model [8]. However, they do the distillation at a\ntemperature of 1 using a large unlabeled dataset and their best distilled model only reduces the error\nrate of the small model by 28% of the gap between the error rates of the large and small models\nwhen they are both trained with hard labels.\n5\nTraining ensembles of specialists on very big datasets\nTraining an ensemble of models is a very simple way to take advantage of parallel computation and\nthe usual objection that an ensemble requires too much computation at test time can be dealt with\nby using distillation. There is, however, another important objection to ensembles: If the individual\nmodels are large neural networks and the dataset is very large, the amount of computation required\nat training time is excessive, even though it is easy to parallelize.\nIn this section we give an example of such a dataset and we show how learning specialist models that\neach focus on a different confusable subset of the classes can reduce the total amount of computation\nrequired to learn an ensemble. The main problem with specialists that focus on making ﬁne-grained\ndistinctions is that they overﬁt very easily and we describe how this overﬁtting may be prevented by\nusing soft targets.\n5.1\nThe JFT dataset\nJFT is an internal Google dataset that has 100 million labeled images with 15,000 labels. When we\ndid this work, Google’s baseline model for JFT was a deep convolutional neural network [7] that had\nbeen trained for about six months using asynchronous stochastic gradient descent on a large number\nof cores. This training used two types of parallelism [2]. First, there were many replicas of the\nneural net running on different sets of cores and processing different mini-batches from the training\nset. Each replica computes the average gradient on its current mini-batch and sends this gradient\nto a sharded parameter server which sends back new values for the parameters. These new values\nreﬂect all of the gradients received by the parameter server since the last time it sent parameters\nto the replica. Second, each replica is spread over multiple cores by putting different subsets of\nthe neurons on each core. Ensemble training is yet a third type of parallelism that can be wrapped\n5\n",
    "JFT 1: Tea party; Easter; Bridal shower; Baby shower; Easter Bunny; ...\nJFT 2: Bridge; Cable-stayed bridge; Suspension bridge; Viaduct; Chimney; ...\nJFT 3: Toyota Corolla E100; Opel Signum; Opel Astra; Mazda Familia; ...\nTable 2: Example classes from clusters computed by our covariance matrix clustering algorithm\naround the other two types, but only if a lot more cores are available. Waiting for several years to\ntrain an ensemble of models was not an option, so we needed a much faster way to improve the\nbaseline model.\n5.2\nSpecialist Models\nWhen the number of classes is very large, it makes sense for the cumbersome model to be an en-\nsemble that contains one generalist model trained on all the data and many “specialist” models, each\nof which is trained on data that is highly enriched in examples from a very confusable subset of the\nclasses (like different types of mushroom). The softmax of this type of specialist can be made much\nsmaller by combining all of the classes it does not care about into a single dustbin class.\nTo reduce overﬁtting and share the work of learning lower level feature detectors, each specialist\nmodel is initialized with the weights of the generalist model. These weights are then slightly modi-\nﬁed by training the specialist with half its examples coming from its special subset and half sampled\nat random from the remainder of the training set. After training, we can correct for the biased train-\ning set by incrementing the logit of the dustbin class by the log of the proportion by which the\nspecialist class is oversampled.\n5.3\nAssigning classes to specialists\nIn order to derive groupings of object categories for the specialists, we decided to focus on categories\nthat our full network often confuses. Even though we could have computed the confusion matrix\nand used it as a way to ﬁnd such clusters, we opted for a simpler approach that does not require the\ntrue labels to construct the clusters.\nIn particular, we apply a clustering algorithm to the covariance matrix of the predictions of our\ngeneralist model, so that a set of classes Sm that are often predicted together will be used as targets\nfor one of our specialist models, m. We applied an on-line version of the K-means algorithm to the\ncolumns of the covariance matrix, and obtained reasonable clusters (shown in Table 2). We tried\nseveral clustering algorithms which produced similar results.\n5.4\nPerforming inference with ensembles of specialists\nBefore investigating what happens when specialist models are distilled, we wanted to see how well\nensembles containing specialists performed. In addition to the specialist models, we always have a\ngeneralist model so that we can deal with classes for which we have no specialists and so that we\ncan decide which specialists to use. Given an input image x, we do top-one classiﬁcation in two\nsteps:\nStep 1: For each test case, we ﬁnd the n most probable classes according to the generalist model.\nCall this set of classes k. In our experiments, we used n = 1.\nStep 2: We then take all the specialist models, m, whose special subset of confusable classes, Sm,\nhas a non-empty intersection with k and call this the active set of specialists Ak (note that this set\nmay be empty). We then ﬁnd the full probability distribution q over all the classes that minimizes:\nKL(pg, q) +\nX\nm∈Ak\nKL(pm, q)\n(5)\nwhere KL denotes the KL divergence, and pm pg denote the probability distribution of a specialist\nmodel or the generalist full model. The distribution pm is a distribution over all the specialist classes\nof m plus a single dustbin class, so when computing its KL divergence from the full q distribution\nwe sum all of the probabilities that the full q distribution assigns to all the classes in m’s dustbin.\n6\n",
    "System\nConditional Test Accuracy\nTest Accuracy\nBaseline\n43.1%\n25.0%\n+ 61 Specialist models\n45.9%\n26.1%\nTable 3: Classiﬁcation accuracy (top 1) on the JFT development set.\n# of specialists covering\n# of test examples\ndelta in top1 correct\nrelative accuracy change\n0\n350037\n0\n0.0%\n1\n141993\n+1421\n+3.4%\n2\n67161\n+1572\n+7.4%\n3\n38801\n+1124\n+8.8%\n4\n26298\n+835\n+10.5%\n5\n16474\n+561\n+11.1%\n6\n10682\n+362\n+11.3%\n7\n7376\n+232\n+12.8%\n8\n4703\n+182\n+13.6%\n9\n4706\n+208\n+16.6%\n10 or more\n9082\n+324\n+14.1%\nTable 4: Top 1 accuracy improvement by # of specialist models covering correct class on the JFT\ntest set.\nEq. 5 does not have a general closed form solution, though when all the models produce a single\nprobability for each class the solution is either the arithmetic or geometric mean, depending on\nwhether we use KL(p, q) or KL(q, p)). We parameterize q = softmax(z) (with T = 1) and we\nuse gradient descent to optimize the logits z w.r.t. eq. 5. Note that this optimization must be carried\nout for each image.\n5.5\nResults\nStarting from the trained baseline full network, the specialists train extremely fast (a few days in-\nstead of many weeks for JFT). Also, all the specialists are trained completely independently. Table\n3 shows the absolute test accuracy for the baseline system and the baseline system combined with\nthe specialist models. With 61 specialist models, there is a 4.4% relative improvement in test ac-\ncuracy overall. We also report conditional test accuracy, which is the accuracy by only considering\nexamples belonging to the specialist classes, and restricting our predictions to that subset of classes.\nFor our JFT specialist experiments, we trained 61 specialist models, each with 300 classes (plus the\ndustbin class). Because the sets of classes for the specialists are not disjoint, we often had multiple\nspecialists covering a particular image class. Table 4 shows the number of test set examples, the\nchange in the number of examples correct at position 1 when using the specialist(s), and the rela-\ntive percentage improvement in top1 accuracy for the JFT dataset broken down by the number of\nspecialists covering the class. We are encouraged by the general trend that accuracy improvements\nare larger when we have more specialists covering a particular class, since training independent\nspecialist models is very easy to parallelize.\n6\nSoft Targets as Regularizers\nOne of our main claims about using soft targets instead of hard targets is that a lot of helpful infor-\nmation can be carried in soft targets that could not possibly be encoded with a single hard target. In\nthis section we demonstrate that this is a very large effect by using far less data to ﬁt the 85M pa-\nrameters of the baseline speech model described earlier. Table 5 shows that with only 3% of the data\n(about 20M examples), training the baseline model with hard targets leads to severe overﬁtting (we\ndid early stopping, as the accuracy drops sharply after reaching 44.5%), whereas the same model\ntrained with soft targets is able to recover almost all the information in the full training set (about\n2% shy). It is even more remarkable to note that we did not have to do early stopping: the system\nwith soft targets simply “converged” to 57%. This shows that soft targets are a very effective way of\ncommunicating the regularities discovered by a model trained on all of the data to another model.\n7\n",
    "System & training set\nTrain Frame Accuracy\nTest Frame Accuracy\nBaseline (100% of training set)\n63.4%\n58.9%\nBaseline (3% of training set)\n67.3%\n44.5%\nSoft Targets (3% of training set)\n65.4%\n57.0%\nTable 5: Soft targets allow a new model to generalize well from only 3% of the training set. The soft\ntargets are obtained by training on the full training set.\n6.1\nUsing soft targets to prevent specialists from overﬁtting\nThe specialists that we used in our experiments on the JFT dataset collapsed all of their non-specialist\nclasses into a single dustbin class. If we allow specialists to have a full softmax over all classes,\nthere may be a much better way to prevent them overﬁtting than using early stopping. A specialist\nis trained on data that is highly enriched in its special classes. This means that the effective size of\nits training set is much smaller and it has a strong tendency to overﬁt on its special classes. This\nproblem cannot be solved by making the specialist a lot smaller because then we lose the very helpful\ntransfer effects we get from modeling all of the non-specialist classes.\nOur experiment using 3% of the speech data strongly suggests that if a specialist is initialized with\nthe weights of the generalist, we can make it retain nearly all of its knowledge about the non-special\nclasses by training it with soft targets for the non-special classes in addition to training it with hard\ntargets. The soft targets can be provided by the generalist. We are currently exploring this approach.\n7\nRelationship to Mixtures of Experts\nThe use of specialists that are trained on subsets of the data has some resemblance to mixtures of\nexperts [6] which use a gating network to compute the probability of assigning each example to each\nexpert. At the same time as the experts are learning to deal with the examples assigned to them, the\ngating network is learning to choose which experts to assign each example to based on the relative\ndiscriminative performance of the experts for that example. Using the discriminative performance\nof the experts to determine the learned assignments is much better than simply clustering the input\nvectors and assigning an expert to each cluster, but it makes the training hard to parallelize: First,\nthe weighted training set for each expert keeps changing in a way that depends on all the other\nexperts and second, the gating network needs to compare the performance of different experts on\nthe same example to know how to revise its assignment probabilities. These difﬁculties have meant\nthat mixtures of experts are rarely used in the regime where they might be most beneﬁcial: tasks\nwith huge datasets that contain distinctly different subsets.\nIt is much easier to parallelize the training of multiple specialists. We ﬁrst train a generalist model\nand then use the confusion matrix to deﬁne the subsets that the specialists are trained on. Once these\nsubsets have been deﬁned the specialists can be trained entirely independently. At test time we can\nuse the predictions from the generalist model to decide which specialists are relevant and only these\nspecialists need to be run.\n8\nDiscussion\nWe have shown that distilling works very well for transferring knowledge from an ensemble or\nfrom a large highly regularized model into a smaller, distilled model. On MNIST distillation works\nremarkably well even when the transfer set that is used to train the distilled model lacks any examples\nof one or more of the classes. For a deep acoustic model that is version of the one used by Android\nvoice search, we have shown that nearly all of the improvement that is achieved by training an\nensemble of deep neural nets can be distilled into a single neural net of the same size which is far\neasier to deploy.\nFor really big neural networks, it can be infeasible even to train a full ensemble, but we have shown\nthat the performance of a single really big net that has been trained for a very long time can be signif-\nicantly improved by learning a large number of specialist nets, each of which learns to discriminate\nbetween the classes in a highly confusable cluster. We have not yet shown that we can distill the\nknowledge in the specialists back into the single large net.\n8\n",
    "Acknowledgments\nWe thank Yangqing Jia for assistance with training models on ImageNet and Ilya Sutskever and\nYoram Singer for helpful discussions.\nReferences\n[1] C. Buciluˇa, R. Caruana, and A. Niculescu-Mizil. Model compression. In Proceedings of the\n12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD\n’06, pages 535–541, New York, NY, USA, 2006. ACM.\n[2] J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin, Q. V. Le, M. Z. Mao, M. Ranzato,\nA. Senior, P. Tucker, K. Yang, and A. Y. Ng. Large scale distributed deep networks. In NIPS,\n2012.\n[3] T. G. Dietterich. Ensemble methods in machine learning. In Multiple classiﬁer systems, pages\n1–15. Springer, 2000.\n[4] G. E. Hinton, L. Deng, D. Yu, G. E. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke,\nP. Nguyen, T. N Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in\nspeech recognition: The shared views of four research groups. Signal Processing Magazine,\nIEEE, 29(6):82–97, 2012.\n[5] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov.\nIm-\nproving neural networks by preventing co-adaptation of feature detectors.\narXiv preprint\narXiv:1207.0580, 2012.\n[6] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts.\nNeural computation, 3(1):79–87, 1991.\n[7] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional\nneural networks. In Advances in Neural Information Processing Systems, pages 1097–1105,\n2012.\n[8] J. Li, R. Zhao, J. Huang, and Y. Gong. Learning small-size dnn with output-distribution-based\ncriteria. In Proceedings Interspeech 2014, pages 1910–1914, 2014.\n[9] N. Srivastava, G.E. Hinton, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Dropout:\nA simple way to prevent neural networks from overﬁtting. The Journal of Machine Learning\nResearch, 15(1):1929–1958, 2014.\n9\n"
  ],
  "full_text": "arXiv:1503.02531v1  [stat.ML]  9 Mar 2015\nDistilling the Knowledge in a Neural Network\nGeoffrey Hinton∗†\nGoogle Inc.\nMountain View\ngeoffhinton@google.com\nOriol Vinyals†\nGoogle Inc.\nMountain View\nvinyals@google.com\nJeff Dean\nGoogle Inc.\nMountain View\njeff@google.com\nAbstract\nA very simple way to improve the performance of almost any machine learning\nalgorithm is to train many different models on the same data and then to average\ntheir predictions [3]. Unfortunately, making predictions using a whole ensemble\nof models is cumbersome and may be too computationally expensive to allow de-\nployment to a large number of users, especially if the individual models are large\nneural nets. Caruana and his collaborators [1] have shown that it is possible to\ncompress the knowledge in an ensemble into a single model which is much eas-\nier to deploy and we develop this approach further using a different compression\ntechnique. We achieve some surprising results on MNIST and we show that we\ncan signiﬁcantly improve the acoustic model of a heavily used commercial system\nby distilling the knowledge in an ensemble of models into a single model. We also\nintroduce a new type of ensemble composed of one or more full models and many\nspecialist models which learn to distinguish ﬁne-grained classes that the full mod-\nels confuse. Unlike a mixture of experts, these specialist models can be trained\nrapidly and in parallel.\n1\nIntroduction\nMany insects have a larval form that is optimized for extracting energy and nutrients from the envi-\nronment and a completely different adult form that is optimized for the very different requirements\nof traveling and reproduction. In large-scale machine learning, we typically use very similar models\nfor the training stage and the deployment stage despite their very different requirements: For tasks\nlike speech and object recognition, training must extract structure from very large, highly redundant\ndatasets but it does not need to operate in real time and it can use a huge amount of computation.\nDeployment to a large number of users, however, has much more stringent requirements on latency\nand computational resources. The analogy with insects suggests that we should be willing to train\nvery cumbersome models if that makes it easier to extract structure from the data. The cumbersome\nmodel could be an ensemble of separately trained models or a single very large model trained with\na very strong regularizer such as dropout [9]. Once the cumbersome model has been trained, we\ncan then use a different kind of training, which we call “distillation” to transfer the knowledge from\nthe cumbersome model to a small model that is more suitable for deployment. A version of this\nstrategy has already been pioneered by Rich Caruana and his collaborators [1]. In their important\npaper they demonstrate convincingly that the knowledge acquired by a large ensemble of models\ncan be transferred to a single small model.\nA conceptual block that may have prevented more investigation of this very promising approach is\nthat we tend to identify the knowledge in a trained model with the learned parameter values and this\nmakes it hard to see how we can change the form of the model but keep the same knowledge. A more\nabstract view of the knowledge, that frees it from any particular instantiation, is that it is a learned\n∗Also afﬁliated with the University of Toronto and the Canadian Institute for Advanced Research.\n†Equal contribution.\n1\n\n\nmapping from input vectors to output vectors. For cumbersome models that learn to discriminate\nbetween a large number of classes, the normal training objective is to maximize the average log\nprobability of the correct answer, but a side-effect of the learning is that the trained model assigns\nprobabilities to all of the incorrect answers and even when these probabilities are very small, some\nof them are much larger than others. The relative probabilities of incorrect answers tell us a lot about\nhow the cumbersome model tends to generalize. An image of a BMW, for example, may only have\na very small chance of being mistaken for a garbage truck, but that mistake is still many times more\nprobable than mistaking it for a carrot.\nIt is generally accepted that the objective function used for training should reﬂect the true objective\nof the user as closely as possible. Despite this, models are usually trained to optimize performance\non the training data when the real objective is to generalize well to new data. It would clearly\nbe better to train models to generalize well, but this requires information about the correct way to\ngeneralize and this information is not normally available. When we are distilling the knowledge\nfrom a large model into a small one, however, we can train the small model to generalize in the same\nway as the large model. If the cumbersome model generalizes well because, for example, it is the\naverage of a large ensemble of different models, a small model trained to generalize in the same way\nwill typically do much better on test data than a small model that is trained in the normal way on the\nsame training set as was used to train the ensemble.\nAn obvious way to transfer the generalization ability of the cumbersome model to a small model is\nto use the class probabilities produced by the cumbersome model as “soft targets” for training the\nsmall model. For this transfer stage, we could use the same training set or a separate “transfer” set.\nWhen the cumbersome model is a large ensemble of simpler models, we can use an arithmetic or\ngeometric mean of their individual predictive distributions as the soft targets. When the soft targets\nhave high entropy, they provide much more information per training case than hard targets and much\nless variance in the gradient between training cases, so the small model can often be trained on much\nless data than the original cumbersome model and using a much higher learning rate.\nFor tasks like MNIST in which the cumbersome model almost always produces the correct answer\nwith very high conﬁdence, much of the information about the learned function resides in the ratios\nof very small probabilities in the soft targets. For example, one version of a 2 may be given a\nprobability of 10−6 of being a 3 and 10−9 of being a 7 whereas for another version it may be the\nother way around. This is valuable information that deﬁnes a rich similarity structure over the data\n(i. e. it says which 2’s look like 3’s and which look like 7’s) but it has very little inﬂuence on the\ncross-entropy cost function during the transfer stage because the probabilities are so close to zero.\nCaruana and his collaborators circumvent this problem by using the logits (the inputs to the ﬁnal\nsoftmax) rather than the probabilities produced by the softmax as the targets for learning the small\nmodel and they minimize the squared difference between the logits produced by the cumbersome\nmodel and the logits produced by the small model. Our more general solution, called “distillation”,\nis to raise the temperature of the ﬁnal softmax until the cumbersome model produces a suitably soft\nset of targets. We then use the same high temperature when training the small model to match these\nsoft targets. We show later that matching the logits of the cumbersome model is actually a special\ncase of distillation.\nThe transfer set that is used to train the small model could consist entirely of unlabeled data [1]\nor we could use the original training set. We have found that using the original training set works\nwell, especially if we add a small term to the objective function that encourages the small model\nto predict the true targets as well as matching the soft targets provided by the cumbersome model.\nTypically, the small model cannot exactly match the soft targets and erring in the direction of the\ncorrect answer turns out to be helpful.\n2\nDistillation\nNeural networks typically produce class probabilities by using a “softmax” output layer that converts\nthe logit, zi, computed for each class into a probability, qi, by comparing zi with the other logits.\nqi =\nexp(zi/T )\nP\nj exp(zj/T )\n(1)\n2\n\n\nwhere T is a temperature that is normally set to 1. Using a higher value for T produces a softer\nprobability distribution over classes.\nIn the simplest form of distillation, knowledge is transferred to the distilled model by training it on\na transfer set and using a soft target distribution for each case in the transfer set that is produced by\nusing the cumbersome model with a high temperature in its softmax. The same high temperature is\nused when training the distilled model, but after it has been trained it uses a temperature of 1.\nWhen the correct labels are known for all or some of the transfer set, this method can be signiﬁcantly\nimproved by also training the distilled model to produce the correct labels. One way to do this is\nto use the correct labels to modify the soft targets, but we found that a better way is to simply use\na weighted average of two different objective functions. The ﬁrst objective function is the cross\nentropy with the soft targets and this cross entropy is computed using the same high temperature in\nthe softmax of the distilled model as was used for generating the soft targets from the cumbersome\nmodel. The second objective function is the cross entropy with the correct labels. This is computed\nusing exactly the same logits in softmax of the distilled model but at a temperature of 1. We found\nthat the best results were generally obtained by using a condiderably lower weight on the second\nobjective function. Since the magnitudes of the gradients produced by the soft targets scale as 1/T 2\nit is important to multiply them by T 2 when using both hard and soft targets. This ensures that the\nrelative contributions of the hard and soft targets remain roughly unchanged if the temperature used\nfor distillation is changed while experimenting with meta-parameters.\n2.1\nMatching logits is a special case of distillation\nEach case in the transfer set contributes a cross-entropy gradient, dC/dzi, with respect to each\nlogit, zi of the distilled model. If the cumbersome model has logits vi which produce soft target\nprobabilities pi and the transfer training is done at a temperature of T , this gradient is given by:\n∂C\n∂zi\n= 1\nT (qi −pi) = 1\nT\n \nezi/T\nP\nj ezj/T −\nevi/T\nP\nj evj/T\n!\n(2)\nIf the temperature is high compared with the magnitude of the logits, we can approximate:\n∂C\n∂zi\n≈1\nT\n \n1 + zi/T\nN + P\nj zj/T −\n1 + vi/T\nN + P\nj vj/T\n!\n(3)\nIf we now assume that the logits have been zero-meaned separately for each transfer case so that\nP\nj zj = P\nj vj = 0 Eq. 3 simpliﬁes to:\n∂C\n∂zi\n≈\n1\nNT 2 (zi −vi)\n(4)\nSo in the high temperature limit, distillation is equivalent to minimizing 1/2(zi −vi)2, provided the\nlogits are zero-meaned separately for each transfer case. At lower temperatures, distillation pays\nmuch less attention to matching logits that are much more negative than the average. This is poten-\ntially advantageous because these logits are almost completely unconstrained by the cost function\nused for training the cumbersome model so they could be very noisy. On the other hand, the very\nnegative logits may convey useful information about the knowledge acquired by the cumbersome\nmodel. Which of these effects dominates is an empirical question. We show that when the distilled\nmodel is much too small to capture all of the knowledege in the cumbersome model, intermedi-\nate temperatures work best which strongly suggests that ignoring the large negative logits can be\nhelpful.\n3\nPreliminary experiments on MNIST\nTo see how well distillation works, we trained a single large neural net with two hidden layers\nof 1200 rectiﬁed linear hidden units on all 60,000 training cases. The net was strongly regularized\nusing dropout and weight-constraints as described in [5]. Dropout can be viewed as a way of training\nan exponentially large ensemble of models that share weights. In addition, the input images were\n3\n\n\njittered by up to two pixels in any direction. This net achieved 67 test errors whereas a smaller\nnet with two hidden layers of 800 rectiﬁed linear hidden units and no regularization achieved 146\nerrors. But if the smaller net was regularized solely by adding the additional task of matching the soft\ntargets produced by the large net at a temperature of 20, it achieved 74 test errors. This shows that\nsoft targets can transfer a great deal of knowledge to the distilled model, including the knowledge\nabout how to generalize that is learned from translated training data even though the transfer set does\nnot contain any translations.\nWhen the distilled net had 300 or more units in each of its two hidden layers, all temperatures above\n8 gave fairly similar results. But when this was radically reduced to 30 units per layer, temperatures\nin the range 2.5 to 4 worked signiﬁcantly better than higher or lower temperatures.\nWe then tried omitting all examples of the digit 3 from the transfer set. So from the perspective\nof the distilled model, 3 is a mythical digit that it has never seen. Despite this, the distilled model\nonly makes 206 test errors of which 133 are on the 1010 threes in the test set. Most of the errors\nare caused by the fact that the learned bias for the 3 class is much too low. If this bias is increased\nby 3.5 (which optimizes overall performance on the test set), the distilled model makes 109 errors\nof which 14 are on 3s. So with the right bias, the distilled model gets 98.6% of the test 3s correct\ndespite never having seen a 3 during training. If the transfer set contains only the 7s and 8s from the\ntraining set, the distilled model makes 47.3% test errors, but when the biases for 7 and 8 are reduced\nby 7.6 to optimize test performance, this falls to 13.2% test errors.\n4\nExperiments on speech recognition\nIn this section, we investigate the effects of ensembling Deep Neural Network (DNN) acoustic\nmodels that are used in Automatic Speech Recognition (ASR). We show that the distillation strategy\nthat we propose in this paper achieves the desired effect of distilling an ensemble of models into a\nsingle model that works signiﬁcantly better than a model of the same size that is learned directly\nfrom the same training data.\nState-of-the-art ASR systems currently use DNNs to map a (short) temporal context of features\nderived from the waveform to a probability distribution over the discrete states of a Hidden Markov\nModel (HMM) [4]. More speciﬁcally, the DNN produces a probability distribution over clusters of\ntri-phone states at each time and a decoder then ﬁnds a path through the HMM states that is the best\ncompromise between using high probability states and producing a transcription that is probable\nunder the language model.\nAlthough it is possible (and desirable) to train the DNN in such a way that the decoder (and, thus,\nthe language model) is taken into account by marginalizing over all possible paths, it is common to\ntrain the DNN to perform frame-by-frame classiﬁcation by (locally) minimizing the cross entropy\nbetween the predictions made by the net and the labels given by a forced alignment with the ground\ntruth sequence of states for each observation:\nθ = arg max\nθ′ P(ht|st; θ′)\nwhere θ are the parameters of our acoustic model P which maps acoustic observations at time t,\nst, to a probability, P(ht|st; θ′) , of the “correct” HMM state ht, which is determined by a forced\nalignment with the correct sequence of words. The model is trained with a distributed stochastic\ngradient descent approach.\nWe use an architecture with 8 hidden layers each containing 2560 rectiﬁed linear units and a ﬁnal\nsoftmax layer with 14,000 labels (HMM targets ht). The input is 26 frames of 40 Mel-scaled ﬁlter-\nbank coefﬁcients with a 10ms advance per frame and we predict the HMM state of 21st frame. The\ntotal number of parameters is about 85M. This is a slightly outdated version of the acoustic model\nused by Android voice search, and should be considered as a very strong baseline. To train the DNN\nacoustic model we use about 2000 hours of spoken English data, which yields about 700M training\nexamples. This system achieves a frame accuracy of 58.9%, and a Word Error Rate (WER) of 10.9%\non our development set.\n4\n\n\nSystem\nTest Frame Accuracy\nWER\nBaseline\n58.9%\n10.9%\n10xEnsemble\n61.1%\n10.7%\nDistilled Single model\n60.8%\n10.7%\nTable 1: Frame classiﬁcation accuracy and WER showing that the distilled single model performs\nabout as well as the averaged predictions of 10 models that were used to create the soft targets.\n4.1\nResults\nWe trained 10 separate models to predict P(ht|st; θ), using exactly the same architecture and train-\ning procedure as the baseline. The models are randomly initialized with different initial parameter\nvalues and we ﬁnd that this creates sufﬁcient diversity in the trained models to allow the averaged\npredictions of the ensemble to signiﬁcantly outperform the individual models. We have explored\nadding diversity to the models by varying the sets of data that each model sees, but we found this\nto not signiﬁcantly change our results, so we opted for the simpler approach. For the distillation we\ntried temperatures of [1, 2, 5, 10] and used a relative weight of 0.5 on the cross-entropy for the hard\ntargets, where bold font indicates the best value that was used for table 1 .\nTable 1 shows that, indeed, our distillation approach is able to extract more useful information from\nthe training set than simply using the hard labels to train a single model. More than 80% of the\nimprovement in frame classiﬁcation accuracy achieved by using an ensemble of 10 models is trans-\nferred to the distilled model which is similar to the improvement we observed in our preliminary\nexperiments on MNIST. The ensemble gives a smaller improvement on the ultimate objective of\nWER (on a 23K-word test set) due to the mismatch in the objective function, but again, the im-\nprovement in WER achieved by the ensemble is transferred to the distilled model.\nWe have recently become aware of related work on learning a small acoustic model by matching\nthe class probabilities of an already trained larger model [8]. However, they do the distillation at a\ntemperature of 1 using a large unlabeled dataset and their best distilled model only reduces the error\nrate of the small model by 28% of the gap between the error rates of the large and small models\nwhen they are both trained with hard labels.\n5\nTraining ensembles of specialists on very big datasets\nTraining an ensemble of models is a very simple way to take advantage of parallel computation and\nthe usual objection that an ensemble requires too much computation at test time can be dealt with\nby using distillation. There is, however, another important objection to ensembles: If the individual\nmodels are large neural networks and the dataset is very large, the amount of computation required\nat training time is excessive, even though it is easy to parallelize.\nIn this section we give an example of such a dataset and we show how learning specialist models that\neach focus on a different confusable subset of the classes can reduce the total amount of computation\nrequired to learn an ensemble. The main problem with specialists that focus on making ﬁne-grained\ndistinctions is that they overﬁt very easily and we describe how this overﬁtting may be prevented by\nusing soft targets.\n5.1\nThe JFT dataset\nJFT is an internal Google dataset that has 100 million labeled images with 15,000 labels. When we\ndid this work, Google’s baseline model for JFT was a deep convolutional neural network [7] that had\nbeen trained for about six months using asynchronous stochastic gradient descent on a large number\nof cores. This training used two types of parallelism [2]. First, there were many replicas of the\nneural net running on different sets of cores and processing different mini-batches from the training\nset. Each replica computes the average gradient on its current mini-batch and sends this gradient\nto a sharded parameter server which sends back new values for the parameters. These new values\nreﬂect all of the gradients received by the parameter server since the last time it sent parameters\nto the replica. Second, each replica is spread over multiple cores by putting different subsets of\nthe neurons on each core. Ensemble training is yet a third type of parallelism that can be wrapped\n5\n\n\nJFT 1: Tea party; Easter; Bridal shower; Baby shower; Easter Bunny; ...\nJFT 2: Bridge; Cable-stayed bridge; Suspension bridge; Viaduct; Chimney; ...\nJFT 3: Toyota Corolla E100; Opel Signum; Opel Astra; Mazda Familia; ...\nTable 2: Example classes from clusters computed by our covariance matrix clustering algorithm\naround the other two types, but only if a lot more cores are available. Waiting for several years to\ntrain an ensemble of models was not an option, so we needed a much faster way to improve the\nbaseline model.\n5.2\nSpecialist Models\nWhen the number of classes is very large, it makes sense for the cumbersome model to be an en-\nsemble that contains one generalist model trained on all the data and many “specialist” models, each\nof which is trained on data that is highly enriched in examples from a very confusable subset of the\nclasses (like different types of mushroom). The softmax of this type of specialist can be made much\nsmaller by combining all of the classes it does not care about into a single dustbin class.\nTo reduce overﬁtting and share the work of learning lower level feature detectors, each specialist\nmodel is initialized with the weights of the generalist model. These weights are then slightly modi-\nﬁed by training the specialist with half its examples coming from its special subset and half sampled\nat random from the remainder of the training set. After training, we can correct for the biased train-\ning set by incrementing the logit of the dustbin class by the log of the proportion by which the\nspecialist class is oversampled.\n5.3\nAssigning classes to specialists\nIn order to derive groupings of object categories for the specialists, we decided to focus on categories\nthat our full network often confuses. Even though we could have computed the confusion matrix\nand used it as a way to ﬁnd such clusters, we opted for a simpler approach that does not require the\ntrue labels to construct the clusters.\nIn particular, we apply a clustering algorithm to the covariance matrix of the predictions of our\ngeneralist model, so that a set of classes Sm that are often predicted together will be used as targets\nfor one of our specialist models, m. We applied an on-line version of the K-means algorithm to the\ncolumns of the covariance matrix, and obtained reasonable clusters (shown in Table 2). We tried\nseveral clustering algorithms which produced similar results.\n5.4\nPerforming inference with ensembles of specialists\nBefore investigating what happens when specialist models are distilled, we wanted to see how well\nensembles containing specialists performed. In addition to the specialist models, we always have a\ngeneralist model so that we can deal with classes for which we have no specialists and so that we\ncan decide which specialists to use. Given an input image x, we do top-one classiﬁcation in two\nsteps:\nStep 1: For each test case, we ﬁnd the n most probable classes according to the generalist model.\nCall this set of classes k. In our experiments, we used n = 1.\nStep 2: We then take all the specialist models, m, whose special subset of confusable classes, Sm,\nhas a non-empty intersection with k and call this the active set of specialists Ak (note that this set\nmay be empty). We then ﬁnd the full probability distribution q over all the classes that minimizes:\nKL(pg, q) +\nX\nm∈Ak\nKL(pm, q)\n(5)\nwhere KL denotes the KL divergence, and pm pg denote the probability distribution of a specialist\nmodel or the generalist full model. The distribution pm is a distribution over all the specialist classes\nof m plus a single dustbin class, so when computing its KL divergence from the full q distribution\nwe sum all of the probabilities that the full q distribution assigns to all the classes in m’s dustbin.\n6\n\n\nSystem\nConditional Test Accuracy\nTest Accuracy\nBaseline\n43.1%\n25.0%\n+ 61 Specialist models\n45.9%\n26.1%\nTable 3: Classiﬁcation accuracy (top 1) on the JFT development set.\n# of specialists covering\n# of test examples\ndelta in top1 correct\nrelative accuracy change\n0\n350037\n0\n0.0%\n1\n141993\n+1421\n+3.4%\n2\n67161\n+1572\n+7.4%\n3\n38801\n+1124\n+8.8%\n4\n26298\n+835\n+10.5%\n5\n16474\n+561\n+11.1%\n6\n10682\n+362\n+11.3%\n7\n7376\n+232\n+12.8%\n8\n4703\n+182\n+13.6%\n9\n4706\n+208\n+16.6%\n10 or more\n9082\n+324\n+14.1%\nTable 4: Top 1 accuracy improvement by # of specialist models covering correct class on the JFT\ntest set.\nEq. 5 does not have a general closed form solution, though when all the models produce a single\nprobability for each class the solution is either the arithmetic or geometric mean, depending on\nwhether we use KL(p, q) or KL(q, p)). We parameterize q = softmax(z) (with T = 1) and we\nuse gradient descent to optimize the logits z w.r.t. eq. 5. Note that this optimization must be carried\nout for each image.\n5.5\nResults\nStarting from the trained baseline full network, the specialists train extremely fast (a few days in-\nstead of many weeks for JFT). Also, all the specialists are trained completely independently. Table\n3 shows the absolute test accuracy for the baseline system and the baseline system combined with\nthe specialist models. With 61 specialist models, there is a 4.4% relative improvement in test ac-\ncuracy overall. We also report conditional test accuracy, which is the accuracy by only considering\nexamples belonging to the specialist classes, and restricting our predictions to that subset of classes.\nFor our JFT specialist experiments, we trained 61 specialist models, each with 300 classes (plus the\ndustbin class). Because the sets of classes for the specialists are not disjoint, we often had multiple\nspecialists covering a particular image class. Table 4 shows the number of test set examples, the\nchange in the number of examples correct at position 1 when using the specialist(s), and the rela-\ntive percentage improvement in top1 accuracy for the JFT dataset broken down by the number of\nspecialists covering the class. We are encouraged by the general trend that accuracy improvements\nare larger when we have more specialists covering a particular class, since training independent\nspecialist models is very easy to parallelize.\n6\nSoft Targets as Regularizers\nOne of our main claims about using soft targets instead of hard targets is that a lot of helpful infor-\nmation can be carried in soft targets that could not possibly be encoded with a single hard target. In\nthis section we demonstrate that this is a very large effect by using far less data to ﬁt the 85M pa-\nrameters of the baseline speech model described earlier. Table 5 shows that with only 3% of the data\n(about 20M examples), training the baseline model with hard targets leads to severe overﬁtting (we\ndid early stopping, as the accuracy drops sharply after reaching 44.5%), whereas the same model\ntrained with soft targets is able to recover almost all the information in the full training set (about\n2% shy). It is even more remarkable to note that we did not have to do early stopping: the system\nwith soft targets simply “converged” to 57%. This shows that soft targets are a very effective way of\ncommunicating the regularities discovered by a model trained on all of the data to another model.\n7\n\n\nSystem & training set\nTrain Frame Accuracy\nTest Frame Accuracy\nBaseline (100% of training set)\n63.4%\n58.9%\nBaseline (3% of training set)\n67.3%\n44.5%\nSoft Targets (3% of training set)\n65.4%\n57.0%\nTable 5: Soft targets allow a new model to generalize well from only 3% of the training set. The soft\ntargets are obtained by training on the full training set.\n6.1\nUsing soft targets to prevent specialists from overﬁtting\nThe specialists that we used in our experiments on the JFT dataset collapsed all of their non-specialist\nclasses into a single dustbin class. If we allow specialists to have a full softmax over all classes,\nthere may be a much better way to prevent them overﬁtting than using early stopping. A specialist\nis trained on data that is highly enriched in its special classes. This means that the effective size of\nits training set is much smaller and it has a strong tendency to overﬁt on its special classes. This\nproblem cannot be solved by making the specialist a lot smaller because then we lose the very helpful\ntransfer effects we get from modeling all of the non-specialist classes.\nOur experiment using 3% of the speech data strongly suggests that if a specialist is initialized with\nthe weights of the generalist, we can make it retain nearly all of its knowledge about the non-special\nclasses by training it with soft targets for the non-special classes in addition to training it with hard\ntargets. The soft targets can be provided by the generalist. We are currently exploring this approach.\n7\nRelationship to Mixtures of Experts\nThe use of specialists that are trained on subsets of the data has some resemblance to mixtures of\nexperts [6] which use a gating network to compute the probability of assigning each example to each\nexpert. At the same time as the experts are learning to deal with the examples assigned to them, the\ngating network is learning to choose which experts to assign each example to based on the relative\ndiscriminative performance of the experts for that example. Using the discriminative performance\nof the experts to determine the learned assignments is much better than simply clustering the input\nvectors and assigning an expert to each cluster, but it makes the training hard to parallelize: First,\nthe weighted training set for each expert keeps changing in a way that depends on all the other\nexperts and second, the gating network needs to compare the performance of different experts on\nthe same example to know how to revise its assignment probabilities. These difﬁculties have meant\nthat mixtures of experts are rarely used in the regime where they might be most beneﬁcial: tasks\nwith huge datasets that contain distinctly different subsets.\nIt is much easier to parallelize the training of multiple specialists. We ﬁrst train a generalist model\nand then use the confusion matrix to deﬁne the subsets that the specialists are trained on. Once these\nsubsets have been deﬁned the specialists can be trained entirely independently. At test time we can\nuse the predictions from the generalist model to decide which specialists are relevant and only these\nspecialists need to be run.\n8\nDiscussion\nWe have shown that distilling works very well for transferring knowledge from an ensemble or\nfrom a large highly regularized model into a smaller, distilled model. On MNIST distillation works\nremarkably well even when the transfer set that is used to train the distilled model lacks any examples\nof one or more of the classes. For a deep acoustic model that is version of the one used by Android\nvoice search, we have shown that nearly all of the improvement that is achieved by training an\nensemble of deep neural nets can be distilled into a single neural net of the same size which is far\neasier to deploy.\nFor really big neural networks, it can be infeasible even to train a full ensemble, but we have shown\nthat the performance of a single really big net that has been trained for a very long time can be signif-\nicantly improved by learning a large number of specialist nets, each of which learns to discriminate\nbetween the classes in a highly confusable cluster. We have not yet shown that we can distill the\nknowledge in the specialists back into the single large net.\n8\n\n\nAcknowledgments\nWe thank Yangqing Jia for assistance with training models on ImageNet and Ilya Sutskever and\nYoram Singer for helpful discussions.\nReferences\n[1] C. Buciluˇa, R. Caruana, and A. Niculescu-Mizil. Model compression. In Proceedings of the\n12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD\n’06, pages 535–541, New York, NY, USA, 2006. ACM.\n[2] J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin, Q. V. Le, M. Z. Mao, M. Ranzato,\nA. Senior, P. Tucker, K. Yang, and A. Y. Ng. Large scale distributed deep networks. In NIPS,\n2012.\n[3] T. G. Dietterich. Ensemble methods in machine learning. In Multiple classiﬁer systems, pages\n1–15. Springer, 2000.\n[4] G. E. Hinton, L. Deng, D. Yu, G. E. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke,\nP. Nguyen, T. N Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in\nspeech recognition: The shared views of four research groups. Signal Processing Magazine,\nIEEE, 29(6):82–97, 2012.\n[5] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov.\nIm-\nproving neural networks by preventing co-adaptation of feature detectors.\narXiv preprint\narXiv:1207.0580, 2012.\n[6] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts.\nNeural computation, 3(1):79–87, 1991.\n[7] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional\nneural networks. In Advances in Neural Information Processing Systems, pages 1097–1105,\n2012.\n[8] J. Li, R. Zhao, J. Huang, and Y. Gong. Learning small-size dnn with output-distribution-based\ncriteria. In Proceedings Interspeech 2014, pages 1910–1914, 2014.\n[9] N. Srivastava, G.E. Hinton, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Dropout:\nA simple way to prevent neural networks from overﬁtting. The Journal of Machine Learning\nResearch, 15(1):1929–1958, 2014.\n9\n"
}